python: process ...
python: tokenizing 2 json files ...
input_path: /home/gcloud/TransCoder/data/test_dataset/python/python.001.json.gz
language: python
output_path: /home/gcloud/TransCoder/data/test_dataset/python/python.001.with_comments.tok
line: b'{"repo_name":"anas-taji/knowledge","ref":"refs/heads/8.0","path":"attachment_edit/models/ir_attachment.py","content":"# -*- coding: utf-8 -*-\\n# \xc2\xa9 2015 Therp BV \\u003chttp://therp.nl\\u003e\\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl.html).\\n# -*- coding: utf-8 -*-\\nfrom openerp import models, fields, api\\n\\n\\nclass IrAttachment(models.Model):\\n    _inherit = \'ir.attachment\'\\n\\n    res_reference = fields.Reference(\\n        selection=\'_selection_res_reference\',\\n        string=\'Resource reference\', compute=\'_compute_res_reference\',\\n        inverse=\'_inverse_res_reference\')\\n\\n    @api.one\\n    @api.depends(\'res_id\', \'res_model\')\\n    def _compute_res_reference(self):\\n        if self.res_model and self.res_id:\\n            self.res_reference = \'%s,%s\' % (self.res_model, self.res_id)\\n\\n    @api.one\\n    def _inverse_res_reference(self):\\n        if self.res_reference:\\n            self.write({\\n                \'res_model\': self.res_reference._model._model,\\n                \'res_id\': self.res_reference.id,\\n            })\\n        else:\\n            self.write({\'res_model\': False, \'res_id\': False})\\n\\n    @api.model\\n    def _selection_res_reference(self):\\n        return self.env[\'ir.model\'].search([\\n            (\'osv_memory\', \'=\', False),\\n            (\'access_ids.group_id.users\', \'=\', self.env.uid)\\n        ]).mapped(lambda rec: (rec.model, rec.name))\\n"}\n'
line: b'{"repo_name":"apocquet/django","ref":"refs/heads/master","path":"django/contrib/gis/db/models/sql/conversion.py","content":"\\"\\"\\"\\nThis module holds simple classes to convert geospatial values from the\\ndatabase.\\n\\"\\"\\"\\n\\nfrom django.contrib.gis.db.models.fields import GeoSelectFormatMixin\\nfrom django.contrib.gis.geometry.backend import Geometry\\nfrom django.contrib.gis.measure import Area, Distance\\n\\n\\nclass BaseField(object):\\n    empty_strings_allowed = True\\n\\n    def get_db_converters(self, connection):\\n        return [self.from_db_value]\\n\\n    def select_format(self, compiler, sql, params):\\n        return sql, params\\n\\n\\nclass AreaField(BaseField):\\n    \\"Wrapper for Area values.\\"\\n    def __init__(self, area_att):\\n        self.area_att = area_att\\n\\n    def from_db_value(self, value, expression, connection, context):\\n        if value is not None:\\n            value = Area(**{self.area_att: value})\\n        return value\\n\\n    def get_internal_type(self):\\n        return \'AreaField\'\\n\\n\\nclass DistanceField(BaseField):\\n    \\"Wrapper for Distance values.\\"\\n    def __init__(self, distance_att):\\n        self.distance_att = distance_att\\n\\n    def from_db_value(self, value, expression, connection, context):\\n        if value is not None:\\n            value = Distance(**{self.distance_att: value})\\n        return value\\n\\n    def get_internal_type(self):\\n        return \'DistanceField\'\\n\\n\\nclass GeomField(GeoSelectFormatMixin, BaseField):\\n    \\"\\"\\"\\n    Wrapper for Geometry values.  It is a lightweight alternative to\\n    using GeometryField (which requires an SQL query upon instantiation).\\n    \\"\\"\\"\\n    # Hacky marker for get_db_converters()\\n    geom_type = None\\n\\n    def from_db_value(self, value, expression, connection, context):\\n        if value is not None:\\n            value = Geometry(value)\\n        return value\\n\\n    def get_internal_type(self):\\n        return \'GeometryField\'\\n\\n\\nclass GMLField(BaseField):\\n    \\"\\"\\"\\n    Wrapper for GML to be used by Oracle to ensure Database.LOB conversion.\\n    \\"\\"\\"\\n\\n    def get_internal_type(self):\\n        return \'GMLField\'\\n\\n    def from_db_value(self, value, expression, connection, context):\\n        return value\\n"}\n'
line: b'{"repo_name":"chenlian2015/skia_from_google","ref":"refs/heads/master","path":"tools/skp/page_sets/skia_youtube_desktop.py","content":"# Copyright 2014 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n# pylint: disable=W0401,W0614\\n\\n\\nfrom telemetry.page import page as page_module\\nfrom telemetry.page import page_set as page_set_module\\n\\n\\nclass SkiaBuildbotDesktopPage(page_module.Page):\\n\\n  def __init__(self, url, page_set):\\n    super(SkiaBuildbotDesktopPage, self).__init__(\\n        url=url,\\n        page_set=page_set,\\n        credentials_path=\'data/credentials.json\')\\n    self.user_agent_type = \'desktop\'\\n    self.archive_data_file = \'data/skia_youtube_desktop.json\'\\n\\n  def RunNavigateSteps(self, action_runner):\\n    action_runner.NavigateToPage(self)\\n    action_runner.Wait(25)\\n\\n\\nclass SkiaYoutubeDesktopPageSet(page_set_module.PageSet):\\n\\n  \\"\\"\\" Pages designed to represent the median, not highly optimized web \\"\\"\\"\\n\\n  def __init__(self):\\n    super(SkiaYoutubeDesktopPageSet, self).__init__(\\n      user_agent_type=\'desktop\',\\n      archive_data_file=\'data/skia_youtube_desktop.json\')\\n\\n    urls_list = [\\n      # Why: #3 (Alexa global)\\n      \'http://www.youtube.com/watch?v=PC57z-oDPLs\',\\n    ]\\n\\n    for url in urls_list:\\n      self.AddPage(SkiaBuildbotDesktopPage(url, self))\\n"}\n'
line: b'{"repo_name":"codeboy/projectile","ref":"refs/heads/master","path":"lib/treebeard/tests.py","content":"\\"Unit/Functional tests\\"\\n\\nimport functools\\nimport os\\nfrom django.contrib.admin.options import ModelAdmin\\nfrom django.contrib.admin.sites import AdminSite\\nfrom django.test import TestCase\\nfrom django.db import models, transaction\\nfrom django.contrib.auth.models import User\\nfrom django.db.models import Q\\nfrom django.conf import settings\\nfrom django import VERSION as DJANGO_VERSION\\n\\nfrom treebeard import numconv\\nfrom treebeard.exceptions import InvalidPosition, InvalidMoveToDescendant, \\\\\\n    PathOverflow, MissingNodeOrderBy\\nfrom treebeard.mp_tree import MP_Node\\nfrom treebeard.al_tree import AL_Node\\nfrom treebeard.ns_tree import NS_Node\\nfrom treebeard.forms import MoveNodeForm\\n\\n# ghetto app detection, there is probably some introspection method,\\n# but meh, this works\\nHAS_DJANGO_AUTH = \'django.contrib.auth\' in settings.INSTALLED_APPS\\n\\nBASE_DATA = [\\n  {\'data\':{\'desc\':\'1\'}},\\n  {\'data\':{\'desc\':\'2\'}, \'children\':[\\n    {\'data\':{\'desc\':\'21\'}},\\n    {\'data\':{\'desc\':\'22\'}},\\n    {\'data\':{\'desc\':\'23\'}, \'children\':[\\n      {\'data\':{\'desc\':\'231\'}},\\n    ]},\\n    {\'data\':{\'desc\':\'24\'}},\\n  ]},\\n  {\'data\':{\'desc\':\'3\'}},\\n  {\'data\':{\'desc\':\'4\'}, \'children\':[\\n    {\'data\':{\'desc\':\'41\'}},\\n  ]},\\n]\\n\\n\\nclass MP_TestNode(MP_Node):\\n    steplen = 3\\n\\n    desc = models.CharField(max_length=255)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass MP_TestNodeSomeDep(models.Model):\\n    node = models.ForeignKey(MP_TestNode)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass NS_TestNode(NS_Node):\\n    desc = models.CharField(max_length=255)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass NS_TestNodeSomeDep(models.Model):\\n    node = models.ForeignKey(NS_TestNode)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass AL_TestNode(AL_Node):\\n    parent = models.ForeignKey(\'self\',\\n                               related_name=\'children_set\',\\n                               null=True,\\n                               db_index=True)\\n    sib_order = models.PositiveIntegerField()\\n    desc = models.CharField(max_length=255)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass AL_TestNodeSomeDep(models.Model):\\n    node = models.ForeignKey(AL_TestNode)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass MP_TestNodeSorted(MP_Node):\\n    steplen = 1\\n    node_order_by = [\'val1\', \'val2\', \'desc\']\\n    val1 = models.IntegerField()\\n    val2 = models.IntegerField()\\n    desc = models.CharField(max_length=255)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass NS_TestNodeSorted(NS_Node):\\n    node_order_by = [\'val1\', \'val2\', \'desc\']\\n    val1 = models.IntegerField()\\n    val2 = models.IntegerField()\\n    desc = models.CharField(max_length=255)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass AL_TestNodeSorted(AL_Node):\\n    parent = models.ForeignKey(\'self\',\\n                               related_name=\'children_set\',\\n                               null=True,\\n                               db_index=True)\\n    node_order_by = [\'val1\', \'val2\', \'desc\']\\n    val1 = models.IntegerField()\\n    val2 = models.IntegerField()\\n    desc = models.CharField(max_length=255)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass MP_TestNodeAlphabet(MP_Node):\\n    steplen = 2\\n\\n    numval = models.IntegerField()\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass MP_TestNodeSmallStep(MP_Node):\\n    steplen = 1\\n    alphabet = \'0123456789\'\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass MP_TestNodeSortedAutoNow(MP_Node):\\n    desc = models.CharField(max_length=255)\\n    created = models.DateTimeField(auto_now_add=True)\\n\\n    node_order_by = [\'created\']\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass MP_TestNodeShortPath(MP_Node):\\n    steplen = 1\\n    alphabet = \'01234\'\\n    desc = models.CharField(max_length=255)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n# This is how you change the default fields defined in a Django abstract class\\n# (in this case, MP_Node), since Django doesn\'t allow overriding fields, only\\n# mehods and attributes\\nMP_TestNodeShortPath._meta.get_field(\'path\').max_length = 4\\n\\n\\nif DJANGO_VERSION \\u003e= (1, 1):  # pragma: no cover\\n\\n    class MP_TestNode_Proxy(MP_TestNode):\\n        class Meta:\\n            proxy = True\\n\\n\\n    class NS_TestNode_Proxy(NS_TestNode):\\n        class Meta:\\n            proxy = True\\n\\n\\n    class AL_TestNode_Proxy(AL_TestNode):\\n        class Meta:\\n            proxy = True\\n\\n\\nclass MP_TestSortedNodeShortPath(MP_Node):\\n    steplen = 1\\n    alphabet = \'01234\'\\n    desc = models.CharField(max_length=255)\\n\\n    node_order_by = [\'desc\']\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\nMP_TestSortedNodeShortPath._meta.get_field(\'path\').max_length = 4\\n\\n\\nif HAS_DJANGO_AUTH:\\n\\n    class MP_TestIssue14(MP_Node):\\n        name = models.CharField(max_length=255)\\n        users = models.ManyToManyField(User)\\n\\n\\ndef testtype(treetype, proxy):\\n\\n    def decorator(f):\\n\\n        @functools.wraps(f)\\n        def _testtype(self):\\n            {\'MP\': self.set_MP,\\n             \'AL\': self.set_AL,\\n             \'NS\': self.set_NS}[treetype](proxy)\\n            try:\\n                f(self)\\n            finally:\\n                transaction.rollback()\\n                self.model = None\\n                self.sorted_model = None\\n                self.dep_model = None\\n        return _testtype\\n    return decorator\\n\\n\\ndef _load_test_methods(cls, proxy=True):\\n    if proxy and DJANGO_VERSION \\u003e= (1, 1):\\n        proxyopts = (False, True)\\n    else:\\n        proxyopts = (False,)\\n    for m in dir(cls):\\n        if not m.startswith(\'_multi_\'):\\n            continue\\n        for t in (\'MP\', \'AL\', \'NS\'):\\n            for p in proxyopts:\\n                deco = testtype(t, p)\\n                name = \'test_%s%s_%s\' % (t.lower(),\\n                                          \'_proxy\' if p else \'\',\\n                                          m.split(\'_\', 2)[2])\\n                setattr(cls, name, deco(getattr(cls, m)))\\n\\n\\nclass TestTreeBase(TestCase):\\n\\n    def setUp(self):\\n        self.set_MP()\\n        self.unchanged = [(u\'1\', 1, 0),\\n                          (u\'2\', 1, 4),\\n                          (u\'21\', 2, 0),\\n                          (u\'22\', 2, 0),\\n                          (u\'23\', 2, 1),\\n                          (u\'231\', 3, 0),\\n                          (u\'24\', 2, 0),\\n                          (u\'3\', 1, 0),\\n                          (u\'4\', 1, 1),\\n                          (u\'41\', 2, 0)]\\n\\n    def set_MP(self, proxy=False):\\n        if proxy and DJANGO_VERSION \\u003e= (1, 1):\\n            self.model = MP_TestNode_Proxy\\n        else:\\n            self.model = MP_TestNode\\n        self.sorted_model = MP_TestNodeSorted\\n        self.dep_model = MP_TestNodeSomeDep\\n\\n    def set_NS(self, proxy=False):\\n        if proxy and DJANGO_VERSION \\u003e= (1, 1):\\n            self.model = NS_TestNode_Proxy\\n        else:\\n            self.model = NS_TestNode\\n        self.sorted_model = NS_TestNodeSorted\\n        self.dep_model = NS_TestNodeSomeDep\\n\\n    def set_AL(self, proxy=False):\\n        if proxy and DJANGO_VERSION \\u003e= (1, 1):\\n            self.model = AL_TestNode_Proxy\\n        else:\\n            self.model = AL_TestNode\\n        self.sorted_model = AL_TestNodeSorted\\n        self.dep_model = AL_TestNodeSomeDep\\n\\n    def got(self):\\n        nsmodels = [NS_TestNode]\\n        if DJANGO_VERSION \\u003e= (1, 1):\\n            nsmodels.append(NS_TestNode_Proxy)\\n        if self.model in nsmodels:\\n            # this slows down nested sets tests quite a bit, but it has the\\n            # advantage that we\'ll check the node edges are correct\\n            d = {}\\n            for tree_id, lft, rgt in self.model.objects.values_list(\'tree_id\',\\n                                                                    \'lft\',\\n                                                                    \'rgt\'):\\n                d.setdefault(tree_id, []).extend([lft, rgt])\\n            for tree_id, got_edges in d.items():\\n                self.assertEqual(len(got_edges), max(got_edges))\\n                good_edges = range(1, len(got_edges) + 1)\\n                self.assertEqual(sorted(got_edges), good_edges)\\n\\n        return [(o.desc, o.get_depth(), o.get_children_count())\\n                for o in self.model.get_tree()]\\n\\n    def _assert_get_annotated_list(self, expected, parent=None):\\n        got = [\\n            (obj[0].desc, obj[1][\'open\'], obj[1][\'close\'], obj[1][\'level\'])\\n            for obj in self.model.get_annotated_list(parent)]\\n        self.assertEqual(expected, got)\\n\\n\\nclass TestEmptyTree(TestTreeBase):\\n\\n    def _multi_load_bulk_empty(self):\\n        ids = self.model.load_bulk(BASE_DATA)\\n        got_descs = [obj.desc\\n                     for obj in self.model.objects.filter(id__in=ids)]\\n        expected_descs = [x[0] for x in self.unchanged]\\n        self.assertEqual(sorted(got_descs), sorted(expected_descs))\\n        self.assertEqual(self.got(), self.unchanged)\\n\\n    def _multi_dump_bulk_empty(self):\\n        self.assertEqual(self.model.dump_bulk(), [])\\n\\n    def _multi_add_root_empty(self):\\n        self.model.add_root(desc=\'1\')\\n        expected = [(u\'1\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_get_root_nodes_empty(self):\\n        got = self.model.get_root_nodes()\\n        expected = []\\n        self.assertEqual([node.desc for node in got], expected)\\n\\n    def _multi_get_first_root_node_empty(self):\\n        got = self.model.get_first_root_node()\\n        self.assertEqual(got, None)\\n\\n    def _multi_get_last_root_node_empty(self):\\n        got = self.model.get_last_root_node()\\n        self.assertEqual(got, None)\\n\\n    def _multi_get_tree(self):\\n        got = list(self.model.get_tree())\\n        self.assertEqual(got, [])\\n\\n    def _multi_get_annotated_list(self):\\n        expected = []\\n        self._assert_get_annotated_list(expected)\\n\\n\\nclass TestNonEmptyTree(TestTreeBase):\\n\\n    def setUp(self):\\n        super(TestNonEmptyTree, self).setUp()\\n        MP_TestNode.load_bulk(BASE_DATA)\\n        AL_TestNode.load_bulk(BASE_DATA)\\n        NS_TestNode.load_bulk(BASE_DATA)\\n\\n\\nclass TestClassMethods(TestNonEmptyTree):\\n\\n    def setUp(self):\\n        super(TestClassMethods, self).setUp()\\n\\n    def _multi_load_bulk_existing(self):\\n\\n        # inserting on an existing node\\n\\n        node = self.model.objects.get(desc=u\'231\')\\n        ids = self.model.load_bulk(BASE_DATA, node)\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 4),\\n                    (u\'1\', 4, 0),\\n                    (u\'2\', 4, 4),\\n                    (u\'21\', 5, 0),\\n                    (u\'22\', 5, 0),\\n                    (u\'23\', 5, 1),\\n                    (u\'231\', 6, 0),\\n                    (u\'24\', 5, 0),\\n                    (u\'3\', 4, 0),\\n                    (u\'4\', 4, 1),\\n                    (u\'41\', 5, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        expected_descs = [u\'1\', u\'2\', u\'21\', u\'22\', u\'23\', u\'231\', u\'24\',\\n                          u\'3\', u\'4\', u\'41\']\\n        got_descs = [obj.desc\\n                     for obj in self.model.objects.filter(id__in=ids)]\\n        self.assertEqual(sorted(got_descs), sorted(expected_descs))\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_get_tree_all(self):\\n        got = [(o.desc, o.get_depth(), o.get_children_count())\\n                for o in self.model.get_tree()]\\n        self.assertEqual(got, self.unchanged)\\n\\n    def _multi_dump_bulk_all(self):\\n        self.assertEqual(self.model.dump_bulk(keep_ids=False), BASE_DATA)\\n\\n    def _multi_get_tree_node(self):\\n        node = self.model.objects.get(desc=u\'231\')\\n        self.model.load_bulk(BASE_DATA, node)\\n\\n        # the tree was modified by load_bulk, so we reload our node object\\n        node = self.model.objects.get(pk=node.id)\\n\\n        got = [(o.desc, o.get_depth(), o.get_children_count())\\n                for o in self.model.get_tree(node)]\\n        expected = [(u\'231\', 3, 4),\\n                    (u\'1\', 4, 0),\\n                    (u\'2\', 4, 4),\\n                    (u\'21\', 5, 0),\\n                    (u\'22\', 5, 0),\\n                    (u\'23\', 5, 1),\\n                    (u\'231\', 6, 0),\\n                    (u\'24\', 5, 0),\\n                    (u\'3\', 4, 0),\\n                    (u\'4\', 4, 1),\\n                    (u\'41\', 5, 0)]\\n        self.assertEqual(got, expected)\\n\\n    def _multi_get_tree_leaf(self):\\n        node = self.model.objects.get(desc=u\'1\')\\n\\n        self.assertEqual(0, node.get_children_count())\\n        got = [(o.desc, o.get_depth(), o.get_children_count())\\n                for o in self.model.get_tree(node)]\\n        expected = [(u\'1\', 1, 0)]\\n        self.assertEqual(got, expected)\\n\\n    def _multi_get_annotated_list_all(self):\\n        expected = [(u\'1\', True, [], 0), (u\'2\', False, [], 0),\\n                    (u\'21\', True, [], 1), (u\'22\', False, [], 1),\\n                    (u\'23\', False, [], 1), (u\'231\', True, [0], 2),\\n                    (u\'24\', False, [0], 1), (u\'3\', False, [], 0),\\n                    (u\'4\', False, [], 0), (u\'41\', True, [0, 1], 1)]\\n        self._assert_get_annotated_list(expected)\\n\\n    def _multi_get_annotated_list_node(self):\\n        node = self.model.objects.get(desc=u\'2\')\\n        expected = [(u\'2\', True, [], 0), (u\'21\', True, [], 1),\\n                    (u\'22\', False, [], 1), (u\'23\', False, [], 1),\\n                    (u\'231\', True, [0], 2), (u\'24\', False, [0, 1], 1)]\\n        self._assert_get_annotated_list(expected, node)\\n\\n    def _multi_get_annotated_list_leaf(self):\\n        node = self.model.objects.get(desc=u\'1\')\\n        expected = [(u\'1\', True, [0], 0)]\\n        self._assert_get_annotated_list(expected, node)\\n\\n    def _multi_dump_bulk_node(self):\\n        node = self.model.objects.get(desc=u\'231\')\\n        self.model.load_bulk(BASE_DATA, node)\\n\\n        # the tree was modified by load_bulk, so we reload our node object\\n        node = self.model.objects.get(pk=node.id)\\n\\n        got = self.model.dump_bulk(node, False)\\n        expected = [{\'data\':{\'desc\':u\'231\'}, \'children\':BASE_DATA}]\\n        self.assertEqual(got, expected)\\n\\n    def _multi_load_and_dump_bulk_keeping_ids(self):\\n        exp = self.model.dump_bulk(keep_ids=True)\\n        self.model.objects.all().delete()\\n        self.model.load_bulk(exp, None, True)\\n        got = self.model.dump_bulk(keep_ids=True)\\n        self.assertEqual(got, exp)\\n        # do we really have an unchaged tree after the dump/delete/load?\\n        got = [(o.desc, o.get_depth(), o.get_children_count())\\n                for o in self.model.get_tree()]\\n        self.assertEqual(got, self.unchanged)\\n\\n    def _multi_get_root_nodes(self):\\n        got = self.model.get_root_nodes()\\n        expected = [\'1\', \'2\', \'3\', \'4\']\\n        self.assertEqual([node.desc for node in got], expected)\\n\\n    def _multi_get_first_root_node(self):\\n        got = self.model.get_first_root_node()\\n        self.assertEqual(got.desc, \'1\')\\n\\n    def _multi_get_last_root_node(self):\\n        got = self.model.get_last_root_node()\\n        self.assertEqual(got.desc, \'4\')\\n\\n    def _multi_add_root(self):\\n        obj = self.model.add_root(desc=\'5\')\\n        self.assertEqual(obj.get_depth(), 1)\\n        self.assertEqual(self.model.get_last_root_node().desc, \'5\')\\n\\n\\nclass TestSimpleNodeMethods(TestNonEmptyTree):\\n\\n    def _multi_is_root(self):\\n        data = [\\n            (\'2\', True),\\n            (\'1\', True),\\n            (\'4\', True),\\n            (\'21\', False),\\n            (\'24\', False),\\n            (\'22\', False),\\n            (\'231\', False),\\n        ]\\n        for desc, expected in data:\\n            got = self.model.objects.get(desc=desc).is_root()\\n            self.assertEqual(got, expected)\\n\\n    def _multi_is_leaf(self):\\n        data = [\\n            (\'2\', False),\\n            (\'23\', False),\\n            (\'231\', True),\\n        ]\\n        for desc, expected in data:\\n            got = self.model.objects.get(desc=desc).is_leaf()\\n            self.assertEqual(got, expected)\\n\\n    def _multi_get_root(self):\\n        data = [\\n            (\'2\', \'2\'),\\n            (\'1\', \'1\'),\\n            (\'4\', \'4\'),\\n            (\'21\', \'2\'),\\n            (\'24\', \'2\'),\\n            (\'22\', \'2\'),\\n            (\'231\', \'2\'),\\n        ]\\n        for desc, expected in data:\\n            node = self.model.objects.get(desc=desc).get_root()\\n            self.assertEqual(node.desc, expected)\\n\\n    def _multi_get_parent(self):\\n        data = [\\n            (\'2\', None),\\n            (\'1\', None),\\n            (\'4\', None),\\n            (\'21\', \'2\'),\\n            (\'24\', \'2\'),\\n            (\'22\', \'2\'),\\n            (\'231\', \'23\'),\\n        ]\\n        data = dict(data)\\n        objs = {}\\n        for desc, expected in data.items():\\n            node = self.model.objects.get(desc=desc)\\n            parent = node.get_parent()\\n            if expected:\\n                self.assertEqual(parent.desc, expected)\\n            else:\\n                self.assertEqual(parent, None)\\n            objs[desc] = node\\n            # corrupt the objects\' parent cache\\n            node._parent_obj = \'CORRUPTED!!!\'\\n\\n        for desc, expected in data.items():\\n            node = objs[desc]\\n            # asking get_parent to not use the parent cache (since we\\n            # corrupted it in the previous loop)\\n            parent = node.get_parent(True)\\n            if expected:\\n                self.assertEqual(parent.desc, expected)\\n            else:\\n                self.assertEqual(parent, None)\\n\\n    def _multi_get_children(self):\\n        data = [\\n            (\'2\', [\'21\', \'22\', \'23\', \'24\']),\\n            (\'23\', [\'231\']),\\n            (\'231\', []),\\n        ]\\n        for desc, expected in data:\\n            children = self.model.objects.get(desc=desc).get_children()\\n            self.assertEqual([node.desc for node in children], expected)\\n\\n    def _multi_get_children_count(self):\\n        data = [\\n            (\'2\', 4),\\n            (\'23\', 1),\\n            (\'231\', 0),\\n        ]\\n        for desc, expected in data:\\n            got = self.model.objects.get(desc=desc).get_children_count()\\n            self.assertEqual(got, expected)\\n\\n    def _multi_get_siblings(self):\\n        data = [\\n            (\'2\', [\'1\', \'2\', \'3\', \'4\']),\\n            (\'21\', [\'21\', \'22\', \'23\', \'24\']),\\n            (\'231\', [\'231\']),\\n        ]\\n        for desc, expected in data:\\n            siblings = self.model.objects.get(desc=desc).get_siblings()\\n            self.assertEqual([node.desc for node in siblings], expected)\\n\\n    def _multi_get_first_sibling(self):\\n        data = [\\n            (\'2\', \'1\'),\\n            (\'1\', \'1\'),\\n            (\'4\', \'1\'),\\n            (\'21\', \'21\'),\\n            (\'24\', \'21\'),\\n            (\'22\', \'21\'),\\n            (\'231\', \'231\'),\\n        ]\\n        for desc, expected in data:\\n            node = self.model.objects.get(desc=desc).get_first_sibling()\\n            self.assertEqual(node.desc, expected)\\n\\n    def _multi_get_prev_sibling(self):\\n        data = [\\n            (\'2\', \'1\'),\\n            (\'1\', None),\\n            (\'4\', \'3\'),\\n            (\'21\', None),\\n            (\'24\', \'23\'),\\n            (\'22\', \'21\'),\\n            (\'231\', None),\\n        ]\\n        for desc, expected in data:\\n            node = self.model.objects.get(desc=desc).get_prev_sibling()\\n            if expected is None:\\n                self.assertEqual(node, None)\\n            else:\\n                self.assertEqual(node.desc, expected)\\n\\n    def _multi_get_next_sibling(self):\\n        data = [\\n            (\'2\', \'3\'),\\n            (\'1\', \'2\'),\\n            (\'4\', None),\\n            (\'21\', \'22\'),\\n            (\'24\', None),\\n            (\'22\', \'23\'),\\n            (\'231\', None),\\n        ]\\n        for desc, expected in data:\\n            node = self.model.objects.get(desc=desc).get_next_sibling()\\n            if expected is None:\\n                self.assertEqual(node, None)\\n            else:\\n                self.assertEqual(node.desc, expected)\\n\\n    def _multi_get_last_sibling(self):\\n        data = [\\n            (\'2\', \'4\'),\\n            (\'1\', \'4\'),\\n            (\'4\', \'4\'),\\n            (\'21\', \'24\'),\\n            (\'24\', \'24\'),\\n            (\'22\', \'24\'),\\n            (\'231\', \'231\'),\\n        ]\\n        for desc, expected in data:\\n            node = self.model.objects.get(desc=desc).get_last_sibling()\\n            self.assertEqual(node.desc, expected)\\n\\n    def _multi_get_first_child(self):\\n        data = [\\n            (\'2\', \'21\'),\\n            (\'21\', None),\\n            (\'23\', \'231\'),\\n            (\'231\', None),\\n        ]\\n        for desc, expected in data:\\n            node = self.model.objects.get(desc=desc).get_first_child()\\n            if expected is None:\\n                self.assertEqual(node, None)\\n            else:\\n                self.assertEqual(node.desc, expected)\\n\\n    def _multi_get_last_child(self):\\n        data = [\\n            (\'2\', \'24\'),\\n            (\'21\', None),\\n            (\'23\', \'231\'),\\n            (\'231\', None),\\n        ]\\n        for desc, expected in data:\\n            node = self.model.objects.get(desc=desc).get_last_child()\\n            if expected is None:\\n                self.assertEqual(node, None)\\n            else:\\n                self.assertEqual(node.desc, expected)\\n\\n    def _multi_get_ancestors(self):\\n        data = [\\n            (\'2\', []),\\n            (\'21\', [\'2\']),\\n            (\'231\', [\'2\', \'23\']),\\n        ]\\n        for desc, expected in data:\\n            nodes = self.model.objects.get(desc=desc).get_ancestors()\\n            self.assertEqual([node.desc for node in nodes], expected)\\n\\n    def _multi_get_descendants(self):\\n        data = [\\n            (\'2\', [\'21\', \'22\', \'23\', \'231\', \'24\']),\\n            (\'23\', [\'231\']),\\n            (\'231\', []),\\n            (\'1\', []),\\n            (\'4\', [\'41\']),\\n        ]\\n        for desc, expected in data:\\n            nodes = self.model.objects.get(desc=desc).get_descendants()\\n            self.assertEqual([node.desc for node in nodes], expected)\\n\\n    def _multi_get_descendant_count(self):\\n        data = [\\n            (\'2\', 5),\\n            (\'23\', 1),\\n            (\'231\', 0),\\n            (\'1\', 0),\\n            (\'4\', 1),\\n        ]\\n        for desc, expected in data:\\n            got = self.model.objects.get(desc=desc).get_descendant_count()\\n            self.assertEqual(got, expected)\\n\\n    def _multi_is_sibling_of(self):\\n        data = [\\n            (\'2\', \'2\', True),\\n            (\'2\', \'1\', True),\\n            (\'21\', \'2\', False),\\n            (\'231\', \'2\', False),\\n            (\'22\', \'23\', True),\\n            (\'231\', \'23\', False),\\n            (\'231\', \'231\', True),\\n        ]\\n        for desc1, desc2, expected in data:\\n            node1 = self.model.objects.get(desc=desc1)\\n            node2 = self.model.objects.get(desc=desc2)\\n            self.assertEqual(node1.is_sibling_of(node2), expected)\\n\\n    def _multi_is_child_of(self):\\n        data = [\\n            (\'2\', \'2\', False),\\n            (\'2\', \'1\', False),\\n            (\'21\', \'2\', True),\\n            (\'231\', \'2\', False),\\n            (\'231\', \'23\', True),\\n            (\'231\', \'231\', False),\\n        ]\\n        for desc1, desc2, expected in data:\\n            node1 = self.model.objects.get(desc=desc1)\\n            node2 = self.model.objects.get(desc=desc2)\\n            self.assertEqual(node1.is_child_of(node2), expected)\\n\\n    def _multi_is_descendant_of(self):\\n        data = [\\n            (\'2\', \'2\', False),\\n            (\'2\', \'1\', False),\\n            (\'21\', \'2\', True),\\n            (\'231\', \'2\', True),\\n            (\'231\', \'23\', True),\\n            (\'231\', \'231\', False),\\n        ]\\n        for desc1, desc2, expected in data:\\n            node1 = self.model.objects.get(desc=desc1)\\n            node2 = self.model.objects.get(desc=desc2)\\n            self.assertEqual(node1.is_descendant_of(node2), expected)\\n\\n\\nclass TestAddChild(TestNonEmptyTree):\\n\\n    def _multi_add_child_to_leaf(self):\\n        self.model.objects.get(desc=u\'231\').add_child(desc=\'2311\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 1),\\n                    (u\'2311\', 4, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_child_to_node(self):\\n        self.model.objects.get(desc=u\'2\').add_child(desc=\'25\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'25\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n\\nclass TestAddSibling(TestNonEmptyTree):\\n\\n    def _multi_add_sibling_invalid_pos(self):\\n        method = self.model.objects.get(desc=u\'231\').add_sibling\\n        self.assertRaises(InvalidPosition, method, \'invalid_pos\')\\n\\n    def _multi_add_sibling_missing_nodeorderby(self):\\n        node_wchildren = self.model.objects.get(desc=u\'2\')\\n        method = node_wchildren.add_sibling\\n        self.assertRaises(MissingNodeOrderBy, method, \'sorted-sibling\',\\n                          desc=\'aaa\')\\n\\n    def _multi_add_sibling_last_root(self):\\n        node_wchildren = self.model.objects.get(desc=u\'2\')\\n        obj = node_wchildren.add_sibling(\'last-sibling\', desc=\'5\')\\n        self.assertEqual(obj.get_depth(), 1)\\n        self.assertEqual(node_wchildren.get_last_sibling().desc, u\'5\')\\n\\n    def _multi_add_sibling_last(self):\\n        node = self.model.objects.get(desc=u\'231\')\\n        obj = node.add_sibling(\'last-sibling\', desc=\'232\')\\n        self.assertEqual(obj.get_depth(), 3)\\n        self.assertEqual(node.get_last_sibling().desc, u\'232\')\\n\\n    def _multi_add_sibling_first_root(self):\\n        node_wchildren = self.model.objects.get(desc=u\'2\')\\n        obj = node_wchildren.add_sibling(\'first-sibling\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 1)\\n        expected = [(u\'new\', 1, 0),\\n                    (u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_sibling_first(self):\\n        node_wchildren = self.model.objects.get(desc=u\'23\')\\n        obj = node_wchildren.add_sibling(\'first-sibling\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 2)\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'new\', 2, 0),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_sibling_left_root(self):\\n        node_wchildren = self.model.objects.get(desc=u\'2\')\\n        obj = node_wchildren.add_sibling(\'left\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 1)\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'new\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_sibling_left(self):\\n        node_wchildren = self.model.objects.get(desc=u\'23\')\\n        obj = node_wchildren.add_sibling(\'left\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 2)\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'new\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_sibling_left_noleft_root(self):\\n        node = self.model.objects.get(desc=u\'1\')\\n        obj = node.add_sibling(\'left\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 1)\\n        expected = [(u\'new\', 1, 0),\\n                    (u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_sibling_left_noleft(self):\\n        node = self.model.objects.get(desc=u\'231\')\\n        obj = node.add_sibling(\'left\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 3)\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 2),\\n                    (u\'new\', 3, 0),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_sibling_right_root(self):\\n        node_wchildren = self.model.objects.get(desc=u\'2\')\\n        obj = node_wchildren.add_sibling(\'right\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 1)\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'new\', 1, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_sibling_right(self):\\n        node_wchildren = self.model.objects.get(desc=u\'23\')\\n        obj = node_wchildren.add_sibling(\'right\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 2)\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'new\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_sibling_right_noright_root(self):\\n        node = self.model.objects.get(desc=u\'4\')\\n        obj = node.add_sibling(\'right\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 1)\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0),\\n                    (u\'new\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_sibling_right_noright(self):\\n        node = self.model.objects.get(desc=u\'231\')\\n        obj = node.add_sibling(\'right\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 3)\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 2),\\n                    (u\'231\', 3, 0),\\n                    (u\'new\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n\\nclass TestDelete(TestNonEmptyTree):\\n\\n    def setUp(self):\\n        super(TestDelete, self).setUp()\\n        for node in self.model.objects.all():\\n            self.dep_model(node=node).save()\\n\\n    def _multi_delete_leaf(self):\\n        self.model.objects.get(desc=u\'231\').delete()\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_delete_node(self):\\n        self.model.objects.get(desc=u\'23\').delete()\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 3),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_delete_root(self):\\n        self.model.objects.get(desc=u\'2\').delete()\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_delete_filter_root_nodes(self):\\n        self.model.objects.filter(desc__in=(\'2\', \'3\')).delete()\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_delete_filter_children(self):\\n        self.model.objects.filter(\\n            desc__in=(\'2\', \'23\', \'231\')).delete()\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_delete_nonexistant_nodes(self):\\n        self.model.objects.filter(desc__in=(\'ZZZ\', \'XXX\')).delete()\\n        self.assertEqual(self.got(), self.unchanged)\\n\\n    def _multi_delete_same_node_twice(self):\\n        self.model.objects.filter(\\n            desc__in=(\'2\', \'2\')).delete()\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_delete_all_root_nodes(self):\\n        self.model.get_root_nodes().delete()\\n        count = self.model.objects.count()\\n        self.assertEqual(count, 0)\\n\\n    def _multi_delete_all_nodes(self):\\n        self.model.objects.all().delete()\\n        count = self.model.objects.count()\\n        self.assertEqual(count, 0)\\n\\n\\nclass TestMoveErrors(TestNonEmptyTree):\\n\\n    def _multi_move_invalid_pos(self):\\n        node = self.model.objects.get(desc=u\'231\')\\n        self.assertRaises(InvalidPosition, node.move, node, \'invalid_pos\')\\n\\n    def _multi_move_to_descendant(self):\\n        node = self.model.objects.get(desc=u\'2\')\\n        target = self.model.objects.get(desc=u\'231\')\\n        self.assertRaises(InvalidMoveToDescendant, node.move, target,\\n            \'first-sibling\')\\n\\n    def _multi_move_missing_nodeorderby(self):\\n        node = self.model.objects.get(desc=u\'231\')\\n        self.assertRaises(MissingNodeOrderBy, node.move, node,\\n                          \'sorted-child\')\\n        self.assertRaises(MissingNodeOrderBy, node.move, node,\\n                          \'sorted-sibling\')\\n\\n\\nclass TestMoveSortedErrors(TestNonEmptyTree):\\n\\n    def _multi_nonsorted_move_in_sorted(self):\\n        node = self.sorted_model.add_root(val1=3, val2=3, desc=\'zxy\')\\n        self.assertRaises(InvalidPosition, node.move, node, \'left\')\\n\\n\\nclass TestMoveLeafRoot(TestNonEmptyTree):\\n\\n    def _multi_move_leaf_last_sibling_root(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'2\'), \'last-sibling\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0),\\n                    (u\'231\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_first_sibling_root(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'2\'), \'first-sibling\')\\n        expected = [(u\'231\', 1, 0),\\n                    (u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_left_sibling_root(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'2\'), \'left\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'231\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_right_sibling_root(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'2\'), \'right\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'231\', 1, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_last_child_root(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'2\'), \'last-child\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'231\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_first_child_root(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'2\'), \'first-child\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'231\', 2, 0),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n\\nclass TestMoveLeaf(TestNonEmptyTree):\\n\\n    def _multi_move_leaf_last_sibling(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'22\'), \'last-sibling\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'231\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_first_sibling(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'22\'), \'first-sibling\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'231\', 2, 0),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_left_sibling(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'22\'), \'left\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'231\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_right_sibling(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'22\'), \'right\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'231\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_left_sibling_itself(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'231\'), \'left\')\\n        self.assertEqual(self.got(), self.unchanged)\\n\\n    def _multi_move_leaf_last_child(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'22\'), \'last-child\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_first_child(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'22\'), \'first-child\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n\\nclass TestMoveBranchRoot(TestNonEmptyTree):\\n\\n    def _multi_move_branch_first_sibling_root(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'2\'), \'first-sibling\')\\n        expected = [(u\'4\', 1, 1),\\n                    (u\'41\', 2, 0),\\n                    (u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_last_sibling_root(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'2\'), \'last-sibling\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_left_sibling_root(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'2\'), \'left\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_right_sibling_root(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'2\'), \'right\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_left_noleft_sibling_root(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'2\').get_first_sibling(), \'left\')\\n        expected = [(u\'4\', 1, 1),\\n                    (u\'41\', 2, 0),\\n                    (u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_right_noright_sibling_root(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'2\').get_last_sibling(), \'right\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_first_child_root(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'2\'), \'first-child\')\\n        expected = [(u\'1\', 1, 0),\\n                   (u\'2\', 1, 5),\\n                   (u\'4\', 2, 1),\\n                   (u\'41\', 3, 0),\\n                   (u\'21\', 2, 0),\\n                   (u\'22\', 2, 0),\\n                   (u\'23\', 2, 1),\\n                   (u\'231\', 3, 0),\\n                   (u\'24\', 2, 0),\\n                   (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_last_child_root(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'2\'), \'last-child\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'4\', 2, 1),\\n                    (u\'41\', 3, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n\\nclass TestMoveBranch(TestNonEmptyTree):\\n\\n    def _multi_move_branch_first_sibling(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'23\'), \'first-sibling\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'4\', 2, 1),\\n                    (u\'41\', 3, 0),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_last_sibling(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'23\'), \'last-sibling\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'4\', 2, 1),\\n                    (u\'41\', 3, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_left_sibling(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'23\'), \'left\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'4\', 2, 1),\\n                    (u\'41\', 3, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_right_sibling(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'23\'), \'right\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'4\', 2, 1),\\n                    (u\'41\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_left_noleft_sibling(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'23\').get_first_sibling(), \'left\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'4\', 2, 1),\\n                    (u\'41\', 3, 0),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_right_noright_sibling(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'23\').get_last_sibling(), \'right\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'4\', 2, 1),\\n                    (u\'41\', 3, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_left_itself_sibling(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'4\'), \'left\')\\n        self.assertEqual(self.got(), self.unchanged)\\n\\n    def _multi_move_branch_first_child(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'23\'), \'first-child\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 2),\\n                    (u\'4\', 3, 1),\\n                    (u\'41\', 4, 0),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_last_child(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'23\'), \'last-child\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 2),\\n                    (u\'231\', 3, 0),\\n                    (u\'4\', 3, 1),\\n                    (u\'41\', 4, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n\\nclass TestTreeSorted(TestTreeBase):\\n\\n    def got(self):\\n        return [(o.val1, o.val2, o.desc, o.get_depth(), o.get_children_count())\\n                 for o in self.sorted_model.get_tree()]\\n\\n    def _multi_add_root_sorted(self):\\n        self.sorted_model.add_root(val1=3, val2=3, desc=\'zxy\')\\n        self.sorted_model.add_root(val1=1, val2=4, desc=\'bcd\')\\n        self.sorted_model.add_root(val1=2, val2=5, desc=\'zxy\')\\n        self.sorted_model.add_root(val1=3, val2=3, desc=\'abc\')\\n        self.sorted_model.add_root(val1=4, val2=1, desc=\'fgh\')\\n        self.sorted_model.add_root(val1=3, val2=3, desc=\'abc\')\\n        self.sorted_model.add_root(val1=2, val2=2, desc=\'qwe\')\\n        self.sorted_model.add_root(val1=3, val2=2, desc=\'vcx\')\\n        expected = [(1, 4, u\'bcd\', 1, 0),\\n                    (2, 2, u\'qwe\', 1, 0),\\n                    (2, 5, u\'zxy\', 1, 0),\\n                    (3, 2, u\'vcx\', 1, 0),\\n                    (3, 3, u\'abc\', 1, 0),\\n                    (3, 3, u\'abc\', 1, 0),\\n                    (3, 3, u\'zxy\', 1, 0),\\n                    (4, 1, u\'fgh\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_child_root_sorted(self):\\n        root = self.sorted_model.add_root(val1=0, val2=0, desc=\'aaa\')\\n        root.add_child(val1=3, val2=3, desc=\'zxy\')\\n        root.add_child(val1=1, val2=4, desc=\'bcd\')\\n        root.add_child(val1=2, val2=5, desc=\'zxy\')\\n        root.add_child(val1=3, val2=3, desc=\'abc\')\\n        root.add_child(val1=4, val2=1, desc=\'fgh\')\\n        root.add_child(val1=3, val2=3, desc=\'abc\')\\n        root.add_child(val1=2, val2=2, desc=\'qwe\')\\n        root.add_child(val1=3, val2=2, desc=\'vcx\')\\n        expected = [(0, 0, u\'aaa\', 1, 8),\\n                    (1, 4, u\'bcd\', 2, 0),\\n                    (2, 2, u\'qwe\', 2, 0),\\n                    (2, 5, u\'zxy\', 2, 0),\\n                    (3, 2, u\'vcx\', 2, 0),\\n                    (3, 3, u\'abc\', 2, 0),\\n                    (3, 3, u\'abc\', 2, 0),\\n                    (3, 3, u\'zxy\', 2, 0),\\n                    (4, 1, u\'fgh\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_child_nonroot_sorted(self):\\n\\n        get_node = lambda node_id: self.sorted_model.objects.get(pk=node_id)\\n\\n        root_id = self.sorted_model.add_root(val1=0, val2=0, desc=\'a\').id\\n        node_id = get_node(root_id).add_child(val1=0, val2=0, desc=\'ac\').id\\n        get_node(root_id).add_child(val1=0, val2=0, desc=\'aa\')\\n        get_node(root_id).add_child(val1=0, val2=0, desc=\'av\')\\n        get_node(node_id).add_child(val1=0, val2=0, desc=\'aca\')\\n        get_node(node_id).add_child(val1=0, val2=0, desc=\'acc\')\\n        get_node(node_id).add_child(val1=0, val2=0, desc=\'acb\')\\n\\n        expected = [(0, 0, u\'a\', 1, 3),\\n                    (0, 0, u\'aa\', 2, 0),\\n                    (0, 0, u\'ac\', 2, 3),\\n                    (0, 0, u\'aca\', 3, 0),\\n                    (0, 0, u\'acb\', 3, 0),\\n                    (0, 0, u\'acc\', 3, 0),\\n                    (0, 0, u\'av\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_sorted(self):\\n        self.sorted_model.add_root(val1=3, val2=3, desc=\'zxy\')\\n        self.sorted_model.add_root(val1=1, val2=4, desc=\'bcd\')\\n        self.sorted_model.add_root(val1=2, val2=5, desc=\'zxy\')\\n        self.sorted_model.add_root(val1=3, val2=3, desc=\'abc\')\\n        self.sorted_model.add_root(val1=4, val2=1, desc=\'fgh\')\\n        self.sorted_model.add_root(val1=3, val2=3, desc=\'abc\')\\n        self.sorted_model.add_root(val1=2, val2=2, desc=\'qwe\')\\n        self.sorted_model.add_root(val1=3, val2=2, desc=\'vcx\')\\n        root_nodes = self.sorted_model.get_root_nodes()\\n        target = root_nodes[0]\\n        for node in root_nodes[1:]:\\n\\n            # because raw queries don\'t update django objects\\n            node = self.sorted_model.objects.get(pk=node.id)\\n            target = self.sorted_model.objects.get(pk=target.id)\\n\\n            node.move(target, \'sorted-child\')\\n        expected = [(1, 4, u\'bcd\', 1, 7),\\n                    (2, 2, u\'qwe\', 2, 0),\\n                    (2, 5, u\'zxy\', 2, 0),\\n                    (3, 2, u\'vcx\', 2, 0),\\n                    (3, 3, u\'abc\', 2, 0),\\n                    (3, 3, u\'abc\', 2, 0),\\n                    (3, 3, u\'zxy\', 2, 0),\\n                    (4, 1, u\'fgh\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n\\nclass TestMP_TreeAlphabet(TestCase):\\n\\n    def test_alphabet(self):\\n        if not os.getenv(\'TREEBEARD_TEST_ALPHABET\', False):\\n            # run this test only if the enviroment variable is set\\n            return\\n        basealpha = numconv.BASE85\\n        got_err = False\\n        last_good = None\\n        for alphabetlen in range(35, len(basealpha) + 1):\\n            alphabet = basealpha[0:alphabetlen]\\n            expected = [alphabet[0] + char for char in alphabet[1:]]\\n            expected.extend([alphabet[1] + char for char in alphabet])\\n            expected.append(alphabet[2] + alphabet[0])\\n\\n            # remove all nodes\\n            MP_TestNodeAlphabet.objects.all().delete()\\n\\n            # change the model\'s alphabet\\n            MP_TestNodeAlphabet.alphabet = alphabet\\n\\n            # insert root nodes\\n            for pos in range(len(alphabet) * 2):\\n                try:\\n                    MP_TestNodeAlphabet.add_root(numval=pos)\\n                except:\\n                    got_err = True\\n                    break\\n            if got_err:\\n                break\\n            got = [obj.path for obj in MP_TestNodeAlphabet.objects.all()]\\n            if got != expected:\\n                got_err = True\\n            last_good = alphabet\\n        print \'\\\\nThe best BASE85 based alphabet for your setup is: %s\' \\\\\\n            % (last_good, )\\n\\n\\nclass TestHelpers(TestTreeBase):\\n\\n    def setUp(self):\\n        for model in (MP_TestNode, AL_TestNode, NS_TestNode):\\n            model.load_bulk(BASE_DATA)\\n            for node in model.get_root_nodes():\\n                model.load_bulk(BASE_DATA, node)\\n            model.add_root(desc=\'5\')\\n\\n    def _multi_descendants_group_count_root(self):\\n        expected = [(o.desc, o.get_descendant_count())\\n                    for o in self.model.get_root_nodes()]\\n        got = [(o.desc, o.descendants_count)\\n               for o in self.model.get_descendants_group_count()]\\n        self.assertEqual(got, expected)\\n\\n    def _multi_descendants_group_count_node(self):\\n        parent = self.model.get_root_nodes().get(desc=\'2\')\\n        expected = [(o.desc, o.get_descendant_count())\\n                    for o in parent.get_children()]\\n        got = [(o.desc, o.descendants_count)\\n               for o in self.model.get_descendants_group_count(parent)]\\n        self.assertEqual(got, expected)\\n\\n\\nclass TestMP_TreeSortedAutoNow(TestCase):\\n    \\"\\"\\"\\n    The sorting mechanism used by treebeard when adding a node can fail if the\\n    ordering is using an \\"auto_now\\" field\\n    \\"\\"\\"\\n\\n    def test_sorted_by_autonow_workaround(self):\\n        \\"\\"\\"\\n        workaround\\n        \\"\\"\\"\\n        import datetime\\n        for i in range(1, 5):\\n            MP_TestNodeSortedAutoNow.add_root(desc=\'node%d\' % (i, ),\\n                                             created=datetime.datetime.now())\\n\\n    def test_sorted_by_autonow_FAIL(self):\\n        \\"\\"\\"\\n        This test asserts that we have a problem.\\n        fix this, somehow\\n        \\"\\"\\"\\n        MP_TestNodeSortedAutoNow.add_root(desc=\'node1\')\\n        self.assertRaises(ValueError, MP_TestNodeSortedAutoNow.add_root,\\n                          desc=\'node2\')\\n\\n\\nclass TestMP_TreeStepOverflow(TestCase):\\n\\n    def test_add_root(self):\\n        method = MP_TestNodeSmallStep.add_root\\n        for i in range(1, 10):\\n            method()\\n        self.assertRaises(PathOverflow, method)\\n\\n    def test_add_child(self):\\n        root = MP_TestNodeSmallStep.add_root()\\n        method = root.add_child\\n        for i in range(1, 10):\\n            method()\\n        self.assertRaises(PathOverflow, method)\\n\\n    def test_add_sibling(self):\\n        root = MP_TestNodeSmallStep.add_root()\\n        for i in range(1, 10):\\n            root.add_child()\\n        method = root.get_last_child().add_sibling\\n        positions = (\'first-sibling\', \'left\', \'right\', \'last-sibling\')\\n        for pos in positions:\\n            self.assertRaises(PathOverflow, method, pos)\\n\\n    def test_move(self):\\n        root = MP_TestNodeSmallStep.add_root()\\n        for i in range(1, 10):\\n            root.add_child()\\n        newroot = MP_TestNodeSmallStep.add_root()\\n        targets = [(root, [\'first-child\', \'last-child\']),\\n                   (root.get_first_child(), [\'first-sibling\',\\n                                            \'left\',\\n                                            \'right\',\\n                                            \'last-sibling\'])]\\n        for target, positions in targets:\\n            for pos in positions:\\n                self.assertRaises(PathOverflow, newroot.move, target, pos)\\n\\n\\nclass TestMP_TreeShortPath(TestCase):\\n    \\"\\"\\"\\n    Here we test a tree with a very small path field (max_length=4) and a\\n    steplen of 1\\n    \\"\\"\\"\\n\\n    def test_short_path(self):\\n        obj = MP_TestNodeShortPath.add_root()\\n        obj = obj.add_child().add_child().add_child()\\n        self.assertRaises(PathOverflow, obj.add_child)\\n\\n\\nclass TestMP_TreeFindProblems(TestTreeBase):\\n\\n    def test_find_problems(self):\\n        model = MP_TestNodeAlphabet\\n        model.alphabet = \'01234\'\\n        model(path=\'01\', depth=1, numchild=0, numval=0).save()\\n        model(path=\'1\', depth=1, numchild=0, numval=0).save()\\n        model(path=\'111\', depth=1, numchild=0, numval=0).save()\\n        model(path=\'abcd\', depth=1, numchild=0, numval=0).save()\\n        model(path=\'qa#$%!\', depth=1, numchild=0, numval=0).save()\\n        model(path=\'0201\', depth=2, numchild=0, numval=0).save()\\n        model(path=\'020201\', depth=3, numchild=0, numval=0).save()\\n        model(path=\'03\', depth=1, numchild=2, numval=0).save()\\n        model(path=\'0301\', depth=2, numchild=0, numval=0).save()\\n        model(path=\'030102\', depth=3, numchild=10, numval=0).save()\\n        model(path=\'04\', depth=10, numchild=1, numval=0).save()\\n        model(path=\'0401\', depth=20, numchild=0, numval=0).save()\\n\\n        evil_chars, bad_steplen, orphans, wrong_depth, wrong_numchild = \\\\\\n                                                        model.find_problems()\\n        self.assertEqual([\'abcd\', \'qa#$%!\'],\\n            [o.path for o in model.objects.filter(id__in=evil_chars)])\\n        self.assertEqual([\'1\', \'111\'],\\n            [o.path for o in model.objects.filter(id__in=bad_steplen)])\\n        self.assertEqual([\'0201\', \'020201\'],\\n            [o.path for o in model.objects.filter(id__in=orphans)])\\n        self.assertEqual([\'03\', \'0301\', \'030102\'],\\n            [o.path for o in model.objects.filter(id__in=wrong_numchild)])\\n        self.assertEqual([\'04\', \'0401\'],\\n            [o.path for o in model.objects.filter(id__in=wrong_depth)])\\n\\n\\nclass TestMP_TreeFix(TestTreeBase):\\n\\n    def setUp(self):\\n        super(TestMP_TreeFix, self).setUp()\\n        self.expected_no_holes = {\\n            MP_TestNodeShortPath: [\\n                (u\'1\', u\'b\', 1, 2),\\n                (u\'11\', u\'u\', 2, 1),\\n                (u\'111\', u\'i\', 3, 1),\\n                (u\'1111\', u\'e\', 4, 0),\\n                (u\'12\', u\'o\', 2, 0),\\n                (u\'2\', u\'d\', 1, 0),\\n                (u\'3\', u\'g\', 1, 0),\\n                (u\'4\', u\'a\', 1, 4),\\n                (u\'41\', u\'a\', 2, 0),\\n                (u\'42\', u\'a\', 2, 0),\\n                (u\'43\', u\'u\', 2, 1),\\n                (u\'431\', u\'i\', 3, 1),\\n                (u\'4311\', u\'e\', 4, 0),\\n                (u\'44\', u\'o\', 2, 0)],\\n            MP_TestSortedNodeShortPath: [\\n                (u\'1\', u\'a\', 1, 4),\\n                (u\'11\', u\'a\', 2, 0),\\n                (u\'12\', u\'a\', 2, 0),\\n                (u\'13\', u\'o\', 2, 0),\\n                (u\'14\', u\'u\', 2, 1),\\n                (u\'141\', u\'i\', 3, 1),\\n                (u\'1411\', u\'e\', 4, 0),\\n                (u\'2\', u\'b\', 1, 2),\\n                (u\'21\', u\'o\', 2, 0),\\n                (u\'22\', u\'u\', 2, 1),\\n                (u\'221\', u\'i\', 3, 1),\\n                (u\'2211\', u\'e\', 4, 0),\\n                (u\'3\', u\'d\', 1, 0),\\n                (u\'4\', u\'g\', 1, 0)]}\\n        self.expected_with_holes = {\\n            MP_TestNodeShortPath: [\\n                (u\'1\', u\'b\', 1L, 2L),\\n                (u\'13\', u\'u\', 2L, 1L),\\n                (u\'134\', u\'i\', 3L, 1L),\\n                (u\'1343\', u\'e\', 4L, 0L),\\n                (u\'14\', u\'o\', 2L, 0L),\\n                (u\'2\', u\'d\', 1L, 0L),\\n                (u\'3\', u\'g\', 1L, 0L),\\n                (u\'4\', u\'a\', 1L, 4L),\\n                (u\'41\', u\'a\', 2L, 0L),\\n                (u\'42\', u\'a\', 2L, 0L),\\n                (u\'43\', u\'u\', 2L, 1L),\\n                (u\'434\', u\'i\', 3L, 1L),\\n                (u\'4343\', u\'e\', 4L, 0L),\\n                (u\'44\', u\'o\', 2L, 0L)],\\n            MP_TestSortedNodeShortPath: [\\n                (u\'1\', u\'b\', 1L, 2L),\\n                (u\'13\', u\'u\', 2L, 1L),\\n                (u\'134\', u\'i\', 3L, 1L),\\n                (u\'1343\', u\'e\', 4L, 0L),\\n                (u\'14\', u\'o\', 2L, 0L),\\n                (u\'2\', u\'d\', 1L, 0L),\\n                (u\'3\', u\'g\', 1L, 0L),\\n                (u\'4\', u\'a\', 1L, 4L),\\n                (u\'41\', u\'a\', 2L, 0L),\\n                (u\'42\', u\'a\', 2L, 0L),\\n                (u\'43\', u\'u\', 2L, 1L),\\n                (u\'434\', u\'i\', 3L, 1L),\\n                (u\'4343\', u\'e\', 4L, 0L),\\n                (u\'44\', u\'o\', 2L, 0L)]}\\n\\n    def got(self, model):\\n        return [(o.path, o.desc, o.get_depth(), o.get_children_count())\\n                for o in model.get_tree()]\\n\\n    def add_broken_test_data(self, model):\\n        model(path=\'4\', depth=2, numchild=2, desc=\'a\').save()\\n        model(path=\'13\', depth=1000, numchild=0, desc=\'u\').save()\\n        model(path=\'14\', depth=4, numchild=500, desc=\'o\').save()\\n        model(path=\'134\', depth=321, numchild=543, desc=\'i\').save()\\n        model(path=\'1343\', depth=321, numchild=543, desc=\'e\').save()\\n        model(path=\'42\', depth=1, numchild=1, desc=\'a\').save()\\n        model(path=\'43\', depth=1000, numchild=0, desc=\'u\').save()\\n        model(path=\'44\', depth=4, numchild=500, desc=\'o\').save()\\n        model(path=\'434\', depth=321, numchild=543, desc=\'i\').save()\\n        model(path=\'4343\', depth=321, numchild=543, desc=\'e\').save()\\n        model(path=\'41\', depth=1, numchild=1, desc=\'a\').save()\\n        model(path=\'3\', depth=221, numchild=322, desc=\'g\').save()\\n        model(path=\'1\', depth=10, numchild=3, desc=\'b\').save()\\n        model(path=\'2\', depth=10, numchild=3, desc=\'d\').save()\\n\\n    def test_fix_tree_non_destructive(self):\\n\\n        for model in (MP_TestNodeShortPath, MP_TestSortedNodeShortPath):\\n            self.add_broken_test_data(model)\\n            model.fix_tree(destructive=False)\\n            self.assertEqual(self.got(model), self.expected_with_holes[model])\\n            model.find_problems()\\n\\n    def test_fix_tree_destructive(self):\\n\\n        for model in (MP_TestNodeShortPath, MP_TestSortedNodeShortPath):\\n            self.add_broken_test_data(model)\\n            model.fix_tree(destructive=True)\\n            self.assertEqual(self.got(model), self.expected_no_holes[model])\\n            model.find_problems()\\n\\n\\nclass TestIssues(TestCase):\\n    \\"test for http://code.google.com/p/django-treebeard/issues/detail?id=14\\"\\n\\n    def test_many_to_many_django_user_anonymous(self):\\n        if not HAS_DJANGO_AUTH:  # pragma: no cover\\n            self.fail(\'this test needs django.contrib.auth in INSTALLED_APPS\')\\n\\n        # Using AnonymousUser() in the querysets will expose non-treebeard\\n        # related problems in Django 1.0\\n        #\\n        # Postgres:\\n        #   ProgrammingError: can\'t adapt\\n        # SQLite:\\n        #   InterfaceError: Error binding parameter 4 - probably unsupported\\n        #   type.\\n        # MySQL compared a string to an integer field:\\n        #   `treebeard_mp_testissue14_users`.`user_id` = \'AnonymousUser\'\\n        #\\n        # Using a None field instead works (will be translated to IS NULL).\\n        #\\n        # anonuserobj = AnonymousUser()\\n        anonuserobj = None\\n\\n        def qs_check(qs, expected):\\n            self.assertEqual(\\n                [o.name for o in qs],\\n                expected)\\n\\n        user = User.objects.create_user(\'test_user\', \'test@example.com\',\\n                                        \'testpasswd\')\\n        user.save()\\n        root = MP_TestIssue14.add_root(name=\\"the root node\\")\\n\\n        root.add_child(name=\\"first\\")\\n        second = root.add_child(name=\\"second\\")\\n\\n        qs_check(root.get_children(), [\'first\', \'second\'])\\n        qs_check(root.get_children().filter(Q(name=\\"first\\")), [\'first\'])\\n        qs_check(root.get_children().filter(Q(users=user)), [])\\n        qs_check(\\n            root.get_children().filter(Q(name=\\"first\\") | Q(users=user)),\\n            [\'first\'])\\n\\n        user = anonuserobj\\n        qs_check(\\n            root.get_children().filter(Q(name=\\"first\\") | Q(users=user)),\\n            [\'first\', \'second\'])\\n\\n        user = User.objects.get(username=\\"test_user\\")\\n        second.users.add(user)\\n\\n        qs_check(\\n            root.get_children().filter(Q(name=\\"first\\") | Q(users=user)),\\n            [\'first\', \'second\'])\\n\\n        user = anonuserobj\\n        qs_check(\\n            root.get_children().filter(Q(name=\\"first\\") | Q(users=user)),\\n            [\'first\'])\\n\\n\\nclass TestModelAdmin(ModelAdmin):\\n    form = MoveNodeForm\\n\\n\\nclass TestMoveNodeForm(TestTreeBase):\\n\\n    tpl = (u\'\\u003ctr\\u003e\\u003cth\\u003e\\u003clabel for=\\"id__position\\"\\u003ePosition:\\u003c/label\\u003e\\u003c/th\\u003e\'\\n           \'\\u003ctd\\u003e\\u003cselect name=\\"_position\\" id=\\"id__position\\"\\u003e\\\\n\'\\n           \'\\u003coption value=\\"first-child\\"\\u003eFirst child of\\u003c/option\\u003e\\\\n\'\\n           \'\\u003coption value=\\"left\\"\\u003eBefore\\u003c/option\\u003e\\\\n\'\\n           \'\\u003coption value=\\"right\\"\\u003eAfter\\u003c/option\\u003e\\\\n\'\\n           \'\\u003c/select\\u003e\\u003c/td\\u003e\\u003c/tr\\u003e\\\\n\'\\n           \'\\u003ctr\\u003e\\u003cth\\u003e\\u003clabel for=\\"id__ref_node_id\\"\\u003eRelative to:\\u003c/label\\u003e\'\\n           \'\\u003c/th\\u003e\\u003ctd\\u003e\\u003cselect name=\\"_ref_node_id\\" id=\\"id__ref_node_id\\"\\u003e\\\\n\'\\n           \'\\u003coption value=\\"0\\"\\u003e-- root --\\u003c/option\\u003e\\\\n\')\\n\\n    def _multi_form_html_root_node(self):\\n        self.model.load_bulk(BASE_DATA)\\n        node = self.model.get_tree()[0]\\n        form = MoveNodeForm(instance=node)\\n        rtpl = self.tpl\\n        self.assertEqual([\'_position\', \'_ref_node_id\'],\\n                         form.base_fields.keys())\\n        for obj in self.model.get_tree():\\n            if node != obj or obj.is_descendant_of(node):\\n                rtpl += \'\\u003coption value=\\"%d\\"\\u003e%sNode %d\\u003c/option\\u003e\\\\n\' % (\\n                    obj.id, \'. . \' * (obj.get_depth() - 1), obj.id)\\n        rtpl += \'\\u003c/select\\u003e\\u003c/td\\u003e\\u003c/tr\\u003e\'\\n        formstr = unicode(form).replace(u\' selected=\\"selected\\"\', u\'\')\\n        self.assertEqual(rtpl, formstr)\\n\\n    def _multi_form_html_leaf_node(self):\\n        self.model.load_bulk(BASE_DATA)\\n        nodes = list(self.model.get_tree())\\n        node = nodes[-1]\\n        form = MoveNodeForm(instance=node)\\n        rtpl = self.tpl\\n        self.assertEqual([\'_position\', \'_ref_node_id\'],\\n                         form.base_fields.keys())\\n        for obj in self.model.get_tree():\\n            if node != obj or obj.is_descendant_of(node):\\n                rtpl += \'\\u003coption value=\\"%d\\"\\u003e%sNode %d\\u003c/option\\u003e\\\\n\' % (\\n                    obj.id, \'. . \' * (obj.get_depth() - 1), obj.id)\\n        rtpl += \'\\u003c/select\\u003e\\u003c/td\\u003e\\u003c/tr\\u003e\'\\n        formstr = unicode(form).replace(u\' selected=\\"selected\\"\', u\'\')\\n        self.assertEqual(rtpl, formstr)\\n\\n    def _multi_admin_html(self):\\n        tpl = (\'\\u003ctr\\u003e\\u003cth\\u003e\\u003clabel for=\\"id_desc\\"\\u003eDesc:\\u003c/label\\u003e\'\\n               \'\\u003c/th\\u003e\\u003ctd\\u003e\\u003cinput id=\\"id_desc\\" type=\\"text\\" class=\\"vTextField\\" \'\\n               \'name=\\"desc\\" maxlength=\\"255\\" /\\u003e\\u003c/td\\u003e\\u003c/tr\\u003e\\\\n\'\\n               \'\\u003ctr\\u003e\\u003cth\\u003e\\u003clabel for=\\"id__position\\"\\u003ePosition:\\u003c/label\\u003e\\u003c/th\\u003e\'\\n               \'\\u003ctd\\u003e\\u003cselect name=\\"_position\\" id=\\"id__position\\"\\u003e\\\\n\'\\n               \'\\u003coption value=\\"first-child\\"\\u003eFirst child of\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"left\\"\\u003eBefore\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"right\\"\\u003eAfter\\u003c/option\\u003e\\\\n\'\\n               \'\\u003c/select\\u003e\\u003c/td\\u003e\\u003c/tr\\u003e\\\\n\'\\n               \'\\u003ctr\\u003e\\u003cth\\u003e\\u003clabel for=\\"id__ref_node_id\\"\\u003eRelative to:\\u003c/label\\u003e\'\\n               \'\\u003c/th\\u003e\\u003ctd\\u003e\\u003cselect name=\\"_ref_node_id\\" id=\\"id__ref_node_id\\"\\u003e\\\\n\'\\n               \'\\u003coption value=\\"0\\"\\u003e-- root --\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003eNode %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003eNode %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003e. . Node %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003e. . Node %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003e. . Node %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003e. . . . Node %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003e. . Node %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003eNode %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003eNode %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003e. . Node %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003c/select\\u003e\\u003c/td\\u003e\\u003c/tr\\u003e\')\\n        request = None\\n        self.model.load_bulk(BASE_DATA)\\n        for node in self.model.objects.all():\\n            site = AdminSite()\\n            ma = TestModelAdmin(self.model, site)\\n            self.assertEqual(\\n                [\'desc\', \'_position\', \'_ref_node_id\'],\\n                ma.get_form(request).base_fields.keys())\\n            self.assertEqual(\\n                [(None, {\'fields\': [\'desc\', \'_position\', \'_ref_node_id\']})],\\n                ma.get_fieldsets(request))\\n            self.assertEqual(\\n                [(None, {\'fields\': [\'desc\', \'_position\', \'_ref_node_id\']})],\\n                ma.get_fieldsets(request, node))\\n            form = ma.get_form(request)()\\n            ids = []\\n            for obj in self.model.get_tree():\\n                ids.extend([obj.id] * 2)\\n            self.assertEqual(tpl % tuple(ids), unicode(form))\\n\\n\\n_load_test_methods(TestMoveNodeForm)\\n_load_test_methods(TestEmptyTree)\\n_load_test_methods(TestClassMethods)\\n_load_test_methods(TestSimpleNodeMethods)\\n_load_test_methods(TestAddChild)\\n_load_test_methods(TestAddSibling)\\n_load_test_methods(TestDelete)\\n_load_test_methods(TestMoveErrors)\\n_load_test_methods(TestMoveLeafRoot)\\n_load_test_methods(TestMoveLeaf)\\n_load_test_methods(TestMoveBranchRoot)\\n_load_test_methods(TestMoveBranch)\\n_load_test_methods(TestHelpers)\\n# we didn\'t create extra sorted-proxy models\\n_load_test_methods(TestMoveSortedErrors, proxy=False)\\n_load_test_methods(TestTreeSorted, proxy=False)\\n"}\n'
line: b'{"repo_name":"2014cdbg14/2014cdbg14","ref":"refs/heads/master","path":"wsgi/static/Brython2.1.0-20140419-113919/Lib/collections/abc.py","content":"# Copyright 2007 Google, Inc. All Rights Reserved.\\n# Licensed to PSF under a Contributor Agreement.\\n\\n\\"\\"\\"Abstract Base Classes (ABCs) for collections, according to PEP 3119.\\n\\nUnit tests are in test_collections.\\n\\"\\"\\"\\n\\nfrom abc import ABCMeta, abstractmethod\\nimport sys\\n\\n__all__ = [\\"Hashable\\", \\"Iterable\\", \\"Iterator\\",\\n           \\"Sized\\", \\"Container\\", \\"Callable\\",\\n           \\"Set\\", \\"MutableSet\\",\\n           \\"Mapping\\", \\"MutableMapping\\",\\n           \\"MappingView\\", \\"KeysView\\", \\"ItemsView\\", \\"ValuesView\\",\\n           \\"Sequence\\", \\"MutableSequence\\",\\n           \\"ByteString\\",\\n           ]\\n\\n# Private list of types that we want to register with the various ABCs\\n# so that they will pass tests like:\\n#       it = iter(somebytearray)\\n#       assert isinstance(it, Iterable)\\n# Note:  in other implementations, these types many not be distinct\\n# and they make have their own implementation specific types that\\n# are not included on this list.\\nbytes_iterator = type(iter(b\'\'))\\nbytearray_iterator = type(iter(bytearray()))\\n#callable_iterator = ???\\ndict_keyiterator = type(iter({}.keys()))\\ndict_valueiterator = type(iter({}.values()))\\ndict_itemiterator = type(iter({}.items()))\\nlist_iterator = type(iter([]))\\nlist_reverseiterator = type(iter(reversed([])))\\nrange_iterator = type(iter(range(0)))\\nset_iterator = type(iter(set()))\\nstr_iterator = type(iter(\\"\\"))\\ntuple_iterator = type(iter(()))\\nzip_iterator = type(iter(zip()))\\n## views ##\\ndict_keys = type({}.keys())\\ndict_values = type({}.values())\\ndict_items = type({}.items())\\n## misc ##\\nmappingproxy = type(type.__dict__)\\n\\n\\n### ONE-TRICK PONIES ###\\n\\nclass Hashable(metaclass=ABCMeta):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __hash__(self):\\n        return 0\\n\\n    @classmethod\\n    def __subclasshook__(cls, C):\\n        if cls is Hashable:\\n            for B in C.__mro__:\\n                if \\"__hash__\\" in B.__dict__:\\n                    if B.__dict__[\\"__hash__\\"]:\\n                        return True\\n                    break\\n        return NotImplemented\\n\\n\\nclass Iterable(metaclass=ABCMeta):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __iter__(self):\\n        while False:\\n            yield None\\n\\n    @classmethod\\n    def __subclasshook__(cls, C):\\n        if cls is Iterable:\\n            if any(\\"__iter__\\" in B.__dict__ for B in C.__mro__):\\n                return True\\n        return NotImplemented\\n\\n\\nclass Iterator(Iterable):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __next__(self):\\n        raise StopIteration\\n\\n    def __iter__(self):\\n        return self\\n\\n    @classmethod\\n    def __subclasshook__(cls, C):\\n        if cls is Iterator:\\n            if (any(\\"__next__\\" in B.__dict__ for B in C.__mro__) and\\n                any(\\"__iter__\\" in B.__dict__ for B in C.__mro__)):\\n                return True\\n        return NotImplemented\\n\\nIterator.register(bytes_iterator)\\nIterator.register(bytearray_iterator)\\n#Iterator.register(callable_iterator)\\nIterator.register(dict_keyiterator)\\nIterator.register(dict_valueiterator)\\nIterator.register(dict_itemiterator)\\nIterator.register(list_iterator)\\nIterator.register(list_reverseiterator)\\nIterator.register(range_iterator)\\nIterator.register(set_iterator)\\nIterator.register(str_iterator)\\nIterator.register(tuple_iterator)\\nIterator.register(zip_iterator)\\n\\nclass Sized(metaclass=ABCMeta):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __len__(self):\\n        return 0\\n\\n    @classmethod\\n    def __subclasshook__(cls, C):\\n        if cls is Sized:\\n            if any(\\"__len__\\" in B.__dict__ for B in C.__mro__):\\n                return True\\n        return NotImplemented\\n\\n\\nclass Container(metaclass=ABCMeta):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __contains__(self, x):\\n        return False\\n\\n    @classmethod\\n    def __subclasshook__(cls, C):\\n        if cls is Container:\\n            if any(\\"__contains__\\" in B.__dict__ for B in C.__mro__):\\n                return True\\n        return NotImplemented\\n\\n\\nclass Callable(metaclass=ABCMeta):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __call__(self, *args, **kwds):\\n        return False\\n\\n    @classmethod\\n    def __subclasshook__(cls, C):\\n        if cls is Callable:\\n            if any(\\"__call__\\" in B.__dict__ for B in C.__mro__):\\n                return True\\n        return NotImplemented\\n\\n\\n### SETS ###\\n\\n\\nclass Set(Sized, Iterable, Container):\\n\\n    \\"\\"\\"A set is a finite, iterable container.\\n\\n    This class provides concrete generic implementations of all\\n    methods except for __contains__, __iter__ and __len__.\\n\\n    To override the comparisons (presumably for speed, as the\\n    semantics are fixed), all you have to do is redefine __le__ and\\n    then the other operations will automatically follow suit.\\n    \\"\\"\\"\\n\\n    __slots__ = ()\\n\\n    def __le__(self, other):\\n        if not isinstance(other, Set):\\n            return NotImplemented\\n        if len(self) \\u003e len(other):\\n            return False\\n        for elem in self:\\n            if elem not in other:\\n                return False\\n        return True\\n\\n    def __lt__(self, other):\\n        if not isinstance(other, Set):\\n            return NotImplemented\\n        return len(self) \\u003c len(other) and self.__le__(other)\\n\\n    def __gt__(self, other):\\n        if not isinstance(other, Set):\\n            return NotImplemented\\n        return other \\u003c self\\n\\n    def __ge__(self, other):\\n        if not isinstance(other, Set):\\n            return NotImplemented\\n        return other \\u003c= self\\n\\n    def __eq__(self, other):\\n        if not isinstance(other, Set):\\n            return NotImplemented\\n        return len(self) == len(other) and self.__le__(other)\\n\\n    def __ne__(self, other):\\n        return not (self == other)\\n\\n    @classmethod\\n    def _from_iterable(cls, it):\\n        \'\'\'Construct an instance of the class from any iterable input.\\n\\n        Must override this method if the class constructor signature\\n        does not accept an iterable for an input.\\n        \'\'\'\\n        return cls(it)\\n\\n    def __and__(self, other):\\n        if not isinstance(other, Iterable):\\n            return NotImplemented\\n        return self._from_iterable(value for value in other if value in self)\\n\\n    def isdisjoint(self, other):\\n        for value in other:\\n            if value in self:\\n                return False\\n        return True\\n\\n    def __or__(self, other):\\n        if not isinstance(other, Iterable):\\n            return NotImplemented\\n        chain = (e for s in (self, other) for e in s)\\n        return self._from_iterable(chain)\\n\\n    def __sub__(self, other):\\n        if not isinstance(other, Set):\\n            if not isinstance(other, Iterable):\\n                return NotImplemented\\n            other = self._from_iterable(other)\\n        return self._from_iterable(value for value in self\\n                                   if value not in other)\\n\\n    def __xor__(self, other):\\n        if not isinstance(other, Set):\\n            if not isinstance(other, Iterable):\\n                return NotImplemented\\n            other = self._from_iterable(other)\\n        return (self - other) | (other - self)\\n\\n    def _hash(self):\\n        \\"\\"\\"Compute the hash value of a set.\\n\\n        Note that we don\'t define __hash__: not all sets are hashable.\\n        But if you define a hashable set type, its __hash__ should\\n        call this function.\\n\\n        This must be compatible __eq__.\\n\\n        All sets ought to compare equal if they contain the same\\n        elements, regardless of how they are implemented, and\\n        regardless of the order of the elements; so there\'s not much\\n        freedom for __eq__ or __hash__.  We match the algorithm used\\n        by the built-in frozenset type.\\n        \\"\\"\\"\\n        MAX = sys.maxsize\\n        MASK = 2 * MAX + 1\\n        n = len(self)\\n        h = 1927868237 * (n + 1)\\n        h \\u0026= MASK\\n        for x in self:\\n            hx = hash(x)\\n            h ^= (hx ^ (hx \\u003c\\u003c 16) ^ 89869747)  * 3644798167\\n            h \\u0026= MASK\\n        h = h * 69069 + 907133923\\n        h \\u0026= MASK\\n        if h \\u003e MAX:\\n            h -= MASK + 1\\n        if h == -1:\\n            h = 590923713\\n        return h\\n\\nSet.register(frozenset)\\n\\n\\nclass MutableSet(Set):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def add(self, value):\\n        \\"\\"\\"Add an element.\\"\\"\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def discard(self, value):\\n        \\"\\"\\"Remove an element.  Do not raise an exception if absent.\\"\\"\\"\\n        raise NotImplementedError\\n\\n    def remove(self, value):\\n        \\"\\"\\"Remove an element. If not a member, raise a KeyError.\\"\\"\\"\\n        if value not in self:\\n            raise KeyError(value)\\n        self.discard(value)\\n\\n    def pop(self):\\n        \\"\\"\\"Return the popped value.  Raise KeyError if empty.\\"\\"\\"\\n        it = iter(self)\\n        try:\\n            value = next(it)\\n        except StopIteration:\\n            raise KeyError\\n        self.discard(value)\\n        return value\\n\\n    def clear(self):\\n        \\"\\"\\"This is slow (creates N new iterators!) but effective.\\"\\"\\"\\n        try:\\n            while True:\\n                self.pop()\\n        except KeyError:\\n            pass\\n\\n    def __ior__(self, it):\\n        for value in it:\\n            self.add(value)\\n        return self\\n\\n    def __iand__(self, it):\\n        for value in (self - it):\\n            self.discard(value)\\n        return self\\n\\n    def __ixor__(self, it):\\n        if it is self:\\n            self.clear()\\n        else:\\n            if not isinstance(it, Set):\\n                it = self._from_iterable(it)\\n            for value in it:\\n                if value in self:\\n                    self.discard(value)\\n                else:\\n                    self.add(value)\\n        return self\\n\\n    def __isub__(self, it):\\n        if it is self:\\n            self.clear()\\n        else:\\n            for value in it:\\n                self.discard(value)\\n        return self\\n\\nMutableSet.register(set)\\n\\n\\n### MAPPINGS ###\\n\\n\\nclass Mapping(Sized, Iterable, Container):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __getitem__(self, key):\\n        raise KeyError\\n\\n    def get(self, key, default=None):\\n        try:\\n            return self[key]\\n        except KeyError:\\n            return default\\n\\n    def __contains__(self, key):\\n        try:\\n            self[key]\\n        except KeyError:\\n            return False\\n        else:\\n            return True\\n\\n    def keys(self):\\n        return KeysView(self)\\n\\n    def items(self):\\n        return ItemsView(self)\\n\\n    def values(self):\\n        return ValuesView(self)\\n\\n    def __eq__(self, other):\\n        if not isinstance(other, Mapping):\\n            return NotImplemented\\n        return dict(self.items()) == dict(other.items())\\n\\n    def __ne__(self, other):\\n        return not (self == other)\\n\\nMapping.register(mappingproxy)\\n\\n\\nclass MappingView(Sized):\\n\\n    def __init__(self, mapping):\\n        self._mapping = mapping\\n\\n    def __len__(self):\\n        return len(self._mapping)\\n\\n    def __repr__(self):\\n        return \'{0.__class__.__name__}({0._mapping!r})\'.format(self)\\n\\n\\nclass KeysView(MappingView, Set):\\n\\n    @classmethod\\n    def _from_iterable(self, it):\\n        return set(it)\\n\\n    def __contains__(self, key):\\n        return key in self._mapping\\n\\n    def __iter__(self):\\n        for key in self._mapping:\\n            yield key\\n\\nKeysView.register(dict_keys)\\n\\n\\nclass ItemsView(MappingView, Set):\\n\\n    @classmethod\\n    def _from_iterable(self, it):\\n        return set(it)\\n\\n    def __contains__(self, item):\\n        key, value = item\\n        try:\\n            v = self._mapping[key]\\n        except KeyError:\\n            return False\\n        else:\\n            return v == value\\n\\n    def __iter__(self):\\n        for key in self._mapping:\\n            yield (key, self._mapping[key])\\n\\nItemsView.register(dict_items)\\n\\n\\nclass ValuesView(MappingView):\\n\\n    def __contains__(self, value):\\n        for key in self._mapping:\\n            if value == self._mapping[key]:\\n                return True\\n        return False\\n\\n    def __iter__(self):\\n        for key in self._mapping:\\n            yield self._mapping[key]\\n\\nValuesView.register(dict_values)\\n\\n\\nclass MutableMapping(Mapping):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __setitem__(self, key, value):\\n        raise KeyError\\n\\n    @abstractmethod\\n    def __delitem__(self, key):\\n        raise KeyError\\n\\n    __marker = object()\\n\\n    def pop(self, key, default=__marker):\\n        try:\\n            value = self[key]\\n        except KeyError:\\n            if default is self.__marker:\\n                raise\\n            return default\\n        else:\\n            del self[key]\\n            return value\\n\\n    def popitem(self):\\n        try:\\n            key = next(iter(self))\\n        except StopIteration:\\n            raise KeyError\\n        value = self[key]\\n        del self[key]\\n        return key, value\\n\\n    def clear(self):\\n        try:\\n            while True:\\n                self.popitem()\\n        except KeyError:\\n            pass\\n\\n    def update(*args, **kwds):\\n        if len(args) \\u003e 2:\\n            raise TypeError(\\"update() takes at most 2 positional \\"\\n                            \\"arguments ({} given)\\".format(len(args)))\\n        elif not args:\\n            raise TypeError(\\"update() takes at least 1 argument (0 given)\\")\\n        self = args[0]\\n        other = args[1] if len(args) \\u003e= 2 else ()\\n\\n        if isinstance(other, Mapping):\\n            for key in other:\\n                self[key] = other[key]\\n        elif hasattr(other, \\"keys\\"):\\n            for key in other.keys():\\n                self[key] = other[key]\\n        else:\\n            for key, value in other:\\n                self[key] = value\\n        for key, value in kwds.items():\\n            self[key] = value\\n\\n    def setdefault(self, key, default=None):\\n        try:\\n            return self[key]\\n        except KeyError:\\n            self[key] = default\\n        return default\\n\\nMutableMapping.register(dict)\\n\\n\\n### SEQUENCES ###\\n\\n\\nclass Sequence(Sized, Iterable, Container):\\n\\n    \\"\\"\\"All the operations on a read-only sequence.\\n\\n    Concrete subclasses must override __new__ or __init__,\\n    __getitem__, and __len__.\\n    \\"\\"\\"\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __getitem__(self, index):\\n        raise IndexError\\n\\n    def __iter__(self):\\n        i = 0\\n        try:\\n            while True:\\n                v = self[i]\\n                yield v\\n                i += 1\\n        except IndexError:\\n            return\\n\\n    def __contains__(self, value):\\n        for v in self:\\n            if v == value:\\n                return True\\n        return False\\n\\n    def __reversed__(self):\\n        for i in reversed(range(len(self))):\\n            yield self[i]\\n\\n    def index(self, value):\\n        for i, v in enumerate(self):\\n            if v == value:\\n                return i\\n        raise ValueError\\n\\n    def count(self, value):\\n        return sum(1 for v in self if v == value)\\n\\nSequence.register(tuple)\\nSequence.register(str)\\nSequence.register(range)\\n\\n\\nclass ByteString(Sequence):\\n\\n    \\"\\"\\"This unifies bytes and bytearray.\\n\\n    XXX Should add all their methods.\\n    \\"\\"\\"\\n\\n    __slots__ = ()\\n\\nByteString.register(bytes)\\nByteString.register(bytearray)\\n\\n\\nclass MutableSequence(Sequence):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __setitem__(self, index, value):\\n        raise IndexError\\n\\n    @abstractmethod\\n    def __delitem__(self, index):\\n        raise IndexError\\n\\n    @abstractmethod\\n    def insert(self, index, value):\\n        raise IndexError\\n\\n    def append(self, value):\\n        self.insert(len(self), value)\\n\\n    def clear(self):\\n        try:\\n            while True:\\n                self.pop()\\n        except IndexError:\\n            pass\\n\\n    def reverse(self):\\n        n = len(self)\\n        for i in range(n//2):\\n            self[i], self[n-i-1] = self[n-i-1], self[i]\\n\\n    def extend(self, values):\\n        for v in values:\\n            self.append(v)\\n\\n    def pop(self, index=-1):\\n        v = self[index]\\n        del self[index]\\n        return v\\n\\n    def remove(self, value):\\n        del self[self.index(value)]\\n\\n    def __iadd__(self, values):\\n        self.extend(values)\\n        return self\\n\\nMutableSequence.register(list)\\nMutableSequence.register(bytearray)  # Multiply inheriting, see ByteString\\n"}\n'
line: b'{"repo_name":"40223205/w16b_test-","ref":"refs/heads/master","path":"static/Brython3.1.1-20150328-091302/Lib/_socket.py","content":"\\"\\"\\"Implementation module for socket operations.\\n\\nSee the socket module for documentation.\\"\\"\\"\\n\\n\\nAF_APPLETALK = 16\\n\\nAF_DECnet = 12\\n\\nAF_INET = 2\\n\\nAF_INET6 = 23\\n\\nAF_IPX = 6\\n\\nAF_IRDA = 26\\n\\nAF_SNA = 11\\n\\nAF_UNSPEC = 0\\n\\nAI_ADDRCONFIG = 1024\\n\\nAI_ALL = 256\\n\\nAI_CANONNAME = 2\\n\\nAI_NUMERICHOST = 4\\n\\nAI_NUMERICSERV = 8\\n\\nAI_PASSIVE = 1\\n\\nAI_V4MAPPED = 2048\\n\\nCAPI = \'\\u003ccapsule object \\"_socket.CAPI\\" at 0x00BC4F38\\u003e\'\\n\\nEAI_AGAIN = 11002\\n\\nEAI_BADFLAGS = 10022\\n\\nEAI_FAIL = 11003\\n\\nEAI_FAMILY = 10047\\n\\nEAI_MEMORY = 8\\n\\nEAI_NODATA = 11001\\n\\nEAI_NONAME = 11001\\n\\nEAI_SERVICE = 10109\\n\\nEAI_SOCKTYPE = 10044\\n\\nINADDR_ALLHOSTS_GROUP = -536870911\\n\\nINADDR_ANY = 0\\n\\nINADDR_BROADCAST = -1\\n\\nINADDR_LOOPBACK = 2130706433\\n\\nINADDR_MAX_LOCAL_GROUP = -536870657\\n\\nINADDR_NONE = -1\\n\\nINADDR_UNSPEC_GROUP = -536870912\\n\\nIPPORT_RESERVED = 1024\\n\\nIPPORT_USERRESERVED = 5000\\n\\nIPPROTO_ICMP = 1\\n\\nIPPROTO_IP = 0\\n\\nIPPROTO_RAW = 255\\n\\nIPPROTO_TCP = 6\\n\\nIPPROTO_UDP = 17\\n\\nIPV6_CHECKSUM = 26\\n\\nIPV6_DONTFRAG = 14\\n\\nIPV6_HOPLIMIT = 21\\n\\nIPV6_HOPOPTS = 1\\n\\nIPV6_JOIN_GROUP = 12\\n\\nIPV6_LEAVE_GROUP = 13\\n\\nIPV6_MULTICAST_HOPS = 10\\n\\nIPV6_MULTICAST_IF = 9\\n\\nIPV6_MULTICAST_LOOP = 11\\n\\nIPV6_PKTINFO = 19\\n\\nIPV6_RECVRTHDR = 38\\n\\nIPV6_RECVTCLASS = 40\\n\\nIPV6_RTHDR = 32\\n\\nIPV6_TCLASS = 39\\n\\nIPV6_UNICAST_HOPS = 4\\n\\nIPV6_V6ONLY = 27\\n\\nIP_ADD_MEMBERSHIP = 12\\n\\nIP_DROP_MEMBERSHIP = 13\\n\\nIP_HDRINCL = 2\\n\\nIP_MULTICAST_IF = 9\\n\\nIP_MULTICAST_LOOP = 11\\n\\nIP_MULTICAST_TTL = 10\\n\\nIP_OPTIONS = 1\\n\\nIP_RECVDSTADDR = 25\\n\\nIP_TOS = 3\\n\\nIP_TTL = 4\\n\\nMSG_BCAST = 1024\\n\\nMSG_CTRUNC = 512\\n\\nMSG_DONTROUTE = 4\\n\\nMSG_MCAST = 2048\\n\\nMSG_OOB = 1\\n\\nMSG_PEEK = 2\\n\\nMSG_TRUNC = 256\\n\\nNI_DGRAM = 16\\n\\nNI_MAXHOST = 1025\\n\\nNI_MAXSERV = 32\\n\\nNI_NAMEREQD = 4\\n\\nNI_NOFQDN = 1\\n\\nNI_NUMERICHOST = 2\\n\\nNI_NUMERICSERV = 8\\n\\nRCVALL_MAX = 3\\n\\nRCVALL_OFF = 0\\n\\nRCVALL_ON = 1\\n\\nRCVALL_SOCKETLEVELONLY = 2\\n\\nSHUT_RD = 0\\n\\nSHUT_RDWR = 2\\n\\nSHUT_WR = 1\\n\\nSIO_KEEPALIVE_VALS = 2550136836\\n\\nSIO_RCVALL = 2550136833\\n\\nSOCK_DGRAM = 2\\n\\nSOCK_RAW = 3\\n\\nSOCK_RDM = 4\\n\\nSOCK_SEQPACKET = 5\\n\\nSOCK_STREAM = 1\\n\\nSOL_IP = 0\\n\\nSOL_SOCKET = 65535\\n\\nSOL_TCP = 6\\n\\nSOL_UDP = 17\\n\\nSOMAXCONN = 2147483647\\n\\nSO_ACCEPTCONN = 2\\n\\nSO_BROADCAST = 32\\n\\nSO_DEBUG = 1\\n\\nSO_DONTROUTE = 16\\n\\nSO_ERROR = 4103\\n\\nSO_EXCLUSIVEADDRUSE = -5\\n\\nSO_KEEPALIVE = 8\\n\\nSO_LINGER = 128\\n\\nSO_OOBINLINE = 256\\n\\nSO_RCVBUF = 4098\\n\\nSO_RCVLOWAT = 4100\\n\\nSO_RCVTIMEO = 4102\\n\\nSO_REUSEADDR = 4\\n\\nSO_SNDBUF = 4097\\n\\nSO_SNDLOWAT = 4099\\n\\nSO_SNDTIMEO = 4101\\n\\nSO_TYPE = 4104\\n\\nSO_USELOOPBACK = 64\\n\\nclass SocketType:\\n    pass\\n\\nTCP_MAXSEG = 4\\n\\nTCP_NODELAY = 1\\n\\n__loader__ = \'\\u003c_frozen_importlib.ExtensionFileLoader object at 0x00CA2D90\\u003e\'\\n\\ndef dup(*args,**kw):\\n    \\"\\"\\"dup(integer) -\\u003e integer    \\n    Duplicate an integer socket file descriptor.  This is like os.dup(), but for\\n    sockets; on some platforms os.dup() won\'t work for socket file descriptors.\\"\\"\\"\\n    pass\\n\\nclass error:\\n    pass\\n\\nclass gaierror:\\n    pass\\n\\ndef getaddrinfo(*args,**kw):\\n    \\"\\"\\"getaddrinfo(host, port [, family, socktype, proto, flags])        -\\u003e list of (family, socktype, proto, canonname, sockaddr)\\n    \\n    Resolve host and port into addrinfo struct.\\"\\"\\"\\n    pass\\n\\ndef getdefaulttimeout(*args,**kw):\\n    \\"\\"\\"getdefaulttimeout() -\\u003e timeout    \\n    Returns the default timeout in seconds (float) for new socket objects.\\n    A value of None indicates that new socket objects have no timeout.\\n    When the socket module is first imported, the default is None.\\"\\"\\"\\n    pass\\n\\ndef gethostbyaddr(*args,**kw):\\n    \\"\\"\\"gethostbyaddr(host) -\\u003e (name, aliaslist, addresslist)    \\n    Return the true host name, a list of aliases, and a list of IP addresses,\\n    for a host.  The host argument is a string giving a host name or IP number.\\"\\"\\"\\n    pass\\n\\ndef gethostbyname(*args,**kw):\\n    \\"\\"\\"gethostbyname(host) -\\u003e address    \\n    Return the IP address (a string of the form \'255.255.255.255\') for a host.\\"\\"\\"\\n    pass\\n\\ndef gethostbyname_ex(*args,**kw):\\n    \\"\\"\\"gethostbyname_ex(host) -\\u003e (name, aliaslist, addresslist)    \\n    Return the true host name, a list of aliases, and a list of IP addresses,\\n    for a host.  The host argument is a string giving a host name or IP number.\\"\\"\\"\\n    pass\\n\\ndef gethostname(*args,**kw):\\n    \\"\\"\\"gethostname() -\\u003e string    \\n    Return the current host name.\\"\\"\\"\\n    pass\\n\\ndef getnameinfo(*args,**kw):\\n    \\"\\"\\"getnameinfo(sockaddr, flags) --\\u003e (host, port)    \\n    Get host and port for a sockaddr.\\"\\"\\"\\n    pass\\n\\ndef getprotobyname(*args,**kw):\\n    \\"\\"\\"getprotobyname(name) -\\u003e integer    \\n    Return the protocol number for the named protocol.  (Rarely used.)\\"\\"\\"\\n    pass\\n\\ndef getservbyname(*args,**kw):\\n    \\"\\"\\"getservbyname(servicename[, protocolname]) -\\u003e integer    \\n    Return a port number from a service name and protocol name.\\n    The optional protocol name, if given, should be \'tcp\' or \'udp\',\\n    otherwise any protocol will match.\\"\\"\\"\\n    pass\\n\\ndef getservbyport(*args,**kw):\\n    \\"\\"\\"getservbyport(port[, protocolname]) -\\u003e string    \\n    Return the service name from a port number and protocol name.\\n    The optional protocol name, if given, should be \'tcp\' or \'udp\',\\n    otherwise any protocol will match.\\"\\"\\"\\n    pass\\n\\nhas_ipv6 = True\\n\\nclass herror:\\n    pass\\n\\ndef htonl(*args,**kw):\\n    \\"\\"\\"htonl(integer) -\\u003e integer    \\n    Convert a 32-bit integer from host to network byte order.\\"\\"\\"\\n    pass\\n\\ndef htons(*args,**kw):\\n    \\"\\"\\"htons(integer) -\\u003e integer    \\n    Convert a 16-bit integer from host to network byte order.\\"\\"\\"\\n    pass\\n\\ndef inet_aton(*args,**kw):\\n    \\"\\"\\"inet_aton(string) -\\u003e bytes giving packed 32-bit IP representation    \\n    Convert an IP address in string format (123.45.67.89) to the 32-bit packed\\n    binary format used in low-level network functions.\\"\\"\\"\\n    pass\\n\\ndef inet_ntoa(*args,**kw):\\n    \\"\\"\\"inet_ntoa(packed_ip) -\\u003e ip_address_string    \\n    Convert an IP address from 32-bit packed binary format to string format\\"\\"\\"\\n    pass\\n\\ndef ntohl(*args,**kw):\\n    \\"\\"\\"ntohl(integer) -\\u003e integer    \\n    Convert a 32-bit integer from network to host byte order.\\"\\"\\"\\n    pass\\n\\ndef ntohs(*args,**kw):\\n    \\"\\"\\"ntohs(integer) -\\u003e integer    \\n    Convert a 16-bit integer from network to host byte order.\\"\\"\\"\\n    pass\\n\\ndef setdefaulttimeout(*args,**kw):\\n    \\"\\"\\"setdefaulttimeout(timeout)    \\n    Set the default timeout in seconds (float) for new socket objects.\\n    A value of None indicates that new socket objects have no timeout.\\n    When the socket module is first imported, the default is None.\\"\\"\\"\\n    pass\\n\\nclass socket:\\n    def __init__(self,*args,**kw):\\n        pass\\n    def bind(self,*args,**kw):\\n        pass\\n    def close(self):\\n        pass\\n\\nclass timeout:\\n    pass\\n"}\n'
line: b'{"repo_name":"indevgr/django","ref":"refs/heads/master","path":"tests/servers/tests.py","content":"# -*- encoding: utf-8 -*-\\n\\"\\"\\"\\nTests for django.core.servers.\\n\\"\\"\\"\\nfrom __future__ import unicode_literals\\n\\nimport contextlib\\nimport errno\\nimport os\\nimport socket\\n\\nfrom django.core.exceptions import ImproperlyConfigured\\nfrom django.test import LiveServerTestCase, override_settings\\nfrom django.utils._os import upath\\nfrom django.utils.http import urlencode\\nfrom django.utils.six import text_type\\nfrom django.utils.six.moves.urllib.error import HTTPError\\nfrom django.utils.six.moves.urllib.request import urlopen\\n\\nfrom .models import Person\\n\\nTEST_ROOT = os.path.dirname(upath(__file__))\\nTEST_SETTINGS = {\\n    \'MEDIA_URL\': \'/media/\',\\n    \'MEDIA_ROOT\': os.path.join(TEST_ROOT, \'media\'),\\n    \'STATIC_URL\': \'/static/\',\\n    \'STATIC_ROOT\': os.path.join(TEST_ROOT, \'static\'),\\n}\\n\\n\\n@override_settings(ROOT_URLCONF=\'servers.urls\', **TEST_SETTINGS)\\nclass LiveServerBase(LiveServerTestCase):\\n\\n    available_apps = [\\n        \'servers\',\\n        \'django.contrib.auth\',\\n        \'django.contrib.contenttypes\',\\n        \'django.contrib.sessions\',\\n    ]\\n    fixtures = [\'testdata.json\']\\n\\n    def urlopen(self, url):\\n        return urlopen(self.live_server_url + url)\\n\\n\\nclass LiveServerAddress(LiveServerBase):\\n    \\"\\"\\"\\n    Ensure that the address set in the environment variable is valid.\\n    Refs #2879.\\n    \\"\\"\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        # Backup original environment variable\\n        address_predefined = \'DJANGO_LIVE_TEST_SERVER_ADDRESS\' in os.environ\\n        old_address = os.environ.get(\'DJANGO_LIVE_TEST_SERVER_ADDRESS\')\\n\\n        # Just the host is not accepted\\n        cls.raises_exception(\'localhost\', ImproperlyConfigured)\\n\\n        # The host must be valid\\n        cls.raises_exception(\'blahblahblah:8081\', socket.error)\\n\\n        # The list of ports must be in a valid format\\n        cls.raises_exception(\'localhost:8081,\', ImproperlyConfigured)\\n        cls.raises_exception(\'localhost:8081,blah\', ImproperlyConfigured)\\n        cls.raises_exception(\'localhost:8081-\', ImproperlyConfigured)\\n        cls.raises_exception(\'localhost:8081-blah\', ImproperlyConfigured)\\n        cls.raises_exception(\'localhost:8081-8082-8083\', ImproperlyConfigured)\\n\\n        # Restore original environment variable\\n        if address_predefined:\\n            os.environ[\'DJANGO_LIVE_TEST_SERVER_ADDRESS\'] = old_address\\n        else:\\n            del os.environ[\'DJANGO_LIVE_TEST_SERVER_ADDRESS\']\\n\\n        # put it in a list to prevent descriptor lookups in test\\n        cls.live_server_url_test = [cls.live_server_url]\\n\\n    @classmethod\\n    def tearDownClass(cls):\\n        # skip it, as setUpClass doesn\'t call its parent either\\n        pass\\n\\n    @classmethod\\n    def raises_exception(cls, address, exception):\\n        os.environ[\'DJANGO_LIVE_TEST_SERVER_ADDRESS\'] = address\\n        try:\\n            super(LiveServerAddress, cls).setUpClass()\\n            raise Exception(\\"The line above should have raised an exception\\")\\n        except exception:\\n            pass\\n        finally:\\n            super(LiveServerAddress, cls).tearDownClass()\\n\\n    def test_live_server_url_is_class_property(self):\\n        self.assertIsInstance(self.live_server_url_test[0], text_type)\\n        self.assertEqual(self.live_server_url_test[0], self.live_server_url)\\n\\n\\nclass LiveServerViews(LiveServerBase):\\n    def test_404(self):\\n        \\"\\"\\"\\n        Ensure that the LiveServerTestCase serves 404s.\\n        Refs #2879.\\n        \\"\\"\\"\\n        try:\\n            self.urlopen(\'/\')\\n        except HTTPError as err:\\n            self.assertEqual(err.code, 404, \'Expected 404 response\')\\n        else:\\n            self.fail(\'Expected 404 response\')\\n\\n    def test_view(self):\\n        \\"\\"\\"\\n        Ensure that the LiveServerTestCase serves views.\\n        Refs #2879.\\n        \\"\\"\\"\\n        with contextlib.closing(self.urlopen(\'/example_view/\')) as f:\\n            self.assertEqual(f.read(), b\'example view\')\\n\\n    def test_static_files(self):\\n        \\"\\"\\"\\n        Ensure that the LiveServerTestCase serves static files.\\n        Refs #2879.\\n        \\"\\"\\"\\n        with contextlib.closing(self.urlopen(\'/static/example_static_file.txt\')) as f:\\n            self.assertEqual(f.read().rstrip(b\'\\\\r\\\\n\'), b\'example static file\')\\n\\n    def test_no_collectstatic_emulation(self):\\n        \\"\\"\\"\\n        Test that LiveServerTestCase reports a 404 status code when HTTP client\\n        tries to access a static file that isn\'t explicitly put under\\n        STATIC_ROOT.\\n        \\"\\"\\"\\n        try:\\n            self.urlopen(\'/static/another_app/another_app_static_file.txt\')\\n        except HTTPError as err:\\n            self.assertEqual(err.code, 404, \'Expected 404 response\')\\n        else:\\n            self.fail(\'Expected 404 response (got %d)\' % err.code)\\n\\n    def test_media_files(self):\\n        \\"\\"\\"\\n        Ensure that the LiveServerTestCase serves media files.\\n        Refs #2879.\\n        \\"\\"\\"\\n        with contextlib.closing(self.urlopen(\'/media/example_media_file.txt\')) as f:\\n            self.assertEqual(f.read().rstrip(b\'\\\\r\\\\n\'), b\'example media file\')\\n\\n    def test_environ(self):\\n        with contextlib.closing(self.urlopen(\'/environ_view/?%s\' % urlencode({\'q\': \'\xd1\x82\xd0\xb5\xd1\x81\xd1\x82\'}))) as f:\\n            self.assertIn(b\\"QUERY_STRING: \'q=%D1%82%D0%B5%D1%81%D1%82\'\\", f.read())\\n\\n\\nclass LiveServerDatabase(LiveServerBase):\\n\\n    def test_fixtures_loaded(self):\\n        \\"\\"\\"\\n        Ensure that fixtures are properly loaded and visible to the\\n        live server thread.\\n        Refs #2879.\\n        \\"\\"\\"\\n        with contextlib.closing(self.urlopen(\'/model_view/\')) as f:\\n            self.assertEqual(f.read().splitlines(), [b\'jane\', b\'robert\'])\\n\\n    def test_database_writes(self):\\n        \\"\\"\\"\\n        Ensure that data written to the database by a view can be read.\\n        Refs #2879.\\n        \\"\\"\\"\\n        self.urlopen(\'/create_model_instance/\')\\n        self.assertQuerysetEqual(\\n            Person.objects.all().order_by(\'pk\'),\\n            [\'jane\', \'robert\', \'emily\'],\\n            lambda b: b.name\\n        )\\n\\n\\nclass LiveServerPort(LiveServerBase):\\n\\n    def test_port_bind(self):\\n        \\"\\"\\"\\n        Each LiveServerTestCase binds to a unique port or fails to start a\\n        server thread when run concurrently (#26011).\\n        \\"\\"\\"\\n        TestCase = type(str(\\"TestCase\\"), (LiveServerBase,), {})\\n        try:\\n            TestCase.setUpClass()\\n        except socket.error as e:\\n            if e.errno == errno.EADDRINUSE:\\n                # We\'re out of ports, LiveServerTestCase correctly fails with\\n                # a socket error.\\n                return\\n            # Unexpected error.\\n            raise\\n        try:\\n            # We\'ve acquired a port, ensure our server threads acquired\\n            # different addresses.\\n            self.assertNotEqual(\\n                self.live_server_url, TestCase.live_server_url,\\n                \\"Acquired duplicate server addresses for server threads: %s\\" % self.live_server_url\\n            )\\n        finally:\\n            TestCase.tearDownClass()\\n"}\n'
line: b'{"repo_name":"bdupharm/sqlalchemy","ref":"refs/heads/master","path":"lib/sqlalchemy/ext/declarative/api.py","content":"# ext/declarative/api.py\\n# Copyright (C) 2005-2016 the SQLAlchemy authors and contributors\\n# \\u003csee AUTHORS file\\u003e\\n#\\n# This module is part of SQLAlchemy and is released under\\n# the MIT License: http://www.opensource.org/licenses/mit-license.php\\n\\"\\"\\"Public API functions and helpers for declarative.\\"\\"\\"\\n\\n\\nfrom ...schema import Table, MetaData, Column\\nfrom ...orm import synonym as _orm_synonym, \\\\\\n    comparable_property,\\\\\\n    interfaces, properties, attributes\\nfrom ...orm.util import polymorphic_union\\nfrom ...orm.base import _mapper_or_none\\nfrom ...util import OrderedDict, hybridmethod, hybridproperty\\nfrom ... import util\\nfrom ... import exc\\nimport weakref\\n\\nfrom .base import _as_declarative, \\\\\\n    _declarative_constructor,\\\\\\n    _DeferredMapperConfig, _add_attribute\\nfrom .clsregistry import _class_resolver\\n\\n\\ndef instrument_declarative(cls, registry, metadata):\\n    \\"\\"\\"Given a class, configure the class declaratively,\\n    using the given registry, which can be any dictionary, and\\n    MetaData object.\\n\\n    \\"\\"\\"\\n    if \'_decl_class_registry\' in cls.__dict__:\\n        raise exc.InvalidRequestError(\\n            \\"Class %r already has been \\"\\n            \\"instrumented declaratively\\" % cls)\\n    cls._decl_class_registry = registry\\n    cls.metadata = metadata\\n    _as_declarative(cls, cls.__name__, cls.__dict__)\\n\\n\\ndef has_inherited_table(cls):\\n    \\"\\"\\"Given a class, return True if any of the classes it inherits from has a\\n    mapped table, otherwise return False.\\n    \\"\\"\\"\\n    for class_ in cls.__mro__[1:]:\\n        if getattr(class_, \'__table__\', None) is not None:\\n            return True\\n    return False\\n\\n\\nclass DeclarativeMeta(type):\\n    def __init__(cls, classname, bases, dict_):\\n        if \'_decl_class_registry\' not in cls.__dict__:\\n            _as_declarative(cls, classname, cls.__dict__)\\n        type.__init__(cls, classname, bases, dict_)\\n\\n    def __setattr__(cls, key, value):\\n        _add_attribute(cls, key, value)\\n\\n\\ndef synonym_for(name, map_column=False):\\n    \\"\\"\\"Decorator, make a Python @property a query synonym for a column.\\n\\n    A decorator version of :func:`~sqlalchemy.orm.synonym`. The function being\\n    decorated is the \'descriptor\', otherwise passes its arguments through to\\n    synonym()::\\n\\n      @synonym_for(\'col\')\\n      @property\\n      def prop(self):\\n          return \'special sauce\'\\n\\n    The regular ``synonym()`` is also usable directly in a declarative setting\\n    and may be convenient for read/write properties::\\n\\n      prop = synonym(\'col\', descriptor=property(_read_prop, _write_prop))\\n\\n    \\"\\"\\"\\n    def decorate(fn):\\n        return _orm_synonym(name, map_column=map_column, descriptor=fn)\\n    return decorate\\n\\n\\ndef comparable_using(comparator_factory):\\n    \\"\\"\\"Decorator, allow a Python @property to be used in query criteria.\\n\\n    This is a  decorator front end to\\n    :func:`~sqlalchemy.orm.comparable_property` that passes\\n    through the comparator_factory and the function being decorated::\\n\\n      @comparable_using(MyComparatorType)\\n      @property\\n      def prop(self):\\n          return \'special sauce\'\\n\\n    The regular ``comparable_property()`` is also usable directly in a\\n    declarative setting and may be convenient for read/write properties::\\n\\n      prop = comparable_property(MyComparatorType)\\n\\n    \\"\\"\\"\\n    def decorate(fn):\\n        return comparable_property(comparator_factory, fn)\\n    return decorate\\n\\n\\nclass declared_attr(interfaces._MappedAttribute, property):\\n    \\"\\"\\"Mark a class-level method as representing the definition of\\n    a mapped property or special declarative member name.\\n\\n    @declared_attr turns the attribute into a scalar-like\\n    property that can be invoked from the uninstantiated class.\\n    Declarative treats attributes specifically marked with\\n    @declared_attr as returning a construct that is specific\\n    to mapping or declarative table configuration.  The name\\n    of the attribute is that of what the non-dynamic version\\n    of the attribute would be.\\n\\n    @declared_attr is more often than not applicable to mixins,\\n    to define relationships that are to be applied to different\\n    implementors of the class::\\n\\n        class ProvidesUser(object):\\n            \\"A mixin that adds a \'user\' relationship to classes.\\"\\n\\n            @declared_attr\\n            def user(self):\\n                return relationship(\\"User\\")\\n\\n    It also can be applied to mapped classes, such as to provide\\n    a \\"polymorphic\\" scheme for inheritance::\\n\\n        class Employee(Base):\\n            id = Column(Integer, primary_key=True)\\n            type = Column(String(50), nullable=False)\\n\\n            @declared_attr\\n            def __tablename__(cls):\\n                return cls.__name__.lower()\\n\\n            @declared_attr\\n            def __mapper_args__(cls):\\n                if cls.__name__ == \'Employee\':\\n                    return {\\n                            \\"polymorphic_on\\":cls.type,\\n                            \\"polymorphic_identity\\":\\"Employee\\"\\n                    }\\n                else:\\n                    return {\\"polymorphic_identity\\":cls.__name__}\\n\\n    .. versionchanged:: 0.8 :class:`.declared_attr` can be used with\\n       non-ORM or extension attributes, such as user-defined attributes\\n       or :func:`.association_proxy` objects, which will be assigned\\n       to the class at class construction time.\\n\\n\\n    \\"\\"\\"\\n\\n    def __init__(self, fget, cascading=False):\\n        super(declared_attr, self).__init__(fget)\\n        self.__doc__ = fget.__doc__\\n        self._cascading = cascading\\n\\n    def __get__(desc, self, cls):\\n        reg = cls.__dict__.get(\'_sa_declared_attr_reg\', None)\\n        if reg is None:\\n            manager = attributes.manager_of_class(cls)\\n            if manager is None:\\n                util.warn(\\n                    \\"Unmanaged access of declarative attribute %s from \\"\\n                    \\"non-mapped class %s\\" %\\n                    (desc.fget.__name__, cls.__name__))\\n            return desc.fget(cls)\\n\\n        if reg is None:\\n            return desc.fget(cls)\\n        elif desc in reg:\\n            return reg[desc]\\n        else:\\n            reg[desc] = obj = desc.fget(cls)\\n            return obj\\n\\n    @hybridmethod\\n    def _stateful(cls, **kw):\\n        return _stateful_declared_attr(**kw)\\n\\n    @hybridproperty\\n    def cascading(cls):\\n        \\"\\"\\"Mark a :class:`.declared_attr` as cascading.\\n\\n        This is a special-use modifier which indicates that a column\\n        or MapperProperty-based declared attribute should be configured\\n        distinctly per mapped subclass, within a mapped-inheritance scenario.\\n\\n        Below, both MyClass as well as MySubClass will have a distinct\\n        ``id`` Column object established::\\n\\n            class HasSomeAttribute(object):\\n                @declared_attr.cascading\\n                def some_id(cls):\\n                    if has_inherited_table(cls):\\n                        return Column(\\n                            ForeignKey(\'myclass.id\'), primary_key=True)\\n                    else:\\n                        return Column(Integer, primary_key=True)\\n\\n                    return Column(\'id\', Integer, primary_key=True)\\n\\n            class MyClass(HasSomeAttribute, Base):\\n                \\"\\"\\n                # ...\\n\\n            class MySubClass(MyClass):\\n                \\"\\"\\n                # ...\\n\\n        The behavior of the above configuration is that ``MySubClass``\\n        will refer to both its own ``id`` column as well as that of\\n        ``MyClass`` underneath the attribute named ``some_id``.\\n\\n        .. seealso::\\n\\n            :ref:`declarative_inheritance`\\n\\n            :ref:`mixin_inheritance_columns`\\n\\n\\n        \\"\\"\\"\\n        return cls._stateful(cascading=True)\\n\\n\\nclass _stateful_declared_attr(declared_attr):\\n    def __init__(self, **kw):\\n        self.kw = kw\\n\\n    def _stateful(self, **kw):\\n        new_kw = self.kw.copy()\\n        new_kw.update(kw)\\n        return _stateful_declared_attr(**new_kw)\\n\\n    def __call__(self, fn):\\n        return declared_attr(fn, **self.kw)\\n\\n\\ndef declarative_base(bind=None, metadata=None, mapper=None, cls=object,\\n                     name=\'Base\', constructor=_declarative_constructor,\\n                     class_registry=None,\\n                     metaclass=DeclarativeMeta):\\n    \\"\\"\\"Construct a base class for declarative class definitions.\\n\\n    The new base class will be given a metaclass that produces\\n    appropriate :class:`~sqlalchemy.schema.Table` objects and makes\\n    the appropriate :func:`~sqlalchemy.orm.mapper` calls based on the\\n    information provided declaratively in the class and any subclasses\\n    of the class.\\n\\n    :param bind: An optional\\n      :class:`~sqlalchemy.engine.Connectable`, will be assigned\\n      the ``bind`` attribute on the :class:`~sqlalchemy.schema.MetaData`\\n      instance.\\n\\n    :param metadata:\\n      An optional :class:`~sqlalchemy.schema.MetaData` instance.  All\\n      :class:`~sqlalchemy.schema.Table` objects implicitly declared by\\n      subclasses of the base will share this MetaData.  A MetaData instance\\n      will be created if none is provided.  The\\n      :class:`~sqlalchemy.schema.MetaData` instance will be available via the\\n      `metadata` attribute of the generated declarative base class.\\n\\n    :param mapper:\\n      An optional callable, defaults to :func:`~sqlalchemy.orm.mapper`. Will\\n      be used to map subclasses to their Tables.\\n\\n    :param cls:\\n      Defaults to :class:`object`. A type to use as the base for the generated\\n      declarative base class. May be a class or tuple of classes.\\n\\n    :param name:\\n      Defaults to ``Base``.  The display name for the generated\\n      class.  Customizing this is not required, but can improve clarity in\\n      tracebacks and debugging.\\n\\n    :param constructor:\\n      Defaults to\\n      :func:`~sqlalchemy.ext.declarative._declarative_constructor`, an\\n      __init__ implementation that assigns \\\\**kwargs for declared\\n      fields and relationships to an instance.  If ``None`` is supplied,\\n      no __init__ will be provided and construction will fall back to\\n      cls.__init__ by way of the normal Python semantics.\\n\\n    :param class_registry: optional dictionary that will serve as the\\n      registry of class names-\\u003e mapped classes when string names\\n      are used to identify classes inside of :func:`.relationship`\\n      and others.  Allows two or more declarative base classes\\n      to share the same registry of class names for simplified\\n      inter-base relationships.\\n\\n    :param metaclass:\\n      Defaults to :class:`.DeclarativeMeta`.  A metaclass or __metaclass__\\n      compatible callable to use as the meta type of the generated\\n      declarative base class.\\n\\n    .. seealso::\\n\\n        :func:`.as_declarative`\\n\\n    \\"\\"\\"\\n    lcl_metadata = metadata or MetaData()\\n    if bind:\\n        lcl_metadata.bind = bind\\n\\n    if class_registry is None:\\n        class_registry = weakref.WeakValueDictionary()\\n\\n    bases = not isinstance(cls, tuple) and (cls,) or cls\\n    class_dict = dict(_decl_class_registry=class_registry,\\n                      metadata=lcl_metadata)\\n\\n    if constructor:\\n        class_dict[\'__init__\'] = constructor\\n    if mapper:\\n        class_dict[\'__mapper_cls__\'] = mapper\\n\\n    return metaclass(name, bases, class_dict)\\n\\n\\ndef as_declarative(**kw):\\n    \\"\\"\\"\\n    Class decorator for :func:`.declarative_base`.\\n\\n    Provides a syntactical shortcut to the ``cls`` argument\\n    sent to :func:`.declarative_base`, allowing the base class\\n    to be converted in-place to a \\"declarative\\" base::\\n\\n        from sqlalchemy.ext.declarative import as_declarative\\n\\n        @as_declarative()\\n        class Base(object):\\n            @declared_attr\\n            def __tablename__(cls):\\n                return cls.__name__.lower()\\n            id = Column(Integer, primary_key=True)\\n\\n        class MyMappedClass(Base):\\n            # ...\\n\\n    All keyword arguments passed to :func:`.as_declarative` are passed\\n    along to :func:`.declarative_base`.\\n\\n    .. versionadded:: 0.8.3\\n\\n    .. seealso::\\n\\n        :func:`.declarative_base`\\n\\n    \\"\\"\\"\\n    def decorate(cls):\\n        kw[\'cls\'] = cls\\n        kw[\'name\'] = cls.__name__\\n        return declarative_base(**kw)\\n\\n    return decorate\\n\\n\\nclass ConcreteBase(object):\\n    \\"\\"\\"A helper class for \'concrete\' declarative mappings.\\n\\n    :class:`.ConcreteBase` will use the :func:`.polymorphic_union`\\n    function automatically, against all tables mapped as a subclass\\n    to this class.   The function is called via the\\n    ``__declare_last__()`` function, which is essentially\\n    a hook for the :meth:`.after_configured` event.\\n\\n    :class:`.ConcreteBase` produces a mapped\\n    table for the class itself.  Compare to :class:`.AbstractConcreteBase`,\\n    which does not.\\n\\n    Example::\\n\\n        from sqlalchemy.ext.declarative import ConcreteBase\\n\\n        class Employee(ConcreteBase, Base):\\n            __tablename__ = \'employee\'\\n            employee_id = Column(Integer, primary_key=True)\\n            name = Column(String(50))\\n            __mapper_args__ = {\\n                            \'polymorphic_identity\':\'employee\',\\n                            \'concrete\':True}\\n\\n        class Manager(Employee):\\n            __tablename__ = \'manager\'\\n            employee_id = Column(Integer, primary_key=True)\\n            name = Column(String(50))\\n            manager_data = Column(String(40))\\n            __mapper_args__ = {\\n                            \'polymorphic_identity\':\'manager\',\\n                            \'concrete\':True}\\n\\n    .. seealso::\\n\\n        :class:`.AbstractConcreteBase`\\n\\n        :ref:`concrete_inheritance`\\n\\n        :ref:`inheritance_concrete_helpers`\\n\\n\\n    \\"\\"\\"\\n\\n    @classmethod\\n    def _create_polymorphic_union(cls, mappers):\\n        return polymorphic_union(OrderedDict(\\n            (mp.polymorphic_identity, mp.local_table)\\n            for mp in mappers\\n        ), \'type\', \'pjoin\')\\n\\n    @classmethod\\n    def __declare_first__(cls):\\n        m = cls.__mapper__\\n        if m.with_polymorphic:\\n            return\\n\\n        mappers = list(m.self_and_descendants)\\n        pjoin = cls._create_polymorphic_union(mappers)\\n        m._set_with_polymorphic((\\"*\\", pjoin))\\n        m._set_polymorphic_on(pjoin.c.type)\\n\\n\\nclass AbstractConcreteBase(ConcreteBase):\\n    \\"\\"\\"A helper class for \'concrete\' declarative mappings.\\n\\n    :class:`.AbstractConcreteBase` will use the :func:`.polymorphic_union`\\n    function automatically, against all tables mapped as a subclass\\n    to this class.   The function is called via the\\n    ``__declare_last__()`` function, which is essentially\\n    a hook for the :meth:`.after_configured` event.\\n\\n    :class:`.AbstractConcreteBase` does produce a mapped class\\n    for the base class, however it is not persisted to any table; it\\n    is instead mapped directly to the \\"polymorphic\\" selectable directly\\n    and is only used for selecting.  Compare to :class:`.ConcreteBase`,\\n    which does create a persisted table for the base class.\\n\\n    Example::\\n\\n        from sqlalchemy.ext.declarative import AbstractConcreteBase\\n\\n        class Employee(AbstractConcreteBase, Base):\\n            pass\\n\\n        class Manager(Employee):\\n            __tablename__ = \'manager\'\\n            employee_id = Column(Integer, primary_key=True)\\n            name = Column(String(50))\\n            manager_data = Column(String(40))\\n\\n            __mapper_args__ = {\\n                \'polymorphic_identity\':\'manager\',\\n                \'concrete\':True}\\n\\n    The abstract base class is handled by declarative in a special way;\\n    at class configuration time, it behaves like a declarative mixin\\n    or an ``__abstract__`` base class.   Once classes are configured\\n    and mappings are produced, it then gets mapped itself, but\\n    after all of its decscendants.  This is a very unique system of mapping\\n    not found in any other SQLAlchemy system.\\n\\n    Using this approach, we can specify columns and properties\\n    that will take place on mapped subclasses, in the way that\\n    we normally do as in :ref:`declarative_mixins`::\\n\\n        class Company(Base):\\n            __tablename__ = \'company\'\\n            id = Column(Integer, primary_key=True)\\n\\n        class Employee(AbstractConcreteBase, Base):\\n            employee_id = Column(Integer, primary_key=True)\\n\\n            @declared_attr\\n            def company_id(cls):\\n                return Column(ForeignKey(\'company.id\'))\\n\\n            @declared_attr\\n            def company(cls):\\n                return relationship(\\"Company\\")\\n\\n        class Manager(Employee):\\n            __tablename__ = \'manager\'\\n\\n            name = Column(String(50))\\n            manager_data = Column(String(40))\\n\\n            __mapper_args__ = {\\n                \'polymorphic_identity\':\'manager\',\\n                \'concrete\':True}\\n\\n    When we make use of our mappings however, both ``Manager`` and\\n    ``Employee`` will have an independently usable ``.company`` attribute::\\n\\n        session.query(Employee).filter(Employee.company.has(id=5))\\n\\n    .. versionchanged:: 1.0.0 - The mechanics of :class:`.AbstractConcreteBase`\\n       have been reworked to support relationships established directly\\n       on the abstract base, without any special configurational steps.\\n\\n    .. seealso::\\n\\n        :class:`.ConcreteBase`\\n\\n        :ref:`concrete_inheritance`\\n\\n        :ref:`inheritance_concrete_helpers`\\n\\n    \\"\\"\\"\\n\\n    __no_table__ = True\\n\\n    @classmethod\\n    def __declare_first__(cls):\\n        cls._sa_decl_prepare_nocascade()\\n\\n    @classmethod\\n    def _sa_decl_prepare_nocascade(cls):\\n        if getattr(cls, \'__mapper__\', None):\\n            return\\n\\n        to_map = _DeferredMapperConfig.config_for_cls(cls)\\n\\n        # can\'t rely on \'self_and_descendants\' here\\n        # since technically an immediate subclass\\n        # might not be mapped, but a subclass\\n        # may be.\\n        mappers = []\\n        stack = list(cls.__subclasses__())\\n        while stack:\\n            klass = stack.pop()\\n            stack.extend(klass.__subclasses__())\\n            mn = _mapper_or_none(klass)\\n            if mn is not None:\\n                mappers.append(mn)\\n        pjoin = cls._create_polymorphic_union(mappers)\\n\\n        # For columns that were declared on the class, these\\n        # are normally ignored with the \\"__no_table__\\" mapping,\\n        # unless they have a different attribute key vs. col name\\n        # and are in the properties argument.\\n        # In that case, ensure we update the properties entry\\n        # to the correct column from the pjoin target table.\\n        declared_cols = set(to_map.declared_columns)\\n        for k, v in list(to_map.properties.items()):\\n            if v in declared_cols:\\n                to_map.properties[k] = pjoin.c[v.key]\\n\\n        to_map.local_table = pjoin\\n\\n        m_args = to_map.mapper_args_fn or dict\\n\\n        def mapper_args():\\n            args = m_args()\\n            args[\'polymorphic_on\'] = pjoin.c.type\\n            return args\\n        to_map.mapper_args_fn = mapper_args\\n\\n        m = to_map.map()\\n\\n        for scls in cls.__subclasses__():\\n            sm = _mapper_or_none(scls)\\n            if sm and sm.concrete and cls in scls.__bases__:\\n                sm._set_concrete_base(m)\\n\\n\\nclass DeferredReflection(object):\\n    \\"\\"\\"A helper class for construction of mappings based on\\n    a deferred reflection step.\\n\\n    Normally, declarative can be used with reflection by\\n    setting a :class:`.Table` object using autoload=True\\n    as the ``__table__`` attribute on a declarative class.\\n    The caveat is that the :class:`.Table` must be fully\\n    reflected, or at the very least have a primary key column,\\n    at the point at which a normal declarative mapping is\\n    constructed, meaning the :class:`.Engine` must be available\\n    at class declaration time.\\n\\n    The :class:`.DeferredReflection` mixin moves the construction\\n    of mappers to be at a later point, after a specific\\n    method is called which first reflects all :class:`.Table`\\n    objects created so far.   Classes can define it as such::\\n\\n        from sqlalchemy.ext.declarative import declarative_base\\n        from sqlalchemy.ext.declarative import DeferredReflection\\n        Base = declarative_base()\\n\\n        class MyClass(DeferredReflection, Base):\\n            __tablename__ = \'mytable\'\\n\\n    Above, ``MyClass`` is not yet mapped.   After a series of\\n    classes have been defined in the above fashion, all tables\\n    can be reflected and mappings created using\\n    :meth:`.prepare`::\\n\\n        engine = create_engine(\\"someengine://...\\")\\n        DeferredReflection.prepare(engine)\\n\\n    The :class:`.DeferredReflection` mixin can be applied to individual\\n    classes, used as the base for the declarative base itself,\\n    or used in a custom abstract class.   Using an abstract base\\n    allows that only a subset of classes to be prepared for a\\n    particular prepare step, which is necessary for applications\\n    that use more than one engine.  For example, if an application\\n    has two engines, you might use two bases, and prepare each\\n    separately, e.g.::\\n\\n        class ReflectedOne(DeferredReflection, Base):\\n            __abstract__ = True\\n\\n        class ReflectedTwo(DeferredReflection, Base):\\n            __abstract__ = True\\n\\n        class MyClass(ReflectedOne):\\n            __tablename__ = \'mytable\'\\n\\n        class MyOtherClass(ReflectedOne):\\n            __tablename__ = \'myothertable\'\\n\\n        class YetAnotherClass(ReflectedTwo):\\n            __tablename__ = \'yetanothertable\'\\n\\n        # ... etc.\\n\\n    Above, the class hierarchies for ``ReflectedOne`` and\\n    ``ReflectedTwo`` can be configured separately::\\n\\n        ReflectedOne.prepare(engine_one)\\n        ReflectedTwo.prepare(engine_two)\\n\\n    .. versionadded:: 0.8\\n\\n    \\"\\"\\"\\n    @classmethod\\n    def prepare(cls, engine):\\n        \\"\\"\\"Reflect all :class:`.Table` objects for all current\\n        :class:`.DeferredReflection` subclasses\\"\\"\\"\\n\\n        to_map = _DeferredMapperConfig.classes_for_base(cls)\\n        for thingy in to_map:\\n            cls._sa_decl_prepare(thingy.local_table, engine)\\n            thingy.map()\\n            mapper = thingy.cls.__mapper__\\n            metadata = mapper.class_.metadata\\n            for rel in mapper._props.values():\\n                if isinstance(rel, properties.RelationshipProperty) and \\\\\\n                        rel.secondary is not None:\\n                    if isinstance(rel.secondary, Table):\\n                        cls._reflect_table(rel.secondary, engine)\\n                    elif isinstance(rel.secondary, _class_resolver):\\n                        rel.secondary._resolvers += (\\n                            cls._sa_deferred_table_resolver(engine, metadata),\\n                        )\\n\\n    @classmethod\\n    def _sa_deferred_table_resolver(cls, engine, metadata):\\n        def _resolve(key):\\n            t1 = Table(key, metadata)\\n            cls._reflect_table(t1, engine)\\n            return t1\\n        return _resolve\\n\\n    @classmethod\\n    def _sa_decl_prepare(cls, local_table, engine):\\n        # autoload Table, which is already\\n        # present in the metadata.  This\\n        # will fill in db-loaded columns\\n        # into the existing Table object.\\n        if local_table is not None:\\n            cls._reflect_table(local_table, engine)\\n\\n    @classmethod\\n    def _reflect_table(cls, table, engine):\\n        Table(table.name,\\n              table.metadata,\\n              extend_existing=True,\\n              autoload_replace=False,\\n              autoload=True,\\n              autoload_with=engine,\\n              schema=table.schema)\\n"}\n'r: process ...
r: tokenizing 2 json files ...
input_path: /home/gcloud/TransCoder/data/test_dataset/r/r.003.json.gz
language: r
output_path: /home/gcloud/TransCoder/data/test_dataset/r/r.003.with_comments.tok
line: b'{ "repo_name": "SurajGupta/r-source", "ref": "refs/heads/master", "path": "src/library/methods/R/ClassUnion.R", "content": "#  File src/library/methods/R/ClassUnion.R\\n#  Part of the R package, https://www.R-project.org\\n#\\n#  Copyright (C) 1995-2012 The R Core Team\\n#\\n#  This program is free software; you can redistribute it and/or modify\\n#  it under the terms of the GNU General Public License as published by\\n#  the Free Software Foundation; either version 2 of the License, or\\n#  (at your option) any later version.\\n#\\n#  This program is distributed in the hope that it will be useful,\\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#  GNU General Public License for more details.\\n#\\n#  A copy of the GNU General Public License is available at\\n#  https://www.R-project.org/Licenses/\\n\\n.InitClassUnion <- function(where) {\\n    setClass(\\"ClassUnionRepresentation\\",  \\"classRepresentation\\",\\n             validity =function(object) {\\n                 if(identical(object@virtual, TRUE) && length(object@slots)==0 &&\\n                    is.null(object@prototype))\\n                     TRUE\\n                 else\\n                     \\"Class must be an empty virtual class with NULL prototype\\"\\n             }, where = where)\\n    ## some classes in methods package are unions--now they can be officially\\n    setClassUnion(\\"OptionalFunction\\", c(\\"function\\", \\"NULL\\"), where)\\n    setClassUnion(\\"PossibleMethod\\", c(\\"function\\", \\"MethodDefinition\\"), where)\\n    clList <- c(\\"ClassUnionRepresentation\\", \\"OptionalFunction\\",\\n                \\"PossibleMethod\\")\\n    assign(\\".SealedClasses\\", c(get(\\".SealedClasses\\", where), clList), where)\\n}\\n\\nsetClassUnion <- function(name, members = character(), where = topenv(parent.frame())) {\\n    if(length(members)>0) {\\n        membersDefined <- sapply(members, isClass, where = as.environment(where))\\n        if(!all(membersDefined))\\n            stop(gettextf(\\"the member classes must be defined: not true of %s\\",\\n                          paste(.dQ(as(members[!membersDefined], \\"character\\")), collapse=\\", \\")), domain = NA)\\n    }\\n    def <- new(\\"ClassUnionRepresentation\\",\\n               makeClassRepresentation(name, package = getPackageName(where), where = where))\\n    prev <- getClassDef(name, where = where)\\n    value <- setClass(name, def, where = where)\\n    failed <- character()\\n    ## the prototype of the union will be from the first non-virtual\\n    ## subclass, except that we prefer NULL if \\"NULL\\" is a subclass\\n    hasNull <- match(\\"NULL\\", members, 0)\\n    if(hasNull)\\n        members <- c(\\"NULL\\", members[-hasNull])\\n    for(what in members) {\\n        if(is(try(setIs(what, name, where = where)), \\"try-error\\")) {\\n            if(!is.character(what))\\n                what <- getClass(what, TRUE, where)@className\\n            failed <- c(failed, what)\\n        }\\n    }\\n    if(length(failed)>0) {\\n        if(is.null(prev))\\n            try(removeClass(name, where = where))\\n        else\\n            try(setClass(name, prev, where = where))\\n        stop(gettextf(\\"unable to create union class:  could not set members %s\\",\\n                      paste(.dQ(failed), collapse=\\", \\")), domain = NA)\\n    }\\n    invisible(value)\\n}\\n\\nisClassUnion <- function(Class) {\\n    ## test the class DEFINITION for representing a union\\n    if(is.character(Class))\\n        Class <- getClass(Class, TRUE) # the real def. or a dummy\\n    extends(class(Class), \\"ClassUnionRepresentation\\")\\n}\\n" }\n'
line: b'{ "repo_name": "SurajGupta/r-source", "ref": "refs/heads/master", "path": "src/library/utils/tests/completion.R", "content": "\\n## test some typical completion attempts\\n\\ntestLine <- function(line, cursor = nchar(line))\\n{\\n    str(utils:::.win32consoleCompletion(line, cursor))\\n}\\n\\ntestLine(\\"\\")\\n\\ntestLine(\\"lib\\")\\ntestLine(\\"data(\\")\\ntestLine(\\"data(US\\")\\ntestLine(\\"data(US\\", 3)\\n\\ntestLine(\\"?INS\\")\\n\\ntestLine(\\"utils::data\\")\\ntestLine(\\"utils:::.show_help_on_topic_\\")\\ntestLine(\\"utils::.show_help_on_topic_\\")\\n\\ntestLine(\\"update(\\")\\n\\ntestLine(\\"version$m\\")\\ntestLine(\\"nchar(version[\\")\\n\\n\\n\\ntestLine(\\"method?coe\\")\\ntestLine(\\"?coe\\")\\ntestLine(\\"?\\\\\\"coerce,AN\\")\\ntestLine(\\"method?\\\\\\"coerce,AN\\")\\n\\n\\n## testLine(\\"\\")\\n## testLine(\\"\\")\\n## testLine(\\"\\")\\n" }\n'
line: b'{ "repo_name": "SurajGupta/r-source", "ref": "refs/heads/master", "path": "src/library/graphics/R/curve.R", "content": "#  File src/library/graphics/R/curve.R\\n#  Part of the R package, https://www.R-project.org\\n#\\n#  Copyright (C) 1995-2012 The R Core Team\\n#\\n#  This program is free software; you can redistribute it and/or modify\\n#  it under the terms of the GNU General Public License as published by\\n#  the Free Software Foundation; either version 2 of the License, or\\n#  (at your option) any later version.\\n#\\n#  This program is distributed in the hope that it will be useful,\\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#  GNU General Public License for more details.\\n#\\n#  A copy of the GNU General Public License is available at\\n#  https://www.R-project.org/Licenses/\\n\\ncurve <- function(expr, from = NULL, to = NULL, n = 101, add = FALSE,\\n                  type = \\"l\\", xname = \\"x\\", xlab = xname,\\n                  ylab = NULL, log = NULL, xlim = NULL, ...)\\n{\\n    sexpr <- substitute(expr)\\n    if (is.name(sexpr)) {\\n        ## beter than parse() !\\n        expr <- call(as.character(sexpr), as.name(xname))\\n    } else {\\n\\tif ( !( (is.call(sexpr) || is.expression(sexpr)) &&\\n              xname %in% all.vars(sexpr) ))\\n\\t    stop(gettextf(\\"\'expr\' must be a function, or a call or an expression containing \'%s\'\\", xname), domain = NA)\\n\\texpr <- sexpr\\n    }\\n    if (dev.cur() == 1L && !identical(add, FALSE)) {\\n        warning(\\"\'add\' will be ignored as there is no existing plot\\")\\n        add <- FALSE\\n    }\\n    addF <- identical(add, FALSE)\\n    if (is.null(ylab)) ylab <- deparse(expr)\\n    if (is.null(from) || is.null(to)) {\\n        xl <- if (!is.null(xlim)) xlim\\n        else if (!addF) {\\n            ## determine xlim of current plot.\\n            pu <- par(\\"usr\\")[1L:2L]\\n            if (par(\\"xaxs\\") == \\"r\\") pu <- extendrange(pu, f = -1/27)\\n            if (par(\\"xlog\\")) 10^pu else pu\\n       } else c(0, 1) # was c(1/27, 26/27) in R < 2.14.0\\n        if (is.null(from)) from <- xl[1L]\\n        if (is.null(to)) to <- xl[2L]\\n    }\\n    lg <- if (length(log)) log else if (!addF && par(\\"xlog\\")) \\"x\\" else \\"\\"\\n    if (length(lg) == 0) lg <- \\"\\"\\n    if (grepl(\\"x\\", lg, fixed = TRUE)) {\\n        if (from <= 0 || to <= 0)\\n            stop(\\"\'from\' and \'to\' must be > 0 with log=\\\\\\"x\\\\\\"\\")\\n        x <- exp(seq.int(log(from), log(to), length.out = n))\\n    } else x <- seq.int(from, to, length.out = n)\\n    ll <- list(x = x); names(ll) <- xname\\n    y <- eval(expr, envir = ll, enclos = parent.frame())\\n    if (length(y) != length(x))\\n        stop(\\"\'expr\' did not evaluate to an object of length \'n\'\\")\\n    if (isTRUE(add))\\n\\tlines(x = x, y = y, type = type, ...)\\n    else\\n        plot(x = x, y = y, type = type, xlab = xlab, ylab = ylab,\\n             xlim = xlim, log = lg, ...)\\n    invisible(list(x = x, y = y))\\n}\\n" }\n'
line: b'{ "repo_name": "SurajGupta/r-source", "ref": "refs/heads/master", "path": "tests/reg-plot-latin1.R", "content": "pdf(file = \\"reg-plot-latin1.pdf\\", encoding = \\"ISOLatin1\\",\\n    width = 7, height = 7, paper = \\"a4r\\", compress = FALSE)\\nlibrary(graphics) # to be sure\\nexample(text)     # has examples that need to he plotted in latin-1\\nq(\\"no\\")\\n" }\n'
line: b'{"repo_name":"tianweizhang/nova","ref":"refs/heads/v0","path":"nova/cmd/network.py","content":"# Copyright 2010 United States Government as represented by the\\n# Administrator of the National Aeronautics and Space Administration.\\n# All Rights Reserved.\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\n\\"\\"\\"Starter script for Nova Network.\\"\\"\\"\\n\\nimport sys\\nimport traceback\\n\\nfrom oslo.config import cfg\\n\\nfrom nova.conductor import rpcapi as conductor_rpcapi\\nfrom nova import config\\nimport nova.db.api\\nfrom nova import exception\\nfrom nova.i18n import _\\nfrom nova import objects\\nfrom nova.objects import base as objects_base\\nfrom nova.openstack.common import log as logging\\nfrom nova.openstack.common.report import guru_meditation_report as gmr\\nfrom nova import service\\nfrom nova import utils\\nfrom nova import version\\n\\nCONF = cfg.CONF\\nCONF.import_opt(\'network_topic\', \'nova.network.rpcapi\')\\nCONF.import_opt(\'use_local\', \'nova.conductor.api\', group=\'conductor\')\\n\\n\\ndef block_db_access():\\n    class NoDB(object):\\n        def __getattr__(self, attr):\\n            return self\\n\\n        def __call__(self, *args, **kwargs):\\n            stacktrace = \\"\\".join(traceback.format_stack())\\n            LOG = logging.getLogger(\'nova.network\')\\n            LOG.error(_(\'No db access allowed in nova-network: %s\'),\\n                      stacktrace)\\n            raise exception.DBNotAllowed(\'nova-network\')\\n\\n    nova.db.api.IMPL = NoDB()\\n\\n\\ndef main():\\n    config.parse_args(sys.argv)\\n    logging.setup(\\"nova\\")\\n    utils.monkey_patch()\\n    objects.register_all()\\n\\n    gmr.TextGuruMeditation.setup_autorun(version)\\n\\n    if not CONF.conductor.use_local:\\n        block_db_access()\\n        objects_base.NovaObject.indirection_api = \\\\\\n            conductor_rpcapi.ConductorAPI()\\n\\n    server = service.Service.create(binary=\'nova-network\',\\n                                    topic=CONF.network_topic,\\n                                    db_allowed=CONF.conductor.use_local)\\n    service.serve(server)\\n    service.wait()\\n"}\n'
line: b'{"repo_name":"apollo13/ansible","ref":"refs/heads/devel","path":"lib/ansible/modules/cloud/amazon/elasticache.py","content":"#!/usr/bin/python\\n#\\n# Copyright (c) 2017 Ansible Project\\n#\\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\\nANSIBLE_METADATA = {\'metadata_version\': \'1.1\',\\n                    \'status\': [\'preview\'],\\n                    \'supported_by\': \'community\'}\\n\\n\\nDOCUMENTATION = \\"\\"\\"\\n---\\nmodule: elasticache\\nshort_description: Manage cache clusters in Amazon Elasticache.\\ndescription:\\n  - Manage cache clusters in Amazon Elasticache.\\n  - Returns information about the specified cache cluster.\\nversion_added: \\"1.4\\"\\nrequirements: [ boto3 ]\\nauthor: \\"Jim Dalton (@jsdalton)\\"\\noptions:\\n  state:\\n    description:\\n      - C(absent) or C(present) are idempotent actions that will create or destroy a cache cluster as needed. C(rebooted) will reboot the cluster,\\n        resulting in a momentary outage.\\n    choices: [\'present\', \'absent\', \'rebooted\']\\n    required: true\\n  name:\\n    description:\\n      - The cache cluster identifier\\n    required: true\\n  engine:\\n    description:\\n      - Name of the cache engine to be used.\\n    default: memcached\\n    choices: [\'redis\', \'memcached\']\\n  cache_engine_version:\\n    description:\\n      - The version number of the cache engine\\n  node_type:\\n    description:\\n      - The compute and memory capacity of the nodes in the cache cluster\\n    default: cache.m1.small\\n  num_nodes:\\n    description:\\n      - The initial number of cache nodes that the cache cluster will have. Required when state=present.\\n  cache_port:\\n    description:\\n      - The port number on which each of the cache nodes will accept connections\\n  cache_parameter_group:\\n    description:\\n      - The name of the cache parameter group to associate with this cache cluster. If this argument is omitted, the default cache parameter group\\n        for the specified engine will be used.\\n    version_added: \\"2.0\\"\\n    aliases: [ \'parameter_group\' ]\\n  cache_subnet_group:\\n    description:\\n      - The subnet group name to associate with. Only use if inside a vpc. Required if inside a vpc\\n    version_added: \\"2.0\\"\\n  security_group_ids:\\n    description:\\n      - A list of vpc security group names to associate with this cache cluster. Only use if inside a vpc\\n    version_added: \\"1.6\\"\\n  cache_security_groups:\\n    description:\\n      - A list of cache security group names to associate with this cache cluster. Must be an empty list if inside a vpc\\n  zone:\\n    description:\\n      - The EC2 Availability Zone in which the cache cluster will be created\\n  wait:\\n    description:\\n      - Wait for cache cluster result before returning\\n    type: bool\\n    default: \'yes\'\\n  hard_modify:\\n    description:\\n      - Whether to destroy and recreate an existing cache cluster if necessary in order to modify its state\\n    type: bool\\n    default: \'no\'\\nextends_documentation_fragment:\\n    - aws\\n    - ec2\\n\\"\\"\\"\\n\\nEXAMPLES = \\"\\"\\"\\n# Note: None of these examples set aws_access_key, aws_secret_key, or region.\\n# It is assumed that their matching environment variables are set.\\n\\n# Basic example\\n- elasticache:\\n    name: \\"test-please-delete\\"\\n    state: present\\n    engine: memcached\\n    cache_engine_version: 1.4.14\\n    node_type: cache.m1.small\\n    num_nodes: 1\\n    cache_port: 11211\\n    cache_security_groups:\\n      - default\\n    zone: us-east-1d\\n\\n\\n# Ensure cache cluster is gone\\n- elasticache:\\n    name: \\"test-please-delete\\"\\n    state: absent\\n\\n# Reboot cache cluster\\n- elasticache:\\n    name: \\"test-please-delete\\"\\n    state: rebooted\\n\\n\\"\\"\\"\\nfrom time import sleep\\nfrom traceback import format_exc\\nfrom ansible.module_utils.basic import AnsibleModule\\nfrom ansible.module_utils.ec2 import ec2_argument_spec, get_aws_connection_info, boto3_conn, HAS_BOTO3, camel_dict_to_snake_dict\\n\\ntry:\\n    import boto3\\n    import botocore\\nexcept ImportError:\\n    pass  # will be detected by imported HAS_BOTO3\\n\\n\\nclass ElastiCacheManager(object):\\n\\n    \\"\\"\\"Handles elasticache creation and destruction\\"\\"\\"\\n\\n    EXIST_STATUSES = [\'available\', \'creating\', \'rebooting\', \'modifying\']\\n\\n    def __init__(self, module, name, engine, cache_engine_version, node_type,\\n                 num_nodes, cache_port, cache_parameter_group, cache_subnet_group,\\n                 cache_security_groups, security_group_ids, zone, wait,\\n                 hard_modify, region, **aws_connect_kwargs):\\n        self.module = module\\n        self.name = name\\n        self.engine = engine.lower()\\n        self.cache_engine_version = cache_engine_version\\n        self.node_type = node_type\\n        self.num_nodes = num_nodes\\n        self.cache_port = cache_port\\n        self.cache_parameter_group = cache_parameter_group\\n        self.cache_subnet_group = cache_subnet_group\\n        self.cache_security_groups = cache_security_groups\\n        self.security_group_ids = security_group_ids\\n        self.zone = zone\\n        self.wait = wait\\n        self.hard_modify = hard_modify\\n\\n        self.region = region\\n        self.aws_connect_kwargs = aws_connect_kwargs\\n\\n        self.changed = False\\n        self.data = None\\n        self.status = \'gone\'\\n        self.conn = self._get_elasticache_connection()\\n        self._refresh_data()\\n\\n    def ensure_present(self):\\n        \\"\\"\\"Ensure cache cluster exists or create it if not\\"\\"\\"\\n        if self.exists():\\n            self.sync()\\n        else:\\n            self.create()\\n\\n    def ensure_absent(self):\\n        \\"\\"\\"Ensure cache cluster is gone or delete it if not\\"\\"\\"\\n        self.delete()\\n\\n    def ensure_rebooted(self):\\n        \\"\\"\\"Ensure cache cluster is gone or delete it if not\\"\\"\\"\\n        self.reboot()\\n\\n    def exists(self):\\n        \\"\\"\\"Check if cache cluster exists\\"\\"\\"\\n        return self.status in self.EXIST_STATUSES\\n\\n    def create(self):\\n        \\"\\"\\"Create an ElastiCache cluster\\"\\"\\"\\n        if self.status == \'available\':\\n            return\\n        if self.status in [\'creating\', \'rebooting\', \'modifying\']:\\n            if self.wait:\\n                self._wait_for_status(\'available\')\\n            return\\n        if self.status == \'deleting\':\\n            if self.wait:\\n                self._wait_for_status(\'gone\')\\n            else:\\n                msg = \\"\'%s\' is currently deleting. Cannot create.\\"\\n                self.module.fail_json(msg=msg % self.name)\\n\\n        kwargs = dict(CacheClusterId=self.name,\\n                      NumCacheNodes=self.num_nodes,\\n                      CacheNodeType=self.node_type,\\n                      Engine=self.engine,\\n                      EngineVersion=self.cache_engine_version,\\n                      CacheSecurityGroupNames=self.cache_security_groups,\\n                      SecurityGroupIds=self.security_group_ids,\\n                      CacheParameterGroupName=self.cache_parameter_group,\\n                      CacheSubnetGroupName=self.cache_subnet_group)\\n        if self.cache_port is not None:\\n            kwargs[\'Port\'] = self.cache_port\\n        if self.zone is not None:\\n            kwargs[\'PreferredAvailabilityZone\'] = self.zone\\n\\n        try:\\n            self.conn.create_cache_cluster(**kwargs)\\n\\n        except botocore.exceptions.ClientError as e:\\n            self.module.fail_json(msg=e.message, exception=format_exc(),\\n                                  **camel_dict_to_snake_dict(e.response))\\n\\n        self._refresh_data()\\n\\n        self.changed = True\\n        if self.wait:\\n            self._wait_for_status(\'available\')\\n        return True\\n\\n    def delete(self):\\n        \\"\\"\\"Destroy an ElastiCache cluster\\"\\"\\"\\n        if self.status == \'gone\':\\n            return\\n        if self.status == \'deleting\':\\n            if self.wait:\\n                self._wait_for_status(\'gone\')\\n            return\\n        if self.status in [\'creating\', \'rebooting\', \'modifying\']:\\n            if self.wait:\\n                self._wait_for_status(\'available\')\\n            else:\\n                msg = \\"\'%s\' is currently %s. Cannot delete.\\"\\n                self.module.fail_json(msg=msg % (self.name, self.status))\\n\\n        try:\\n            response = self.conn.delete_cache_cluster(CacheClusterId=self.name)\\n        except botocore.exceptions.ClientError as e:\\n            self.module.fail_json(msg=e.message, exception=format_exc(),\\n                                  **camel_dict_to_snake_dict(e.response))\\n\\n        cache_cluster_data = response[\'CacheCluster\']\\n        self._refresh_data(cache_cluster_data)\\n\\n        self.changed = True\\n        if self.wait:\\n            self._wait_for_status(\'gone\')\\n\\n    def sync(self):\\n        \\"\\"\\"Sync settings to cluster if required\\"\\"\\"\\n        if not self.exists():\\n            msg = \\"\'%s\' is %s. Cannot sync.\\"\\n            self.module.fail_json(msg=msg % (self.name, self.status))\\n\\n        if self.status in [\'creating\', \'rebooting\', \'modifying\']:\\n            if self.wait:\\n                self._wait_for_status(\'available\')\\n            else:\\n                # Cluster can only be synced if available. If we can\'t wait\\n                # for this, then just be done.\\n                return\\n\\n        if self._requires_destroy_and_create():\\n            if not self.hard_modify:\\n                msg = \\"\'%s\' requires destructive modification. \'hard_modify\' must be set to true to proceed.\\"\\n                self.module.fail_json(msg=msg % self.name)\\n            if not self.wait:\\n                msg = \\"\'%s\' requires destructive modification. \'wait\' must be set to true.\\"\\n                self.module.fail_json(msg=msg % self.name)\\n            self.delete()\\n            self.create()\\n            return\\n\\n        if self._requires_modification():\\n            self.modify()\\n\\n    def modify(self):\\n        \\"\\"\\"Modify the cache cluster. Note it\'s only possible to modify a few select options.\\"\\"\\"\\n        nodes_to_remove = self._get_nodes_to_remove()\\n        try:\\n            self.conn.modify_cache_cluster(CacheClusterId=self.name,\\n                                           NumCacheNodes=self.num_nodes,\\n                                           CacheNodeIdsToRemove=nodes_to_remove,\\n                                           CacheSecurityGroupNames=self.cache_security_groups,\\n                                           CacheParameterGroupName=self.cache_parameter_group,\\n                                           SecurityGroupIds=self.security_group_ids,\\n                                           ApplyImmediately=True,\\n                                           EngineVersion=self.cache_engine_version)\\n        except botocore.exceptions.ClientError as e:\\n            self.module.fail_json(msg=e.message, exception=format_exc(),\\n                                  **camel_dict_to_snake_dict(e.response))\\n\\n        self._refresh_data()\\n\\n        self.changed = True\\n        if self.wait:\\n            self._wait_for_status(\'available\')\\n\\n    def reboot(self):\\n        \\"\\"\\"Reboot the cache cluster\\"\\"\\"\\n        if not self.exists():\\n            msg = \\"\'%s\' is %s. Cannot reboot.\\"\\n            self.module.fail_json(msg=msg % (self.name, self.status))\\n        if self.status == \'rebooting\':\\n            return\\n        if self.status in [\'creating\', \'modifying\']:\\n            if self.wait:\\n                self._wait_for_status(\'available\')\\n            else:\\n                msg = \\"\'%s\' is currently %s. Cannot reboot.\\"\\n                self.module.fail_json(msg=msg % (self.name, self.status))\\n\\n        # Collect ALL nodes for reboot\\n        cache_node_ids = [cn[\'CacheNodeId\'] for cn in self.data[\'CacheNodes\']]\\n        try:\\n            self.conn.reboot_cache_cluster(CacheClusterId=self.name,\\n                                           CacheNodeIdsToReboot=cache_node_ids)\\n        except botocore.exceptions.ClientError as e:\\n            self.module.fail_json(msg=e.message, exception=format_exc(),\\n                                  **camel_dict_to_snake_dict(e.response))\\n\\n        self._refresh_data()\\n\\n        self.changed = True\\n        if self.wait:\\n            self._wait_for_status(\'available\')\\n\\n    def get_info(self):\\n        \\"\\"\\"Return basic info about the cache cluster\\"\\"\\"\\n        info = {\\n            \'name\': self.name,\\n            \'status\': self.status\\n        }\\n        if self.data:\\n            info[\'data\'] = self.data\\n        return info\\n\\n    def _wait_for_status(self, awaited_status):\\n        \\"\\"\\"Wait for status to change from present status to awaited_status\\"\\"\\"\\n        status_map = {\\n            \'creating\': \'available\',\\n            \'rebooting\': \'available\',\\n            \'modifying\': \'available\',\\n            \'deleting\': \'gone\'\\n        }\\n        if self.status == awaited_status:\\n            # No need to wait, we\'re already done\\n            return\\n        if status_map[self.status] != awaited_status:\\n            msg = \\"Invalid awaited status. \'%s\' cannot transition to \'%s\'\\"\\n            self.module.fail_json(msg=msg % (self.status, awaited_status))\\n\\n        if awaited_status not in set(status_map.values()):\\n            msg = \\"\'%s\' is not a valid awaited status.\\"\\n            self.module.fail_json(msg=msg % awaited_status)\\n\\n        while True:\\n            sleep(1)\\n            self._refresh_data()\\n            if self.status == awaited_status:\\n                break\\n\\n    def _requires_modification(self):\\n        \\"\\"\\"Check if cluster requires (nondestructive) modification\\"\\"\\"\\n        # Check modifiable data attributes\\n        modifiable_data = {\\n            \'NumCacheNodes\': self.num_nodes,\\n            \'EngineVersion\': self.cache_engine_version\\n        }\\n        for key, value in modifiable_data.items():\\n            if value is not None and value and self.data[key] != value:\\n                return True\\n\\n        # Check cache security groups\\n        cache_security_groups = []\\n        for sg in self.data[\'CacheSecurityGroups\']:\\n            cache_security_groups.append(sg[\'CacheSecurityGroupName\'])\\n        if set(cache_security_groups) != set(self.cache_security_groups):\\n            return True\\n\\n        # check vpc security groups\\n        if self.security_group_ids:\\n            vpc_security_groups = []\\n            security_groups = self.data[\'SecurityGroups\'] or []\\n            for sg in security_groups:\\n                vpc_security_groups.append(sg[\'SecurityGroupId\'])\\n            if set(vpc_security_groups) != set(self.security_group_ids):\\n                return True\\n\\n        return False\\n\\n    def _requires_destroy_and_create(self):\\n        \\"\\"\\"\\n        Check whether a destroy and create is required to synchronize cluster.\\n        \\"\\"\\"\\n        unmodifiable_data = {\\n            \'node_type\': self.data[\'CacheNodeType\'],\\n            \'engine\': self.data[\'Engine\'],\\n            \'cache_port\': self._get_port()\\n        }\\n        # Only check for modifications if zone is specified\\n        if self.zone is not None:\\n            unmodifiable_data[\'zone\'] = self.data[\'PreferredAvailabilityZone\']\\n        for key, value in unmodifiable_data.items():\\n            if getattr(self, key) is not None and getattr(self, key) != value:\\n                return True\\n        return False\\n\\n    def _get_elasticache_connection(self):\\n        \\"\\"\\"Get an elasticache connection\\"\\"\\"\\n        region, ec2_url, aws_connect_params = get_aws_connection_info(self.module, boto3=True)\\n        if region:\\n            return boto3_conn(self.module, conn_type=\'client\', resource=\'elasticache\',\\n                              region=region, endpoint=ec2_url, **aws_connect_params)\\n        else:\\n            self.module.fail_json(msg=\\"region must be specified\\")\\n\\n    def _get_port(self):\\n        \\"\\"\\"Get the port. Where this information is retrieved from is engine dependent.\\"\\"\\"\\n        if self.data[\'Engine\'] == \'memcached\':\\n            return self.data[\'ConfigurationEndpoint\'][\'Port\']\\n        elif self.data[\'Engine\'] == \'redis\':\\n            # Redis only supports a single node (presently) so just use\\n            # the first and only\\n            return self.data[\'CacheNodes\'][0][\'Endpoint\'][\'Port\']\\n\\n    def _refresh_data(self, cache_cluster_data=None):\\n        \\"\\"\\"Refresh data about this cache cluster\\"\\"\\"\\n\\n        if cache_cluster_data is None:\\n            try:\\n                response = self.conn.describe_cache_clusters(CacheClusterId=self.name, ShowCacheNodeInfo=True)\\n            except botocore.exceptions.ClientError as e:\\n                if e.response[\'Error\'][\'Code\'] == \'CacheClusterNotFound\':\\n                    self.data = None\\n                    self.status = \'gone\'\\n                    return\\n                else:\\n                    self.module.fail_json(msg=e.message, exception=format_exc(),\\n                                          **camel_dict_to_snake_dict(e.response))\\n            cache_cluster_data = response[\'CacheClusters\'][0]\\n        self.data = cache_cluster_data\\n        self.status = self.data[\'CacheClusterStatus\']\\n\\n        # The documentation for elasticache lies -- status on rebooting is set\\n        # to \'rebooting cache cluster nodes\' instead of \'rebooting\'. Fix it\\n        # here to make status checks etc. more sane.\\n        if self.status == \'rebooting cache cluster nodes\':\\n            self.status = \'rebooting\'\\n\\n    def _get_nodes_to_remove(self):\\n        \\"\\"\\"If there are nodes to remove, it figures out which need to be removed\\"\\"\\"\\n        num_nodes_to_remove = self.data[\'NumCacheNodes\'] - self.num_nodes\\n        if num_nodes_to_remove \\u003c= 0:\\n            return []\\n\\n        if not self.hard_modify:\\n            msg = \\"\'%s\' requires removal of cache nodes. \'hard_modify\' must be set to true to proceed.\\"\\n            self.module.fail_json(msg=msg % self.name)\\n\\n        cache_node_ids = [cn[\'CacheNodeId\'] for cn in self.data[\'CacheNodes\']]\\n        return cache_node_ids[-num_nodes_to_remove:]\\n\\n\\ndef main():\\n    \\"\\"\\" elasticache ansible module \\"\\"\\"\\n    argument_spec = ec2_argument_spec()\\n    argument_spec.update(dict(\\n        state=dict(required=True, choices=[\'present\', \'absent\', \'rebooted\']),\\n        name=dict(required=True),\\n        engine=dict(default=\'memcached\'),\\n        cache_engine_version=dict(default=\\"\\"),\\n        node_type=dict(default=\'cache.t2.small\'),\\n        num_nodes=dict(default=1, type=\'int\'),\\n        # alias for compat with the original PR 1950\\n        cache_parameter_group=dict(default=\\"\\", aliases=[\'parameter_group\']),\\n        cache_port=dict(type=\'int\'),\\n        cache_subnet_group=dict(default=\\"\\"),\\n        cache_security_groups=dict(default=[], type=\'list\'),\\n        security_group_ids=dict(default=[], type=\'list\'),\\n        zone=dict(),\\n        wait=dict(default=True, type=\'bool\'),\\n        hard_modify=dict(type=\'bool\')\\n    ))\\n\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n    )\\n\\n    if not HAS_BOTO3:\\n        module.fail_json(msg=\'boto3 required for this module\')\\n\\n    region, ec2_url, aws_connect_kwargs = get_aws_connection_info(module)\\n\\n    name = module.params[\'name\']\\n    state = module.params[\'state\']\\n    engine = module.params[\'engine\']\\n    cache_engine_version = module.params[\'cache_engine_version\']\\n    node_type = module.params[\'node_type\']\\n    num_nodes = module.params[\'num_nodes\']\\n    cache_port = module.params[\'cache_port\']\\n    cache_subnet_group = module.params[\'cache_subnet_group\']\\n    cache_security_groups = module.params[\'cache_security_groups\']\\n    security_group_ids = module.params[\'security_group_ids\']\\n    zone = module.params[\'zone\']\\n    wait = module.params[\'wait\']\\n    hard_modify = module.params[\'hard_modify\']\\n    cache_parameter_group = module.params[\'cache_parameter_group\']\\n\\n    if cache_subnet_group and cache_security_groups:\\n        module.fail_json(msg=\\"Can\'t specify both cache_subnet_group and cache_security_groups\\")\\n\\n    if state == \'present\' and not num_nodes:\\n        module.fail_json(msg=\\"\'num_nodes\' is a required parameter. Please specify num_nodes \\u003e 0\\")\\n\\n    elasticache_manager = ElastiCacheManager(module, name, engine,\\n                                             cache_engine_version, node_type,\\n                                             num_nodes, cache_port,\\n                                             cache_parameter_group,\\n                                             cache_subnet_group,\\n                                             cache_security_groups,\\n                                             security_group_ids, zone, wait,\\n                                             hard_modify, region, **aws_connect_kwargs)\\n\\n    if state == \'present\':\\n        elasticache_manager.ensure_present()\\n    elif state == \'absent\':\\n        elasticache_manager.ensure_absent()\\n    elif state == \'rebooted\':\\n        elasticache_manager.ensure_rebooted()\\n\\n    facts_result = dict(changed=elasticache_manager.changed,\\n                        elasticache=elasticache_manager.get_info())\\n\\n    module.exit_json(**facts_result)\\n\\n\\nif __name__ == \'__main__\':\\n    main()\\n"}\n'
line: b'{ "repo_name": "SurajGupta/r-source", "ref": "refs/heads/master", "path": "src/library/base/R/lapply.R", "content": "#  File src/library/base/R/lapply.R\\n#  Part of the R package, https://www.R-project.org\\n#\\n#  Copyright (C) 1995-2012 The R Core Team\\n#\\n#  This program is free software; you can redistribute it and/or modify\\n#  it under the terms of the GNU General Public License as published by\\n#  the Free Software Foundation; either version 2 of the License, or\\n#  (at your option) any later version.\\n#\\n#  This program is distributed in the hope that it will be useful,\\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#  GNU General Public License for more details.\\n#\\n#  A copy of the GNU General Public License is available at\\n#  https://www.R-project.org/Licenses/\\n\\nlapply <- function (X, FUN, ...)\\n{\\n    FUN <- match.fun(FUN)\\n    ## internal code handles all vector types, including expressions\\n    ## However, it would be OK to have attributes which is.vector\\n    ## disallows.\\n    if(!is.vector(X) || is.object(X)) X <- as.list(X)\\n    ## Note ... is not passed down.  Rather the internal code\\n    ## evaluates FUN(X[i], ...) in the frame of this function\\n    .Internal(lapply(X, FUN))\\n}\\n\\nrapply <-\\n    function(object, f, classes = \\"ANY\\", deflt = NULL,\\n             how = c(\\"unlist\\", \\"replace\\", \\"list\\"), ...)\\n{\\n    if(typeof(object) != \\"list\\")\\n        stop(\\"\'object\' must be a list\\")\\n    how <- match.arg(how)\\n    res <- .Internal(rapply(object, f, classes, deflt, how))\\n    if(how == \\"unlist\\") unlist(res, recursive = TRUE) else res\\n}\\n" }\n'
line: b'{ "repo_name": "SurajGupta/r-source", "ref": "refs/heads/master", "path": "src/library/stats/R/mcnemar.test.R", "content": "#  File src/library/stats/R/mcnemar.test.R\\n#  Part of the R package, https://www.R-project.org\\n#\\n#  Copyright (C) 1995-2013 The R Core Team\\n#\\n#  This program is free software; you can redistribute it and/or modify\\n#  it under the terms of the GNU General Public License as published by\\n#  the Free Software Foundation; either version 2 of the License, or\\n#  (at your option) any later version.\\n#\\n#  This program is distributed in the hope that it will be useful,\\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#  GNU General Public License for more details.\\n#\\n#  A copy of the GNU General Public License is available at\\n#  https://www.R-project.org/Licenses/\\n\\nmcnemar.test <- function(x, y = NULL, correct = TRUE)\\n{\\n    if (is.matrix(x)) {\\n        r <- nrow(x)\\n        if ((r < 2) || (ncol (x) != r))\\n            stop(\\"\'x\' must be square with at least two rows and columns\\")\\n        if (any(x < 0) || anyNA(x))\\n            stop(\\"all entries of \'x\' must be nonnegative and finite\\")\\n        DNAME <- deparse(substitute(x))\\n    }\\n    else {\\n        if (is.null(y))\\n            stop(\\"if \'x\' is not a matrix, \'y\' must be given\\")\\n        if (length(x) != length(y))\\n            stop(\\"\'x\' and \'y\' must have the same length\\")\\n        DNAME <- paste(deparse(substitute(x)), \\"and\\",\\n                       deparse(substitute(y)))\\n        OK <- complete.cases(x, y)\\n        x <- as.factor(x[OK])\\n        y <- as.factor(y[OK])\\n        r <- nlevels(x)\\n        if ((r < 2) || (nlevels(y) != r))\\n            stop(\\"\'x\' and \'y\' must have the same number of levels (minimum 2)\\")\\n        x <- table(x, y)\\n    }\\n\\n    PARAMETER <- r * (r-1) / 2\\n    METHOD <- \\"McNemar\'s Chi-squared test\\"\\n\\n    if (correct && (r == 2) && any(x - t(x) != 0)) {\\n        y <- (abs(x - t(x)) - 1)\\n        METHOD <- paste(METHOD, \\"with continuity correction\\")\\n    }\\n    else\\n        y <- x - t(x)\\n    x <- x + t(x)\\n\\n    STATISTIC <- sum(y[upper.tri(x)]^2 / x[upper.tri(x)])\\n    PVAL <- pchisq(STATISTIC, PARAMETER, lower.tail = FALSE)\\n    names(STATISTIC) <- \\"McNemar\'s chi-squared\\"\\n    names(PARAMETER) <- \\"df\\"\\n\\n    RVAL <- list(statistic = STATISTIC,\\n                 parameter = PARAMETER,\\n                 p.value = PVAL,\\n                 method = METHOD,\\n                 data.name = DNAME)\\n    class(RVAL) <- \\"htest\\"\\n    return(RVAL)\\n}\\n" }\n'
line: b'{ "repo_name": "SurajGupta/r-source", "ref": "refs/heads/master", "path": "src/library/base/R/outer.R", "content": "#  File src/library/base/R/outer.R\\n#  Part of the R package, https://www.R-project.org\\n#\\n#  Copyright (C) 1995-2013 The R Core Team\\n#\\n#  This program is free software; you can redistribute it and/or modify\\n#  it under the terms of the GNU General Public License as published by\\n#  the Free Software Foundation; either version 2 of the License, or\\n#  (at your option) any later version.\\n#\\n#  This program is distributed in the hope that it will be useful,\\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#  GNU General Public License for more details.\\n#\\n#  A copy of the GNU General Public License is available at\\n#  https://www.R-project.org/Licenses/\\n\\nouter <- function (X, Y, FUN = \\"*\\", ...)\\n{\\n    if(is.array(X)) {\\n        dX <- dim(X)\\n        nx <- dimnames(X)\\n        no.nx <- is.null(nx)\\n    } else { # a vector\\n        dX <- length(X)  # cannot be long, as form a matrix below\\n        no.nx <- is.null(names(X))\\n        if(!no.nx) nx <- list(names(X))\\n    }\\n    if(is.array(Y)) {\\n        dY <- dim(Y)\\n        ny <- dimnames(Y)\\n        no.ny <- is.null(ny)\\n    } else { # a vector\\n        dY <- length(Y)\\n        no.ny <- is.null(names(Y))\\n        if(!no.ny) ny <- list(names(Y))\\n    }\\n    if (is.character(FUN) && FUN==\\"*\\") {\\n        if(!missing(...)) stop(\'using ... with FUN = \\"*\\" is an error\')\\n        # this is for numeric vectors, so dropping attributes is OK\\n        robj <- as.vector(X) %*% t(as.vector(Y))\\n        dim(robj) <- c(dX, dY)\\n    } else {\\n        FUN <- match.fun(FUN)\\n        ## Y may have a class, so don\'t use rep.int\\n        Y <- rep(Y, rep.int(length(X), length(Y)))\\n        ##  length.out is not an argument of the generic rep()\\n        ##  X <- rep(X, length.out = length(Y))\\n        if(length(X))\\n            X <- rep(X, times = ceiling(length(Y)/length(X)))\\n        robj <- FUN(X, Y, ...)\\n        dim(robj) <- c(dX, dY) # careful not to lose class here\\n    }\\n    ## no dimnames if both don\'t have ..\\n    if(!(no.nx && no.ny)) {\\n\\tif(no.nx) nx <- vector(\\"list\\", length(dX)) else\\n\\tif(no.ny) ny <- vector(\\"list\\", length(dY))\\n\\tdimnames(robj) <- c(nx, ny)\\n    }\\n    robj\\n}\\n\\n## Binary operator, hence don\'t simply do \\"%o%\\" <- outer.\\n`%o%` <- function(X, Y) outer(X, Y)\\n" }\n'
line: b'{ "repo_name": "SurajGupta/r-source", "ref": "refs/heads/master", "path": "src/library/stats/R/p.adjust.R", "content": "#  File src/library/stats/R/p.adjust.R\\n#  Part of the R package, https://www.R-project.org\\n#\\n#  Copyright (C) 1995-2012 The R Core Team\\n#\\n#  This program is free software; you can redistribute it and/or modify\\n#  it under the terms of the GNU General Public License as published by\\n#  the Free Software Foundation; either version 2 of the License, or\\n#  (at your option) any later version.\\n#\\n#  This program is distributed in the hope that it will be useful,\\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#  GNU General Public License for more details.\\n#\\n#  A copy of the GNU General Public License is available at\\n#  https://www.R-project.org/Licenses/\\n\\np.adjust.methods <-\\n    c(\\"holm\\", \\"hochberg\\", \\"hommel\\", \\"bonferroni\\", \\"BH\\", \\"BY\\", \\"fdr\\", \\"none\\")\\n\\np.adjust <- function(p, method = p.adjust.methods, n = length(p))\\n{\\n    ## Methods \'Hommel\', \'BH\', \'BY\' and speed improvements\\n    ## contributed by Gordon Smyth\\n    method <- match.arg(method)\\n    if(method == \\"fdr\\") method <- \\"BH\\"\\t# back compatibility\\n    nm <- names(p)\\n    p <- as.numeric(p)\\n    p0 <- setNames(p, nm)\\n    if(all(nna <- !is.na(p))) nna <- TRUE\\n    p <- p[nna]\\n    lp <- length(p)\\n    stopifnot(n >= lp)\\n    if (n <= 1) return(p0)\\n    if (n == 2 && method == \\"hommel\\") method <- \\"hochberg\\"\\n\\n    p0[nna] <-\\n\\tswitch(method,\\n\\t       bonferroni = pmin(1, n * p),\\n\\t       holm = {\\n\\t\\t   i <- seq_len(lp)\\n\\t\\t   o <- order(p)\\n\\t\\t   ro <- order(o)\\n\\t\\t   pmin(1, cummax( (n - i + 1L) * p[o] ))[ro]\\n\\t       },\\n\\t       hommel = { ## needs n-1 >= 2 in for() below\\n\\t\\t   if(n > lp) p <- c(p, rep.int(1, n-lp))\\n\\t\\t   i <- seq_len(n)\\n\\t\\t   o <- order(p)\\n\\t\\t   p <- p[o]\\n\\t\\t   ro <- order(o)\\n\\t\\t   q <- pa <- rep.int( min(n*p/i), n)\\n\\t\\t   for (j in (n-1):2) {\\n\\t\\t       ij <- seq_len(n-j+1)\\n\\t\\t       i2 <- (n-j+2):n\\n\\t\\t       q1 <- min(j*p[i2]/(2:j))\\n\\t\\t       q[ij] <- pmin(j*p[ij], q1)\\n\\t\\t       q[i2] <- q[n-j+1]\\n\\t\\t       pa <- pmax(pa,q)\\n\\t\\t   }\\n\\t\\t   pmax(pa,p)[if(lp < n) ro[1:lp] else ro]\\n\\t       },\\n\\t       hochberg = {\\n\\t\\t   i <- lp:1L\\n\\t\\t   o <- order(p, decreasing = TRUE)\\n\\t\\t   ro <- order(o)\\n\\t\\t   pmin(1, cummin( (n - i + 1L) * p[o] ))[ro]\\n\\t       },\\n\\t       BH = {\\n\\t\\t   i <- lp:1L\\n\\t\\t   o <- order(p, decreasing = TRUE)\\n\\t\\t   ro <- order(o)\\n\\t\\t   pmin(1, cummin( n / i * p[o] ))[ro]\\n\\t       },\\n\\t       BY = {\\n\\t\\t   i <- lp:1L\\n\\t\\t   o <- order(p, decreasing = TRUE)\\n\\t\\t   ro <- order(o)\\n\\t\\t   q <- sum(1L/(1L:n))\\n\\t\\t   pmin(1, cummin(q * n / i * p[o]))[ro]\\n\\t       },\\n\\t       none = p)\\n    p0\\n}\\n" }\n'
line: b'{ "repo_name": "SurajGupta/r-source", "ref": "refs/heads/master", "path": "src/library/base/R/paste.R", "content": "#  File src/library/base/R/paste.R\\n#  Part of the R package, https://www.R-project.org\\n#\\n#  Copyright (C) 1995-2012 The R Core Team\\n#\\n#  This program is free software; you can redistribute it and/or modify\\n#  it under the terms of the GNU General Public License as published by\\n#  the Free Software Foundation; either version 2 of the License, or\\n#  (at your option) any later version.\\n#\\n#  This program is distributed in the hope that it will be useful,\\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#  GNU General Public License for more details.\\n#\\n#  A copy of the GNU General Public License is available at\\n#  https://www.R-project.org/Licenses/\\n\\npaste <- function (..., sep = \\" \\", collapse = NULL)\\n    .Internal(paste(list(...), sep, collapse))\\npaste0 <- function(..., collapse = NULL)\\n    .Internal(paste0(list(...), collapse))\\n\\n##=== Could we extend  paste(.) to (optionally) accept a\\n##    2-vector for collapse ?\\t With the following functionality\\n\\n##- paste.extra <- function(r, collapse=c(\\", \\",\\" and \\")) {\\n##-\\t    n <- length(r)\\n##-\\t    if(n <= 1) paste(r)\\n##-\\t    else\\n##-\\t      paste(paste(r[-n],collapse=collapse[1L]),\\n##-\\t\\t    r[n], sep=collapse[min(2,length(collapse))])\\n##- }\\n" }\n'
line: b'{ "repo_name": "jimthompson5802/kaggle-BNP-Paribas", "ref": "refs/heads/master", "path": "src/L0_xgb21/train_model.R", "content": "###\\n# training skeleton\\n###\\n\\nlibrary(data.table)\\nlibrary(caret)\\n# add any model specific package library commands\\nlibrary(xgboost)\\n\\n# set working directory\\nWORK.DIR <- \\"./src/L0_xgb21\\"  # modify to specify directory to contain model artififacts\\n\\n# Common Functions and Global variables\\nsource(\\"./src/CommonFunctions.R\\")\\n\\n# import model configuration parameters\\nsource(paste0(WORK.DIR,\\"/model_parameters.R\\"))\\n\\nMODEL.COMMENT <- \\"Build Model\\"\\n\\n\\n# model specific training parameter\\nCARET.TRAIN.CTRL <- trainControl(method=\\"none\\",\\n                                 number=5,\\n                                 repeats=1,\\n                                 verboseIter=FALSE,\\n                                 classProbs=TRUE,\\n                                 summaryFunction=caretLogLossSummary)\\n\\nCARET.TRAIN.OTHER.PARMS <- list(trControl=CARET.TRAIN.CTRL,\\n                            maximize=FALSE,\\n                           tuneGrid=CARET.TUNE.GRID,\\n                           tuneLength=5,\\n                           metric=\\"LogLoss\\")\\n\\n\\n\\n# amount of data to train\\nFRACTION.TRAIN.DATA <- 1.0\\n\\n# force recording model flag\\nFORCE_RECORDING_MODEL <- TRUE\\n\\n# get training data\\ntrain.df <- fread(paste0(DATA.DIR,\\"/train.csv\\"))\\nsetkey(train.raw,ID)\\n\\nif (FRACTION.TRAIN.DATA != 1.0) {\\n    # extract subset for inital training\\n    set.seed(29)\\n    idx <- createDataPartition(train.df$target,p=FRACTION.TRAIN.DATA,list=FALSE)\\n    train.df <- train.df[idx,]\\n}\\n\\n# prepare data for training\\ntrain.data <- PREPARE.MODEL.DATA(train.df)\\n\\nlibrary(doMC)\\nregisterDoMC(cores = 6)\\n\\n# library(doSNOW)\\n# cl <- makeCluster(5,type=\\"SOCK\\")\\n# registerDoSNOW(cl)\\n# clusterExport(cl,list(\\"logLossEval\\"))\\n\\n# train the model\\nSys.time()\\nset.seed(825)\\n\\ntime.data <- system.time(mdl.fit <- do.call(train,c(list(x=train.data$predictors,\\n                                                         y=train.data$response),\\n                                                    CARET.TRAIN.PARMS,\\n                                                    MODEL.SPECIFIC.PARMS,\\n                                                    CARET.TRAIN.OTHER.PARMS)))\\n\\ntime.data\\nmdl.fit\\n# stopCluster(cl)\\n\\ncat(\\"saving...\\\\n\\")\\ndate.time <- as.character(Sys.time())\\nfile.name <- paste0(\\"model_\\",CARET.TRAIN.PARMS$method,\\"_\\",date.time[last.idx],\\".RData\\")\\nfile.name <- gsub(\\" \\",\\"_\\",file.name)\\nfile.name <- gsub(\\":\\",\\"_\\",file.name)\\n\\nsave(mdl.fit,PREPARE.MODEL.DATA,file=paste0(WORK.DIR,\\"/\\",file.name))\\n\\n# estalish pointer to current model\\nwriteLines(file.name,paste0(WORK.DIR,\\"/this_model\\"))\\n\\n\\n\\n\\n" } \n'
line: b'{ "repo_name": "jimthompson5802/kaggle-BNP-Paribas", "ref": "refs/heads/master", "path": "src/L0_xtc31/train_model.R", "content": "###\\n# training skeleton\\n###\\n\\nlibrary(data.table)\\nlibrary(caret)\\n# add any model specific package library commands\\n\\n\\n# set working directory\\nWORK.DIR <- \\"./src/L0_xtc31\\"  # modify to specify directory to contain model artififacts\\n\\n# Common Functions and Global variables\\nsource(\\"./src/CommonFunctions.R\\")\\n\\n# import model configuration parameters\\nsource(paste0(WORK.DIR,\\"/model_parameters.R\\"))\\n\\n# import model configuration parameters\\nsource(paste0(WORK.DIR,\\"/model_parameters.R\\"))\\n\\nMODEL.COMMENT <- \\"K-Fold, Build Model\\"\\n\\n\\n# amount of data to train\\nFRACTION.TRAIN.DATA <- 1.0\\n\\n# force recording model flag\\nFORCE_RECORDING_MODEL <- FALSE\\n\\n# get training data\\ntrain.df <- fread(paste0(DATA.DIR,\\"/train.csv\\"))\\nsetkey(train.df,ID)\\n\\nif (FRACTION.TRAIN.DATA != 1.0) {\\n    # extract subset for inital training\\n    set.seed(29)\\n    idx <- createDataPartition(train.df$target,p=FRACTION.TRAIN.DATA,list=FALSE)\\n    train.df <- train.df[idx,]\\n}\\n\\n# prepare data for training\\ntrain.data <- PREPARE.MODEL.DATA(train.df)\\n\\n# save prepared training data for Python function\\n# put response as first column in data set\\nwrite.table(cbind(response=train.data$response,train.data$predictors),\\n            file=paste0(WORK.DIR,\\"/py_train.tsv\\"),row.names = FALSE,\\n            sep=\\"\\\\t\\")\\n\\n\\n# invoke Python training model\\npython.train.command <- paste(PYTHON_COMMAND,paste0(WORK.DIR,\\"/train_model.py\\"),WORK.DIR)\\n\\nSys.time()\\n\\n\\ntime.data <- system.time(system(python.train.command))\\n\\ntime.data\\n\\n\\ncat(\\"saving...\\\\n\\")\\ndate.time <- as.character(Sys.time())\\nfile.name <- paste0(\\"model_\\",MODEL.NAME,\\"_\\",date.time,\\".RData\\")\\nfile.name <- gsub(\\" \\",\\"_\\",file.name)\\nfile.name <- gsub(\\":\\",\\"_\\",file.name)\\nsave(PREPARE.MODEL.DATA,file=paste0(WORK.DIR,\\"/\\",file.name))\\n\\n# save Python model data\\npy.file.name <- paste0(\\"model_\\",MODEL.NAME,\\"_\\",date.time,\\".PyData\\")\\npy.file.name <- gsub(\\" \\",\\"_\\",py.file.name)\\npy.file.name <- gsub(\\":\\",\\"_\\",py.file.name)\\nfile.rename(paste0(WORK.DIR,\\"/possible_model\\"),paste0(WORK.DIR,\\"/\\",py.file.name))\\n\\n# estalish pointer to current model\\nwriteLines(c(file.name,py.file.name),paste0(WORK.DIR,\\"/this_model\\"))\\n\\n# clean up files no longer needed\\nfile.remove(paste0(WORK.DIR,\\"/py_train.tsv\\"))\\n              #paste0(WORK.DIR,\\"/py_test.tsv\\"),\\n              #paste0(WORK.DIR,\\"/py_test_predictions.tsv\\")\\n              #))\\n\\n\\n" } \n'
line: b'{ "repo_name": "jimthompson5802/kaggle-BNP-Paribas", "ref": "refs/heads/master", "path": "src/L2_nnet1/create_submission.R", "content": "###\\n#  create ensemble model combining selected models\\n###\\n\\nlibrary(data.table)\\nlibrary(caret)\\n\\n\\n# import global variabels and common functions\\nsource(\\"./src/CommonFunctions.R\\")\\nWORK.DIR <- \\"./src/L2_nnet1\\"\\n\\n# retrive generated blending weights data structure\\nmodel.file.name <- readLines(paste0(WORK.DIR,\\"/this_model\\"))\\nload(paste0(WORK.DIR,\\"/\\",model.file.name))\\n\\n# retrieve Level 1 submissions\\n# L1_gbm2\\ngbm2.pred.probs <- read.csv(\\"./src/L1_gbm2/submission.csv\\")\\n\\n#L1_nnet1\\nnnet1.pred.probs <- read.csv(\\"./src/L1_nnet1/submission.csv\\")\\n\\n#create data for predictions\\nsubmission <- list()\\nsubmission$predictors <- cbind(gbm2=gbm2.pred.probs[,\\"PredictedProb\\"],\\n                               nnet1=nnet1.pred.probs[,\\"PredictedProb\\"])\\n\\n# make prediction\\npred.probs <- predict(mdl.fit,newdata = submission$predictors,type = \\"prob\\")\\n\\n#create kaggle submission file\\nwrite.csv(data.frame(ID=gbm2.pred.probs[,\\"ID\\"],PredictedProb=pred.probs[,\\"Class_1\\"]),file=paste0(WORK.DIR,\\"/submission.csv\\"),\\n          row.names=FALSE)\\n\\n" }\n'
line: b'{ "repo_name": "jimthompson5802/kaggle-BNP-Paribas", "ref": "refs/heads/master", "path": "src/L0_xtc31/create_level1_features.R", "content": "###\\n# Model training\\n###\\n\\nlibrary(data.table)\\nlibrary(plyr)\\nlibrary(caret)\\n# add any model specific package library commands\\n\\n\\n# set working directory\\nWORK.DIR <- \\"./src/L0_xtc31\\"  # modify to specify directory to contain model artififacts\\n\\n# Common Functions and Global variables\\nsource(\\"./src/CommonFunctions.R\\")\\n\\n# import model configuration parameters\\nsource(paste0(WORK.DIR,\\"/model_parameters.R\\"))\\n\\n\\n\\nMODEL.COMMENT <- \\"prepL0FeatureSet3, 5-fold training\\"\\n\\n# amount of data to train\\nFRACTION.TRAIN.DATA <- 1.0\\n\\n# force recording model flag\\nFORCE_RECORDING_MODEL <- FALSE\\n\\n# get training data\\ntrain.raw <- fread(paste0(DATA.DIR,\\"/train.csv\\"))\\nsetkey(train.raw,ID)\\n\\n# get data fold specification\\nload(paste0(DATA.DIR,\\"/fold_specification.RData\\"))\\n\\n\\ntrainFolds <- function(this.fold) {\\n    # prepare data for training\\n    test.data <- PREPARE.MODEL.DATA(train.raw[this.fold,])\\n    test.data$ID <- train.raw[this.fold,ID]\\n    \\n    train.data <- PREPARE.MODEL.DATA(train.raw[-this.fold,])\\n    \\n    \\n    if (FRACTION.TRAIN.DATA != 1 ) {\\n        # extract subset for inital training\\n        set.seed(29)\\n        idx <- createDataPartition(train.data$response,p=FRACTION.TRAIN.DATA,list=FALSE)\\n        train.data$predictors <- train.data$predictors[idx,]\\n        train.data$response <- train.data$response[idx]\\n    }\\n    \\n    # save prepared training data for Python function\\n    # put response as first column in data set\\n    write.table(cbind(response=train.data$response,train.data$predictors),\\n                file=paste0(WORK.DIR,\\"/py_train.tsv\\"),row.names = FALSE,\\n                sep=\\"\\\\t\\")\\n    \\n    \\n    # invoke Python training model\\n    python.train.command <- paste(PYTHON_COMMAND,paste0(WORK.DIR,\\"/train_model.py\\"),WORK.DIR)\\n    \\n    Sys.time()\\n    \\n    \\n    time.data <- system.time(system(python.train.command))\\n    \\n    time.data\\n    # stopCluster(cl)\\n    \\n    # prepare data for training\\n    write.table(test.data$predictors,file=paste0(WORK.DIR,\\"/py_test.tsv\\"),row.names = FALSE,\\n                sep=\\"\\\\t\\")\\n    \\n    # execute Python prediction code\\n    python.test.command <- paste(PYTHON_COMMAND,paste0(WORK.DIR,\\"/make_prediction.py\\"),\\n                                 WORK.DIR,\\n                                 \\"possible_model\\",\\n                                 \\"py_test.tsv\\",\\n                                 \\"py_test_predictions.tsv\\")\\n    system(python.test.command)\\n    \\n    # get predictions from Python model\\n    pred.probs <- fread(paste0(WORK.DIR,\\"/py_test_predictions.tsv\\"), sep=\\"\\\\t\\")\\n    \\n    score <- logLossEval(pred.probs[,Class_1],test.data$response)\\n    score\\n    \\n    # clean up files no longer needed\\n    file.remove(c(paste0(WORK.DIR,\\"/py_train.tsv\\"),paste0(WORK.DIR,\\"/py_test.tsv\\"),\\n                  paste0(WORK.DIR,\\"/possible_model\\"),\\n                  paste0(WORK.DIR,\\"/py_test_predictions.tsv\\")))\\n    \\n    ans <- list(score=score,\\n                level1.features=data.frame(ID=test.data$ID,pred.probs,response=test.data$response))\\n    \\n    return(ans)\\n    \\n}\\n# train the model\\nSys.time()\\n\\ntime.data <- system.time(ll <- llply(data.folds,trainFolds,.parallel = FALSE))\\n\\ntime.data\\n\\nfold.scores <- unlist(lapply(ll,function(x){x$score}))\\nlevel1.features <- do.call(rbind,lapply(ll,function(x){x$level1.features}))\\n\\nmean(fold.scores)\\n\\n# record Model performance\\nmodelPerf.df <- read.delim(paste0(WORK.DIR,\\"/model_performance.tsv\\"),\\n                         stringsAsFactors=FALSE)\\n# determine if score improved\\nimproved <- ifelse(mean(fold.scores) < min(modelPerf.df$score),\\"Yes\\",\\"No\\")\\n\\nrecordModelPerf(paste0(WORK.DIR,\\"/model_performance.tsv\\"),\\n                                MODEL.NAME,\\n                              time.data,\\n                              data.frame(),\\n                              mean(fold.scores),\\n                              improved=improved,\\n                              bestTune=NA,\\n                              tune.grid=NA,\\n                              model.parms=NA,\\n                              comment=MODEL.COMMENT)\\n\\nmodelPerf.df <- read.delim(paste0(WORK.DIR,\\"/model_performance.tsv\\"),\\n                         stringsAsFactors=FALSE)\\n\\n\\n#display model performance record for this run\\ntail(modelPerf.df[,1:10],1)\\n\\n# if last score recorded is better than previous ones save model object\\nlast.idx <- length(modelPerf.df$score)\\nif (last.idx == 1 || improved == \\"Yes\\"  || FORCE_RECORDING_MODEL) {\\n    cat(\\"found improved model, saving...\\\\n\\")\\n    flush.console()\\n    #yes we have improvement or first score, save generated model\\n    file.name <- paste0(\\"level1_features_\\",MODEL.NAME,\\"_\\",modelPerf.df$date.time[last.idx],\\".RData\\")\\n    file.name <- gsub(\\" \\",\\"_\\",file.name)\\n    file.name <- gsub(\\":\\",\\"_\\",file.name)\\n    \\n    save(level1.features,PREPARE.MODEL.DATA,file=paste0(WORK.DIR,\\"/\\",file.name))\\n    \\n    # estalish pointer to current model\\n    writeLines(file.name,paste0(WORK.DIR,\\"/this_level1_features\\"))\\n} else {\\n    cat(\\"no improvement!!!\\\\n\\")\\n    flush.console()\\n}\\n\\n\\n\\n" }\n'
line: b'{ "repo_name": "jimthompson5802/kaggle-BNP-Paribas", "ref": "refs/heads/master", "path": "src/L1_nnet11/train_model.R", "content": "###\\n# neural network model\\n###\\n\\nlibrary(data.table)\\nlibrary(caret)\\n# add any model specific package library commands\\nlibrary(nnet)\\n\\n# set working directory\\nWORK.DIR <- \\"./src/L1_nnet11\\"  # modify to specify directory to contain model artififacts\\n\\n# Common Functions and Global variables\\nsource(\\"./src/CommonFunctions.R\\")\\n\\n# set caret training parameters\\nCARET.TRAIN.PARMS <- list(method=\\"nnet\\")   # Replace MODEL.METHOD with appropriate caret model\\n\\nCARET.TUNE.GRID <-  NULL  # NULL provides model specific default tuning parameters\\n\\n# user specified tuning parameters\\n#CARET.TUNE.GRID <- expand.grid(nIter=c(100))\\n\\n# model specific training parameter\\nCARET.TRAIN.CTRL <- trainControl(method=\\"repeatedcv\\",\\n                                 number=5,\\n                                 repeats=1,\\n                                 verboseIter=FALSE,\\n                                 classProbs=TRUE,\\n                                 summaryFunction=caretLogLossSummary)\\n\\nCARET.TRAIN.OTHER.PARMS <- list(trControl=CARET.TRAIN.CTRL,\\n                            maximize=FALSE,\\n                           tuneGrid=CARET.TUNE.GRID,\\n                           tuneLength=7,\\n                           metric=\\"LogLoss\\")\\n\\nMODEL.SPECIFIC.PARMS <- list(verbose=FALSE) #NULL # Other model specific parameters\\n\\nPREPARE.MODEL.DATA <- prepL1FeatureSet1\\n\\nMODEL.COMMENT <- \\"Only Class_1 probabilites as features\\"\\n\\n\\nLEVEL0.MODELS <- c(\\"L0_gbm21\\",\\n                   \\"L0_gbm41\\",\\n                   \\"L0_xtc11\\",\\n                   \\"L0_xtc21\\",\\n                   \\"L0_xtc31\\",\\n                   #\\"L0_xtc4\\",  did not improve score\\n                   \\"L0_xtc51\\",\\n                   #\\"L0_nnet1\\",\\n                   \\"L0_xgb21\\",\\n                   \\"L0_xgb31\\")\\n\\n\\n# amount of data to train\\nFRACTION.TRAIN.DATA <- 1.0\\n\\n# force recording model flag\\nFORCE_RECORDING_MODEL <- FALSE\\n\\n\\n# get training data\\ntrain.data <- prepL1FeatureSet3(LEVEL0.MODELS)\\n\\n\\n# # create the partitions\\n# set.seed(13)\\n# data.folds <- createFolds(raw$target, k=5)\\n\\nlibrary(doMC)\\nregisterDoMC(cores = 7)\\n\\n\\n# train the model\\nSys.time()\\nset.seed(825)\\n\\ntime.data <- system.time(mdl.fit <- do.call(train,c(list(x=train.data$predictors,\\n                                                         y=train.data$response),\\n                                                    CARET.TRAIN.PARMS,\\n                                                    MODEL.SPECIFIC.PARMS,\\n                                                    CARET.TRAIN.OTHER.PARMS)))\\n\\ntime.data\\nmdl.fit\\n# stopCluster(cl)\\n\\nscore <- mean(mdl.fit$resample$LogLoss)\\nscore\\n\\n# record Model performance\\nmodelPerf.df <- read.delim(paste0(WORK.DIR,\\"/model_performance.tsv\\"),\\n                         stringsAsFactors=FALSE)\\n# determine if score improved\\nimproved <- ifelse(score < min(modelPerf.df$score),\\"Yes\\",\\"No\\")\\n\\nrecordModelPerf(paste0(WORK.DIR,\\"/model_performance.tsv\\"),\\n                              mdl.fit$method,\\n                              time.data,\\n                              train.data$predictors,\\n                              score,\\n                              improved=improved,\\n                              bestTune=flattenDF(mdl.fit$bestTune),\\n                              tune.grid=flattenDF(CARET.TUNE.GRID),\\n                              model.parms=paste(names(MODEL.SPECIFIC.PARMS),\\n                                                as.character(MODEL.SPECIFIC.PARMS),\\n                                                sep=\\"=\\",collapse=\\",\\"),\\n                              comment=paste0(MODEL.COMMENT,\\":\\",paste0(LEVEL0.MODELS,collapse=\\", \\")))\\n\\nmodelPerf.df <- read.delim(paste0(WORK.DIR,\\"/model_performance.tsv\\"),\\n                         stringsAsFactors=FALSE)\\n\\n\\n#display model performance record for this run\\ntail(modelPerf.df[,1:10],1)\\n\\n# if last score recorded is better than previous ones save model object\\nlast.idx <- length(modelPerf.df$score)\\nif (last.idx == 1 || improved == \\"Yes\\" || FORCE_RECORDING_MODEL) {\\n    cat(\\"found improved model, saving...\\\\n\\")\\n    flush.console()\\n    #yes we have improvement or first score, save generated model\\n    # file.name <- paste0(\\"model_\\",mdl.fit$method,\\"_\\",modelPerf.df$date.time[last.idx],\\".RData\\")\\n    file.name <- paste0(\\"model_\\",mdl.fit$method,\\"_\\",as.character(Sys.time()),\\".RData\\")\\n    file.name <- gsub(\\" \\",\\"_\\",file.name)\\n    file.name <- gsub(\\":\\",\\"_\\",file.name)\\n\\n    save(LEVEL0.MODELS,PREPARE.MODEL.DATA,mdl.fit,file=paste0(WORK.DIR,\\"/\\",file.name))\\n\\n    # estalish pointer to current model\\n    writeLines(file.name,paste0(WORK.DIR,\\"/this_model\\"))\\n} else {\\n    cat(\\"no improvement!!!\\\\n\\")\\n    flush.console()\\n}\\n\\n\\n\\n" }\n'
line: b'{"repo_name":"jasonseminara/OpenSourceFinal","ref":"refs/heads/master","path":"myvenv/lib/python3.5/site-packages/django/template/smartif.py","content":"\\"\\"\\"\\nParser and utilities for the smart \'if\' tag\\n\\"\\"\\"\\nimport warnings\\n\\nfrom django.utils.deprecation import RemovedInDjango110Warning\\n\\n\\n# Using a simple top down parser, as described here:\\n#    http://effbot.org/zone/simple-top-down-parsing.htm.\\n# \'led\' = left denotation\\n# \'nud\' = null denotation\\n# \'bp\' = binding power (left = lbp, right = rbp)\\n\\nclass TokenBase(object):\\n    \\"\\"\\"\\n    Base class for operators and literals, mainly for debugging and for throwing\\n    syntax errors.\\n    \\"\\"\\"\\n    id = None  # node/token type name\\n    value = None  # used by literals\\n    first = second = None  # used by tree nodes\\n\\n    def nud(self, parser):\\n        # Null denotation - called in prefix context\\n        raise parser.error_class(\\n            \\"Not expecting \'%s\' in this position in if tag.\\" % self.id\\n        )\\n\\n    def led(self, left, parser):\\n        # Left denotation - called in infix context\\n        raise parser.error_class(\\n            \\"Not expecting \'%s\' as infix operator in if tag.\\" % self.id\\n        )\\n\\n    def display(self):\\n        \\"\\"\\"\\n        Returns what to display in error messages for this node\\n        \\"\\"\\"\\n        return self.id\\n\\n    def __repr__(self):\\n        out = [str(x) for x in [self.id, self.first, self.second] if x is not None]\\n        return \\"(\\" + \\" \\".join(out) + \\")\\"\\n\\n\\ndef infix(bp, func):\\n    \\"\\"\\"\\n    Creates an infix operator, given a binding power and a function that\\n    evaluates the node\\n    \\"\\"\\"\\n    class Operator(TokenBase):\\n        lbp = bp\\n\\n        def led(self, left, parser):\\n            self.first = left\\n            self.second = parser.expression(bp)\\n            return self\\n\\n        def eval(self, context):\\n            try:\\n                return func(context, self.first, self.second)\\n            except Exception:\\n                # Templates shouldn\'t throw exceptions when rendering.  We are\\n                # most likely to get exceptions for things like {% if foo in bar\\n                # %} where \'bar\' does not support \'in\', so default to False\\n                return False\\n\\n    return Operator\\n\\n\\ndef prefix(bp, func):\\n    \\"\\"\\"\\n    Creates a prefix operator, given a binding power and a function that\\n    evaluates the node.\\n    \\"\\"\\"\\n    class Operator(TokenBase):\\n        lbp = bp\\n\\n        def nud(self, parser):\\n            self.first = parser.expression(bp)\\n            self.second = None\\n            return self\\n\\n        def eval(self, context):\\n            try:\\n                return func(context, self.first)\\n            except Exception:\\n                return False\\n\\n    return Operator\\n\\n\\n# Operator precedence follows Python.\\n# NB - we can get slightly more accurate syntax error messages by not using the\\n# same object for \'==\' and \'=\'.\\n# We defer variable evaluation to the lambda to ensure that terms are\\n# lazily evaluated using Python\'s boolean parsing logic.\\nOPERATORS = {\\n    \'or\': infix(6, lambda context, x, y: x.eval(context) or y.eval(context)),\\n    \'and\': infix(7, lambda context, x, y: x.eval(context) and y.eval(context)),\\n    \'not\': prefix(8, lambda context, x: not x.eval(context)),\\n    \'in\': infix(9, lambda context, x, y: x.eval(context) in y.eval(context)),\\n    \'not in\': infix(9, lambda context, x, y: x.eval(context) not in y.eval(context)),\\n    # This should be removed in Django 1.10:\\n    \'=\': infix(10, lambda context, x, y: x.eval(context) == y.eval(context)),\\n    \'==\': infix(10, lambda context, x, y: x.eval(context) == y.eval(context)),\\n    \'!=\': infix(10, lambda context, x, y: x.eval(context) != y.eval(context)),\\n    \'\\u003e\': infix(10, lambda context, x, y: x.eval(context) \\u003e y.eval(context)),\\n    \'\\u003e=\': infix(10, lambda context, x, y: x.eval(context) \\u003e= y.eval(context)),\\n    \'\\u003c\': infix(10, lambda context, x, y: x.eval(context) \\u003c y.eval(context)),\\n    \'\\u003c=\': infix(10, lambda context, x, y: x.eval(context) \\u003c= y.eval(context)),\\n}\\n\\n# Assign \'id\' to each:\\nfor key, op in OPERATORS.items():\\n    op.id = key\\n\\n\\nclass Literal(TokenBase):\\n    \\"\\"\\"\\n    A basic self-resolvable object similar to a Django template variable.\\n    \\"\\"\\"\\n    # IfParser uses Literal in create_var, but TemplateIfParser overrides\\n    # create_var so that a proper implementation that actually resolves\\n    # variables, filters etc is used.\\n    id = \\"literal\\"\\n    lbp = 0\\n\\n    def __init__(self, value):\\n        self.value = value\\n\\n    def display(self):\\n        return repr(self.value)\\n\\n    def nud(self, parser):\\n        return self\\n\\n    def eval(self, context):\\n        return self.value\\n\\n    def __repr__(self):\\n        return \\"(%s %r)\\" % (self.id, self.value)\\n\\n\\nclass EndToken(TokenBase):\\n    lbp = 0\\n\\n    def nud(self, parser):\\n        raise parser.error_class(\\"Unexpected end of expression in if tag.\\")\\n\\nEndToken = EndToken()\\n\\n\\nclass IfParser(object):\\n    error_class = ValueError\\n\\n    def __init__(self, tokens):\\n        # pre-pass necessary to turn  \'not\',\'in\' into single token\\n        l = len(tokens)\\n        mapped_tokens = []\\n        i = 0\\n        while i \\u003c l:\\n            token = tokens[i]\\n            if token == \\"not\\" and i + 1 \\u003c l and tokens[i + 1] == \\"in\\":\\n                token = \\"not in\\"\\n                i += 1  # skip \'in\'\\n            mapped_tokens.append(self.translate_token(token))\\n            i += 1\\n\\n        self.tokens = mapped_tokens\\n        self.pos = 0\\n        self.current_token = self.next_token()\\n\\n    def translate_token(self, token):\\n        try:\\n            op = OPERATORS[token]\\n        except (KeyError, TypeError):\\n            return self.create_var(token)\\n        else:\\n            if token == \'=\':\\n                warnings.warn(\\n                    \\"Operator \'=\' is deprecated and will be removed in Django 1.10. Use \'==\' instead.\\",\\n                    RemovedInDjango110Warning, stacklevel=2\\n                )\\n            return op()\\n\\n    def next_token(self):\\n        if self.pos \\u003e= len(self.tokens):\\n            return EndToken\\n        else:\\n            retval = self.tokens[self.pos]\\n            self.pos += 1\\n            return retval\\n\\n    def parse(self):\\n        retval = self.expression()\\n        # Check that we have exhausted all the tokens\\n        if self.current_token is not EndToken:\\n            raise self.error_class(\\"Unused \'%s\' at end of if expression.\\" %\\n                                   self.current_token.display())\\n        return retval\\n\\n    def expression(self, rbp=0):\\n        t = self.current_token\\n        self.current_token = self.next_token()\\n        left = t.nud(self)\\n        while rbp \\u003c self.current_token.lbp:\\n            t = self.current_token\\n            self.current_token = self.next_token()\\n            left = t.led(left, self)\\n        return left\\n\\n    def create_var(self, value):\\n        return Literal(value)\\n"}\n'
line: b'{"repo_name":"shsingh/ansible","ref":"refs/heads/devel","path":"lib/ansible/modules/cloud/azure/azure_rm_securitygroup.py","content":"#!/usr/bin/python\\n#\\n# Copyright (c) 2016 Matt Davis, \\u003cmdavis@ansible.com\\u003e\\n#                    Chris Houseknecht, \\u003chouse@redhat.com\\u003e\\n#\\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\\n\\nfrom __future__ import absolute_import, division, print_function\\n__metaclass__ = type\\n\\n\\nANSIBLE_METADATA = {\'metadata_version\': \'1.1\',\\n                    \'status\': [\'preview\'],\\n                    \'supported_by\': \'community\'}\\n\\n\\nDOCUMENTATION = \'\'\'\\n---\\nmodule: azure_rm_securitygroup\\nversion_added: \\"2.1\\"\\nshort_description: Manage Azure network security groups.\\ndescription:\\n    - Create, update or delete a network security group. A security group contains Access Control List (ACL) rules\\n      that allow or deny network traffic to subnets or individual network interfaces. A security group is created\\n      with a set of default security rules and an empty set of security rules. Shape traffic flow by adding\\n      rules to the empty set of security rules.\\n\\noptions:\\n    default_rules:\\n        description:\\n            - The set of default rules automatically added to a security group at creation. In general default\\n              rules will not be modified. Modify rules to shape the flow of traffic to or from a subnet or NIC. See\\n              rules below for the makeup of a rule dict.\\n    location:\\n        description:\\n            - Valid azure location. Defaults to location of the resource group.\\n    name:\\n        description:\\n            - Name of the security group to operate on.\\n    purge_default_rules:\\n        description:\\n            - Remove any existing rules not matching those defined in the default_rules parameter.\\n        type: bool\\n        default: \'no\'\\n    purge_rules:\\n        description:\\n            - Remove any existing rules not matching those defined in the rules parameters.\\n        type: bool\\n        default: \'no\'\\n    resource_group:\\n        description:\\n            - Name of the resource group the security group belongs to.\\n        required: true\\n    rules:\\n        description:\\n            - Set of rules shaping traffic flow to or from a subnet or NIC. Each rule is a dictionary.\\n        suboptions:\\n            name:\\n                description:\\n                  - Unique name for the rule.\\n                required: true\\n            description:\\n                description:\\n                  - Short description of the rule\'s purpose.\\n            protocol:\\n                description: Accepted traffic protocol.\\n                choices:\\n                  - Udp\\n                  - Tcp\\n                  - \\"*\\"\\n                default: \\"*\\"\\n            source_port_range:\\n                description:\\n                  - Port or range of ports from which traffic originates.\\n                  - It can accept string type or a list of string type.\\n                default: \\"*\\"\\n            destination_port_range:\\n                description:\\n                  - Port or range of ports to which traffic is headed.\\n                  - It can accept string type or a list of string type.\\n                default: \\"*\\"\\n            source_address_prefix:\\n                description:\\n                  - The CIDR or source IP range.\\n                  - Asterisk C(*) can also be used to match all source IPs.\\n                  - Default tags such as C(VirtualNetwork), C(AzureLoadBalancer) and C(Internet) can also be used.\\n                  - If this is an ingress rule, specifies where network traffic originates from.\\n                  - It can accept string type or a list of string type.\\n                default: \\"*\\"\\n            destination_address_prefix:\\n                description:\\n                  - The destination address prefix.\\n                  - CIDR or destination IP range.\\n                  - Asterisk C(*) can also be used to match all source IPs.\\n                  - Default tags such as C(VirtualNetwork), C(AzureLoadBalancer) and C(Internet) can also be used.\\n                  - It can accept string type or a list of string type.\\n                default: \\"*\\"\\n            access:\\n                description:\\n                  - Whether or not to allow the traffic flow.\\n                choices:\\n                  - Allow\\n                  - Deny\\n                default: Allow\\n            priority:\\n                description:\\n                  - Order in which to apply the rule. Must a unique integer between 100 and 4096 inclusive.\\n                required: true\\n            direction:\\n                description:\\n                  - Indicates the direction of the traffic flow.\\n                choices:\\n                  - Inbound\\n                  - Outbound\\n                default: Inbound\\n    state:\\n        description:\\n            - Assert the state of the security group. Set to C(present) to create or update a security group. Set to\\n              C(absent) to remove a security group.\\n        default: present\\n        choices:\\n            - absent\\n            - present\\n\\nextends_documentation_fragment:\\n    - azure\\n    - azure_tags\\n\\nauthor:\\n    - \\"Chris Houseknecht (@chouseknecht)\\"\\n    - \\"Matt Davis (@nitzmahone)\\"\\n\\n\'\'\'\\n\\nEXAMPLES = \'\'\'\\n\\n# Create a security group\\n- azure_rm_securitygroup:\\n      resource_group: myResourceGroup\\n      name: mysecgroup\\n      purge_rules: yes\\n      rules:\\n          - name: DenySSH\\n            protocol: Tcp\\n            destination_port_range: 22\\n            access: Deny\\n            priority: 100\\n            direction: Inbound\\n          - name: \'AllowSSH\'\\n            protocol: Tcp\\n            source_address_prefix:\\n              - \'174.109.158.0/24\'\\n              - \'174.109.159.0/24\'\\n            destination_port_range: 22\\n            access: Allow\\n            priority: 101\\n            direction: Inbound\\n          - name: \'AllowMultiplePorts\'\\n            protocol: Tcp\\n            source_address_prefix:\\n              - \'174.109.158.0/24\'\\n              - \'174.109.159.0/24\'\\n            destination_port_range:\\n              - 80\\n              - 443\\n            access: Allow\\n            priority: 102\\n\\n# Update rules on existing security group\\n- azure_rm_securitygroup:\\n      resource_group: myResourceGroup\\n      name: mysecgroup\\n      rules:\\n          - name: DenySSH\\n            protocol: Tcp\\n            destination_port_range: 22-23\\n            access: Deny\\n            priority: 100\\n            direction: Inbound\\n          - name: AllowSSHFromHome\\n            protocol: Tcp\\n            source_address_prefix: \'174.109.158.0/24\'\\n            destination_port_range: 22-23\\n            access: Allow\\n            priority: 102\\n            direction: Inbound\\n      tags:\\n          testing: testing\\n          delete: on-exit\\n\\n# Delete security group\\n- azure_rm_securitygroup:\\n      resource_group: myResourceGroup\\n      name: mysecgroup\\n      state: absent\\n\'\'\'\\n\\nRETURN = \'\'\'\\nstate:\\n    description: Current state of the security group.\\n    returned: always\\n    type: dict\\n    sample: {\\n        \\"default_rules\\": [\\n            {\\n                \\"access\\": \\"Allow\\",\\n                \\"description\\": \\"Allow inbound traffic from all VMs in VNET\\",\\n                \\"destination_address_prefix\\": \\"VirtualNetwork\\",\\n                \\"destination_port_range\\": \\"*\\",\\n                \\"direction\\": \\"Inbound\\",\\n                \\"etag\\": \'W/\\"edf48d56-b315-40ca-a85d-dbcb47f2da7d\\"\',\\n                \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroup/myResourceGroup/providers/Microsoft.Network/networkSecurityGroups/mysecgroup/defaultSecurityRules/AllowVnetInBound\\",\\n                \\"name\\": \\"AllowVnetInBound\\",\\n                \\"priority\\": 65000,\\n                \\"protocol\\": \\"*\\",\\n                \\"provisioning_state\\": \\"Succeeded\\",\\n                \\"source_address_prefix\\": \\"VirtualNetwork\\",\\n                \\"source_port_range\\": \\"*\\"\\n            },\\n            {\\n                \\"access\\": \\"Allow\\",\\n                \\"description\\": \\"Allow inbound traffic from azure load balancer\\",\\n                \\"destination_address_prefix\\": \\"*\\",\\n                \\"destination_port_range\\": \\"*\\",\\n                \\"direction\\": \\"Inbound\\",\\n                \\"etag\\": \'W/\\"edf48d56-b315-40ca-a85d-dbcb47f2da7d\\"\',\\n                \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroup/myResourceGroup/providers/Microsoft.Network/networkSecurityGroups/mysecgroup/defaultSecurityRules/AllowAzureLoadBalancerInBound\\",\\n                \\"name\\": \\"AllowAzureLoadBalancerInBound\\",\\n                \\"priority\\": 65001,\\n                \\"protocol\\": \\"*\\",\\n                \\"provisioning_state\\": \\"Succeeded\\",\\n                \\"source_address_prefix\\": \\"AzureLoadBalancer\\",\\n                \\"source_port_range\\": \\"*\\"\\n            },\\n            {\\n                \\"access\\": \\"Deny\\",\\n                \\"description\\": \\"Deny all inbound traffic\\",\\n                \\"destination_address_prefix\\": \\"*\\",\\n                \\"destination_port_range\\": \\"*\\",\\n                \\"direction\\": \\"Inbound\\",\\n                \\"etag\\": \'W/\\"edf48d56-b315-40ca-a85d-dbcb47f2da7d\\"\',\\n                \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroup/myResourceGroup/providers/Microsoft.Network/networkSecurityGroups/mysecgroup/defaultSecurityRules/DenyAllInBound\\",\\n                \\"name\\": \\"DenyAllInBound\\",\\n                \\"priority\\": 65500,\\n                \\"protocol\\": \\"*\\",\\n                \\"provisioning_state\\": \\"Succeeded\\",\\n                \\"source_address_prefix\\": \\"*\\",\\n                \\"source_port_range\\": \\"*\\"\\n            },\\n            {\\n                \\"access\\": \\"Allow\\",\\n                \\"description\\": \\"Allow outbound traffic from all VMs to all VMs in VNET\\",\\n                \\"destination_address_prefix\\": \\"VirtualNetwork\\",\\n                \\"destination_port_range\\": \\"*\\",\\n                \\"direction\\": \\"Outbound\\",\\n                \\"etag\\": \'W/\\"edf48d56-b315-40ca-a85d-dbcb47f2da7d\\"\',\\n                \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroup/myResourceGroup/providers/Microsoft.Network/networkSecurityGroups/mysecgroup/defaultSecurityRules/AllowVnetOutBound\\",\\n                \\"name\\": \\"AllowVnetOutBound\\",\\n                \\"priority\\": 65000,\\n                \\"protocol\\": \\"*\\",\\n                \\"provisioning_state\\": \\"Succeeded\\",\\n                \\"source_address_prefix\\": \\"VirtualNetwork\\",\\n                \\"source_port_range\\": \\"*\\"\\n            },\\n            {\\n                \\"access\\": \\"Allow\\",\\n                \\"description\\": \\"Allow outbound traffic from all VMs to Internet\\",\\n                \\"destination_address_prefix\\": \\"Internet\\",\\n                \\"destination_port_range\\": \\"*\\",\\n                \\"direction\\": \\"Outbound\\",\\n                \\"etag\\": \'W/\\"edf48d56-b315-40ca-a85d-dbcb47f2da7d\\"\',\\n                \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroup/myResourceGroup/providers/Microsoft.Network/networkSecurityGroups/mysecgroup/defaultSecurityRules/AllowInternetOutBound\\",\\n                \\"name\\": \\"AllowInternetOutBound\\",\\n                \\"priority\\": 65001,\\n                \\"protocol\\": \\"*\\",\\n                \\"provisioning_state\\": \\"Succeeded\\",\\n                \\"source_address_prefix\\": \\"*\\",\\n                \\"source_port_range\\": \\"*\\"\\n            },\\n            {\\n                \\"access\\": \\"Deny\\",\\n                \\"description\\": \\"Deny all outbound traffic\\",\\n                \\"destination_address_prefix\\": \\"*\\",\\n                \\"destination_port_range\\": \\"*\\",\\n                \\"direction\\": \\"Outbound\\",\\n                \\"etag\\": \'W/\\"edf48d56-b315-40ca-a85d-dbcb47f2da7d\\"\',\\n                \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroup/myResourceGroup/providers/Microsoft.Network/networkSecurityGroups/mysecgroup/defaultSecurityRules/DenyAllOutBound\\",\\n                \\"name\\": \\"DenyAllOutBound\\",\\n                \\"priority\\": 65500,\\n                \\"protocol\\": \\"*\\",\\n                \\"provisioning_state\\": \\"Succeeded\\",\\n                \\"source_address_prefix\\": \\"*\\",\\n                \\"source_port_range\\": \\"*\\"\\n            }\\n        ],\\n        \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroup/myResourceGroup/providers/Microsoft.Network/networkSecurityGroups/mysecgroup\\",\\n        \\"location\\": \\"westus\\",\\n        \\"name\\": \\"mysecgroup\\",\\n        \\"network_interfaces\\": [],\\n        \\"rules\\": [\\n            {\\n                \\"access\\": \\"Deny\\",\\n                \\"description\\": null,\\n                \\"destination_address_prefix\\": \\"*\\",\\n                \\"destination_port_range\\": \\"22\\",\\n                \\"direction\\": \\"Inbound\\",\\n                \\"etag\\": \'W/\\"edf48d56-b315-40ca-a85d-dbcb47f2da7d\\"\',\\n                \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroup/myResourceGroup/providers/Microsoft.Network/networkSecurityGroups/mysecgroup/securityRules/DenySSH\\",\\n                \\"name\\": \\"DenySSH\\",\\n                \\"priority\\": 100,\\n                \\"protocol\\": \\"Tcp\\",\\n                \\"provisioning_state\\": \\"Succeeded\\",\\n                \\"source_address_prefix\\": \\"*\\",\\n                \\"source_port_range\\": \\"*\\"\\n            },\\n            {\\n                \\"access\\": \\"Allow\\",\\n                \\"description\\": null,\\n                \\"destination_address_prefix\\": \\"*\\",\\n                \\"destination_port_range\\": \\"22\\",\\n                \\"direction\\": \\"Inbound\\",\\n                \\"etag\\": \'W/\\"edf48d56-b315-40ca-a85d-dbcb47f2da7d\\"\',\\n                \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroup/myResourceGroup/providers/Microsoft.Network/networkSecurityGroups/mysecgroup/securityRules/AllowSSH\\",\\n                \\"name\\": \\"AllowSSH\\",\\n                \\"priority\\": 101,\\n                \\"protocol\\": \\"Tcp\\",\\n                \\"provisioning_state\\": \\"Succeeded\\",\\n                \\"source_address_prefix\\": \\"174.109.158.0/24\\",\\n                \\"source_port_range\\": \\"*\\"\\n            }\\n        ],\\n        \\"subnets\\": [],\\n        \\"tags\\": {\\n            \\"delete\\": \\"on-exit\\",\\n            \\"foo\\": \\"bar\\",\\n            \\"testing\\": \\"testing\\"\\n        },\\n        \\"type\\": \\"Microsoft.Network/networkSecurityGroups\\"\\n    }\\n\'\'\'  # NOQA\\n\\ntry:\\n    from msrestazure.azure_exceptions import CloudError\\n    from azure.mgmt.network import NetworkManagementClient\\nexcept ImportError:\\n    # This is handled in azure_rm_common\\n    pass\\n\\nfrom ansible.module_utils.azure_rm_common import AzureRMModuleBase\\nfrom ansible.module_utils.six import integer_types\\nfrom ansible.module_utils._text import to_native\\n\\n\\ndef validate_rule(self, rule, rule_type=None):\\n    \'\'\'\\n    Apply defaults to a rule dictionary and check that all values are valid.\\n\\n    :param rule: rule dict\\n    :param rule_type: Set to \'default\' if the rule is part of the default set of rules.\\n    :return: None\\n    \'\'\'\\n    priority = rule.get(\'priority\', 0)\\n    if rule_type != \'default\' and (priority \\u003c 100 or priority \\u003e 4096):\\n        raise Exception(\\"Rule priority must be between 100 and 4096\\")\\n\\n    def check_plural(src, dest):\\n        if isinstance(rule.get(src), list):\\n            rule[dest] = rule[src]\\n            rule[src] = None\\n\\n    check_plural(\'destination_address_prefix\', \'destination_address_prefixes\')\\n    check_plural(\'source_address_prefix\', \'source_address_prefixes\')\\n    check_plural(\'source_port_range\', \'source_port_ranges\')\\n    check_plural(\'destination_port_range\', \'destination_port_ranges\')\\n\\n\\ndef compare_rules_change(old_list, new_list, purge_list):\\n    old_list = old_list or []\\n    new_list = new_list or []\\n    changed = False\\n\\n    for old_rule in old_list:\\n        matched = next((x for x in new_list if x[\'name\'] == old_rule[\'name\']), [])\\n        if matched:  # if the new one is in the old list, check whether it is updated\\n            changed = changed or compare_rules(old_rule, matched)\\n        elif not purge_list:  # keep this rule\\n            new_list.append(old_rule)\\n        else:  # one rule is removed\\n            changed = True\\n    # Compare new list and old list is the same? here only compare names\\n    if not changed:\\n        new_names = [to_native(x[\'name\']) for x in new_list]\\n        old_names = [to_native(x[\'name\']) for x in old_list]\\n        changed = (set(new_names) != set(old_names))\\n    return changed, new_list\\n\\n\\ndef compare_rules(old_rule, rule):\\n    changed = False\\n    if old_rule[\'name\'] != rule[\'name\']:\\n        changed = True\\n    if rule.get(\'description\', None) != old_rule[\'description\']:\\n        changed = True\\n    if rule[\'protocol\'] != old_rule[\'protocol\']:\\n        changed = True\\n    if str(rule[\'source_port_range\']) != str(old_rule[\'source_port_range\']):\\n        changed = True\\n    if str(rule[\'destination_port_range\']) != str(old_rule[\'destination_port_range\']):\\n        changed = True\\n    if rule[\'access\'] != old_rule[\'access\']:\\n        changed = True\\n    if rule[\'priority\'] != old_rule[\'priority\']:\\n        changed = True\\n    if rule[\'direction\'] != old_rule[\'direction\']:\\n        changed = True\\n    if str(rule[\'source_address_prefix\']) != str(old_rule[\'source_address_prefix\']):\\n        changed = True\\n    if str(rule[\'destination_address_prefix\']) != str(old_rule[\'destination_address_prefix\']):\\n        changed = True\\n    if set(rule.get(\'source_address_prefixes\') or []) != set(old_rule.get(\'source_address_prefixes\') or []):\\n        changed = True\\n    if set(rule.get(\'destination_address_prefixes\') or []) != set(old_rule.get(\'destination_address_prefixes\') or []):\\n        changed = True\\n    if set(rule.get(\'source_port_ranges\') or []) != set(old_rule.get(\'source_port_ranges\') or []):\\n        changed = True\\n    if set(rule.get(\'destination_port_ranges\') or []) != set(old_rule.get(\'destination_port_ranges\') or []):\\n        changed = True\\n    return changed\\n\\n\\ndef create_rule_instance(self, rule):\\n    \'\'\'\\n    Create an instance of SecurityRule from a dict.\\n\\n    :param rule: dict\\n    :return: SecurityRule\\n    \'\'\'\\n    return self.nsg_models.SecurityRule(\\n        description=rule.get(\'description\', None),\\n        protocol=rule.get(\'protocol\', None),\\n        source_port_range=rule.get(\'source_port_range\', None),\\n        destination_port_range=rule.get(\'destination_port_range\', None),\\n        source_address_prefix=rule.get(\'source_address_prefix\', None),\\n        source_address_prefixes=rule.get(\'source_address_prefixes\', None),\\n        destination_address_prefix=rule.get(\'destination_address_prefix\', None),\\n        destination_address_prefixes=rule.get(\'destination_address_prefixes\', None),\\n        source_port_ranges=rule.get(\'source_port_ranges\', None),\\n        destination_port_ranges=rule.get(\'destination_port_ranges\', None),\\n        access=rule.get(\'access\', None),\\n        priority=rule.get(\'priority\', None),\\n        direction=rule.get(\'direction\', None),\\n        provisioning_state=rule.get(\'provisioning_state\', None),\\n        name=rule.get(\'name\', None),\\n        etag=rule.get(\'etag\', None)\\n    )\\n\\n\\ndef create_rule_dict_from_obj(rule):\\n    \'\'\'\\n    Create a dict from an instance of a SecurityRule.\\n\\n    :param rule: SecurityRule\\n    :return: dict\\n    \'\'\'\\n    return dict(\\n        id=rule.id,\\n        name=rule.name,\\n        description=rule.description,\\n        protocol=rule.protocol,\\n        source_port_range=rule.source_port_range,\\n        destination_port_range=rule.destination_port_range,\\n        source_address_prefix=rule.source_address_prefix,\\n        destination_address_prefix=rule.destination_address_prefix,\\n        source_port_ranges=rule.source_port_ranges,\\n        destination_port_ranges=rule.destination_port_ranges,\\n        source_address_prefixes=rule.source_address_prefixes,\\n        destination_address_prefixes=rule.destination_address_prefixes,\\n        access=rule.access,\\n        priority=rule.priority,\\n        direction=rule.direction,\\n        provisioning_state=rule.provisioning_state,\\n        etag=rule.etag\\n    )\\n\\n\\ndef create_network_security_group_dict(nsg):\\n    results = dict(\\n        id=nsg.id,\\n        name=nsg.name,\\n        type=nsg.type,\\n        location=nsg.location,\\n        tags=nsg.tags,\\n    )\\n    results[\'rules\'] = []\\n    if nsg.security_rules:\\n        for rule in nsg.security_rules:\\n            results[\'rules\'].append(create_rule_dict_from_obj(rule))\\n\\n    results[\'default_rules\'] = []\\n    if nsg.default_security_rules:\\n        for rule in nsg.default_security_rules:\\n            results[\'default_rules\'].append(create_rule_dict_from_obj(rule))\\n\\n    results[\'network_interfaces\'] = []\\n    if nsg.network_interfaces:\\n        for interface in nsg.network_interfaces:\\n            results[\'network_interfaces\'].append(interface.id)\\n\\n    results[\'subnets\'] = []\\n    if nsg.subnets:\\n        for subnet in nsg.subnets:\\n            results[\'subnets\'].append(subnet.id)\\n\\n    return results\\n\\n\\nrule_spec = dict(\\n    name=dict(type=\'str\', required=True),\\n    description=dict(type=\'str\'),\\n    protocol=dict(type=\'str\', choices=[\'Udp\', \'Tcp\', \'*\'], default=\'*\'),\\n    source_port_range=dict(type=\'raw\', default=\'*\'),\\n    destination_port_range=dict(type=\'raw\', default=\'*\'),\\n    source_address_prefix=dict(type=\'raw\', default=\'*\'),\\n    destination_address_prefix=dict(type=\'raw\', default=\'*\'),\\n    access=dict(type=\'str\', choices=[\'Allow\', \'Deny\'], default=\'Allow\'),\\n    priority=dict(type=\'int\', required=True),\\n    direction=dict(type=\'str\', choices=[\'Inbound\', \'Outbound\'], default=\'Inbound\')\\n)\\n\\n\\nclass AzureRMSecurityGroup(AzureRMModuleBase):\\n\\n    def __init__(self):\\n\\n        self.module_arg_spec = dict(\\n            default_rules=dict(type=\'list\', elements=\'dict\', options=rule_spec),\\n            location=dict(type=\'str\'),\\n            name=dict(type=\'str\', required=True),\\n            purge_default_rules=dict(type=\'bool\', default=False),\\n            purge_rules=dict(type=\'bool\', default=False),\\n            resource_group=dict(required=True, type=\'str\'),\\n            rules=dict(type=\'list\', elements=\'dict\', options=rule_spec),\\n            state=dict(type=\'str\', default=\'present\', choices=[\'present\', \'absent\']),\\n        )\\n\\n        self.default_rules = None\\n        self.location = None\\n        self.name = None\\n        self.purge_default_rules = None\\n        self.purge_rules = None\\n        self.resource_group = None\\n        self.rules = None\\n        self.state = None\\n        self.tags = None\\n        self.nsg_models = None  # type: azure.mgmt.network.models\\n\\n        self.results = dict(\\n            changed=False,\\n            state=dict()\\n        )\\n\\n        super(AzureRMSecurityGroup, self).__init__(self.module_arg_spec,\\n                                                   supports_check_mode=True)\\n\\n    def exec_module(self, **kwargs):\\n        # tighten up poll interval for security groups; default 30s is an eternity\\n        # this value is still overridden by the response Retry-After header (which is set on the initial operation response to 10s)\\n        self.network_client.config.long_running_operation_timeout = 3\\n        self.nsg_models = self.network_client.network_security_groups.models\\n\\n        for key in list(self.module_arg_spec.keys()) + [\'tags\']:\\n            setattr(self, key, kwargs[key])\\n\\n        changed = False\\n        results = dict()\\n\\n        resource_group = self.get_resource_group(self.resource_group)\\n        if not self.location:\\n            # Set default location\\n            self.location = resource_group.location\\n\\n        if self.rules:\\n            for rule in self.rules:\\n                try:\\n                    validate_rule(self, rule)\\n                except Exception as exc:\\n                    self.fail(\\"Error validating rule {0} - {1}\\".format(rule, str(exc)))\\n\\n        if self.default_rules:\\n            for rule in self.default_rules:\\n                try:\\n                    validate_rule(self, rule, \'default\')\\n                except Exception as exc:\\n                    self.fail(\\"Error validating default rule {0} - {1}\\".format(rule, str(exc)))\\n\\n        try:\\n            nsg = self.network_client.network_security_groups.get(self.resource_group, self.name)\\n            results = create_network_security_group_dict(nsg)\\n            self.log(\\"Found security group:\\")\\n            self.log(results, pretty_print=True)\\n            self.check_provisioning_state(nsg, self.state)\\n            if self.state == \'present\':\\n                pass\\n            elif self.state == \'absent\':\\n                self.log(\\"CHANGED: security group found but state is \'absent\'\\")\\n                changed = True\\n        except CloudError:  # TODO: actually check for ResourceMissingError\\n            if self.state == \'present\':\\n                self.log(\\"CHANGED: security group not found and state is \'present\'\\")\\n                changed = True\\n\\n        if self.state == \'present\' and not changed:\\n            # update the security group\\n            self.log(\\"Update security group {0}\\".format(self.name))\\n\\n            update_tags, results[\'tags\'] = self.update_tags(results[\'tags\'])\\n            if update_tags:\\n                changed = True\\n\\n            rule_changed, new_rule = compare_rules_change(results[\'rules\'], self.rules, self.purge_rules)\\n            if rule_changed:\\n                changed = True\\n                results[\'rules\'] = new_rule\\n            rule_changed, new_rule = compare_rules_change(results[\'default_rules\'], self.default_rules, self.purge_default_rules)\\n            if rule_changed:\\n                changed = True\\n                results[\'default_rules\'] = new_rule\\n\\n            self.results[\'changed\'] = changed\\n            self.results[\'state\'] = results\\n            if not self.check_mode and changed:\\n                self.results[\'state\'] = self.create_or_update(results)\\n\\n        elif self.state == \'present\' and changed:\\n            # create the security group\\n            self.log(\\"Create security group {0}\\".format(self.name))\\n\\n            if not self.location:\\n                self.fail(\\"Parameter error: location required when creating a security group.\\")\\n\\n            results[\'name\'] = self.name\\n            results[\'location\'] = self.location\\n            results[\'rules\'] = []\\n            results[\'default_rules\'] = []\\n            results[\'tags\'] = {}\\n\\n            if self.rules:\\n                results[\'rules\'] = self.rules\\n            if self.default_rules:\\n                results[\'default_rules\'] = self.default_rules\\n            if self.tags:\\n                results[\'tags\'] = self.tags\\n\\n            self.results[\'changed\'] = changed\\n            self.results[\'state\'] = results\\n            if not self.check_mode:\\n                self.results[\'state\'] = self.create_or_update(results)\\n\\n        elif self.state == \'absent\' and changed:\\n            self.log(\\"Delete security group {0}\\".format(self.name))\\n            self.results[\'changed\'] = changed\\n            self.results[\'state\'] = dict()\\n            if not self.check_mode:\\n                self.delete()\\n                # the delete does not actually return anything. if no exception, then we\'ll assume\\n                # it worked.\\n                self.results[\'state\'][\'status\'] = \'Deleted\'\\n\\n        return self.results\\n\\n    def create_or_update(self, results):\\n        parameters = self.nsg_models.NetworkSecurityGroup()\\n        if results.get(\'rules\'):\\n            parameters.security_rules = []\\n            for rule in results.get(\'rules\'):\\n                parameters.security_rules.append(create_rule_instance(self, rule))\\n        if results.get(\'default_rules\'):\\n            parameters.default_security_rules = []\\n            for rule in results.get(\'default_rules\'):\\n                parameters.default_security_rules.append(create_rule_instance(self, rule))\\n        parameters.tags = results.get(\'tags\')\\n        parameters.location = results.get(\'location\')\\n\\n        try:\\n            poller = self.network_client.network_security_groups.create_or_update(resource_group_name=self.resource_group,\\n                                                                                  network_security_group_name=self.name,\\n                                                                                  parameters=parameters)\\n            result = self.get_poller_result(poller)\\n        except CloudError as exc:\\n            self.fail(\\"Error creating/updating security group {0} - {1}\\".format(self.name, str(exc)))\\n        return create_network_security_group_dict(result)\\n\\n    def delete(self):\\n        try:\\n            poller = self.network_client.network_security_groups.delete(resource_group_name=self.resource_group, network_security_group_name=self.name)\\n            result = self.get_poller_result(poller)\\n        except CloudError as exc:\\n            raise Exception(\\"Error deleting security group {0} - {1}\\".format(self.name, str(exc)))\\n        return result\\n\\n\\ndef main():\\n    AzureRMSecurityGroup()\\n\\n\\nif __name__ == \'__main__\':\\n    main()\\n"}\n'
line: b'{ "repo_name": "jeffheaton/aifh", "ref": "refs/heads/master", "path": "vol1/r-examples/ch5/kmeans.R", "content": "## Artificial Intelligence for Humans\\n## Volume 1: Fundamental Algorithms\\n## R Version\\n## http://www.aifh.org\\n## http://www.jeffheaton.com\\n##\\n## Code repository:\\n## https://github.com/jeffheaton/aifh\\n##\\n## Copyright 2013 by Jeff Heaton\\n##\\n## Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n## you may not use this file except in compliance with the License.\\n## You may obtain a copy of the License at\\n##\\n##     http://www.apache.org/licenses/LICENSE-2.0\\n##\\n## Unless required by applicable law or agreed to in writing, software\\n## distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n## See the License for the specific language governing permissions and\\n## limitations under the License.\\n##\\n## For more information on Heaton Research copyrights, licenses\\n## and trademarks visit:\\n## http://www.heatonresearch.com/copyright\\n\\n## Chapter 5 Example: Random numbers\\n\\n## first load the iris data set\\nirisdata <- read.csv(file=\\"iris.csv\\",head=TRUE,sep=\\",\\")\\n\\n## perform the kmeans into 3 clusters, max iterations of 1000\\niris.kmeans <- kmeans(irisdata[, -5], 3, iter.max = 1000)\\n\\n## display the kmeans cluster as a table\\ntable(irisdata[, 5], iris.kmeans$cluster)\\n\\n# The iris data is 4-dimension.  To display that in a chart we must scale the\\n# dimensions down to just 2.\\niris.dist <- dist(iris[, -5])\\niris.mds <- cmdscale(iris.dist)\\n\\n# Ideal species assignments will be characters (from the iris data)\\nideal.chars <- c(\\"*\\", \\"o\\", \\"+\\")[as.integer(iris$Species)]\\n\\n# Actual cluster assignments will be colors (from the kmeans cluster)\\nactual.colors <- rainbow(3)[iris.kmeans$cluster]\\n\\n# You now plot\\nplot(iris.mds, col = actual.colors, pch = ideal.chars, xlab = \\"X\\", ylab = \\"Y\\")\\n\\n# You will notice items on the graph.  Their char type shows their real (ideal) species.\\n# The colors shows what cluster kMeans put them into.  You will notice that most errors\\n# occur right at the border between the two clusters on the right of the graph.  That is\\n# because there is no clearly defined border between them for kmeans to figure out on its \\n# own.\\n" }\n'
line: b'{ "repo_name": "jeffheaton/aifh", "ref": "refs/heads/master", "path": "vol1/r-examples/ch7/learnPoly.R", "content": "## Artificial Intelligence for Humans\\n## Volume 1: Fundamental Algorithms\\n## R Version\\n## http://www.aifh.org\\n## http://www.jeffheaton.com\\n##\\n## Code repository:\\n## https://github.com/jeffheaton/aifh\\n##\\n## Copyright 2013 by Jeff Heaton\\n##\\n## Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n## you may not use this file except in compliance with the License.\\n## You may obtain a copy of the License at\\n##\\n##     http://www.apache.org/licenses/LICENSE-2.0\\n##\\n## Unless required by applicable law or agreed to in writing, software\\n## distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n## See the License for the specific language governing permissions and\\n## limitations under the License.\\n##\\n## For more information on Heaton Research copyrights, licenses\\n## and trademarks visit:\\n## http://www.heatonresearch.com/copyright\\n\\n## Chapter 7 Example: Towards Machine Learning, Learn Polynomial w/ Greedy Random\\n## http://en.wikipedia.org/wiki/Polynomial\\n\\n# Calculate the error with Mean Square Error\\n# http://www.heatonresearch.com/wiki/Mean_Square_Error\\nmse <- function(a,b) {\\n  sum(( a - b )^2) /length(a)  \\n}\\n\\n##################################################################################\\n## Generate the input data for training.  We use the x integer values of \\n## -50 to 50 to train the polynomial over.\\n##################################################################################\\n\\n# Generate input data range\\ninputData = -50:50\\n\\n# Create a 2d matrix from the 1d vector\\ninput <- matrix(inputData,ncol=1,byrow=TRUE)\\n\\n# Convert into a data table\\ninput <- as.table(input)\\n\\n# Name the columns\\ncolnames(input) <- c(\\"x\\")\\n\\n# Name the rows, simply provide a index number for each.  Not really used.\\nrownames(input) = 1:nrow(input)\\n\\n##################################################################################\\n## Generate the ideal data for training.  We calculate the y value of the polynomial\\n## for each of the input values from the previous step.\\n##################################################################################\\n\\n# Generate input data range (2x^2 + 4x + 6)\\nidealData = (2*inputData)^2 + (4*inputData) + 6\\n\\n# Create a 2d matrix from the 1d vector\\nideal <- matrix(idealData,ncol=1,byrow=TRUE)\\n\\n# Convert into a data table\\nideal <- as.table(ideal)\\n\\n# Name the columns\\ncolnames(ideal) <- c(\\"y\\")\\n\\n# Name the rows, simply provide a index number for each.  Not really used.\\nrownames(ideal) = 1:nrow(ideal)\\n\\n# Calculate the polynomial from the coefficient\\ncalcPolynomial <- function(coef)\\n{\\n  actual <- (input[,1]*coef[1])^2 + (input[,1]*coef[2]) + coef[3]\\n  actual <- as.table(matrix(actual,ncol=1,byrow=TRUE))\\n  rownames(actual) = 1:nrow(actual)\\n  colnames(actual) <- c(\\"y\\")\\n  actual\\n}\\n\\n# Score the polynomial\\nscore <- function(coef) {\\n  actual <-calcPolynomial(coef)\\n  mse(actual,ideal)\\n}\\n\\n# Begin the iterations\\niteration <- 0\\nbestScore <- .Machine$double.xmax\\nupdate <- -1\\n\\nrepeat {\\n  newCoef <- runif(3,-10,10)\\n  s <- score(newCoef)\\n  \\n  # Greedy, only improve\\n  if( s<bestScore )\\n  {\\n    bestScore <- s\\n    coef <- newCoef\\n  }\\n  \\n  update <- update + 1\\n  \\n  if( update>=1000 ) \\n  {\\n    update<-0\\n    cat(\\"Iteration: \\", iteration, \\", Score: \\" , bestScore, \\"\\\\n\\")\\n    if( iteration>100000 ) {\\n      break\\n    }\\n  }\\n  \\n  iteration <- iteration + 1\\n}\\n\\ncoef\\ncat( coef[1],\\"x^2 + \\", coef[2], \\"x + \\", coef[3] ) \\n\\n" }\n'
line: b'{ "repo_name": "jeffheaton/aifh", "ref": "refs/heads/master", "path": "vol1/r-examples/ch9/tsp.R", "content": "## Artificial Intelligence for Humans\\n## Volume 1: Fundamental Algorithms\\n## R Version\\n## http://www.aifh.org\\n## http://www.jeffheaton.com\\n##\\n## Code repository:\\n## https://github.com/jeffheaton/aifh\\n##\\n## Copyright 2013 by Jeff Heaton\\n##\\n## Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n## you may not use this file except in compliance with the License.\\n## You may obtain a copy of the License at\\n##\\n##     http://www.apache.org/licenses/LICENSE-2.0\\n##\\n## Unless required by applicable law or agreed to in writing, software\\n## distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n## See the License for the specific language governing permissions and\\n## limitations under the License.\\n##\\n## For more information on Heaton Research copyrights, licenses\\n## and trademarks visit:\\n## http://www.heatonresearch.com/copyright\\n\\n## Chapter 9 Example: Discrete Optimization, Traveling Salesman\\n## \\n## http://xkcd.com/399/\\n## http://en.wikipedia.org/wiki/Travelling_salesman_problem\\n\\n# how many cities\\ncityCount <- 30\\n\\n# define cities to be in a circle\\nratio <- (2 * pi) / cityCount\\ncity.x <- (cos(1:cityCount*ratio) * 10) + 10\\ncity.y <- (sin(1:cityCount*ratio) * 10) + 10\\n\\n# define an initial, random, path\\npath <- sample(1:cityCount, cityCount, replace=F)\\n\\n# display a plot of the initial path\\nplot(city.x, city.y, xlim=c(0,20), ylim=c(0,20),asp = 1,xlab = \\"\\", ylab = \\"\\", axes = TRUE, main = \\"Traveling Salesman (before)\\")\\narrows(city.x[path[1:(cityCount-1)]],\\n  city.y[path[1:(cityCount-1)]],\\n  city.x[path[2:cityCount]],\\n  city.y[path[2:cityCount]],\\n  angle = 10, col = \\"blue\\")\\n\\n# Define a score function, this sumes the euclidean distance over the entire path\\ndistance <- function(path) {  \\n  path2 <- embed(path,2)\\n  sqrt(sum(  (city.x[path2[,1]]-city.x[path2[,2]])^2 +  (city.y[path2[,1]]-city.y[path2[,2]])^2  ))\\n}\\n\\n# move to neighbor solution, swap two cities on the path\\nmoveNeighbor <- function(path) {  \\n  idx <- seq(2, length(path)-1)\\n  changepoints <- sample(idx, size = 2, replace = FALSE)\\n  tmp <- path[changepoints[1]]\\n  path[changepoints[1]] <- path[changepoints[2]]\\n  path[changepoints[2]] <- tmp\\n  path\\n}\\n\\n# perform the simulated annealing\\nresult <- optim(path, distance, moveNeighbor, method = \\"SANN\\",\\n             control = list(maxit = 200000, temp = 10, tmax=20, trace = TRUE,\\n                            REPORT = 500) )\\n\\n# get the result path\\nresultPath <- result$par\\n\\n# plot the result path\\nplot(city.x, city.y, xlim=c(0,20), ylim=c(0,20),asp = 1,xlab = \\"\\", ylab = \\"\\", axes = TRUE, main = \\"Traveling Salesman (After)\\")\\narrows(city.x[resultPath[1:(cityCount-1)]],\\n       city.y[resultPath[1:(cityCount-1)]],\\n       city.x[resultPath[2:cityCount]],\\n       city.y[resultPath[2:cityCount]],\\n       angle = 10, length=0.15, col = \\"blue\\")\\n\\n\\n" }\n'
line: b'{ "repo_name": "tonyfischetti/sake", "ref": "refs/heads/master", "path": "functests/test1/dui-correlates.R", "content": "#!/usr/bin/rscript --vanilla\\n\\nrm(list=ls())\\n\\n\\n\\ndui.frame <- read.table(\\"duistats\xe2\x98\x8e.tsv\\", stringsAsFactors=FALSE, \\n                        sep=\'\\\\t\', header=TRUE)\\n\\nteen.frame <- read.csv(\\"teenstats.csv\\", stringsAsFactors=FALSE)\\n\\n\\n# dui frame to upper\\ndui.frame$State <- toupper(dui.frame$State)\\ndui.frame[9,1] <- \\"D.C.\\"\\n\\n# control for population size (the higher, the worse)\\ndui.frame$dui.score <- dui.frame$DUI.Arrests..2012. / dui.frame$Population.Size\\n\\n# rank states by fewest dui arrests by population\\ndui.frame <- dui.frame[ order(dui.frame$dui.score), ]\\ndui.frame$rank <- 1:nrow(dui.frame)\\n\\nnames(teen.frame)[1] <- \\"State\\"\\nnames(teen.frame)[2] <- \\"Percent.HS.Grad\\"\\ntest <- merge(dui.frame, teen.frame[,c(1,2)])\\n\\nplot(test$dui.score ~ test$Percent.HS.Grad)\\ndev.copy(png,\'correlation.png\')\\ndev.off()\\n\\nres <- lm(test$dui.score ~ test$Percent.HS.Grad)\\n\\nwrite(res$coefficients, \\"lmcoeffs.txt\\")\\n\\n" }\n'
line: b'{ "repo_name": "marchtaylor/sinkr", "ref": "refs/heads/master", "path": "R/pca_loocv.R", "content": "#\' Principal component analysis \\"leave-one-out\\" cross-validation\\n#\'\\n#\' @param X Matrix to be subjected to svd\\n#\' @param npc.max The maximum number of principal components to test. Default=ncol(X)\\n#\'\\n#\' @return Matrix of square error values for each element in X  \\n#\' \\n#\' @references \\\\url{http://stats.stackexchange.com/a/115477/10675}\\n#\' @export\\n#\'\\n#\' @examples\\n#\' \\n#\' X <- as.matrix(iris[,1:4])\\n#\' res <- pca_loocv(X)\\n#\' res2 <- lapply(res, colSums)\\n#\' res2\\n#\' \\n#\' COL <- 2:4\\n#\' LTY <- 1:3\\n#\' op <- par(mar=c(4,4,2,1), tcl=-0.25, mgp=c(2.5,0.5,0))\\n#\' for(i in seq(res)){\\n#\'   if(i==1) {\\n#\'     plot(res2[[i]], t=\\"n\\", ylim=range(unlist(res2)), \\n#\'      main=\\"iris\\", xlab=\\"n PCs\\", ylab=\\"PRESS\\")\\n#\'     grid()\\n#\'   } \\n#\'   lines(res2[[i]], t=\\"b\\", bg=c(NaN,COL[i])[(res2[[i]]==min(res2[[i]])) + 1],\\n#\'    col=COL[i], lty=LTY[i], pch=21)\\n#\' }\\n#\' legend(\\"topright\\", legend=c(\\"naive\\", \\"approximate\\", \\"pseudoinverse\\"),\\n#\'  col=COL, lty=LTY, pch=21, bty=\\"n\\")\\n#\' par(op)\\n#\' \\n#\' \\npca_loocv <- function(X, npc.max=ncol(X)){\\n  error1 <- matrix(NaN, nrow=dim(X)[1], ncol=min(dim(X)[2],npc.max))\\n  error2 <- matrix(NaN, nrow=dim(X)[1], ncol=min(dim(X)[2],npc.max))\\n  error3 <- matrix(NaN, nrow=dim(X)[1], ncol=min(dim(X)[2],npc.max))\\n  for(n in 1:dim(X)[1]){\\n    Xtrain = X[-n,]\\n    Xtrain = scale(Xtrain, center=TRUE, scale=FALSE)\\n    V = svd(Xtrain)$v\\n    Xtest = X[n,,drop = FALSE]\\n    Xtest = scale(Xtest, center=attr(Xtrain, \\"scaled:center\\"), scale=FALSE)\\n    for(j in 1:min(dim(V)[2],npc.max)){\\n        P = V[,1:j] %*% t(V[,1:j])\\n        err1 <- Xtest %*% (diag(length(diag(P))) - P)\\n        err2 <- Xtest %*% (diag(length(diag(P))) - P + diag(diag(P)))\\n        err3 <- array(NaN, dim=dim(Xtest))\\n        for(k in 1:dim(Xtest)[2]){\\n          proj = Xtest[,-k] %*% t(expmat(V[-k,1:j])) %*% t(V[,1:j])\\n          err3[k] = Xtest[k] - proj[k]\\n        }\\n        error1[n,j] <- sum(sqrt(err1^2))\\n        error2[n,j] <- sum(sqrt(err2^2))\\n        error3[n,j] <- sum(sqrt(err3^2))\\n    }\\n  }\\n  res <- list(\\n    error1=error1,\\n    error2=error2,\\n    error3=error3\\n  )\\n  return(res)\\n}\\n" }\n'
line: b'{ "repo_name": "pablo14/funModeling", "ref": "refs/heads/master", "path": "R/numbers.R", "content": "#\' @title Outliers Data Preparation\\n#\' @description\\n#\' Deal with outliers by setting an \'NA value\' or by \'stopping\' them at a certain. The parameters: \'top_percent\'/\'bottom_percent\' are used to consider a value as outlier.\\n#\' Setting NA is recommended when doing statistical analysis, parameter: type=\'set_na\'.\\n#\' Stopping is recommended when creating a predictive model without biasing the result due to outliers, parameter: type=\'stop\'.\\n#\' @param data data frame\\n#\' @param str_input string input variable (if empty, it runs for all numeric variable).\\n#\' @param top_percent value from 0 to 1, represents the highest X percentage of values to treat\\n#\' @param bottom_percent value from 0 to 1, represents the lowest X percentage of values to treat\\n#\' @param type can be \'stop\' or \'set_na\', in the first case the original variable is stopped at the desiered percentile, \'set_na\'  sets NA to the same values.\\n#\' @examples\\n#\' # Creating data frame with outliers\\n#\' set.seed(10)\\n#\' df=data.frame(var1=rchisq(1000,df = 1), var2=rnorm(1000))\\n#\' df=rbind(df, 1135, 2432) # forcing outliers\\n#\' df$id=as.character(seq(1:1002))\\n#\'\\n#\' # for var1: mean is ~ 4.56, and max 2432\\n#\' summary(df)\\n#\'\\n#\' ########################################################\\n#\' ### PREPARING OUTLIERS FOR DESCRIPTIVE STATISTICS\\n#\' ########################################################\\n#\'\\n#\' #### EXAMPLE 1: Removing top 1% for a single variable\\n#\' # checking the value for the top 1% of highest values (percentile 0.99), which is ~ 7.05\\n#\' quantile(df$var1, 0.99)\\n#\'\\n#\' # Setting type=\'set_na\' sets NA to the highest value)\\n#\' var1_treated=prep_outliers(data = df,  str_input = \'var1\', type=\'set_na\', top_percent  = 0.01)\\n#\'\\n#\' # now the mean (~ 0.94) is more accurate, and note that: 1st, median and 3rd quartiles remaining very similar to the original variable.\\n#\' summary(var1_treated)\\n#\'\\n#\' #### EXAMPLE 2: if \'str_input\' is missing, then it runs for all numeric variables (which have 3 or more distinct values).\\n#\' df_treated2=prep_outliers(data = df, type=\'set_na\', top_percent  = 0.01)\\n#\' summary(df_treated2)\\n#\'\\n#\' #### EXAMPLE 3: Removing top 1% (and bottom 1%) for \'N\' specific variables.\\n#\' vars_to_process=c(\'var1\', \'var2\')\\n#\' df_treated3=prep_outliers(data = df, str_input = vars_to_process, type=\'set_na\', bottom_percent = 0.01, top_percent  = 0.01)\\n#\' summary(df_treated3)\\n#\'\\n#\' ########################################################\\n#\' ### PREPARING OUTLIERS FOR PREDICTIVE MODELING\\n#\' ########################################################\\n#\'\\n#\' #### EXAMPLE 4: Stopping outliers at the top 1% value for all variables. For example if the top 1% has a value of 7, then all values above will be set to 7. Useful when modeling because outlier cases can be used.\\n#\' df_treated4=prep_outliers(data = df, top_percent  = 0.01, type=\'stop\')\\n\\n#\' @return A vector or data frame with the desired outlier transformation\\n#\' @export\\nprep_outliers <- function(data, str_input, type=c(\'stop\', \'set_na\'), top_percent, bottom_percent)\\n{\\n\\tif(!(type %in% c(\'stop\', \'set_na\', \'sigmoid\')))\\n\\t\\tstop(\\"Parameter \'type\' must be one \'stop\' or \'set_na\'\\")\\n\\n\\n\\tif(missing(str_input))\\n\\t\\tstr_input=give_me_num_vars(data)\\n\\n\\n\\t# #########################################################\\n\\t# ### Sigmoid procesing\\n\\t# #########################################################\\n\\t# if(type == \'sigmoid\')\\n\\t# {\\n\\t# \\tfor(i in 1:length(str_input))\\n\\t#   {\\n\\t#    \\tdata[, str_input[i]]=sigmoid(as.numeric(scale(data[, str_input[i]])))\\n\\t# \\t}\\n\\t# \\treturn(data)\\n\\t# }\\n\\n\\t#########################################################\\n\\t### Stopping and Setting NA processing\\n\\t#########################################################\\n\\t## If not sigmoid, then it\'s stop or set_na, thus it has to have top or bottom param.\\n\\tif(missing(top_percent) & missing(bottom_percent))\\n\\t\\tstop(\\"Parameters \'top_percent\' and \'bottom_percent\' cannot be missing at the same time\\")\\n\\n\\t## Logic for top value\\n\\tif(!missing(top_percent))\\n\\t{\\n\\t  for(i in 1:length(str_input))\\n\\t  {\\n\\t   \\ttop_value=round(quantile(data[,str_input[i]], probs=(1-top_percent), names=F, na.rm=T))\\n\\t   \\tdata[, str_input[i]][data[, str_input[i]]>top_value]=ifelse(type==\'stop\', top_value, NA)\\n\\t  }\\n\\t}\\n\\n\\t## Logic for bottom value\\n\\tif(!missing(bottom_percent))\\n\\t{\\n\\t  for(i in 1:length(str_input))\\n\\t  {\\n\\t   \\tbottom_value=round(quantile(data[,str_input[i]], probs=bottom_percent, names=F, na.rm=T))\\n\\t   \\tdata[, str_input[i]][data[, str_input[i]]<bottom_value]=ifelse(type==\'stop\', bottom_value, NA)\\n\\t  }\\n\\t}\\n\\n\\t## Return the input vector if only 1 var was desired, otherwise it returns all the data frame transformed\\n\\tif(length(str_input)==1) {\\n\\t\\treturn(data[, str_input[i]])\\n\\t} else {\\n \\t\\treturn(data)\\n\\t}\\n\\n}\\n\\n#\' @title Compare two vectors of keys\\n#\' @description Obtain correlation table of all variables that belongs to data against target variable\\n#\' @param data data frame\\n#\' @param str_target string variable to predict\\n#\' @examples\\n#\' v1=c(1,2,4)\\n#\' v2=c(1,2,5,6)\\n#\' res=compare_df(key_x=v1, key_y=v2)\\n#\' # Print the keys that didn\'t match\\n#\' res\\n#\' # Accessing the keys not present in\\n#\' @return Correlation index for all data input variable\\n#\' @export\\ncompare_df <- function(key_x, key_y)\\n{\\n\\t# key_x=v1;key_y=v2\\n  df_x=data.frame(key_x=key_x, flag_x=1)\\n  df_y=data.frame(key_y=key_y, flag_y=1)\\n\\n  df_x$key_x=as.character(df_x$key_x)\\n\\tdf_y$key_y=as.character(df_y$key_y)\\n\\n  merge_all=merge(df_x, df_y, by.x=\'key_x\', by.y=\'key_y\', all=T)\\n\\n  names(merge_all)[1]=\\"key\\"\\n\\n  merge_all_nona=merge_all[!is.na(merge_all$flag_x) & !is.na(merge_all$flag_y),]\\n\\n  not_in_x=merge_all[is.na(merge_all$flag_x),]\\n  not_in_y=merge_all[is.na(merge_all$flag_y),]\\n\\n  print(sprintf(\\"Coincident in both: %s\\", nrow(merge_all_nona)))\\n  print(sprintf(\\"Rows not present in X: %s\\", nrow(not_in_x)))\\n  print(sprintf(\\"Rows not present in Y: %s\\", nrow(not_in_y)))\\n\\n\\n  list_diff=list()\\n\\n  res=list(\\n    present_in_both=merge_all_nona$key,\\n    rows_not_in_X=not_in_x$key,\\n    rows_not_in_Y=not_in_y$key\\n  \\t)\\n\\n  return(res)\\n}\\n\\n#\' @title Correlation analyisis against target variable\\n#\' @description Obtain correlation table of all variables that belongs to data against target variable. Only numeric variables are analyzed.\\n#\' @param data data frame\\n#\' @param str_target string variable to predict\\n#\' @examples\\n#\' correlation_table(data=heart_disease, str_target=\\"has_heart_disease\\")\\n#\' @return Correlation index for all data input variable\\n#\' @export\\ncorrelation_table <- function(data, str_target)\\n{\\n\\tdata[, str_target]=as.numeric(data[, str_target])\\n\\n\\tdata=data[, c(give_me_num_vars(data, str_target), str_target)]\\n\\n  df_cor=as.data.frame(round(cor(data, use=\\"complete.obs\\"\\t),2))\\n  df_cor$Variable = rownames(df_cor)\\n  df_cor=df_cor[, names(df_cor) %in% c(str_target, \\"Variable\\")]\\n\\n  df_cor=df_cor[interp(~order(df_cor, -v) , v=as.name(str_target)),  ]\\n\\n  row.names(df_cor) = NULL\\n  df_cor=df_cor[, c(2,1)]\\n\\n  df_cor[order(-df_cor[,2]) , ]\\n\\n  return(df_cor)\\n}\\n\\n#\' @title Sigmoid function\\n#\' @description Sigmoid function, also known as logistic or s-shaped\\n#\' @param x numeric input vector\\n#\' @param a constant to multiply \'x\', default=1\\n#\' @examples\\n#\' sigmoid()\\n#\' @return vector transformed with sigmoid\\n#\' @export\\nsigmoid<-function(x, a=1)\\n{\\n\\tif(missing(a))\\n\\t\\ta=1\\n\\n\\ty = 1/(1 + exp(-a*x))\\n\\n\\treturn(y)\\n}\\n\\n#\' @title Transform a variable into the 0 to 1 range\\n#\' @description Range a variable into [0-1], assigning 0 to the min and 1 to the max of the input variable.\\n#\' @param x numeric input vector\\n#\' @examples\\n#\' range01(mtcars$cyl)\\n#\' @return vector ranged into 0-1\\n#\' @export\\nrange01 <- function(x)\\n{\\n\\treturn((x-min(x, na.rm=T))/(max(x, na.rm=T)-min(x, na.rm=T)))\\n}\\n" }\n'
line: b'{"repo_name":"srvg/ansible","ref":"refs/heads/devel","path":"lib/ansible/modules/network/netvisor/pn_vtep.py","content":"#!/usr/bin/python\\n# Copyright: (c) 2018, Pluribus Networks\\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\\n\\nfrom __future__ import absolute_import, division, print_function\\n__metaclass__ = type\\n\\n\\nANSIBLE_METADATA = {\'metadata_version\': \'1.1\',\\n                    \'status\': [\'preview\'],\\n                    \'supported_by\': \'community\'}\\n\\n\\nDOCUMENTATION = \\"\\"\\"\\n---\\nmodule: pn_vtep\\nauthor: \\"Pluribus Networks (@rajaspachipulusu17)\\"\\nversion_added: \\"2.9\\"\\nshort_description: CLI command to create/delete vtep\\ndescription:\\n  - This module can be used to create a vtep and delete a vtep.\\noptions:\\n  pn_cliswitch:\\n    description:\\n      - Target switch to run the CLI on.\\n    required: false\\n    type: str\\n  state:\\n    description:\\n      - vtep configuration command.\\n    required: false\\n    choices: [\'present\', \'absent\']\\n    type: str\\n    default: \'present\'\\n  pn_name:\\n    description:\\n      - vtep name.\\n    required: false\\n    type: str\\n  pn_ip:\\n    description:\\n      - Primary IP address.\\n    required: false\\n    type: str\\n  pn_vrouter_name:\\n    description:\\n      - name of the vrouter service.\\n    required: false\\n    type: str\\n  pn_virtual_ip:\\n    description:\\n      - Virtual/Secondary IP address.\\n    required: false\\n    type: str\\n  pn_location:\\n    description:\\n      - switch name.\\n    required: false\\n    type: str\\n  pn_switch_in_cluster:\\n    description:\\n      - Tells whether switch in cluster or not.\\n    required: false\\n    type: bool\\n    default: True\\n\\"\\"\\"\\n\\nEXAMPLES = \\"\\"\\"\\n- name: create vtep\\n  pn_vtep:\\n    pn_cliswitch: \'sw01\'\\n    pn_name: \'foo\'\\n    pn_vrouter_name: \'foo-vrouter\'\\n    pn_ip: \'22.22.22.2\'\\n    pn_location: \'sw01\'\\n    pn_virtual_ip: \\"22.22.22.1\\"\\n\\n- name: delete vtep\\n  pn_vtep:\\n    pn_cliswitch: \'sw01\'\\n    state: \'absent\'\\n    pn_name: \'foo\'\\n\\"\\"\\"\\n\\nRETURN = \\"\\"\\"\\ncommand:\\n  description: the CLI command run on the target node.\\n  returned: always\\n  type: str\\nstdout:\\n  description: set of responses from the vtep command.\\n  returned: always\\n  type: list\\nstderr:\\n  description: set of error responses from the vtep command.\\n  returned: on error\\n  type: list\\nchanged:\\n  description: indicates whether the CLI caused changes on the target.\\n  returned: always\\n  type: bool\\n\\"\\"\\"\\n\\nfrom ansible.module_utils.basic import AnsibleModule\\nfrom ansible.module_utils.network.netvisor.pn_nvos import pn_cli, run_cli\\nfrom ansible.module_utils.network.netvisor.netvisor import run_commands\\n\\n\\ndef check_cli(module, cli):\\n    \\"\\"\\"\\n    This method checks for idempotency using the vtep-show command.\\n    If a name exists, return True if name exists else False.\\n    :param module: The Ansible module to fetch input parameters\\n    :param cli: The CLI string\\n    \\"\\"\\"\\n    name = module.params[\'pn_name\']\\n\\n    cli += \' vtep-show format name no-show-headers\'\\n    out = run_commands(module, cli)[1]\\n\\n    if out:\\n        out = out.split()\\n\\n    return True if name in out else False\\n\\n\\ndef main():\\n    \\"\\"\\" This section is for arguments parsing \\"\\"\\"\\n\\n    state_map = dict(\\n        present=\'vtep-create\',\\n        absent=\'vtep-delete\'\\n    )\\n\\n    argument_spec = dict(\\n        pn_cliswitch=dict(required=False, type=\'str\'),\\n        state=dict(required=False, type=\'str\', choices=state_map.keys(), default=\'present\'),\\n        pn_name=dict(required=False, type=\'str\'),\\n        pn_ip=dict(required=False, type=\'str\'),\\n        pn_vrouter_name=dict(required=False, type=\'str\'),\\n        pn_virtual_ip=dict(required=False, type=\'str\'),\\n        pn_location=dict(required=False, type=\'str\'),\\n        pn_switch_in_cluster=dict(required=False, type=\'bool\', default=\'True\')\\n    )\\n\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        required_if=(\\n            [\\"state\\", \\"present\\", [\\"pn_name\\", \\"pn_ip\\", \\"pn_vrouter_name\\", \\"pn_location\\"]],\\n            [\\"state\\", \\"absent\\", [\\"pn_name\\"]],\\n        ),\\n    )\\n\\n    # Accessing the arguments\\n    cliswitch = module.params[\'pn_cliswitch\']\\n    state = module.params[\'state\']\\n    name = module.params[\'pn_name\']\\n    ip = module.params[\'pn_ip\']\\n    vrouter_name = module.params[\'pn_vrouter_name\']\\n    virtual_ip = module.params[\'pn_virtual_ip\']\\n    location = module.params[\'pn_location\']\\n    switch_in_cluster = module.params[\'pn_switch_in_cluster\']\\n\\n    if switch_in_cluster and not virtual_ip and state == \'present\':\\n        module.exit_json(\\n            failed=True,\\n            msg=\'virtual ip is required when switch is in cluster\'\\n        )\\n\\n    command = state_map[state]\\n\\n    # Building the CLI command string\\n    cli = pn_cli(module, cliswitch)\\n\\n    NAME_EXISTS = check_cli(module, cli)\\n\\n    cli += \' %s name %s \' % (command, name)\\n\\n    if command == \'vtep-delete\':\\n        if NAME_EXISTS is False:\\n            module.exit_json(\\n                skipped=True,\\n                msg=\'vtep with name %s does not exist\' % name\\n            )\\n\\n    if command == \'vtep-create\':\\n        if NAME_EXISTS is True:\\n            module.exit_json(\\n                skipped=True,\\n                msg=\'vtpe with name %s already exists\' % name\\n            )\\n\\n        cli += \'vrouter-name %s \' % vrouter_name\\n        cli += \'ip %s \' % ip\\n        cli += \'location %s \' % location\\n\\n        if virtual_ip:\\n            cli += \'virtual-ip %s \' % virtual_ip\\n\\n    run_cli(module, cli, state_map)\\n\\n\\nif __name__ == \'__main__\':\\n    main()\\n"}\n'
line: b'{"repo_name":"dragonpt/Kernel_3.4.67_KK_Wiko_DarkMoon","ref":"refs/heads/master","path":"tools/perf/scripts/python/futex-contention.py","content":"# futex contention\\n# (c) 2010, Arnaldo Carvalho de Melo \\u003cacme@redhat.com\\u003e\\n# Licensed under the terms of the GNU GPL License version 2\\n#\\n# Translation of:\\n#\\n# http://sourceware.org/systemtap/wiki/WSFutexContention\\n#\\n# to perf python scripting.\\n#\\n# Measures futex contention\\n\\nimport os, sys\\nsys.path.append(os.environ[\'PERF_EXEC_PATH\'] + \'/scripts/python/Perf-Trace-Util/lib/Perf/Trace\')\\nfrom Util import *\\n\\nprocess_names = {}\\nthread_thislock = {}\\nthread_blocktime = {}\\n\\nlock_waits = {} # long-lived stats on (tid,lock) blockage elapsed time\\nprocess_names = {} # long-lived pid-to-execname mapping\\n\\ndef syscalls__sys_enter_futex(event, ctxt, cpu, s, ns, tid, comm,\\n\\t\\t\\t      nr, uaddr, op, val, utime, uaddr2, val3):\\n\\tcmd = op \\u0026 FUTEX_CMD_MASK\\n\\tif cmd != FUTEX_WAIT:\\n\\t\\treturn # we don\'t care about originators of WAKE events\\n\\n\\tprocess_names[tid] = comm\\n\\tthread_thislock[tid] = uaddr\\n\\tthread_blocktime[tid] = nsecs(s, ns)\\n\\ndef syscalls__sys_exit_futex(event, ctxt, cpu, s, ns, tid, comm,\\n\\t\\t\\t     nr, ret):\\n\\tif thread_blocktime.has_key(tid):\\n\\t\\telapsed = nsecs(s, ns) - thread_blocktime[tid]\\n\\t\\tadd_stats(lock_waits, (tid, thread_thislock[tid]), elapsed)\\n\\t\\tdel thread_blocktime[tid]\\n\\t\\tdel thread_thislock[tid]\\n\\ndef trace_begin():\\n\\tprint \\"Press control+C to stop and show the summary\\"\\n\\ndef trace_end():\\n\\tfor (tid, lock) in lock_waits:\\n\\t\\tmin, max, avg, count = lock_waits[tid, lock]\\n\\t\\tprint \\"%s[%d] lock %x contended %d times, %d avg ns\\" % \\\\\\n\\t\\t      (process_names[tid], tid, lock, count, avg)\\n\\n"}\n'
line: b'{"repo_name":"bert9bert/statsmodels","ref":"refs/heads/master","path":"statsmodels/sandbox/nonparametric/kernel_extras.py","content":"\\"\\"\\"\\nMultivariate Conditional and Unconditional Kernel Density Estimation\\nwith Mixed Data Types\\n\\nReferences\\n----------\\n[1] Racine, J., Li, Q. Nonparametric econometrics: theory and practice.\\n    Princeton University Press. (2007)\\n[2] Racine, Jeff. \\"Nonparametric Econometrics: A Primer,\\" Foundation\\n    and Trends in Econometrics: Vol 3: No 1, pp1-88. (2008)\\n    http://dx.doi.org/10.1561/0800000009\\n[3] Racine, J., Li, Q. \\"Nonparametric Estimation of Distributions\\n    with Categorical and Continuous Data.\\" Working Paper. (2000)\\n[4] Racine, J. Li, Q. \\"Kernel Estimation of Multivariate Conditional\\n    Distributions Annals of Economics and Finance 5, 211-235 (2004)\\n[5] Liu, R., Yang, L. \\"Kernel estimation of multivariate\\n    cumulative distribution function.\\"\\n    Journal of Nonparametric Statistics (2008)\\n[6] Li, R., Ju, G. \\"Nonparametric Estimation of Multivariate CDF\\n    with Categorical and Continuous Data.\\" Working Paper\\n[7] Li, Q., Racine, J. \\"Cross-validated local linear nonparametric\\n    regression\\" Statistica Sinica 14(2004), pp. 485-512\\n[8] Racine, J.: \\"Consistent Significance Testing for Nonparametric\\n        Regression\\" Journal of Business \\u0026 Economics Statistics\\n[9] Racine, J., Hart, J., Li, Q., \\"Testing the Significance of\\n        Categorical Predictor Variables in Nonparametric Regression\\n        Models\\", 2006, Econometric Reviews 25, 523-544\\n\\n\\"\\"\\"\\n\\n# TODO: make default behavior efficient=True above a certain n_obs\\n\\nfrom statsmodels.compat.python import range, next\\nimport numpy as np\\nfrom scipy import optimize\\nfrom scipy.stats.mstats import mquantiles\\n\\nfrom statsmodels.nonparametric.api import KDEMultivariate, KernelReg\\nfrom statsmodels.nonparametric._kernel_base import \\\\\\n    gpke, LeaveOneOut, _get_type_pos, _adjust_shape\\n\\n\\n__all__ = [\'SingleIndexModel\', \'SemiLinear\', \'TestFForm\']\\n\\n\\nclass TestFForm(object):\\n    \\"\\"\\"\\n    Nonparametric test for functional form.\\n\\n    Parameters\\n    ----------\\n    endog: list\\n        Dependent variable (training set)\\n    exog: list of array_like objects\\n        The independent (right-hand-side) variables\\n    bw: array_like, str\\n        Bandwidths for exog or specify method for bandwidth selection\\n    fform: function\\n        The functional form ``y = g(b, x)`` to be tested. Takes as inputs\\n        the RHS variables `exog` and the coefficients ``b`` (betas)\\n        and returns a fitted ``y_hat``.\\n    var_type: str\\n        The type of the independent `exog` variables:\\n\\n            - c: continuous\\n            - o: ordered\\n            - u: unordered\\n\\n    estimator: function\\n        Must return the estimated coefficients b (betas). Takes as inputs\\n        ``(endog, exog)``.  E.g. least square estimator::\\n\\n            lambda (x,y): np.dot(np.pinv(np.dot(x.T, x)), np.dot(x.T, y))\\n\\n    References\\n    ----------\\n    See Racine, J.: \\"Consistent Significance Testing for Nonparametric\\n    Regression\\" Journal of Business \\\\\\u0026 Economics Statistics.\\n\\n    See chapter 12 in [1]  pp. 355-357.\\n\\n    \\"\\"\\"\\n    def __init__(self, endog, exog, bw, var_type, fform, estimator, nboot=100):\\n        self.endog = endog\\n        self.exog = exog\\n        self.var_type = var_type\\n        self.fform = fform\\n        self.estimator = estimator\\n        self.nboot = nboot\\n        self.bw = KDEMultivariate(exog, bw=bw, var_type=var_type).bw\\n        self.sig = self._compute_sig()\\n\\n    def _compute_sig(self):\\n        Y = self.endog\\n        X = self.exog\\n        b = self.estimator(Y, X)\\n        m = self.fform(X, b)\\n        n = np.shape(X)[0]\\n        resid = Y - m\\n        resid = resid - np.mean(resid)  # center residuals\\n        self.test_stat = self._compute_test_stat(resid)\\n        sqrt5 = np.sqrt(5.)\\n        fct1 = (1 - sqrt5) / 2.\\n        fct2 = (1 + sqrt5) / 2.\\n        u1 = fct1 * resid\\n        u2 = fct2 * resid\\n        r = fct2 / sqrt5\\n        I_dist = np.empty((self.nboot,1))\\n        for j in range(self.nboot):\\n            u_boot = u2.copy()\\n\\n            prob = np.random.uniform(0,1, size = (n,))\\n            ind = prob \\u003c r\\n            u_boot[ind] = u1[ind]\\n            Y_boot = m + u_boot\\n            b_hat = self.estimator(Y_boot, X)\\n            m_hat = self.fform(X, b_hat)\\n            u_boot_hat = Y_boot - m_hat\\n            I_dist[j] = self._compute_test_stat(u_boot_hat)\\n\\n        self.boots_results = I_dist\\n        sig = \\"Not Significant\\"\\n        if self.test_stat \\u003e mquantiles(I_dist, 0.9):\\n            sig = \\"*\\"\\n        if self.test_stat \\u003e mquantiles(I_dist, 0.95):\\n            sig = \\"**\\"\\n        if self.test_stat \\u003e mquantiles(I_dist, 0.99):\\n            sig = \\"***\\"\\n        return sig\\n\\n    def _compute_test_stat(self, u):\\n        n = np.shape(u)[0]\\n        XLOO = LeaveOneOut(self.exog)\\n        uLOO = LeaveOneOut(u[:,None]).__iter__()\\n        I = 0\\n        S2 = 0\\n        for i, X_not_i in enumerate(XLOO):\\n            u_j = next(uLOO)\\n            u_j = np.squeeze(u_j)\\n            # See Bootstrapping procedure on p. 357 in [1]\\n            K = gpke(self.bw, data=-X_not_i, data_predict=-self.exog[i, :],\\n                     var_type=self.var_type, tosum=False)\\n            f_i = (u[i] * u_j * K)\\n            assert u_j.shape == K.shape\\n            I += f_i.sum()  # See eq. 12.7 on p. 355 in [1]\\n            S2 += (f_i**2).sum()  # See Theorem 12.1 on p.356 in [1]\\n            assert np.size(I) == 1\\n            assert np.size(S2) == 1\\n\\n        I *= 1. / (n * (n - 1))\\n        ix_cont = _get_type_pos(self.var_type)[0]\\n        hp = self.bw[ix_cont].prod()\\n        S2 *= 2 * hp / (n * (n - 1))\\n        T = n * I * np.sqrt(hp / S2)\\n        return T\\n\\n\\nclass SingleIndexModel(KernelReg):\\n    \\"\\"\\"\\n    Single index semiparametric model ``y = g(X * b) + e``.\\n\\n    Parameters\\n    ----------\\n    endog: array_like\\n        The dependent variable\\n    exog: array_like\\n        The independent variable(s)\\n    var_type: str\\n        The type of variables in X:\\n\\n            - c: continuous\\n            - o: ordered\\n            - u: unordered\\n\\n    Attributes\\n    ----------\\n    b: array_like\\n        The linear coefficients b (betas)\\n    bw: array_like\\n        Bandwidths\\n\\n    Methods\\n    -------\\n    fit(): Computes the fitted values ``E[Y|X] = g(X * b)``\\n           and the marginal effects ``dY/dX``.\\n\\n    References\\n    ----------\\n    See chapter on semiparametric models in [1]\\n\\n    Notes\\n    -----\\n    This model resembles the binary choice models. The user knows\\n    that X and b interact linearly, but ``g(X * b)`` is unknown.\\n    In the parametric binary choice models the user usually assumes\\n    some distribution of g() such as normal or logistic.\\n\\n    \\"\\"\\"\\n    def __init__(self, endog, exog, var_type):\\n        self.var_type = var_type\\n        self.K = len(var_type)\\n        self.var_type = self.var_type[0]\\n        self.endog = _adjust_shape(endog, 1)\\n        self.exog = _adjust_shape(exog, self.K)\\n        self.nobs = np.shape(self.exog)[0]\\n        self.data_type = self.var_type\\n        self.func = self._est_loc_linear\\n\\n        self.b, self.bw = self._est_b_bw()\\n\\n    def _est_b_bw(self):\\n        params0 = np.random.uniform(size=(self.K + 1, ))\\n        b_bw = optimize.fmin(self.cv_loo, params0, disp=0)\\n        b = b_bw[0:self.K]\\n        bw = b_bw[self.K:]\\n        bw = self._set_bw_bounds(bw)\\n        return b, bw\\n\\n    def cv_loo(self, params):\\n        # See p. 254 in Textbook\\n        params = np.asarray(params)\\n        b = params[0 : self.K]\\n        bw = params[self.K:]\\n        LOO_X = LeaveOneOut(self.exog)\\n        LOO_Y = LeaveOneOut(self.endog).__iter__()\\n        L = 0\\n        for i, X_not_i in enumerate(LOO_X):\\n            Y = next(LOO_Y)\\n            #print b.shape, np.dot(self.exog[i:i+1, :], b).shape, bw,\\n            G = self.func(bw, endog=Y, exog=-np.dot(X_not_i, b)[:,None],\\n                          #data_predict=-b*self.exog[i, :])[0]\\n                          data_predict=-np.dot(self.exog[i:i+1, :], b))[0]\\n            #print G.shape\\n            L += (self.endog[i] - G) ** 2\\n\\n        # Note: There might be a way to vectorize this. See p.72 in [1]\\n        return L / self.nobs\\n\\n    def fit(self, data_predict=None):\\n        if data_predict is None:\\n            data_predict = self.exog\\n        else:\\n            data_predict = _adjust_shape(data_predict, self.K)\\n\\n        N_data_predict = np.shape(data_predict)[0]\\n        mean = np.empty((N_data_predict,))\\n        mfx = np.empty((N_data_predict, self.K))\\n        for i in range(N_data_predict):\\n            mean_mfx = self.func(self.bw, self.endog,\\n                                 np.dot(self.exog, self.b)[:,None],\\n                                 data_predict=np.dot(data_predict[i:i+1, :],self.b))\\n            mean[i] = mean_mfx[0]\\n            mfx_c = np.squeeze(mean_mfx[1])\\n            mfx[i, :] = mfx_c\\n\\n        return mean, mfx\\n\\n    def __repr__(self):\\n        \\"\\"\\"Provide something sane to print.\\"\\"\\"\\n        repr = \\"Single Index Model \\\\n\\"\\n        repr += \\"Number of variables: K = \\" + str(self.K) + \\"\\\\n\\"\\n        repr += \\"Number of samples:   nobs = \\" + str(self.nobs) + \\"\\\\n\\"\\n        repr += \\"Variable types:      \\" + self.var_type + \\"\\\\n\\"\\n        repr += \\"BW selection method: cv_ls\\" + \\"\\\\n\\"\\n        repr += \\"Estimator type: local constant\\" + \\"\\\\n\\"\\n        return repr\\n\\n\\nclass SemiLinear(KernelReg):\\n    \\"\\"\\"\\n    Semiparametric partially linear model, ``Y = Xb + g(Z) + e``.\\n\\n    Parameters\\n    ----------\\n    endog: array_like\\n        The dependent variable\\n    exog: array_like\\n        The linear component in the regression\\n    exog_nonparametric: array_like\\n        The nonparametric component in the regression\\n    var_type: str\\n        The type of the variables in the nonparametric component;\\n\\n            - c: continuous\\n            - o: ordered\\n            - u: unordered\\n\\n    k_linear : int\\n        The number of variables that comprise the linear component.\\n\\n    Attributes\\n    ----------\\n    bw: array_like\\n        Bandwidths for the nonparametric component exog_nonparametric\\n    b: array_like\\n        Coefficients in the linear component\\n    nobs : int\\n        The number of observations.\\n    k_linear : int\\n        The number of variables that comprise the linear component.\\n\\n    Methods\\n    -------\\n    fit(): Returns the fitted mean and marginal effects dy/dz\\n\\n    Notes\\n    -----\\n    This model uses only the local constant regression estimator\\n\\n    References\\n    ----------\\n    See chapter on Semiparametric Models in [1]\\n    \\"\\"\\"\\n\\n    def __init__(self, endog, exog, exog_nonparametric, var_type, k_linear):\\n        self.endog = _adjust_shape(endog, 1)\\n        self.exog = _adjust_shape(exog, k_linear)\\n        self.K = len(var_type)\\n        self.exog_nonparametric = _adjust_shape(exog_nonparametric, self.K)\\n        self.k_linear = k_linear\\n        self.nobs = np.shape(self.exog)[0]\\n        self.var_type = var_type\\n        self.data_type = self.var_type\\n        self.func = self._est_loc_linear\\n\\n        self.b, self.bw = self._est_b_bw()\\n\\n    def _est_b_bw(self):\\n        \\"\\"\\"\\n        Computes the (beta) coefficients and the bandwidths.\\n\\n        Minimizes ``cv_loo`` with respect to ``b`` and ``bw``.\\n        \\"\\"\\"\\n        params0 = np.random.uniform(size=(self.k_linear + self.K, ))\\n        b_bw = optimize.fmin(self.cv_loo, params0, disp=0)\\n        b = b_bw[0 : self.k_linear]\\n        bw = b_bw[self.k_linear:]\\n        #bw = self._set_bw_bounds(np.asarray(bw))\\n        return b, bw\\n\\n    def cv_loo(self, params):\\n        \\"\\"\\"\\n        Similar to the cross validation leave-one-out estimator.\\n\\n        Modified to reflect the linear components.\\n\\n        Parameters\\n        ----------\\n        params: array_like\\n            Vector consisting of the coefficients (b) and the bandwidths (bw).\\n            The first ``k_linear`` elements are the coefficients.\\n\\n        Returns\\n        -------\\n        L: float\\n            The value of the objective function\\n\\n        References\\n        ----------\\n        See p.254 in [1]\\n        \\"\\"\\"\\n        params = np.asarray(params)\\n        b = params[0 : self.k_linear]\\n        bw = params[self.k_linear:]\\n        LOO_X = LeaveOneOut(self.exog)\\n        LOO_Y = LeaveOneOut(self.endog).__iter__()\\n        LOO_Z = LeaveOneOut(self.exog_nonparametric).__iter__()\\n        Xb = np.dot(self.exog, b)[:,None]\\n        L = 0\\n        for ii, X_not_i in enumerate(LOO_X):\\n            Y = next(LOO_Y)\\n            Z = next(LOO_Z)\\n            Xb_j = np.dot(X_not_i, b)[:,None]\\n            Yx = Y - Xb_j\\n            G = self.func(bw, endog=Yx, exog=-Z,\\n                          data_predict=-self.exog_nonparametric[ii, :])[0]\\n            lt = Xb[ii, :] #.sum()  # linear term\\n            L += (self.endog[ii] - lt - G) ** 2\\n\\n        return L\\n\\n    def fit(self, exog_predict=None, exog_nonparametric_predict=None):\\n        \\"\\"\\"Computes fitted values and marginal effects\\"\\"\\"\\n\\n        if exog_predict is None:\\n            exog_predict = self.exog\\n        else:\\n            exog_predict = _adjust_shape(exog_predict, self.k_linear)\\n\\n        if exog_nonparametric_predict is None:\\n            exog_nonparametric_predict = self.exog_nonparametric\\n        else:\\n            exog_nonparametric_predict = _adjust_shape(exog_nonparametric_predict, self.K)\\n\\n        N_data_predict = np.shape(exog_nonparametric_predict)[0]\\n        mean = np.empty((N_data_predict,))\\n        mfx = np.empty((N_data_predict, self.K))\\n        Y = self.endog - np.dot(exog_predict, self.b)[:,None]\\n        for i in range(N_data_predict):\\n            mean_mfx = self.func(self.bw, Y, self.exog_nonparametric,\\n                                 data_predict=exog_nonparametric_predict[i, :])\\n            mean[i] = mean_mfx[0]\\n            mfx_c = np.squeeze(mean_mfx[1])\\n            mfx[i, :] = mfx_c\\n\\n        return mean, mfx\\n\\n    def __repr__(self):\\n        \\"\\"\\"Provide something sane to print.\\"\\"\\"\\n        repr = \\"Semiparamatric Partially Linear Model \\\\n\\"\\n        repr += \\"Number of variables: K = \\" + str(self.K) + \\"\\\\n\\"\\n        repr += \\"Number of samples:   N = \\" + str(self.nobs) + \\"\\\\n\\"\\n        repr += \\"Variable types:      \\" + self.var_type + \\"\\\\n\\"\\n        repr += \\"BW selection method: cv_ls\\" + \\"\\\\n\\"\\n        repr += \\"Estimator type: local constant\\" + \\"\\\\n\\"\\n        return repr\\n"}\n'
line: b'{ "repo_name": "zozlak/ZPD", "ref": "refs/heads/master", "path": "R/pobierz_schematy_odp.R", "content": "#\' @title Pobiera dystraktory kryteri\xc3\xb3w oceny\\r\\n#\' @description\\r\\n#\' W wypadku pobierania z bazy wynik\xc3\xb3w w postaci niewypunktowanej wybrana przez \\r\\n#\' ucznia odpowied\xc5\xba zakodowana jest liczbowo. Dane pobierane funkcj\xc4\x85 \\r\\n#\' pobierz_schematy_odp() pozwalaj\xc4\x85 przekodowa\xc4\x87 je na faktyczne oznaczenia u\xc5\xbcyte\\r\\n#\' w te\xc5\x9bcie.\\r\\n#\' \\r\\n#\' Innym zastosowaniem mo\xc5\xbce by\xc4\x87 sprawdzanie, czy zbi\xc3\xb3r danych z wynikami testu \\r\\n#\' nie zawiera warto\xc5\x9bci spoza mo\xc5\xbcliwych do wyboru dla danego zadania odpowiedzi.\\r\\n#\' @param src uchwyt \xc5\xbar\xc3\xb3d\xc5\x82a danych dplyr-a \\r\\n#\' @import dplyr\\r\\n#\' @export\\r\\npobierz_schematy_odp = function(\\r\\n  src\\r\\n){\\r\\n  stopifnot(is.src(src))\\r\\n  \\r\\n  query = \\"\\r\\n    SELECT \'k_\' || id_kryterium AS kryterium, dystraktor, kolejnosc AS kolejnosc_dystr\\r\\n    FROM \\r\\n      pytania \\r\\n      JOIN kryteria_oceny USING (id_pytania) \\r\\n      JOIN sl_schematy_odp_dystr USING (schemat_odp)\\r\\n    ORDER BY 1, 3\\r\\n  \\"\\r\\n  data = tbl(src, sql(e(query)))\\r\\n  return(data)\\r\\n}\\r\\nattr(pobierz_schematy_odp, \'grupa\') = \'kryteriaOceny\'\\r\\n" }\n'
line: b'{ "repo_name": "zozlak/ZPD", "ref": "refs/heads/master", "path": "tests/testthat/test-normalizuj.R", "content": "context(\'normalizuj\')\\n\\nsrc = polacz()\\n\\ntest_that(\'normalizuj dzia\xc5\x82a\', {\\n  dane = data.frame(wynik = rep(1:40, 50))\\n  \\n  norm = normalizuj(dane)\\n  \\n  expect_equal(norm$wynik, dane$wynik)\\n  expect_less_than(abs(mean(norm$wynik_norm) - 100), 1)\\n  expect_less_than(abs(median(norm$wynik_norm) - 100), 1)\\n  expect_less_than(abs(sd(norm$wynik_norm) - 15), 1)\\n  \\n  skale = pobierz_skale(src) %>% \\n    filter(posiada_normy == T, rodzaj_egzaminu == \'sprawdzian\', rok == 2010, rodzaj_skali == \'zr\xc3\xb3wnywanie\') %>% \\n    select(id_skali, skalowanie, grupa) %>%\\n    distinct() %>%\\n    collect()\\n  norm = normalizuj(dane, src, idSkali = skale$id_skali[1], skalowanie = skale$skalowanie[1], grupa = skale$grupa[1])\\n  \\n  expect_equal(norm$wynik, dane$wynik)\\n  expect_more_than(min(norm$wynik_norm), 0)\\n  expect_less_than(max(norm$wynik_norm), 40.001)\\n  expect_less_than(sd(norm$wynik_norm), 15)\\n})" }\n'
line: b'{ "repo_name": "zozlak/ZPD", "ref": "refs/heads/master", "path": "R/pobierz_wyniki_zrownywania.R", "content": "#\' @title Pobiera ramke danych z wynikami egzaminacyjnymi testow zrownujacych\\r\\n#\' @param src uchwyt \xc5\xbar\xc3\xb3d\xc5\x82a danych dplyr-a\\r\\n#\' @param rodzajEgzaminu rodzaj egzaminu, ktorego wyniki maja zostac pobrane\\r\\n#\' @param punktuj wybor, czy dane maja byc pobrane w postaci dystraktorow, czy punktow\\r\\n#\' @param rok rok, z ktorego dane maja zostac pobrane\\r\\n#\' @param idSkali identyfikator skali, ktora ma zostac zastosowana do danych\\r\\n#\' @param skroc czy do danych zastosowac skrocenia skal opisane w skali\\r\\n#\' @import dplyr\\r\\n#\' @export\\r\\npobierz_wyniki_zrownywania = function(\\r\\n  src,\\r\\n\\trodzajEgzaminu, \\r\\n\\trok, \\r\\n\\tpunktuj = TRUE, \\r\\n\\tidSkali = NULL,\\r\\n\\tskroc   = TRUE\\r\\n){\\r\\n  stopifnot(\\r\\n    is.src(src),\\r\\n    is.vector(rodzajEgzaminu), is.character(rodzajEgzaminu), length(rodzajEgzaminu) == 1,\\r\\n    is.vector(rok), is.numeric(rok), length(rok) == 1, \\r\\n    is.vector(punktuj), is.logical(punktuj), length(punktuj) == 1, punktuj %in% c(T, F),\\r\\n    is.null(idSkali) | is.vector(idSkali) & is.numeric(idSkali) & length(idSkali) == 1,\\r\\n    is.vector(skroc), is.logical(skroc), length(skroc) == 1, skroc %in% c(T, F)\\r\\n  )\\r\\n  \\r\\n  regExp = e(paste0(\'^zr\xc3\xb3wnywanie;\', rodzajEgzaminu, \';\', rok, \';.*$\'))\\r\\n\\ttests = pobierz_testy(src) %>% \\r\\n    collect() %>%\\r\\n    filter_(~grepl(regExp, opis_testu))\\r\\n\\tif(nrow(tests) == 0){\\r\\n\\t\\tstop(e(\'w bazie nie ma takiego zrownywania\'))\\r\\n\\t}\\r\\n\\r\\n\\ttmpName = sub(\'[.]\', \'_\', paste0(\'t\', as.numeric(Sys.time(), runif(1))))\\r\\n\\tDBI::dbGetQuery(src$con, e(paste0(\\"CREATE TEMPORARY VIEW \\", tmpName, \\" AS SELECT 1\\")))\\r\\n\\tquery = sprintf(\\r\\n    \\"SELECT zbuduj_widok_zrownywania(%s, %s, %d, %s, %s, %s, true)\\",\\r\\n    escape(tmpName),\\r\\n    escape(rodzajEgzaminu),\\r\\n    rok,\\r\\n    ifelse(punktuj, \'true\', \'false\'),\\r\\n    ifelse(is.null(idSkali), \'null\', as.numeric(idSkali)),\\r\\n    ifelse(skroc, \'true\', \'false\')\\r\\n  )\\r\\n\\tDBI::dbGetQuery(src$con, e(query))\\r\\n\\tdata = tbl(src, sql(e(paste0(\\"SELECT * FROM \\", tmpName))))\\r\\n\\r\\n\\tattr(data, \'idSkali\') = idSkali\\r\\n  \\r\\n\\treturn(data)\\r\\n}\\r\\nattr(pobierz_wyniki_zrownywania, \'grupa\') = \'wyniki\'\\r\\nattr(pobierz_wyniki_zrownywania, \'testArgs\') = list(\\r\\n  \'rodzajEgzaminu\' = \'sprawdzian\', \'rok\' = 2013, \'idSkali\' = 41\\r\\n)\\r\\n" }\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Decimal-to-binary.R","content":"# Program to convert decimal number into binary number using recursive function\\nconvert_to_binary <- function(n) {\\n  if(n > 1) {\\n    convert_to_binary(as.integer(n/2))\\n  }\\n  cat(n %% 2)\\n}\\n\\nconvert_to_binary(52)"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Factorial.R","content":"# take input from the user\\nfactorial <- function(n) {\\nfactorial = 1\\n# check is the number is negative, positive or zero\\nif(num < 0) {\\n  print(\'Sorry factorial does not exist for negative numbers\')\\n} else if(num == 0) {\\n  print(\'The factorial of 0 is 1\')\\n} else {\\n  for(i in 1:num) {\\n    factorial = factorial * i\\n  }\\n  print(paste(\'The factorial of\', num ,\'is\',factorial))\\n}\\n}\\n\\nfactorial(4)"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/First-10-Fibonacci.R","content":"fibonacci <- function(n) {\\nFibonacci <- numeric(n)\\nFibonacci[1] <- Fibonacci[2] <- 1\\nfor (i in 3:10) Fibonacci[i] <- Fibonacci[i - 2] + Fibonacci[i - 1]\\nprint(\'First 10 Fibonacci numbers:\'\')\\nprint(Fibonacci)\\n}\\n\\nfibonacci(10)"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Max-Min-of-Vector.R","content":"max_min_vector <- function() {\\nnums = c(10, 20, 30, 40, 50, 60)\\nprint(\'Original vector:\')\\nprint(nums)   \\nprint(paste(\'Maximum value of the said vector:\',max(nums)))\\nprint(paste(\'Minimum value of the said vector:\',min(nums)))\\n}\\n\\nmax_min_vector()"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Odd-or-Even.R","content":"# Program to check if the input number is odd or even.\\n# A number is even if division by 2 give a remainder of 0.\\n# If remainder is 1, it is odd.\\nodd_or_even <- function(n){\\nnum = n\\nif((num %% 2) == 0) {\\n  print(paste(num,\'is Even\'))\\n} else {\\n  print(paste(num,\'is Odd\'))\\n}\\n}\\n\\nodd_or_even(4)"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Prime-Numbers.R","content":"prime_numbers <- function(n) {\\n  if (n >= 2) {\\n    x = seq(2, n)\\n    prime_nums = c()\\n    for (i in seq(2, n)) {\\n      if (any(x == i)) {\\n        prime_nums = c(prime_nums, i)\\n        x = c(x[(x %% i) != 0], i)\\n      }\\n    }\\n    return(prime_nums)\\n  }\\n  else \\n  {\\n    stop(\'Input number should be at least 2.\'\')\\n  }\\n} \\nprime_numbers(12)"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Read-csv-file.R","content":"read_csv_file <- function() {\\nmovie_data = read.csv(file=movies.csv, header=TRUE, sep=\',\')\\nprint(\'Content of the .csv file:\'\')\\nprint(movie_data)\\n}\\n\\nread_csv_file()"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/add1.r","content":"add1 <- function(n) {\\n   print( n+1)\\n}\\n\\nadd1(90)\\n"}\n'
line: b'{"repo_name":"peonycredit/peonycredit","ref":"refs/heads/master","path":"qa/pull-tester/pull-tester.py","content":"#!/usr/bin/python\\n# Copyright (c) 2013 The Bitcoin Core developers\\n# Distributed under the MIT/X11 software license, see the accompanying\\n# file COPYING or http://www.opensource.org/licenses/mit-license.php.\\n#\\nimport json\\nfrom urllib import urlopen\\nimport requests\\nimport getpass\\nfrom string import Template\\nimport sys\\nimport os\\nimport subprocess\\n\\nclass RunError(Exception):\\n    def __init__(self, value):\\n        self.value = value\\n    def __str__(self):\\n        return repr(self.value)\\n\\ndef run(command, **kwargs):\\n    fail_hard = kwargs.pop(\\"fail_hard\\", True)\\n    # output to /dev/null by default:\\n    kwargs.setdefault(\\"stdout\\", open(\'/dev/null\', \'w\'))\\n    kwargs.setdefault(\\"stderr\\", open(\'/dev/null\', \'w\'))\\n    command = Template(command).substitute(os.environ)\\n    if \\"TRACE\\" in os.environ:\\n        if \'cwd\' in kwargs:\\n            print(\\"[cwd=%s] %s\\"%(kwargs[\'cwd\'], command))\\n        else: print(command)\\n    try:\\n        process = subprocess.Popen(command.split(\' \'), **kwargs)\\n        process.wait()\\n    except KeyboardInterrupt:\\n        process.terminate()\\n        raise\\n    if process.returncode != 0 and fail_hard:\\n        raise RunError(\\"Failed: \\"+command)\\n    return process.returncode\\n\\ndef checkout_pull(clone_url, commit, out):\\n    # Init\\n    build_dir=os.environ[\\"BUILD_DIR\\"]\\n    run(\\"umount ${CHROOT_COPY}/proc\\", fail_hard=False)\\n    run(\\"rsync --delete -apv ${CHROOT_MASTER}/ ${CHROOT_COPY}\\")\\n    run(\\"rm -rf ${CHROOT_COPY}${SCRIPTS_DIR}\\")\\n    run(\\"cp -a ${SCRIPTS_DIR} ${CHROOT_COPY}${SCRIPTS_DIR}\\")\\n    # Merge onto upstream/master\\n    run(\\"rm -rf ${BUILD_DIR}\\")\\n    run(\\"mkdir -p ${BUILD_DIR}\\")\\n    run(\\"git clone ${CLONE_URL} ${BUILD_DIR}\\")\\n    run(\\"git remote add pull \\"+clone_url, cwd=build_dir, stdout=out, stderr=out)\\n    run(\\"git fetch pull\\", cwd=build_dir, stdout=out, stderr=out)\\n    if run(\\"git merge \\"+ commit, fail_hard=False, cwd=build_dir, stdout=out, stderr=out) != 0:\\n        return False\\n    run(\\"chown -R ${BUILD_USER}:${BUILD_GROUP} ${BUILD_DIR}\\", stdout=out, stderr=out)\\n    run(\\"mount --bind /proc ${CHROOT_COPY}/proc\\")\\n    return True\\n\\ndef commentOn(commentUrl, success, inMerge, needTests, linkUrl):\\n    common_message = \\"\\"\\"\\nThis test script verifies pulls every time they are updated. It, however, dies sometimes and fails to test properly.  If you are waiting on a test, please check timestamps to verify that the test.log is moving at http://jenkins.bluematt.me/pull-tester/current/\\nContact BlueMatt on freenode if something looks broken.\\"\\"\\"\\n\\n    # Remove old BitcoinPullTester comments (I\'m being lazy and not paginating here)\\n    recentcomments = requests.get(commentUrl+\\"?sort=created\\u0026direction=desc\\",\\n                                  auth=(os.environ[\'GITHUB_USER\'], os.environ[\\"GITHUB_AUTH_TOKEN\\"])).json\\n    for comment in recentcomments:\\n        if comment[\\"user\\"][\\"login\\"] == os.environ[\\"GITHUB_USER\\"] and common_message in comment[\\"body\\"]:\\n            requests.delete(comment[\\"url\\"],\\n                                  auth=(os.environ[\'GITHUB_USER\'], os.environ[\\"GITHUB_AUTH_TOKEN\\"]))\\n\\n    if success == True:\\n        if needTests:\\n            message = \\"Automatic sanity-testing: PLEASE ADD TEST-CASES, though technically passed. See \\" + linkUrl + \\" for binaries and test log.\\"\\n        else:\\n            message = \\"Automatic sanity-testing: PASSED, see \\" + linkUrl + \\" for binaries and test log.\\"\\n\\n        post_data = { \\"body\\" : message + common_message}\\n    elif inMerge:\\n        post_data = { \\"body\\" : \\"Automatic sanity-testing: FAILED MERGE, see \\" + linkUrl + \\" for test log.\\" + \\"\\"\\"\\n\\nThis pull does not merge cleanly onto current master\\"\\"\\" + common_message}\\n    else:\\n        post_data = { \\"body\\" : \\"Automatic sanity-testing: FAILED BUILD/TEST, see \\" + linkUrl + \\" for binaries and test log.\\" + \\"\\"\\"\\n\\nThis could happen for one of several reasons:\\n1. It chanages changes build scripts in a way that made them incompatible with the automated testing scripts (please tweak those patches in qa/pull-tester)\\n2. It adds/modifies tests which test network rules (thanks for doing that), which conflicts with a patch applied at test time\\n3. It does not build on either Linux i386 or Win32 (via MinGW cross compile)\\n4. The test suite fails on either Linux i386 or Win32\\n5. The block test-cases failed (lookup the first bNN identifier which failed in https://github.com/TheBlueMatt/test-scripts/blob/master/FullBlockTestGenerator.java)\\n\\nIf you believe this to be in error, please ping BlueMatt on freenode or TheBlueMatt here.\\n\\"\\"\\" + common_message}\\n\\n    resp = requests.post(commentUrl, json.dumps(post_data), auth=(os.environ[\'GITHUB_USER\'], os.environ[\\"GITHUB_AUTH_TOKEN\\"]))\\n\\ndef testpull(number, comment_url, clone_url, commit):\\n    print(\\"Testing pull %d: %s : %s\\"%(number, clone_url,commit))\\n\\n    dir = os.environ[\\"RESULTS_DIR\\"] + \\"/\\" + commit + \\"/\\"\\n    print(\\" ouput to %s\\"%dir)\\n    if os.path.exists(dir):\\n        os.system(\\"rm -r \\" + dir)\\n    os.makedirs(dir)\\n    currentdir = os.environ[\\"RESULTS_DIR\\"] + \\"/current\\"\\n    os.system(\\"rm -r \\"+currentdir)\\n    os.system(\\"ln -s \\" + dir + \\" \\" + currentdir)\\n    out = open(dir + \\"test.log\\", \'w+\')\\n\\n    resultsurl = os.environ[\\"RESULTS_URL\\"] + commit\\n    checkedout = checkout_pull(clone_url, commit, out)\\n    if checkedout != True:\\n        print(\\"Failed to test pull - sending comment to: \\" + comment_url)\\n        commentOn(comment_url, False, True, False, resultsurl)\\n        open(os.environ[\\"TESTED_DB\\"], \\"a\\").write(commit + \\"\\\\n\\")\\n        return\\n\\n    run(\\"rm -rf ${CHROOT_COPY}/${OUT_DIR}\\", fail_hard=False);\\n    run(\\"mkdir -p ${CHROOT_COPY}/${OUT_DIR}\\", fail_hard=False);\\n    run(\\"chown -R ${BUILD_USER}:${BUILD_GROUP} ${CHROOT_COPY}/${OUT_DIR}\\", fail_hard=False)\\n\\n    script = os.environ[\\"BUILD_PATH\\"]+\\"/qa/pull-tester/pull-tester.sh\\"\\n    script += \\" ${BUILD_PATH} ${MINGW_DEPS_DIR} ${SCRIPTS_DIR}/BitcoindComparisonTool_jar/BitcoindComparisonTool.jar 0 6 ${OUT_DIR}\\"\\n    returncode = run(\\"chroot ${CHROOT_COPY} sudo -u ${BUILD_USER} -H timeout ${TEST_TIMEOUT} \\"+script,\\n                     fail_hard=False, stdout=out, stderr=out)\\n\\n    run(\\"mv ${CHROOT_COPY}/${OUT_DIR} \\" + dir)\\n    run(\\"mv ${BUILD_DIR} \\" + dir)\\n\\n    if returncode == 42:\\n        print(\\"Successfully tested pull (needs tests) - sending comment to: \\" + comment_url)\\n        commentOn(comment_url, True, False, True, resultsurl)\\n    elif returncode != 0:\\n        print(\\"Failed to test pull - sending comment to: \\" + comment_url)\\n        commentOn(comment_url, False, False, False, resultsurl)\\n    else:\\n        print(\\"Successfully tested pull - sending comment to: \\" + comment_url)\\n        commentOn(comment_url, True, False, False, resultsurl)\\n    open(os.environ[\\"TESTED_DB\\"], \\"a\\").write(commit + \\"\\\\n\\")\\n\\ndef environ_default(setting, value):\\n    if not setting in os.environ:\\n        os.environ[setting] = value\\n\\nif getpass.getuser() != \\"root\\":\\n\\tprint(\\"Run me as root!\\")\\n\\tsys.exit(1)\\n\\nif \\"GITHUB_USER\\" not in os.environ or \\"GITHUB_AUTH_TOKEN\\" not in os.environ:\\n    print(\\"GITHUB_USER and/or GITHUB_AUTH_TOKEN environment variables not set\\")\\n    sys.exit(1)\\n\\nenviron_default(\\"CLONE_URL\\", \\"https://github.com/bitcoin/bitcoin.git\\")\\nenviron_default(\\"MINGW_DEPS_DIR\\", \\"/mnt/w32deps\\")\\nenviron_default(\\"SCRIPTS_DIR\\", \\"/mnt/test-scripts\\")\\nenviron_default(\\"CHROOT_COPY\\", \\"/mnt/chroot-tmp\\")\\nenviron_default(\\"CHROOT_MASTER\\", \\"/mnt/chroot\\")\\nenviron_default(\\"OUT_DIR\\", \\"/mnt/out\\")\\nenviron_default(\\"BUILD_PATH\\", \\"/mnt/bitcoin\\")\\nos.environ[\\"BUILD_DIR\\"] = os.environ[\\"CHROOT_COPY\\"] + os.environ[\\"BUILD_PATH\\"]\\nenviron_default(\\"RESULTS_DIR\\", \\"/mnt/www/pull-tester\\")\\nenviron_default(\\"RESULTS_URL\\", \\"http://jenkins.bluematt.me/pull-tester/\\")\\nenviron_default(\\"GITHUB_REPO\\", \\"bitcoin/bitcoin\\")\\nenviron_default(\\"TESTED_DB\\", \\"/mnt/commits-tested.txt\\")\\nenviron_default(\\"BUILD_USER\\", \\"matt\\")\\nenviron_default(\\"BUILD_GROUP\\", \\"matt\\")\\nenviron_default(\\"TEST_TIMEOUT\\", str(60*60*2))\\n\\nprint(\\"Optional usage: pull-tester.py 2112\\")\\n\\nf = open(os.environ[\\"TESTED_DB\\"])\\ntested = set( line.rstrip() for line in f.readlines() )\\nf.close()\\n\\nif len(sys.argv) \\u003e 1:\\n    pull = requests.get(\\"https://api.github.com/repos/\\"+os.environ[\\"GITHUB_REPO\\"]+\\"/pulls/\\"+sys.argv[1],\\n                        auth=(os.environ[\'GITHUB_USER\'], os.environ[\\"GITHUB_AUTH_TOKEN\\"])).json\\n    testpull(pull[\\"number\\"], pull[\\"_links\\"][\\"comments\\"][\\"href\\"],\\n             pull[\\"head\\"][\\"repo\\"][\\"clone_url\\"], pull[\\"head\\"][\\"sha\\"])\\n\\nelse:\\n    for page in range(1,100):\\n        result = requests.get(\\"https://api.github.com/repos/\\"+os.environ[\\"GITHUB_REPO\\"]+\\"/pulls?state=open\\u0026page=%d\\"%(page,),\\n                              auth=(os.environ[\'GITHUB_USER\'], os.environ[\\"GITHUB_AUTH_TOKEN\\"])).json\\n        if len(result) == 0: break;\\n        for pull in result:\\n            if pull[\\"head\\"][\\"sha\\"] in tested:\\n                print(\\"Pull %d already tested\\"%(pull[\\"number\\"],))\\n                continue\\n            testpull(pull[\\"number\\"], pull[\\"_links\\"][\\"comments\\"][\\"href\\"],\\n                     pull[\\"head\\"][\\"repo\\"][\\"clone_url\\"], pull[\\"head\\"][\\"sha\\"])\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/add2numbers.r","content":"add2nums <- function(num1, num2) {\\n   print( num1+num2)\\n}\\n\\nadd2nums(34, 6)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/areaofsquare.r","content":"areaSquare <- function(num) {\\n   msg1 <- \'Invalid measurement\'\\n   msg2 <- \'Area of the Square is: \'\\n   if  (num <= 0){\\n        print(msg1)\\n   }else {\\n       print (msg2)\\n       print(num*num)\\n   }\\n  \\n}\\n\\nareaSquare(6)"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/divisibleby10.r","content":"divisibleby10 <- function(num) {\\nif(num %% 10 == 0){\\n    print(\'True\')\\n}else{\\n    print(\'False\')\\n}\\n\\n}\\n\\ndivisibleby10(60)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/integertype.r","content":"integerType <- function(num){\\nif(num > 0) {\\nprint(\'Positive number\')\\n} else {\\nif(num == 0) {\\nprint(\'Zero\')\\n} else {\\nprint(\'Negative number\')\\n}\\n}\\n}\\n\\nintegerType(-90)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/lengthoflist.r","content":"\\nlengthofVector <- function(vector){\\ncount <- 0\\nfor (i in vector){\\n    count <- count + 1\\n}\\nprint(count)\\n}\\n\\narray <- c(3,4,5,1,6)\\nlengthofVector(array)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/palindrome.r","content":"\\npalindrome <- function(n){\\nrev = 0\\n    num = n\\n\\n    while (n > 0) {\\n      r = n %% 10\\n      rev = rev * 10 + r\\n      n = n %/% 10\\n    }\\n\\n    if (rev == num)\\n    {\\n      print(paste(\'Number is palindrome :\', rev))\\n    }\\n    else{\\n      print(paste(\'Number is not palindrome :\', rev))\\n    }\\n}\\npalindrome(121)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/sort.r","content":"integerType <- function(num){\\nif(num > 0) {\\nsortvector <- function(vector){\\nprint(sort(vector))\\n}\\n\\narray <- c(23,12,11,34,21)\\nsortvector(array)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/spmv.r","content":"\\nsum <- function(vector){\\nresult <- 0\\nfor(i in vector){\\n    result = result + i\\n}\\nprint(result)\\n}\\n\\nproduct <- function(vector){\\n    result <- 1\\n    for(i in vector){\\n        result = result*i\\n    }\\n    print(result)\\n}\\n\\narray <- c(1,2,3,4)\\nsum(array)\\nproduct(array)\\nmean(array, na.rm=TRUE)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/upperlower.r","content":"\\nupperCase <- function(word){\\nresult <- toupper(word)\\nprint(result)\\n}\\n\\nlowerCase <- function(word){\\nresult <- tolower(word)\\nprint(result)\\n}\\n\\nupperCase(\'Function\')\\nlowerCase(\'FUNCTION\')\\n"}\n'
line: b'{ "repo_name": "mathii/europe_selection", "ref": "refs/heads/master", "path": "analysis/genome_wide_scan_imputed.R", "content": "#This is the genome-wide scan but using the read level information to try and\\n#get some idea about diploid calls. For reasons of speed, we break this one\\n#by chromosome. Genomic correction has to be done in another step for this reason.\\n\\n#Test whether the modern population frequencies can be modelled as a mixture of the\\n#Three ancestral populations.\\n#Using genotype probabilities imputed with Beagle.\\n\\nsource(\\"~/selection/code/lib/3pop_lib.R\\")\\n\\n#Modern GBR, CEU, IBS, TSI\\n#Ancient WHG, ENeo, Yamnaya\\n\\n########################################################################\\n## Details\\nchr <- 1                                #set manually, or from --args\\nversion <- \\"vx\\" #v6, v7 etc...\\nresults.tag <- \\"\\"\\nwhich.impute <- \\"within\\"\\n\\ncA <- commandArgs(TRUE)\\nif(length(cA)){\\n  chr <- cA[1]\\n  version <- cA[2]\\n  if(length(cA)>2){\\n    results.tag <- cA[3]\\n  }\\n  if(length(cA)>3){\\n    which.impute <- cA[4]\\n  }\\n}\\n\\nverbose=TRUE\\n## Supposed to check if running on cluster, but YMMV\\nif( Sys.info()[\\"login\\"]!=Sys.info()[\\"user\\"]){\\n    verbose=FALSE\\n}\\n\\n########################################################################\\n## Details\\nroot <- paste0(\\"~/selection/counts/\\", version, \\"/all\\")\\nout <- paste0(\\"~/selection/analysis/\\", version, \\"/gscan/\\")\\nindfile <- paste0(\\"~/data/\\", version, \\"/use/\\", version,\\"1kg_europe2names.ind\\")\\nimpute.file <- paste0(\\"~/selection/imputation/\\", version, \\"/imputed.\\", which.impute ,\\".chr\\", chr, \\".vcf.gz\\")\\nerror.prob <- 0.001\\n\\npops <- c(\\"WHG\\", \\"EN\\", \\"Yamnaya\\", \\"CEU\\", \\"GBR\\", \\"IBS\\", \\"TSI\\")\\n#Check if the SNP is monomorphic in these populations. \\nmonocheck <- c(\\"CEU\\", \\"GBR\\", \\"IBS\\", \\"TSI\\", \\"HungaryGamba_HG\\", \\"Loschbour\\", \\"Stuttgart\\",\\n               \\"LBK_EN\\", \\"HungaryGamba_EN\\", \\"Spain_EN\\", \\"Starcevo_EN\\", \\"LBKT_EN\\", \\"Yamnaya\\")\\nA <- matrix(c(0.164, 0.366, 0.470, 0.213, 0.337, 0.450, 0, 0.773, 0.227, 0, 0.712, 0.288),3, 4) \\n\\n########################################################################\\n\\nif(version==\\"v6\\" | version==\\"v7\\"){\\n\\n  pops <- c(\\"WHG\\", \\"EN\\", \\"Yamnaya\\", \\"CEU\\", \\"GBR\\", \\"IBS\\", \\"TSI\\")\\n#Check if the SNP is monomorphic in these populations. \\n  monocheck <- c(\\"CEU\\", \\"GBR\\", \\"IBS\\", \\"TSI\\", \\"HungaryGamba_HG\\", \\"Loschbour\\", \\"Stuttgart\\",\\n               \\"LBK_EN\\", \\"HungaryGamba_EN\\", \\"Spain_EN\\", \\"Starcevo_EN\\", \\"LBKT_EN\\", \\"Yamnaya\\")\\n  A <- matrix(c(0.164, 0.366, 0.470, 0.213, 0.337, 0.450, 0, 0.773, 0.227, 0, 0.712, 0.288),3, 4) \\n\\n  include.counts <- list(                 #Include these populations as hard calls. \\n    \\"WHG\\"=\\"Loschbour\\",\\n    \\"EN\\"=\\"Stuttgart\\",\\n    \\"CEU\\"=\\"CEU\\", \\"GBR\\"=\\"GBR\\", \\"IBS\\"=\\"IBS\\", \\"TSI\\"=\\"TSI\\" )\\n  \\ninclude.probs <- list(                  #Include these populations as reads\\n    ## \\"WHG\\"=c(\\"LaBrana1\\", \\"HungaryGamba_HG\\"), #Replace LaBrana1 with SpanishMesolithic for the high coverage LaBrana I0585\\n    \\"WHG\\"=c(\\"SpanishMesolithic\\", \\"HungaryGamba_HG\\"), #Replace LaBrana1 with SpanishMesolithic for the high coverage LaBrana I0585\\n    \\"EN\\"=c(\\"LBK_EN\\", \\"HungaryGamba_EN\\", \\"Spain_EN\\", \\"Starcevo_EN\\", \\"LBKT_EN\\"), \\n    \\"Yamnaya\\"=\\"Yamnaya\\")\\n}\\nif(version==\\"v7\\"){\\n  include.probs[[\\"WHG\\"]] <- gsub(\\"SpanishMesolithic\\", \\"Iberian_Mesolithic\\", include.probs[[\\"WHG\\"]], fixed=TRUE)\\n}\\nif(version==\\"v8\\"){\\n  mix.dir <- \\"~/selection/code/files/v8/mixtures/\\"\\n  \\n  if(results.tag==\\"\\"){stop(\\"Must specify results tag - group from 1-6 - for v8 analysis\\")}\\n  include.counts <- list( \\"CEU\\"=\\"CEU\\", \\"GBR\\"=\\"GBR\\", \\"IBS\\"=\\"IBS\\", \\"TSI\\"=\\"TSI\\" )\\n  always.counts <- c(\\"Loschbour\\", \\"Stuttgart\\")\\n  group <- results.tag\\n  choice <- read.table(paste0(mix.dir, \\"Choice\\", results.tag), as.is=TRUE, header=FALSE)\\n  include.probs <- list(c(), c(), c())\\n  names(include.probs) <- unique(choice[,2])\\n  for(i in 1:NROW(choice)){\\n    if(choice[i,1] %in% c(\\"Loschbour\\", \\"Stuttgart\\")){\\n      include.counts[[choice[i,2]]] <- choice[i,1]\\n    } else{\\n      include.probs[[choice[i,2]]] <- c(include.probs[[choice[i,2]]], choice[i,1])\\n    }\\n  }\\n  mix.mat <- read.table(paste0(mix.dir, \\"Proportion\\", results.tag), as.is=TRUE, header=TRUE)\\n  rownames(mix.mat) <- mix.mat[,1]\\n  mix.mat <- mix.mat[,2:NCOL(mix.mat)]\\n  \\n  anc.pops <- names(include.probs)\\n  mod.pops <- c(\\"CEU\\", \\"GBR\\", \\"IBS\\", \\"TSI\\")\\n  pops <- c(anc.pops, mod.pops)\\n  A <- t(mix.mat)[anc.pops,mod.pops]\\n\\n  monocheck <- c(unlist(include.probs), unlist(include.counts))\\n  names(monocheck) <- NULL\\n}\\n\\n##################################################################################################\\n\\n## Setup the count data. \\ncounts <- read.table(paste0(root, \\".count\\"), header=TRUE, as.is=TRUE)\\ntotals <- read.table(paste0(root, \\".total\\"), header=TRUE, as.is=TRUE)\\ndata <- counts[,1:5]\\ninclude <- data$CHR==chr\\ndata <- data[include,]\\n\\ncounts <- data.matrix(counts[,6:NCOL(counts)])\\ntotals <- data.matrix(totals[,6:NCOL(totals)])\\ncounts <- counts[include,]\\ntotals <- totals[include,]\\n\\n#Load imputed likelihoods\\nimpute <- read.table(impute.file, comment.char=\\"\\", as.is=TRUE, header=FALSE, sep=\\"\\\\t\\", fill=TRUE)\\ncomment.lines <- sum(grepl(\\"^##\\", impute[,1]))\\nimpute <- read.table(impute.file, comment.char=\\"\\", as.is=TRUE, header=TRUE, skip=comment.lines, sep=\\"\\\\t\\", fill=TRUE)\\nrownames(impute) <- impute$ID\\nimpute <- impute[impute$ID %in% data[,1],]\\nimpute.info <- impute[,8]\\nimpute <- impute[,10:NCOL(impute)]\\n\\n## get list of samples in each population of reads\\ninclude.prob.samples <- read.samples(indfile, include.probs)\\n\\n## set up results\\nresults <- matrix(0, nrow=NROW(data), ncol=3)\\nrownames(results) <- data$ID\\n\\n## Data structure\\nempty.data <- make.empty.data(pops)\\n\\nfor(i in 1:NROW(data)){\\n    this.snp <- data[i,1]\\n    if(verbose){cat(paste0(\\"\\\\r\\", i, \\" \\", this.snp))}\\n    this.prob <- impute[i,]\\n\\n    freq.data <- make.prob.freq.data(pops, include.probs, include.prob.samples, include.counts,\\n                                this.prob, counts[i,], totals[i,], empty.data)\\n    monomorphic <- all(counts[i,monocheck]==0)|all(counts[i,monocheck]==totals[i,monocheck])\\n    if(monomorphic){\\n        results[i,] <- NA\\n    }else{\\n        AR2 <- as.numeric(strsplit(strsplit(impute.info[i], \\";\\", fixed=TRUE)[[1]][1], \\"=\\", fixed=TRUE)[[1]][2])\\n        results[i,] <- c(test.3pop.reads(freq.data, A, error.prob=error.prob), AR2)\\n    }\\n}\\n\\nresults <- results[!is.na(results[,2]),]\\n\\nresults <- cbind(rownames(results), results)\\ncolnames(results) <- c(\\"ID\\", \\"ChiSq\\", \\"uncorrected.p\\", \\"AR2\\")\\nresults <- data.frame(results)\\nout.file <-  paste0(\\"~/selection/analysis/\\",version,\\"/gscan/scan_results_imputed\\", results.tag, \\".\\", which.impute, \\".chr\\", chr, \\".txt\\")\\nprint(out.file)\\nwrite.table(results,out.file, row.names=FALSE, col.names=TRUE, quote=FALSE, sep=\\"\\\\t\\")\\n\\n" }\n'
line: b'{"repo_name":"haad/ansible","ref":"refs/heads/devel","path":"lib/ansible/modules/cloud/lxd/lxd_profile.py","content":"#!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n\\n# (c) 2016, Hiroaki Nakamura \\u003chnakamur@gmail.com\\u003e\\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\\n\\nfrom __future__ import absolute_import, division, print_function\\n__metaclass__ = type\\n\\n\\nANSIBLE_METADATA = {\'metadata_version\': \'1.1\',\\n                    \'status\': [\'preview\'],\\n                    \'supported_by\': \'community\'}\\n\\n\\nDOCUMENTATION = \'\'\'\\n---\\nmodule: lxd_profile\\nshort_description: Manage LXD profiles\\nversion_added: \\"2.2\\"\\ndescription:\\n  - Management of LXD profiles\\nauthor: \\"Hiroaki Nakamura (@hnakamur)\\"\\noptions:\\n    name:\\n        description:\\n          - Name of a profile.\\n        required: true\\n    description:\\n        description:\\n          - Description of the profile.\\n        version_added: \\"2.5\\"\\n    config:\\n        description:\\n          - \'The config for the container (e.g. {\\"limits.memory\\": \\"4GB\\"}).\\n            See U(https://github.com/lxc/lxd/blob/master/doc/rest-api.md#patch-3)\'\\n          - If the profile already exists and its \\"config\\" value in metadata\\n            obtained from\\n            GET /1.0/profiles/\\u003cname\\u003e\\n            U(https://github.com/lxc/lxd/blob/master/doc/rest-api.md#get-19)\\n            are different, they this module tries to apply the configurations.\\n          - Not all config values are supported to apply the existing profile.\\n            Maybe you need to delete and recreate a profile.\\n        required: false\\n    devices:\\n        description:\\n          - \'The devices for the profile\\n            (e.g. {\\"rootfs\\": {\\"path\\": \\"/dev/kvm\\", \\"type\\": \\"unix-char\\"}).\\n            See U(https://github.com/lxc/lxd/blob/master/doc/rest-api.md#patch-3)\'\\n        required: false\\n    new_name:\\n        description:\\n          - A new name of a profile.\\n          - If this parameter is specified a profile will be renamed to this name.\\n            See U(https://github.com/lxc/lxd/blob/master/doc/rest-api.md#post-11)\\n        required: false\\n    state:\\n        choices:\\n          - present\\n          - absent\\n        description:\\n          - Define the state of a profile.\\n        required: false\\n        default: present\\n    url:\\n        description:\\n          - The unix domain socket path or the https URL for the LXD server.\\n        required: false\\n        default: unix:/var/lib/lxd/unix.socket\\n    key_file:\\n        description:\\n          - The client certificate key file path.\\n        required: false\\n        default: \'\\"{}/.config/lxc/client.key\\" .format(os.environ[\\"HOME\\"])\'\\n    cert_file:\\n        description:\\n          - The client certificate file path.\\n        required: false\\n        default: \'\\"{}/.config/lxc/client.crt\\" .format(os.environ[\\"HOME\\"])\'\\n    trust_password:\\n        description:\\n          - The client trusted password.\\n          - You need to set this password on the LXD server before\\n            running this module using the following command.\\n            lxc config set core.trust_password \\u003csome random password\\u003e\\n            See U(https://www.stgraber.org/2016/04/18/lxd-api-direct-interaction/)\\n          - If trust_password is set, this module send a request for\\n            authentication before sending any requests.\\n        required: false\\nnotes:\\n  - Profiles must have a unique name. If you attempt to create a profile\\n    with a name that already existed in the users namespace the module will\\n    simply return as \\"unchanged\\".\\n\'\'\'\\n\\nEXAMPLES = \'\'\'\\n# An example for creating a profile\\n- hosts: localhost\\n  connection: local\\n  tasks:\\n    - name: Create a profile\\n      lxd_profile:\\n        name: macvlan\\n        state: present\\n        config: {}\\n        description: my macvlan profile\\n        devices:\\n          eth0:\\n            nictype: macvlan\\n            parent: br0\\n            type: nic\\n\\n# An example for creating a profile via http connection\\n- hosts: localhost\\n  connection: local\\n  tasks:\\n  - name: create macvlan profile\\n    lxd_profile:\\n      url: https://127.0.0.1:8443\\n      # These cert_file and key_file values are equal to the default values.\\n      #cert_file: \\"{{ lookup(\'env\', \'HOME\') }}/.config/lxc/client.crt\\"\\n      #key_file: \\"{{ lookup(\'env\', \'HOME\') }}/.config/lxc/client.key\\"\\n      trust_password: mypassword\\n      name: macvlan\\n      state: present\\n      config: {}\\n      description: my macvlan profile\\n      devices:\\n        eth0:\\n          nictype: macvlan\\n          parent: br0\\n          type: nic\\n\\n# An example for deleting a profile\\n- hosts: localhost\\n  connection: local\\n  tasks:\\n    - name: Delete a profile\\n      lxd_profile:\\n        name: macvlan\\n        state: absent\\n\\n# An example for renaming a profile\\n- hosts: localhost\\n  connection: local\\n  tasks:\\n    - name: Rename a profile\\n      lxd_profile:\\n        name: macvlan\\n        new_name: macvlan2\\n        state: present\\n\'\'\'\\n\\nRETURN = \'\'\'\\nold_state:\\n  description: The old state of the profile\\n  returned: success\\n  type: string\\n  sample: \\"absent\\"\\nlogs:\\n  description: The logs of requests and responses.\\n  returned: when ansible-playbook is invoked with -vvvv.\\n  type: list\\n  sample: \\"(too long to be placed here)\\"\\nactions:\\n  description: List of actions performed for the profile.\\n  returned: success\\n  type: list\\n  sample: \'[\\"create\\"]\'\\n\'\'\'\\n\\nimport os\\n\\nfrom ansible.module_utils.basic import AnsibleModule\\nfrom ansible.module_utils.lxd import LXDClient, LXDClientException\\n\\n\\n# PROFILE_STATES is a list for states supported\\nPROFILES_STATES = [\\n    \'present\', \'absent\'\\n]\\n\\n# CONFIG_PARAMS is a list of config attribute names.\\nCONFIG_PARAMS = [\\n    \'config\', \'description\', \'devices\'\\n]\\n\\n\\nclass LXDProfileManagement(object):\\n    def __init__(self, module):\\n        \\"\\"\\"Management of LXC containers via Ansible.\\n\\n        :param module: Processed Ansible Module.\\n        :type module: ``object``\\n        \\"\\"\\"\\n        self.module = module\\n        self.name = self.module.params[\'name\']\\n        self._build_config()\\n        self.state = self.module.params[\'state\']\\n        self.new_name = self.module.params.get(\'new_name\', None)\\n\\n        self.url = self.module.params[\'url\']\\n        self.key_file = self.module.params.get(\'key_file\', None)\\n        self.cert_file = self.module.params.get(\'cert_file\', None)\\n        self.debug = self.module._verbosity \\u003e= 4\\n        try:\\n            self.client = LXDClient(\\n                self.url, key_file=self.key_file, cert_file=self.cert_file,\\n                debug=self.debug\\n            )\\n        except LXDClientException as e:\\n            self.module.fail_json(msg=e.msg)\\n        self.trust_password = self.module.params.get(\'trust_password\', None)\\n        self.actions = []\\n\\n    def _build_config(self):\\n        self.config = {}\\n        for attr in CONFIG_PARAMS:\\n            param_val = self.module.params.get(attr, None)\\n            if param_val is not None:\\n                self.config[attr] = param_val\\n\\n    def _get_profile_json(self):\\n        return self.client.do(\\n            \'GET\', \'/1.0/profiles/{0}\'.format(self.name),\\n            ok_error_codes=[404]\\n        )\\n\\n    @staticmethod\\n    def _profile_json_to_module_state(resp_json):\\n        if resp_json[\'type\'] == \'error\':\\n            return \'absent\'\\n        return \'present\'\\n\\n    def _update_profile(self):\\n        if self.state == \'present\':\\n            if self.old_state == \'absent\':\\n                if self.new_name is None:\\n                    self._create_profile()\\n                else:\\n                    self.module.fail_json(\\n                        msg=\'new_name must not be set when the profile does not exist and the specified state is present\',\\n                        changed=False)\\n            else:\\n                if self.new_name is not None and self.new_name != self.name:\\n                    self._rename_profile()\\n                if self._needs_to_apply_profile_configs():\\n                    self._apply_profile_configs()\\n        elif self.state == \'absent\':\\n            if self.old_state == \'present\':\\n                if self.new_name is None:\\n                    self._delete_profile()\\n                else:\\n                    self.module.fail_json(\\n                        msg=\'new_name must not be set when the profile exists and the specified state is absent\',\\n                        changed=False)\\n\\n    def _create_profile(self):\\n        config = self.config.copy()\\n        config[\'name\'] = self.name\\n        self.client.do(\'POST\', \'/1.0/profiles\', config)\\n        self.actions.append(\'create\')\\n\\n    def _rename_profile(self):\\n        config = {\'name\': self.new_name}\\n        self.client.do(\'POST\', \'/1.0/profiles/{}\'.format(self.name), config)\\n        self.actions.append(\'rename\')\\n        self.name = self.new_name\\n\\n    def _needs_to_change_profile_config(self, key):\\n        if key not in self.config:\\n            return False\\n        old_configs = self.old_profile_json[\'metadata\'].get(key, None)\\n        return self.config[key] != old_configs\\n\\n    def _needs_to_apply_profile_configs(self):\\n        return (\\n            self._needs_to_change_profile_config(\'config\') or\\n            self._needs_to_change_profile_config(\'description\') or\\n            self._needs_to_change_profile_config(\'devices\')\\n        )\\n\\n    def _apply_profile_configs(self):\\n        config = self.old_profile_json.copy()\\n        for k, v in self.config.items():\\n            config[k] = v\\n        self.client.do(\'PUT\', \'/1.0/profiles/{}\'.format(self.name), config)\\n        self.actions.append(\'apply_profile_configs\')\\n\\n    def _delete_profile(self):\\n        self.client.do(\'DELETE\', \'/1.0/profiles/{}\'.format(self.name))\\n        self.actions.append(\'delete\')\\n\\n    def run(self):\\n        \\"\\"\\"Run the main method.\\"\\"\\"\\n\\n        try:\\n            if self.trust_password is not None:\\n                self.client.authenticate(self.trust_password)\\n\\n            self.old_profile_json = self._get_profile_json()\\n            self.old_state = self._profile_json_to_module_state(self.old_profile_json)\\n            self._update_profile()\\n\\n            state_changed = len(self.actions) \\u003e 0\\n            result_json = {\\n                \'changed\': state_changed,\\n                \'old_state\': self.old_state,\\n                \'actions\': self.actions\\n            }\\n            if self.client.debug:\\n                result_json[\'logs\'] = self.client.logs\\n            self.module.exit_json(**result_json)\\n        except LXDClientException as e:\\n            state_changed = len(self.actions) \\u003e 0\\n            fail_params = {\\n                \'msg\': e.msg,\\n                \'changed\': state_changed,\\n                \'actions\': self.actions\\n            }\\n            if self.client.debug:\\n                fail_params[\'logs\'] = e.kwargs[\'logs\']\\n            self.module.fail_json(**fail_params)\\n\\n\\ndef main():\\n    \\"\\"\\"Ansible Main module.\\"\\"\\"\\n\\n    module = AnsibleModule(\\n        argument_spec=dict(\\n            name=dict(\\n                type=\'str\',\\n                required=True\\n            ),\\n            new_name=dict(\\n                type=\'str\',\\n            ),\\n            config=dict(\\n                type=\'dict\',\\n            ),\\n            description=dict(\\n                type=\'str\',\\n            ),\\n            devices=dict(\\n                type=\'dict\',\\n            ),\\n            state=dict(\\n                choices=PROFILES_STATES,\\n                default=\'present\'\\n            ),\\n            url=dict(\\n                type=\'str\',\\n                default=\'unix:/var/lib/lxd/unix.socket\'\\n            ),\\n            key_file=dict(\\n                type=\'str\',\\n                default=\'{}/.config/lxc/client.key\'.format(os.environ[\'HOME\'])\\n            ),\\n            cert_file=dict(\\n                type=\'str\',\\n                default=\'{}/.config/lxc/client.crt\'.format(os.environ[\'HOME\'])\\n            ),\\n            trust_password=dict(type=\'str\', no_log=True)\\n        ),\\n        supports_check_mode=False,\\n    )\\n\\n    lxd_manage = LXDProfileManagement(module=module)\\n    lxd_manage.run()\\n\\n\\nif __name__ == \'__main__\':\\n    main()\\n"}\n'
line: b'{ "repo_name": "jonhoo/volley", "ref": "refs/heads/master", "path": "benchmark/plot.R", "content": "#!/usr/bin/env Rint\\nlibrary(grDevices)\\nlibrary(utils)\\nX11(width=12, height=10)\\n\\nlibrary(ggplot2)\\nargs <- commandArgs(trailingOnly = TRUE)\\nargs <- if (length(args) == 0) Sys.getenv(\\"ARGS\\") else args\\nargs <- if (args[1] == \\"\\") \\"plot.dat\\" else args\\n\\nd <- data.frame(read.table(\\n\\t\\t\\t   text=gsub(\'us \', \' \', readLines(file(args[1]))),\\n\\t\\t\\t   col.names=c(\\"server\\", \\"clients\\", \\"cores\\", \\"time\\", \\"stddev\\", \\"n\\")\\n\\t\\t\\t   ))\\n\\nd$ci = 2.58 * d$stddev / sqrt(d$n)\\nd$ops = d$clients/(d$time/1000.0/1000.0)\\nd$min = d$clients/((d$time-d$ci)/1000.0/1000.0)\\nd$max = d$clients/((d$time+d$ci)/1000.0/1000.0)\\n\\n#d = d[d[, \\"clients\\"] == 80,]\\n#d = d[grep(\\"^go\\", d[, \\"server\\"]),]\\nprint(d)\\np <- ggplot(data=d, aes(x = cores, y = ops, ymin = min, ymax = max, color = server))\\np <- p + geom_line()\\np <- p + ylim(0, 2000000)\\np <- p + geom_errorbar()\\np <- p + facet_wrap(~ clients)\\np <- p + xlab(\\"CPU cores\\")\\np <- p + ylab(\\"Mean ops/s\\")\\n\\np\\nggsave(\\"plot.png\\", plot = p, width = 8, height = 6)\\n" }\n'
line: b'{ "repo_name": "jaredhuling/rfunctions", "ref": "refs/heads/master", "path": "R/lanczos.R", "content": "\\n\\n#\' Compute Largest Singular Value of Matrix x\\n#\'\\n#\' @param x matrix input\\n#\' @param v numeric vector. Initialize GKL bidiagonalization with random vector on unit sphere\\n#\' @param maxit integer. Maximum number of Lanczos iterations\\n#\' @param reorthog Takes values in 0:2. 0 for no reorthogonalization, 1 for reorthogonalization of \\n#\' V vectors (slower, more accurate), 2 for reorthogonalization of V and U vectors (slowest and most \\n#\' memory, most accurate. Not implemented yet)\\n#\' @param upper.bound.prob upper bound probability for the largest singular value\\n#\' @return list \\n#\' @references Hochstenbach (2013) Probabilistic upper bounds for the matrix two-norm, http://link.springer.com/article/10.1007/s10915-013-9716-x \\n#\' Journal of Scientific Computing, Vol. 57(3), Dec. 2013\\n#\' @export\\n#\' @examples\\n#\'n.obs <- 1e5\\n#\'n.vars <- 150\\n#\'\\n#\'x <- matrix(rnorm(n.obs * n.vars), n.obs, n.vars)\\n#\'\\n#\'## compute largest singular value of x\\n#\'lanczos <- gklBidiag(x, runif(ncol(x)), 10L, reorth = 0, upper.bound.prob = 0.99)\\n#\'str(lanczos)\\nsetGeneric(\\"gklBidiag\\", function(x, v, maxit, reorthog = 0, upper.bound.prob = NULL) {\\n  stopifnot(is.numeric(x) | inherits(x, \\"CsparseMatrix\\"))\\n  stopifnot(is.numeric(v))\\n  reorthog <- as.integer(reorthog)\\n  if (inherits(x, \\"CsparseMatrix\\")) {\\n    .Call(\\"GKLBidiagSparse\\", A = x, v = v, k = maxit, reorthog = reorthog, PACKAGE = \\"rfunctions\\")\\n  } else {\\n    .Call(\\"GKLBidiag\\", A = x, v = v, k = maxit, reorthog = reorthog, PACKAGE = \\"rfunctions\\")\\n  }\\n})\\n\\nsetMethod(\\"gklBidiag\\", signature(x = \\"matrix\\", v = \\"numeric\\", maxit = \\"numeric\\", \\n                                 reorthog = \\"numeric\\", upper.bound.prob = \\"numeric\\"),\\n            function(x, v, maxit = 50L, reorthog = 0, upper.bound.prob = NULL) {\\n              maxit <- as.integer(maxit)\\n              reorthog <- as.integer(reorthog)\\n              delt <- computeDelta(eps = 1 - upper.bound.prob, p = ncol(x))\\n              res <- .Call(\\"GKLBidiag\\", A = x, v = v, k = maxit, reorthog = reorthog, PACKAGE = \\"rfunctions\\")\\n              upper <- computeUpperBound(res, delt)\\n              res$upper.bound <- upper\\n              res$upper.bound.prob <- upper.bound.prob\\n              res\\n            })\\n\\nsetMethod(\\"gklBidiag\\", signature(x = \\"dgeMatrix\\", v = \\"numeric\\", maxit = \\"numeric\\", \\n                                 reorthog = \\"numeric\\", upper.bound.prob = \\"numeric\\"),\\n          function(x, v, maxit = 50L, reorthog = 0, upper.bound.prob = NULL) {\\n            maxit <- as.integer(maxit)\\n            reorthog <- as.integer(reorthog)\\n            delt <- computeDelta(eps = 1 - upper.bound.prob, p = ncol(x))\\n            res <- .Call(\\"GKLBidiag\\", A = x, v = v, k = maxit, reorthog = reorthog, PACKAGE = \\"rfunctions\\")\\n            upper <- computeUpperBound(res, delt)\\n            res$upper.bound <- upper\\n            res$upper.bound.prob <- upper.bound.prob\\n            res\\n          })\\n\\n\\nsetMethod(\\"gklBidiag\\", signature(x = \\"CsparseMatrix\\", v = \\"numeric\\", maxit = \\"numeric\\", \\n                                 reorthog = \\"numeric\\", upper.bound.prob = \\"numeric\\"),\\n          function(x, v, maxit = 50L, reorthog = 0, upper.bound.prob = NULL) {\\n            maxit <- as.integer(maxit)\\n            reorthog = as.integer(reorthog)\\n            delt <- computeDelta(eps = 1 - upper.bound.prob, p = ncol(x))\\n            res <- .Call(\\"GKLBidiagSparse\\", A = x, v = v, k = maxit, reorthog = reorthog, PACKAGE = \\"rfunctions\\")\\n            upper <- computeUpperBound(res, delt)\\n            res$upper.bound <- upper\\n            res$upper.bound.prob <- upper.bound.prob\\n            res\\n          })\\n\\n\\nsetMethod(\\"gklBidiag\\", signature(x = \\"matrix\\", v = \\"numeric\\", maxit = \\"numeric\\", \\n                                 reorthog = \\"numeric\\", upper.bound.prob = \\"missing\\"),\\n          function(x, v, maxit = 50L, reorthog = 0, upper.bound.prob = NULL) {\\n            maxit <- as.integer(maxit)\\n            reorthog <- as.integer(reorthog)\\n            res <- .Call(\\"GKLBidiag\\", A = x, v = v, k = maxit, reorthog = reorthog, PACKAGE = \\"rfunctions\\")\\n            res$upper.bound <- NULL\\n            res$upper.bound.prob <- NULL\\n            res\\n          })\\n\\nsetMethod(\\"gklBidiag\\", signature(x = \\"dgeMatrix\\", v = \\"numeric\\", maxit = \\"numeric\\", \\n                                 reorthog = \\"numeric\\", upper.bound.prob = \\"missing\\"),\\n          function(x, v, maxit = 50L, reorthog = 0, upper.bound.prob = NULL) {\\n            maxit <- as.integer(maxit)\\n            reorthog <- as.integer(reorthog)\\n            res <- .Call(\\"GKLBidiag\\", A = x, v = v, k = maxit, reorthog = reorthog, PACKAGE = \\"rfunctions\\")\\n            res$upper.bound <- NULL\\n            res$upper.bound.prob <- NULL\\n            res\\n          })\\n\\n\\nsetMethod(\\"gklBidiag\\", signature(x = \\"CsparseMatrix\\", v = \\"numeric\\", maxit = \\"numeric\\", \\n                                 reorthog = \\"numeric\\", upper.bound.prob = \\"missing\\"),\\n          function(x, v, maxit = 50L, reorthog = 0, upper.bound.prob = NULL) {\\n            maxit <- as.integer(maxit)\\n            reorthog <- as.integer(reorthog)\\n            res <- .Call(\\"GKLBidiagSparse\\", A = x, v = v, k = maxit, reorthog = reorthog, PACKAGE = \\"rfunctions\\")\\n            res$upper.bound <- NULL\\n            res$upper.bound.prob <- NULL\\n            res\\n          })\\n\\n" }\n'
line: b'{ "repo_name": "edzer/gstat", "ref": "refs/heads/master", "path": "tests/sim.R", "content": "options(digits=6)\\nlibrary(sp)\\ndata(meuse)\\nset.seed(158229572)\\nnew.locs <- data.frame(x = c(181170, 180310, 180205, 178673, 178770, 178270),\\n\\ty = c(333250, 332189, 331707, 330066, 330675, 331075))\\nlibrary(gstat)\\nkrige(zinc ~ 1, ~ x + y, meuse, newdata = new.locs, \\n\\t\\tmodel = vgm(1.34e5, \\"Sph\\", 800, nug = 2.42e4), \\n\\t\\tblock = c(40,40), nmax = 40, nsim = 10)\\n" }\n'
line: b'{ "repo_name": "edzer/gstat", "ref": "refs/heads/master", "path": "tests/na.action.R", "content": "library(sp)\\n\\ndata(meuse)\\ndata(meuse.grid)\\n\\nset.seed(13131) # reproduce results\\n\\n# select 10 random rows;\\n# create two missing values in the coordinates:\\nm = meuse.grid[sample(nrow(meuse.grid), 10), ]\\nm[c(2,8), \\"x\\"] = NA\\n\\nlibrary(gstat)\\n## this is not allowed anymore:\\ntry(krige(log(zinc)~1,~x+y,meuse,m, na.action = na.pass))\\ntry(krige(log(zinc)~1,~x+y,meuse,m, na.action = na.omit))\\ntry(krige(log(zinc)~1,~x+y,meuse,m, na.action = na.exclude))\\ntry(krige(log(zinc)~1,~x+y,meuse,m, na.action = na.fail))\\n\\n# select 10 random rows;\\n# create two missing values in the regressor variable:\\nm = meuse.grid[sample(nrow(meuse.grid), 10), ]\\nm[c(3,7), \\"dist\\"] = NA\\nkrige(log(zinc)~dist,~x+y,meuse,m, na.action = na.pass)\\nkrige(log(zinc)~dist,~x+y,meuse,m, na.action = na.omit)\\nkrige(log(zinc)~dist,~x+y,meuse,m, na.action = na.exclude)\\ntry(krige(log(zinc)~dist,~x+y,meuse,m, na.action = na.fail))\\n" }\n'
line: b'{"repo_name":"michalkurka/h2o-3","ref":"refs/heads/master","path":"h2o-py/tests/testdir_algos/pca/pyunit_pubdev_4961_pca_implementations.py","content":"from __future__ import print_function\\nfrom builtins import str\\nfrom builtins import range\\nimport sys\\nsys.path.insert(1,\\"../../../\\")\\nimport h2o\\nfrom tests import pyunit_utils\\nfrom h2o.estimators.pca import H2OPrincipalComponentAnalysisEstimator as H2OPCA\\n\\n\\ndef pca_arrests():\\n  print(\\"Importing USArrests.csv data...\\")\\n  arrestsH2O = h2o.upload_file(pyunit_utils.locate(\\"smalldata/pca_test/USArrests.csv\\"))\\n\\n  print(\\"Testing to see whether the trained PCA are essentially the same using different implementation...\\")\\n  \\n  eigenvector_standard = None\\n  for impl in [\\"MTJ_EVD_DENSEMATRIX\\", \\"MTJ_EVD_SYMMMATRIX\\", \\"MTJ_SVD_DENSEMATRIX\\", \\"JAMA\\"]:\\n    print(\\"Run PCA with implementation: \\" + impl)\\n    model = H2OPCA(k = 4, pca_impl=impl, seed=1234)\\n    model.train(x=list(range(4)), training_frame=arrestsH2O)\\n    eigenvectors = model._model_json[\\"output\\"][\\"eigenvectors\\"]\\n    if eigenvector_standard is not None:\\n      # Compare to see if they are fundamentally the same\\n      pyunit_utils.assert_H2OTwoDimTable_equal(\\n        eigenvector_standard,\\n        eigenvectors,\\n        model._model_json[\\"output\\"][\\"names\\"],\\n        tolerance=1e-6,\\n        check_sign=True,\\n        check_all=False)\\n    else:\\n      eigenvector_standard = eigenvectors\\n\\nif __name__ == \\"__main__\\":\\n  pyunit_utils.standalone_test(pca_arrests)\\nelse:\\n  pca_arrests()\\n"}\n'
line: b'{"repo_name":"carragom/modoboa","ref":"refs/heads/master","path":"modoboa/admin/models/domain_alias.py","content":"\\"\\"\\"Models related to domain aliases management.\\"\\"\\"\\n\\nfrom django.db import models\\nfrom django.utils.encoding import python_2_unicode_compatible, smart_text\\nfrom django.utils.translation import ugettext as _, ugettext_lazy\\n\\nfrom django.contrib.contenttypes.fields import GenericRelation\\n\\nfrom reversion import revisions as reversion\\n\\nfrom modoboa.core import models as core_models\\nfrom modoboa.core import signals as core_signals\\nfrom modoboa.lib.exceptions import BadRequest, Conflict\\n\\nfrom .base import AdminObject\\nfrom .domain import Domain\\n\\n\\nclass DomainAliasManager(models.Manager):\\n\\n    def get_for_admin(self, admin):\\n        \\"\\"\\"Return the domain aliases belonging to this admin.\\n\\n        The result is a ``QuerySet`` object, so this function can be used\\n        to fill ``ModelChoiceField`` objects.\\n        \\"\\"\\"\\n        if admin.is_superuser:\\n            return self.get_queryset()\\n        return self.get_queryset().filter(owners__user=admin)\\n\\n\\n@python_2_unicode_compatible\\nclass DomainAlias(AdminObject):\\n\\n    \\"\\"\\"Domain aliases.\\"\\"\\"\\n\\n    name = models.CharField(ugettext_lazy(\\"name\\"), max_length=100, unique=True,\\n                            help_text=ugettext_lazy(\\"The alias name\\"))\\n    target = models.ForeignKey(\\n        Domain, verbose_name=ugettext_lazy(\'target\'),\\n        help_text=ugettext_lazy(\\"The domain this alias points to\\")\\n    )\\n    enabled = models.BooleanField(\\n        ugettext_lazy(\'enabled\'),\\n        help_text=ugettext_lazy(\\"Check to activate this alias\\"),\\n        default=True\\n    )\\n\\n    owners = GenericRelation(core_models.ObjectAccess)\\n\\n    objects = DomainAliasManager()\\n\\n    class Meta:\\n        permissions = (\\n            (\\"view_domaliases\\", \\"View domain aliases\\"),\\n        )\\n        app_label = \\"admin\\"\\n\\n    def __str__(self):\\n        return smart_text(self.name)\\n\\n    def from_csv(self, user, row):\\n        \\"\\"\\"Create a domain alias from a CSV row\\n\\n        Expected format: [\\"domainalias\\", domain alias name, targeted domain, enabled]\\n\\n        :param user: a ``User`` object\\n        :param row: a list containing the alias definition\\n        \\"\\"\\"\\n        if len(row) \\u003c 4:\\n            raise BadRequest(_(\\"Invalid line\\"))\\n        self.name = row[1].strip()\\n        for model in [DomainAlias, Domain]:\\n            if model.objects.filter(name=self.name).exists():\\n                raise Conflict\\n        domname = row[2].strip()\\n        try:\\n            self.target = Domain.objects.get(name=domname)\\n        except Domain.DoesNotExist:\\n            raise BadRequest(_(\\"Unknown domain %s\\") % domname)\\n        core_signals.can_create_object.send(\\n            sender=\\"import\\", context=self.target, object_type=\\"domain_aliases\\")\\n        self.enabled = row[3].strip() in [\\"True\\", \\"1\\", \\"yes\\", \\"y\\"]\\n        self.save(creator=user)\\n\\n    def to_csv(self, csvwriter):\\n        \\"\\"\\"Export a domain alias using CSV format\\n\\n        :param csvwriter: a ``csv.writer`` object\\n        \\"\\"\\"\\n        csvwriter.writerow([\\"domainalias\\", self.name,\\n                            self.target.name, self.enabled])\\n\\nreversion.register(DomainAlias)\\n"}\n'
line: b'{"repo_name":"roandelyf/iTerm2","ref":"refs/heads/master","path":"tests/esctest/tests/el.py","content":"from esc import NUL, blank\\nimport escargs\\nimport esccmd\\nimport escio\\nfrom esctypes import Point, Rect\\nfrom escutil import AssertEQ, AssertScreenCharsInRectEqual, GetCursorPosition, knownBug\\n\\nclass ELTests(object):\\n  def prepare(self):\\n    \\"\\"\\"Initializes the screen to abcdefghij on the first line with the cursor\\n    on the \'e\'.\\"\\"\\"\\n    esccmd.CUP(Point(1, 1))\\n    escio.Write(\\"abcdefghij\\")\\n    esccmd.CUP(Point(5, 1))\\n\\n  def test_EL_Default(self):\\n    \\"\\"\\"Should erase to right of cursor.\\"\\"\\"\\n    self.prepare()\\n    esccmd.EL()\\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\\n                                 [ \\"abcd\\" + 6 * NUL ])\\n\\n  def test_EL_0(self):\\n    \\"\\"\\"Should erase to right of cursor.\\"\\"\\"\\n    self.prepare()\\n    esccmd.EL(0)\\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\\n                                 [ \\"abcd\\" + 6 * NUL ])\\n\\n  def test_EL_1(self):\\n    \\"\\"\\"Should erase to left of cursor.\\"\\"\\"\\n    self.prepare()\\n    esccmd.EL(1)\\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\\n                                 [ 5 * blank() + \\"fghij\\" ])\\n\\n  def test_EL_2(self):\\n    \\"\\"\\"Should erase whole line.\\"\\"\\"\\n    self.prepare()\\n    esccmd.EL(2)\\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\\n                                 [ 10 * NUL ])\\n\\n  def test_EL_IgnoresScrollRegion(self):\\n    \\"\\"\\"Should erase whole line.\\"\\"\\"\\n    self.prepare()\\n    esccmd.DECSET(esccmd.DECLRMM)\\n    esccmd.DECSLRM(2, 4)\\n    esccmd.CUP(Point(5, 1))\\n    esccmd.EL(2)\\n    esccmd.DECRESET(esccmd.DECLRMM)\\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\\n                                 [ 10 * NUL ])\\n\\n  def test_EL_doesNotRespectDECProtection(self):\\n    \\"\\"\\"EL respects DECSCA.\\"\\"\\"\\n    escio.Write(\\"a\\")\\n    escio.Write(\\"b\\")\\n    esccmd.DECSCA(1)\\n    escio.Write(\\"c\\")\\n    esccmd.DECSCA(0)\\n    esccmd.CUP(Point(1, 1))\\n    esccmd.EL(2)\\n    AssertScreenCharsInRectEqual(Rect(1, 1, 3, 1),\\n                                 [ NUL * 3 ])\\n\\n  @knownBug(terminal=\\"iTerm2\\",\\n            reason=\\"Protection not implemented.\\")\\n  def test_EL_respectsISOProtection(self):\\n    \\"\\"\\"EL respects SPA/EPA.\\"\\"\\"\\n    escio.Write(\\"a\\")\\n    escio.Write(\\"b\\")\\n    esccmd.SPA()\\n    escio.Write(\\"c\\")\\n    esccmd.EPA()\\n    esccmd.CUP(Point(1, 1))\\n    esccmd.EL(2)\\n    AssertScreenCharsInRectEqual(Rect(1, 1, 3, 1),\\n                                 [ blank() * 2 + \\"c\\" ])\\n\\n"}\n'
line: b'{"repo_name":"yatinkumbhare/openstack-nova","ref":"refs/heads/master","path":"nova/db/sqlalchemy/migrate_repo/versions/284_placeholder.py","content":"#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\n# This is a placeholder for Kilo backports.\\n# Do not use this number for new Liberty work.  New work starts after\\n# all the placeholders.\\n#\\n# See this for more information:\\n# http://lists.openstack.org/pipermail/openstack-dev/2013-March/006827.html\\n\\n\\ndef upgrade(migrate_engine):\\n    pass\\n"}\n'
line: b'{ "repo_name": "edzer/gstat", "ref": "refs/heads/master", "path": "demo/grass.R", "content": "# $Id: grass.R,v 1.4 2006-02-10 19:05:02 edzer Exp $\\n# this demo assumes quite a lot:\\n#  a. it assumes GRASS gis is running\\n#  b. it assumes that the meuse data zinc variable is available as a site list\\n#  c. it assumes that mask_map is present, and contains the mask map values\\n#     (i.e., the study area)\\n\\nlibrary(sp)\\nlibrary(GRASS)           # load R GRASS interface\\n\\nG = gmeta()              # retrieves active data base locations and topology\\nd = sites.get(G, \\"zinc\\") # retrieve zinc observations \\nplot(d$east, d$north, asp=1)\\nnames(d)[4] = \\"zinc\\"     # rename attribute\\nmask = rast.get\\n\\nhist(d$zinc)\\nhist(log(d$zinc))\\n\\nmask = rast.get(G, \\"mask_map\\")\\nplot(G, mask$mask.map)\\npoints(d$east,d$north, pch=\\"+\\")\\n\\nlibrary(gstat)           # load gstat library\\n\\nbubble(d, zcol = \\"zinc\\", col=c(4,5), maxsize=2)\\n\\n# explain S formulae: ~\\nv = variogram(log(zinc)~1, ~east+north, d)\\nplot(v)\\n\\nv.mod = vgm(.6, \\"Sph\\", 900, .1)\\nplot(v, model = v.mod)\\n\\nv.fit = fit.variogram(v, v.mod)\\nplot(v, model = v.fit)\\n\\nzinc.g = gstat(NULL, \\"lzinc\\", log(zinc)~1, ~east+north, d, model = v.fit)\\nnew.data = data.frame(east = east(G), north = north(G))\\nnew.data[is.na(mask$mask.map), ] = c(NA,NA)\\n\\nzinc.kr = predict(zinc.g, new.data)\\nimage(zinc.kr)\\n\\nlibrary(lattice)\\n\\nlevelplot(lzinc.pred~east+north, zinc.kr, asp=1.34, col.regions=bpy.colors(100))\\n\\n# push prediction and variances grids back into GRASS data base:\\nrast.put(G, \\"lzinc.pred\\", zinc.kr$lzinc.pred)\\nrast.put(G, \\"lzinc.var\\",  zinc.kr$lzinc.var)\\n\\n# push cross validation residuals back to GRASS data base:\\nxv = krige.cv(log(zinc)~1, ~east+north, d, v.fit, nmax = 40, verb=F)\\nsites.put2(G, data = xv, dims = c(\\"east\\", \\"north\\", \\"residual\\", \\"zscore\\"), \\n\\tlname = \\"lzinc.xv\\")\\n" }\n'
line: b'{ "repo_name": "edzer/gstat", "ref": "refs/heads/master", "path": "R/xyz2img.R", "content": "# $Id: xyz2img.q,v 1.4 2006-02-10 19:01:07 edzer Exp $\\n\\n\\"xyz2img\\" <-\\nfunction (xyz, zcol = 3, xcol = 1, ycol = 2, tolerance = 10 * .Machine$double.eps) \\n{\\n    if (ncol(xyz) < 3) \\n        stop(\\"xyz object should have at least three columns\\")\\n    z = xyz[, zcol]\\n    x = xyz[, xcol]\\n    y = xyz[, ycol]\\n    xx = sort(unique(x))\\n    yy = sort(unique(y))\\n    nx = length(xx)\\n    ny = length(yy)\\n    nmax = max(nx, ny)\\n    difx = diff(xx)\\n    if (diff(range(unique(difx))) > tolerance) \\n        stop(\\"x intervals are not constant\\")\\n    dify = diff(yy)\\n    if (diff(range(unique(dify))) > tolerance) \\n        stop(\\"y intervals are not constant\\")\\n    dx = mean(difx)\\n    dy = mean(dify)\\n    xmin = min(xx)\\n    xmax = max(xx)\\n    xrange = xmax - xmin\\n    ymin = min(yy)\\n    ymax = max(yy)\\n    yrange = ymax - ymin\\n    row = round((x - xmin)/dx) + 1\\n    col = round((y - ymin)/dy) + 1\\n\\tzz = rep(as.numeric(NA), nx * ny)\\n\\tzz[row + nx * (col - 1)] = z\\n\\tzz = matrix(zz, nrow = nx, ncol = ny)\\n    list(x = seq(xmin, xmax, dx), y = seq(ymin, ymax, dy), z = zz)\\n}\\n" }\n'
line: b'{ "repo_name": "edzer/gstat", "ref": "refs/heads/master", "path": "tests/vdist.R", "content": "library(sp)\\nlibrary(gstat)\\n\\ndata(meuse)\\ncoordinates(meuse) = ~x+y\\ndata(meuse.grid)\\ngridded(meuse.grid) = ~x+y\\n\\nmg = meuse.grid\\ngridded(mg) = FALSE\\nmg= mg[1500,]\\nkrige(log(zinc)~1,meuse,mg,vgm(1, \\"Exp\\", 300, anis=c(0,0.01)),\\n\\tvdist=FALSE, maxdist=1000,nmax=10)\\nkrige(log(zinc)~1,meuse,mg,vgm(1, \\"Exp\\", 300, anis=c(0,0.01)),\\n\\tvdist=TRUE, maxdist=1000,nmax=10)\\n" }\n'
line: b'{ "repo_name": "edzer/gstat", "ref": "refs/heads/master", "path": "demo/ikr.R", "content": "library(sp)\\nlibrary(gstat)\\ndata(meuse)\\ndata(meuse.grid)\\ncoordinates(meuse)=~x+y\\ngridded(meuse.grid)=~x+y\\nv = variogram(I(zinc < 500)~1,meuse)\\nplot(v)\\nvm = fit.variogram(v, vgm(1, \\"Sph\\", 300, 1))\\nplot(v,vm)\\nvm\\n# possibly adjust sum of sill to be max. 0.25?\\nik = krige(I(zinc>500)~1, meuse, meuse.grid, vm)\\nspplot(ik[1],col.regions=bpy.colors())\\nsummary(ik[[1]])\\n# adjust values outside [0,1] to nearest limit:\\nik[[1]][ik[[1]]<0] = 0\\nik[[1]][ik[[1]]>1] = 1\\nsummary(ik[[1]])\\nspplot(ik[1],col.regions=bpy.colors())\\n" }\n'
line: b'{ "repo_name": "google/rappor", "ref": "refs/heads/master", "path": "analysis/R/alternative.R", "content": "# Copyright 2014 Google Inc. All rights reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\nlibrary(limSolve)\\nlibrary(Matrix)\\n\\n# The next two functions create a matrix (G) and a vector (H) encoding\\n# linear inequality constraints that a solution vector (x) must satisfy:\\n#                       G * x >= H\\n\\n# Currently represent three sets of constraints on the solution vector:\\n#  - all solution coefficients are nonnegative\\n#  - the sum total of all solution coefficients is no more than 1\\n#  - in each of the coordinates of the target vector (estimated Bloom filter)\\n#    we don\'t overshoot by more than three standard deviations.\\nMakeG <- function(n, X) {\\n  d <- Diagonal(n)\\n  last <- rep(-1, n)\\n  rbind2(rbind2(d, last), -X)\\n}\\n\\nMakeH <- function(n, Y, stds) {\\n  # set the floor at 0.01 to avoid degenerate cases\\n  YY <- apply(Y + 3 * stds,  # in each bin don\'t overshoot by more than 3 stds\\n              1:2,\\n              function(x) min(1, max(0.01, x)))  # clamp the bound to [0.01,1]\\n\\n  c(rep(0, n),  # non-negativity condition\\n    -1,         # coefficients sum up to no more than 1\\n    -as.vector(t(YY))   # t is important!\\n    )\\n}\\n\\nMakeLseiModel <- function(X, Y, stds) {\\n  m <- dim(X)[1]\\n  n <- dim(X)[2]\\n\\n# no slack variables for now\\n#   slack <- Matrix(FALSE, nrow = m, ncol = m, sparse = TRUE)\\n#   colnames(slack) <- 1:m\\n#   diag(slack) <- TRUE\\n#\\n#   G <- MakeG(n + m)\\n#   H <- MakeH(n + m)\\n#\\n#   G[n+m+1,n:(n+m)] <- -0.1\\n#  A = cbind2(X, slack)\\n\\n  w <- as.vector(t(1 / stds))\\n  w_median <- median(w[!is.infinite(w)])\\n  if(is.na(w_median))  # all w are infinite\\n    w_median <- 1\\n  w[w > w_median * 2] <- w_median * 2\\n  w <- w / mean(w)\\n\\n  list(# coerce sparse Boolean matrix X to sparse numeric matrix\\n       A = Diagonal(x = w) %*% (X + 0),\\n       B = as.vector(t(Y)) * w,  # transform to vector in the row-first order\\n       G = MakeG(n, X),\\n       H = MakeH(n, Y, stds),\\n       type = 2)  # Since there are no equality constraints, lsei defaults to\\n                  # solve.QP anyway, but outputs a warning unless type == 2.\\n}\\n\\n# CustomLM(X, Y)\\nConstrainedLinModel <- function(X,Y) {\\n  model <- MakeLseiModel(X, Y$estimates, Y$stds)\\n  coefs <- do.call(lsei, model)$X\\n  names(coefs) <- colnames(X)\\n\\n  coefs\\n}" }\n'
line: b'{ "repo_name": "google/rappor", "ref": "refs/heads/master", "path": "analysis/R/fast_em.R", "content": "# fast_em.R: Wrapper around analysis/cpp/fast_em.cc.\\n#\\n# This serializes the input, shells out, and deserializes the output.\\n\\n.Flatten <- function(list_of_matrices) {\\n  listOfVectors <- lapply(list_of_matrices, as.vector)\\n  #print(listOfVectors)\\n\\n  # unlist takes list to vector.\\n  unlist(listOfVectors)\\n}\\n\\n.WriteListOfMatrices <- function(list_of_matrices, f) {\\n  flattened <- .Flatten(list_of_matrices)\\n\\n  # NOTE: UpdateJointConditional does outer product of dimensions!\\n\\n  # 3 letter strings are null terminated\\n  writeBin(\'ne \', con = f)\\n  num_entries <- length(list_of_matrices)\\n  writeBin(num_entries, con = f)\\n\\n  Log(\'Wrote num_entries = %d\', num_entries)\\n\\n  # For 2x3, this is 6\\n  writeBin(\'es \', con = f)\\n\\n  entry_size <- as.integer(prod(dim(list_of_matrices[[1]])))\\n  writeBin(entry_size, con = f)\\n\\n  Log(\'Wrote entry_size = %d\', entry_size)\\n\\n  # now write the data\\n  writeBin(\'dat\', con = f)\\n  writeBin(flattened, con = f)\\n}\\n\\n.ExpectTag <- function(f, tag) {\\n  # Read a single NUL-terminated character string.\\n  actual <- readBin(con = f, what = \\"char\\", n = 1)\\n\\n  # Assert that we got what was expected.\\n  if (length(actual) != 1) {\\n    stop(sprintf(\\"Failed to read a tag \'%s\'\\", tag))\\n  }\\n  if (actual != tag) {\\n    stop(sprintf(\\"Expected \'%s\', got \'%s\'\\", tag, actual))\\n  }\\n}\\n\\n.ReadResult <- function (f, entry_size, matrix_dims) {\\n  .ExpectTag(f, \\"emi\\")\\n  # NOTE: assuming R integers are 4 bytes (uint32_t)\\n  num_em_iters <- readBin(con = f, what = \\"int\\", n = 1)\\n\\n  .ExpectTag(f, \\"pij\\")\\n  pij <- readBin(con = f, what = \\"double\\", n = entry_size)\\n\\n  # Adjust dimensions\\n  dim(pij) <- matrix_dims\\n\\n  Log(\\"Number of EM iterations: %d\\", num_em_iters)\\n  Log(\\"PIJ read from external implementation:\\")\\n  print(pij)\\n   \\n  # est, sd, var_cov, hist\\n  list(est = pij, num_em_iters = num_em_iters)\\n}\\n\\n.SanityChecks <- function(joint_conditional) {\\n  # Display some stats before sending it over to C++.\\n\\n  inf_counts <- lapply(joint_conditional, function(m) {\\n    sum(m == Inf)\\n  })\\n  total_inf <- sum(as.numeric(inf_counts))\\n\\n  nan_counts <- lapply(joint_conditional, function(m) {\\n    sum(is.nan(m))\\n  })\\n  total_nan <- sum(as.numeric(nan_counts))\\n\\n  zero_counts <- lapply(joint_conditional, function(m) {\\n    sum(m == 0.0)\\n  })\\n  total_zero <- sum(as.numeric(zero_counts))\\n\\n  #sum(joint_conditional[joint_conditional == Inf, ])\\n  Log(\'total inf: %s\', total_inf)\\n  Log(\'total nan: %s\', total_nan)\\n  Log(\'total zero: %s\', total_zero)\\n}\\n\\nConstructFastEM <- function(em_executable, tmp_dir) {\\n\\n  return(function(joint_conditional, max_em_iters = 1000,\\n                  epsilon = 10 ^ -6, verbose = FALSE,\\n                  estimate_var = FALSE) {\\n    matrix_dims <- dim(joint_conditional[[1]])\\n    # Check that number of dimensions is 2.\\n    if (length(matrix_dims) != 2) {\\n      Log(\'FATAL: Expected 2 dimensions, got %d\', length(matrix_dims))\\n      stop()\\n    }\\n\\n    entry_size <- prod(matrix_dims)\\n    Log(\'entry size: %d\', entry_size)\\n\\n    .SanityChecks(joint_conditional)\\n\\n    input_path <- file.path(tmp_dir, \'list_of_matrices.bin\')\\n    Log(\\"Writing flattened list of matrices to %s\\", input_path)\\n    f <- file(input_path, \'wb\')  # binary file\\n    .WriteListOfMatrices(joint_conditional, f)\\n    close(f)\\n    Log(\\"Done writing %s\\", input_path)\\n     \\n    output_path <- file.path(tmp_dir, \'pij.bin\')\\n\\n    cmd <- sprintf(\\"%s %s %s %s\\", em_executable, input_path, output_path,\\n                   max_em_iters)\\n\\n    Log(\\"Shell command: %s\\", cmd)\\n    exit_code <- system(cmd)\\n\\n    Log(\\"Done running shell command\\")\\n    if (exit_code != 0) {\\n      stop(sprintf(\\"Command failed with code %d\\", exit_code))\\n    }\\n\\n    f <- file(output_path, \'rb\')\\n    result <- .ReadResult(f, entry_size, matrix_dims)\\n    close(f)\\n\\n    result\\n  })\\n}\\n" }\n'
line: b'{ "repo_name": "kwanjeeraw/grinn", "ref": "refs/heads/master", "path": "R/getGrinnDb.R", "content": "#\'Get Grinn database location\\n#\'@description get Grinn database location of the current working envornment. \\n#\'By default Grinn database lacation is http://grinn.genomecenter.ucdavis.edu:7474/db/data/cypher \\n#\'which contains only human database.\\n#\'@usage getGrinnDb()\\n#\'@return url of Grinn database location.\\n#\'@author Kwanjeera W \\\\email{kwanich@@ucdavis.edu}\\n#\'@export\\ngetGrinnDb <- function(){\\n  print(nld)\\n}" }\n'
line: b'{ "repo_name": "kwanjeeraw/grinn", "ref": "refs/heads/master", "path": "R/fetchtRelation.R", "content": "#\' \\\\code{fetchRelation} get path information\\n#\'@description get path information as the output for further uses by \\\\code{formatNetworkOutput}.\\n#\'@seealso \\\\code{formatNetworkOutput}\\n#\'#result <- fetchRelation(\\"http://localhost:7474/db/data/relationship/53\\")\\n#\'#return start-relation-end\\n\\nfetchRelation <- function(url){\\n  out <- tryCatch(\\n  {\\n    path = curlRequestUrlToList(url)\\n    start = curlRequestUrlToList(path$start)\\n    end = curlRequestUrlToList(path$end)\\n    type = path$type\\n    dataSource = path$data$source\\n    startGID = start$data$GID\\n    startName = start$data$name\\n    startXref = paste0(start$data$xref,collapse = \\"||\\")\\n    startLabel = start$metadata$labels[[1]]\\n    endGID = end$data$GID\\n    endName = end$data$name\\n    endXref = paste0(end$data$xref,collapse = \\"||\\")\\n    endLabel = end$metadata$labels[[1]]\\n    ## Set the name for the class\\n    relation = list(startGID=startGID, startName=startName, startXref=startXref, startLabel=startLabel, \\n                    endGID=endGID, endName=endName, endXref=endXref, endLabel=endLabel, type=type, dataSource=dataSource)\\n  },\\n  error=function(e) {\\n    message(e)\\n    cat(\\"\\\\n..RETURN empty list of relations\\")\\n    out = list() # Choose a return value in case of error\\n  })    \\n  return(out)\\n}\\n\\nfetchRelation.TRANSACTION <- function(graph){\\n  out <- tryCatch(\\n    {\\n      ## Set the name for the class\\n      data.frame(t(sapply(graph$graph$relationships, \\n                                 function(x) list(source=x$startNode, target=x$endNode, relname=x$type, relsource=x$properties[\\"source\\"]))))\\n      \\n      #relationInfo = list(source=graph$graph$relationships[[1]]$startNode, target=graph$graph$relationships[[1]]$endNode, relname=graph$graph$relationships[[1]]$type, relsource=graph$graph$relationships[[1]]$properties[\\"source\\"])\\n    },\\n    error=function(e) {\\n      message(e)\\n      cat(\\"\\\\n..RETURN empty list of relations\\")\\n      out = data.frame() # Choose a return value in case of error\\n    })    \\n  return(out)\\n}\\n\\nfetchNode.TRANSACTION <- function(node){\\n  out <- tryCatch(\\n    {\\n      data.frame(t(sapply(node, \\n                          function(x) list(id=x$id, gid=x$properties$GID, nodename=x$properties$name, xref=paste0(x$properties$xref,collapse = \\"||\\"), nodetype=x$labels))))\\n      #nodeInfo = list(id=node$id, gid=node$properties$GID, nodename=node$properties$name, xref=paste0(node$properties$xref,collapse = \\"||\\"), nodetype=node$labels)\\n    },\\n    error=function(e) {\\n      message(e)\\n      cat(\\"\\\\n..RETURN empty list of relations\\")\\n      out = data.frame() # Choose a return value in case of error\\n    })    \\n  return(out)\\n}" }\n'
line: b'{ "repo_name": "kwanjeeraw/grinn", "ref": "refs/heads/master", "path": "inst/shiny/layout/setdb.R", "content": "mainPanel(width=12,\\n          fluidRow(column(12,\\n                          mainPanel(width=12,\\n                                    h3(\\"setGrinnDb\\"),\\n                                    p(\\"Set the graph database location for the currently working environment, see \\",\\n                                      a(href=\'http://kwanjeeraw.github.io/grinn/setdb.html\',target=\'_blank\',\'here\'),\' for argument details.\'\\n                                    )\\n                          )#end mainPanel\\n          )),\\n          wellPanel(\\n            h4(\\"Current database location:\\"),\\n            fluidRow(\\n              column(12, verbatimTextOutput(\\"currentdb\\"))\\n            ),\\n            h4(\\"Input arguments:\\"), helpText(\\"* required field\\"),\\n            fluidRow(\\n              column(6, textInput(inputId=\'dburl\', label=\'url *\', value=\\"\\"))\\n            ),\\n            hr(),\\n            actionButton(\\"submit\\",\\"Submit\\")\\n          )\\n)" }\n'
line: b'{ "repo_name": "kwanjeeraw/grinn", "ref": "refs/heads/master", "path": "R/fetchNodeRelation.R", "content": "#\' \\\\code{fetchNodeRelation} format get node relationships\\n#\'@description get node relationships as the output for further uses by \\\\code{formatNodeOutput}.\\n#\'@seealso \\\\code{formatNodeOutput}\\n#\'#result <- fetchNodeRelation(\\"http://localhost:7474/db/data/node/53/relationships/in\\")\\n#\'#return start-relation-end\\n\\nfetchNodeRelation <- function(url){\\n  path = curlRequestUrlToList(url)\\n  relations = list()\\n  if(length(path)>0){\\n    for(i in 1:length(path)){\\n      start = curlRequestUrlToList(path[[i]]$start)\\n      end = curlRequestUrlToList(path[[i]]$end)\\n      type = path[[i]]$type\\n      dataSource = path[[i]]$data$source\\n      startGID = start$data$GID\\n      startName = start$data$name\\n      startXref = paste0(start$data$xref,collapse = \\"||\\")\\n      startLabel = start$metadata$labels[[1]]\\n      endGID = end$data$GID\\n      endName = end$data$name\\n      endXref = paste0(end$data$xref,collapse = \\"||\\")\\n      endLabel = end$metadata$labels[[1]]\\n      ## Set the name for the class\\n      relation = data.frame(startGID=startGID, startName=startName, startXref=startXref, startLabel=startLabel, \\n                      endGID=endGID, endName=endName, endXref=endXref, endLabel=endLabel, type=type, dataSource=dataSource)\\n      relations = rbind(relations,relation)\\n    }\\n    relations <- unique(relations)\\n  }else{\\n    #cat(\\"\\\\n..RETURN empty list\\")\\n    relations = list() # Choose a return value in case of error\\n  } \\n  return(relations)\\n}" }\n'
line: b'{ "repo_name": "sestelo/shiny_npregfast", "ref": "refs/heads/master", "path": "ui.R", "content": "#detach(\\"package:npregfast\\")\\nlibrary(shiny)\\n#library(shinyjs)\\nlibrary(miniUI)\\nlibrary(wesanderson)\\nlibrary(npregfast)\\n\\n\\nshinyUI(fluidPage(\\n  title = \\"Demo of npregfast\\",\\n  tags$head(includeCSS(file.path(\'www\', \'style.css\'))),   \\n  shinyjs::useShinyjs(),\\n  \\n  fluidRow(id = \\"title-row\\",\\n           column(12,\\n                  h1(\\"Demo of\\",em(a(\\"npregfast\\", href = \\"https://github.com/sestelo/npregfast\\"))),\\n                  h4(\\"Example with\\", a(\\"barnacle\\", href = \\"https://github.com/sestelo/npregfast/blob/master/man/barnacle.Rd\\"),\\" data set\\"),\\n                  div(\\"Created by\\", a(\\"Marta Sestelo\\", href = \\"http://sestelo.github.io\\"),\\n                      \\"and\\", a(\\"Nora M. Villanueva\\",href = \\"http://noramvillanueva.github.io\\"), HTML(\\"&bull;\\"),\\n                      \\"Code on\\", a(\\"GitHub\\", href = \\"https://github.com/sestelo/shiny_npregfast/\\")\\n                  )\\n           )\\n  ),\\n  \\n  \\n  \\n  div(id = \\"loading-content\\", h2(\\"Loading...\\")),\\n  \\n  fluidRow(id = \\"app-content\\",\\n           column(2, wellPanel(\\n             class = \\"settings\\",\\n             h4(class = \\"settings-title\\", \\"Estimation\\"),\\n             \\n             selectInput(inputId = \\"type\\", \\n                         label = \\"Factor-by-curve interaction?\\",\\n                         choices = c(\\"Without\\" = \\"without\\", \\"With\\" = \\"with\\")),\\n             \\n             selectInput(inputId = \\"kernel\\",\\n                         label = \\"Choose a kernel:\\",\\n                         choices = c(\\"Epanechnikov\\" = \\"epanech\\", \\n                                     \\"Gaussian\\" = \\"gaussian\\",\\n                                     \\"Triangular\\" = \\"triang\\")),\\n             \\n             selectInput(inputId = \\"poly\\",\\n                         label = \\"Polynomial degree:\\",\\n                         choices = c(1, \\n                                     2,\\n                                     3),\\n                         selected = 3),\\n             \\n             radioButtons(inputId = \\"selband\\",\\n                          label = \\"Bandwidth selection:\\",\\n                          choices = c(\\"Cross-validation\\" = \\"cv\\", \\n                                      \\"Manual\\" = \\"man\\"),\\n                          selected = \\"cv\\"),\\n             \\n             conditionalPanel(\\n               condition = \\"input.selband == \'man\'\\",\\n               sliderInput(inputId = \\"band\\",\\n                           label = \\"Bandwidth selection:\\",\\n                           min = 0,\\n                           max = 1,\\n                           value = 0.5,\\n                           step = 0.1, \\n                           ticks = TRUE,\\n                           animate = TRUE))\\n             \\n           )),\\n           \\n           \\n           \\n           \\n           \\n           column(2, wellPanel(\\n             class = \\"settings\\",\\n             h4(class = \\"settings-title\\", \\"Graphical\\"),\\n             \\n             conditionalPanel(\\n               condition = \\"input.poly == 1\\",\\n               checkboxGroupInput(inputId = \\"der1\\",\\n                                  label = \\"Output:\\",\\n                                  choices = c(\\"Conditional mean\\" = \'0\'),\\n                                  selected = \'0\')),\\n             \\n             conditionalPanel(\\n               condition = \\"input.poly == 2\\",\\n               checkboxGroupInput(inputId = \\"der2\\",\\n                                  label = \\"Output:\\",\\n                                  choices = c(\\"Conditional mean\\" = \'0\', \\n                                              \\"First derivative\\" = \'1\'),\\n                                  selected = \'0\')),\\n             \\n             conditionalPanel(\\n               condition = \\"input.poly == 3\\",\\n               checkboxGroupInput(inputId = \\"der3\\",\\n                                  label = \\"Output:\\",\\n                                  choices = c(\\"Conditional mean\\" = \'0\', \\n                                              \\"First derivative\\" = \'1\',\\n                                              \\"Second derivative\\" = \'2\'),\\n                                  selected = \'0\')),\\n             \\n             \\n             \\n             \\n             div(id = \\"marginal-settings\\",\\n                 shinyjs::colourInput(\\"colmu\\", \\"Line color\\", \\"#D67236\\", \\n                                      showColour = \\"background\\",\\n                                      palette = \\"limited\\",\\n                                      allowedCols = unlist(wes_palettes),\\n                                      allowTransparent = FALSE),\\n                 \\n                 \\n                 shinyjs::colourInput(\\"colci\\", \\"CI color\\", \\"#5B1A18\\", \\n                                      showColour = \\"background\\",\\n                                      palette = \\"limited\\",\\n                                      allowedCols = unlist(wes_palettes),\\n                                      allowTransparent = FALSE)\\n             ),\\n             \\n             conditionalPanel(\\n               condition =\\"input.poly == 1 & input.der1[0] == \'0\'||input.poly == 2 & input.der2[0] == \'0\'||input.poly == 3 & input.der3[0] == \'0\'\\",\\n               checkboxInput(\\"show_points\\", \\"Show data points\\", TRUE),\\n               conditionalPanel(\\n                 condition =\\"input.show_points == true\\",\\n                 shinyjs::colourInput(\\"pcol\\", \\"Points color\\", \\"#899DA4\\", \\n                                      showColour = \\"background\\",\\n                                      palette = \\"limited\\",\\n                                      allowedCols = unlist(wes_palettes),\\n                                      allowTransparent = FALSE)\\n               )\\n             )\\n           )),\\n           \\n           \\n           \\n           column(8,\\n                  plotOutput(\\"distPlot\\", \\n                             height = \\"500px\\", \\n                             width = \\"100%\\",\\n                             click = \\"plot1_click\\",\\n                             brush = brushOpts(id = \\"plot1_brush\\"),\\n                  ),\\n                  \\n                  miniButtonBlock(\\n                   \\n                    actionButton(\\"exclude_toggle\\", \\"Toggle points\\",\\n                                 icon = icon(\\"fa fa-codiepie\\", class = \\"fa-1x\\")),\\n                    \\n                    actionButton(\\"exclude_reset\\", \\"Reset\\", \\n                                 icon = icon(\\"fa fa-refresh\\", class = \\"fa-1x\\")),\\n                    actionButton(inputId =\\"info_btn\\", \\n                                 label = \\"Info\\", \\n                                 icon = icon(\\"fa fa-info-circle\\", class = \\"fa-1x\\"))\\n                    \\n                  )\\n                  \\n                  \\n                 # includeMarkdown(\\"plot_shiny.md\\")\\n                  \\n                  \\n                  \\n           )\\n           \\n  )\\n))\\n\\n\\n\\n\\n\\n\\n\\n\\n" }\n'
line: b'{"repo_name":"tacaswell/datamuxer","ref":"refs/heads/master","path":"datamuxer.py","content":"# ######################################################################\\n# Copyright (c) 2014, Brookhaven Science Associates, Brookhaven        #\\n# National Laboratory. All rights reserved.                            #\\n#                                                                      #\\n# Redistribution and use in source and binary forms, with or without   #\\n# modification, are permitted provided that the following conditions   #\\n# are met:                                                             #\\n#                                                                      #\\n# * Redistributions of source code must retain the above copyright     #\\n#   notice, this list of conditions and the following disclaimer.      #\\n#                                                                      #\\n# * Redistributions in binary form must reproduce the above copyright  #\\n#   notice this list of conditions and the following disclaimer in     #\\n#   the documentation and/or other materials provided with the         #\\n#   distribution.                                                      #\\n#                                                                      #\\n# * Neither the name of the Brookhaven Science Associates, Brookhaven  #\\n#   National Laboratory nor the names of its contributors may be used  #\\n#   to endorse or promote products derived from this software without  #\\n#   specific prior written permission.                                 #\\n#                                                                      #\\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS  #\\n# \\"AS IS\\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT    #\\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS    #\\n# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE       #\\n# COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,           #\\n# INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES   #\\n# (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR   #\\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)   #\\n# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,  #\\n# STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OTHERWISE) ARISING   #\\n# IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE   #\\n# POSSIBILITY OF SUCH DAMAGE.                                          #\\n########################################################################\\nfrom __future__ import (absolute_import, division, print_function,\\n                        unicode_literals)\\nimport six\\nfrom collections import namedtuple, deque\\nimport logging\\nimport pandas as pd\\nimport tzlocal\\nimport numpy as np\\nfrom scipy.interpolate import interp1d\\nimport pandas.core.groupby  # to get custom exception\\n\\n\\nlogger = logging.getLogger(__name__)\\n__all__ = [\'DataMuxer\', \'dataframe_to_dict\']\\n\\nTZ = str(tzlocal.get_localzone())\\n\\n\\nclass BinningError(Exception):\\n    \\"\\"\\"\\n    An exception to raise if there are insufficient sampling rules to\\n    upsampling or downsample a data column into specified bins.\\n    \\"\\"\\"\\n    pass\\n\\n\\nclass BadDownsamplerError(Exception):\\n    \\"\\"\\"\\n    An exception to raise if a downsampler produces unexpected output.\\n    \\"\\"\\"\\n    pass\\n\\n\\nclass ColSpec(namedtuple(\\n              \'ColSpec\', [\'name\', \'ndim\', \'shape\', \'upsample\', \'downsample\'])):\\n    \\"\\"\\"\\n    Named-tuple sub-class to validate the column specifications for the\\n    DataMuxer\\n\\n    Parameters\\n    ----------\\n    name : hashable\\n    ndim : uint\\n        Dimensionality of the data stored in the column\\n    shape : tuple or None\\n        like ndarray.shape, where 0 or None are scalar\\n    upsample : {None, \'linear\', \'nearest\', \'zero\', \'slinear\', \'quadratic\', \'cubic\', \'ffill\', \'bfill\'}\\n        None means that each time bin must have at least one value.\\n        The names refer to kinds of scipy.interpolator. See documentation\\n        link below.\\n    downsample : None or a function\\n        None if the data cannot be downsampled (reduced). Otherwise,\\n        any callable that reduces multiple data points (of whatever dimension)\\n        to a single data point.\\n\\n    References\\n    ----------\\n    http://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html\\n    \\"\\"\\"\\n    # These reflect the \'method\' argument of pandas.DataFrame.fillna\\n    upsampling_methods = {\'None\', \'linear\', \'nearest\', \'zero\', \'slinear\',\\n                          \'quadratic\', \'cubic\', \'ffill\', \'bfill\'}\\n    downsampling_methods = {\'None\', \'last\', \'first\', \'median\', \'mean\', \'sum\',\\n                            \'min\', \'max\'}\\n    _downsample_mapping = {\'last\': lambda x: x[-1],\\n                           \'first\': lambda x: x[0],\\n                           # new in np 1.9\\n                           \'median\': lambda x: np.median(x, 0),\\n                           \'mean\': lambda x: np.mean(x, 0),\\n                           \'sum\': lambda x: np.sum(x, 0),\\n                           \'min\': lambda x: np.min(x, 0),\\n                           \'max\': lambda x: np.max(x, 0)}\\n\\n    __slots__ = ()\\n\\n    def __new__(cls, name, ndim, shape, upsample, downsample):\\n        # Validations\\n        upsample = _validate_upsample(upsample)\\n        downsample = _validate_downsample(downsample)\\n        if int(ndim) \\u003c 0:\\n            raise ValueError(\\"ndim must be positive not {}\\".format(ndim))\\n        if shape is not None:\\n            shape = tuple(shape)\\n\\n        return super(ColSpec, cls).__new__(\\n            cls, name, int(ndim), shape, upsample, downsample)\\n\\n\\ndef _validate_upsample(input):\\n    # TODO The upsampling method could be any callable.\\n    if input is None or input == \'None\':\\n        return \'None\'\\n    if not (input in ColSpec.upsampling_methods):\\n        raise ValueError(\\"{} is not a valid upsampling method. It \\"\\n                         \\"must be one of {}\\".format(\\n                             input, ColSpec.upsampling_methods))\\n    return input.lower()\\n\\n\\ndef _validate_downsample(input):\\n    # TODO The downsampling methods could have string aliases like \'mean\'.\\n    if (input is not None) and (not (callable(input) or\\n                                     input in ColSpec.downsampling_methods)):\\n        raise ValueError(\\"The downsampling method must be a callable, None, \\"\\n                         \\"or one of {}.\\".format(ColSpec.downsampling_methods))\\n    if input is None:\\n        return \'None\'\\n    return input\\n\\n\\nclass DataMuxer(object):\\n    \\"\\"\\"\\n    This class provides a wrapper layer of signals and slots\\n    around a pandas DataFrame to make plugging stuff in for live\\n    view easier.\\n\\n    The data collection/event model being used is all measurements\\n    (that is values that come off of the hardware) are time stamped\\n    to ring time.\\n\\n    The language being used through out is that of pandas data frames.\\n\\n    The data model is that of a sparse table keyed on time stamps which\\n    is \'densified\' on demand by propagating measurements forwards.  Not\\n    all measurements (ex images) can be filled.  This behavior is controlled\\n    by the `col_info` tuple.\\n\\n\\n    Parameters\\n    ----------\\n    events : list\\n        list of Events (any object with the expected attributes will do)\\n    \\"\\"\\"\\n    class Planner(object):\\n        def __init__(self, dm):\\n            self.dm = dm\\n\\n        def determine_upsample(self, interpolation=None, use_cols=None):\\n            \\"Resolve (and if necessary validate) upsampling rules.\\"\\n            if interpolation is None:\\n                interpolation = dict()\\n            if use_cols is None:\\n                use_cols = self.dm.columns\\n            rules = dict()\\n            for name in use_cols:\\n                col_info = self.dm.col_info[name]\\n                rule = _validate_upsample(\\n                    interpolation.get(name, col_info.upsample))\\n                rule = _normalize_string_none(rule)\\n                if (rule is not None) and (col_info.ndim \\u003e 0):\\n                    raise NotImplementedError(\\n                        \\"Only scalar data can be upsampled. \\"\\n                        \\"The {0}-dimensional source {1} was given the \\"\\n                        \\"upsampling rule {2}.\\".format(\\n                            col_info.ndim, name, rule))\\n                rules[name] = rule\\n            return rules\\n\\n        def determine_downsample(self, agg=None, use_cols=None):\\n            \\"Resolve (and if necessary validate) sampling rules.\\"\\n            if agg is None:\\n                agg = dict()\\n            if use_cols is None:\\n                use_cols = self.dm.columns\\n            rules = dict()\\n            for name in use_cols:\\n                col_info = self.dm.col_info[name]\\n                rule = _validate_downsample(agg.get(name, col_info.downsample))\\n                rule = _normalize_string_none(rule)\\n                rules[name] = rule\\n            return rules\\n\\n        def bin_by_edges(self, bin_edges, bin_anchors, interpolation=None,\\n                         agg=None, use_cols=None):\\n            \\"\\"\\"Explain operation of DataMuxer.bin_by_edges\\n\\n            Parameters\\n            ----------\\n            bin_edges : list\\n                list of two-element items like [(t1, t2), (t3, t4), ...]\\n            bin_anchors : list\\n                These are time points where interpolated values will be\\n                evaluated. Bin centers are usually a good choice.\\n            interpolation : dict, optional\\n                Override the default interpolation (upsampling) behavior of any\\n                data source by passing a dictionary of source names mapped onto\\n                one of the following interpolation methods.\\n\\n                {None, \'linear\', \'nearest\', \'zero\', \'slinear\', \'quadratic\',\\n                \'cubic\', \'ffill\', \'bfill\'}\\n\\n                None means that each time bin must have at least one value.\\n                See scipy.interpolator for more on the other methods.\\n            agg : dict, optional\\n                Override the default reduction (downsampling) behavior of any\\n                data source by passing a dictionary of source names mapped onto\\n                any callable that reduces multiple data points (of whatever\\n                dimension) to a single data point.\\n            use_cols : list, optional\\n                List of columns to include in binning; use all columns by\\n                default.\\n\\n            Returns\\n            -------\\n            df : pandas.DataFrame\\n                table giving upsample and downsample rules for each data column\\n                and indicating whether those rules are applicable\\n\\n            References\\n            ----------\\n            http://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html\\n            \\"\\"\\"\\n            bin_anchors, binning = self.dm._bin_by_edges(bin_anchors, bin_edges)\\n            # TODO Cache the grouping for reuse by resample.\\n            grouped = self.dm._dataframe.groupby(binning)\\n            counts = grouped.count()\\n            df = pd.DataFrame.from_dict(_is_resampling_applicable(counts))\\n            df[\'upsample\'] = self.determine_upsample(interpolation, use_cols)\\n            df[\'downsample\'] = self.determine_downsample(agg, use_cols)\\n            return df\\n\\n        def bin_on(self, source_name, interpolation=None, agg=None,\\n                   use_cols=None):\\n            \\"\\"\\"Explain operation of DataMuxer.bin_on.\\n\\n            Parameters\\n            ----------\\n            source_name : string\\n            interpolation : dict, optional\\n                Override the default interpolation (upsampling) behavior of any\\n                data source by passing a dictionary of source names mapped onto\\n                one of the following interpolation methods.\\n\\n                {None, \'linear\', \'nearest\', \'zero\', \'slinear\', \'quadratic\',\\n                \'cubic\'}\\n\\n                None means that each time bin must have at least one value.\\n                See scipy.interpolator for more on the other methods.\\n            agg : dict, optional\\n                Override the default reduction (downsampling) behavior of any\\n                data source by passing a dictionary of source names mapped onto\\n                any callable that reduces multiple data points (of whatever\\n                dimension) to a single data point.\\n            use_cols : list, optional\\n                List of columns to include in binning; use all columns by\\n                default.\\n\\n            Returns\\n            -------\\n            df : pandas.DataFrame\\n                table giving upsample and downsample rules for each data column\\n                and indicating whether those rules are applicable\\n\\n            References\\n            ----------\\n            http://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html\\n            \\"\\"\\"\\n            centers, bin_edges = self.dm._bin_on(source_name)\\n            bin_anchors, binning = self.dm._bin_by_edges(centers, bin_edges)\\n            # TODO Cache the grouping for reuse by resample.\\n            grouped = self.dm._dataframe.groupby(binning)\\n            counts = grouped.count()\\n            df = pd.DataFrame.from_dict(_is_resampling_applicable(counts))\\n            df[\'upsample\'] = self.determine_upsample(interpolation, use_cols)\\n            df[\'downsample\'] = self.determine_downsample(agg, use_cols)\\n            return df\\n\\n    default_upsample = None\\n    default_downsample = None\\n\\n    def __init__(self):\\n        self.sources = {}\\n        self.col_info = {}\\n        self.col_info[\'time\'] = ColSpec(\'time\', 0, [], \'linear\', \'mean\')\\n\\n        self._data = deque()\\n        self._time = deque()\\n        self._timestamps = deque()\\n\\n        self._timestamps_as_data = set()\\n        self._known_events = set()\\n        self._known_descriptors = set()\\n        self._stale = True\\n\\n        self.plan = self.Planner(self)\\n        self.convert_times = True\\n        self._reference_time = None\\n\\n    @property\\n    def reference_time(self):\\n        return self._reference_time\\n\\n    @reference_time.setter\\n    def reference_time(self, val):\\n        self._reference_time = pd.Timestamp(val, unit=\'s\')\\n\\n    @property\\n    def columns(self):\\n        \\"The columns of DataFrames returned by methods that return DataFrames.\\"\\n        return set(self.sources) | self._time_columns\\n\\n    @property\\n    def _time_columns(self):\\n        ts_names = [name + \'_timestamp\' for name in self._timestamps_as_data]\\n        return {\'time\'} | set(ts_names)\\n\\n    @classmethod\\n    def from_events(cls, events, verbose=False):\\n        \\"\\"\\"\\n        Create a DataMuxer from a list of Events.\\n\\n        Parameters\\n        ----------\\n        events : list\\n            list of Events (any objects with the expected attributes will do)\\n        \\"\\"\\"\\n        \\n        instance = cls()\\n        instance.append_events(events, verbose)\\n        return instance\\n\\n    def append_events(self, events, verbose=False):\\n        \\"\\"\\"Add a list of events to the DataMuxer.\\n\\n        Parameters\\n        ----------\\n        events : list\\n            list of Events (any objects with the expected attributes will do)\\n        \\"\\"\\"\\n        for idx, event in enumerate(events):\\n            if verbose and idx % 25 == 0:\\n                print(\'loading event %s\' % idx),\\n            self.append_event(event)\\n\\n    def append_event(self, event):\\n        \\"\\"\\"Add an event to the DataMuxer.\\n\\n        Parameters\\n        ----------\\n        event : Event\\n            Event Document or any object with the expected attributes\\n\\n        Returns\\n        -------\\n        is_new : bool\\n            True if event was added, False is it has already been added\\n        \\"\\"\\"\\n        if event.uid in self._known_events:\\n            return False\\n        self._known_events.add(event.uid)\\n        self._stale = True\\n        if event.descriptor.uid not in self._known_descriptors:\\n            self._process_new_descriptor(event.descriptor)\\n        # Both scalar and nonscalar data will get stored in the DataFrame.\\n        # This may be optimized later, but it might not actually help much.\\n        self._data.append(\\n            {name: data for name, data in six.iteritems(event.data)})\\n        self._timestamps.append(\\n            {name: ts for name, ts in six.iteritems(event.timestamps)})\\n        self._time.append(event.time)\\n        return True\\n\\n    def _process_new_descriptor(self, descriptor):\\n        \\"Build a ColSpec and update state.\\"\\n        for name, description in six.iteritems(descriptor.data_keys):\\n\\n            # If we already have this source name, the unique source\\n            # identifiers must match. Ambiguous names are not allowed.\\n            if name in self.sources:\\n                if self.sources[name] != description[\'source\']:\\n                    raise ValueError(\\"In a previously loaded descriptor, \\"\\n                                     \\"\'{0}\' refers to {1} but in Event \\"\\n                                     \\"Descriptor {2} it refers to {3}.\\".format(\\n                                         name, self.sources[name],\\n                                         descriptor.uid,\\n                                         description[\'source\']))\\n                if name == \'time\':\\n                    # We can argue later about how best to handle this corner\\n                    # case, but anything is better than silently mislabeling\\n                    # data.\\n                    raise ValueError(\\"The name \'time\' is reserved and cannot \\"\\n                                     \\"be used as an alias.\\")\\n\\n            # If it is a new name, determine a ColSpec.\\n            else:\\n                self.sources[name] = description[\'source\']\\n                if \'external\' in description and \'shape\' in description:\\n                    shape = description[\'shape\']\\n                    ndim = len(shape)\\n                else:\\n                    # External data can be scalar. Nonscalar data must\\n                    # have a specified shape. Thus, if no shape is given,\\n                    # assume scalar.\\n                    shape = None\\n                    ndim = 0\\n                upsample = self.default_upsample\\n                if ndim \\u003e 0:\\n                    upsample = None\\n\\n                col_info = ColSpec(name, ndim, shape, upsample,\\n                                   self.default_downsample)  # defaults\\n                # TODO Look up source-specific default in a config file\\n                # or some other source of reference data.\\n                self.col_info[name] = col_info\\n        self._known_descriptors.add(descriptor.uid)\\n\\n    @property\\n    def _dataframe(self):\\n        \\"See also to_sparse_dataframe, the public version of this.\\"\\n        # Rebuild the DataFrame if more data has been added.\\n        if self._stale:\\n            df = pd.DataFrame(list(self._data))\\n            df[\'time\'] = list(self._time)\\n            if self._timestamps_as_data:\\n                # Only build this if we need it.\\n                # TODO: We shouldn\'t have to build\\n                # the whole thing, but there is already a lot of trickiness\\n                # here so we\'ll worry about optimization later.\\n                timestamps = pd.DataFrame(list(self._timestamps))\\n            for source_name in self._timestamps_as_data:\\n                col_name = _timestamp_col_name(source_name)\\n                df[col_name] = timestamps[source_name]\\n                logger.debug(\\"Including %s timestamps as data\\", source_name)\\n            self._df = df.sort(\'time\').reset_index(drop=True)\\n            self._stale = False\\n        return self._df\\n\\n    def to_sparse_dataframe(self, include_all_timestamps=False):\\n        \\"\\"\\"Obtain all measurements in a DataFrame, one row per Event time.\\n\\n        Parameters\\n        ----------\\n        include_all_timestamps : bool\\n            The result will always contain a \'time\' column but, by default,\\n            not timestamps for individual data sources like \'motor_timestamp\'.\\n            Set this to True to export timestamp columns for each data column\\n\\n        Returns\\n        -------\\n        df : pandas.DataFrame\\n        \\"\\"\\"\\n        if include_all_timestamps:\\n            raise NotImplementedError(\\"TODO\\")\\n\\n        result = self._dataframe.copy()\\n        for col_name in self._time_columns:\\n            result[col_name] = self._maybe_convert_times(result[col_name])\\n        return result\\n\\n    def _maybe_convert_times(self, data):\\n        if self.convert_times:\\n            t = pd.to_datetime(data, unit=\'s\', utc=True).dt.tz_localize(TZ)\\n            if self.reference_time is None:\\n                return t\\n            else:\\n                return t - self.reference_time\\n        return data  # no-op\\n\\n    def include_timestamp_data(self, source_name):\\n        \\"\\"\\"Add the exact timing of a data source as a data column.\\n\\n        Parameters\\n        ----------\\n        source_name : string\\n            one of the source names in DataMuxer.sources\\n        \\"\\"\\"\\n        # self._timestamps_as_data is a set of sources who timestamps\\n        # should be treated as data in the _dataframe method above.\\n        self._timestamps_as_data.add(source_name)\\n        name = _timestamp_col_name(source_name)\\n        self.col_info[name] = ColSpec(name, 0, None, None, np.mean)\\n        self._stale = True\\n\\n    def remove_timestamp_data(self, source_name):\\n        \\"\\"\\"Remove the exact timing of a data source from the data columns.\\n\\n        Parameters\\n        ----------\\n        source_name : string\\n            one of the source names in DataMuxer.sources\\n        \\"\\"\\"\\n        self._timestamps_as_data.remove(source_name)\\n        # Do not force a rebuilt (i.e., self._stale). Just remove it here.\\n        del self._df[_timestamp_col_name(source_name)]\\n\\n    def bin_on(self, source_name, interpolation=None, agg=None, use_cols=None):\\n        \\"\\"\\"\\n        Return data resampled to align with the data from a particular source.\\n\\n        Parameters\\n        ----------\\n        source_name : string\\n        interpolation : dict, optional\\n            Override the default interpolation (upsampling) behavior of any\\n            data source by passing a dictionary of source names mapped onto\\n            one of the following interpolation methods.\\n\\n            {None, \'linear\', \'nearest\', \'zero\', \'slinear\', \'quadratic\',\\n             \'cubic\'}\\n\\n            None means that each time bin must have at least one value.\\n            See scipy.interpolator for more on the other methods.\\n        agg : dict, optional\\n            Override the default reduction (downsampling) behavior of any data\\n            source by passing a dictionary of source names mapped onto any\\n            callable that reduces multiple data points (of whatever dimension)\\n            to a single data point.\\n        use_cols : list, optional\\n            List of columns to include in binning; use all columns by default.\\n\\n        Returns\\n        -------\\n        resampled_df : pandas.DataFrame\\n\\n        References\\n        ----------\\n        http://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html\\n        \\"\\"\\"\\n        centers, bin_edges = self._bin_on(source_name)\\n        return self.bin_by_edges(bin_edges, bin_anchors=centers,\\n                                 interpolation=interpolation, agg=agg,\\n                                 use_cols=use_cols)\\n\\n    def _bin_on(self, source_name):\\n        \\"Compute bin edges spaced around centers defined by source_name points.\\"\\n        col = self._dataframe[source_name]\\n        centers = self._dataframe[\'time\'].reindex_like(col.dropna()).values\\n\\n        # [2, 4, 6] -\\u003e [-inf, 3, 5, inf]\\n        bin_edges = np.mean([centers[1:], centers[:-1]], 0)\\n        # [-inf, 3, 5, inf] -\\u003e [(-inf, 3), (3, 5), (5, inf)]\\n        bin_edges = [-np.inf] + list(np.repeat(bin_edges, 2)) + [np.inf]\\n        bin_edges = np.reshape(bin_edges, (-1, 2))\\n        return centers, bin_edges\\n\\n    def bin_by_edges(self, bin_edges, bin_anchors, interpolation=None, agg=None,\\n                     use_cols=None):\\n        \\"\\"\\"\\n        Return data resampled into bins with the specified edges.\\n\\n        Parameters\\n        ----------\\n        bin_edges : list\\n            list of two-element items like [(t1, t2), (t3, t4), ...]\\n        bin_anchors : list\\n            These are time points where interpolated values will be evaluated.\\n            Bin centers are usually a good choice.\\n        interpolation : dict, optional\\n            Override the default interpolation (upsampling) behavior of any\\n            data source by passing a dictionary of source names mapped onto\\n            one of the following interpolation methods.\\n\\n            {None, \'linear\', \'nearest\', \'zero\', \'slinear\', \'quadratic\',\\n             \'cubic\'}\\n\\n            None means that each time bin must have at least one value.\\n            See scipy.interpolator for more on the other methods.\\n        agg : dict, optional\\n            Override the default reduction (downsampling) behavior of any data\\n            source by passing a dictionary of source names mapped onto any\\n            callable that reduces multiple data points (of whatever dimension)\\n            to a single data point.\\n        use_cols : list, optional\\n            List of columns to include in binning; use all columns by default.\\n\\n        Returns\\n        -------\\n        resampled_df : pandas.DataFrame\\n\\n        References\\n        ----------\\n        http://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html\\n        \\"\\"\\"\\n        bin_anchors, binning = self._bin_by_edges(bin_anchors, bin_edges)\\n        return self.resample(bin_anchors, binning, interpolation, agg,\\n                             use_cols=use_cols)\\n\\n    def _bin_by_edges(self, bin_anchors, bin_edges):\\n        \\"Compute bin assignment and, if needed, bin_anchors.\\"\\n        time = self._dataframe[\'time\'].values\\n        # Get edges into 1D array[L, R, L, R, ...]\\n        edges_as_pairs = np.reshape(bin_edges, (-1, 2))\\n        all_edges = np.ravel(edges_as_pairs)\\n        if not np.all(np.diff(all_edges) \\u003e= 0):\\n            raise ValueError(\\"Illegal binning: the left edge must be less \\"\\n                             \\"than the right edge.\\")\\n        # Sort out where the array each time would be inserted.\\n        binning = np.searchsorted(all_edges, time).astype(float)\\n        # Times that would get inserted at even positions are between bins.\\n        # Mark them\\n        binning[binning % 2 == 0] = np.nan\\n        binning //= 2  # Make bin number sequential, not odds only.\\n        if bin_anchors is None:\\n            bin_anchors = np.mean(edges_as_pairs, axis=1)  # bin centers\\n        else:\\n            if len(bin_anchors) != len(bin_edges):\\n                raise ValueError(\\"There are {0} bin_anchors but {1} pairs of \\"\\n                                 \\"bin_edges. These must match.\\".format(\\n                                     len(bin_anchors), len(bin_edges)))\\n        return bin_anchors, binning\\n\\n    def resample(self, bin_anchors, binning, interpolation=None, agg=None,\\n                 verify_integrity=True, use_cols=None):\\n        \\"\\"\\"\\n        Return data resampled into bins with the specified edges.\\n\\n        Parameters\\n        ----------\\n        bin_anchors : list\\n            These are time points where interpolated values will be evaluated.\\n            Bin centers are usually a good choice.\\n        bin_anchors : list\\n            Bin assignment. Example: [1, 1, 2, 2, 3, 3] puts six data points\\n            into three bins with two points each.\\n        interpolation : dict, optional\\n            Override the default interpolation (upsampling) behavior of any\\n            data source by passing a dictionary of source names mapped onto\\n            one of the following interpolation methods.\\n\\n            {None, \'linear\', \'nearest\', \'zero\', \'slinear\', \'quadratic\',\\n             \'cubic\'}\\n\\n            None means that each time bin must have at least one value.\\n            See scipy.interpolator for more on the other methods.\\n        agg : dict, optional\\n            Override the default reduction (downsampling) behavior of any data\\n            source by passing a dictionary of source names mapped onto any\\n            callable that reduces multiple data points (of whatever dimension)\\n            to a single data point.\\n        verify_integrity : bool, optional\\n            For a cost in performance, verify that the downsampling function\\n            produces data of the expected shape. True by default.\\n        use_cols : list, optional\\n            List of columns to include in binning; use all columns by default.\\n\\n        Returns\\n        -------\\n        resampled_df : pandas.DataFrame\\n\\n        References\\n        ----------\\n        http://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html\\n        \\"\\"\\"\\n        if use_cols is None:\\n            use_cols = self.columns\\n        plan = self.Planner(self)\\n        upsampling_rules = plan.determine_upsample(interpolation, use_cols)\\n        downsampling_rules = plan.determine_downsample(agg, use_cols)\\n        grouped = self._dataframe.groupby(binning)\\n        first_point = grouped.first()\\n        counts = grouped.count()\\n        resampling_requirements = _is_resampling_applicable(counts)\\n        index = np.arange(len(bin_anchors))\\n        result = {}  # dict of DataFrames, to become one MultiIndexed DataFrame\\n        for name in use_cols:\\n            upsample = upsampling_rules[name]\\n            downsample = downsampling_rules[name]\\n            upsampling_possible = resampling_requirements[\'upsampling_possible\'][name]\\n            downsampling_needed = resampling_requirements[\'downsampling_needed\'][name]\\n            result[name] = pd.DataFrame(index=index)\\n            # Put the first (maybe only) value into a Series.\\n            # We will overwrite as needed below.\\n            result[name][\'val\'] = pd.Series(data=first_point[name])\\n\\n            # Short-circuit if we are done.\\n            if not (upsampling_possible or downsampling_needed):\\n                logger.debug(\\"%s has exactly one data point per bin\\", name)\\n                continue\\n\\n            result[name][\'count\'] = counts[name]\\n\\n            # If any bin has no data, use the upsampling rule to interpolate\\n            # at the center of the empty bins. If there is no rule, simply\\n            # leave some bins empty. Do not raise an error.\\n            if upsampling_possible and (upsample is not None):\\n                if upsample in (\'ffill\', \'bfill\'):\\n                    result[name][\'val\'].fillna(method=upsample, inplace=True)\\n                else:\\n                    dense_col = self._dataframe[name].dropna()\\n                    y = dense_col.values\\n                    x = self._dataframe[\'time\'].reindex_like(dense_col).values\\n                    interpolator = interp1d(x, y, kind=upsample)\\n                    # Outside the limits of the data, the interpolator will\\n                    # fail.  Leave any such entires empty.\\n                    is_safe = ((bin_anchors \\u003e np.min(x)) \\u0026\\n                               (bin_anchors \\u003c np.max(x)))\\n                    safe_times = bin_anchors[is_safe]\\n                    safe_bins = index[is_safe]\\n                    interp_points = pd.Series(interpolator(safe_times),\\n                                              index=safe_bins)\\n                    logger.debug(\\"Interpolating to fill %d of %d \\"\\n                                 \\"empty bins in %s\\",\\n                                 len(safe_bins), (counts[name] == 0).sum(),\\n                                 name)\\n                    result[name][\'val\'].fillna(interp_points, inplace=True)\\n\\n            # Short-circuit if we are done.\\n            if not downsampling_needed:\\n                logger.debug(\\"%s has at most one data point per bin\\", name)\\n                continue\\n\\n            # Multi-valued bins must be downsampled (reduced). If there is no\\n            # rule for downsampling, we have no recourse: we must raise.\\n            if (downsample is None):\\n                raise BinningError(\\"The specified binning puts multiple \\"\\n                                   \\"\'{0}\' measurements in at least one bin, \\"\\n                                   \\"and there is no rule for downsampling \\"\\n                                   \\"(i.e., reducing) it.\\".format(name))\\n            if verify_integrity and callable(downsample):\\n                downsample = _build_verified_downsample(\\n                    downsample, self.col_info[name].shape)\\n\\n            g = grouped[name]  # for brevity\\n            if self.col_info[name].ndim == 0:\\n                logger.debug(\\"The scalar column %s must be downsampled.\\", name)\\n                # For scalars, pandas knows what to do.\\n                downsampled = g.agg(downsample)\\n                std_series = g.std()\\n                max_series = g.max()\\n                min_series = g.min()\\n            else:\\n                # For nonscalars, we are abusing groupby and must go to a\\n                # a little more trouble to guarantee success.\\n                logger.debug(\\"The nonscalar column %s must be downsampled.\\",\\n                             name)\\n                if not callable(downsample):\\n                    # Do this lookup here so that strings can be passed\\n                    # in the call to resample.\\n                    downsample = ColSpec._downsample_mapping[downsample]\\n                downsampled = g.apply(lambda x: downsample(np.asarray(x.dropna())))\\n                std_series = g.apply(lambda x: np.std(np.asarray(x.dropna()), 0))\\n                max_series = g.apply(lambda x: np.max(np.asarray(x.dropna()), 0))\\n                min_series = g.apply(lambda x: np.min(np.asarray(x.dropna()), 0))\\n\\n            # This (counts[name] \\u003e 1) is redundant, but there is no clean way to\\n            # pass it here without refactoring. Not a huge cost.\\n            result[name][\'val\'].where(~(counts[name] \\u003e 1), downsampled, inplace=True)\\n            result[name][\'std\'] = std_series\\n            result[name][\'max\'] = max_series\\n            result[name][\'min\'] = min_series\\n\\n        result = pd.concat(result, axis=1)  # one MultiIndexed DataFrame\\n        result.index.name = \'bin\'\\n\\n        # Convert time timestamp or timedelta, depending on the state of\\n        # self.convert_times and self.reference_time.\\n        for col_name in self._time_columns:\\n            if isinstance(result[col_name], pd.DataFrame):\\n                subcols = result[col_name].columns\\n                for subcol in subcols \\u0026 {\'max\', \'min\', \'val\'}:\\n                    result[(col_name, subcol)] = self._maybe_convert_times(\\n                            result[(col_name, subcol)])\\n                for subcol in subcols \\u0026 {\'std\'}:\\n                    result[(col_name, subcol)] = pd.to_timedelta(\\n                            result[(col_name, subcol)], unit=\'s\')\\n            else:\\n                result[col_name] = self._maybe_convert_times(\\n                        result[col_name])\\n        return result\\n\\n    def __getitem__(self, source_name):\\n        if source_name not in list(self.col_info.keys()) + [\'time\']:\\n            raise KeyError(\\"No data from a source called \'{0}\' has been \\"\\n                           \\"added.\\".format(source_name))\\n        # Unlike output from binning functions, this is indexed\\n        # on time.\\n        result = self._dataframe[source_name].dropna()\\n        result.index = self._dataframe[\'time\'].reindex_like(result)\\n        return result\\n\\n    def __getattr__(self, attr):\\n        # Developer beware: if any properties raise an AttributeError,\\n        # this will mask it. Comment this magic method to debug properties.\\n        if attr in self.col_info.keys():\\n            return self[attr]\\n        else:\\n            raise AttributeError(\\"DataMuxer has no attribute {0} and no \\"\\n                                  \\"data source named \'{0}\'\\".format(attr))\\n\\n    @property\\n    def ncols(self):\\n        \\"\\"\\"\\n        The number of columns that the DataMuxer contains\\n        \\"\\"\\"\\n        return len(self.col_info)\\n\\n    @property\\n    def col_info_by_ndim(self):\\n        \\"\\"\\"Dictionary mapping dimensionality (ndim) onto a list of ColSpecs\\"\\"\\"\\n\\n        result = {}\\n        for name, col_spec in six.iteritems(self.col_info):\\n            try:\\n                result[col_spec.ndim]\\n            except KeyError:\\n                result[col_spec.ndim] = []\\n            result[col_spec.ndim].append(col_spec)\\n        return result\\n\\n\\ndef dataframe_to_dict(df):\\n    \\"\\"\\"\\n    Turn a DataFrame into a dict of lists.\\n\\n    Parameters\\n    ----------\\n    df : DataFrame\\n\\n    Returns\\n    -------\\n    index : ndarray\\n        The index of the data frame\\n    data : dict\\n        Dictionary keyed on column name of the column.  The value is\\n        one of (ndarray, list, pd.Series)\\n    \\"\\"\\"\\n    dict_of_lists = {col: df[col].to_list() for col in df.columns}\\n    return df.index.values, dict_of_lists\\n\\n\\ndef _build_verified_downsample(downsample, expected_shape):\\n    # Ensure two things:\\n    # 1. The downsampling function shouldn\'t touch bins with only one point.\\n    # 2. The result of downsample should have the right shape.\\n    def _downsample(data):\\n        if len(data) == 1:\\n            return data\\n        downsampled = downsample(data)\\n        if (expected_shape is None or expected_shape == 0):\\n            if not np.isscalar(downsampled):\\n                raise BadDownsamplerError(\\"The \'agg\' (downsampling) function \\"\\n                                          \\"for {0} is expected to produce \\"\\n                                          \\"a scalar from the data in each \\"\\n                                          \\"bin.\\".format(downsampled))\\n        elif downsampled.shape != expected_shape:\\n            raise BadDownsamplerError(\\"An \'agg\' (downsampling) function \\"\\n                                      \\"returns data shaped {0} but the \\"\\n                                      \\"shape {1} is expected.\\".format(\\n                                          downsampled.shape, expected_shape))\\n        return downsampled\\n    return _downsample\\n\\n\\ndef _timestamp_col_name(source_name):\\n    return \'{0}_timestamp\'.format(source_name)\\n\\n\\ndef _normalize_string_none(val):\\n    \\"Replay passes \'None\' to mean None.\\"\\n    try:\\n        lowercase_val = val.lower()\\n    except AttributeError:\\n        return val\\n    if lowercase_val == \'none\':\\n        return None\\n    else:\\n        return val\\n\\n\\ndef _is_resampling_applicable(counts):\\n    has_no_points = counts == 0\\n    has_multiple_points = counts \\u003e 1\\n    upsampling_possible = has_no_points.any()\\n    downsampling_needed = has_multiple_points.any()\\n    result = {}\\n    result[\'upsampling_possible\'] = upsampling_possible.to_dict()\\n    result[\'downsampling_needed\'] = downsampling_needed.to_dict()\\n    return result\\n"}\n'
line: b'{ "repo_name": "FrissAnalytics/shinyJsTutorials", "ref": "refs/heads/master", "path": "tutorials/materials2/C3/R/C3Pie.R", "content": "#\' <Add Title>\\n#\'\\n#\' <Add Description>\\n#\'\\n#\' @import htmlwidgets\\n#\'\\n#\' @export\\nC3Pie <- function(values, legendPosition = \\"bottom\\", width = NULL, height = NULL) {\\n\\n  # forward options using x\\n  x = list(\\n    values = values,\\n    legendPosition = legendPosition\\n  )\\n\\n  # create widget\\n  htmlwidgets::createWidget(\\n    name = \'C3Pie\',\\n    x,\\n    width = width,\\n    height = height,\\n    package = \'C3\'\\n  )\\n}\\n\\n#\' Shiny bindings for C3Pie\\n#\'\\n#\' Output and render functions for using C3Pie within Shiny\\n#\' applications and interactive Rmd documents.\\n#\'\\n#\' @param outputId output variable to read from\\n#\' @param width,height Must be a valid CSS unit (like \\\\code{\'100\\\\%\'},\\n#\'   \\\\code{\'400px\'}, \\\\code{\'auto\'}) or a number, which will be coerced to a\\n#\'   string and have \\\\code{\'px\'} appended.\\n#\' @param expr An expression that generates a C3Pie\\n#\' @param env The environment in which to evaluate \\\\code{expr}.\\n#\' @param quoted Is \\\\code{expr} a quoted expression (with \\\\code{quote()})? This\\n#\'   is useful if you want to save an expression in a variable.\\n#\'\\n#\' @name C3Pie-shiny\\n#\'\\n#\' @export\\nC3PieOutput <- function(outputId, width = \'100%\', height = \'400px\'){\\n  htmlwidgets::shinyWidgetOutput(outputId, \'C3Pie\', width, height, package = \'C3\')\\n}\\n\\n#\' @rdname C3Pie-shiny\\n#\' @export\\nrenderC3Pie <- function(expr, env = parent.frame(), quoted = FALSE) {\\n  if (!quoted) { expr <- substitute(expr) } # force quoted\\n  htmlwidgets::shinyRenderWidget(expr, C3PieOutput, env, quoted = TRUE)\\n}\\n" }\n'
line: b'{ "repo_name": "FrissAnalytics/shinyJsTutorials", "ref": "refs/heads/master", "path": "tutorials/materials2/C3/R/C3LineBarChart.R", "content": "#\' <Add Title>\\n#\'\\n#\' <Add Description>\\n#\'\\n#\' @import htmlwidgets\\n#\'\\n#\' @export\\nC3LineBarChart <- function(dataset, colors, width = NULL, height = NULL) {\\n\\n  # forward options using x\\n  x = list(\\n    dataset  = dataset,\\n    colors   = colors\\n  )\\n\\n  # create widget\\n  htmlwidgets::createWidget(\\n    name = \'C3LineBarChart\',\\n    x,\\n    width = width,\\n    height = height,\\n    package = \'C3\'\\n  )\\n}\\n\\n#\' Shiny bindings for C3LineBarChart\\n#\'\\n#\' Output and render functions for using C3LineBarChart within Shiny\\n#\' applications and interactive Rmd documents.\\n#\'\\n#\' @param outputId output variable to read from\\n#\' @param width,height Must be a valid CSS unit (like \\\\code{\'100\\\\%\'},\\n#\'   \\\\code{\'400px\'}, \\\\code{\'auto\'}) or a number, which will be coerced to a\\n#\'   string and have \\\\code{\'px\'} appended.\\n#\' @param expr An expression that generates a C3LineBarChart\\n#\' @param env The environment in which to evaluate \\\\code{expr}.\\n#\' @param quoted Is \\\\code{expr} a quoted expression (with \\\\code{quote()})? This\\n#\'   is useful if you want to save an expression in a variable.\\n#\'\\n#\' @name C3LineBarChart-shiny\\n#\'\\n#\' @export\\nC3LineBarChartOutput <- function(outputId, width = \'100%\', height = \'400px\'){\\n  htmlwidgets::shinyWidgetOutput(outputId, \'C3LineBarChart\', width, height, package = \'C3\')\\n}\\n\\n#\' @rdname C3LineBarChart-shiny\\n#\' @export\\nrenderC3LineBarChart <- function(expr, env = parent.frame(), quoted = FALSE) {\\n  if (!quoted) { expr <- substitute(expr) } # force quoted\\n  htmlwidgets::shinyRenderWidget(expr, C3LineBarChartOutput, env, quoted = TRUE)\\n}\\n" }\n'
line: b'{ "repo_name": "swarm-lab/Shiny", "ref": "refs/heads/master", "path": "aggregation_segregation/global.R", "content": "rw <- function(df) {\\n  n <- nrow(df)\\n  \\n  within(df, {\\n    h <- h + rnorm(n, sd = pi / 6)\\n    x <- x + s * cos(h)\\n    y <- y + s * sin(h)\\n    \\n    h[x < -1] <- 0\\n    h[x > 1] <- pi\\n    h[y < -1] <- pi / 2\\n    h[y > 1] <- -pi / 2\\n  })\\n}\\n\\nsigmoid <- function(x, a = 0, k = 1, b = 0.1, m = 100, v = 1, q = 1) {\\n  a + (k - a) / ((1 + q * exp(-b * (x - m))) ^ (1 / v))\\n}\\n\\npdist <- function(A, B) {\\n  an = apply(A, 1, function(rvec) crossprod(rvec,rvec))\\n  bn = apply(B, 1, function(rvec) crossprod(rvec,rvec))\\n  \\n  m = nrow(A)\\n  n = nrow(B)\\n  \\n  tmp = matrix(rep(an, n), nrow = m) \\n  tmp = tmp +  matrix(rep(bn, m), nrow = m, byrow = TRUE)\\n  sqrt( tmp - 2 * tcrossprod(A,B) )\\n}\\n\\nsp <- function(df, affinSame = 0, affinOther = 0) {\\n  dist <- pdist(as.matrix(df[, 1:2]), as.matrix(df[, 1:2]))\\n  neighbor <- dist < 0.125\\n  b_neighbor <- apply(df$col == \\"#107AB6\\" & neighbor, 2, sum) - (df$col == \\"#107AB6\\")\\n  r_neighbor <- apply(df$col == \\"#D86810\\" & neighbor, 2, sum) - (df$col == \\"#D86810\\")\\n  \\n  nb <- (df$col == \\"#107AB6\\") * affinSame * b_neighbor + \\n    (df$col == \\"#D86810\\") * affinSame * r_neighbor + \\n    (df$col == \\"#107AB6\\") * affinOther * r_neighbor + \\n    (df$col == \\"#D86810\\") * affinOther * b_neighbor\\n  \\n  df$s <- sigmoid(nb, k = 0.1, m = 4, b = -1)\\n  df\\n}\\n\\npop <- NULL\\nrun <- FALSE\\n" }\n'
line: b'{ "repo_name": "swarm-lab/Shiny", "ref": "refs/heads/master", "path": "wisdom_of_crowds/model/old/server_old.R", "content": "library(shiny)\\nlibrary(dplyr)\\nlibrary(ggplot2)\\nsource(\\"ibm.R\\")\\n\\nshinyServer(function(input, output, session) {  \\n  \\n  dat <- reactiveValues(tab = NULL)\\n  counter <- reactiveValues(count = -1)\\n  \\n  observe({\\n    if (counter$count != input$goButton) {\\n      counter$count <- input$goButton\\n      \\n      withProgress(message = \\"Simulating 1000 experiments\\", value = 0, {\\n        n <- 1000\\n        \\n        m1 <- replicate(n, woc(n = input$n, \\n                               val = input$val,\\n                               error = input$error,\\n                               soc = 0))\\n        \\n        m2 <- replicate(n, woc(n = input$n, \\n                               val = input$val,\\n                               error = input$error,\\n                               soc = input$soc))\\n        \\n        dat$tab <- data.frame(SOC = as.factor(rep(c(\\"Control   \\", \\"Experimental   \\"), each = n * 2)),\\n                              TYPE = rep(c(\\"mean\\", \\"sd\\", \\"mean\\", \\"sd\\"), each = n),\\n                              VAL = c(apply(m1, 2, mean),\\n                                      apply(m1, 2, sd),\\n                                      apply(m2, 2, mean),\\n                                      apply(m2, 2, sd)))\\n      }) \\n    }\\n  })\\n  \\n  output$IBM.plot1 <- renderPlot({\\n    g <- ggplot(filter(dat$tab, TYPE == \\"mean\\"),\\n                aes(x = VAL, color = SOC)) + \\n      geom_density(size = 1) +\\n      theme_minimal(base_size = 18) + \\n      theme(legend.position = \\"top\\", legend.title = element_blank()) +\\n      xlab(\\"Group average\\") + ylab(\\"Density\\") +\\n      scale_color_manual(values = c(\\"tomato3\\", \\"dodgerblue3\\"))\\n    \\n    print(g)\\n  })\\n  \\n  output$IBM.plot2 <- renderPlot({\\n    g <- ggplot(filter(dat$tab, TYPE == \\"sd\\"),\\n                aes(x = VAL, color = SOC)) + \\n      geom_density(size = 1) +\\n      theme_minimal(base_size = 18) + \\n      theme(legend.position = \\"top\\", legend.title = element_blank()) +\\n      xlab(\\"Group standard deviation\\") + ylab(\\"Density\\") +\\n      scale_color_manual(values = c(\\"tomato3\\", \\"dodgerblue3\\"))\\n    \\n    print(g)\\n  })\\n})\\n" }\n'
line: b'{ "repo_name": "swarm-lab/Shiny", "ref": "refs/heads/master", "path": "wisdom_of_crowds/model/server.R", "content": "shinyServer(function(input, output, session) {\\n  \\n  react <- reactiveValues(tab = {}, count = -1)\\n  \\n  observe({\\n    if (react$count != input$goButton) {\\n      react$count <- input$goButton\\n      \\n      withProgress(message = \\"Simulating 1000 experiments\\", {\\n        m1 <- replicate(N, woc(n = input$n, \\n                               val = 200,\\n                               error = input$error / 100,\\n                               soc = 0))\\n        tmp1 <- abs(m1 - 200) \\n        tmp2 <- abs(matrix(apply(m1, 2, mean), nrow = input$n, ncol = 1000, byrow = TRUE) - 200)\\n        r1 <- apply(tmp2 < tmp1, 2, sum)\\n        \\n        m2 <- replicate(N, woc(n = input$n, \\n                               val = 200,\\n                               error = input$error / 100,\\n                               soc = input$soc))\\n        tmp1 <- abs(m2 - 200) \\n        tmp2 <- abs(matrix(apply(m2, 2, mean), nrow = input$n, ncol = 1000, byrow = TRUE) - 200)\\n        r2 <- apply(tmp2 < tmp1, 2, sum)\\n        \\n        react$tab <- data.frame(\\n          SOC = as.factor(rep(c(\\"Control   \\", \\"Experimental   \\"), each = N * 2)),\\n          TYPE = rep(c(\\"mean\\", \\"sd\\", \\"mean\\", \\"sd\\"), each = N),\\n          VAL = c(100 * (r1 / input$n),\\n                  apply(m1, 2, sd),\\n                  100 * (r2 / input$n),\\n                  apply(m2, 2, sd)))\\n      }) \\n    }\\n  })\\n  \\n  output$IBM.plot1 <- renderPlot({\\n    g <- ggplot(filter(react$tab, TYPE == \\"mean\\"),\\n                aes(x = VAL, color = SOC, fill = SOC)) + \\n      geom_histogram(position = \\"identity\\", bins = 40) +\\n      geom_vline(xintercept = 50, linetype = 2) +\\n      xlim(0, 100) +\\n      theme_minimal(base_size = 16) + \\n      theme(legend.position = \\"top\\", legend.title = element_blank()) +\\n      xlab(\\"Average > x% of group members\\") + ylab(\\"Density\\") +\\n      scale_color_manual(values = c(\\"tomato3\\", \\"dodgerblue3\\")) + \\n      scale_fill_manual(values = alpha(c(\\"tomato3\\", \\"dodgerblue3\\"), 0.25))\\n    \\n    print(g)\\n  })\\n  \\n  output$IBM.plot2 <- renderPlot({\\n    g <- ggplot(filter(react$tab, TYPE == \\"sd\\"),\\n                aes(x = VAL, color = SOC, fill = SOC)) + \\n      geom_histogram(position = \\"identity\\", bins = 40) +\\n      theme_minimal(base_size = 16) + \\n      theme(legend.position = \\"top\\", legend.title = element_blank()) +\\n      xlab(\\"Group standard deviation\\") + ylab(\\"Density\\") +\\n      scale_color_manual(values = c(\\"tomato3\\", \\"dodgerblue3\\")) + \\n      scale_fill_manual(values = alpha(c(\\"tomato3\\", \\"dodgerblue3\\"), 0.25))\\n    \\n    print(g)\\n  })\\n})\\n" }\n'
line: b'{"repo_name":"glaubitz/fs-uae-debian","ref":"refs/heads/master","path":"launcher/launcher/game_paths.py","content":"import os\\n\\nimport fsui\\nfrom fsbc.paths import Paths\\nfrom fsgs.FSGSDirectories import FSGSDirectories\\nfrom .launcher_config import LauncherConfig\\nfrom .launcher_settings import LauncherSettings\\nfrom .ui.Constants import Constants\\n\\n\\nclass GamePaths(object):\\n    @staticmethod\\n    def current():\\n        model = LauncherConfig.get(\\"amiga_model\\")\\n        if model.startswith(\\"CD32\\"):\\n            platform = \\"CD32\\"\\n        elif model == \\"CDTV\\":\\n            platform = \\"CDTV\\"\\n        else:\\n            platform = \\"Amiga\\"\\n        name = LauncherSettings.get(\\"config_name\\")\\n        uuid = LauncherConfig.get(\\"x_game_uuid\\")\\n        return GamePaths(name, platform, uuid)\\n\\n    def __init__(self, name, platform, uuid):\\n        self.uuid = uuid\\n\\n        self.config_name = name\\n        if \\"(\\" in name:\\n            parts = name.split(\\"(\\", 1)\\n            self.name, self.variant = parts\\n            self.name = self.name.strip()\\n            self.variant = self.variant.strip()\\n            if self.variant.endswith(\\")\\"):\\n                self.variant = self.variant[:-1]\\n            self.variant = self.variant.replace(\\") (\\", \\", \\")\\n            self.variant = self.variant.replace(\\")(\\", \\", \\")\\n        else:\\n            self.name = name\\n            self.variant = \\"\\"\\n        self.platform = platform\\n\\n    def get_name(self):\\n        return self.name\\n\\n    def get_variant(self):\\n        return self.variant\\n\\n    @staticmethod\\n    def get_override_path(name):\\n        path = LauncherConfig.get(name)\\n        if not path:\\n            return \\"\\"\\n        path = Paths.expand_path(path)\\n        return path\\n\\n    def get_screenshot_path(self, number):\\n        if number == 0:\\n            sha1 = LauncherConfig.get(\\"title_sha1\\")\\n        else:\\n            sha1 = LauncherConfig.get(\\"screen{0}_sha1\\".format(number))\\n        if sha1:\\n            return \\"sha1:\\" + sha1\\n\\n        if number == 0:\\n            path = self.get_override_path(\\"title_image\\")\\n        else:\\n            path = self.get_override_path(\\"screen{0}_image\\".format(number))\\n        if path and os.path.exists(path):\\n            return path\\n        if self.uuid:\\n            if number == 0:\\n                name = \\"title.png\\"\\n            else:\\n                name = \\"screen{0}.png\\".format(number)\\n            paths = FSGSDirectories.get_images_dirs()\\n            for dir_ in paths:\\n                p = os.path.join(dir_, self.platform, \\"Images\\",\\n                                 self.uuid[:2], self.uuid, name)\\n                if os.path.exists(p):\\n                    return p\\n                p = os.path.join(dir_, self.platform, \\"Thumbnails\\",\\n                                 self.uuid[:2], self.uuid, name)\\n                if os.path.exists(p):\\n                    return p\\n        letter = self.get_letter(self.name)\\n        if not letter:\\n            return None\\n        name = self.name\\n        if number == 0:\\n            override_dir = LauncherConfig.get(\\"titles_dir\\")\\n            if override_dir:\\n                paths = [Paths.expand_path(override_dir)]\\n            else:\\n                paths = FSGSDirectories.get_titles_dirs()\\n        else:\\n            override_dir = LauncherConfig.get(\\"screenshots_dir\\")\\n            if override_dir:\\n                paths = [Paths.expand_path(override_dir)]\\n            else:\\n                paths = FSGSDirectories.get_screenshots_dirs()\\n        if number \\u003e= 2:\\n            name = \\"{0}_{1}\\".format(name, number)\\n        for dir_ in paths:\\n            path = os.path.join(dir_, letter, name + \\".png\\")\\n            if os.path.exists(path):\\n                return path\\n            path = os.path.join(dir_, letter, name + \\".gif\\")\\n            if os.path.exists(path):\\n                return path\\n            path = os.path.join(dir_, name + \\".png\\")\\n            if os.path.exists(path):\\n                return path\\n            path = os.path.join(dir_, letter, name + \\".gif\\")\\n            if os.path.exists(path):\\n                return path\\n        return None\\n\\n    def load_screenshot(self, number):\\n        path = self.get_screenshot_path(number)\\n        if path:\\n            return fsui.Image(path)\\n\\n    def load_screenshot_preview(self, number):\\n        image = self.load_screenshot(number)\\n        if image is None:\\n            return image\\n        if image.size == Constants.SCREEN_SIZE:\\n            return image\\n        if image.size[0] \\u003c 400:\\n            image.resize((image.size[0] * 2, image.size[1] * 2),\\n                         fsui.Image.NEAREST)\\n        image.resize(Constants.SCREEN_SIZE)\\n        return image\\n\\n    def get_cover_path(self):\\n        sha1 = LauncherConfig.get(\\"front_sha1\\")\\n        if sha1:\\n            return \\"sha1:\\" + sha1\\n\\n        path = self.get_override_path(\\"cover_image\\")\\n        if path and os.path.exists(path):\\n            return path\\n        if self.uuid:\\n            paths = FSGSDirectories.get_images_dirs()\\n            for dir_ in paths:\\n                p = os.path.join(dir_, self.platform, \\"Images\\",\\n                                 self.uuid[:2], self.uuid, \\"front.png\\")\\n                if os.path.exists(p):\\n                    return p\\n                p = os.path.join(dir_, self.platform, \\"Thumbnails\\",\\n                                 self.uuid[:2], self.uuid, \\"front.png\\")\\n                if os.path.exists(p):\\n                    return p\\n        letter = self.get_letter(self.name)\\n        if not letter:\\n            return None\\n        name = self.name\\n        override_dir = LauncherConfig.get(\\"covers_dir\\")\\n        if override_dir:\\n            paths = [Paths.expand_path(override_dir)]\\n        else:\\n            paths = FSGSDirectories.get_covers_dirs()\\n        for dir_ in paths:\\n            path = os.path.join(dir_, letter, name + \\".jpg\\")\\n            if os.path.exists(path):\\n                return path\\n            path = os.path.join(dir_, letter, name + \\".png\\")\\n            if os.path.exists(path):\\n                return path\\n            path = os.path.join(dir_, name + \\".jpg\\")\\n            if os.path.exists(path):\\n                return path\\n            path = os.path.join(dir_, name + \\".png\\")\\n            if os.path.exists(path):\\n                return path\\n        return None\\n\\n    def load_cover(self):\\n        path = self.get_cover_path()\\n        print(path)\\n        if path:\\n            return fsui.Image(path)\\n\\n    def load_cover_preview(self):\\n        image = self.load_cover()\\n        if image is None:\\n            return image\\n        image.resize(Constants.COVER_SIZE)\\n        return image\\n\\n    def get_theme_path(self):\\n        letter = self.get_letter(self.name)\\n        if not letter:\\n            return None\\n        paths = FSGSDirectories.get_themes_dirs()\\n        for dir_ in paths:\\n            path = os.path.join(dir_, letter, self.name)\\n            if os.path.exists(path):\\n                return path\\n        return None\\n\\n    def _get_state_dir(self):\\n        config_name = self.config_name\\n        if not config_name:\\n            config_name = \\"Default\\"\\n\\n        # use a temporary state dir, for now, to avoid problems with\\n        # floppy overlays etc interfering with net play\\n        from .netplay.netplay import Netplay\\n        if Netplay.current():\\n            # it is possible to manually specify the state dir\\n            config_name = LauncherConfig.get(\\"__netplay_state_dir_name\\")\\n            if not config_name:\\n                # this is the default behavior, create a clean state\\n                # dir for the net play session\\n                netplay_game = LauncherConfig.get(\\"__netplay_game\\")\\n                if netplay_game:\\n                    config_name = \\"Net Play ({0})\\".format(netplay_game)\\n\\n        letter = self.get_letter(config_name)\\n        if not letter:\\n            config_name = \\"Default\\"\\n            letter = self.get_letter(config_name)\\n        # we use an existing state dir in a \\"letter\\" dir if it exists\\n        # (legacy support).\\n        path = os.path.join(FSGSDirectories.get_save_states_dir(), letter,\\n                            config_name)\\n        if os.path.exists(path):\\n            return path\\n        # if not, we use a direct sub-folder of save states dir\\n        path = os.path.join(FSGSDirectories.get_save_states_dir(),\\n                            config_name)\\n        return path\\n\\n    def get_state_dir(self):\\n        state_dir = self._get_state_dir()\\n        if not os.path.exists(state_dir):\\n            os.makedirs(state_dir)\\n        return state_dir\\n\\n    @staticmethod\\n    def get_letter(name):\\n        letter_name = name.upper()\\n        if letter_name.startswith(\\"THE \\"):\\n            letter_name = letter_name[4:]\\n        if letter_name.startswith(\\"A \\"):\\n            letter_name = letter_name[2:]\\n        for i in range(len(letter_name)):\\n            letter = letter_name[i]\\n            if letter in \\"01234567890\\":\\n                letter = \\"0\\"\\n                break\\n            if letter in \\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\\":\\n                break\\n        else:\\n            return None\\n        return letter\\n"}\n'
line: b'{ "repo_name": "swarm-lab/Shiny", "ref": "refs/heads/master", "path": "opinion_dynamic/global.R", "content": "grid_sys <- function(time, init, parms) {\\n  o1 <- init\\n  o1[o1 < 0] <- 0\\n  \\n  o2 <- init\\n  o2[o2 > 0] <- 0\\n  \\n  p1 <- eightneighbors(o1) / 8\\n  p2 <- p1 - eightneighbors(o2) / 8\\n  \\n  r <- runif(nrow(init) * ncol(init))\\n  \\n  idx1 <- r <= p1\\n  idx2 <- r <= p2 & r > p1\\n  \\n  init[idx1] <- 1 * parms$w1\\n  init[idx2] <- -1 * parms$w2\\n  \\n  init\\n}\\n\\n\\n\\n\\n\\n\\n" }\n'
line: b'{ "repo_name": "swarm-lab/Shiny", "ref": "refs/heads/master", "path": "wisdom_of_crowds/experiment/panels/panel3.R", "content": "panel3 <- bsCollapsePanelNoHead(\\n  title = \\"NULL\\", id = \\"col3\\", value = \\"col3\\",\\n\\n  div(class = \\"button\\",\\n      uiOutput(\\"plot_title\\"),\\n      \\n      plotOutput(\\"dots\\", width = \\"400px\\", inline = TRUE),\\n      \\n      h4(textOutput(\\"time_left\\"))\\n  )\\n)\\n" }\n'
line: b'{ "repo_name": "bryantrobbins/baseball", "ref": "refs/heads/master", "path": "worker/build/staging/generic.R", "content": "library(jsonlite)\\nlibrary(plyr)\\nquery <- fromJSON(\'config.json\')\\n\\ntableName <- query$table\\nload(paste(tableName,\'Rdata\', sep = \'.\'))\\nmy.table <- eval(parse(text = tableName))\\n\\nfilterv <- rep(TRUE, nrow(my.table))\\nif (\'filter\' %in% names(query$metadata)) {\\n\\tfilters <- query$metadata$filter\\n\\tfor (i in 1:length(filters)){\\n\\t\\tif(!is.na(filters[i,1])) {\\n\\t\\t\\tfilterv <- filterv & grepl(filters[i,1], my.table[[query$metadata$colName[i]]]) \\n\\t\\t}\\n\\t\\tif (!is.na(filters[i,2])) {\\n\\t\\t\\tfilterv <- filterv & my.table[[query$metadata$colName[i]]] > filters[i,2]\\n\\t\\t}\\n\\t\\tif (!is.na(filters[i,3])) {\\n\\t\\t\\tfilterv <- filterv & my.table[[query$metadata$colName[i]]] < filters[i,3]\\t\\n\\t\\t}\\n\\t}\\n}\\n\\nout.tab <- my.table[query$metadata$colName][which(filterv),]\\n\\nif (\'fields\' %in% names(query$export)) {\\n\\tdecr <- query$export$fields[[1]]$value[2] == \'desc\' \\n\\tout.tab <- out.tab[order(out.tab[[query$export$fields[[1]]$value[1]]], decreasing = decr),]\\n}\\n\\n\\n#fix POS variable\\nif(\'POS\' %in% colnames(out.tab)) {\\n\\tref.v <- c(\'P\' = 1, \'C\' = 2, \'1B\' = 3, \'2B\' = 4, \'3B\' = 5, \'SS\' = 6, \'LF\' = 7, \'CF\' = 8, \'RF\' = 9, \'OF\' = 10, \'DH\' = 11)\\n\\tout.tab$POS <- names(ref.v)[out.tab$POS]\\n}\\nwrite.csv(out.tab, file = \'output.csv\')\\n" }\n'
line: b'{ "repo_name": "broadinstitute/gatk", "ref": "refs/heads/master", "path": "src/main/resources/org/broadinstitute/hellbender/tools/picard/analysis/qualityScoreDistribution.R", "content": "# Script to generate a chart of quality score distribution in a file\\n# @author Tim Fennell\\n\\n# Parse the arguments\\nargs <- commandArgs(trailing=T)\\nmetricsFile  <- args[1]\\noutputFile   <- args[2]\\nbamFile  <- args[3]\\nsubtitle <- ifelse(length(args) < 4, \\"\\", args[4])\\n\\n# Figure out where the metrics and the histogram are in the file and parse them out\\nstartFinder <- scan(metricsFile, what=\\"character\\", sep=\\"\\\\n\\", quiet=TRUE, blank.lines.skip=FALSE)\\n\\nfirstBlankLine=0\\n\\nfor (i in 1:length(startFinder))\\n{\\n        if (startFinder[i] == \\"\\") {\\n                if (firstBlankLine==0) {\\n                        firstBlankLine=i+1\\n                } else {\\n                        secondBlankLine=i+1\\n                        break\\n                }\\n        }\\n}\\n\\nmetrics <- read.table(metricsFile, header=T, nrows=1, sep=\\"\\\\t\\", skip=firstBlankLine)\\nhistogram <- read.table(metricsFile, header=T, sep=\\"\\\\t\\", skip=secondBlankLine)\\n\\n# Then plot the histogram as a PDF\\npdf(outputFile)\\n\\nplot(histogram$QUALITY,\\n     histogram$COUNT_OF_Q,\\n     type=\\"n\\",\\n     main=paste(\\"Quality Score Distribution\\\\nin file \\",bamFile,\\" \\",ifelse(subtitle == \\"\\",\\"\\",paste(\\"(\\",subtitle,\\")\\",sep=\\"\\")),sep=\\"\\"),\\n     xlab=\\"Quality Score\\",\\n     ylab=\\"Observations\\")\\n\\nqColor  <- \\"blue\\"\\noqColor <- \\"lightcyan2\\"\\nwidth <- 5\\n\\n# Plot OQ first so that it\'s \\"behind\\" the regular qualities\\nif (!is.null(histogram$COUNT_OF_OQ)) {\\n    lines(histogram$QUALITY+0.25, histogram$COUNT_OF_OQ, type=\\"h\\", col=oqColor, lty=1, lwd=width, lend=\\"square\\");\\n}\\n\\n# Then plot the regular qualities\\nlines(histogram$QUALITY, histogram$COUNT_OF_Q, type=\\"h\\", col=qColor, lty=1, lwd=width, lend=\\"square\\");\\n\\n# And add a legend\\nlegend(\\"topleft\\", pch=c(15,15), legend=c(\\"Quality Scores\\", \\"Original Quality Scores\\"), col=c(qColor, oqColor))\\n\\ndev.off()\\n\\n" }\n'
line: b'{ "repo_name": "RevolutionAnalytics/miniCRAN", "ref": "refs/heads/master", "path": "inst/doc/miniCRAN-non-CRAN-repos.R", "content": "## ----setup---------------------------------------------------------------\\n# Wrapper around available.packages ---------------------------------------\\n \\nindex <- function(url, type=\\"source\\", filters=NULL, head=5, cols=c(\\"Package\\", \\"Version\\")){\\n  contribUrl <- contrib.url(url, type=type)\\n  p <- available.packages(contribUrl, type=type, filters=filters)\\n  p[1:head, cols]\\n}\\n \\n\\n## ----CRAN, eval=FALSE----------------------------------------------------\\n#  CRAN <- \\"http://cran.r-project.org\\"\\n#  index(CRAN)\\n\\n## ----revo, eval=FALSE----------------------------------------------------\\n#  revoStable <- \\"http://packages.revolutionanalytics.com/cran/3.1/stable\\"\\n#  index(revoStable)\\n#  \\n#  revoMirror <- \\"http://cran.revolutionanalytics.com\\"\\n#  index(revoMirror)\\n\\n## ----rforge, eval=FALSE--------------------------------------------------\\n#  rforge <- \\"http://r-forge.r-project.org\\"\\n#  index(rforge)\\n\\n## ----bioc, eval=FALSE----------------------------------------------------\\n#  bioc <- local({\\n#    env <- new.env()\\n#    on.exit(rm(env))\\n#    evalq(source(\\"http://bioconductor.org/biocLite.R\\", local=TRUE), env)\\n#    biocinstallRepos()\\n#  })\\n#  \\n#  bioc\\n#  bioc[grep(\\"BioC\\", names(bioc))]\\n#  \\n#  \\n#  index(bioc[\\"BioCsoft\\"])\\n\\n" }\n'
line: b'{ "repo_name": "RevolutionAnalytics/miniCRAN", "ref": "refs/heads/master", "path": "inst/examples/example_pkgDep.R", "content": "\\n\\\\dontrun{\\npkgDep(pkg = c(\\"ggplot2\\", \\"plyr\\", \\"reshape2\\"), \\n       repos = c(CRAN = \\"http://mran.microsoft.com\\")\\n)\\n}\\n\\npdb <- cranJuly2014\\n\\\\dontrun{\\npdb <- pkgAvail(repos = c(CRAN = \\"http://mran.microsoft.com\\"))\\n}\\n\\npkgDep(pkg=c(\\"ggplot2\\", \\"plyr\\", \\"reshape2\\"), pdb)\\n\\n" }\n'
line: b'{ "repo_name": "RevolutionAnalytics/miniCRAN", "ref": "refs/heads/master", "path": "inst/examples/example_plot.pkgDepGraph.R", "content": "tags <- \\"chron\\"\\n\\n# Plot using defaults\\npdb <- cranJuly2014\\n\\n\\\\dontrun{\\n  pdb <- pkgAvail(\\n    repos = c(CRAN = \\"http://mran.microsoft.com\\"),\\n    type=\\"source\\"\\n  )\\n}\\n\\ndg <- makeDepGraph(tags, availPkgs = pdb  , includeBasePkgs=FALSE, suggests=TRUE, enhances=TRUE)\\n\\nset.seed(42); \\nplot(dg)\\n\\n# Move edge legend to top left\\nset.seed(42); \\nplot(dg, legendPosition=c(-1, 1))\\n\\n# Change font size and shape size\\nset.seed(42); \\nplot(dg, legendPosition=c(-1, 1), vertex.size=20,  cex=0.5)\\n\\n\\n# Move vertex legend to top right\\nset.seed(42); \\nplot(dg, legendPosition=c(1, 1), vertex.size=20,  cex=0.5)\\n\\n" }\n'
line: b'{ "repo_name": "RevolutionAnalytics/miniCRAN", "ref": "refs/heads/master", "path": "R/pkgDepTools.R", "content": "# Code copied from the pkgDepTools project\\n# Copyright (C) Seth Falcon\\n# http://www.bioconductor.org/packages/release/bioc/html/pkgDepTools.html\\n# \\n# This program is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU General Public License version 2\\n# as published by the Free Software Foundation\\n\\n\\n\\n# Code copied from the pkgDepTools project\\n# Copyright (C) Seth Falcon\\n# http://www.bioconductor.org/packages/release/bioc/html/pkgDepTools.html\\n\\n\\n# Copy of tools:::split_op_version.\\n\\n# @rdname pkgDepTools\\n# @keywords internal\\nsplit_op_version <- function (x) {\\n  pat <- \\"^([^\\\\\\\\([:space:]]+)[[:space:]]*\\\\\\\\(([^\\\\\\\\)]+)\\\\\\\\).*\\"\\n  x1 <- sub(pat, \\"\\\\\\\\1\\", x)\\n  x2 <- sub(pat, \\"\\\\\\\\2\\", x)\\n  if (x2 != x1) {\\n    pat <- \\"[[:space:]]*([[<>=!]+)[[:space:]]+(.*)\\"\\n    version <- sub(pat, \\"\\\\\\\\2\\", x2)\\n    if (!grepl(\\"^r\\", version)) \\n      version <- package_version(version)\\n    list(name = x1, op = sub(pat, \\"\\\\\\\\1\\", x2), version = version)\\n  }\\n  else list(name = x1)\\n}\\n\\n\\n# Copy of tools:::.split_dependencies.\\n\\n# @rdname pkgDepTools\\n# @keywords internal\\nsplit_dependencies <- function (x) {\\n  if (!length(x)) \\n    return(list())\\n  x <- unlist(strsplit(x, \\",\\"))\\n  x <- sub(\\"[[:space:]]+$\\", \\"\\", x)\\n  x <- unique(sub(\\"^[[:space:]]*(.*)\\", \\"\\\\\\\\1\\", x))\\n  names(x) <- sub(\\"^([[:alnum:].]+).*$\\", \\"\\\\\\\\1\\", x)\\n  lapply(x, split_op_version)\\n}\\n\\n\\n# Clean package fields.\\n# \\n# Given the value from a field like \'Depends\' in a package\'s DESCRIPTION file, return a character vector of package names with the version restrictions stripped and \\\\R~removed.\\n# @param val Value from a field like \'Depends\' in a package\'s DESCRIPTION file\\n# @rdname pkgDepTools\\n# @keywords internal\\ncleanPkgField <- function(val) {\\n  if (is.na(val))\\n    return(character(0))\\n  val <- names(split_dependencies(val))\\n  if (is.null(val))\\n    return(character(0))\\n  val <- val[! val %in% \\"R\\"]\\n  if (length(val))\\n    return(val)\\n  return(character(0))\\n}\\n" }\n'
line: b'{"repo_name":"Nirlendu/Dummy-Search-Engine","ref":"refs/heads/master","path":"tornado-3.2/build/lib.win32-2.7/tornado/wsgi.py","content":"#!/usr/bin/env python\\n#\\n# Copyright 2009 Facebook\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n# not use this file except in compliance with the License. You may obtain\\n# a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n# License for the specific language governing permissions and limitations\\n# under the License.\\n\\n\\"\\"\\"WSGI support for the Tornado web framework.\\n\\nWSGI is the Python standard for web servers, and allows for interoperability\\nbetween Tornado and other Python web frameworks and servers.  This module\\nprovides WSGI support in two ways:\\n\\n* `WSGIApplication` is a version of `tornado.web.Application` that can run\\n  inside a WSGI server.  This is useful for running a Tornado app on another\\n  HTTP server, such as Google App Engine.  See the `WSGIApplication` class\\n  documentation for limitations that apply.\\n* `WSGIContainer` lets you run other WSGI applications and frameworks on the\\n  Tornado HTTP server.  For example, with this class you can mix Django\\n  and Tornado handlers in a single server.\\n\\"\\"\\"\\n\\nfrom __future__ import absolute_import, division, print_function, with_statement\\n\\nimport sys\\nimport time\\nimport copy\\nimport tornado\\n\\nfrom tornado import escape\\nfrom tornado import httputil\\nfrom tornado.log import access_log\\nfrom tornado import web\\nfrom tornado.escape import native_str, parse_qs_bytes\\nfrom tornado.util import bytes_type, unicode_type\\n\\ntry:\\n    from io import BytesIO  # python 3\\nexcept ImportError:\\n    from cStringIO import StringIO as BytesIO  # python 2\\n\\ntry:\\n    import Cookie  # py2\\nexcept ImportError:\\n    import http.cookies as Cookie  # py3\\n\\ntry:\\n    import urllib.parse as urllib_parse  # py3\\nexcept ImportError:\\n    import urllib as urllib_parse\\n\\n# PEP 3333 specifies that WSGI on python 3 generally deals with byte strings\\n# that are smuggled inside objects of type unicode (via the latin1 encoding).\\n# These functions are like those in the tornado.escape module, but defined\\n# here to minimize the temptation to use them in non-wsgi contexts.\\nif str is unicode_type:\\n    def to_wsgi_str(s):\\n        assert isinstance(s, bytes_type)\\n        return s.decode(\'latin1\')\\n\\n    def from_wsgi_str(s):\\n        assert isinstance(s, str)\\n        return s.encode(\'latin1\')\\nelse:\\n    def to_wsgi_str(s):\\n        assert isinstance(s, bytes_type)\\n        return s\\n\\n    def from_wsgi_str(s):\\n        assert isinstance(s, str)\\n        return s\\n\\n\\nclass WSGIApplication(web.Application):\\n    \\"\\"\\"A WSGI equivalent of `tornado.web.Application`.\\n\\n    `WSGIApplication` is very similar to `tornado.web.Application`,\\n    except no asynchronous methods are supported (since WSGI does not\\n    support non-blocking requests properly). If you call\\n    ``self.flush()`` or other asynchronous methods in your request\\n    handlers running in a `WSGIApplication`, we throw an exception.\\n\\n    Example usage::\\n\\n        import tornado.web\\n        import tornado.wsgi\\n        import wsgiref.simple_server\\n\\n        class MainHandler(tornado.web.RequestHandler):\\n            def get(self):\\n                self.write(\\"Hello, world\\")\\n\\n        if __name__ == \\"__main__\\":\\n            application = tornado.wsgi.WSGIApplication([\\n                (r\\"/\\", MainHandler),\\n            ])\\n            server = wsgiref.simple_server.make_server(\'\', 8888, application)\\n            server.serve_forever()\\n\\n    See the `appengine demo\\n    \\u003chttps://github.com/facebook/tornado/tree/master/demos/appengine\\u003e`_\\n    for an example of using this module to run a Tornado app on Google\\n    App Engine.\\n\\n    WSGI applications use the same `.RequestHandler` class, but not\\n    ``@asynchronous`` methods or ``flush()``.  This means that it is\\n    not possible to use `.AsyncHTTPClient`, or the `tornado.auth` or\\n    `tornado.websocket` modules.\\n    \\"\\"\\"\\n    def __init__(self, handlers=None, default_host=\\"\\", **settings):\\n        web.Application.__init__(self, handlers, default_host, transforms=[],\\n                                 wsgi=True, **settings)\\n\\n    def __call__(self, environ, start_response):\\n        handler = web.Application.__call__(self, HTTPRequest(environ))\\n        assert handler._finished\\n        reason = handler._reason\\n        status = str(handler._status_code) + \\" \\" + reason\\n        headers = list(handler._headers.get_all())\\n        if hasattr(handler, \\"_new_cookie\\"):\\n            for cookie in handler._new_cookie.values():\\n                headers.append((\\"Set-Cookie\\", cookie.OutputString(None)))\\n        start_response(status,\\n                       [(native_str(k), native_str(v)) for (k, v) in headers])\\n        return handler._write_buffer\\n\\n\\nclass HTTPRequest(object):\\n    \\"\\"\\"Mimics `tornado.httpserver.HTTPRequest` for WSGI applications.\\"\\"\\"\\n    def __init__(self, environ):\\n        \\"\\"\\"Parses the given WSGI environment to construct the request.\\"\\"\\"\\n        self.method = environ[\\"REQUEST_METHOD\\"]\\n        self.path = urllib_parse.quote(from_wsgi_str(environ.get(\\"SCRIPT_NAME\\", \\"\\")))\\n        self.path += urllib_parse.quote(from_wsgi_str(environ.get(\\"PATH_INFO\\", \\"\\")))\\n        self.uri = self.path\\n        self.arguments = {}\\n        self.query_arguments = {}\\n        self.body_arguments = {}\\n        self.query = environ.get(\\"QUERY_STRING\\", \\"\\")\\n        if self.query:\\n            self.uri += \\"?\\" + self.query\\n            self.arguments = parse_qs_bytes(native_str(self.query),\\n                                            keep_blank_values=True)\\n            self.query_arguments = copy.deepcopy(self.arguments)\\n        self.version = \\"HTTP/1.1\\"\\n        self.headers = httputil.HTTPHeaders()\\n        if environ.get(\\"CONTENT_TYPE\\"):\\n            self.headers[\\"Content-Type\\"] = environ[\\"CONTENT_TYPE\\"]\\n        if environ.get(\\"CONTENT_LENGTH\\"):\\n            self.headers[\\"Content-Length\\"] = environ[\\"CONTENT_LENGTH\\"]\\n        for key in environ:\\n            if key.startswith(\\"HTTP_\\"):\\n                self.headers[key[5:].replace(\\"_\\", \\"-\\")] = environ[key]\\n        if self.headers.get(\\"Content-Length\\"):\\n            self.body = environ[\\"wsgi.input\\"].read(\\n                int(self.headers[\\"Content-Length\\"]))\\n        else:\\n            self.body = \\"\\"\\n        self.protocol = environ[\\"wsgi.url_scheme\\"]\\n        self.remote_ip = environ.get(\\"REMOTE_ADDR\\", \\"\\")\\n        if environ.get(\\"HTTP_HOST\\"):\\n            self.host = environ[\\"HTTP_HOST\\"]\\n        else:\\n            self.host = environ[\\"SERVER_NAME\\"]\\n\\n        # Parse request body\\n        self.files = {}\\n        httputil.parse_body_arguments(self.headers.get(\\"Content-Type\\", \\"\\"),\\n                                      self.body, self.body_arguments, self.files)\\n\\n        for k, v in self.body_arguments.items():\\n            self.arguments.setdefault(k, []).extend(v)\\n\\n        self._start_time = time.time()\\n        self._finish_time = None\\n\\n    def supports_http_1_1(self):\\n        \\"\\"\\"Returns True if this request supports HTTP/1.1 semantics\\"\\"\\"\\n        return self.version == \\"HTTP/1.1\\"\\n\\n    @property\\n    def cookies(self):\\n        \\"\\"\\"A dictionary of Cookie.Morsel objects.\\"\\"\\"\\n        if not hasattr(self, \\"_cookies\\"):\\n            self._cookies = Cookie.SimpleCookie()\\n            if \\"Cookie\\" in self.headers:\\n                try:\\n                    self._cookies.load(\\n                        native_str(self.headers[\\"Cookie\\"]))\\n                except Exception:\\n                    self._cookies = None\\n        return self._cookies\\n\\n    def full_url(self):\\n        \\"\\"\\"Reconstructs the full URL for this request.\\"\\"\\"\\n        return self.protocol + \\"://\\" + self.host + self.uri\\n\\n    def request_time(self):\\n        \\"\\"\\"Returns the amount of time it took for this request to execute.\\"\\"\\"\\n        if self._finish_time is None:\\n            return time.time() - self._start_time\\n        else:\\n            return self._finish_time - self._start_time\\n\\n\\nclass WSGIContainer(object):\\n    r\\"\\"\\"Makes a WSGI-compatible function runnable on Tornado\'s HTTP server.\\n\\n    Wrap a WSGI function in a `WSGIContainer` and pass it to `.HTTPServer` to\\n    run it. For example::\\n\\n        def simple_app(environ, start_response):\\n            status = \\"200 OK\\"\\n            response_headers = [(\\"Content-type\\", \\"text/plain\\")]\\n            start_response(status, response_headers)\\n            return [\\"Hello world!\\\\n\\"]\\n\\n        container = tornado.wsgi.WSGIContainer(simple_app)\\n        http_server = tornado.httpserver.HTTPServer(container)\\n        http_server.listen(8888)\\n        tornado.ioloop.IOLoop.instance().start()\\n\\n    This class is intended to let other frameworks (Django, web.py, etc)\\n    run on the Tornado HTTP server and I/O loop.\\n\\n    The `tornado.web.FallbackHandler` class is often useful for mixing\\n    Tornado and WSGI apps in the same server.  See\\n    https://github.com/bdarnell/django-tornado-demo for a complete example.\\n    \\"\\"\\"\\n    def __init__(self, wsgi_application):\\n        self.wsgi_application = wsgi_application\\n\\n    def __call__(self, request):\\n        data = {}\\n        response = []\\n\\n        def start_response(status, response_headers, exc_info=None):\\n            data[\\"status\\"] = status\\n            data[\\"headers\\"] = response_headers\\n            return response.append\\n        app_response = self.wsgi_application(\\n            WSGIContainer.environ(request), start_response)\\n        try:\\n            response.extend(app_response)\\n            body = b\\"\\".join(response)\\n        finally:\\n            if hasattr(app_response, \\"close\\"):\\n                app_response.close()\\n        if not data:\\n            raise Exception(\\"WSGI app did not call start_response\\")\\n\\n        status_code = int(data[\\"status\\"].split()[0])\\n        headers = data[\\"headers\\"]\\n        header_set = set(k.lower() for (k, v) in headers)\\n        body = escape.utf8(body)\\n        if status_code != 304:\\n            if \\"content-length\\" not in header_set:\\n                headers.append((\\"Content-Length\\", str(len(body))))\\n            if \\"content-type\\" not in header_set:\\n                headers.append((\\"Content-Type\\", \\"text/html; charset=UTF-8\\"))\\n        if \\"server\\" not in header_set:\\n            headers.append((\\"Server\\", \\"TornadoServer/%s\\" % tornado.version))\\n\\n        parts = [escape.utf8(\\"HTTP/1.1 \\" + data[\\"status\\"] + \\"\\\\r\\\\n\\")]\\n        for key, value in headers:\\n            parts.append(escape.utf8(key) + b\\": \\" + escape.utf8(value) + b\\"\\\\r\\\\n\\")\\n        parts.append(b\\"\\\\r\\\\n\\")\\n        parts.append(body)\\n        request.write(b\\"\\".join(parts))\\n        request.finish()\\n        self._log(status_code, request)\\n\\n    @staticmethod\\n    def environ(request):\\n        \\"\\"\\"Converts a `tornado.httpserver.HTTPRequest` to a WSGI environment.\\n        \\"\\"\\"\\n        hostport = request.host.split(\\":\\")\\n        if len(hostport) == 2:\\n            host = hostport[0]\\n            port = int(hostport[1])\\n        else:\\n            host = request.host\\n            port = 443 if request.protocol == \\"https\\" else 80\\n        environ = {\\n            \\"REQUEST_METHOD\\": request.method,\\n            \\"SCRIPT_NAME\\": \\"\\",\\n            \\"PATH_INFO\\": to_wsgi_str(escape.url_unescape(\\n                request.path, encoding=None, plus=False)),\\n            \\"QUERY_STRING\\": request.query,\\n            \\"REMOTE_ADDR\\": request.remote_ip,\\n            \\"SERVER_NAME\\": host,\\n            \\"SERVER_PORT\\": str(port),\\n            \\"SERVER_PROTOCOL\\": request.version,\\n            \\"wsgi.version\\": (1, 0),\\n            \\"wsgi.url_scheme\\": request.protocol,\\n            \\"wsgi.input\\": BytesIO(escape.utf8(request.body)),\\n            \\"wsgi.errors\\": sys.stderr,\\n            \\"wsgi.multithread\\": False,\\n            \\"wsgi.multiprocess\\": True,\\n            \\"wsgi.run_once\\": False,\\n        }\\n        if \\"Content-Type\\" in request.headers:\\n            environ[\\"CONTENT_TYPE\\"] = request.headers.pop(\\"Content-Type\\")\\n        if \\"Content-Length\\" in request.headers:\\n            environ[\\"CONTENT_LENGTH\\"] = request.headers.pop(\\"Content-Length\\")\\n        for key, value in request.headers.items():\\n            environ[\\"HTTP_\\" + key.replace(\\"-\\", \\"_\\").upper()] = value\\n        return environ\\n\\n    def _log(self, status_code, request):\\n        if status_code \\u003c 400:\\n            log_method = access_log.info\\n        elif status_code \\u003c 500:\\n            log_method = access_log.warning\\n        else:\\n            log_method = access_log.error\\n        request_time = 1000.0 * request.request_time()\\n        summary = request.method + \\" \\" + request.uri + \\" (\\" + \\\\\\n            request.remote_ip + \\")\\"\\n        log_method(\\"%d %s %.2fms\\", status_code, summary, request_time)\\n"}\n'
line: b'{ "repo_name": "RevolutionAnalytics/miniCRAN", "ref": "refs/heads/master", "path": "tests/testthat/test-3-makeRepo.R", "content": "if (interactive()) {library(testthat); Sys.setenv(NOT_CRAN = \\"true\\")}\\n\\ncontext(\\"makeRepo\\")\\n\\nrevolution <- MRAN(\\"2014-10-15\\")\\nif(!miniCRAN:::is.online(revolution, tryHttp = FALSE)) {\\n  # Use http:// for older versions of R\\n  revolution <- sub(\\"^https://\\", \\"http://\\", revolution)\\n}\\nrvers = \\"3.2\\"\\npkgs <- c(\\"Bmix\\")\\nrepo_root <- file.path(tempdir(), \\"miniCRAN\\", Sys.Date())\\nif (file.exists(repo_root)) unlink(repo_root, recursive = TRUE)\\n\\n# list.files(repo_root, recursive = TRUE)\\n\\n\\ntypes <- c(\\"source\\", \\"win.binary\\", \\"mac.binary\\", \\"mac.binary.mavericks\\")\\nnames(types) <- c(\\"source\\", \\"win.binary\\", \\"mac.binary\\", \\"mac.binary\\")\\n\\nfor (pkg_type in names(types)) {\\n  test_that(sprintf(\\"makeRepo downloads %s files and builds PACKAGES file\\", pkg_type), {\\n    skip_on_cran()\\n    skip_if_offline()\\n\\n    pdb <- pkgAvail(repos = revolution, type = pkg_type, Rversion = rvers)\\n    pkgList <- pkgDep(pkgs, availPkgs = pdb, repos = revolution, type = pkg_type,\\n                      suggests = FALSE, Rversion = rvers)\\n    prefix <- miniCRAN:::repoPrefix(pkg_type, Rversion = rvers)\\n    dir.create(repo_root, recursive = TRUE, showWarnings = FALSE)\\n\\n    ret <- makeRepo(pkgList, path = repo_root, repos = revolution, \\n             type = pkg_type, quiet = TRUE, Rversion = rvers)\\n    \\n    expect_is(ret, \\"character\\")\\n    expect_equal(length(ret), length(pkgList))\\n\\n    expect_true(\\n      miniCRAN:::.checkForRepoFiles(repo_root, pkgList, prefix)\\n    )\\n    expect_true(\\n      file.exists(file.path(repo_root, prefix, \\"PACKAGES.gz\\"))\\n    )\\n    expect_true(\\n      all(\\n        pkgList %in% pkgAvail(repos = repo_root, type = pkg_type, Rversion = rvers)[, \\"Package\\"]\\n      )\\n    )\\n  })\\n}\\n\\nunlink(repo_root, recursive = TRUE)\\n" }\n'
line: b'{ "repo_name": "RevolutionAnalytics/miniCRAN", "ref": "refs/heads/master", "path": "tests/testthat/test-2-makeDepGraph.R", "content": "checkPkgDepFunctions <- function(pkg, availPkgs = cranJuly2014, \\n                                 repos = MRAN(), \\n                                 type=\\"source\\", \\n                                 suggests=TRUE, \\n                                 enhances=FALSE, \\n                                 includeBasePkgs=FALSE){\\n  \\n  if(!require(igraph, quietly = TRUE)){\\n    skip(\\"package igraph not installed\\")\\n  }\\n  p1 <- pkgDep(pkg, availPkgs=availPkgs, \\n               repos=repos, type=type, \\n               suggests=suggests, enhances=enhances, \\n               includeBasePkgs=includeBasePkgs)\\n  p2 <- makeDepGraph(pkg, availPkgs=availPkgs, \\n                     repos=repos, type=type, \\n                     suggests=suggests, enhances=enhances, \\n                     includeBasePkgs=includeBasePkgs)\\n\\n  vnames <- V(p2)$name\\n  diff1 <- setdiff(vnames, p1)\\n  diff2 <- setdiff(p1, vnames)\\n  result <- length(diff1) == 0 & length(diff2) == 0\\n  if(!result) {\\n    msg <- paste0(\\"\\\\nmakeDepGraph() results not in pkgDep(): \\\\n - \\", paste(diff1, collapse=\\", \\"),\\n                  \\"\\\\npkgDep() results not in makeDepGraph(): \\\\n - \\", paste(diff2, collapse=\\", \\"))\\n    \\n    warning(msg)\\n  }\\n  result\\n}\\n\\n\\ncontext(\\"makeDepGraph \\")\\n\\nmock_require <- function(pkg, ...){\\n  packages.to.exclude <- c(\\"igraph\\")\\n  inSearchPath <- any(\\n    grepl(sprintf(\\"package:%s$\\", paste(packages.to.exclude, collapse = \\"|\\")), search())\\n  )\\n  if(inSearchPath) stop(\\"Required package already in search path\\")\\n  \\n  package <- as.character(substitute(pkg))\\n  if(package %in% packages.to.exclude)\\n    FALSE \\n  else \\n    base::requireNamespace(package, character.only = TRUE, ...)\\n}\\n\\n\\ntest_that(\\"throws error if igraph not available\\", {\\n  skip_if_offline()\\n  with_mock(\\n    `base::requireNamespace` = function(pkg, ...){\\n      packages.to.exclude <- c(\\"igraph\\")\\n      inSearchPath <- any(\\n        grepl(sprintf(\\"package:%s$\\", paste(packages.to.exclude, collapse = \\"|\\")), search())\\n      )\\n      if(inSearchPath) stop(\\"Required package already in search path\\")\\n      \\n      package <- as.character(substitute(pkg))\\n      if(package %in% packages.to.exclude)\\n        FALSE \\n      else \\n        base::requireNamespace(package, character.only = TRUE, ...)\\n    }, \\n{\\n  expect_false(requireNamespace(\\"igraph\\"))\\n  \\n  tag <- \\"MASS\\"\\n  \\n  expect_error(\\n    makeDepGraph(tag, availPkgs=cranJuly2014)\\n  )\\n  \\n})\\n\\n})\\n\\ntest_that(\\"makeDepGraph and pgkDep gives similar results for MASS\\", {\\n  skip_if_offline()\\n\\n  tag <- \\"MASS\\"\\n  \\n  expect_true(\\n    checkPkgDepFunctions(tag)\\n  )\\n  \\n  skip_on_cran()\\n  \\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE)\\n  )\\n  \\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, suggests=FALSE)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, enhances=TRUE)\\n  )\\n  \\n})\\n\\n\\ntest_that(\\"makeDepGraph and pgkDep gives similar results for chron\\", {\\n  \\n  skip_on_cran()\\n  \\n  tag <- \\"chron\\"\\n  \\n  expect_true(\\n    checkPkgDepFunctions(tag)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, suggests=FALSE)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, enhances=TRUE)\\n  )\\n  \\n})\\n\\n\\ntest_that(\\"makeDepGraph and pgkDep gives similar results for data.table\\", {\\n  \\n  skip_on_cran()\\n  \\n  tag <- \\"data.table\\"\\n  \\n  expect_true(\\n    checkPkgDepFunctions(tag)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, suggests=FALSE)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, enhances=TRUE)\\n  )\\n  \\n})\\n\\ntest_that(\\"makeDepGraph and pgkDep gives similar results for ggplot2\\", {\\n  \\n  skip_on_cran()\\n  \\n  tag <- \\"ggplot2\\"\\n  \\n  expect_true(\\n    checkPkgDepFunctions(tag)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, suggests=FALSE)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, enhances=TRUE)\\n  )\\n  \\n})\\n\\n\\ntest_that(\\"makeDepGraph and pgkDep gives similar results for complex query\\", {\\n  \\n  skip_on_cran()\\n  \\n  tag <- c(\\"ggplot2\\", \\"data.table\\", \\"plyr\\", \\"knitr\\", \\"shiny\\", \\"xts\\", \\"lattice\\")\\n  \\n  expect_true(\\n    checkPkgDepFunctions(tag)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, suggests=FALSE)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, enhances=TRUE)\\n  )\\n  \\n})\\n\\n" }\n'
line: b'{ "repo_name": "WinVector/CampaignPlanner", "ref": "refs/heads/master", "path": "server.R", "content": "\\n# This is the server logic for a Shiny web application.\\n# You can find out more about building applications with Shiny here:\\n#\\n# http://shiny.rstudio.com\\n#\\n\\nlibrary(\'shiny\')\\nlibrary(\'ggplot2\')\\nsource(\\"functions.R\\")\\n\\n\\nmakeTypicalTable = function(planTable, sizes, dummy) {\\n  dummy  # cause dummy promise to be evaluated (and trigger recalc)\\n  ptab = typicalTable(planTable, sizes)\\n  if(sum(sizes) > 0) {\\n    ptab$expectedSuccessRate = with(ptab, (Successes+0.5)/(Actions+1)) # posteriors from Jeffrey prior\\n    ptab$expectedValuePerAction = ptab$expectedSuccessRate*planTable$ValueSuccess\\n  }\\n  ptab\\n}\\n\\nlabeledPlan = function(sizes,rates,values,countGoal) {\\n  data.frame(Label=paste(\\"Campaign_\\", seq_len(length(sizes)), sep=\'\'),\\n             ActionsToMeetErrorGoals=sizes,\\n             ActionsToMeetCountGoals=ceiling(countGoal/rates),\\n             MatchingRates=c(rates[2]*values[2]/values[1],rates[1]*values[1]/values[2]))\\n}\\n\\ndisplayGraph = function(pgraph, doplot) {\\n  if(doplot) {\\n    plotSample(pgraph)\\n  } else NULL\\n}\\n\\n# ---------------------------------\\n\\nassembleResultTable = function(actions, successes, values, wishPrice) {\\n  ptab = data.frame(Label=paste(\\"Campaign_\\", seq_len(length(successes)), sep=\'\'),\\n             Actions=actions,\\n             Successes=successes,\\n             ValueSuccess=values)\\n  ptab$observedSuccessRate = (successes+0.5)/(actions+1) #  posterior Jeffreys prior 0.5,0.5 smoothing\\n  ptab$observedValuePerAction = ptab$observedSuccessRate*values\\n  ptab$pAboveWishPrice = pbeta(wishPrice/values,\\n                               shape1=0.5+successes,\\n                               shape2=0.5+actions-successes,\\n                               lower.tail=FALSE)\\n  ptab\\n}\\n\\nshinyServer(function(input, output) {\\n   #\\n   # for planning campaign\\n   #\\n   cprobabilities = reactive(c(input$conv1a, input$conv2a))\\n   values = reactive(c(input$value1a, input$value2a))\\n   sizes = reactive(c(input$sizes1a, input$sizes2a))\\n   proposedsizes = reactive(heuristicPowerPlan(data.frame(Probability=cprobabilities(), \\n                                                          ValueSuccess=values()), \\n                                       errorProbability=input$errorProb,relativeError=input$relErr)) \\n   countGoalV <- reactive(input$countGoal)\\n   \\n  docalc = reactive(sum(sizes()) != 0)\\n\\n  planTable = reactive(data.frame(Label=c(\'Campaign1\',\'Campaign2\'),\\n                                  Probability=cprobabilities(), \\n                                  ValueSuccess=values()))\\n  typicalTable = reactive(makeTypicalTable(planTable(), sizes(), input$reseed))\\n  pgraph2T = reactive(posteriorGraph(typicalTable()))\\n  output$planGraph2T = renderPlot(plotPosterior(pgraph2T()))\\n  output$probTable2T = renderPrint(computeProbsGEP(typicalTable(),pgraph2T()$graph))\\n  \\n  pgraph = reactive(sampleGraph(planTable(),sizes()))\\n  bgraph = reactive(computeProbsGES(planTable(),pgraph()))\\n\\n  output$plan = renderTable(labeledPlan(proposedsizes(),cprobabilities(),values(),countGoalV()),digits=4)\\n  output$typicalTable = renderPrint(typicalTable()) # I\'ll render it verbatim, rather than as a table.\\n                                               # Saves me from having to worry about sig figs\\n  output$planGraph = renderPlot(displayGraph(pgraph(), docalc()))\\n  output$probTable = renderPrint(bgraph())\\n\\n\\n  #\\n  # for evaluating campaign\\n  #\\n  actions = reactive(c(input$actions1b, input$actions2b))\\n  successes = reactive(c(input$success1b, input$success2b))\\n  svalues = reactive(c(input$value1b, input$value2b))\\n\\n  resTable = reactive(assembleResultTable(round(input$rescale*actions()), \\n                                          round(input$rescale*successes()), \\n                                          svalues(),\\n                                          input$wishPrice))\\n  pgraph2 = reactive(posteriorGraph(resTable()))\\n\\n  output$resTable = renderPrint(resTable())\\n  output$planGraph2 = renderPlot(plotPosterior(pgraph2(),input$wishPrice))\\n  output$probTable2 = renderPrint(computeProbsGEP(resTable(),pgraph2()$graph))\\n\\n})\\n" }\n'
line: b'{ "repo_name": "Reproducible-Science-Curriculum/rr-organization1", "ref": "refs/heads/master", "path": "files/file-org/forensic-science/rev1_final_analysis.R", "content": "data <- read.csv(file = \\"data1_full.csv\\", header = T)\\n\\nsub_data <- data[data$country == \\"Canada\\", ]\\n\\nwrite.csv(Canada, file = \\"/Users/csoderbe/rr-organization1/files/Canada.csv\\", row.names = FALSE)\\n\\nlibrary(ggplot2)\\nggplot(data = sub_data, aes(x = year, y = lifeExp)) +\\n  geom_point() +\\n  geom_line() \\nggsave(\\"graph.png\\")\\n\\n\\n\\n" }\n'
line: b'{ "repo_name": "Shians/Glimma", "ref": "refs/heads/master", "path": "R/gllink.R", "content": "#\' Plot linkages\\n#\' \\n#\' Helper function for writing the link properties in interactive Glimma plots\\n#\' \\n#\' @param from the index of the plot from which the event is dispatched.\\n#\' @param to the index of the plot which receives the event and performs an action.\\n#\' @param src the action that is performed in the \\"from\\" plot.\\n#\' @param dest the action that is performed in the \\"to\\" plot.\\n#\' @param flag indicates special links for particular chart types.\\n#\' @param both creates symmetric links whereby the \\"dest\\" action in \\"to\\" also triggers the \\"src\\" action in \\"from\\".\\n#\' @param info additional info for creating the link.\\n#\' \\n#\' @return a link object containing the plot linking information.\\n#\' \\n#\' @examples\\n#\' data(iris)\\n#\' data <- data.frame(Name=paste(\\"Flower\\", 1:nrow(iris), sep=\\"-\\"), iris)\\n#\' \\\\donttest{\\n#\' plot1 <- glScatter(data, xval=\\"Sepal.Length\\", yval=\\"Sepal.Width\\", colval=\\"Species\\")\\n#\' plot2 <- glScatter(data, xval=\\"Species\\", yval=\\"Petal.Length\\", colval=\\"Species\\")\\n#\' link1 <- gllink(1, 2, src=\\"hover\\", dest=\\"hover\\", both=TRUE)\\n#\' glimma(plot1, plot2, link1, layout=c(1,2))\\n#\' }\\n\\ngllink <- function(from, to, src=\\"none\\", dest=\\"none\\", flag=\\"none\\", both=FALSE, info=\\"none\\") {\\n    out <- list()\\n\\n    if (src != \\"none\\" && dest == \\"none\\") {\\n        stop(\\"src cannot be defined while dest is \'none\'\\")\\n    }\\n\\n    if (src == \\"none\\" && dest != \\"none\\") {\\n        stop(\\"dest cannot be defined while src is \'none\'\\")\\n    }   \\n\\n    if (src == \\"none\\" && dest == \\"none\\" && flag == \\"none\\") {\\n        stop(\\"\'src\', \'dest\' and \'flag\' cannot simultaneously be \'none\'\\")\\n    }\\n\\n    out$link <- data.frame(from=from, to=to, src=src, dest=dest, flag=flag, info=info)\\n    if (both) {\\n        out$link <- rbind(out$link, data.frame(from=to, to=from, src=dest, dest=src, flag=flag, info=info))\\n    }\\n\\n    out$type <- \\"link\\"\\n\\n    class(out) <- \\"jslink\\"\\n    return(out)\\n}\\n" }\n'
line: b'{ "repo_name": "Shians/Glimma", "ref": "refs/heads/master", "path": "R/glMDSPlot.R", "content": "#\' Glimma MDS Plot\\n#\'\\n#\' Draw an interactive MDS plot gene expression matrix with distances calculated from most variable genes.\\n#\'\\n#\' @author Shian Su, Gordon Smyth\\n#\'\\n#\' @param x the data.frame containing data to plot.\\n#\' @param ... additional arguments affecting the plots produced. See specific methods for detailed arguments.\\n#\'\\n#\' @return Draws a two-panel interactive MDS plot in an html page. The left panel contains the plot between two MDS dimensions, with annotations displayed on hover. The right panel contains a bar plot of the eigenvalues of each dimension, clicking on any of the bars will plot the corresponding dimension against the next dimension.\\n#\'\\n#\' @seealso \\\\code{\\\\link{glMDSPlot.default}}, \\\\code{\\\\link{glMDSPlot.DGEList}}\\n#\'\\n#\' @examples\\n#\' data(lymphomaRNAseq)\\n#\' genotype <- relevel(lymphomaRNAseq$samples$group, \\"Smchd1-null\\")\\n#\' \\\\donttest{\\n#\' glMDSPlot(lymphomaRNAseq, labels=1:7, groups=genotype)\\n#\' }\\n#\'\\n#\' @export\\n\\nglMDSPlot <- function(x, ...) {\\n    UseMethod(\\"glMDSPlot\\")\\n}\\n\\n#\' Glimma MDS Plot\\n#\'\\n#\' Draw an interactive MDS plot from a gene expression matrix with distances calculated from most variable genes.\\n#\'\\n#\' @author Shian Su, Gordon Smyth\\n#\'\\n#\' @param x the matrix containing the gene expressions.\\n#\' @param top the number of top most variable genes to use.\\n#\' @param labels the labels for each sample.\\n#\' @param groups the experimental group to which samples belong.\\n#\' @param gene.selection \\"pairwise\\" if most variable genes are to be chosen for each pair of samples or \\"common\\" to select the same genes for all comparisons.\\n#\' @param main the title of the plot.\\n#\' @param path the path in which the folder will be created.\\n#\' @param folder the name of the fold to save html file to.\\n#\' @param html the name of the html file to save plots to.\\n#\' @param launch TRUE to launch plot after call.\\n#\' @param ... additional arguments.\\n#\'\\n#\' @return Draws a two-panel interactive MDS plot in an html page. The left panel contains the plot between two MDS dimensions, with annotations displayed on hover. The right panel contains a bar plot of the eigenvalues of each dimension, clicking on any of the bars will plot the corresponding dimension against the next dimension.\\n#\'\\n#\' @method glMDSPlot default\\n#\'\\n#\' @importFrom stats cmdscale as.dist\\n#\'\\n#\' @export\\n\\n# Code taken from plotMDS of limma bioConductor package with alterations\\nglMDSPlot.default <- function(x, top=500, labels=1:ncol(x),\\n                            groups=rep(1, ncol(x)), gene.selection=\\"pairwise\\",\\n                            main=\\"MDS Plot\\", path=getwd(),\\n                            folder=\\"glimma-plots\\", html=\\"MDS-Plot\\",\\n                            launch=TRUE, ...) {\\n    #   Multi-dimensional scaling with top-distance\\n    #   Di Wu and Gordon Smyth\\n    #   19 March 2009.  Last modified 14 Jan 2015\\n    #   Modified by Shian Su on 25 Jan 2016\\n\\n    ##\\n    # Check Inputs\\n\\n    x <- as.matrix(x)\\n    nsamples <- ncol(x)\\n    ndim <- nsamples - 1\\n\\n    if (nsamples < 3) {\\n        stop(paste(\\"Only\\", nsamples, \\"columns of data: need at least 3\\"))\\n    }\\n\\n    cn <- colnames(x)\\n    bad <- rowSums(is.finite(x)) < nsamples\\n\\n    if (any(bad)) {\\n        x <- x[!bad, drop=FALSE]\\n    }\\n\\n    nprobes <- nrow(x)\\n    top <- min(top, nprobes)\\n\\n    #\\n    ##\\n\\n    plot.title <- quotify(main)\\n\\n    gene.selection <- match.arg(gene.selection, c(\\"pairwise\\", \\"common\\"))\\n\\n    # Distance matrix from pairwise leading fold changes\\n    dd <- matrix(0, nrow=nsamples, ncol=nsamples, dimnames=list(cn, cn))\\n    if (gene.selection == \\"pairwise\\") {\\n    # Distance measure is mean of top squared deviations for each pair of arrays\\n        topindex <- nprobes - top + 1L\\n        for (i in 2L:(nsamples)) {\\n            for (j in 1L:(i - 1L)) {\\n                dist <- sort.int((x[, i] - x[, j])^2, partial=topindex)\\n                topdist <- dist[topindex:nprobes]\\n                dd[i, j] <- sqrt(mean(topdist))\\n            }\\n        }\\n    } else {\\n    # Same genes used for all comparisons\\n        if (nprobes > top) {\\n            s <- rowMeans((x-rowMeans(x))^2)\\n            o <- order(s, decreasing=TRUE)\\n            x <- x[o[1L:top],, drop=FALSE]\\n        }\\n        for (i in 2L:(nsamples))\\n            dist <- sqrt(colMeans( (x[, i]-x[, 1:(i-1), drop=FALSE])^2 ))\\n            dd[i, 1L:(i-1L)] <- dist\\n        axislabel <- \\"Principal Component\\"\\n    }\\n\\n    # Multi-dimensional scaling\\n    a1 <- suppressWarnings(cmdscale(as.dist(dd), k=min(ndim, 8), eig=TRUE))\\n\\n    # Method for MDS objects\\n    points <- a1$points\\n\\n    if (!is.data.frame(groups)) {\\n    # Rename for the column name in dataframe\\n        group <- groups\\n        groups <- data.frame(group)\\n    }\\n\\n    first.col.name <- colnames(groups)[1]\\n\\n    points <- data.frame(points)\\n    names(points) <- paste0(\\"dim\\", 1:ncol(points))\\n    points <- data.frame(points, label=labels, groups)\\n\\n    eigen <- data.frame(name = 1:min(ndim, 8),\\n                        eigen = round(a1$eig[1:min(ndim, 8)]/sum(a1$eig), 2))\\n\\n    plot1 <- glScatter(points, xval=\\"dim1\\", yval=\\"dim2\\", point.size=4,\\n                        xlab=\\"Dimension 1\\", ylab=\\"Dimension 2\\",\\n                        annot=c(\\"label\\", first.col.name, \\"dim1\\", \\"dim2\\"),\\n                        colval=first.col.name, main=main,\\n                        info=list(groupsNames=colnames(groups)))\\n\\n    plot2 <- glBar(eigen, names.arg=\\"name\\", yval=\\"eigen\\",\\n                    main=\\"Variance Explained\\",\\n                    xlab=\\"Dimension\\", ylab=\\"Proportion\\",\\n                    height=300, width=300, info=list(dims=ndim))\\n\\n    link1 <- gllink(2, 1, flag=\\"mds\\")\\n\\n    glimma(plot1, plot2, link1, layout=c(1, 2), overwrite=TRUE,\\n            path=path, folder=folder, html=html, launch=launch)\\n}\\n\\n#\' Glimma MDS Plot\\n#\'\\n#\' Draw an interactive MD plot from a DGEList object with distances calculated from most variable genes.\\n#\'\\n#\' @author Shian Su, Gordon Smyth\\n#\'\\n#\' @param x the DGEList containing the gene expressions.\\n#\' @param top the number of top most variable genes to use.\\n#\' @param labels the labels for each sample.\\n#\' @param groups the experimental group to which samples belong.\\n#\' @param gene.selection \\"pairwise\\" if most variable genes are to be chosen for each pair of samples or \\"common\\" to select the same genes for all comparisons.\\n#\' @param main the title of the plot.\\n#\' @param path the path in which the folder will be created.\\n#\' @param folder the name of the fold to save html file to.\\n#\' @param html the name of the html file to save plots to.\\n#\' @param launch TRUE to launch plot after call.\\n#\' @param ... additional arguments.\\n#\'\\n#\' @return Draws a two-panel interactive MDS plot in an html page. The left panel contains the plot between two MDS dimensions, with annotations displayed on hover. The right panel contains a bar plot of the eigenvalues of each dimension, clicking on any of the bars will plot the corresponding dimension against the next dimension.\\n#\'\\n#\' @method glMDSPlot DGEList\\n#\'\\n#\' @export\\nglMDSPlot.DGEList <- function (x, top=500, labels=1:ncol(x),\\n                            groups=rep(1, ncol(x)), gene.selection=\\"pairwise\\",\\n                            main=\\"MDS Plot\\", path=getwd(),\\n                            folder=\\"glimma-plots\\", html=\\"MDS-Plot\\",\\n                            launch=TRUE, ...) {\\n    x <- edgeR::cpm(x, log=TRUE)\\n    glMDSPlot.default(x, top=500, labels=labels, groups=groups,\\n                    gene.selection=\\"pairwise\\", main=main, path=path,\\n                    folder=folder, html=html, launch=launch, ...)\\n}\\n" }\n'
line: b'{ "repo_name": "percyfal/biomake", "ref": "refs/heads/master", "path": "scripts/plotThetas.R", "content": "#! /usr/bin/Rscript --vanilla\\n# File: plotThetas.R\\n# Created: Tue Oct 29 17:12:00 2013\\n# $Id: $\\n#\\n# Copyright (C) 2013 by Per Unneberg\\n#\\n# Author: Per Unneberg\\n#\\n# Description:\\n#\\n\\nlibrary(utils)\\nlibrary(lattice)\\nlibrary(RColorBrewer)\\ncpal <- colorRampPalette(brewer.pal(9,\\"Paired\\"))(1000)\\n\\nargs <- commandArgs(TRUE)\\nif (length(args) != 2) {\\n    message(\\"Usage: plotThetas.R infile outfile!\\")\\n    quit(\\"yes\\")\\n}\\n\\ninfile <- args[1]\\n\\ndir <- basename(dirname(normalizePath(infile)))\\nif (grep(\\"w[0-9]+_[0-9]+\\", dir)) {\\n    message(\\"We have a window match\\")\\n    window <- gsub(\\"w([0-9]+)_.*\\", \\"\\\\\\\\1\\", dir)\\n    step <- gsub(\\".*_([0-9]+)\\", \\"\\\\\\\\1\\", dir)\\n} else {\\n    window <- 50000\\n    step <- 50000\\n}\\npop = gsub(\\"(^[A-Z]+)_.*\\", \\"\\\\\\\\1\\", infile)\\n\\nd <- read.table(infile)\\nnames(d) <- c(\\"range\\", \\"chr\\", \\"pos\\", \\"tW\\", \\"tP\\", \\"tF\\", \\"tH\\", \\"tL\\", \\"tajD\\", \\"fulif\\", \\"fuliD\\", \\"fayH\\", \\"zengsE\\", \\"numSites\\")\\nd <- cbind(d, d[,c(\\"tW\\", \\"tP\\", \\"tF\\", \\"tH\\", \\"tL\\")] / d$numSites)\\nnames(d) <- c(\\"range\\", \\"chr\\", \\"pos\\", \\"tW\\", \\"tP\\", \\"tF\\", \\"tH\\", \\"tL\\", \\"tajD\\", \\"fulif\\", \\"fuliD\\", \\"fayH\\", \\"zengsE\\", \\"numSites\\", \\"tW.norm\\", \\"tP.norm\\", \\"tF.norm\\", \\"tH.norm\\", \\"tL.norm\\")\\nd$pop = pop\\nd.stack <- cbind(stack(d[,c(\\"numSites\\", \\"tajD\\", \\"fuliD\\", \\"fulif\\", \\"fayH\\", \\"zengsE\\", \\"tW\\", \\"tP\\", \\"tW.norm\\", \\"tP.norm\\")]), pop=d$pop, pos=d$pos, chr=d$chr)\\noutfile <- args[2]\\n\\n# Redefine factor levels\\nd.stack$ind <- factor(d.stack$ind, levels = c(\\"numSites\\",  \\"tajD\\", \\"tW\\", \\"fuliD\\", \\"tW.norm\\", \\"fulif\\", \\"tP\\", \\"fayH\\", \\"tP.norm\\",   \\"zengsE\\"))\\n\\npdf(outfile)\\nprint(xyplot(values ~ pos/1e6 | ind, data=d.stack, type=\\"l\\", layout=c(2,5), xlab=\\"pos (Mb)\\", scales=list(y=list(rot=45, relation=\\"free\\")), strip=FALSE, strip.left=TRUE, main=paste(basename(infile), \\", w:\\", window, \\", s:\\", step, sep=\\"\\"), par.settings=simpleTheme()))\\n\\n\\nd.stack$ind <- factor(d.stack$ind, levels = c(\\"numSites\\", \\"tW\\",  \\"tW.norm\\",  \\"tP\\", \\"tP.norm\\", \\"tajD\\", \\"fuliD\\", \\"fulif\\", \\"fayH\\", \\"zengsE\\"))\\nprint(xyplot(values ~ pos/1e6 | ind, data=d.stack, type=\\"l\\", layout=c(1,10), xlab=\\"pos (Mb)\\", scales=list(y=list(rot=45, relation=\\"free\\")), strip=FALSE, strip.left=TRUE, main=paste(basename(infile), \\", w:\\", window, \\", s:\\", step, sep=\\"\\"), par.settings=simpleTheme()))\\n\\ndev.off()\\n" }\n'
line: b'{ "repo_name": "kbseah/genome-bin-tools", "ref": "refs/heads/master", "path": "gbtools/R/setOperation.R", "content": "#\' Generic operation for merging and subsetting two gbtbin objects\\n#\' \\n#\' @param x1 Object of class gbtbin\\n#\' @param x2 Object of class gbtbin\\n#\' @param shortlist Vector of contig IDs to make new bin\\n#\' @return Object of class gbtbin\\n#\' @keywords internal\\n\\nsetOperation <- function(x1, x2, shortlist) UseMethod(\\"setOperation\\")\\n" }\n'
line: b'{ "repo_name": "kbseah/genome-bin-tools", "ref": "refs/heads/master", "path": "gbtools/R/choosebinPolygon.gbt.R", "content": "#\' Choose bin from gbt object by defining polygon in coverage-GC or differential coverage plot\\n#\'\\n#\' Choose genome bin by specifying coordinates of a polygon from\\n#\' GC-coverage or differential coverage plot of a gbt object\\n#\'\\n#\' @param x Object of class gbt\\n#\' @param slice Slice parameter for drawing the polygon\\n#\' @param polygon Polygon that would define the bin\\n#\'\\n#\' @importFrom sp point.in.polygon\\n#\' @return Object of class gbtbin\\n#\' @seealso \\\\code{\\\\link{plot.gbt}}\\n#\' @export\\n#\'\\nchoosebinPolygon.gbt <- function(x,  # Object of class gbt\\n                          slice,  # Which slices used for the plot from which points to be chosen?\\n                          taxon=\\"Class\\",  # Deprecated - user don\'t change this\\n                          binpolygon=NA, # The polygon\\n                          save=FALSE,  # Save list of contigs in bin to external file?\\n                          file=\\"interactive_bin.list\\"  # Name of file to save list of contigs in bin\\n                          ) {\\n    require(sp)\\n## Wrapper for picking bin interactively from GC-cov or diff-cov plot\\n    if (!is.numeric(slice) || length(slice) > 2) {\\n        cat (\\"gbtools ERROR: Please specify the library(-ies) used to make the plot in focus\\\\n\\")\\n    } else {\\n        if (length(slice)==1) {  # Pick bin from GC-coverage plot\\n            X <- merge(data.frame(ID=x$scaff$ID,\\n                                  Ref_GC=x$scaff$Ref_GC),\\n                       data.frame(ID=x$covs$ID,\\n                                  Avg_fold=x$covs[slice[1]+1]),\\n                       by=\\"ID\\")\\n            names(X) <- c(\\"ID\\",\\"Ref_GC\\",\\"Avg_fold\\")\\n            inpolygon <- sp::point.in.polygon(X$Ref_GC,\\n                                              X$Avg_fold,\\n                                              binpolygon$x,\\n                                              binpolygon$y)\\n        }\\n        else if (length(slice)==2) {  # Pick bin from differential coverage plot\\n            X <- merge(data.frame(ID=x$scaff$ID,\\n                                  Ref_GC=x$scaff$Ref_GC),\\n                       data.frame(ID=x$covs$ID,\\n                                  Avg_fold_1=x$covs[slice[1]+1],\\n                                  Avg_fold_2=x$covs[slice[2]+1]),\\n                       by=\\"ID\\")\\n            names(X) <- c(\\"ID\\",\\"Ref_GC\\",\\"Avg_fold_1\\",\\"Avg_fold_2\\")\\n            inpolygon <- sp::point.in.polygon(X$Avg_fold_1,\\n                                              X$Avg_fold_2,\\n                                              binpolygon$x,\\n                                              binpolygon$y)\\n        }\\n        X.subset <- X[which(inpolygon==1),]\\n        X.shortlist <- as.character(X.subset$ID)\\n        result <- gbtbin(shortlist=X.shortlist,\\n                         x=x,\\n                         slice=slice,\\n                         taxon=taxon,\\n                         points=binpolygon,\\n                         save=save,\\n                         file=file)\\n        result$call[[length(result$call)+1]] <- match.call()  # Record choosebin() function call\\n        return(result)\\n    }\\n}\\n" }\n'
line: b'{ "repo_name": "kbseah/genome-bin-tools", "ref": "refs/heads/master", "path": "gbtools/R/countSingleFromTable.R", "content": "#\' Tabulate objects and count how many singletons\\n#\'\\n#\' @param x Object of class data.frame or vector\\n#\' @return Numeric vector of length 1\\n#\' @keywords internal\\ncountSingleFromTable <- function(x) {\\n    x.tab <- table(x)\\n    uniq <- length(which(x.tab==1))\\n    return(uniq)\\n}\\n" }\n'
line: b'{ "repo_name": "kbseah/genome-bin-tools", "ref": "refs/heads/master", "path": "gbtools/R/lej.gbtbin.R", "content": "#\' Take difference between two gbtbin objects\\n#\'\\n#\' Takes the reverse complement of two gbtbin objects. Equivalent to setdiff\\n#\' in R, or left-exclusive-join in SQL. Non commutative!\\n#\'\\n#\' Self explanatory...\\n#\'\\n#\' @inheritParams add\\n#\'\\n#\' @seealso \\\\code{\\\\link{add}}\\n#\'\\n#\' @export\\nlej.gbtbin <- function(x1,x2) {\\n## Take difference between two bins - non commutative! i.e. left exclusive join\\n    shortlist <- x1$scaff$ID[which(!x1$scaff$ID %in% x2$scaff$ID)]\\n    result <- setOperation.gbtbin(x1=x1,\\n                                  x2=x2,\\n                                  shortlist=shortlist)\\n    result$call[[length(result$call)+1]] <- match.call()  # Record function call \\n    return(result)\\n}\\n" }\n'
line: b'{ "repo_name": "kbseah/genome-bin-tools", "ref": "refs/heads/master", "path": "gbtools/R/generatePlotColors2.R", "content": "#\' Generates colors for marker gene phylotypes in plot by cumulative weight\\n#\'\\n#\' For each taxon, calculates the total contigLength*coverage, and assigns\\n#\' colors for the top taxa which in total account for more than a specified\\n#\' minimum weight.\\n#\'\\n#\' @param scaffold.stats Scaff table from gbt object\\n#\' @param marker.list markTab table from gbt object\\n#\' @param taxon Taxonomic level to do coloring\\n#\' @param consensus Logical - if taxon assignments conflict, take consensus?\\n#\' @param weightCutoff Cutoff quantile for contig length*coverage weight (between 0 and 1)\\n#\' @return data.frame with color assignments for each scaffold\\n#\' @keywords internal\\n#\'\\ngeneratePlotColors2 <- function(scaffold.stats,  # scaff table from gbt object\\n                               marker.list,  # markTab table from gbt object\\n                               taxon,  # Taxonomic level to do the coloring\\n                               consensus,  # Logical-if taxon assgs conflict, take consens?\\n                               weightCutoff # Cutoff quantile for assigning colors (between 0 and 1)\\n                               ) {           # This took a very long time to get it right\\n## Generates colors for marker gene phylotypes in plot\\n    ## Merge tables to have points to plot for the markers #########################\\n    marker.stats <- mergeScaffMarker(scaffold.stats,\\n                                     marker.list,\\n                                     taxon,\\n                                     consensus)\\n    ## Calculate total weight for each taxon #######################################\\n    taxon.agg <- aggregate(marker.stats$Length*marker.stats$Avg_fold,\\n                           by=list(marker.stats$taxon),\\n                           FUN=sum\\n                           )\\n    total.weight <- sum(taxon.agg$x)\\n    taxon.agg.order <- taxon.agg[order(taxon.agg$x,decreasing=TRUE),] # Sort descending\\n    ## Make list of top taxa #######################################################\\n    if (weightCutoff > 1 || weightCutoff < 0) { # Catch errors for weightCutoff\\n        cat (\\"gbtools ERROR: weightCutoff parameter must be between 0 and 1\\\\n\\")\\n    } \\n    else {\\n        # Count taxa which have the highest weight, until weightCutoff\\n        numAboveCutoff <- length(\\n                                 which(\\n                                       cumsum(taxon.agg.order$x) <= weightCutoff*total.weight\\n                                       )\\n                                 )\\n        numBelowCutoff <- length(\\n                                 which(\\n                                       cumsum(taxon.agg.order$x) > weightCutoff*total.weight\\n                                       )\\n                                 )\\n        # Generate colors from red to violet for taxa above cutoff\\n        thecolors <- rainbow (numAboveCutoff,\\n                              start=0,\\n                              end=3/4)\\n        # Everything else is colored grey\\n        repgrey <- rep (\\"grey50\\", numBelowCutoff)\\n        # Combine the two vectors\\n        thecolors <- c(thecolors, repgrey)\\n        colorframe <- data.frame(taxon=taxon.agg.order[,1],\\n                                 colors=thecolors)\\n        # Merge into marker.stats df\\n        marker.stats <- merge(marker.stats,\\n                              colorframe,\\n                              by=\\"taxon\\")\\n        # Return table\\n        return(marker.stats)\\n    }\\n\\n}" }\n'
line: b'{ "repo_name": "kbseah/genome-bin-tools", "ref": "refs/heads/master", "path": "gbtools/R/identify.gbt.R", "content": "#\' Identify points in gbtools plot\\n#\'\\n#\' Click on GC-coverage plot or differential coverage plot to identify contigs.\\n#\' In cluttered plots it may not be very accurate! Will write the contig ID\\n#\' as a label overlay on the plot.\\n#\'\\n#\' @return Object of class gbtbin containing identified contigs\\n#\' @export\\n#\' @seealso \\\\code{\\\\link{plot.gbt}}\\n#\' @seealso \\\\code{\\\\link{choosebin}}\\n\\nidentify.gbt <- function(d,\\n#\' @param d Object of class gbt or gbtbin, in plot\\n                         slice=1,\\n#\' @param slice Which sample data was plotted? (Default: 1)\\n                         ...\\n#\' @param ... Further arguments passed to identify.default()\\n                         ) {\\n    # Catch invalid \\"slice\\" parameters\\n    if (is.na(slice) || !is.numeric(slice)) {\\n        cat (\\"gbtools ERROR: Please specify valid value for slice parameter\\\\n\\")\\n    } else {\\n        # Data frame for GC-coverage plots\\n        if (length (slice) == 1) {\\n            X <- merge(data.frame(ID=d$scaff$ID,\\n                                  Ref_GC=d$scaff$Ref_GC,\\n                                  Length=d$scaff$Length,\\n                                  Avg_fold=d$scaff$Avg_fold,\\n                                  xVals=d$scaff$Ref_GC),\\n                       data.frame(ID=d$covs$ID,\\n                                  yVals=d$covs[[slice[1]+1]]\\n                                  ),\\n                       by=\\"ID\\")\\n            names(X) <- c(\\"ID\\",\\"Ref_GC\\",\\"Length\\",\\"Avg_fold\\",\\"xVals\\",\\"yVals\\")\\n        }\\n        # Data frame for differential coverage plots\\n        else if (length(slice) == 2) {\\n            X <- merge (data.frame (ID=d$scaff$ID,\\n                                    Ref_GC=d$scaff$Ref_GC,\\n                                    Length=d$scaff$Length,\\n                                    Avg_fold=d$scaff$Avg_fold),\\n                        data.frame (ID=d$covs$ID,\\n                                    xVals=d$covs[[slice[1]+1]],\\n                                    yVals=d$covs[[slice[2]+1]]),\\n                        by=\\"ID\\")\\n            names(X) <- c(\\"ID\\",\\"Ref_GC\\",\\"Length\\",\\"Avg_fold\\",\\"xVals\\",\\"yVals\\")\\n        }\\n        # Implement the identify parameter...\\n        shortlist <- identify(x=X$xVals,\\n                              y=X$yVals,\\n                              labels=X$ID,\\n                              ...\\n                              )\\n        shortlist.contigs <- X$ID[shortlist]\\n        return(gbtbin(as.character(shortlist.contigs),d,slice=slice))\\n        #return(shortlist)\\n    }\\n}" }\n'
line: b'{ "repo_name": "kbseah/genome-bin-tools", "ref": "refs/heads/master", "path": "gbtools/R/userAdd.gbt.R", "content": "#\' Add custom user annotations to gbt object\\n#\'\\n#\' Custom user annotations for each scaffold can be added to existing gbt\\n#\' objects. The annotations should be in a data.frame, with at least column\\n#\' \\"scaffold\\" that matches scaffold IDs in the gbt object. Pass the name of the\\n#\' data.frame to the userTab parameter. Give a unique name for this annotation\\n#\' to the userSource parameter.\\n#\'\\n#\' @param x Object of class gbt\\n#\' @param userTab data.frame with user annotations, see Details\\n#\' @param userSource Name for this annotation table\\n#\' @return Object of class gbt\\n#\' @seealso \\\\code{\\\\link{gbt}} \\\\code{\\\\link{plot.gbt}}\\n#\' @export\\nuserAdd.gbt <- function(x,\\n                        userTab,\\n                        userSource=NA\\n                        ) {\\n    ## Check that userTab is data.frame with col \\"scaffold\\" ###################\\n    if (!is.data.frame(userTab) ||\\n        length(which(names(userTab)==\\"scaffold\\"))==0 ||\\n        is.na(userSource) ) {\\n        cat(\\"gbtools ERROR: Please check inputs. See help(userAdd) \\\\n\\")\\n    } else {\\n        ## Check that userTab scaffold IDs match x scaffold IDs ###############\\n        if (length(which(userTab$scaffold %in% x$scaff$ID))==0) {\\n            cat (\\"gbtools ERROR: Scaffold IDs in userTab don\'t match gbt object\\\\n\\")\\n        } else {\\n            x$userTab[[length(x$userTab)+1]] <- userTab  # Append userTab\\n            x$userSource[length(x$userTab)] <- userSource # Append userSource\\n            # NB: Using c() will create discrepancy between userTab and userSource\\n            # because c() on an empty vector will create first element \\"\\"\\n            x$call[[length(x$call)+1]] <- match.call()  # Record function call\\n            return(x)  # Return result\\n        }\\n    }\\n}\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/binary/matrix/UaggOuterChainEquals.R", "content": "#-------------------------------------------------------------\\r\\n#\\r\\n# (C) Copyright IBM Corp. 2010, 2015\\r\\n#\\r\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\r\\n# you may not use this file except in compliance with the License.\\r\\n# You may obtain a copy of the License at\\r\\n#\\r\\n#     http://www.apache.org/licenses/LICENSE-2.0\\r\\n#\\r\\n# Unless required by applicable law or agreed to in writing, software\\r\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\r\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\r\\n# See the License for the specific language governing permissions and\\r\\n# limitations under the License.\\r\\n#\\r\\n#-------------------------------------------------------------\\r\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nA <- as.matrix(readMM(paste(args[1], \\"A.mtx\\", sep=\\"\\")))\\r\\nB <- as.matrix(readMM(paste(args[1], \\"B.mtx\\", sep=\\"\\")))\\r\\n\\r\\nC = rowSums(outer(A,B,\\"==\\"));\\r\\n\\r\\nwriteMM(as(C, \\"CsparseMatrix\\"), paste(args[2], \\"C\\", sep=\\"\\")); \\r\\n\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/aggregate/RowMaxs.R", "content": "#-------------------------------------------------------------\\r\\n#\\r\\n# (C) Copyright IBM Corp. 2010, 2015\\r\\n#\\r\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\r\\n# you may not use this file except in compliance with the License.\\r\\n# You may obtain a copy of the License at\\r\\n#\\r\\n#     http://www.apache.org/licenses/LICENSE-2.0\\r\\n#\\r\\n# Unless required by applicable law or agreed to in writing, software\\r\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\r\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\r\\n# See the License for the specific language governing permissions and\\r\\n# limitations under the License.\\r\\n#\\r\\n#-------------------------------------------------------------\\r\\n\\r\\nargs <- commandArgs(TRUE)\\r\\n\\r\\nif(!(\\"matrixStats\\" %in% rownames(installed.packages()))){\\r\\n   install.packages(\\"matrixStats\\")\\r\\n}\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\nlibrary(\\"matrixStats\\") \\r\\n\\r\\nA <- as.matrix(readMM(paste(args[1], \\"A.mtx\\", sep=\\"\\")))\\r\\nB <- rowMaxs(A);\\r\\n\\r\\nwriteMM(as(B, \\"CsparseMatrix\\"), paste(args[2], \\"B\\", sep=\\"\\")); " }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/misc/ScalarFunctionTest2.R", "content": "#-------------------------------------------------------------\\r\\n#\\r\\n# (C) Copyright IBM Corp. 2010, 2015\\r\\n#\\r\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\r\\n# you may not use this file except in compliance with the License.\\r\\n# You may obtain a copy of the License at\\r\\n#\\r\\n#     http://www.apache.org/licenses/LICENSE-2.0\\r\\n#\\r\\n# Unless required by applicable law or agreed to in writing, software\\r\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\r\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\r\\n# See the License for the specific language governing permissions and\\r\\n# limitations under the License.\\r\\n#\\r\\n#-------------------------------------------------------------\\r\\n\\r\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nsquare <- function(a) {\\r\\n   b = a*a;   \\r\\n   return(b);\\r\\n}\\r\\n\\r\\nx = 1.9/2.9;\\r\\ny = square(x);\\r\\nR = as.matrix(y);\\r\\n\\r\\nwriteMM(as(R, \\"CsparseMatrix\\"), paste(args[1], \\"R\\", sep=\\"\\")); \\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/binary/matrix/UaggOuterChain.R", "content": "#-------------------------------------------------------------\\r\\n#\\r\\n# (C) Copyright IBM Corp. 2010, 2015\\r\\n#\\r\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\r\\n# you may not use this file except in compliance with the License.\\r\\n# You may obtain a copy of the License at\\r\\n#\\r\\n#     http://www.apache.org/licenses/LICENSE-2.0\\r\\n#\\r\\n# Unless required by applicable law or agreed to in writing, software\\r\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\r\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\r\\n# See the License for the specific language governing permissions and\\r\\n# limitations under the License.\\r\\n#\\r\\n#-------------------------------------------------------------\\r\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nA <- as.matrix(readMM(paste(args[1], \\"A.mtx\\", sep=\\"\\")))\\r\\nB <- as.matrix(readMM(paste(args[1], \\"B.mtx\\", sep=\\"\\")))\\r\\n\\r\\nC = rowSums(outer(A,B,\\"<\\"));\\r\\n\\r\\nwriteMM(as(C, \\"CsparseMatrix\\"), paste(args[2], \\"C\\", sep=\\"\\")); \\r\\n\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/vect/VectorizeLixRowPos.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nA = as.matrix(readMM(paste(args[1], \\"A.mtx\\", sep=\\"\\")))\\r\\n\\r\\nR = A;\\r\\nR[3,7] = as.matrix(3);\\r\\nR[3,8] = as.matrix(4);\\r\\n\\r\\n\\r\\nwriteMM(as(R, \\"CsparseMatrix\\"), paste(args[2], \\"R\\", sep=\\"\\")); \\r\\n\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/parfor/parfor_optimizer3.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nV1 <- readMM(paste(args[1], \\"V.mtx\\", sep=\\"\\"))\\r\\nV <- as.matrix(V1);\\r\\nn <- ncol(V); \\r\\nn2 <- n/2;\\r\\n\\r\\nR <- array(0,dim=c(1,n2))\\r\\n\\r\\nfor( i in 1:n2 )\\r\\n{\\r\\n   X <- V[,i];                 \\r\\n   Y <- V[,n-i+1];                \\r\\n   R[1,i] <- sum(X)+sum(Y);\\r\\n}   \\r\\n\\r\\nwriteMM(as(R, \\"CsparseMatrix\\"), paste(args[2], \\"Rout\\", sep=\\"\\")); \\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/unary/matrix/replace_maxmin.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nA <- as.matrix(readMM(paste(args[1], \\"A.mtx\\", sep=\\"\\")))\\r\\n\\r\\nC <- replace(A, A==as.numeric(min(A)), max(A));\\r\\n\\r\\nwriteMM(as(C, \\"CsparseMatrix\\"), paste(args[3], \\"C\\", sep=\\"\\")); \\r\\n\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/binary/matrix_full_other/IntegerDivision_mod.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nA <- as.matrix(readMM(paste(args[1], \\"A.mtx\\", sep=\\"\\")))\\r\\nB <- as.matrix(readMM(paste(args[1], \\"B.mtx\\", sep=\\"\\")))\\r\\nif( nrow(A)==1 ){ #support for scalars        \\r\\n   A <- as.numeric(A);\\r\\n}\\r\\nif( nrow(B)==1 ){ #support for scalars\\r\\n   B <- as.numeric(B);\\r\\n}\\r\\nC <- A%%B;\\r\\n\\r\\n#note: writeMM replaces NaN and Inf\\r\\nwriteMM(as(C, \\"CsparseMatrix\\"), paste(args[2], \\"C\\", sep=\\"\\")); \\r\\n\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/parfor/parfor_optimizer2.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\n\\r\\nD1 <- readMM(paste(args[1], \\"D.mtx\\", sep=\\"\\"))\\r\\nS11 <- readMM(paste(args[1], \\"S1.mtx\\", sep=\\"\\"))\\r\\nS21 <- readMM(paste(args[1], \\"S2.mtx\\", sep=\\"\\"))\\r\\nK11 <- readMM(paste(args[1], \\"K1.mtx\\", sep=\\"\\"))\\r\\nK21 <- readMM(paste(args[1], \\"K2.mtx\\", sep=\\"\\"))\\r\\nD <- as.matrix(D1);\\r\\nS1 <- as.matrix(S11);\\r\\nS2 <- as.matrix(S21);\\r\\nK1 <- as.matrix(K11);\\r\\nK2 <- as.matrix(K21);\\r\\n\\r\\nnumPairs <- ncol(S1) * ncol(S2); # number of attribute pairs (|S1|*|S2|)\\r\\nmaxC <- args[2]; # max number of categories in any categorical attribute\\r\\n\\r\\ns1size <- ncol(S1);\\r\\ns2size <- ncol(S2);\\r\\n\\r\\n# R, chisq, cramers, spearman, eta, anovaf\\r\\nnumstats <- 8;\\r\\nbasestats <- array(0,dim=c(numstats,numPairs)); \\r\\ncat_counts <- array(0,dim=c(maxC,numPairs)); \\r\\ncat_means <- array(0,dim=c(maxC,numPairs));\\r\\ncat_vars <- array(0,dim=c(maxC,numPairs));\\r\\n\\r\\n\\r\\nfor( i in 1:s1size ) { \\r\\n    a1 <- S1[,i];\\r\\n    k1 <- K1[1,i];\\r\\n    A1 <- as.matrix(D[,a1]);\\r\\n\\r\\n    for( j in 1:s2size ) {\\r\\n        pairID <-(i-1)*s2size+j;\\r\\n        a2 <- S2[,j];\\r\\n        k2 <- K2[1,j];\\r\\n        A2 <- as.matrix(D[,a2]);\\r\\n    \\r\\n        if (k1 == k2) {\\r\\n            if (k1 == 1) {   \\r\\n                # scale-scale\\r\\n                print(\\"scale-scale\\");\\r\\n                basestats[1,pairID] <- cor(D[,a1], D[,a2]);\\r\\n                #basestats[1,pairID] <- cor(A1, A2);\\r\\n                \\r\\n                print(basestats[1,pairID]);\\r\\n            } else {\\r\\n                # nominal-nominal or ordinal-ordinal\\r\\n                print(\\"categorical-categorical\\");\\r\\n                F <- table(A1,A2);\\r\\n                cst <- chisq.test(F);\\r\\n                chi_squared <- as.numeric(cst[1]);\\r\\n                degFreedom <- (nrow(F)-1)*(ncol(F)-1);\\r\\n                pValue <- as.numeric(cst[3]);\\r\\n                q <- min(dim(F));\\r\\n                W <- sum(F);\\r\\n                cramers_v <- sqrt(chi_squared/(W*(q-1)));\\r\\n\\r\\n                basestats[2,pairID] <- chi_squared;\\r\\n                basestats[3,pairID] <- degFreedom;\\r\\n                basestats[4,pairID] <- pValue;\\r\\n                basestats[5,pairID] <- cramers_v;\\r\\n\\r\\n                if ( k1 == 3 ) {\\r\\n                    # ordinal-ordinal   \\r\\n                    print(\\"ordinal-ordinal\\");\\r\\n                    basestats[6,pairID] <- cor(A1,A2, method=\\"spearman\\");\\r\\n                }\\r\\n            }\\r\\n        } \\r\\n        else {       \\r\\n            if (k1 == 1 || k2 == 1) {    \\r\\n                # Scale-nominal/ordinal\\r\\n                print(\\"scale-categorical\\");\\r\\n                if ( k1 == 1 ) {\\r\\n                    Av <- as.matrix(A2); \\r\\n                    Yv <- as.matrix(A1); \\r\\n                }\\r\\n                else {\\r\\n                    Av <- as.matrix(A1); \\r\\n                    Yv <- as.matrix(A2); \\r\\n                }\\r\\n                \\r\\n                W <- nrow(Av);\\r\\n                my <- mean(Yv); \\r\\n                varY <- var(Yv);\\r\\n                \\r\\n                CFreqs <- as.matrix(table(Av)); \\r\\n                CMeans <- as.matrix(aggregate(Yv, by=list(Av), \\"mean\\")$V1);\\r\\n                CVars <- as.matrix(aggregate(Yv, by=list(Av), \\"var\\")$V1);\\r\\n                R <- nrow(CFreqs);\\r\\n              \\r\\n                Eta <- sqrt(1 - ( sum((CFreqs-1)*CVars) / ((W-1)*varY) ));\\r\\n                anova_num <- sum( (CFreqs*(CMeans-my)^2) )/(R-1);\\r\\n                anova_den <- sum( (CFreqs-1)*CVars )/(W-R);\\r\\n                ANOVAF <- anova_num/anova_den;\\r\\n\\r\\n                basestats[7,pairID] <- Eta;\\r\\n                basestats[8,pairID] <- ANOVAF;\\r\\n\\r\\n                cat_counts[ 1:length(CFreqs),pairID] <- CFreqs;\\r\\n                cat_means[ 1:length(CMeans),pairID] <- CMeans;\\r\\n                cat_vars[ 1:length(CVars),pairID] <- CVars;\\r\\n            }\\r\\n            else {\\r\\n                # nominal-ordinal or ordinal-nominal    \\r\\n                print(\\"nomial-ordinal\\"); #TODO should not be same code            \\r\\n                F <- table(A1,A2);\\r\\n                cst <- chisq.test(F);\\r\\n                chi_squared <- as.numeric(cst[1]);\\r\\n                degFreedom <- (nrow(F)-1)*(ncol(F)-1);\\r\\n                pValue <- as.numeric(cst[3]);\\r\\n                q <- min(dim(F));\\r\\n                W <- sum(F);\\r\\n                cramers_v <- sqrt(chi_squared/(W*(q-1)));\\r\\n                \\r\\n                basestats[2,pairID] <- chi_squared;\\r\\n                basestats[3,pairID] <- degFreedom;\\r\\n                basestats[4,pairID] <- pValue;\\r\\n                basestats[5,pairID] <- cramers_v;\\r\\n            }\\r\\n        }\\r\\n    }\\r\\n}\\r\\n\\r\\nwriteMM(as(basestats, \\"CsparseMatrix\\"), paste(args[3], \\"bivar.stats\\", sep=\\"\\"));\\r\\nwriteMM(as(cat_counts, \\"CsparseMatrix\\"), paste(args[3], \\"category.counts\\", sep=\\"\\"));\\r\\nwriteMM(as(cat_means, \\"CsparseMatrix\\"), paste(args[3], \\"category.means\\", sep=\\"\\"));\\r\\nwriteMM(as(cat_vars, \\"CsparseMatrix\\"), paste(args[3], \\"category.variances\\", sep=\\"\\"));\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/unary/matrix/Floor.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nA <- as.matrix(readMM(paste(args[1], \\"math.mtx\\", sep=\\"\\")))\\r\\n\\r\\nR = floor(A);\\r\\n\\r\\nwriteMM(as(R, \\"CsparseMatrix\\"), paste(args[2], \\"R\\", sep=\\"\\")); \\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/binary/matrix/UltraSparseMatrixMultiplication.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nA <- as.matrix(readMM(paste(args[1], \\"A.mtx\\", sep=\\"\\")))\\r\\nB <- as.matrix(readMM(paste(args[1], \\"B.mtx\\", sep=\\"\\")))\\r\\n\\r\\nP <- diag( as.vector(B==2) )\\r\\nPx <- P[rowSums((P==0) | is.na(P)) != ncol(P),];\\r\\n\\r\\nC <- Px %*% A;\\r\\n\\r\\nwriteMM(as(C, \\"CsparseMatrix\\"), paste(args[2], \\"C\\", sep=\\"\\")); \\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/applications/descriptivestats/ScaleCategoricalWithWeightsTest.R", "content": "#-------------------------------------------------------------\\r\\n#\\r\\n# (C) Copyright IBM Corp. 2010, 2015\\r\\n#\\r\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\r\\n# you may not use this file except in compliance with the License.\\r\\n# You may obtain a copy of the License at\\r\\n#\\r\\n#     http://www.apache.org/licenses/LICENSE-2.0\\r\\n#\\r\\n# Unless required by applicable law or agreed to in writing, software\\r\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\r\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\r\\n# See the License for the specific language governing permissions and\\r\\n# limitations under the License.\\r\\n#\\r\\n#-------------------------------------------------------------\\r\\n\\r\\n# JUnit test class: dml.test.integration.descriptivestats.BivariateScaleCategoricalTest.java\\r\\n# command line invocation assuming $SC_HOME is set to the home of the R script\\r\\n# Rscript $SC_HOME/ScaleCategorical.R $SC_HOME/in/ $SC_HOME/expected/\\r\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n# Usage: R --vanilla -args Xfile X < ScaleCategoricalTest.R\\r\\n\\r\\n#parseCommandArgs()\\r\\n######################\\r\\nAtemp = readMM(paste(args[1], \\"A.mtx\\", sep=\\"\\"));\\r\\nYtemp = readMM(paste(args[1], \\"Y.mtx\\", sep=\\"\\"));\\r\\nWM = readMM(paste(args[1], \\"WM.mtx\\", sep=\\"\\"));\\r\\n\\r\\nYv=rep(Ytemp[,1],WM[,1])\\r\\nAv=rep(Atemp[,1],WM[,1])\\r\\n\\r\\nW = sum(WM);\\r\\nmy = sum(Yv)/W;\\r\\nvarY = var(Yv);\\r\\n\\r\\nCFreqs = as.matrix(table(Av)); \\r\\nCMeans = as.matrix(aggregate(Yv, by=list(Av), \\"mean\\")$x);\\r\\nCVars = as.matrix(aggregate(Yv, by=list(Av), \\"var\\")$x);\\r\\n\\r\\n# number of categories\\r\\nR = nrow(CFreqs);\\r\\n\\r\\nEta = sqrt(1 - ( sum((CFreqs-1)*CVars) / ((W-1)*varY) ));\\r\\n\\r\\nanova_num = sum( (CFreqs*(CMeans-my)^2) )/(R-1);\\r\\nanova_den = sum( (CFreqs-1)*CVars )/(W-R);\\r\\nANOVAF = anova_num/anova_den;\\r\\n\\r\\nprint(W, digits=15);\\r\\nprint(R, digits=15);\\r\\nprint(anova_num, digits=15);\\r\\nprint(anova_den, digits=15);\\r\\n\\r\\n#######################\\r\\n\\r\\nwrite(Eta, paste(args[2], \\"Eta\\", sep=\\"\\"));\\r\\n\\r\\nwrite(ANOVAF, paste(args[2], \\"AnovaF\\", sep=\\"\\"));\\r\\n\\r\\nwrite(varY, paste(args[2], \\"VarY\\", sep=\\"\\"));\\r\\n\\r\\nwrite(my, paste(args[2], \\"MeanY\\", sep=\\"\\"));\\r\\n\\r\\nwriteMM(as(CVars,\\"CsparseMatrix\\"), paste(args[2], \\"CVars\\", sep=\\"\\"), format=\\"text\\");\\r\\nwriteMM(as(CFreqs,\\"CsparseMatrix\\"), paste(args[2], \\"CFreqs\\", sep=\\"\\"), format=\\"text\\");\\r\\nwriteMM(as(CMeans,\\"CsparseMatrix\\"), paste(args[2], \\"CMeans\\", sep=\\"\\"), format=\\"text\\");\\r\\n\\r\\n\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/quaternary/WeightedSigmoidP4.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nX = as.matrix(readMM(paste(args[1], \\"X.mtx\\", sep=\\"\\")))\\r\\nU = as.matrix(readMM(paste(args[1], \\"U.mtx\\", sep=\\"\\")))\\r\\nV = as.matrix(readMM(paste(args[1], \\"V.mtx\\", sep=\\"\\")))\\r\\n\\r\\nUV = -(U%*%t(V));\\r\\nR = X * log(1/(1 + exp(-UV)));\\r\\n\\r\\nwriteMM(as(R, \\"CsparseMatrix\\"), paste(args[2], \\"R\\", sep=\\"\\")); \\r\\n\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/binary/matrix/UaggOuterChainNotEqualsRowIndexMin.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\nargs <- commandArgs(TRUE)\\noptions(digits=22)\\n\\nlibrary(\\"Matrix\\")\\n\\nA <- as.vector(readMM(paste(args[1], \\"A.mtx\\", sep=\\"\\")))\\nB <- as.vector(readMM(paste(args[1], \\"B.mtx\\", sep=\\"\\")))\\n\\nI <- as.matrix(outer(A,B,\\"!=\\"));\\nC <- max.col(-I,ties.method=\\"first\\");\\n\\nwriteMM(as(C, \\"CsparseMatrix\\"), paste(args[2], \\"C\\", sep=\\"\\"));" }\n'
line: b'{"repo_name":"octacoin-project/beta","ref":"refs/heads/master","path":"share/qt/extract_strings_qt.py","content":"#!/usr/bin/python\\n\'\'\'\\nExtract _(\\"...\\") strings for translation and convert to Qt4 stringdefs so that\\nthey can be picked up by Qt linguist.\\n\'\'\'\\nfrom subprocess import Popen, PIPE\\nimport glob\\nimport operator\\nimport os\\nimport sys\\n\\nOUT_CPP=\\"qt/bitcoinstrings.cpp\\"\\nEMPTY=[\'\\"\\"\']\\n\\ndef parse_po(text):\\n    \\"\\"\\"\\n    Parse \'po\' format produced by xgettext.\\n    Return a list of (msgid,msgstr) tuples.\\n    \\"\\"\\"\\n    messages = []\\n    msgid = []\\n    msgstr = []\\n    in_msgid = False\\n    in_msgstr = False\\n\\n    for line in text.split(\'\\\\n\'):\\n        line = line.rstrip(\'\\\\r\')\\n        if line.startswith(\'msgid \'):\\n            if in_msgstr:\\n                messages.append((msgid, msgstr))\\n                in_msgstr = False\\n            # message start\\n            in_msgid = True\\n            \\n            msgid = [line[6:]]\\n        elif line.startswith(\'msgstr \'):\\n            in_msgid = False\\n            in_msgstr = True\\n            msgstr = [line[7:]]\\n        elif line.startswith(\'\\"\'):\\n            if in_msgid:\\n                msgid.append(line)\\n            if in_msgstr:\\n                msgstr.append(line)\\n\\n    if in_msgstr:\\n        messages.append((msgid, msgstr))\\n\\n    return messages\\n\\nfiles = sys.argv[1:]\\n\\n# xgettext -n --keyword=_ $FILES\\nXGETTEXT=os.getenv(\'XGETTEXT\', \'xgettext\')\\nchild = Popen([XGETTEXT,\'--output=-\',\'-n\',\'--keyword=_\'] + files, stdout=PIPE)\\n(out, err) = child.communicate()\\n\\nmessages = parse_po(out) \\n\\nf = open(OUT_CPP, \'w\')\\nf.write(\\"\\"\\"\\n\\n#include \\u003cQtGlobal\\u003e\\n\\n// Automatically generated by extract_strings.py\\n#ifdef __GNUC__\\n#define UNUSED __attribute__((unused))\\n#else\\n#define UNUSED\\n#endif\\n\\"\\"\\")\\nf.write(\'static const char UNUSED *bitcoin_strings[] = {\\\\n\')\\nmessages.sort(key=operator.itemgetter(0))\\nfor (msgid, msgstr) in messages:\\n    if msgid != EMPTY:\\n        f.write(\'QT_TRANSLATE_NOOP(\\"bitcoin-core\\", %s),\\\\n\' % (\'\\\\n\'.join(msgid)))\\nf.write(\'};\\\\n\')\\nf.close()\\n"}\n'
line: b'{"repo_name":"FHannes/intellij-community","ref":"refs/heads/master","path":"python/testData/intentions/returnTypeInPy3Annotation2_after.py","content":"def my_func(p1=1) -\\u003e object:\\n    return p1\\n\\nd = my_func(1)"}\n'
line: b'{"repo_name":"BhallaLab/moose","ref":"refs/heads/master","path":"moose-examples/passive/passive_soma.py","content":"\\"\\"\\" passive_soma.py: \\n\\nIn this script, we simulate a single compartment soma in MOOSE.\\n\\nThis soma does not have any ion-channels, only passive properties. It should\\nbehave like a RC circuit. A current is injected into soma.\\n\\n\\"\\"\\"\\n    \\n__author__           = \\"Dilawar Singh\\"\\n__copyright__        = \\"Copyright 2015, Dilawar Singh and NCBS Bangalore\\"\\n__credits__          = [\\"NCBS Bangalore\\"]\\n__license__          = \\"GNU GPL\\"\\n__version__          = \\"1.0.0\\"\\n__maintainer__       = \\"Dilawar Singh\\"\\n__email__            = \\"dilawars@ncbs.res.in\\"\\n__status__           = \\"Development\\"\\n\\n   \\nimport moose\\nimport pylab\\n\\n\\nmodel = None\\nsoma = None \\nvmtab = None\\n\\ndef buildModel():\\n    global model \\n    global soma\\n    model = moose.Neutral(\'/model\')\\n    soma = moose.Compartment(\'/model/soma\')\\n    soma.Em = -60e-3\\n    soma.Rm = 1e10\\n    soma.Cm = 1e-10\\n    return model\\n\\ndef stimulus():\\n    global soma\\n    global vmtab\\n    pulse = moose.PulseGen(\'/model/pulse\')\\n    pulse.delay[0] = 50e-3\\n    pulse.width[0] = 100e-3\\n    pulse.level[0] = 1e-9\\n    pulse.delay[1] = 1e9\\n    vmtab = moose.Table(\'/soma_Vm\')\\n    moose.connect(pulse, \'output\', soma, \'injectMsg\')\\n    moose.connect(vmtab, \'requestOut\', soma , \'getVm\')\\n\\ndef main():\\n    global vmtab\\n    buildModel()\\n    stimulus()\\n    moose.reinit()\\n    t = 500e-2\\n    moose.start(t)\\n    time_vector = pylab.linspace(0, t, len(vmtab.vector))\\n    pylab.plot(time_vector, vmtab.vector)\\n    pylab.show( )\\n    # pylab.savefig(\'soma_passive.png\')\\n\\nif __name__ == \'__main__\':\\n    main()\\n"}\n'
line: b'{"repo_name":"tommy-u/chaco","ref":"refs/heads/master","path":"chaco/base_plot_container.py","content":"\\"\\"\\" Defines the BasePlotContainer class.\\n\\"\\"\\"\\nimport warnings\\n\\n# Enthought library imports\\nfrom enable.api import Container\\nfrom traits.api import Bool, Instance, Property, Str, Tuple\\n\\n# Local, relative imports\\nfrom plot_component import DEFAULT_DRAWING_ORDER, PlotComponent\\n\\n\\nclass BasePlotContainer(Container):\\n    \\"\\"\\"\\n    A container for PlotComponents that conforms to being laid out by\\n    PlotFrames.  Serves as the base class for other PlotContainers.\\n\\n    PlotContainers define a layout, i.e., a spatial relationship between\\n    their contained components.  (BasePlotContainer doesn\'t define one,\\n    but its various subclasses do.)\\n\\n    BasePlotContainer is a subclass of Enable Container, so it is possible to\\n    insert Enable-level components into it.  However, because Enable\\n    components don\'t have the correct interfaces to participate in layout,\\n    the visual results will probably be incorrect.\\n    \\"\\"\\"\\n\\n    # Redefine the container layers to name the main layer as \\"plot\\" instead\\n    # of the Enable default of \\"mainlayer\\"\\n    container_under_layers = Tuple(\\"background\\", \\"image\\", \\"underlay\\", \\"plot\\")\\n\\n    #------------------------------------------------------------------------\\n    # Duplicate trait declarations from PlotComponent.  We don\'t subclass\\n    # PlotComponent to avoid MRO complications with trait handlers and property\\n    # getters/setters.\\n    #------------------------------------------------------------------------\\n\\n    draw_order = Instance(list, args=(DEFAULT_DRAWING_ORDER,))\\n    draw_layer = Str(\\"plot\\")\\n\\n    #------------------------------------------------------------------------\\n    # Deprecated traits\\n    #------------------------------------------------------------------------\\n\\n    # Deprecated flag to indicate that a component needed to do old-style\\n    # drawing.  Unused by any recent Chaco component.\\n    use_draw_order = Bool(True)\\n\\n    # Deprecated property for accessing the components in the container.\\n    plot_components = Property\\n\\n    def _get_plot_components(self):\\n        warnings.warn(\\"Use of plot_components attribute deprecated.\\" \\\\\\n                      \\"Use components attribute instead.\\", DeprecationWarning)\\n        return self._components\\n\\n    def _set_plot_components(self, new):\\n        warnings.warn(\\"Use of plot_components attribute deprecated.\\" \\\\\\n                      \\"Use components attribute instead.\\", DeprecationWarning)\\n        self._components = new\\n\\n    def _use_draw_order_changed(self, old, new):\\n        \\"\\"\\" Handler to catch the case when someone is trying to use the\\n        old-style drawing mechanism, which is now unsupported.\\n        \\"\\"\\"\\n        if new == False:\\n            raise RuntimeError(\\"The old-style drawing mechanism is no longer \\" \\\\\\n                    \\"supported in Chaco.\\")\\n\\n# EOF\\n"}\n'
line: b'{"repo_name":"factorybuild/stbgui","ref":"refs/heads/master","path":"lib/python/Screens/ChannelSelection.py","content":"from Tools.Profile import profile\\n\\nfrom Screen import Screen\\nimport Screens.InfoBar\\nimport Components.ParentalControl\\nfrom Components.Button import Button\\nfrom Components.ServiceList import ServiceList, refreshServiceList\\nfrom Components.ActionMap import NumberActionMap, ActionMap, HelpableActionMap\\nfrom Components.MenuList import MenuList\\nfrom Components.ServiceEventTracker import ServiceEventTracker, InfoBarBase\\nprofile(\\"ChannelSelection.py 1\\")\\nfrom EpgSelection import EPGSelection\\nfrom enigma import eServiceReference, eEPGCache, eServiceCenter, eRCInput, eTimer, eDVBDB, iPlayableService, iServiceInformation, getPrevAsciiCode, eEnv\\nfrom Components.config import config, configfile, ConfigSubsection, ConfigText, ConfigYesNo\\nfrom Tools.NumericalTextInput import NumericalTextInput\\nprofile(\\"ChannelSelection.py 2\\")\\nfrom Components.NimManager import nimmanager\\nprofile(\\"ChannelSelection.py 2.1\\")\\nfrom Components.Sources.RdsDecoder import RdsDecoder\\nprofile(\\"ChannelSelection.py 2.2\\")\\nfrom Components.Sources.ServiceEvent import ServiceEvent\\nfrom Components.Sources.Event import Event\\nprofile(\\"ChannelSelection.py 2.3\\")\\nfrom Components.Input import Input\\nprofile(\\"ChannelSelection.py 3\\")\\nfrom Components.ChoiceList import ChoiceList, ChoiceEntryComponent\\nfrom Components.SystemInfo import SystemInfo\\nfrom Screens.InputBox import PinInput\\nfrom Screens.VirtualKeyBoard import VirtualKeyBoard\\nfrom Screens.MessageBox import MessageBox\\nfrom Screens.ServiceInfo import ServiceInfo\\nfrom Screens.Hotkey import InfoBarHotkey, hotkeyActionMap, getHotkeyFunctions\\nprofile(\\"ChannelSelection.py 4\\")\\nfrom Screens.PictureInPicture import PictureInPicture\\nfrom Screens.RdsDisplay import RassInteractive\\nfrom ServiceReference import ServiceReference\\nfrom Tools.BoundFunction import boundFunction\\nfrom Tools import Notifications\\nfrom Tools.Alternatives import CompareWithAlternatives, GetWithAlternative\\nfrom Tools.Directories import fileExists\\nfrom Plugins.Plugin import PluginDescriptor\\nfrom Components.PluginComponent import plugins\\nfrom Screens.ChoiceBox import ChoiceBox\\nfrom Screens.EventView import EventViewEPGSelect\\nimport os, unicodedata\\nprofile(\\"ChannelSelection.py after imports\\")\\n\\nFLAG_SERVICE_NEW_FOUND = 64\\nFLAG_IS_DEDICATED_3D = 128\\nFLAG_HIDE_VBI = 512 #define in lib/dvb/idvb.h as dxNewFound = 64 and dxIsDedicated3D = 128\\n\\nclass BouquetSelector(Screen):\\n\\tdef __init__(self, session, bouquets, selectedFunc, enableWrapAround=True):\\n\\t\\tScreen.__init__(self, session)\\n\\t\\tself.setTitle(_(\\"Choose bouquet\\"))\\n\\n\\t\\tself.selectedFunc=selectedFunc\\n\\n\\t\\tself[\\"actions\\"] = ActionMap([\\"OkCancelActions\\"],\\n\\t\\t\\t{\\n\\t\\t\\t\\t\\"ok\\": self.okbuttonClick,\\n\\t\\t\\t\\t\\"cancel\\": self.cancelClick\\n\\t\\t\\t})\\n\\t\\tentrys = [ (x[0], x[1]) for x in bouquets ]\\n\\t\\tself[\\"menu\\"] = MenuList(entrys, enableWrapAround)\\n\\n\\tdef getCurrent(self):\\n\\t\\tcur = self[\\"menu\\"].getCurrent()\\n\\t\\treturn cur and cur[1]\\n\\n\\tdef okbuttonClick(self):\\n\\t\\tself.selectedFunc(self.getCurrent())\\n\\n\\tdef up(self):\\n\\t\\tself[\\"menu\\"].up()\\n\\n\\tdef down(self):\\n\\t\\tself[\\"menu\\"].down()\\n\\n\\tdef cancelClick(self):\\n\\t\\tself.close(False)\\n\\nclass SilentBouquetSelector:\\n\\tdef __init__(self, bouquets, enableWrapAround=False, current=0):\\n\\t\\tself.bouquets = [b[1] for b in bouquets]\\n\\t\\tself.pos = current\\n\\t\\tself.count = len(bouquets)\\n\\t\\tself.enableWrapAround = enableWrapAround\\n\\n\\tdef up(self):\\n\\t\\tif self.pos \\u003e 0 or self.enableWrapAround:\\n\\t\\t\\tself.pos = (self.pos - 1) % self.count\\n\\n\\tdef down(self):\\n\\t\\tif self.pos \\u003c (self.count - 1) or self.enableWrapAround:\\n\\t\\t\\tself.pos = (self.pos + 1) % self.count\\n\\n\\tdef getCurrent(self):\\n\\t\\treturn self.bouquets[self.pos]\\n\\n# csel.bouquet_mark_edit values\\nOFF = 0\\nEDIT_BOUQUET = 1\\nEDIT_ALTERNATIVES = 2\\n\\ndef append_when_current_valid(current, menu, args, level=0, key=\\"\\"):\\n\\tif current and current.valid() and level \\u003c= config.usage.setup_level.index:\\n\\t\\tmenu.append(ChoiceEntryComponent(key, args))\\n\\ndef removed_userbouquets_available():\\n\\tfor file in os.listdir(\\"/etc/enigma2/\\"):\\n\\t\\tif file.startswith(\\"userbouquet\\") and file.endswith(\\".del\\"):\\n\\t\\t\\treturn True\\n\\treturn False\\n\\nclass ChannelContextMenu(Screen):\\n\\tdef __init__(self, session, csel):\\n\\n\\t\\tScreen.__init__(self, session)\\n\\t\\tself.csel = csel\\n\\t\\tself.bsel = None\\n\\t\\tif self.isProtected():\\n\\t\\t\\tself.onFirstExecBegin.append(boundFunction(self.session.openWithCallback, self.protectResult, PinInput, pinList=[x.value for x in config.ParentalControl.servicepin], triesEntry=config.ParentalControl.retries.servicepin, title=_(\\"Please enter the correct pin code\\"), windowTitle=_(\\"Enter pin code\\")))\\n\\n\\t\\tself[\\"actions\\"] = ActionMap([\\"OkCancelActions\\", \\"ColorActions\\", \\"NumberActions\\", \\"MenuActions\\"],\\n\\t\\t\\t{\\n\\t\\t\\t\\t\\"ok\\": self.okbuttonClick,\\n\\t\\t\\t\\t\\"cancel\\": self.cancelClick,\\n\\t\\t\\t\\t\\"blue\\": self.showServiceInPiP,\\n\\t\\t\\t\\t\\"red\\": self.playMain,\\n\\t\\t\\t\\t\\"menu\\": self.openSetup,\\n\\t\\t\\t\\t\\"2\\": self.renameEntry,\\n\\t\\t\\t\\t\\"3\\": self.findCurrentlyPlayed,\\n\\t\\t\\t\\t\\"5\\": self.addServiceToBouquetOrAlternative,\\n\\t\\t\\t\\t\\"6\\": self.toggleMoveModeSelect,\\n\\t\\t\\t\\t\\"8\\": self.removeEntry\\n\\t\\t\\t})\\n\\t\\tmenu = [ ]\\n\\n\\t\\tself.removeFunction = False\\n\\t\\tself.addFunction = False\\n\\t\\tcurrent = csel.getCurrentSelection()\\n\\t\\tcurrent_root = csel.getRoot()\\n\\t\\tcurrent_sel_path = current.getPath()\\n\\t\\tcurrent_sel_flags = current.flags\\n\\t\\tinBouquetRootList = current_root and \'FROM BOUQUET \\"bouquets.\' in current_root.getPath() #FIXME HACK\\n\\t\\tinAlternativeList = current_root and \'FROM BOUQUET \\"alternatives\' in current_root.getPath()\\n\\t\\tself.inBouquet = csel.getMutableList() is not None\\n\\t\\thaveBouquets = config.usage.multibouquet.value\\n\\t\\tfrom Components.ParentalControl import parentalControl\\n\\t\\tself.parentalControl = parentalControl\\n\\t\\tself.parentalControlEnabled = config.ParentalControl.servicepin[0].value and config.ParentalControl.servicepinactive.value\\n\\t\\tif not (current_sel_path or current_sel_flags \\u0026 (eServiceReference.isDirectory|eServiceReference.isMarker)) or current_sel_flags \\u0026 eServiceReference.isGroup:\\n\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"show transponder info\\"), self.showServiceInformations), level=2)\\n\\t\\tif csel.bouquet_mark_edit == OFF and not csel.entry_marked:\\n\\t\\t\\tif not inBouquetRootList:\\n\\t\\t\\t\\tisPlayable = not (current_sel_flags \\u0026 (eServiceReference.isMarker|eServiceReference.isDirectory))\\n\\t\\t\\t\\tif isPlayable:\\n\\t\\t\\t\\t\\tfor p in plugins.getPlugins(PluginDescriptor.WHERE_CHANNEL_CONTEXT_MENU):\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (p.name, boundFunction(self.runPlugin, p)), key=\\"bullet\\")\\n\\t\\t\\t\\t\\tif config.servicelist.startupservice.value == current.toString():\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"stop using as startup service\\"), self.unsetStartupService), level=0)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"set as startup service\\"), self.setStartupService), level=0)\\n\\t\\t\\t\\t\\tif self.parentalControlEnabled:\\n\\t\\t\\t\\t\\t\\tif self.parentalControl.getProtectionLevel(current.toCompareString()) == -1:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"add to parental protection\\"), boundFunction(self.addParentalProtection, current)), level=0)\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tif self.parentalControl.isServiceProtectionBouquet(current.toCompareString()):\\n\\t\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"service is in bouquet parental protection\\"), self.cancelClick), level=0)\\n\\t\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove from parental protection\\"), boundFunction(self.removeParentalProtection, current)), level=0)\\n\\t\\t\\t\\t\\t\\tif config.ParentalControl.hideBlacklist.value and not parentalControl.sessionPinCached and config.ParentalControl.storeservicepin.value != \\"never\\":\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"Unhide parental control services\\"), self.unhideParentalServices), level=0, key=\\"1\\")\\n\\t\\t\\t\\t\\tif SystemInfo[\\"3DMode\\"] and  fileExists(\\"/usr/lib/enigma2/python/Plugins/SystemPlugins/OSD3DSetup/plugin.py\\"):\\n\\t\\t\\t\\t\\t\\tif eDVBDB.getInstance().getFlag(eServiceReference(current.toString())) \\u0026 FLAG_IS_DEDICATED_3D:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"Unmark service as dedicated 3D service\\"), self.removeDedicated3DFlag), level=0)\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"Mark service as dedicated 3D service\\"), self.addDedicated3DFlag), level=0)\\n\\t\\t\\t\\t\\tif not (current_sel_path):\\n\\t\\t\\t\\t\\t\\tif eDVBDB.getInstance().getFlag(eServiceReference(current.toString())) \\u0026 FLAG_HIDE_VBI:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"Uncover dashed flickering line for this service\\"), self.removeHideVBIFlag), level=0)\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"Cover dashed flickering line for this service\\"), self.addHideVBIFlag), level=0)\\n\\t\\t\\t\\t\\tif haveBouquets:\\n\\t\\t\\t\\t\\t\\tbouquets = self.csel.getBouquetList()\\n\\t\\t\\t\\t\\t\\tif bouquets is None:\\n\\t\\t\\t\\t\\t\\t\\tbouquetCnt = 0\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tbouquetCnt = len(bouquets)\\n\\t\\t\\t\\t\\t\\tif not self.inBouquet or bouquetCnt \\u003e 1:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"add service to bouquet\\"), self.addServiceToBouquetSelected), level=0, key=\\"5\\")\\n\\t\\t\\t\\t\\t\\t\\tself.addFunction = self.addServiceToBouquetSelected\\n\\t\\t\\t\\t\\t\\tif not self.inBouquet:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove entry\\"), self.removeEntry), level = 0, key=\\"8\\")\\n\\t\\t\\t\\t\\t\\t\\tself.removeFunction = self.removeSatelliteService\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tif not self.inBouquet:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"add service to favourites\\"), self.addServiceToBouquetSelected), level=0, key=\\"5\\")\\n\\t\\t\\t\\t\\t\\t\\tself.addFunction = self.addServiceToBouquetSelected\\n\\t\\t\\t\\t\\tif SystemInfo[\\"PIPAvailable\\"]:\\n\\t\\t\\t\\t\\t\\tif not self.parentalControlEnabled or self.parentalControl.getProtectionLevel(current.toCompareString()) == -1:\\n\\t\\t\\t\\t\\t\\t\\tif self.csel.dopipzap:\\n\\t\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"play in mainwindow\\"), self.playMain), level=0, key=\\"red\\")\\n\\t\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"play as picture in picture\\"), self.showServiceInPiP), level=0, key=\\"blue\\")\\n\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"find currently played service\\"), self.findCurrentlyPlayed), level=0, key=\\"3\\")\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tif \'FROM SATELLITES\' in current_root.getPath() and current and _(\\"Services\\") in eServiceCenter.getInstance().info(current).getName(current):\\n\\t\\t\\t\\t\\t\\tunsigned_orbpos = current.getUnsignedData(4) \\u003e\\u003e 16\\n\\t\\t\\t\\t\\t\\tif unsigned_orbpos == 0xFFFF:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove cable services\\"), self.removeSatelliteServices), level = 0)\\n\\t\\t\\t\\t\\t\\telif unsigned_orbpos == 0xEEEE:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove terrestrial services\\"), self.removeSatelliteServices), level = 0)\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove selected satellite\\"), self.removeSatelliteServices), level = 0)\\n\\t\\t\\t\\t\\tif haveBouquets:\\n\\t\\t\\t\\t\\t\\tif not self.inBouquet and not \\"PROVIDERS\\" in current_sel_path:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"copy to bouquets\\"), self.copyCurrentToBouquetList), level=0)\\n\\t\\t\\t\\t\\tif (\\"flags == %d\\" %(FLAG_SERVICE_NEW_FOUND)) in current_sel_path:\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove all new found flags\\"), self.removeAllNewFoundFlags), level=0)\\n\\t\\t\\t\\tif self.inBouquet:\\n\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"rename entry\\"), self.renameEntry), level=0, key=\\"2\\")\\n\\t\\t\\t\\t\\tif not inAlternativeList:\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove entry\\"), self.removeEntry), level=0, key=\\"8\\")\\n\\t\\t\\t\\t\\t\\tself.removeFunction = self.removeCurrentService\\n\\t\\t\\t\\tif current_root and (\\"flags == %d\\" %(FLAG_SERVICE_NEW_FOUND)) in current_root.getPath():\\n\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove new found flag\\"), self.removeNewFoundFlag), level=0)\\n\\t\\t\\telse:\\n\\t\\t\\t\\t\\tif self.parentalControlEnabled:\\n\\t\\t\\t\\t\\t\\tif self.parentalControl.getProtectionLevel(current.toCompareString()) == -1:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"add bouquet to parental protection\\"), boundFunction(self.addParentalProtection, current)), level=0)\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove bouquet from parental protection\\"), boundFunction(self.removeParentalProtection, current)), level=0)\\n\\t\\t\\t\\t\\tmenu.append(ChoiceEntryComponent(text=(_(\\"add bouquet\\"), self.showBouquetInputBox)))\\n\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"rename entry\\"), self.renameEntry), level=0, key=\\"2\\")\\n\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove entry\\"), self.removeEntry), level=0, key=\\"8\\")\\n\\t\\t\\t\\t\\tself.removeFunction = self.removeBouquet\\n\\t\\t\\t\\t\\tif removed_userbouquets_available():\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"purge deleted userbouquets\\"), self.purgeDeletedBouquets), level=0)\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"restore deleted userbouquets\\"), self.restoreDeletedBouquets), level=0)\\n\\t\\tif self.inBouquet: # current list is editable?\\n\\t\\t\\tif csel.bouquet_mark_edit == OFF:\\n\\t\\t\\t\\tif csel.movemode:\\n\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"disable move mode\\"), self.toggleMoveMode), level=0, key=\\"6\\")\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"enable move mode\\"), self.toggleMoveMode), level=1, key=\\"6\\")\\n\\t\\t\\t\\tif not csel.entry_marked and not inBouquetRootList and current_root and not (current_root.flags \\u0026 eServiceReference.isGroup):\\n\\t\\t\\t\\t\\tif current.type != -1:\\n\\t\\t\\t\\t\\t\\tmenu.append(ChoiceEntryComponent(text=(_(\\"add marker\\"), self.showMarkerInputBox)))\\n\\t\\t\\t\\t\\tif not csel.movemode:\\n\\t\\t\\t\\t\\t\\tif haveBouquets:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"enable bouquet edit\\"), self.bouquetMarkStart), level=0)\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"enable favourite edit\\"), self.bouquetMarkStart), level=0)\\n\\t\\t\\t\\t\\tif current_sel_flags \\u0026 eServiceReference.isGroup:\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"edit alternatives\\"), self.editAlternativeServices), level=2)\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"show alternatives\\"), self.showAlternativeServices), level=2)\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove all alternatives\\"), self.removeAlternativeServices), level=2)\\n\\t\\t\\t\\t\\telif not current_sel_flags \\u0026 eServiceReference.isMarker:\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"add alternatives\\"), self.addAlternativeServices), level=2)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tif csel.bouquet_mark_edit == EDIT_BOUQUET:\\n\\t\\t\\t\\t\\tif haveBouquets:\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"end bouquet edit\\"), self.bouquetMarkEnd), level=0)\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"abort bouquet edit\\"), self.bouquetMarkAbort), level=0)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"end favourites edit\\"), self.bouquetMarkEnd), level=0)\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"abort favourites edit\\"), self.bouquetMarkAbort), level=0)\\n\\t\\t\\t\\t\\tif current_sel_flags \\u0026 eServiceReference.isMarker:\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"rename entry\\"), self.renameEntry), level=0, key=\\"2\\")\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove entry\\"), self.removeEntry), level=0, key=\\"8\\")\\n\\t\\t\\t\\t\\t\\tself.removeFunction = self.removeCurrentService\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"end alternatives edit\\"), self.bouquetMarkEnd), level=0)\\n\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"abort alternatives edit\\"), self.bouquetMarkAbort), level=0)\\n\\n\\t\\tmenu.append(ChoiceEntryComponent(\\"menu\\", (_(\\"Configuration...\\"), self.openSetup)))\\n\\t\\tself[\\"menu\\"] = ChoiceList(menu)\\n\\n\\tdef set3DMode(self, value):\\n\\t\\tplayingref = self.session.nav.getCurrentlyPlayingServiceReference()\\n\\t\\tif config.plugins.OSD3DSetup.mode.value == \\"auto\\" and (playingref and playingref == self.csel.getCurrentSelection()):\\n\\t\\t\\tfrom Plugins.SystemPlugins.OSD3DSetup.plugin import applySettings\\n\\t\\t\\tapplySettings(value and \\"sidebyside\\" or config.plugins.OSD3DSetup.mode.value)\\n\\n\\tdef addDedicated3DFlag(self):\\n\\t\\teDVBDB.getInstance().addFlag(eServiceReference(self.csel.getCurrentSelection().toString()), FLAG_IS_DEDICATED_3D)\\n\\t\\teDVBDB.getInstance().reloadBouquets()\\n\\t\\tself.set3DMode(True)\\n\\t\\tself.close()\\n\\n\\tdef removeDedicated3DFlag(self):\\n\\t\\teDVBDB.getInstance().removeFlag(eServiceReference(self.csel.getCurrentSelection().toString()), FLAG_IS_DEDICATED_3D)\\n\\t\\teDVBDB.getInstance().reloadBouquets()\\n\\t\\tself.set3DMode(False)\\n\\t\\tself.close()\\n\\n\\tdef addHideVBIFlag(self):\\n\\t\\teDVBDB.getInstance().addFlag(eServiceReference(self.csel.getCurrentSelection().toString()), FLAG_HIDE_VBI)\\n\\t\\teDVBDB.getInstance().reloadBouquets()\\n\\t\\tScreens.InfoBar.InfoBar.instance.showHideVBI()\\n\\t\\tself.close()\\n\\n\\tdef removeHideVBIFlag(self):\\n\\t\\teDVBDB.getInstance().removeFlag(eServiceReference(self.csel.getCurrentSelection().toString()), FLAG_HIDE_VBI)\\n\\t\\teDVBDB.getInstance().reloadBouquets()\\n\\t\\tScreens.InfoBar.InfoBar.instance.showHideVBI()\\n\\t\\tself.close()\\n\\n\\tdef isProtected(self):\\n\\t\\treturn self.csel.protectContextMenu and config.ParentalControl.setuppinactive.value and config.ParentalControl.config_sections.context_menus.value\\n\\n\\tdef protectResult(self, answer):\\n\\t\\tif answer:\\n\\t\\t\\tself.csel.protectContextMenu = False\\n\\t\\telif answer is not None:\\n\\t\\t\\tself.session.openWithCallback(self.close, MessageBox, _(\\"The pin code you entered is wrong.\\"), MessageBox.TYPE_ERROR)\\n\\t\\telse:\\n\\t\\t\\tself.close()\\n\\n\\tdef addServiceToBouquetOrAlternative(self):\\n\\t\\tif self.addFunction:\\n\\t\\t\\tself.addFunction()\\n\\t\\telse:\\n\\t\\t\\treturn 0\\n\\n\\tdef getCurrentSelectionName(self):\\n\\t\\tcur = self.csel.getCurrentSelection()\\n\\t\\tif cur and cur.valid():\\n\\t\\t\\tname = eServiceCenter.getInstance().info(cur).getName(cur) or ServiceReference(cur).getServiceName() or \\"\\"\\n\\t\\t\\tname = name.replace(\'\\\\xc2\\\\x86\', \'\').replace(\'\\\\xc2\\\\x87\', \'\')\\n\\t\\t\\treturn name\\n\\t\\treturn \\"\\"\\n\\n\\tdef removeEntry(self):\\n\\t\\tif self.removeFunction and self.csel.servicelist.getCurrent() and self.csel.servicelist.getCurrent().valid():\\n\\t\\t\\tif self.csel.confirmRemove:\\n\\t\\t\\t\\tlist = [(_(\\"yes\\"), True), (_(\\"no\\"), False), (_(\\"yes\\") + \\" \\" + _(\\"and never ask again this session again\\"), \\"never\\")]\\n\\t\\t\\t\\tself.session.openWithCallback(self.removeFunction, MessageBox, _(\\"Are you sure to remove this entry?\\") + \\"\\\\n%s\\" % self.getCurrentSelectionName(), list=list)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.removeFunction(True)\\n\\t\\telse:\\n\\t\\t\\treturn 0\\n\\n\\tdef removeCurrentService(self, answer):\\n\\t\\tif answer:\\n\\t\\t\\tif answer == \\"never\\":\\n\\t\\t\\t\\tself.csel.confirmRemove = False\\n\\t\\t\\tself.csel.removeCurrentService()\\n\\t\\t\\tself.close()\\n\\n\\tdef removeSatelliteService(self, answer):\\n\\t\\tif answer:\\n\\t\\t\\tif answer == \\"never\\":\\n\\t\\t\\t\\tself.csel.confirmRemove = False\\n\\t\\t\\tself.csel.removeSatelliteService()\\n\\t\\t\\tself.close()\\n\\n\\tdef removeBouquet(self, answer):\\n\\t\\tif answer:\\n\\t\\t\\tself.csel.removeBouquet()\\n\\t\\t\\teDVBDB.getInstance().reloadBouquets()\\n\\t\\t\\tself.close()\\n\\n\\tdef purgeDeletedBouquets(self):\\n\\t\\tself.session.openWithCallback(self.purgeDeletedBouquetsCallback, MessageBox, _(\\"Are you sure to purge all deleted userbouquets?\\"))\\n\\n\\tdef purgeDeletedBouquetsCallback(self, answer):\\n\\t\\tif answer:\\n\\t\\t\\tfor file in os.listdir(\\"/etc/enigma2/\\"):\\n\\t\\t\\t\\tif file.startswith(\\"userbouquet\\") and file.endswith(\\".del\\"):\\n\\t\\t\\t\\t\\tfile = \\"/etc/enigma2/\\" + file\\n\\t\\t\\t\\t\\tprint \\"permantly remove file \\", file\\n\\t\\t\\t\\t\\tos.remove(file)\\n\\t\\t\\tself.close()\\n\\n\\tdef restoreDeletedBouquets(self):\\n\\t\\tfor file in os.listdir(\\"/etc/enigma2/\\"):\\n\\t\\t\\tif file.startswith(\\"userbouquet\\") and file.endswith(\\".del\\"):\\n\\t\\t\\t\\tfile = \\"/etc/enigma2/\\" + file\\n\\t\\t\\t\\tprint \\"restore file \\", file[:-4]\\n\\t\\t\\t\\tos.rename(file, file[:-4])\\n\\t\\teDVBDBInstance = eDVBDB.getInstance()\\n\\t\\teDVBDBInstance.setLoadUnlinkedUserbouquets(True)\\n\\t\\teDVBDBInstance.reloadBouquets()\\n\\t\\teDVBDBInstance.setLoadUnlinkedUserbouquets(config.misc.load_unlinked_userbouquets.value)\\n\\t\\trefreshServiceList()\\n\\t\\tself.csel.showFavourites()\\n\\t\\tself.close()\\n\\n\\tdef playMain(self):\\n\\t\\tsel = self.csel.getCurrentSelection()\\n\\t\\tif sel and sel.valid() and self.csel.dopipzap and (not self.parentalControlEnabled or self.parentalControl.getProtectionLevel(self.csel.getCurrentSelection().toCompareString()) == -1):\\n\\t\\t\\tself.csel.zap()\\n\\t\\t\\tself.csel.setCurrentSelection(sel)\\n\\t\\t\\tself.close(True)\\n\\t\\telse:\\n\\t\\t\\treturn 0\\n\\n\\tdef okbuttonClick(self):\\n\\t\\tself[\\"menu\\"].getCurrent()[0][1]()\\n\\n\\tdef openSetup(self):\\n\\t\\tfrom Screens.Setup import Setup\\n\\t\\tself.session.openWithCallback(self.cancelClick, Setup, \\"userinterface\\")\\n\\n\\tdef cancelClick(self, dummy=False):\\n\\t\\tself.close(False)\\n\\n\\tdef showServiceInformations(self):\\n\\t\\tcurrent = self.csel.getCurrentSelection()\\n\\t\\tif current.flags \\u0026 eServiceReference.isGroup:\\n\\t\\t\\tplayingref = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\t\\tif playingref and playingref == current:\\n\\t\\t\\t\\tcurrent = self.session.nav.getCurrentlyPlayingServiceReference()\\n\\t\\t\\telse:\\n\\t\\t\\t\\tcurrent = eServiceReference(GetWithAlternative(current.toString()))\\n\\t\\tself.session.open(ServiceInfo, current)\\n\\t\\tself.close()\\n\\n\\tdef setStartupService(self):\\n\\t\\tself.session.openWithCallback(self.setStartupServiceCallback, MessageBox, _(\\"Set startup service\\"), list = [(_(\\"Only on startup\\"), \\"startup\\"), (_(\\"Also on standby\\"), \\"standby\\")])\\n\\n\\tdef setStartupServiceCallback(self, answer):\\n\\t\\tif answer:\\n\\t\\t\\tconfig.servicelist.startupservice.value = self.csel.getCurrentSelection().toString()\\n\\t\\t\\tpath = \';\'.join([i.toString() for i in self.csel.servicePath])\\n\\t\\t\\tconfig.servicelist.startuproot.value = path\\n\\t\\t\\tconfig.servicelist.startupmode.value = config.servicelist.lastmode.value\\n\\t\\t\\tconfig.servicelist.startupservice_onstandby.value = answer == \\"standby\\"\\n\\t\\t\\tconfig.servicelist.save()\\n\\t\\t\\tconfigfile.save()\\n\\t\\t\\tself.close()\\n\\n\\tdef unsetStartupService(self):\\n\\t\\tconfig.servicelist.startupservice.value = \'\'\\n\\t\\tconfig.servicelist.startupservice_onstandby.value = False\\n\\t\\tconfig.servicelist.save()\\n\\t\\tconfigfile.save()\\n\\t\\tself.close()\\n\\n\\tdef showBouquetInputBox(self):\\n\\t\\tself.session.openWithCallback(self.bouquetInputCallback, VirtualKeyBoard, title=_(\\"Please enter a name for the new bouquet\\"), text=\\"bouquetname\\", maxSize=False, visible_width=56, type=Input.TEXT)\\n\\n\\tdef bouquetInputCallback(self, bouquet):\\n\\t\\tif bouquet is not None:\\n\\t\\t\\tself.csel.addBouquet(bouquet, None)\\n\\t\\tself.close()\\n\\n\\tdef addParentalProtection(self, service):\\n\\t\\tself.parentalControl.protectService(service.toCompareString())\\n\\t\\tif config.ParentalControl.hideBlacklist.value and not self.parentalControl.sessionPinCached:\\n\\t\\t\\tself.csel.servicelist.resetRoot()\\n\\t\\tself.close()\\n\\n\\tdef removeParentalProtection(self, service):\\n\\t\\tself.session.openWithCallback(boundFunction(self.pinEntered, service.toCompareString()), PinInput, pinList=[config.ParentalControl.servicepin[0].value], triesEntry=config.ParentalControl.retries.servicepin, title=_(\\"Enter the service pin\\"), windowTitle=_(\\"Enter pin code\\"))\\n\\n\\tdef pinEntered(self, service, answer):\\n\\t\\tif answer:\\n\\t\\t\\tself.parentalControl.unProtectService(service)\\n\\t\\t\\tself.close()\\n\\t\\telif answer is not None:\\n\\t\\t\\tself.session.openWithCallback(self.close, MessageBox, _(\\"The pin code you entered is wrong.\\"), MessageBox.TYPE_ERROR)\\n\\t\\telse:\\n\\t\\t\\tself.close()\\n\\n\\tdef unhideParentalServices(self):\\n\\t\\tif self.csel.protectContextMenu:\\n\\t\\t\\tself.session.openWithCallback(self.unhideParentalServicesCallback, PinInput, pinList=[config.ParentalControl.servicepin[0].value], triesEntry=config.ParentalControl.retries.servicepin, title=_(\\"Enter the service pin\\"), windowTitle=_(\\"Enter pin code\\"))\\n\\t\\telse:\\n\\t\\t\\tself.unhideParentalServicesCallback(True)\\n\\n\\tdef unhideParentalServicesCallback(self, answer):\\n\\t\\tif answer:\\n\\t\\t\\tservice = self.csel.servicelist.getCurrent()\\n\\t\\t\\tself.parentalControl.setSessionPinCached()\\n\\t\\t\\tself.parentalControl.hideBlacklist()\\n\\t\\t\\tself.csel.servicelist.resetRoot()\\n\\t\\t\\tself.csel.servicelist.setCurrent(service)\\n\\t\\t\\tself.close()\\n\\t\\telif answer is not None:\\n\\t\\t\\tself.session.openWithCallback(self.close, MessageBox, _(\\"The pin code you entered is wrong.\\"), MessageBox.TYPE_ERROR)\\n\\t\\telse:\\n\\t\\t\\tself.close()\\n\\n\\tdef showServiceInPiP(self):\\n\\t\\tif self.csel.dopipzap or (self.parentalControlEnabled and not self.parentalControl.getProtectionLevel(self.csel.getCurrentSelection().toCompareString()) == -1):\\n\\t\\t\\treturn 0\\n\\t\\tif self.session.pipshown:\\n\\t\\t\\tdel self.session.pip\\n\\t\\tself.session.pip = self.session.instantiateDialog(PictureInPicture)\\n\\t\\tself.session.pip.show()\\n\\t\\tnewservice = self.csel.servicelist.getCurrent()\\n\\t\\tcurrentBouquet = self.csel.servicelist and self.csel.servicelist.getRoot()\\n\\t\\tif newservice and newservice.valid():\\n\\t\\t\\tif self.session.pip.playService(newservice):\\n\\t\\t\\t\\tself.session.pipshown = True\\n\\t\\t\\t\\tself.session.pip.servicePath = self.csel.getCurrentServicePath()\\n\\t\\t\\t\\tself.session.pip.servicePath[1] = currentBouquet\\n\\t\\t\\t\\tself.close(True)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.session.pipshown = False\\n\\t\\t\\t\\tdel self.session.pip\\n\\t\\t\\t\\tself.session.openWithCallback(self.close, MessageBox, _(\\"Could not open Picture in Picture\\"), MessageBox.TYPE_ERROR)\\n\\t\\telse:\\n\\t\\t\\treturn 0\\n\\n\\tdef addServiceToBouquetSelected(self):\\n\\t\\tbouquets = self.csel.getBouquetList()\\n\\t\\tif bouquets is None:\\n\\t\\t\\tcnt = 0\\n\\t\\telse:\\n\\t\\t\\tcnt = len(bouquets)\\n\\t\\tif cnt \\u003e 1: # show bouquet list\\n\\t\\t\\tself.bsel = self.session.openWithCallback(self.bouquetSelClosed, BouquetSelector, bouquets, self.addCurrentServiceToBouquet)\\n\\t\\telif cnt == 1: # add to only one existing bouquet\\n\\t\\t\\tself.addCurrentServiceToBouquet(bouquets[0][1], closeBouquetSelection=False)\\n\\n\\tdef bouquetSelClosed(self, recursive):\\n\\t\\tself.bsel = None\\n\\t\\tif recursive:\\n\\t\\t\\tself.close(False)\\n\\n\\tdef removeSatelliteServices(self):\\n\\t\\tself.csel.removeSatelliteServices()\\n\\t\\tself.close()\\n\\n\\tdef copyCurrentToBouquetList(self):\\n\\t\\tself.csel.copyCurrentToBouquetList()\\n\\t\\tself.close()\\n\\n\\tdef showMarkerInputBox(self):\\n\\t\\tself.session.openWithCallback(self.markerInputCallback, VirtualKeyBoard, title=_(\\"Please enter a name for the new marker\\"), text=\\"markername\\", maxSize=False, visible_width=56, type=Input.TEXT)\\n\\n\\tdef markerInputCallback(self, marker):\\n\\t\\tif marker is not None:\\n\\t\\t\\tself.csel.addMarker(marker)\\n\\t\\tself.close()\\n\\n\\tdef addCurrentServiceToBouquet(self, dest, closeBouquetSelection=True):\\n\\t\\tself.csel.addServiceToBouquet(dest)\\n\\t\\tif self.bsel is not None:\\n\\t\\t\\tself.bsel.close(True)\\n\\t\\telse:\\n\\t\\t\\tself.close(closeBouquetSelection) # close bouquet selection\\n\\n\\tdef renameEntry(self):\\n\\t\\tif self.inBouquet and self.csel.servicelist.getCurrent() and self.csel.servicelist.getCurrent().valid() and not self.csel.entry_marked:\\n\\t\\t\\tself.csel.renameEntry()\\n\\t\\t\\tself.close()\\n\\t\\telse:\\n\\t\\t\\treturn 0\\n\\n\\tdef toggleMoveMode(self):\\n\\t\\tif self.inBouquet and self.csel.servicelist.getCurrent() and self.csel.servicelist.getCurrent().valid():\\n\\t\\t\\tself.csel.toggleMoveMode()\\n\\t\\t\\tself.close()\\n\\t\\telse:\\n\\t\\t\\treturn 0\\n\\n\\tdef toggleMoveModeSelect(self):\\n\\t\\tif self.inBouquet and self.csel.servicelist.getCurrent() and self.csel.servicelist.getCurrent().valid():\\n\\t\\t\\tself.csel.toggleMoveMode(True)\\n\\t\\t\\tself.close()\\n\\t\\telse:\\n\\t\\t\\treturn 0\\n\\n\\tdef bouquetMarkStart(self):\\n\\t\\tself.csel.startMarkedEdit(EDIT_BOUQUET)\\n\\t\\tself.close()\\n\\n\\tdef bouquetMarkEnd(self):\\n\\t\\tself.csel.endMarkedEdit(abort=False)\\n\\t\\tself.close()\\n\\n\\tdef bouquetMarkAbort(self):\\n\\t\\tself.csel.endMarkedEdit(abort=True)\\n\\t\\tself.close()\\n\\n\\tdef removeNewFoundFlag(self):\\n\\t\\teDVBDB.getInstance().removeFlag(self.csel.getCurrentSelection(), FLAG_SERVICE_NEW_FOUND)\\n\\t\\tself.close()\\n\\n\\tdef removeAllNewFoundFlags(self):\\n\\t\\tcurpath = self.csel.getCurrentSelection().getPath()\\n\\t\\tidx = curpath.find(\\"satellitePosition == \\")\\n\\t\\tif idx != -1:\\n\\t\\t\\ttmp = curpath[idx+21:]\\n\\t\\t\\tidx = tmp.find(\')\')\\n\\t\\t\\tif idx != -1:\\n\\t\\t\\t\\tsatpos = int(tmp[:idx])\\n\\t\\t\\t\\teDVBDB.getInstance().removeFlags(FLAG_SERVICE_NEW_FOUND, -1, -1, -1, satpos)\\n\\t\\tself.close()\\n\\n\\tdef editAlternativeServices(self):\\n\\t\\tself.csel.startMarkedEdit(EDIT_ALTERNATIVES)\\n\\t\\tself.close()\\n\\n\\tdef showAlternativeServices(self):\\n\\t\\tself.csel[\\"Service\\"].editmode = True\\n\\t\\tself.csel.enterPath(self.csel.getCurrentSelection())\\n\\t\\tself.close()\\n\\n\\tdef removeAlternativeServices(self):\\n\\t\\tself.csel.removeAlternativeServices()\\n\\t\\tself.close()\\n\\n\\tdef addAlternativeServices(self):\\n\\t\\tself.csel.addAlternativeServices()\\n\\t\\tself.csel.startMarkedEdit(EDIT_ALTERNATIVES)\\n\\t\\tself.close()\\n\\n\\tdef findCurrentlyPlayed(self):\\n\\t\\tsel = self.csel.getCurrentSelection()\\n\\t\\tif sel and sel.valid() and not self.csel.entry_marked:\\n\\t\\t\\tcurrentPlayingService = (hasattr(self.csel, \\"dopipzap\\") and self.csel.dopipzap) and self.session.pip.getCurrentService() or self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\t\\tself.csel.servicelist.setCurrent(currentPlayingService, adjust=False)\\n\\t\\t\\tif self.csel.getCurrentSelection() != currentPlayingService:\\n\\t\\t\\t\\tself.csel.setCurrentSelection(sel)\\n\\t\\t\\tself.close()\\n\\t\\telse:\\n\\t\\t\\treturn 0\\n\\n\\tdef runPlugin(self, plugin):\\n\\t\\tplugin(session=self.session, service=self.csel.getCurrentSelection())\\n\\t\\tself.close()\\n\\nclass SelectionEventInfo:\\n\\tdef __init__(self):\\n\\t\\tself[\\"Service\\"] = self[\\"ServiceEvent\\"] = ServiceEvent()\\n\\t\\tself[\\"Event\\"] = Event()\\n\\t\\tself.servicelist.connectSelChanged(self.__selectionChanged)\\n\\t\\tself.timer = eTimer()\\n\\t\\tself.timer.callback.append(self.updateEventInfo)\\n\\t\\tself.onShown.append(self.__selectionChanged)\\n\\n\\tdef __selectionChanged(self):\\n\\t\\tif self.execing:\\n\\t\\t\\tself.timer.start(100, True)\\n\\n\\tdef updateEventInfo(self):\\n\\t\\tcur = self.getCurrentSelection()\\n\\t\\tservice = self[\\"Service\\"]\\n\\t\\tservice.newService(cur)\\n\\t\\tself[\\"Event\\"].newEvent(service.event)\\n\\nclass ChannelSelectionEPG(InfoBarHotkey):\\n\\tdef __init__(self):\\n\\t\\tself.hotkeys = [(\\"Info (EPG)\\", \\"info\\", \\"Infobar/openEventView\\"),\\n\\t\\t\\t(\\"Info (EPG)\\" + \\" \\" + _(\\"long\\"), \\"info_long\\", \\"Infobar/showEventInfoPlugins\\"),\\n\\t\\t\\t(\\"Epg/Guide\\", \\"epg\\", \\"Plugins/Extensions/GraphMultiEPG/1\\"),\\n\\t\\t\\t(\\"Epg/Guide\\" + \\" \\" + _(\\"long\\"), \\"epg_long\\", \\"Infobar/showEventInfoPlugins\\")]\\n\\t\\tself[\\"ChannelSelectEPGActions\\"] = hotkeyActionMap([\\"ChannelSelectEPGActions\\"], dict((x[1], self.hotkeyGlobal) for x in self.hotkeys))\\n\\t\\tself.eventViewEPG = self.start_bouquet = self.epg_bouquet = None\\n\\t\\tself.currentSavedPath = []\\n\\n\\tdef getKeyFunctions(self, key):\\n\\t\\tselection = eval(\\"config.misc.hotkey.\\" + key + \\".value.split(\',\')\\")\\n\\t\\tselected = []\\n\\t\\tfor x in selection:\\n\\t\\t\\tfunction = list(function for function in getHotkeyFunctions() if function[1] == x and function[2] == \\"EPG\\")\\n\\t\\t\\tif function:\\n\\t\\t\\t\\tselected.append(function[0])\\n\\t\\treturn selected\\n\\n\\tdef runPlugin(self, plugin):\\n\\t\\tScreens.InfoBar.InfoBar.instance.runPlugin(plugin)\\n\\n\\tdef getEPGPluginList(self, getAll=False):\\n\\t\\tpluginlist = [(p.name, boundFunction(self.runPlugin, p), p.path) for p in plugins.getPlugins(where = PluginDescriptor.WHERE_EVENTINFO) \\\\\\n\\t\\t\\t\\tif \'selectedevent\' not in p.__call__.func_code.co_varnames] or []\\n\\t\\tfrom Components.ServiceEventTracker import InfoBarCount\\n\\t\\tif getAll or InfoBarCount == 1:\\n\\t\\t\\tpluginlist.append((_(\\"Show EPG for current channel...\\"), self.openSingleServiceEPG, \\"current_channel\\"))\\n\\t\\tpluginlist.append((_(\\"Multi EPG\\"), self.openMultiServiceEPG, \\"multi_epg\\"))\\n\\t\\tpluginlist.append((_(\\"Current event EPG\\"), self.openEventView, \\"event_epg\\"))\\n\\t\\treturn pluginlist\\n\\n\\tdef showEventInfoPlugins(self):\\n\\t\\tpluginlist = self.getEPGPluginList()\\n\\t\\tif pluginlist:\\n\\t\\t\\tself.session.openWithCallback(self.EventInfoPluginChosen, ChoiceBox, title=_(\\"Please choose an extension...\\"), list = pluginlist, skin_name = \\"EPGExtensionsList\\")\\n\\t\\telse:\\n\\t\\t\\tself.openSingleServiceEPG()\\n\\n\\tdef EventInfoPluginChosen(self, answer):\\n\\t\\tif answer is not None:\\n\\t\\t\\tanswer[1]()\\n\\n\\tdef openEventView(self):\\n\\t\\tepglist = [ ]\\n\\t\\tself.epglist = epglist\\n\\t\\tref = self.getCurrentSelection()\\n\\t\\tepg = eEPGCache.getInstance()\\n\\t\\tnow_event = epg.lookupEventTime(ref, -1, 0)\\n\\t\\tif now_event:\\n\\t\\t\\tepglist.append(now_event)\\n\\t\\t\\tnext_event = epg.lookupEventTime(ref, -1, 1)\\n\\t\\t\\tif next_event:\\n\\t\\t\\t\\tepglist.append(next_event)\\n\\t\\tif epglist:\\n\\t\\t\\tself.eventViewEPG = self.session.openWithCallback(self.eventViewEPGClosed, EventViewEPGSelect, epglist[0], ServiceReference(ref), self.eventViewEPGCallback, self.openSingleServiceEPG, self.openMultiServiceEPG, self.openSimilarList)\\n\\n\\tdef eventViewEPGCallback(self, setEvent, setService, val):\\n\\t\\tepglist = self.epglist\\n\\t\\tif len(epglist) \\u003e 1:\\n\\t\\t\\ttmp = epglist[0]\\n\\t\\t\\tepglist[0] = epglist[1]\\n\\t\\t\\tepglist[1] = tmp\\n\\t\\t\\tsetEvent(epglist[0])\\n\\n\\tdef eventViewEPGClosed(self, ret=False):\\n\\t\\tself.eventViewEPG = None\\n\\t\\tif ret:\\n\\t\\t\\tself.close()\\n\\n\\tdef openMultiServiceEPG(self):\\n\\t\\tref = self.getCurrentSelection()\\n\\t\\tif ref:\\n\\t\\t\\tself.start_bouquet = self.epg_bouquet = self.servicelist.getRoot()\\n\\t\\t\\tself.savedService = ref\\n\\t\\t\\tself.currentSavedPath = self.servicePath[:]\\n\\t\\t\\tservices = self.getServicesList(self.servicelist.getRoot())\\n\\t\\t\\tself.session.openWithCallback(self.SingleMultiEPGClosed, EPGSelection, services, self.zapToService, None, bouquetChangeCB=self.changeBouquetForMultiEPG)\\n\\n\\tdef openSingleServiceEPG(self):\\n\\t\\tref = self.getCurrentSelection()\\n\\t\\tif ref:\\n\\t\\t\\tself.start_bouquet = self.epg_bouquet = self.servicelist.getRoot()\\n\\t\\t\\tself.savedService = ref\\n\\t\\t\\tself.currentSavedPath = self.servicePath[:]\\n\\t\\t\\tself.session.openWithCallback(self.SingleMultiEPGClosed, EPGSelection, ref, self.zapToService, serviceChangeCB=self.changeServiceCB, bouquetChangeCB=self.changeBouquetForSingleEPG)\\n\\n\\tdef openSimilarList(self, eventid, refstr):\\n\\t\\tself.session.open(EPGSelection, refstr, None, eventid)\\n\\n\\tdef getServicesList(self, root):\\n\\t\\tservices = [ ]\\n\\t\\tservicelist = root and eServiceCenter.getInstance().list(root)\\n\\t\\tif not servicelist is None:\\n\\t\\t\\twhile True:\\n\\t\\t\\t\\tservice = servicelist.getNext()\\n\\t\\t\\t\\tif not service.valid():\\n\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\tif service.flags \\u0026 (eServiceReference.isDirectory | eServiceReference.isMarker):\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\tservices.append(ServiceReference(service))\\n\\t\\treturn services\\n\\n\\tdef SingleMultiEPGClosed(self, ret=False):\\n\\t\\tif ret:\\n\\t\\t\\tservice = self.getCurrentSelection()\\n\\t\\t\\tif self.eventViewEPG:\\n\\t\\t\\t\\tself.eventViewEPG.close(service)\\n\\t\\t\\telif service is not None:\\n\\t\\t\\t\\tself.close()\\n\\t\\telse:\\n\\t\\t\\tif self.start_bouquet != self.epg_bouquet and len(self.currentSavedPath) \\u003e 0:\\n\\t\\t\\t\\tself.clearPath()\\n\\t\\t\\t\\tself.enterPath(self.bouquet_root)\\n\\t\\t\\t\\tself.epg_bouquet = self.start_bouquet\\n\\t\\t\\t\\tself.enterPath(self.epg_bouquet)\\n\\t\\t\\tself.setCurrentSelection(self.savedService)\\n\\n\\tdef changeBouquetForSingleEPG(self, direction, epg):\\n\\t\\tif config.usage.multibouquet.value:\\n\\t\\t\\tinBouquet = self.getMutableList() is not None\\n\\t\\t\\tif inBouquet and len(self.servicePath) \\u003e 1:\\n\\t\\t\\t\\tself.pathUp()\\n\\t\\t\\t\\tif direction \\u003c 0:\\n\\t\\t\\t\\t\\tself.moveUp()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself.moveDown()\\n\\t\\t\\t\\tcur = self.getCurrentSelection()\\n\\t\\t\\t\\tself.enterPath(cur)\\n\\t\\t\\t\\tself.epg_bouquet = self.servicelist.getRoot()\\n\\t\\t\\t\\tepg.setService(ServiceReference(self.getCurrentSelection()))\\n\\n\\tdef changeBouquetForMultiEPG(self, direction, epg):\\n\\t\\tif config.usage.multibouquet.value:\\n\\t\\t\\tinBouquet = self.getMutableList() is not None\\n\\t\\t\\tif inBouquet and len(self.servicePath) \\u003e 1:\\n\\t\\t\\t\\tself.pathUp()\\n\\t\\t\\t\\tif direction \\u003c 0:\\n\\t\\t\\t\\t\\tself.moveUp()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself.moveDown()\\n\\t\\t\\t\\tcur = self.getCurrentSelection()\\n\\t\\t\\t\\tself.enterPath(cur)\\n\\t\\t\\t\\tself.epg_bouquet = self.servicelist.getRoot()\\n\\t\\t\\t\\tservices = self.getServicesList(self.epg_bouquet)\\n\\t\\t\\t\\tepg.setServices(services)\\n\\n\\tdef changeServiceCB(self, direction, epg):\\n\\t\\tbeg = self.getCurrentSelection()\\n\\t\\twhile True:\\n\\t\\t\\tif direction \\u003e 0:\\n\\t\\t\\t\\tself.moveDown()\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.moveUp()\\n\\t\\t\\tcur = self.getCurrentSelection()\\n\\t\\t\\tif cur == beg or not (cur.flags \\u0026 eServiceReference.isMarker):\\n\\t\\t\\t\\tbreak\\n\\t\\tepg.setService(ServiceReference(self.getCurrentSelection()))\\n\\n\\tdef zapToService(self, service, preview=False, zapback=False):\\n\\t\\tif self.startServiceRef is None:\\n\\t\\t\\tself.startServiceRef = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\tif service is not None:\\n\\t\\t\\tif self.servicelist.getRoot() != self.epg_bouquet:\\n\\t\\t\\t\\tself.servicelist.clearPath()\\n\\t\\t\\t\\tif self.servicelist.bouquet_root != self.epg_bouquet:\\n\\t\\t\\t\\t\\tself.servicelist.enterPath(self.servicelist.bouquet_root)\\n\\t\\t\\t\\tself.servicelist.enterPath(self.epg_bouquet)\\n\\t\\t\\tself.servicelist.setCurrent(service)\\n\\t\\tif not zapback or preview:\\n\\t\\t\\tself.zap(enable_pipzap=True)\\n\\t\\tif (self.dopipzap or zapback) and not preview:\\n\\t\\t\\tself.zapBack()\\n\\t\\tif not preview:\\n\\t\\t\\tself.startServiceRef = None\\n\\t\\t\\tself.startRoot = None\\n\\t\\t\\tself.revertMode = None\\n\\nclass ChannelSelectionEdit:\\n\\tdef __init__(self):\\n\\t\\tself.entry_marked = False\\n\\t\\tself.bouquet_mark_edit = OFF\\n\\t\\tself.mutableList = None\\n\\t\\tself.__marked = [ ]\\n\\t\\tself.saved_title = None\\n\\t\\tself.saved_root = None\\n\\t\\tself.current_ref = None\\n\\t\\tself.editMode = False\\n\\t\\tself.confirmRemove = True\\n\\n\\t\\tclass ChannelSelectionEditActionMap(ActionMap):\\n\\t\\t\\tdef __init__(self, csel, contexts=[ ], actions={ }, prio=0):\\n\\t\\t\\t\\tActionMap.__init__(self, contexts, actions, prio)\\n\\t\\t\\t\\tself.csel = csel\\n\\n\\t\\t\\tdef action(self, contexts, action):\\n\\t\\t\\t\\tif action == \\"cancel\\":\\n\\t\\t\\t\\t\\tself.csel.handleEditCancel()\\n\\t\\t\\t\\t\\treturn 0 # fall-trough\\n\\t\\t\\t\\telif action == \\"ok\\":\\n\\t\\t\\t\\t\\treturn 0 # fall-trough\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\treturn ActionMap.action(self, contexts, action)\\n\\n\\t\\tself[\\"ChannelSelectEditActions\\"] = ChannelSelectionEditActionMap(self, [\\"ChannelSelectEditActions\\", \\"OkCancelActions\\"],\\n\\t\\t\\t{\\n\\t\\t\\t\\t\\"contextMenu\\": self.doContext,\\n\\t\\t\\t})\\n\\n\\tdef getMutableList(self, root=eServiceReference()):\\n\\t\\tif not self.mutableList is None:\\n\\t\\t\\treturn self.mutableList\\n\\t\\tserviceHandler = eServiceCenter.getInstance()\\n\\t\\tif not root.valid():\\n\\t\\t\\troot=self.getRoot()\\n\\t\\tlist = root and serviceHandler.list(root)\\n\\t\\tif list is not None:\\n\\t\\t\\treturn list.startEdit()\\n\\t\\treturn None\\n\\n\\tdef buildBouquetID(self, name):\\n\\t\\tname = unicodedata.normalize(\'NFKD\', unicode(name, \'utf_8\', errors=\'ignore\')).encode(\'ASCII\', \'ignore\').translate(None, \'\\u003c\\u003e:\\"/\\\\|?*() \')\\n\\t\\twhile os.path.isfile((self.mode == MODE_TV and \\"/etc/enigma2/userbouquet.%s.tv\\" or \\"/etc/enigma2/userbouquet.%s.radio\\") % name):\\n\\t\\t\\tname = name.rsplit(\\"_\\", 1)\\n\\t\\t\\tname = \\"_\\".join((name[0], len(name) == 2 and name[1].isdigit() and str(int(name[1]) + 1) or \\"1\\"))\\n\\t\\treturn name\\n\\n\\tdef renameEntry(self):\\n\\t\\tself.editMode = True\\n\\t\\tcur = self.getCurrentSelection()\\n\\t\\tif cur and cur.valid():\\n\\t\\t\\tname = eServiceCenter.getInstance().info(cur).getName(cur) or ServiceReference(cur).getServiceName() or \\"\\"\\n\\t\\t\\tname = name.replace(\'\\\\xc2\\\\x86\', \'\').replace(\'\\\\xc2\\\\x87\', \'\')\\n\\t\\t\\tif name:\\n\\t\\t\\t\\tself.session.openWithCallback(self.renameEntryCallback, VirtualKeyBoard, title=_(\\"Please enter new name:\\"), text=name)\\n\\t\\telse:\\n\\t\\t\\treturn 0\\n\\n\\tdef renameEntryCallback(self, name):\\n\\t\\tif name:\\n\\t\\t\\tmutableList = self.getMutableList()\\n\\t\\t\\tif mutableList:\\n\\t\\t\\t\\tcurrent = self.servicelist.getCurrent()\\n\\t\\t\\t\\tcurrent.setName(name)\\n\\t\\t\\t\\tindex = self.servicelist.getCurrentIndex()\\n\\t\\t\\t\\tmutableList.removeService(current, False)\\n\\t\\t\\t\\tmutableList.addService(current)\\n\\t\\t\\t\\tmutableList.moveService(current, index)\\n\\t\\t\\t\\tmutableList.flushChanges()\\n\\t\\t\\t\\tself.servicelist.addService(current, True)\\n\\t\\t\\t\\tself.servicelist.removeCurrent()\\n\\t\\t\\t\\tif not self.servicelist.atEnd():\\n\\t\\t\\t\\t\\tself.servicelist.moveUp()\\n\\n\\tdef addMarker(self, name):\\n\\t\\tcurrent = self.servicelist.getCurrent()\\n\\t\\tmutableList = self.getMutableList()\\n\\t\\tcnt = 0\\n\\t\\twhile mutableList:\\n\\t\\t\\tstr = \'1:64:%d:0:0:0:0:0:0:0::%s\'%(cnt, name)\\n\\t\\t\\tref = eServiceReference(str)\\n\\t\\t\\tif current and current.valid():\\n\\t\\t\\t\\tif not mutableList.addService(ref, current):\\n\\t\\t\\t\\t\\tself.servicelist.addService(ref, True)\\n\\t\\t\\t\\t\\tmutableList.flushChanges()\\n\\t\\t\\t\\t\\tbreak\\n\\t\\t\\telif not mutableList.addService(ref):\\n\\t\\t\\t\\tself.servicelist.addService(ref, True)\\n\\t\\t\\t\\tmutableList.flushChanges()\\n\\t\\t\\t\\tbreak\\n\\t\\t\\tcnt+=1\\n\\n\\tdef addAlternativeServices(self):\\n\\t\\tcur_service = ServiceReference(self.getCurrentSelection())\\n\\t\\tend = self.atEnd()\\n\\t\\troot = self.getRoot()\\n\\t\\tcur_root = root and ServiceReference(root)\\n\\t\\tmutableBouquet = cur_root.list().startEdit()\\n\\t\\tif mutableBouquet:\\n\\t\\t\\tname = cur_service.getServiceName()\\n\\t\\t\\trefstr = \'_\'.join(cur_service.ref.toString().split(\':\'))\\n\\t\\t\\tif self.mode == MODE_TV:\\n\\t\\t\\t\\tstr = \'1:134:1:0:0:0:0:0:0:0:FROM BOUQUET \\\\\\"alternatives.%s.tv\\\\\\" ORDER BY bouquet\'%(refstr)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tstr = \'1:134:2:0:0:0:0:0:0:0:FROM BOUQUET \\\\\\"alternatives.%s.radio\\\\\\" ORDER BY bouquet\'%(refstr)\\n\\t\\t\\tnew_ref = ServiceReference(str)\\n\\t\\t\\tif not mutableBouquet.addService(new_ref.ref, cur_service.ref):\\n\\t\\t\\t\\tmutableBouquet.removeService(cur_service.ref)\\n\\t\\t\\t\\tmutableBouquet.flushChanges()\\n\\t\\t\\t\\teDVBDB.getInstance().reloadBouquets()\\n\\t\\t\\t\\tmutableAlternatives = new_ref.list().startEdit()\\n\\t\\t\\t\\tif mutableAlternatives:\\n\\t\\t\\t\\t\\tmutableAlternatives.setListName(name)\\n\\t\\t\\t\\t\\tif mutableAlternatives.addService(cur_service.ref):\\n\\t\\t\\t\\t\\t\\tprint \\"add\\", cur_service.ref.toString(), \\"to new alternatives failed\\"\\n\\t\\t\\t\\t\\tmutableAlternatives.flushChanges()\\n\\t\\t\\t\\t\\tself.servicelist.addService(new_ref.ref, True)\\n\\t\\t\\t\\t\\tself.servicelist.removeCurrent()\\n\\t\\t\\t\\t\\tif not end:\\n\\t\\t\\t\\t\\t\\tself.servicelist.moveUp()\\n\\t\\t\\t\\t\\tif cur_service.ref.toString() == self.lastservice.value:\\n\\t\\t\\t\\t\\t\\tself.saveChannel(new_ref.ref)\\n\\t\\t\\t\\t\\tif self.startServiceRef and cur_service.ref == self.startServiceRef:\\n\\t\\t\\t\\t\\t\\tself.startServiceRef = new_ref.ref\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tprint \\"get mutable list for new created alternatives failed\\"\\n\\t\\t\\telse:\\n\\t\\t\\t\\tprint \\"add\\", str, \\"to\\", cur_root.getServiceName(), \\"failed\\"\\n\\t\\telse:\\n\\t\\t\\tprint \\"bouquetlist is not editable\\"\\n\\n\\tdef addBouquet(self, bName, services):\\n\\t\\tserviceHandler = eServiceCenter.getInstance()\\n\\t\\tmutableBouquetList = serviceHandler.list(self.bouquet_root).startEdit()\\n\\t\\tif mutableBouquetList:\\n\\t\\t\\tbName = self.buildBouquetID(bName)\\n\\t\\t\\tnew_bouquet_ref = eServiceReference((self.mode == MODE_TV and \'1:7:1:0:0:0:0:0:0:0:FROM BOUQUET \\"userbouquet.%s.tv\\" ORDER BY bouquet\' or \'1:7:2:0:0:0:0:0:0:0:FROM BOUQUET \\"userbouquet.%s.radio\\" ORDER BY bouquet\') % bName)\\n\\t\\t\\tif not mutableBouquetList.addService(new_bouquet_ref):\\n\\t\\t\\t\\tmutableBouquetList.flushChanges()\\n\\t\\t\\t\\teDVBDB.getInstance().reloadBouquets()\\n\\t\\t\\t\\tmutableBouquet = serviceHandler.list(new_bouquet_ref).startEdit()\\n\\t\\t\\t\\tif mutableBouquet:\\n\\t\\t\\t\\t\\tmutableBouquet.setListName(bName)\\n\\t\\t\\t\\t\\tif services is not None:\\n\\t\\t\\t\\t\\t\\tfor service in services:\\n\\t\\t\\t\\t\\t\\t\\tif mutableBouquet.addService(service):\\n\\t\\t\\t\\t\\t\\t\\t\\tprint \\"add\\", service.toString(), \\"to new bouquet failed\\"\\n\\t\\t\\t\\t\\tmutableBouquet.flushChanges()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tprint \\"get mutable list for new created bouquet failed\\"\\n\\t\\t\\t\\t# do some voodoo to check if current_root is equal to bouquet_root\\n\\t\\t\\t\\tcur_root = self.getRoot();\\n\\t\\t\\t\\tstr1 = cur_root and cur_root.toString()\\n\\t\\t\\t\\tpos1 = str1 and str1.find(\\"FROM BOUQUET\\") or -1\\n\\t\\t\\t\\tpos2 = self.bouquet_rootstr.find(\\"FROM BOUQUET\\")\\n\\t\\t\\t\\tif pos1 != -1 and pos2 != -1 and str1[pos1:] == self.bouquet_rootstr[pos2:]:\\n\\t\\t\\t\\t\\tself.servicelist.addService(new_bouquet_ref)\\n\\t\\t\\t\\t\\tself.servicelist.resetRoot()\\n\\t\\t\\telse:\\n\\t\\t\\t\\tprint \\"add\\", str, \\"to bouquets failed\\"\\n\\t\\telse:\\n\\t\\t\\tprint \\"bouquetlist is not editable\\"\\n\\n\\tdef copyCurrentToBouquetList(self):\\n\\t\\tprovider = ServiceReference(self.getCurrentSelection())\\n\\t\\tproviderName = provider.getServiceName()\\n\\t\\tserviceHandler = eServiceCenter.getInstance()\\n\\t\\tservices = serviceHandler.list(provider.ref)\\n\\t\\tself.addBouquet(providerName, services and services.getContent(\'R\', True))\\n\\n\\tdef removeAlternativeServices(self):\\n\\t\\tcur_service = ServiceReference(self.getCurrentSelection())\\n\\t\\tend = self.atEnd()\\n\\t\\troot = self.getRoot()\\n\\t\\tcur_root = root and ServiceReference(root)\\n\\t\\tlist = cur_service.list()\\n\\t\\tfirst_in_alternative = list and list.getNext()\\n\\t\\tif first_in_alternative:\\n\\t\\t\\tedit_root = cur_root and cur_root.list().startEdit()\\n\\t\\t\\tif edit_root:\\n\\t\\t\\t\\tif not edit_root.addService(first_in_alternative, cur_service.ref):\\n\\t\\t\\t\\t\\tself.servicelist.addService(first_in_alternative, True)\\n\\t\\t\\t\\t\\tif cur_service.ref.toString() == self.lastservice.value:\\n\\t\\t\\t\\t\\t\\tself.saveChannel(first_in_alternative)\\n\\t\\t\\t\\t\\tif self.startServiceRef and cur_service.ref == self.startServiceRef:\\n\\t\\t\\t\\t\\t\\tself.startServiceRef = first_in_alternative\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tprint \\"couldn\'t add first alternative service to current root\\"\\n\\t\\t\\telse:\\n\\t\\t\\t\\tprint \\"couldn\'t edit current root!!\\"\\n\\t\\telse:\\n\\t\\t\\tprint \\"remove empty alternative list !!\\"\\n\\t\\tself.removeBouquet()\\n\\t\\tif not end:\\n\\t\\t\\tself.servicelist.moveUp()\\n\\n\\tdef removeBouquet(self):\\n\\t\\trefstr = self.getCurrentSelection().toString()\\n\\t\\tprint \\"removeBouquet\\", refstr\\n\\t\\tpos = refstr.find(\'FROM BOUQUET \\"\')\\n\\t\\tfilename = None\\n\\t\\tself.removeCurrentService(bouquet=True)\\n\\n\\tdef removeSatelliteService(self):\\n\\t\\tcurrent = self.getCurrentSelection()\\n\\t\\teDVBDB.getInstance().removeService(current)\\n\\t\\trefreshServiceList()\\n\\t\\tif not self.atEnd():\\n\\t\\t\\tself.servicelist.moveUp()\\n\\n\\tdef removeSatelliteServices(self):\\n\\t\\tcurrent = self.getCurrentSelection()\\n\\t\\tunsigned_orbpos = current.getUnsignedData(4) \\u003e\\u003e 16\\n\\t\\tif unsigned_orbpos == 0xFFFF:\\n\\t\\t\\tmessageText = _(\\"Are you sure to remove all cable services?\\")\\n\\t\\telif unsigned_orbpos == 0xEEEE:\\n\\t\\t\\tmessageText = _(\\"Are you sure to remove all terrestrial services?\\")\\n\\t\\telse:\\n\\t\\t\\tif unsigned_orbpos \\u003e 1800:\\n\\t\\t\\t\\tunsigned_orbpos = 3600 - unsigned_orbpos\\n\\t\\t\\t\\tdirection = _(\\"W\\")\\n\\t\\t\\telse:\\n\\t\\t\\t\\tdirection = _(\\"E\\")\\n\\t\\t\\tmessageText = _(\\"Are you sure to remove all %d.%d%s%s services?\\") % (unsigned_orbpos/10, unsigned_orbpos%10, \\"\\\\xc2\\\\xb0\\", direction)\\n\\t\\tself.session.openWithCallback(self.removeSatelliteServicesCallback, MessageBox, messageText)\\n\\n\\tdef removeSatelliteServicesCallback(self, answer):\\n\\t\\tif answer:\\n\\t\\t\\tcurrentIndex = self.servicelist.getCurrentIndex()\\n\\t\\t\\tcurrent = self.getCurrentSelection()\\n\\t\\t\\tunsigned_orbpos = current.getUnsignedData(4) \\u003e\\u003e 16\\n\\t\\t\\tif unsigned_orbpos == 0xFFFF:\\n\\t\\t\\t\\teDVBDB.getInstance().removeServices(int(\\"0xFFFF0000\\", 16) - 0x100000000)\\n\\t\\t\\telif unsigned_orbpos == 0xEEEE:\\n\\t\\t\\t\\teDVBDB.getInstance().removeServices(int(\\"0xEEEE0000\\", 16) - 0x100000000)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tcurpath = current.getPath()\\n\\t\\t\\t\\tidx = curpath.find(\\"satellitePosition == \\")\\n\\t\\t\\t\\tif idx != -1:\\n\\t\\t\\t\\t\\ttmp = curpath[idx + 21:]\\n\\t\\t\\t\\t\\tidx = tmp.find(\')\')\\n\\t\\t\\t\\t\\tif idx != -1:\\n\\t\\t\\t\\t\\t\\tsatpos = int(tmp[:idx])\\n\\t\\t\\t\\t\\t\\teDVBDB.getInstance().removeServices(-1, -1, -1, satpos)\\n\\t\\t\\trefreshServiceList()\\n\\t\\t\\tif hasattr(self, \'showSatellites\'):\\n\\t\\t\\t\\tself.showSatellites()\\n\\t\\t\\t\\tself.servicelist.moveToIndex(currentIndex)\\n\\t\\t\\t\\tif currentIndex != self.servicelist.getCurrentIndex():\\n\\t\\t\\t\\t\\tself.servicelist.instance.moveSelection(self.servicelist.instance.moveEnd)\\n\\n#  multiple marked entry stuff ( edit mode, later multiepg selection )\\n\\tdef startMarkedEdit(self, type):\\n\\t\\tself.savedPath = self.servicePath[:]\\n\\t\\tif type == EDIT_ALTERNATIVES:\\n\\t\\t\\tself.current_ref = self.getCurrentSelection()\\n\\t\\t\\tself.enterPath(self.current_ref)\\n\\t\\tself.mutableList = self.getMutableList()\\n\\t\\t# add all services from the current list to internal marked set in listboxservicecontent\\n\\t\\tself.clearMarks() # this clears the internal marked set in the listboxservicecontent\\n\\t\\tself.saved_title = self.getTitle()\\n\\t\\tpos = self.saved_title.find(\')\')\\n\\t\\tnew_title = self.saved_title[:pos+1]\\n\\t\\tif type == EDIT_ALTERNATIVES:\\n\\t\\t\\tself.bouquet_mark_edit = EDIT_ALTERNATIVES\\n\\t\\t\\tnew_title += \' \' + _(\\"[alternative edit]\\")\\n\\t\\telse:\\n\\t\\t\\tself.bouquet_mark_edit = EDIT_BOUQUET\\n\\t\\t\\tif config.usage.multibouquet.value:\\n\\t\\t\\t\\tnew_title += \' \' + _(\\"[bouquet edit]\\")\\n\\t\\t\\telse:\\n\\t\\t\\t\\tnew_title += \' \' + _(\\"[favourite edit]\\")\\n\\t\\tself.setTitle(new_title)\\n\\t\\tself.__marked = self.servicelist.getRootServices()\\n\\t\\tfor x in self.__marked:\\n\\t\\t\\tself.servicelist.addMarked(eServiceReference(x))\\n\\t\\tself[\\"Service\\"].editmode = True\\n\\n\\tdef endMarkedEdit(self, abort):\\n\\t\\tif not abort and self.mutableList is not None:\\n\\t\\t\\tnew_marked = set(self.servicelist.getMarked())\\n\\t\\t\\told_marked = set(self.__marked)\\n\\t\\t\\tremoved = old_marked - new_marked\\n\\t\\t\\tadded = new_marked - old_marked\\n\\t\\t\\tchanged = False\\n\\t\\t\\tfor x in removed:\\n\\t\\t\\t\\tchanged = True\\n\\t\\t\\t\\tself.mutableList.removeService(eServiceReference(x))\\n\\t\\t\\tfor x in added:\\n\\t\\t\\t\\tchanged = True\\n\\t\\t\\t\\tself.mutableList.addService(eServiceReference(x))\\n\\t\\t\\tif changed:\\n\\t\\t\\t\\tif self.bouquet_mark_edit == EDIT_ALTERNATIVES and not new_marked and self.__marked:\\n\\t\\t\\t\\t\\tself.mutableList.addService(eServiceReference(self.__marked[0]))\\n\\t\\t\\t\\tself.mutableList.flushChanges()\\n\\t\\tself.__marked = []\\n\\t\\tself.clearMarks()\\n\\t\\tself.bouquet_mark_edit = OFF\\n\\t\\tself.mutableList = None\\n\\t\\tself.setTitle(self.saved_title)\\n\\t\\tself.saved_title = None\\n\\t\\t# self.servicePath is just a reference to servicePathTv or Radio...\\n\\t\\t# so we never ever do use the asignment operator in self.servicePath\\n\\t\\tdel self.servicePath[:] # remove all elements\\n\\t\\tself.servicePath += self.savedPath # add saved elements\\n\\t\\tdel self.savedPath\\n\\t\\tself.setRoot(self.servicePath[-1])\\n\\t\\tif self.current_ref:\\n\\t\\t\\tself.setCurrentSelection(self.current_ref)\\n\\t\\t\\tself.current_ref = None\\n\\n\\tdef clearMarks(self):\\n\\t\\tself.servicelist.clearMarks()\\n\\n\\tdef doMark(self):\\n\\t\\tref = self.servicelist.getCurrent()\\n\\t\\tif self.servicelist.isMarked(ref):\\n\\t\\t\\tself.servicelist.removeMarked(ref)\\n\\t\\telse:\\n\\t\\t\\tself.servicelist.addMarked(ref)\\n\\n\\tdef removeCurrentEntry(self, bouquet=False):\\n\\t\\tif self.confirmRemove:\\n\\t\\t\\tlist = [(_(\\"yes\\"), True), (_(\\"no\\"), False), (_(\\"yes\\") + \\" \\" + _(\\"and never ask again this session again\\"), \\"never\\")]\\n\\t\\t\\tself.session.openWithCallback(boundFunction(self.removeCurrentEntryCallback, bouquet), MessageBox, _(\\"Are you sure to remove this entry?\\"), list=list)\\n\\t\\telse:\\n\\t\\t\\tself.removeCurrentEntryCallback(bouquet, True)\\n\\n\\tdef removeCurrentEntryCallback(self, bouquet, answer):\\n\\t\\tif answer:\\n\\t\\t\\tif answer == \\"never\\":\\n\\t\\t\\t\\tself.confirmRemove = False\\n\\t\\t\\tif bouquet:\\n\\t\\t\\t\\tself.removeBouquet()\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.removeCurrentService()\\n\\n\\tdef removeCurrentService(self, bouquet=False):\\n\\t\\tself.editMode = True\\n\\t\\tref = self.servicelist.getCurrent()\\n\\t\\tmutableList = self.getMutableList()\\n\\t\\tif ref.valid() and mutableList is not None:\\n\\t\\t\\tif not mutableList.removeService(ref):\\n\\t\\t\\t\\tmutableList.flushChanges() #FIXME dont flush on each single removed service\\n\\t\\t\\t\\tself.servicelist.removeCurrent()\\n\\t\\t\\t\\tself.servicelist.resetRoot()\\n\\t\\t\\t\\tplayingref = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\t\\t\\tif not bouquet and playingref and ref == playingref:\\n\\t\\t\\t\\t\\tself.channelSelected(doClose=False)\\n\\n\\tdef addServiceToBouquet(self, dest, service=None):\\n\\t\\tmutableList = self.getMutableList(dest)\\n\\t\\tif not mutableList is None:\\n\\t\\t\\tif service is None: #use current selected service\\n\\t\\t\\t\\tservice = self.servicelist.getCurrent()\\n\\t\\t\\tif not mutableList.addService(service):\\n\\t\\t\\t\\tmutableList.flushChanges()\\n\\t\\t\\t\\t# do some voodoo to check if current_root is equal to dest\\n\\t\\t\\t\\tcur_root = self.getRoot();\\n\\t\\t\\t\\tstr1 = cur_root and cur_root.toString() or -1\\n\\t\\t\\t\\tstr2 = dest.toString()\\n\\t\\t\\t\\tpos1 = str1.find(\\"FROM BOUQUET\\")\\n\\t\\t\\t\\tpos2 = str2.find(\\"FROM BOUQUET\\")\\n\\t\\t\\t\\tif pos1 != -1 and pos2 != -1 and str1[pos1:] == str2[pos2:]:\\n\\t\\t\\t\\t\\tself.servicelist.addService(service)\\n\\t\\t\\t\\tself.servicelist.resetRoot()\\n\\n\\tdef toggleMoveMode(self, select=False):\\n\\t\\tself.editMode = True\\n\\t\\tif self.movemode:\\n\\t\\t\\tif self.entry_marked:\\n\\t\\t\\t\\tself.toggleMoveMarked() # unmark current entry\\n\\t\\t\\tself.movemode = False\\n\\t\\t\\tself.mutableList.flushChanges() # FIXME add check if changes was made\\n\\t\\t\\tself.mutableList = None\\n\\t\\t\\tself.setTitle(self.saved_title)\\n\\t\\t\\tself.saved_title = None\\n\\t\\t\\tself.servicelist.resetRoot()\\n\\t\\t\\tself.servicelist.l.setHideNumberMarker(config.usage.hide_number_markers.value)\\n\\t\\t\\tself.setCurrentSelection(self.servicelist.getCurrent())\\n\\t\\telse:\\n\\t\\t\\tself.mutableList = self.getMutableList()\\n\\t\\t\\tself.movemode = True\\n\\t\\t\\tselect and self.toggleMoveMarked()\\n\\t\\t\\tself.saved_title = self.getTitle()\\n\\t\\t\\tpos = self.saved_title.find(\')\')\\n\\t\\t\\tself.setTitle(self.saved_title[:pos+1] + \' \' + _(\\"[move mode]\\") + self.saved_title[pos+1:]);\\n\\t\\t\\tself.servicelist.l.setHideNumberMarker(False)\\n\\t\\t\\tself.setCurrentSelection(self.servicelist.getCurrent())\\n\\t\\tself[\\"Service\\"].editmode = True\\n\\n\\tdef handleEditCancel(self):\\n\\t\\tif self.movemode: #movemode active?\\n\\t\\t\\tself.toggleMoveMode() # disable move mode\\n\\t\\telif self.bouquet_mark_edit != OFF:\\n\\t\\t\\tself.endMarkedEdit(True) # abort edit mode\\n\\n\\tdef toggleMoveMarked(self):\\n\\t\\tif self.entry_marked:\\n\\t\\t\\tself.servicelist.setCurrentMarked(False)\\n\\t\\t\\tself.entry_marked = False\\n\\t\\t\\tself.pathChangeDisabled = False # re-enable path change\\n\\t\\telse:\\n\\t\\t\\tself.servicelist.setCurrentMarked(True)\\n\\t\\t\\tself.entry_marked = True\\n\\t\\t\\tself.pathChangeDisabled = True # no path change allowed in movemod\\n\\n\\tdef doContext(self):\\n\\t\\tself.session.openWithCallback(self.exitContext, ChannelContextMenu, self)\\n\\n\\tdef exitContext(self, close=False):\\n\\t\\tif close:\\n\\t\\t\\tself.cancel()\\n\\nMODE_TV = 0\\nMODE_RADIO = 1\\n\\n# type 1 = digital television service\\n# type 4 = nvod reference service (NYI)\\n# type 17 = MPEG-2 HD digital television service\\n# type 22 = advanced codec SD digital television\\n# type 24 = advanced codec SD NVOD reference service (NYI)\\n# type 25 = advanced codec HD digital television\\n# type 27 = advanced codec HD NVOD reference service (NYI)\\n# type 2 = digital radio sound service\\n# type 10 = advanced codec digital radio sound service\\n# type 31 = High Efficiency Video Coing digital television\\n\\nservice_types_tv = \'1:7:1:0:0:0:0:0:0:0:(type == 1) || (type == 17) || (type == 22) || (type == 25) || (type == 31) || (type == 134) || (type == 195)\'\\nservice_types_radio = \'1:7:2:0:0:0:0:0:0:0:(type == 2) || (type == 10)\'\\n\\nclass ChannelSelectionBase(Screen):\\n\\tdef __init__(self, session):\\n\\t\\tScreen.__init__(self, session)\\n\\t\\tself.setScreenPathMode(None)\\n\\t\\tself[\\"key_red\\"] = Button(_(\\"All\\"))\\n\\t\\tself[\\"key_green\\"] = Button(_(\\"Satellites\\"))\\n\\t\\tself[\\"key_yellow\\"] = Button(_(\\"Provider\\"))\\n\\t\\tself[\\"key_blue\\"] = Button(_(\\"Favourites\\"))\\n\\n\\t\\tself[\\"list\\"] = ServiceList(self)\\n\\t\\tself.servicelist = self[\\"list\\"]\\n\\n\\t\\tself.numericalTextInput = NumericalTextInput(handleTimeout=False)\\n\\t\\tself.numericalTextInput.setUseableChars(u\'1234567890ABCDEFGHIJKLMNOPQRSTUVWXYZ\')\\n\\n\\t\\tself.servicePathTV = [ ]\\n\\t\\tself.servicePathRadio = [ ]\\n\\t\\tself.servicePath = [ ]\\n\\t\\tself.history = [ ]\\n\\t\\tself.rootChanged = False\\n\\t\\tself.startRoot = None\\n\\t\\tself.selectionNumber = \\"\\"\\n\\t\\tself.clearNumberSelectionNumberTimer = eTimer()\\n\\t\\tself.clearNumberSelectionNumberTimer.callback.append(self.clearNumberSelectionNumber)\\n\\t\\tself.protectContextMenu = True\\n\\n\\t\\tself.mode = MODE_TV\\n\\t\\tself.dopipzap = False\\n\\t\\tself.pathChangeDisabled = False\\n\\t\\tself.movemode = False\\n\\t\\tself.showSatDetails = False\\n\\n\\t\\tself[\\"ChannelSelectBaseActions\\"] = NumberActionMap([\\"ChannelSelectBaseActions\\", \\"NumberActions\\", \\"InputAsciiActions\\"],\\n\\t\\t\\t{\\n\\t\\t\\t\\t\\"showFavourites\\": self.showFavourites,\\n\\t\\t\\t\\t\\"showAllServices\\": self.showAllServices,\\n\\t\\t\\t\\t\\"showProviders\\": self.showProviders,\\n\\t\\t\\t\\t\\"showSatellites\\": boundFunction(self.showSatellites, changeMode=True),\\n\\t\\t\\t\\t\\"nextBouquet\\": self.nextBouquet,\\n\\t\\t\\t\\t\\"prevBouquet\\": self.prevBouquet,\\n\\t\\t\\t\\t\\"nextMarker\\": self.nextMarker,\\n\\t\\t\\t\\t\\"prevMarker\\": self.prevMarker,\\n\\t\\t\\t\\t\\"gotAsciiCode\\": self.keyAsciiCode,\\n\\t\\t\\t\\t\\"keyLeft\\": self.keyLeft,\\n\\t\\t\\t\\t\\"keyRight\\": self.keyRight,\\n\\t\\t\\t\\t\\"keyRecord\\": self.keyRecord,\\n\\t\\t\\t\\t\\"1\\": self.keyNumberGlobal,\\n\\t\\t\\t\\t\\"2\\": self.keyNumberGlobal,\\n\\t\\t\\t\\t\\"3\\": self.keyNumberGlobal,\\n\\t\\t\\t\\t\\"4\\": self.keyNumberGlobal,\\n\\t\\t\\t\\t\\"5\\": self.keyNumberGlobal,\\n\\t\\t\\t\\t\\"6\\": self.keyNumberGlobal,\\n\\t\\t\\t\\t\\"7\\": self.keyNumberGlobal,\\n\\t\\t\\t\\t\\"8\\": self.keyNumberGlobal,\\n\\t\\t\\t\\t\\"9\\": self.keyNumberGlobal,\\n\\t\\t\\t\\t\\"0\\": self.keyNumber0\\n\\t\\t\\t}, -2)\\n\\t\\tself.maintitle = _(\\"Channel selection\\")\\n\\t\\tself.recallBouquetMode()\\n\\n\\tdef getBouquetNumOffset(self, bouquet):\\n\\t\\tif not config.usage.multibouquet.value:\\n\\t\\t\\treturn 0\\n\\t\\tstr = bouquet.toString()\\n\\t\\toffset = 0\\n\\t\\tif \'userbouquet.\' in bouquet.toCompareString():\\n\\t\\t\\tserviceHandler = eServiceCenter.getInstance()\\n\\t\\t\\tservicelist = serviceHandler.list(bouquet)\\n\\t\\t\\tif not servicelist is None:\\n\\t\\t\\t\\twhile True:\\n\\t\\t\\t\\t\\tserviceIterator = servicelist.getNext()\\n\\t\\t\\t\\t\\tif not serviceIterator.valid(): #check if end of list\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\t\\tnumber = serviceIterator.getChannelNum()\\n\\t\\t\\t\\t\\tif number \\u003e 0:\\n\\t\\t\\t\\t\\t\\toffset = number - 1\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\treturn offset\\n\\n\\tdef recallBouquetMode(self):\\n\\t\\tif self.mode == MODE_TV:\\n\\t\\t\\tself.service_types = service_types_tv\\n\\t\\t\\tif config.usage.multibouquet.value:\\n\\t\\t\\t\\tself.bouquet_rootstr = \'1:7:1:0:0:0:0:0:0:0:FROM BOUQUET \\"bouquets.tv\\" ORDER BY bouquet\'\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.bouquet_rootstr = \'%s FROM BOUQUET \\"userbouquet.favourites.tv\\" ORDER BY bouquet\'%(self.service_types)\\n\\t\\telse:\\n\\t\\t\\tself.service_types = service_types_radio\\n\\t\\t\\tif config.usage.multibouquet.value:\\n\\t\\t\\t\\tself.bouquet_rootstr = \'1:7:1:0:0:0:0:0:0:0:FROM BOUQUET \\"bouquets.radio\\" ORDER BY bouquet\'\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.bouquet_rootstr = \'%s FROM BOUQUET \\"userbouquet.favourites.radio\\" ORDER BY bouquet\'%(self.service_types)\\n\\t\\tself.bouquet_root = eServiceReference(self.bouquet_rootstr)\\n\\n\\tdef setTvMode(self):\\n\\t\\tself.mode = MODE_TV\\n\\t\\tself.servicePath = self.servicePathTV\\n\\t\\tself.recallBouquetMode()\\n\\t\\ttitle = self.maintitle\\n\\t\\tpos = title.find(\\" (\\")\\n\\t\\tif pos != -1:\\n\\t\\t\\ttitle = title[:pos]\\n\\t\\ttitle += _(\\" (TV)\\")\\n\\t\\tself.setTitle(title)\\n\\n\\tdef setRadioMode(self):\\n\\t\\tself.mode = MODE_RADIO\\n\\t\\tself.servicePath = self.servicePathRadio\\n\\t\\tself.recallBouquetMode()\\n\\t\\ttitle = self.maintitle\\n\\t\\tpos = title.find(\\" (\\")\\n\\t\\tif pos != -1:\\n\\t\\t\\ttitle = title[:pos]\\n\\t\\ttitle += _(\\" (Radio)\\")\\n\\t\\tself.setTitle(title)\\n\\n\\tdef setRoot(self, root, justSet=False):\\n\\t\\tif self.startRoot is None:\\n\\t\\t\\tself.startRoot = self.getRoot()\\n\\t\\tpath = root.getPath()\\n\\t\\tisBouquet = \'FROM BOUQUET\' in path and (root.flags \\u0026 eServiceReference.isDirectory)\\n\\t\\tinBouquetRootList = \'FROM BOUQUET \\"bouquets.\' in path #FIXME HACK\\n\\t\\tif not inBouquetRootList and isBouquet:\\n\\t\\t\\tself.servicelist.setMode(ServiceList.MODE_FAVOURITES)\\n\\t\\telse:\\n\\t\\t\\tself.servicelist.setMode(ServiceList.MODE_NORMAL)\\n\\t\\tself.servicelist.setRoot(root, justSet)\\n\\t\\tself.rootChanged = True\\n\\t\\tself.buildTitleString()\\n\\n\\tdef removeModeStr(self, str):\\n\\t\\tif self.mode == MODE_TV:\\n\\t\\t\\tpos = str.find(_(\\" (TV)\\"))\\n\\t\\telse:\\n\\t\\t\\tpos = str.find(_(\\" (Radio)\\"))\\n\\t\\tif pos != -1:\\n\\t\\t\\treturn str[:pos]\\n\\t\\treturn str\\n\\n\\tdef getServiceName(self, ref):\\n\\t\\tstr = self.removeModeStr(ServiceReference(ref).getServiceName())\\n\\t\\tif \'bouquets\' in str.lower():\\n\\t\\t\\treturn _(\\"User - bouquets\\")\\n\\t\\tif not str:\\n\\t\\t\\tpathstr = ref.getPath()\\n\\t\\t\\tif \'FROM PROVIDERS\' in pathstr:\\n\\t\\t\\t\\treturn _(\\"Provider\\")\\n\\t\\t\\tif \'FROM SATELLITES\' in pathstr:\\n\\t\\t\\t\\treturn _(\\"Satellites\\")\\n\\t\\t\\tif \') ORDER BY name\' in pathstr:\\n\\t\\t\\t\\treturn _(\\"All\\")\\n\\t\\treturn str\\n\\n\\tdef buildTitleString(self):\\n\\t\\ttitleStr = self.getTitle()\\n\\t\\tpos = titleStr.find(\']\')\\n\\t\\tif pos == -1:\\n\\t\\t\\tpos = titleStr.find(\')\')\\n\\t\\tif pos != -1:\\n\\t\\t\\ttitleStr = titleStr[:pos+1]\\n\\t\\t\\tLen = len(self.servicePath)\\n\\t\\t\\tif Len \\u003e 0:\\n\\t\\t\\t\\tbase_ref = self.servicePath[0]\\n\\t\\t\\t\\tif Len \\u003e 1:\\n\\t\\t\\t\\t\\tend_ref = self.servicePath[Len-1]\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tend_ref = None\\n\\t\\t\\t\\tnameStr = self.getServiceName(base_ref)\\n\\t\\t\\t\\ttitleStr += \' - \' + nameStr\\n\\t\\t\\t\\tif end_ref is not None:\\n\\t\\t\\t\\t\\tif Len \\u003e 2:\\n\\t\\t\\t\\t\\t\\ttitleStr += \'/../\'\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\ttitleStr += \'/\'\\n\\t\\t\\t\\t\\tnameStr = self.getServiceName(end_ref)\\n\\t\\t\\t\\t\\ttitleStr += nameStr\\n\\t\\t\\t\\tself.setTitle(titleStr)\\n\\n\\tdef moveUp(self):\\n\\t\\tself.servicelist.moveUp()\\n\\n\\tdef moveDown(self):\\n\\t\\tself.servicelist.moveDown()\\n\\n\\tdef clearPath(self):\\n\\t\\tdel self.servicePath[:]\\n\\n\\tdef enterPath(self, ref, justSet=False):\\n\\t\\tself.servicePath.append(ref)\\n\\t\\tself.setRoot(ref, justSet)\\n\\n\\tdef enterUserbouquet(self, root, save_root=True):\\n\\t\\tself.clearPath()\\n\\t\\tself.recallBouquetMode()\\n\\t\\tif self.bouquet_root:\\n\\t\\t\\tself.enterPath(self.bouquet_root)\\n\\t\\tself.enterPath(root)\\n\\t\\tself.startRoot = None\\n\\t\\tif save_root:\\n\\t\\t\\tself.saveRoot()\\n\\n\\tdef pathUp(self, justSet=False):\\n\\t\\tprev = self.servicePath.pop()\\n\\t\\tif self.servicePath:\\n\\t\\t\\tcurrent = self.servicePath[-1]\\n\\t\\t\\tself.setRoot(current, justSet)\\n\\t\\t\\tif not justSet:\\n\\t\\t\\t\\tself.setCurrentSelection(prev)\\n\\t\\treturn prev\\n\\n\\tdef isBasePathEqual(self, ref):\\n\\t\\tif len(self.servicePath) \\u003e 1 and self.servicePath[0] == ref:\\n\\t\\t\\treturn True\\n\\t\\treturn False\\n\\n\\tdef isPrevPathEqual(self, ref):\\n\\t\\tlength = len(self.servicePath)\\n\\t\\tif length \\u003e 1 and self.servicePath[length-2] == ref:\\n\\t\\t\\treturn True\\n\\t\\treturn False\\n\\n\\tdef preEnterPath(self, refstr):\\n\\t\\treturn False\\n\\n\\tdef showAllServices(self):\\n\\t\\tif not self.pathChangeDisabled:\\n\\t\\t\\trefstr = \'%s ORDER BY name\'%(self.service_types)\\n\\t\\t\\tif not self.preEnterPath(refstr):\\n\\t\\t\\t\\tref = eServiceReference(refstr)\\n\\t\\t\\t\\tcurrentRoot = self.getRoot()\\n\\t\\t\\t\\tif currentRoot is None or currentRoot != ref:\\n\\t\\t\\t\\t\\tself.clearPath()\\n\\t\\t\\t\\t\\tself.enterPath(ref)\\n\\t\\t\\t\\t\\tplayingref = self.session.nav.getCurrentlyPlayingServiceReference()\\n\\t\\t\\t\\t\\tif playingref:\\n\\t\\t\\t\\t\\t\\tself.setCurrentSelectionAlternative(playingref)\\n\\n\\tdef showSatellites(self, changeMode=False):\\n\\t\\tif not self.pathChangeDisabled:\\n\\t\\t\\trefstr = \'%s FROM SATELLITES ORDER BY satellitePosition\'%(self.service_types)\\n\\t\\t\\tif not self.preEnterPath(refstr):\\n\\t\\t\\t\\tref = eServiceReference(refstr)\\n\\t\\t\\t\\tjustSet=False\\n\\t\\t\\t\\tprev = None\\n\\n\\t\\t\\t\\tif self.isBasePathEqual(ref):\\n\\t\\t\\t\\t\\tif self.isPrevPathEqual(ref):\\n\\t\\t\\t\\t\\t\\tjustSet=True\\n\\t\\t\\t\\t\\tprev = self.pathUp(justSet)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tcurrentRoot = self.getRoot()\\n\\t\\t\\t\\t\\tif currentRoot is None or currentRoot != ref:\\n\\t\\t\\t\\t\\t\\tjustSet=True\\n\\t\\t\\t\\t\\t\\tself.clearPath()\\n\\t\\t\\t\\t\\t\\tself.enterPath(ref, True)\\n\\t\\t\\t\\t\\tif changeMode and currentRoot and currentRoot == ref:\\n\\t\\t\\t\\t\\t\\tself.showSatDetails = not self.showSatDetails\\n\\t\\t\\t\\t\\t\\tjustSet = True\\n\\t\\t\\t\\t\\t\\tself.clearPath()\\n\\t\\t\\t\\t\\t\\tself.enterPath(ref, True)\\n\\t\\t\\t\\tif justSet:\\n\\t\\t\\t\\t\\taddCableAndTerrestrialLater = []\\n\\t\\t\\t\\t\\tserviceHandler = eServiceCenter.getInstance()\\n\\t\\t\\t\\t\\tservicelist = serviceHandler.list(ref)\\n\\t\\t\\t\\t\\tif not servicelist is None:\\n\\t\\t\\t\\t\\t\\twhile True:\\n\\t\\t\\t\\t\\t\\t\\tservice = servicelist.getNext()\\n\\t\\t\\t\\t\\t\\t\\tif not service.valid(): #check if end of list\\n\\t\\t\\t\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\t\\t\\t\\tunsigned_orbpos = service.getUnsignedData(4) \\u003e\\u003e 16\\n\\t\\t\\t\\t\\t\\t\\torbpos = service.getData(4) \\u003e\\u003e 16\\n\\t\\t\\t\\t\\t\\t\\tif orbpos \\u003c 0:\\n\\t\\t\\t\\t\\t\\t\\t\\torbpos += 3600\\n\\t\\t\\t\\t\\t\\t\\tif \\"FROM PROVIDER\\" in service.getPath():\\n\\t\\t\\t\\t\\t\\t\\t\\tservice_type = self.showSatDetails and _(\\"Providers\\")\\n\\t\\t\\t\\t\\t\\t\\telif (\\"flags == %d\\" %(FLAG_SERVICE_NEW_FOUND)) in service.getPath():\\n\\t\\t\\t\\t\\t\\t\\t\\tservice_type = self.showSatDetails and _(\\"New\\")\\n\\t\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\t\\tservice_type = _(\\"Services\\")\\n\\t\\t\\t\\t\\t\\t\\tif service_type:\\n\\t\\t\\t\\t\\t\\t\\t\\tif unsigned_orbpos == 0xFFFF: #Cable\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tservice_name = _(\\"Cable\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\taddCableAndTerrestrialLater.append((\\"%s - %s\\" % (service_name, service_type), service.toString()))\\n\\t\\t\\t\\t\\t\\t\\t\\telif unsigned_orbpos == 0xEEEE: #Terrestrial\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tservice_name = _(\\"Terrestrial\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\taddCableAndTerrestrialLater.append((\\"%s - %s\\" % (service_name, service_type), service.toString()))\\n\\t\\t\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tservice_name = str(nimmanager.getSatDescription(orbpos))\\n\\t\\t\\t\\t\\t\\t\\t\\t\\texcept:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tif orbpos \\u003e 1800: # west\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\torbpos = 3600 - orbpos\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\th = _(\\"W\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\th = _(\\"E\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tservice_name = (\\"%d.%d\\" + h) % (orbpos / 10, orbpos % 10)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tservice.setName(\\"%s - %s\\" % (service_name, service_type))\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tself.servicelist.addService(service)\\n\\t\\t\\t\\t\\t\\tcur_ref = self.session.nav.getCurrentlyPlayingServiceReference()\\n\\t\\t\\t\\t\\t\\tself.servicelist.l.sort()\\n\\t\\t\\t\\t\\t\\tif cur_ref:\\n\\t\\t\\t\\t\\t\\t\\tpos = self.service_types.rfind(\':\')\\n\\t\\t\\t\\t\\t\\t\\trefstr = \'%s (channelID == %08x%04x%04x) \\u0026\\u0026 %s ORDER BY name\' %(self.service_types[:pos+1],\\n\\t\\t\\t\\t\\t\\t\\t\\tcur_ref.getUnsignedData(4), # NAMESPACE\\n\\t\\t\\t\\t\\t\\t\\t\\tcur_ref.getUnsignedData(2), # TSID\\n\\t\\t\\t\\t\\t\\t\\t\\tcur_ref.getUnsignedData(3), # ONID\\n\\t\\t\\t\\t\\t\\t\\t\\tself.service_types[pos+1:])\\n\\t\\t\\t\\t\\t\\t\\tref = eServiceReference(refstr)\\n\\t\\t\\t\\t\\t\\t\\tref.setName(_(\\"Current transponder\\"))\\n\\t\\t\\t\\t\\t\\t\\tself.servicelist.addService(ref, beforeCurrent=True)\\n\\t\\t\\t\\t\\t\\tfor (service_name, service_ref) in addCableAndTerrestrialLater:\\n\\t\\t\\t\\t\\t\\t\\tref = eServiceReference(service_ref)\\n\\t\\t\\t\\t\\t\\t\\tref.setName(service_name)\\n\\t\\t\\t\\t\\t\\t\\tself.servicelist.addService(ref, beforeCurrent=True)\\n\\t\\t\\t\\t\\t\\tself.servicelist.l.FillFinished()\\n\\t\\t\\t\\t\\t\\tif prev is not None:\\n\\t\\t\\t\\t\\t\\t\\tself.setCurrentSelection(prev)\\n\\t\\t\\t\\t\\t\\telif cur_ref:\\n\\t\\t\\t\\t\\t\\t\\trefstr = cur_ref.toString()\\n\\t\\t\\t\\t\\t\\t\\top = \\"\\".join(refstr.split(\':\', 10)[6:7])\\n\\t\\t\\t\\t\\t\\t\\tif len(op) \\u003e= 4:\\n\\t\\t\\t\\t\\t\\t\\t\\thop = int(op[:-4],16)\\n\\t\\t\\t\\t\\t\\t\\t\\tif len(op) \\u003e= 7 and not op.endswith(\'0000\'):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\top = op[:-4] + \'0000\'\\n\\t\\t\\t\\t\\t\\t\\t\\trefstr = \'1:7:0:0:0:0:%s:0:0:0:(satellitePosition == %s) \\u0026\\u0026 %s ORDER BY name\' % (op, hop, self.service_types[self.service_types.rfind(\':\')+1:])\\n\\t\\t\\t\\t\\t\\t\\t\\tself.setCurrentSelectionAlternative(eServiceReference(refstr))\\n\\n\\tdef showProviders(self):\\n\\t\\tif not self.pathChangeDisabled:\\n\\t\\t\\trefstr = \'%s FROM PROVIDERS ORDER BY name\'%(self.service_types)\\n\\t\\t\\tif not self.preEnterPath(refstr):\\n\\t\\t\\t\\tref = eServiceReference(refstr)\\n\\t\\t\\t\\tif self.isBasePathEqual(ref):\\n\\t\\t\\t\\t\\tself.pathUp()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tcurrentRoot = self.getRoot()\\n\\t\\t\\t\\t\\tif currentRoot is None or currentRoot != ref:\\n\\t\\t\\t\\t\\t\\tself.clearPath()\\n\\t\\t\\t\\t\\t\\tself.enterPath(ref)\\n\\t\\t\\t\\t\\t\\tservice = self.session.nav.getCurrentService()\\n\\t\\t\\t\\t\\t\\tif service:\\n\\t\\t\\t\\t\\t\\t\\tinfo = service.info()\\n\\t\\t\\t\\t\\t\\t\\tif info:\\n\\t\\t\\t\\t\\t\\t\\t\\tprovider = info.getInfoString(iServiceInformation.sProvider)\\n\\t\\t\\t\\t\\t\\t\\t\\trefstr = \'1:7:0:0:0:0:0:0:0:0:(provider == \\\\\\"%s\\\\\\") \\u0026\\u0026 %s ORDER BY name:%s\' % (provider, self.service_types[self.service_types.rfind(\':\')+1:],provider)\\n\\t\\t\\t\\t\\t\\t\\t\\tself.setCurrentSelectionAlternative(eServiceReference(refstr))\\n\\n\\tdef changeBouquet(self, direction):\\n\\t\\tif not self.pathChangeDisabled:\\n\\t\\t\\tif len(self.servicePath) \\u003e 1:\\n\\t\\t\\t\\t#when enter satellite root list we must do some magic stuff..\\n\\t\\t\\t\\tref = eServiceReference(\'%s FROM SATELLITES ORDER BY satellitePosition\'%(self.service_types))\\n\\t\\t\\t\\tif self.isBasePathEqual(ref):\\n\\t\\t\\t\\t\\tself.showSatellites()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself.pathUp()\\n\\t\\t\\t\\tif direction \\u003c 0:\\n\\t\\t\\t\\t\\tself.moveUp()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself.moveDown()\\n\\t\\t\\t\\tref = self.getCurrentSelection()\\n\\t\\t\\t\\tself.enterPath(ref)\\n\\n\\tdef inBouquet(self):\\n\\t\\tif self.servicePath and self.servicePath[0] == self.bouquet_root:\\n\\t\\t\\treturn True\\n\\t\\treturn False\\n\\n\\tdef atBegin(self):\\n\\t\\treturn self.servicelist.atBegin()\\n\\n\\tdef atEnd(self):\\n\\t\\treturn self.servicelist.atEnd()\\n\\n\\tdef nextBouquet(self):\\n\\t\\tif self.shown and config.usage.oldstyle_channel_select_controls.value:\\n\\t\\t\\tself.servicelist.instance.moveSelection(self.servicelist.instance.pageUp)\\n\\t\\telif \\"reverseB\\" in config.usage.servicelist_cursor_behavior.value:\\n\\t\\t\\tself.changeBouquet(-1)\\n\\t\\telse:\\n\\t\\t\\tself.changeBouquet(+1)\\n\\n\\tdef prevBouquet(self):\\n\\t\\tif self.shown and config.usage.oldstyle_channel_select_controls.value:\\n\\t\\t\\tself.servicelist.instance.moveSelection(self.servicelist.instance.pageDown)\\n\\t\\telif \\"reverseB\\" in config.usage.servicelist_cursor_behavior.value:\\n\\t\\t\\tself.changeBouquet(+1)\\n\\t\\telse:\\n\\t\\t\\tself.changeBouquet(-1)\\n\\n\\tdef keyLeft(self):\\n\\t\\tif config.usage.oldstyle_channel_select_controls.value:\\n\\t\\t\\tself.changeBouquet(-1)\\n\\t\\telse:\\n\\t\\t\\tself.servicelist.instance.moveSelection(self.servicelist.instance.pageUp)\\n\\n\\tdef keyRight(self):\\n\\t\\tif config.usage.oldstyle_channel_select_controls.value:\\n\\t\\t\\tself.changeBouquet(+1)\\n\\t\\telse:\\n\\t\\t\\tself.servicelist.instance.moveSelection(self.servicelist.instance.pageDown)\\n\\n\\tdef keyRecord(self):\\n\\t\\tref = self.getCurrentSelection()\\n\\t\\tif ref and not(ref.flags \\u0026 (eServiceReference.isMarker|eServiceReference.isDirectory)):\\n\\t\\t\\tScreens.InfoBar.InfoBar.instance.instantRecord(serviceRef=ref)\\n\\n\\tdef showFavourites(self):\\n\\t\\tif not self.pathChangeDisabled:\\n\\t\\t\\tif not self.preEnterPath(self.bouquet_rootstr):\\n\\t\\t\\t\\tif self.isBasePathEqual(self.bouquet_root):\\n\\t\\t\\t\\t\\tself.pathUp()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tcurrentRoot = self.getRoot()\\n\\t\\t\\t\\t\\tif currentRoot is None or currentRoot != self.bouquet_root:\\n\\t\\t\\t\\t\\t\\tself.clearPath()\\n\\t\\t\\t\\t\\t\\tself.enterPath(self.bouquet_root)\\n\\n\\tdef keyNumber0(self, number):\\n\\t\\tif len(self.servicePath) \\u003e 1 and not self.selectionNumber:\\n\\t\\t\\tself.keyGoUp()\\n\\t\\telse:\\n\\t\\t\\tself.keyNumberGlobal(number)\\n\\n\\tdef keyNumberGlobal(self, number):\\n\\t\\tif self.isBasePathEqual(self.bouquet_root):\\n\\t\\t\\tif hasattr(self, \\"editMode\\") and self.editMode:\\n\\t\\t\\t\\tif number == 2:\\n\\t\\t\\t\\t\\tself.renameEntry()\\n\\t\\t\\t\\tif number == 6:\\n\\t\\t\\t\\t\\tself.toggleMoveMode(select=True)\\n\\t\\t\\t\\tif number == 8:\\n\\t\\t\\t\\t\\tself.removeCurrentEntry(bouquet=False)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.numberSelectionActions(number)\\n\\t\\telse:\\n\\t\\t\\tcurrent_root = self.getRoot()\\n\\t\\t\\tif  current_root and \'FROM BOUQUET \\"bouquets.\' in current_root.getPath():\\n\\t\\t\\t\\tif hasattr(self, \\"editMode\\") and self.editMode:\\n\\t\\t\\t\\t\\tif number == 2:\\n\\t\\t\\t\\t\\t\\tself.renameEntry()\\n\\t\\t\\t\\t\\tif number == 6:\\n\\t\\t\\t\\t\\t\\tself.toggleMoveMode(select=True)\\n\\t\\t\\t\\t\\tif number == 8:\\n\\t\\t\\t\\t\\t\\tself.removeCurrentEntry(bouquet=True)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself.numberSelectionActions(number)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tunichar = self.numericalTextInput.getKey(number)\\n\\t\\t\\t\\tcharstr = unichar.encode(\\"utf-8\\")\\n\\t\\t\\t\\tif len(charstr) == 1:\\n\\t\\t\\t\\t\\tself.servicelist.moveToChar(charstr[0])\\n\\n\\tdef numberSelectionActions(self, number):\\n\\t\\tif not(hasattr(self, \\"movemode\\") and self.movemode):\\n\\t\\t\\tif len(self.selectionNumber)\\u003e4:\\n\\t\\t\\t\\tself.clearNumberSelectionNumber()\\n\\t\\t\\tself.selectionNumber = self.selectionNumber + str(number)\\n\\t\\t\\tref, bouquet = Screens.InfoBar.InfoBar.instance.searchNumber(int(self.selectionNumber), bouquet=self.getRoot())\\n\\t\\t\\tif ref:\\n\\t\\t\\t\\tif not ref.flags \\u0026 eServiceReference.isMarker:\\n\\t\\t\\t\\t\\tself.enterUserbouquet(bouquet, save_root=False)\\n\\t\\t\\t\\t\\tself.setCurrentSelection(ref)\\n\\t\\t\\t\\tself.clearNumberSelectionNumberTimer.start(1000, True)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.clearNumberSelectionNumber()\\n\\n\\tdef clearNumberSelectionNumber(self):\\n\\t\\tself.clearNumberSelectionNumberTimer.stop()\\n\\t\\tself.selectionNumber = \\"\\"\\n\\n\\tdef keyAsciiCode(self):\\n\\t\\tunichar = unichr(getPrevAsciiCode())\\n\\t\\tcharstr = unichar.encode(\\"utf-8\\")\\n\\t\\tif len(charstr) == 1:\\n\\t\\t\\tself.servicelist.moveToChar(charstr[0])\\n\\n\\tdef getRoot(self):\\n\\t\\treturn self.servicelist.getRoot()\\n\\n\\tdef getCurrentSelection(self):\\n\\t\\treturn self.servicelist.getCurrent()\\n\\n\\tdef setCurrentSelection(self, service):\\n\\t\\tif service:\\n\\t\\t\\tself.servicelist.setCurrent(service, adjust=False)\\n\\n\\tdef setCurrentSelectionAlternative(self, ref):\\n\\t\\tif self.bouquet_mark_edit == EDIT_ALTERNATIVES and not (ref.flags \\u0026 eServiceReference.isDirectory):\\n\\t\\t\\tfor markedService in self.servicelist.getMarked():\\n\\t\\t\\t\\tmarkedService = eServiceReference(markedService)\\n\\t\\t\\t\\tself.setCurrentSelection(markedService)\\n\\t\\t\\t\\tif markedService == self.getCurrentSelection():\\n\\t\\t\\t\\t\\treturn\\n\\t\\tself.setCurrentSelection(ref)\\n\\n\\tdef getBouquetList(self):\\n\\t\\tbouquets = [ ]\\n\\t\\tserviceHandler = eServiceCenter.getInstance()\\n\\t\\tif config.usage.multibouquet.value:\\n\\t\\t\\tlist = serviceHandler.list(self.bouquet_root)\\n\\t\\t\\tif list:\\n\\t\\t\\t\\twhile True:\\n\\t\\t\\t\\t\\ts = list.getNext()\\n\\t\\t\\t\\t\\tif not s.valid():\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\t\\tif s.flags \\u0026 eServiceReference.isDirectory and not s.flags \\u0026 eServiceReference.isInvisible:\\n\\t\\t\\t\\t\\t\\tinfo = serviceHandler.info(s)\\n\\t\\t\\t\\t\\t\\tif info:\\n\\t\\t\\t\\t\\t\\t\\tbouquets.append((info.getName(s), s))\\n\\t\\t\\t\\treturn bouquets\\n\\t\\telse:\\n\\t\\t\\tinfo = serviceHandler.info(self.bouquet_root)\\n\\t\\t\\tif info:\\n\\t\\t\\t\\tbouquets.append((info.getName(self.bouquet_root), self.bouquet_root))\\n\\t\\t\\treturn bouquets\\n\\t\\treturn None\\n\\n\\tdef keyGoUp(self):\\n\\t\\tif len(self.servicePath) \\u003e 1:\\n\\t\\t\\tif self.isBasePathEqual(self.bouquet_root):\\n\\t\\t\\t\\tself.showFavourites()\\n\\t\\t\\telse:\\n\\t\\t\\t\\tref = eServiceReference(\'%s FROM SATELLITES ORDER BY satellitePosition\'%(self.service_types))\\n\\t\\t\\t\\tif self.isBasePathEqual(ref):\\n\\t\\t\\t\\t\\tself.showSatellites()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tref = eServiceReference(\'%s FROM PROVIDERS ORDER BY name\'%(self.service_types))\\n\\t\\t\\t\\t\\tif self.isBasePathEqual(ref):\\n\\t\\t\\t\\t\\t\\tself.showProviders()\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tself.showAllServices()\\n\\n\\tdef nextMarker(self):\\n\\t\\tself.servicelist.moveToNextMarker()\\n\\n\\tdef prevMarker(self):\\n\\t\\tself.servicelist.moveToPrevMarker()\\n\\n\\tdef gotoCurrentServiceOrProvider(self, ref):\\n\\t\\tstr = ref.toString()\\n\\t\\tif _(\\"Providers\\") in str:\\n\\t\\t\\tservice = self.session.nav.getCurrentService()\\n\\t\\t\\tif service:\\n\\t\\t\\t\\tinfo = service.info()\\n\\t\\t\\t\\tif info:\\n\\t\\t\\t\\t\\tprovider = info.getInfoString(iServiceInformation.sProvider)\\n\\t\\t\\t\\t\\top = int(self.session.nav.getCurrentlyPlayingServiceOrGroup().toString().split(\':\')[6][:-4] or \\"0\\",16)\\n\\t\\t\\t\\t\\trefstr = \'1:7:0:0:0:0:0:0:0:0:(provider == \\\\\\"%s\\\\\\") \\u0026\\u0026 (satellitePosition == %s) \\u0026\\u0026 %s ORDER BY name:%s\' % (provider, op, self.service_types[self.service_types.rfind(\':\')+1:],provider)\\n\\t\\t\\t\\t\\tself.setCurrentSelection(eServiceReference(refstr))\\n\\t\\telif not self.isBasePathEqual(self.bouquet_root) or self.bouquet_mark_edit == EDIT_ALTERNATIVES:\\n\\t\\t\\tplayingref = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\t\\tif playingref:\\n\\t\\t\\t\\tself.setCurrentSelectionAlternative(playingref)\\n\\nHISTORYSIZE = 20\\n\\n#config for lastservice\\nconfig.tv = ConfigSubsection()\\nconfig.tv.lastservice = ConfigText()\\nconfig.tv.lastroot = ConfigText()\\nconfig.radio = ConfigSubsection()\\nconfig.radio.lastservice = ConfigText()\\nconfig.radio.lastroot = ConfigText()\\nconfig.servicelist = ConfigSubsection()\\nconfig.servicelist.lastmode = ConfigText(default = \\"tv\\")\\nconfig.servicelist.startupservice = ConfigText()\\nconfig.servicelist.startupservice_onstandby = ConfigYesNo(default = False)\\nconfig.servicelist.startuproot = ConfigText()\\nconfig.servicelist.startupmode = ConfigText(default = \\"tv\\")\\n\\nclass ChannelSelection(ChannelSelectionBase, ChannelSelectionEdit, ChannelSelectionEPG, SelectionEventInfo):\\n\\tdef __init__(self, session):\\n\\t\\tChannelSelectionBase.__init__(self,session)\\n\\t\\tChannelSelectionEdit.__init__(self)\\n\\t\\tChannelSelectionEPG.__init__(self)\\n\\t\\tSelectionEventInfo.__init__(self)\\n\\n\\t\\tself[\\"actions\\"] = ActionMap([\\"OkCancelActions\\", \\"TvRadioActions\\"],\\n\\t\\t\\t{\\n\\t\\t\\t\\t\\"cancel\\": self.cancel,\\n\\t\\t\\t\\t\\"ok\\": self.channelSelected,\\n\\t\\t\\t\\t\\"keyRadio\\": self.doRadioButton,\\n\\t\\t\\t\\t\\"keyTV\\": self.doTVButton,\\n\\t\\t\\t})\\n\\n\\t\\tself.__event_tracker = ServiceEventTracker(screen=self, eventmap=\\n\\t\\t\\t{\\n\\t\\t\\t\\tiPlayableService.evStart: self.__evServiceStart,\\n\\t\\t\\t\\tiPlayableService.evEnd: self.__evServiceEnd\\n\\t\\t\\t})\\n\\n\\t\\tself.startServiceRef = None\\n\\n\\t\\tself.history = [ ]\\n\\t\\tself.history_pos = 0\\n\\n\\t\\tif config.servicelist.startupservice.value and config.servicelist.startuproot.value:\\n\\t\\t\\tconfig.servicelist.lastmode.value = config.servicelist.startupmode.value\\n\\t\\t\\tif config.servicelist.lastmode.value == \\"tv\\":\\n\\t\\t\\t\\tconfig.tv.lastservice.value = config.servicelist.startupservice.value\\n\\t\\t\\t\\tconfig.tv.lastroot.value = config.servicelist.startuproot.value\\n\\t\\t\\telif config.servicelist.lastmode.value == \\"radio\\":\\n\\t\\t\\t\\tconfig.radio.lastservice.value = config.servicelist.startupservice.value\\n\\t\\t\\t\\tconfig.radio.lastroot.value = config.servicelist.startuproot.value\\n\\n\\t\\tself.lastservice = config.tv.lastservice\\n\\t\\tself.lastroot = config.tv.lastroot\\n\\t\\tself.revertMode = None\\n\\t\\tconfig.usage.multibouquet.addNotifier(self.multibouquet_config_changed)\\n\\t\\tself.new_service_played = False\\n\\t\\tself.dopipzap = False\\n\\t\\tself.onExecBegin.append(self.asciiOn)\\n\\t\\tself.mainScreenMode = None\\n\\t\\tself.mainScreenRoot = None\\n\\n\\t\\tself.lastChannelRootTimer = eTimer()\\n\\t\\tself.lastChannelRootTimer.callback.append(self.__onCreate)\\n\\t\\tself.lastChannelRootTimer.start(100,True)\\n\\t\\tself.pipzaptimer = eTimer()\\n\\n\\tdef asciiOn(self):\\n\\t\\trcinput = eRCInput.getInstance()\\n\\t\\trcinput.setKeyboardMode(rcinput.kmAscii)\\n\\n\\tdef asciiOff(self):\\n\\t\\trcinput = eRCInput.getInstance()\\n\\t\\trcinput.setKeyboardMode(rcinput.kmNone)\\n\\n\\tdef multibouquet_config_changed(self, val):\\n\\t\\tself.recallBouquetMode()\\n\\n\\tdef __evServiceStart(self):\\n\\t\\tif self.dopipzap and hasattr(self.session, \'pip\'):\\n\\t\\t\\tself.servicelist.setPlayableIgnoreService(self.session.pip.getCurrentServiceReference() or eServiceReference())\\n\\t\\telse:\\n\\t\\t\\tservice = self.session.nav.getCurrentService()\\n\\t\\t\\tif service:\\n\\t\\t\\t\\tinfo = service.info()\\n\\t\\t\\t\\tif info:\\n\\t\\t\\t\\t\\trefstr = info.getInfoString(iServiceInformation.sServiceref)\\n\\t\\t\\t\\t\\tself.servicelist.setPlayableIgnoreService(eServiceReference(refstr))\\n\\n\\tdef __evServiceEnd(self):\\n\\t\\tself.servicelist.setPlayableIgnoreService(eServiceReference())\\n\\n\\tdef setMode(self):\\n\\t\\tself.rootChanged = True\\n\\t\\tself.restoreRoot()\\n\\t\\tlastservice = eServiceReference(self.lastservice.value)\\n\\t\\tif lastservice.valid():\\n\\t\\t\\tself.setCurrentSelection(lastservice)\\n\\n\\tdef doTVButton(self):\\n\\t\\tif self.mode == MODE_TV:\\n\\t\\t\\tself.channelSelected(doClose = False)\\n\\t\\telse:\\n\\t\\t\\tself.setModeTv()\\n\\n\\tdef setModeTv(self):\\n\\t\\tif self.revertMode is None:\\n\\t\\t\\tself.revertMode = self.mode\\n\\t\\tself.lastservice = config.tv.lastservice\\n\\t\\tself.lastroot = config.tv.lastroot\\n\\t\\tconfig.servicelist.lastmode.value = \\"tv\\"\\n\\t\\tself.setTvMode()\\n\\t\\tself.setMode()\\n\\n\\tdef doRadioButton(self):\\n\\t\\tif self.mode == MODE_RADIO:\\n\\t\\t\\tself.channelSelected(doClose=False)\\n\\t\\telse:\\n\\t\\t\\tself.setModeRadio()\\n\\n\\tdef setModeRadio(self):\\n\\t\\tif self.revertMode is None:\\n\\t\\t\\tself.revertMode = self.mode\\n\\t\\tif config.usage.e1like_radio_mode.value:\\n\\t\\t\\tself.lastservice = config.radio.lastservice\\n\\t\\t\\tself.lastroot = config.radio.lastroot\\n\\t\\t\\tconfig.servicelist.lastmode.value = \\"radio\\"\\n\\t\\t\\tself.setRadioMode()\\n\\t\\t\\tself.setMode()\\n\\n\\tdef __onCreate(self):\\n\\t\\tif config.usage.e1like_radio_mode.value:\\n\\t\\t\\tif config.servicelist.lastmode.value == \\"tv\\":\\n\\t\\t\\t\\tself.setModeTv()\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.setModeRadio()\\n\\t\\telse:\\n\\t\\t\\tself.setModeTv()\\n\\t\\tlastservice = eServiceReference(self.lastservice.value)\\n\\t\\tif lastservice.valid():\\n\\t\\t\\tself.zap()\\n\\n\\tdef channelSelected(self, doClose = True):\\n\\t\\tplayingref = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\tif config.usage.channelselection_preview.value and (playingref is None or self.getCurrentSelection() and self.getCurrentSelection() != playingref):\\n\\t\\t\\tdoClose = False\\n\\t\\tif not self.startServiceRef and not doClose:\\n\\t\\t\\tself.startServiceRef = playingref\\n\\t\\tref = self.getCurrentSelection()\\n\\t\\tif self.movemode and (self.isBasePathEqual(self.bouquet_root) or \\"userbouquet.\\" in ref.toString()):\\n\\t\\t\\tself.toggleMoveMarked()\\n\\t\\telif (ref.flags \\u0026 eServiceReference.flagDirectory) == eServiceReference.flagDirectory:\\n\\t\\t\\tif Components.ParentalControl.parentalControl.isServicePlayable(ref, self.bouquetParentalControlCallback, self.session):\\n\\t\\t\\t\\tself.enterPath(ref)\\n\\t\\t\\t\\tself.gotoCurrentServiceOrProvider(ref)\\n\\t\\t\\t\\tself.revertMode = None\\n\\t\\telif self.bouquet_mark_edit != OFF:\\n\\t\\t\\tif not (self.bouquet_mark_edit == EDIT_ALTERNATIVES and ref.flags \\u0026 eServiceReference.isGroup):\\n\\t\\t\\t\\tself.doMark()\\n\\t\\telif not (ref.flags \\u0026 eServiceReference.isMarker or ref.type == -1):\\n\\t\\t\\troot = self.getRoot()\\n\\t\\t\\tif not root or not (root.flags \\u0026 eServiceReference.isGroup):\\n\\t\\t\\t\\tself.zap(enable_pipzap=doClose, preview_zap=not doClose)\\n\\t\\t\\t\\tself.asciiOff()\\n\\t\\t\\t\\tif doClose:\\n\\t\\t\\t\\t\\tif self.dopipzap:\\n\\t\\t\\t\\t\\t\\tself.zapBack()\\n\\t\\t\\t\\t\\tself.startServiceRef = None\\n\\t\\t\\t\\t\\tself.startRoot = None\\n\\t\\t\\t\\t\\tself.correctChannelNumber()\\n\\t\\t\\t\\t\\tself.movemode and self.toggleMoveMode()\\n\\t\\t\\t\\t\\tself.editMode = False\\n\\t\\t\\t\\t\\tself.protectContextMenu = True\\n\\t\\t\\t\\t\\tself.close(ref)\\n\\n\\tdef bouquetParentalControlCallback(self, ref):\\n\\t\\tself.enterPath(ref)\\n\\t\\tself.gotoCurrentServiceOrProvider(ref)\\n\\t\\tself.revertMode = None\\n\\n\\tdef togglePipzap(self):\\n\\t\\tassert(self.session.pip)\\n\\t\\ttitle = self.instance.getTitle()\\n\\t\\tpos = title.find(\\" (\\")\\n\\t\\tif pos != -1:\\n\\t\\t\\ttitle = title[:pos]\\n\\t\\tif self.dopipzap:\\n\\t\\t\\t# Mark PiP as inactive and effectively deactivate pipzap\\n\\t\\t\\tself.hidePipzapMessage()\\n\\t\\t\\tself.dopipzap = False\\n\\n\\t\\t\\t# Disable PiP if not playing a service\\n\\t\\t\\tif self.session.pip.pipservice is None:\\n\\t\\t\\t\\tself.session.pipshown = False\\n\\t\\t\\t\\tdel self.session.pip\\n\\t\\t\\tself.__evServiceStart()\\n\\t\\t\\t# Move to playing service\\n\\t\\t\\tlastservice = eServiceReference(self.lastservice.value)\\n\\t\\t\\tif lastservice.valid() and self.getCurrentSelection() != lastservice:\\n\\t\\t\\t\\tself.setCurrentSelection(lastservice)\\n\\t\\t\\t\\tif self.getCurrentSelection() != lastservice:\\n\\t\\t\\t\\t\\tself.servicelist.setCurrent(lastservice)\\n\\n\\t\\t\\ttitle += _(\\" (TV)\\")\\n\\t\\telse:\\n\\t\\t\\t# Mark PiP as active and effectively active pipzap\\n\\t\\t\\tself.showPipzapMessage()\\n\\t\\t\\tself.dopipzap = True\\n\\t\\t\\tself.__evServiceStart()\\n\\t\\t\\t# Move to service playing in pip (will not work with subservices)\\n\\t\\t\\tself.setCurrentSelection(self.session.pip.getCurrentService())\\n\\n\\t\\t\\ttitle += _(\\" (PiP)\\")\\n\\t\\tself.setTitle(title)\\n\\t\\tself.buildTitleString()\\n\\n\\tdef showPipzapMessage(self):\\n\\t\\ttime = config.usage.infobar_timeout.index\\n\\t\\tif time:\\n\\t\\t\\tself.pipzaptimer.callback.append(self.hidePipzapMessage)\\n\\t\\t\\tself.pipzaptimer.startLongTimer(time)\\n\\t\\tself.session.pip.active()\\n\\n\\tdef hidePipzapMessage(self):\\n\\t\\tif self.pipzaptimer.isActive():\\n\\t\\t\\tself.pipzaptimer.callback.remove(self.hidePipzapMessage)\\n\\t\\t\\tself.pipzaptimer.stop()\\n\\t\\tself.session.pip.inactive()\\n\\n\\t#called from infoBar and channelSelected\\n\\tdef zap(self, enable_pipzap=False, preview_zap=False, checkParentalControl=True, ref=None):\\n\\t\\tself.curRoot = self.startRoot\\n\\t\\tnref = ref or self.getCurrentSelection()\\n\\t\\tref = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\tif enable_pipzap and self.dopipzap:\\n\\t\\t\\tref = self.session.pip.getCurrentService()\\n\\t\\t\\tif ref is None or ref != nref:\\n\\t\\t\\t\\tnref = self.session.pip.resolveAlternatePipService(nref)\\n\\t\\t\\t\\tif nref and (not checkParentalControl or Components.ParentalControl.parentalControl.isServicePlayable(nref, boundFunction(self.zap, enable_pipzap=True, checkParentalControl=False))):\\n\\t\\t\\t\\t\\tself.session.pip.playService(nref)\\n\\t\\t\\t\\t\\tself.__evServiceStart()\\n\\t\\t\\t\\t\\tself.showPipzapMessage()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself.setStartRoot(self.curRoot)\\n\\t\\t\\t\\t\\tself.setCurrentSelection(ref)\\n\\t\\telif ref is None or ref != nref:\\n\\t\\t\\tScreens.InfoBar.InfoBar.instance.checkTimeshiftRunning(boundFunction(self.zapCheckTimeshiftCallback, enable_pipzap, preview_zap, nref))\\n\\t\\telif not preview_zap:\\n\\t\\t\\tself.saveRoot()\\n\\t\\t\\tself.saveChannel(nref)\\n\\t\\t\\tconfig.servicelist.lastmode.save()\\n\\t\\t\\tself.setCurrentSelection(nref)\\n\\t\\t\\tif self.startServiceRef is None or nref != self.startServiceRef:\\n\\t\\t\\t\\tself.addToHistory(nref)\\n\\t\\t\\tself.rootChanged = False\\n\\t\\t\\tself.revertMode = None\\n\\n\\tdef zapCheckTimeshiftCallback(self, enable_pipzap, preview_zap, nref, answer):\\n\\t\\tif answer:\\n\\t\\t\\tself.new_service_played = True\\n\\t\\t\\tself.session.nav.playService(nref)\\n\\t\\t\\tif not preview_zap:\\n\\t\\t\\t\\tself.saveRoot()\\n\\t\\t\\t\\tself.saveChannel(nref)\\n\\t\\t\\t\\tconfig.servicelist.lastmode.save()\\n\\t\\t\\t\\tif self.startServiceRef is None or nref != self.startServiceRef:\\n\\t\\t\\t\\t\\tself.addToHistory(nref)\\n\\t\\t\\t\\tif self.dopipzap:\\n\\t\\t\\t\\t\\tself.setCurrentSelection(self.session.pip.getCurrentService())\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself.mainScreenMode = config.servicelist.lastmode.value\\n\\t\\t\\t\\t\\tself.mainScreenRoot = self.getRoot()\\n\\t\\t\\t\\tself.revertMode = None\\n\\t\\t\\telse:\\n\\t\\t\\t\\tNotifications.RemovePopup(\\"Parental control\\")\\n\\t\\t\\t\\tself.setCurrentSelection(nref)\\n\\t\\telse:\\n\\t\\t\\tself.setStartRoot(self.curRoot)\\n\\t\\t\\tself.setCurrentSelection(self.session.nav.getCurrentlyPlayingServiceOrGroup())\\n\\t\\tif not preview_zap:\\n\\t\\t\\tself.hide()\\n\\n\\tdef newServicePlayed(self):\\n\\t\\tret = self.new_service_played\\n\\t\\tself.new_service_played = False\\n\\t\\treturn ret\\n\\n\\tdef addToHistory(self, ref):\\n\\t\\tif self.servicePath is not None:\\n\\t\\t\\ttmp=self.servicePath[:]\\n\\t\\t\\ttmp.append(ref)\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tdel self.history[self.history_pos+1:]\\n\\t\\t\\texcept:\\n\\t\\t\\t\\tpass\\n\\t\\t\\tself.history.append(tmp)\\n\\t\\t\\thlen = len(self.history)\\n\\t\\t\\tif hlen \\u003e HISTORYSIZE:\\n\\t\\t\\t\\tdel self.history[0]\\n\\t\\t\\t\\thlen -= 1\\n\\t\\t\\tself.history_pos = hlen-1\\n\\n\\tdef historyBack(self):\\n\\t\\thlen = len(self.history)\\n\\t\\tcurrentPlayedRef = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\tif hlen \\u003e 0 and currentPlayedRef and self.history[self.history_pos][-1] != currentPlayedRef:\\n\\t\\t\\tself.addToHistory(currentPlayedRef)\\n\\t\\t\\thlen = len(self.history)\\n\\t\\tif hlen \\u003e 1 and self.history_pos \\u003e 0:\\n\\t\\t\\tself.history_pos -= 1\\n\\t\\t\\tself.setHistoryPath()\\n\\n\\tdef historyNext(self):\\n\\t\\thlen = len(self.history)\\n\\t\\tif hlen \\u003e 1 and self.history_pos \\u003c (hlen-1):\\n\\t\\t\\tself.history_pos += 1\\n\\t\\t\\tself.setHistoryPath()\\n\\n\\tdef setHistoryPath(self, doZap=True):\\n\\t\\tpath = self.history[self.history_pos][:]\\n\\t\\tref = path.pop()\\n\\t\\tdel self.servicePath[:]\\n\\t\\tself.servicePath += path\\n\\t\\tself.saveRoot()\\n\\t\\troot = path[-1]\\n\\t\\tcur_root = self.getRoot()\\n\\t\\tif cur_root and cur_root != root:\\n\\t\\t\\tself.setRoot(root)\\n\\t\\tif doZap:\\n\\t\\t\\tself.session.nav.playService(ref, adjust=False)\\n\\t\\tif self.dopipzap:\\n\\t\\t\\tself.setCurrentSelection(self.session.pip.getCurrentService())\\n\\t\\telse:\\n\\t\\t\\tself.setCurrentSelection(ref)\\n\\t\\tself.saveChannel(ref)\\n\\n\\tdef saveRoot(self):\\n\\t\\tpath = \'\'\\n\\t\\tfor i in self.servicePath:\\n\\t\\t\\tpath += i.toString()\\n\\t\\t\\tpath += \';\'\\n\\t\\tif path and path != self.lastroot.value:\\n\\t\\t\\tif self.mode == MODE_RADIO and \'FROM BOUQUET \\"bouquets.tv\\"\' in path:\\n\\t\\t\\t\\tself.setModeTv()\\n\\t\\t\\telif self.mode == MODE_TV and \'FROM BOUQUET \\"bouquets.radio\\"\' in path:\\n\\t\\t\\t\\tself.setModeRadio()\\n\\t\\t\\tself.lastroot.value = path\\n\\t\\t\\tself.lastroot.save()\\n\\n\\tdef restoreRoot(self):\\n\\t\\ttmp = [x for x in self.lastroot.value.split(\';\') if x != \'\']\\n\\t\\tcurrent = [x.toString() for x in self.servicePath]\\n\\t\\tif tmp != current or self.rootChanged:\\n\\t\\t\\tself.clearPath()\\n\\t\\t\\tcnt = 0\\n\\t\\t\\tfor i in tmp:\\n\\t\\t\\t\\tself.servicePath.append(eServiceReference(i))\\n\\t\\t\\t\\tcnt += 1\\n\\t\\t\\tif cnt:\\n\\t\\t\\t\\tpath = self.servicePath.pop()\\n\\t\\t\\t\\tself.enterPath(path)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.showFavourites()\\n\\t\\t\\t\\tself.saveRoot()\\n\\t\\t\\tself.rootChanged = False\\n\\n\\tdef preEnterPath(self, refstr):\\n\\t\\tif self.servicePath and self.servicePath[0] != eServiceReference(refstr):\\n\\t\\t\\tpathstr = self.lastroot.value\\n\\t\\t\\tif pathstr is not None and refstr in pathstr:\\n\\t\\t\\t\\tself.restoreRoot()\\n\\t\\t\\t\\tlastservice = eServiceReference(self.lastservice.value)\\n\\t\\t\\t\\tif lastservice.valid():\\n\\t\\t\\t\\t\\tself.setCurrentSelection(lastservice)\\n\\t\\t\\t\\treturn True\\n\\t\\treturn False\\n\\n\\tdef saveChannel(self, ref):\\n\\t\\tif ref is not None:\\n\\t\\t\\trefstr = ref.toString()\\n\\t\\telse:\\n\\t\\t\\trefstr = \\"\\"\\n\\t\\tif refstr != self.lastservice.value and not Components.ParentalControl.parentalControl.isProtected(ref):\\n\\t\\t\\tself.lastservice.value = refstr\\n\\t\\t\\tself.lastservice.save()\\n\\n\\tdef setCurrentServicePath(self, path, doZap=True):\\n\\t\\thlen = len(self.history)\\n\\t\\tif not hlen:\\n\\t\\t\\tself.history.append(path)\\n\\t\\t\\tself.history_pos = 0\\n\\t\\tif hlen == 1:\\n\\t\\t\\tself.history[self.history_pos] = path\\n\\t\\telse:\\n\\t\\t\\tif path in self.history:\\n\\t\\t\\t\\tself.history.remove(path)\\n\\t\\t\\t\\tself.history_pos -= 1\\n\\t\\t\\ttmp = self.history[self.history_pos][:]\\n\\t\\t\\tself.history.append(tmp)\\n\\t\\t\\tself.history_pos += 1\\n\\t\\t\\tself.history[self.history_pos] = path\\n\\t\\tself.setHistoryPath(doZap)\\n\\n\\tdef getCurrentServicePath(self):\\n\\t\\tif self.history:\\n\\t\\t\\treturn self.history[self.history_pos]\\n\\t\\treturn None\\n\\n\\tdef recallPrevService(self):\\n\\t\\thlen = len(self.history)\\n\\t\\tcurrentPlayedRef = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\tif hlen \\u003e 0 and currentPlayedRef and self.history[self.history_pos][-1] != currentPlayedRef:\\n\\t\\t\\tself.addToHistory(currentPlayedRef)\\n\\t\\t\\thlen = len(self.history)\\n\\t\\tif hlen \\u003e 1:\\n\\t\\t\\tif self.history_pos == hlen-1:\\n\\t\\t\\t\\ttmp = self.history[self.history_pos]\\n\\t\\t\\t\\tself.history[self.history_pos] = self.history[self.history_pos-1]\\n\\t\\t\\t\\tself.history[self.history_pos-1] = tmp\\n\\t\\t\\telse:\\n\\t\\t\\t\\ttmp = self.history[self.history_pos+1]\\n\\t\\t\\t\\tself.history[self.history_pos+1] = self.history[self.history_pos]\\n\\t\\t\\t\\tself.history[self.history_pos] = tmp\\n\\t\\t\\tself.setHistoryPath()\\n\\n\\tdef cancel(self):\\n\\t\\tif self.revertMode is None:\\n\\t\\t\\tself.restoreRoot()\\n\\t\\t\\tif self.dopipzap:\\n\\t\\t\\t\\t# This unfortunately won\'t work with subservices\\n\\t\\t\\t\\tself.setCurrentSelection(self.session.pip.getCurrentService())\\n\\t\\t\\telse:\\n\\t\\t\\t\\tlastservice = eServiceReference(self.lastservice.value)\\n\\t\\t\\t\\tif lastservice.valid() and self.getCurrentSelection() != lastservice:\\n\\t\\t\\t\\t\\tself.setCurrentSelection(lastservice)\\n\\t\\tself.asciiOff()\\n\\t\\tself.zapBack()\\n\\t\\tself.correctChannelNumber()\\n\\t\\tself.editMode = False\\n\\t\\tself.protectContextMenu = True\\n\\t\\tself.close(None)\\n\\n\\tdef zapBack(self):\\n\\t\\tplayingref = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\tif self.startServiceRef and (playingref is None or playingref != self.startServiceRef):\\n\\t\\t\\tself.setStartRoot(self.startRoot)\\n\\t\\t\\tself.new_service_played = True\\n\\t\\t\\tself.session.nav.playService(self.startServiceRef)\\n\\t\\t\\tself.saveChannel(self.startServiceRef)\\n\\t\\telse:\\n\\t\\t\\tself.restoreMode()\\n\\t\\tself.startServiceRef = None\\n\\t\\tself.startRoot = None\\n\\t\\tif self.dopipzap:\\n\\t\\t\\t# This unfortunately won\'t work with subservices\\n\\t\\t\\tself.setCurrentSelection(self.session.pip.getCurrentService())\\n\\t\\telse:\\n\\t\\t\\tlastservice = eServiceReference(self.lastservice.value)\\n\\t\\t\\tif lastservice.valid() and self.getCurrentSelection() == lastservice:\\n\\t\\t\\t\\tpass\\t# keep current selection\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.setCurrentSelection(playingref)\\n\\n\\tdef setStartRoot(self, root):\\n\\t\\tif root:\\n\\t\\t\\tif self.revertMode == MODE_TV:\\n\\t\\t\\t\\tself.setModeTv()\\n\\t\\t\\telif self.revertMode == MODE_RADIO:\\n\\t\\t\\t\\tself.setModeRadio()\\n\\t\\t\\tself.revertMode = None\\n\\t\\t\\tself.enterUserbouquet(root)\\n\\n\\tdef restoreMode(self):\\n\\t\\tif self.revertMode == MODE_TV:\\n\\t\\t\\tself.setModeTv()\\n\\t\\telif self.revertMode == MODE_RADIO:\\n\\t\\t\\tself.setModeRadio()\\n\\t\\tself.revertMode = None\\n\\n\\tdef correctChannelNumber(self):\\n\\t\\tcurrent_ref = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\tif self.dopipzap:\\n\\t\\t\\ttmp_mode = config.servicelist.lastmode.value\\n\\t\\t\\ttmp_root = self.getRoot()\\n\\t\\t\\ttmp_ref = self.getCurrentSelection()\\n\\t\\t\\tpip_ref = self.session.pip.getCurrentService()\\n\\t\\t\\tif tmp_ref and pip_ref and tmp_ref != pip_ref:\\n\\t\\t\\t\\tself.revertMode = None\\n\\t\\t\\t\\treturn\\n\\t\\t\\tif self.mainScreenMode == \\"tv\\":\\n\\t\\t\\t\\tself.setModeTv()\\n\\t\\t\\telif self.mainScreenMode == \\"radio\\":\\n\\t\\t\\t\\tself.setModeRadio()\\n\\t\\t\\tif self.mainScreenRoot:\\n\\t\\t\\t\\tself.setRoot(self.mainScreenRoot)\\n\\t\\t\\t\\tself.setCurrentSelection(current_ref)\\n\\t\\tselected_ref = self.getCurrentSelection()\\n\\t\\tif selected_ref and current_ref and selected_ref.getChannelNum() != current_ref.getChannelNum():\\n\\t\\t\\toldref = self.session.nav.currentlyPlayingServiceReference\\n\\t\\t\\tif oldref and selected_ref == oldref or (oldref != current_ref and selected_ref == current_ref):\\n\\t\\t\\t\\tself.session.nav.currentlyPlayingServiceOrGroup = selected_ref\\n\\t\\t\\t\\tself.session.nav.pnav.navEvent(iPlayableService.evStart)\\n\\t\\tif self.dopipzap:\\n\\t\\t\\tif tmp_mode == \\"tv\\":\\n\\t\\t\\t\\tself.setModeTv()\\n\\t\\t\\telif tmp_mode == \\"radio\\":\\n\\t\\t\\t\\tself.setModeRadio()\\n\\t\\t\\tself.enterUserbouquet(tmp_root)\\n\\t\\t\\ttitle = self.instance.getTitle()\\n\\t\\t\\tpos = title.find(\\" (\\")\\n\\t\\t\\tif pos != -1:\\n\\t\\t\\t\\ttitle = title[:pos]\\n\\t\\t\\t\\ttitle += _(\\" (PiP)\\")\\n\\t\\t\\t\\tself.setTitle(title)\\n\\t\\t\\t\\tself.buildTitleString()\\n\\t\\t\\tif tmp_ref and pip_ref and tmp_ref.getChannelNum() != pip_ref.getChannelNum():\\n\\t\\t\\t\\tself.session.pip.currentService = tmp_ref\\n\\t\\t\\tself.setCurrentSelection(tmp_ref)\\n\\t\\tself.revertMode = None\\n\\nclass RadioInfoBar(Screen):\\n\\tdef __init__(self, session):\\n\\t\\tScreen.__init__(self, session)\\n\\t\\tself[\\"RdsDecoder\\"] = RdsDecoder(self.session.nav)\\n\\nclass ChannelSelectionRadio(ChannelSelectionBase, ChannelSelectionEdit, ChannelSelectionEPG, InfoBarBase, SelectionEventInfo):\\n\\tALLOW_SUSPEND = True\\n\\n\\tdef __init__(self, session, infobar):\\n\\t\\tChannelSelectionBase.__init__(self, session)\\n\\t\\tChannelSelectionEdit.__init__(self)\\n\\t\\tChannelSelectionEPG.__init__(self)\\n\\t\\tInfoBarBase.__init__(self)\\n\\t\\tSelectionEventInfo.__init__(self)\\n\\t\\tself.infobar = infobar\\n\\t\\tself.startServiceRef = None\\n\\t\\tself.onLayoutFinish.append(self.onCreate)\\n\\n\\t\\tself.info = session.instantiateDialog(RadioInfoBar) # our simple infobar\\n\\n\\t\\tself[\\"actions\\"] = ActionMap([\\"OkCancelActions\\", \\"TvRadioActions\\"],\\n\\t\\t\\t{\\n\\t\\t\\t\\t\\"keyTV\\": self.cancel,\\n\\t\\t\\t\\t\\"keyRadio\\": self.cancel,\\n\\t\\t\\t\\t\\"cancel\\": self.cancel,\\n\\t\\t\\t\\t\\"ok\\": self.channelSelected,\\n\\t\\t\\t})\\n\\n\\t\\tself.__event_tracker = ServiceEventTracker(screen=self, eventmap=\\n\\t\\t\\t{\\n\\t\\t\\t\\tiPlayableService.evStart: self.__evServiceStart,\\n\\t\\t\\t\\tiPlayableService.evEnd: self.__evServiceEnd\\n\\t\\t\\t})\\n\\n########## RDS Radiotext / Rass Support BEGIN\\n\\t\\tself.infobar = infobar # reference to real infobar (the one and only)\\n\\t\\tself[\\"RdsDecoder\\"] = self.info[\\"RdsDecoder\\"]\\n\\t\\tself[\\"RdsActions\\"] = HelpableActionMap(self, \\"InfobarRdsActions\\",\\n\\t\\t{\\n\\t\\t\\t\\"startRassInteractive\\": (self.startRassInteractive, _(\\"View Rass interactive...\\"))\\n\\t\\t},-1)\\n\\t\\tself[\\"RdsActions\\"].setEnabled(False)\\n\\t\\tinfobar.rds_display.onRassInteractivePossibilityChanged.append(self.RassInteractivePossibilityChanged)\\n\\t\\tself.onClose.append(self.__onClose)\\n\\t\\tself.onExecBegin.append(self.__onExecBegin)\\n\\t\\tself.onExecEnd.append(self.__onExecEnd)\\n\\n\\tdef __onClose(self):\\n\\t\\tlastservice = eServiceReference(config.tv.lastservice.value)\\n\\t\\tself.session.nav.playService(lastservice)\\n\\n\\tdef startRassInteractive(self):\\n\\t\\tself.info.hide();\\n\\t\\tself.infobar.rass_interactive = self.session.openWithCallback(self.RassInteractiveClosed, RassInteractive)\\n\\n\\tdef RassInteractiveClosed(self):\\n\\t\\tself.info.show()\\n\\t\\tself.infobar.rass_interactive = None\\n\\t\\tself.infobar.RassSlidePicChanged()\\n\\n\\tdef RassInteractivePossibilityChanged(self, state):\\n\\t\\tself[\\"RdsActions\\"].setEnabled(state)\\n########## RDS Radiotext / Rass Support END\\n\\n\\tdef __onExecBegin(self):\\n\\t\\tself.info.show()\\n\\n\\tdef __onExecEnd(self):\\n\\t\\tself.info.hide()\\n\\n\\tdef cancel(self):\\n\\t\\tself.infobar.rds_display.onRassInteractivePossibilityChanged.remove(self.RassInteractivePossibilityChanged)\\n\\t\\tself.info.hide()\\n\\t\\t#set previous tv service\\n\\t\\tself.close(None)\\n\\n\\tdef __evServiceStart(self):\\n\\t\\tservice = self.session.nav.getCurrentService()\\n\\t\\tif service:\\n\\t\\t\\tinfo = service.info()\\n\\t\\t\\tif info:\\n\\t\\t\\t\\trefstr = info.getInfoString(iServiceInformation.sServiceref)\\n\\t\\t\\t\\tself.servicelist.setPlayableIgnoreService(eServiceReference(refstr))\\n\\n\\tdef __evServiceEnd(self):\\n\\t\\tself.servicelist.setPlayableIgnoreService(eServiceReference())\\n\\n\\tdef saveRoot(self):\\n\\t\\tpath = \'\'\\n\\t\\tfor i in self.servicePathRadio:\\n\\t\\t\\tpath += i.toString()\\n\\t\\t\\tpath += \';\'\\n\\t\\tif path and path != config.radio.lastroot.value:\\n\\t\\t\\tconfig.radio.lastroot.value = path\\n\\t\\t\\tconfig.radio.lastroot.save()\\n\\n\\tdef restoreRoot(self):\\n\\t\\ttmp = [x for x in config.radio.lastroot.value.split(\';\') if x != \'\']\\n\\t\\tcurrent = [x.toString() for x in self.servicePath]\\n\\t\\tif tmp != current or self.rootChanged:\\n\\t\\t\\tcnt = 0\\n\\t\\t\\tfor i in tmp:\\n\\t\\t\\t\\tself.servicePathRadio.append(eServiceReference(i))\\n\\t\\t\\t\\tcnt += 1\\n\\t\\t\\tif cnt:\\n\\t\\t\\t\\tpath = self.servicePathRadio.pop()\\n\\t\\t\\t\\tself.enterPath(path)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.showFavourites()\\n\\t\\t\\t\\tself.saveRoot()\\n\\t\\t\\tself.rootChanged = False\\n\\n\\tdef preEnterPath(self, refstr):\\n\\t\\tif self.servicePathRadio and self.servicePathRadio[0] != eServiceReference(refstr):\\n\\t\\t\\tpathstr = config.radio.lastroot.value\\n\\t\\t\\tif pathstr is not None and refstr in pathstr:\\n\\t\\t\\t\\tself.restoreRoot()\\n\\t\\t\\t\\tlastservice = eServiceReference(config.radio.lastservice.value)\\n\\t\\t\\t\\tif lastservice.valid():\\n\\t\\t\\t\\t\\tself.setCurrentSelection(lastservice)\\n\\t\\t\\t\\treturn True\\n\\t\\treturn False\\n\\n\\tdef onCreate(self):\\n\\t\\tself.setRadioMode()\\n\\t\\tself.restoreRoot()\\n\\t\\tlastservice = eServiceReference(config.radio.lastservice.value)\\n\\t\\tif lastservice.valid():\\n\\t\\t\\tself.servicelist.setCurrent(lastservice)\\n\\t\\t\\tself.session.nav.playService(lastservice)\\n\\t\\telse:\\n\\t\\t\\tself.session.nav.stopService()\\n\\t\\tself.info.show()\\n\\n\\tdef channelSelected(self, doClose=False): # just return selected service\\n\\t\\tref = self.getCurrentSelection()\\n\\t\\tif self.movemode:\\n\\t\\t\\tself.toggleMoveMarked()\\n\\t\\telif (ref.flags \\u0026 eServiceReference.flagDirectory) == eServiceReference.flagDirectory:\\n\\t\\t\\tself.enterPath(ref)\\n\\t\\t\\tself.gotoCurrentServiceOrProvider(ref)\\n\\t\\telif self.bouquet_mark_edit != OFF:\\n\\t\\t\\tif not (self.bouquet_mark_edit == EDIT_ALTERNATIVES and ref.flags \\u0026 eServiceReference.isGroup):\\n\\t\\t\\t\\tself.doMark()\\n\\t\\telif not (ref.flags \\u0026 eServiceReference.isMarker): # no marker\\n\\t\\t\\tcur_root = self.getRoot()\\n\\t\\t\\tif not cur_root or not (cur_root.flags \\u0026 eServiceReference.isGroup):\\n\\t\\t\\t\\tplayingref = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\t\\t\\tif playingref is None or playingref != ref:\\n\\t\\t\\t\\t\\tself.session.nav.playService(ref)\\n\\t\\t\\t\\t\\tconfig.radio.lastservice.value = ref.toString()\\n\\t\\t\\t\\t\\tconfig.radio.lastservice.save()\\n\\t\\t\\t\\tself.saveRoot()\\n\\n\\tdef zapBack(self):\\n\\t\\tself.channelSelected()\\n\\nclass SimpleChannelSelection(ChannelSelectionBase, SelectionEventInfo):\\n\\tdef __init__(self, session, title, currentBouquet=False, returnBouquet=False, setService=None, setBouquet=None):\\n\\t\\tChannelSelectionBase.__init__(self, session)\\n\\t\\tSelectionEventInfo.__init__(self)\\n\\t\\tself[\\"actions\\"] = ActionMap([\\"OkCancelActions\\", \\"TvRadioActions\\"],\\n\\t\\t\\t{\\n\\t\\t\\t\\t\\"cancel\\": self.close,\\n\\t\\t\\t\\t\\"ok\\": self.channelSelected,\\n\\t\\t\\t\\t\\"keyRadio\\": self.setModeRadio,\\n\\t\\t\\t\\t\\"keyTV\\": self.setModeTv,\\n\\t\\t\\t})\\n\\t\\tself.bouquet_mark_edit = OFF\\n\\t\\tif isinstance(title, str):\\n\\t\\t\\tself.maintitle = title\\n\\t\\tself.currentBouquet = currentBouquet\\n\\t\\tself.returnBouquet = returnBouquet\\n\\t\\tself.setService = setService\\n\\t\\tself.setBouquet = setBouquet\\n\\t\\tself.onLayoutFinish.append(self.layoutFinished)\\n\\n\\tdef layoutFinished(self):\\n\\t\\tself.setModeTv()\\n\\t\\tif self.currentBouquet or self.setBouquet:\\n\\t\\t\\tref = self.setBouquet or Screens.InfoBar.InfoBar.instance.servicelist.getRoot()\\n\\t\\t\\tif ref:\\n\\t\\t\\t\\tself.enterPath(ref)\\n\\t\\t\\t\\tself.gotoCurrentServiceOrProvider(ref)\\n\\t\\tif self.setService:\\n\\t\\t\\tself.setCurrentSelection(self.setService)\\n\\n\\tdef saveRoot(self):\\n\\t\\tpass\\n\\n\\tdef keyRecord(self):\\n\\t\\treturn 0\\n\\n\\tdef channelSelected(self): # just return selected service\\n\\t\\tref = self.getCurrentSelection()\\n\\t\\tif (ref.flags \\u0026 eServiceReference.flagDirectory) == eServiceReference.flagDirectory:\\n\\t\\t\\tself.enterPath(ref)\\n\\t\\t\\tself.gotoCurrentServiceOrProvider(ref)\\n\\t\\telif not (ref.flags \\u0026 eServiceReference.isMarker):\\n\\t\\t\\tref = self.getCurrentSelection()\\n\\t\\t\\tif self.returnBouquet and len(self.servicePath):\\n\\t\\t\\t\\tself.close(ref, self.servicePath[-1])\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.close(ref)\\n\\n\\tdef setModeTv(self):\\n\\t\\tself.setTvMode()\\n\\t\\tself.showFavourites()\\n\\n\\tdef setModeRadio(self):\\n\\t\\tself.setRadioMode()\\n\\t\\tself.showFavourites()\\n"}\n'
line: b'{"repo_name":"giorgiop/scipy","ref":"refs/heads/master","path":"scipy/interpolate/tests/test_fitpack2.py","content":"#!/usr/bin/env python\\n# Created by Pearu Peterson, June 2003\\nfrom __future__ import division, print_function, absolute_import\\n\\nimport warnings\\n\\nimport numpy as np\\nfrom numpy.testing import (assert_equal, assert_almost_equal, assert_array_equal,\\n        assert_array_almost_equal, assert_allclose, assert_raises, TestCase,\\n        run_module_suite)\\nfrom numpy import array, diff, linspace, meshgrid, ones, pi, shape\\nfrom scipy.interpolate.fitpack import bisplrep, bisplev\\nfrom scipy.interpolate.fitpack2 import (UnivariateSpline,\\n        LSQUnivariateSpline, InterpolatedUnivariateSpline,\\n        LSQBivariateSpline, SmoothBivariateSpline, RectBivariateSpline,\\n        LSQSphereBivariateSpline, SmoothSphereBivariateSpline,\\n        RectSphereBivariateSpline)\\n\\n\\nclass TestUnivariateSpline(TestCase):\\n    def test_linear_constant(self):\\n        x = [1,2,3]\\n        y = [3,3,3]\\n        lut = UnivariateSpline(x,y,k=1)\\n        assert_array_almost_equal(lut.get_knots(),[1,3])\\n        assert_array_almost_equal(lut.get_coeffs(),[3,3])\\n        assert_almost_equal(lut.get_residual(),0.0)\\n        assert_array_almost_equal(lut([1,1.5,2]),[3,3,3])\\n\\n    def test_preserve_shape(self):\\n        x = [1, 2, 3]\\n        y = [0, 2, 4]\\n        lut = UnivariateSpline(x, y, k=1)\\n        arg = 2\\n        assert_equal(shape(arg), shape(lut(arg)))\\n        assert_equal(shape(arg), shape(lut(arg, nu=1)))\\n        arg = [1.5, 2, 2.5]\\n        assert_equal(shape(arg), shape(lut(arg)))\\n        assert_equal(shape(arg), shape(lut(arg, nu=1)))\\n\\n    def test_linear_1d(self):\\n        x = [1,2,3]\\n        y = [0,2,4]\\n        lut = UnivariateSpline(x,y,k=1)\\n        assert_array_almost_equal(lut.get_knots(),[1,3])\\n        assert_array_almost_equal(lut.get_coeffs(),[0,4])\\n        assert_almost_equal(lut.get_residual(),0.0)\\n        assert_array_almost_equal(lut([1,1.5,2]),[0,1,2])\\n\\n    def test_subclassing(self):\\n        # See #731\\n\\n        class ZeroSpline(UnivariateSpline):\\n            def __call__(self, x):\\n                return 0*array(x)\\n\\n        sp = ZeroSpline([1,2,3,4,5], [3,2,3,2,3], k=2)\\n        assert_array_equal(sp([1.5, 2.5]), [0., 0.])\\n\\n    def test_empty_input(self):\\n        # Test whether empty input returns an empty output. Ticket 1014\\n        x = [1,3,5,7,9]\\n        y = [0,4,9,12,21]\\n        spl = UnivariateSpline(x, y, k=3)\\n        assert_array_equal(spl([]), array([]))\\n\\n    def test_resize_regression(self):\\n        \\"\\"\\"Regression test for #1375.\\"\\"\\"\\n        x = [-1., -0.65016502, -0.58856235, -0.26903553, -0.17370892,\\n             -0.10011001, 0., 0.10011001, 0.17370892, 0.26903553, 0.58856235,\\n             0.65016502, 1.]\\n        y = [1.,0.62928599, 0.5797223, 0.39965815, 0.36322694, 0.3508061,\\n             0.35214793, 0.3508061, 0.36322694, 0.39965815, 0.5797223,\\n             0.62928599, 1.]\\n        w = [1.00000000e+12, 6.88875973e+02, 4.89314737e+02, 4.26864807e+02,\\n             6.07746770e+02, 4.51341444e+02, 3.17480210e+02, 4.51341444e+02,\\n             6.07746770e+02, 4.26864807e+02, 4.89314737e+02, 6.88875973e+02,\\n             1.00000000e+12]\\n        spl = UnivariateSpline(x=x, y=y, w=w, s=None)\\n        desired = array([0.35100374, 0.51715855, 0.87789547, 0.98719344])\\n        assert_allclose(spl([0.1, 0.5, 0.9, 0.99]), desired, atol=5e-4)\\n\\n    def test_out_of_range_regression(self):\\n        # Test different extrapolation modes. See ticket 3557\\n        x = np.arange(5, dtype=float)\\n        y = x**3\\n\\n        xp = linspace(-8, 13, 100)\\n        xp_zeros = xp.copy()\\n        xp_zeros[np.logical_or(xp_zeros \\u003c 0., xp_zeros \\u003e 4.)] = 0\\n        xp_clip = xp.copy()\\n        xp_clip[xp_clip \\u003c x[0]] = x[0]\\n        xp_clip[xp_clip \\u003e x[-1]] = x[-1]\\n\\n        for cls in [UnivariateSpline, InterpolatedUnivariateSpline]:\\n            spl = cls(x=x, y=y)\\n            for ext in [0, \'extrapolate\']:\\n                assert_allclose(spl(xp, ext=ext), xp**3, atol=1e-16)\\n                assert_allclose(cls(x, y, ext=ext)(xp), xp**3, atol=1e-16)\\n            for ext in [1, \'zeros\']:\\n                assert_allclose(spl(xp, ext=ext), xp_zeros**3, atol=1e-16)\\n                assert_allclose(cls(x, y, ext=ext)(xp), xp_zeros**3, atol=1e-16)\\n            for ext in [2, \'raise\']:\\n                assert_raises(ValueError, spl, xp, **dict(ext=ext))\\n            for ext in [3, \'const\']:\\n                assert_allclose(spl(xp, ext=ext), xp_clip**3, atol=1e-16)\\n                assert_allclose(cls(x, y, ext=ext)(xp), xp_clip**3, atol=1e-16)\\n\\n        # also test LSQUnivariateSpline [which needs explicit knots]\\n        t = spl.get_knots()[3:4]  # interior knots w/ default k=3\\n        spl = LSQUnivariateSpline(x, y, t)\\n        assert_allclose(spl(xp, ext=0), xp**3, atol=1e-16)\\n        assert_allclose(spl(xp, ext=1), xp_zeros**3, atol=1e-16)\\n        assert_raises(ValueError, spl, xp, **dict(ext=2))\\n        assert_allclose(spl(xp, ext=3), xp_clip**3, atol=1e-16)\\n\\n        # also make sure that unknown values for `ext` are caught early\\n        for ext in [-1, \'unknown\']:\\n            spl = UnivariateSpline(x, y)\\n            assert_raises(ValueError, spl, xp, **dict(ext=ext))\\n            assert_raises(ValueError, UnivariateSpline,\\n                    **dict(x=x, y=y, ext=ext))\\n\\n    def test_lsq_fpchec(self):\\n        xs = np.arange(100) * 1.\\n        ys = np.arange(100) * 1.\\n        knots = np.linspace(0, 99, 10)\\n        bbox = (-1, 101)\\n        assert_raises(ValueError, LSQUnivariateSpline, xs, ys, knots,\\n                      bbox=bbox)\\n\\n    def test_derivative_and_antiderivative(self):\\n        # Thin wrappers to splder/splantider, so light smoke test only.\\n        x = np.linspace(0, 1, 70)**3\\n        y = np.cos(x)\\n\\n        spl = UnivariateSpline(x, y, s=0)\\n        spl2 = spl.antiderivative(2).derivative(2)\\n        assert_allclose(spl(0.3), spl2(0.3))\\n\\n        spl2 = spl.antiderivative(1)\\n        assert_allclose(spl2(0.6) - spl2(0.2),\\n                        spl.integral(0.2, 0.6))\\n\\n    def test_nan(self):\\n        # bail out early if the input data contains nans\\n        x = np.arange(10, dtype=float)\\n        y = x**3\\n        for z in [np.nan, np.inf, -np.inf]:\\n            y[-1] = z\\n            assert_raises(ValueError, UnivariateSpline,\\n                    **dict(x=x, y=y, check_finite=True))\\n\\n\\nclass TestLSQBivariateSpline(TestCase):\\n    # NOTE: The systems in this test class are rank-deficient\\n    def test_linear_constant(self):\\n        x = [1,1,1,2,2,2,3,3,3]\\n        y = [1,2,3,1,2,3,1,2,3]\\n        z = [3,3,3,3,3,3,3,3,3]\\n        s = 0.1\\n        tx = [1+s,3-s]\\n        ty = [1+s,3-s]\\n        with warnings.catch_warnings(record=True):  # coefficients of the ...\\n            lut = LSQBivariateSpline(x,y,z,tx,ty,kx=1,ky=1)\\n\\n        assert_almost_equal(lut(2,2), 3.)\\n\\n    def test_bilinearity(self):\\n        x = [1,1,1,2,2,2,3,3,3]\\n        y = [1,2,3,1,2,3,1,2,3]\\n        z = [0,7,8,3,4,7,1,3,4]\\n        s = 0.1\\n        tx = [1+s,3-s]\\n        ty = [1+s,3-s]\\n        with warnings.catch_warnings():\\n            # This seems to fail (ier=1, see ticket 1642).\\n            warnings.simplefilter(\'ignore\', UserWarning)\\n            lut = LSQBivariateSpline(x,y,z,tx,ty,kx=1,ky=1)\\n\\n        tx, ty = lut.get_knots()\\n        for xa, xb in zip(tx[:-1], tx[1:]):\\n            for ya, yb in zip(ty[:-1], ty[1:]):\\n                for t in [0.1, 0.5, 0.9]:\\n                    for s in [0.3, 0.4, 0.7]:\\n                        xp = xa*(1-t) + xb*t\\n                        yp = ya*(1-s) + yb*s\\n                        zp = (+ lut(xa, ya)*(1-t)*(1-s)\\n                              + lut(xb, ya)*t*(1-s)\\n                              + lut(xa, yb)*(1-t)*s\\n                              + lut(xb, yb)*t*s)\\n                        assert_almost_equal(lut(xp,yp), zp)\\n\\n    def test_integral(self):\\n        x = [1,1,1,2,2,2,8,8,8]\\n        y = [1,2,3,1,2,3,1,2,3]\\n        z = array([0,7,8,3,4,7,1,3,4])\\n\\n        s = 0.1\\n        tx = [1+s,3-s]\\n        ty = [1+s,3-s]\\n        with warnings.catch_warnings(record=True):  # coefficients of the ...\\n            lut = LSQBivariateSpline(x,y,z,tx,ty,kx=1,ky=1)\\n        tx, ty = lut.get_knots()\\n        tz = lut(tx, ty)\\n        trpz = .25*(diff(tx)[:,None]*diff(ty)[None,:]\\n                    * (tz[:-1,:-1]+tz[1:,:-1]+tz[:-1,1:]+tz[1:,1:])).sum()\\n\\n        assert_almost_equal(lut.integral(tx[0], tx[-1], ty[0], ty[-1]),\\n                            trpz)\\n\\n    def test_empty_input(self):\\n        # Test whether empty inputs returns an empty output. Ticket 1014\\n        x = [1,1,1,2,2,2,3,3,3]\\n        y = [1,2,3,1,2,3,1,2,3]\\n        z = [3,3,3,3,3,3,3,3,3]\\n        s = 0.1\\n        tx = [1+s,3-s]\\n        ty = [1+s,3-s]\\n        with warnings.catch_warnings(record=True):  # coefficients of the ...\\n            lut = LSQBivariateSpline(x,y,z,tx,ty,kx=1,ky=1)\\n\\n        assert_array_equal(lut([], []), np.zeros((0,0)))\\n        assert_array_equal(lut([], [], grid=False), np.zeros((0,)))\\n\\n\\nclass TestSmoothBivariateSpline(TestCase):\\n    def test_linear_constant(self):\\n        x = [1,1,1,2,2,2,3,3,3]\\n        y = [1,2,3,1,2,3,1,2,3]\\n        z = [3,3,3,3,3,3,3,3,3]\\n        lut = SmoothBivariateSpline(x,y,z,kx=1,ky=1)\\n        assert_array_almost_equal(lut.get_knots(),([1,1,3,3],[1,1,3,3]))\\n        assert_array_almost_equal(lut.get_coeffs(),[3,3,3,3])\\n        assert_almost_equal(lut.get_residual(),0.0)\\n        assert_array_almost_equal(lut([1,1.5,2],[1,1.5]),[[3,3],[3,3],[3,3]])\\n\\n    def test_linear_1d(self):\\n        x = [1,1,1,2,2,2,3,3,3]\\n        y = [1,2,3,1,2,3,1,2,3]\\n        z = [0,0,0,2,2,2,4,4,4]\\n        lut = SmoothBivariateSpline(x,y,z,kx=1,ky=1)\\n        assert_array_almost_equal(lut.get_knots(),([1,1,3,3],[1,1,3,3]))\\n        assert_array_almost_equal(lut.get_coeffs(),[0,0,4,4])\\n        assert_almost_equal(lut.get_residual(),0.0)\\n        assert_array_almost_equal(lut([1,1.5,2],[1,1.5]),[[0,0],[1,1],[2,2]])\\n\\n    def test_integral(self):\\n        x = [1,1,1,2,2,2,4,4,4]\\n        y = [1,2,3,1,2,3,1,2,3]\\n        z = array([0,7,8,3,4,7,1,3,4])\\n\\n        with warnings.catch_warnings():\\n            # This seems to fail (ier=1, see ticket 1642).\\n            warnings.simplefilter(\'ignore\', UserWarning)\\n            lut = SmoothBivariateSpline(x, y, z, kx=1, ky=1, s=0)\\n\\n        tx = [1,2,4]\\n        ty = [1,2,3]\\n\\n        tz = lut(tx, ty)\\n        trpz = .25*(diff(tx)[:,None]*diff(ty)[None,:]\\n                    * (tz[:-1,:-1]+tz[1:,:-1]+tz[:-1,1:]+tz[1:,1:])).sum()\\n        assert_almost_equal(lut.integral(tx[0], tx[-1], ty[0], ty[-1]), trpz)\\n\\n        lut2 = SmoothBivariateSpline(x, y, z, kx=2, ky=2, s=0)\\n        assert_almost_equal(lut2.integral(tx[0], tx[-1], ty[0], ty[-1]), trpz,\\n                            decimal=0)  # the quadratures give 23.75 and 23.85\\n\\n        tz = lut(tx[:-1], ty[:-1])\\n        trpz = .25*(diff(tx[:-1])[:,None]*diff(ty[:-1])[None,:]\\n                    * (tz[:-1,:-1]+tz[1:,:-1]+tz[:-1,1:]+tz[1:,1:])).sum()\\n        assert_almost_equal(lut.integral(tx[0], tx[-2], ty[0], ty[-2]), trpz)\\n\\n    def test_rerun_lwrk2_too_small(self):\\n        # in this setting, lwrk2 is too small in the default run. Here we\\n        # check for equality with the bisplrep/bisplev output because there,\\n        # an automatic re-run of the spline representation is done if ier\\u003e10.\\n        x = np.linspace(-2, 2, 80)\\n        y = np.linspace(-2, 2, 80)\\n        z = x + y\\n        xi = np.linspace(-1, 1, 100)\\n        yi = np.linspace(-2, 2, 100)\\n        tck = bisplrep(x, y, z)\\n        res1 = bisplev(xi, yi, tck)\\n        interp_ = SmoothBivariateSpline(x, y, z)\\n        res2 = interp_(xi, yi)\\n        assert_almost_equal(res1, res2)\\n\\n\\nclass TestLSQSphereBivariateSpline(TestCase):\\n    def setUp(self):\\n        # define the input data and coordinates\\n        ntheta, nphi = 70, 90\\n        theta = linspace(0.5/(ntheta - 1), 1 - 0.5/(ntheta - 1), ntheta) * pi\\n        phi = linspace(0.5/(nphi - 1), 1 - 0.5/(nphi - 1), nphi) * 2. * pi\\n        data = ones((theta.shape[0], phi.shape[0]))\\n        # define knots and extract data values at the knots\\n        knotst = theta[::5]\\n        knotsp = phi[::5]\\n        knotdata = data[::5, ::5]\\n        # calculate spline coefficients\\n        lats, lons = meshgrid(theta, phi)\\n        lut_lsq = LSQSphereBivariateSpline(lats.ravel(), lons.ravel(),\\n                                           data.T.ravel(), knotst, knotsp)\\n        self.lut_lsq = lut_lsq\\n        self.data = knotdata\\n        self.new_lons, self.new_lats = knotsp, knotst\\n\\n    def test_linear_constant(self):\\n        assert_almost_equal(self.lut_lsq.get_residual(), 0.0)\\n        assert_array_almost_equal(self.lut_lsq(self.new_lats, self.new_lons),\\n                                  self.data)\\n\\n    def test_empty_input(self):\\n        assert_array_almost_equal(self.lut_lsq([], []), np.zeros((0,0)))\\n        assert_array_almost_equal(self.lut_lsq([], [], grid=False), np.zeros((0,)))\\n\\n\\nclass TestSmoothSphereBivariateSpline(TestCase):\\n    def setUp(self):\\n        theta = array([.25*pi, .25*pi, .25*pi, .5*pi, .5*pi, .5*pi, .75*pi,\\n                       .75*pi, .75*pi])\\n        phi = array([.5 * pi, pi, 1.5 * pi, .5 * pi, pi, 1.5 * pi, .5 * pi, pi,\\n                     1.5 * pi])\\n        r = array([3, 3, 3, 3, 3, 3, 3, 3, 3])\\n        self.lut = SmoothSphereBivariateSpline(theta, phi, r, s=1E10)\\n\\n    def test_linear_constant(self):\\n        assert_almost_equal(self.lut.get_residual(), 0.)\\n        assert_array_almost_equal(self.lut([1, 1.5, 2],[1, 1.5]),\\n                                  [[3, 3], [3, 3], [3, 3]])\\n\\n    def test_empty_input(self):\\n        assert_array_almost_equal(self.lut([], []), np.zeros((0,0)))\\n        assert_array_almost_equal(self.lut([], [], grid=False), np.zeros((0,)))\\n\\n\\nclass TestRectBivariateSpline(TestCase):\\n    def test_defaults(self):\\n        x = array([1,2,3,4,5])\\n        y = array([1,2,3,4,5])\\n        z = array([[1,2,1,2,1],[1,2,1,2,1],[1,2,3,2,1],[1,2,2,2,1],[1,2,1,2,1]])\\n        lut = RectBivariateSpline(x,y,z)\\n        assert_array_almost_equal(lut(x,y),z)\\n\\n    def test_evaluate(self):\\n        x = array([1,2,3,4,5])\\n        y = array([1,2,3,4,5])\\n        z = array([[1,2,1,2,1],[1,2,1,2,1],[1,2,3,2,1],[1,2,2,2,1],[1,2,1,2,1]])\\n        lut = RectBivariateSpline(x,y,z)\\n\\n        xi = [1, 2.3, 5.3, 0.5, 3.3, 1.2, 3]\\n        yi = [1, 3.3, 1.2, 4.0, 5.0, 1.0, 3]\\n        zi = lut.ev(xi, yi)\\n        zi2 = array([lut(xp, yp)[0,0] for xp, yp in zip(xi, yi)])\\n\\n        assert_almost_equal(zi, zi2)\\n\\n    def test_derivatives_grid(self):\\n        x = array([1,2,3,4,5])\\n        y = array([1,2,3,4,5])\\n        z = array([[1,2,1,2,1],[1,2,1,2,1],[1,2,3,2,1],[1,2,2,2,1],[1,2,1,2,1]])\\n        dx = array([[0,0,-20,0,0],[0,0,13,0,0],[0,0,4,0,0],\\n            [0,0,-11,0,0],[0,0,4,0,0]])/6.\\n        dy = array([[4,-1,0,1,-4],[4,-1,0,1,-4],[0,1.5,0,-1.5,0],\\n            [2,.25,0,-.25,-2],[4,-1,0,1,-4]])\\n        dxdy = array([[40,-25,0,25,-40],[-26,16.25,0,-16.25,26],\\n            [-8,5,0,-5,8],[22,-13.75,0,13.75,-22],[-8,5,0,-5,8]])/6.\\n        lut = RectBivariateSpline(x,y,z)\\n        assert_array_almost_equal(lut(x,y,dx=1),dx)\\n        assert_array_almost_equal(lut(x,y,dy=1),dy)\\n        assert_array_almost_equal(lut(x,y,dx=1,dy=1),dxdy)\\n\\n    def test_derivatives(self):\\n        x = array([1,2,3,4,5])\\n        y = array([1,2,3,4,5])\\n        z = array([[1,2,1,2,1],[1,2,1,2,1],[1,2,3,2,1],[1,2,2,2,1],[1,2,1,2,1]])\\n        dx = array([0,0,2./3,0,0])\\n        dy = array([4,-1,0,-.25,-4])\\n        dxdy = array([160,65,0,55,32])/24.\\n        lut = RectBivariateSpline(x,y,z)\\n        assert_array_almost_equal(lut(x,y,dx=1,grid=False),dx)\\n        assert_array_almost_equal(lut(x,y,dy=1,grid=False),dy)\\n        assert_array_almost_equal(lut(x,y,dx=1,dy=1,grid=False),dxdy)\\n\\n    def test_broadcast(self):\\n        x = array([1,2,3,4,5])\\n        y = array([1,2,3,4,5])\\n        z = array([[1,2,1,2,1],[1,2,1,2,1],[1,2,3,2,1],[1,2,2,2,1],[1,2,1,2,1]])\\n        lut = RectBivariateSpline(x,y,z)\\n        assert_allclose(lut(x, y), lut(x[:,None], y[None,:], grid=False))\\n\\n\\nclass TestRectSphereBivariateSpline(TestCase):\\n    def test_defaults(self):\\n        y = linspace(0.01, 2*pi-0.01, 7)\\n        x = linspace(0.01, pi-0.01, 7)\\n        z = array([[1,2,1,2,1,2,1],[1,2,1,2,1,2,1],[1,2,3,2,1,2,1],\\n                   [1,2,2,2,1,2,1],[1,2,1,2,1,2,1],[1,2,2,2,1,2,1],\\n                   [1,2,1,2,1,2,1]])\\n        lut = RectSphereBivariateSpline(x,y,z)\\n        assert_array_almost_equal(lut(x,y),z)\\n\\n    def test_evaluate(self):\\n        y = linspace(0.01, 2*pi-0.01, 7)\\n        x = linspace(0.01, pi-0.01, 7)\\n        z = array([[1,2,1,2,1,2,1],[1,2,1,2,1,2,1],[1,2,3,2,1,2,1],\\n                   [1,2,2,2,1,2,1],[1,2,1,2,1,2,1],[1,2,2,2,1,2,1],\\n                   [1,2,1,2,1,2,1]])\\n        lut = RectSphereBivariateSpline(x,y,z)\\n        yi = [0.2, 1, 2.3, 2.35, 3.0, 3.99, 5.25]\\n        xi = [1.5, 0.4, 1.1, 0.45, 0.2345, 1., 0.0001]\\n        zi = lut.ev(xi, yi)\\n        zi2 = array([lut(xp, yp)[0,0] for xp, yp in zip(xi, yi)])\\n        assert_almost_equal(zi, zi2)\\n\\n    def test_derivatives_grid(self):\\n        y = linspace(0.01, 2*pi-0.01, 7)\\n        x = linspace(0.01, pi-0.01, 7)\\n        z = array([[1,2,1,2,1,2,1],[1,2,1,2,1,2,1],[1,2,3,2,1,2,1],\\n                   [1,2,2,2,1,2,1],[1,2,1,2,1,2,1],[1,2,2,2,1,2,1],\\n                   [1,2,1,2,1,2,1]])\\n\\n        lut = RectSphereBivariateSpline(x,y,z)\\n\\n        y = linspace(0.02, 2*pi-0.02, 7)\\n        x = linspace(0.02, pi-0.02, 7)\\n\\n        assert_allclose(lut(x, y, dtheta=1), _numdiff_2d(lut, x, y, dx=1),\\n                        rtol=1e-4, atol=1e-4)\\n        assert_allclose(lut(x, y, dphi=1), _numdiff_2d(lut, x, y, dy=1),\\n                        rtol=1e-4, atol=1e-4)\\n        assert_allclose(lut(x, y, dtheta=1, dphi=1), _numdiff_2d(lut, x, y, dx=1, dy=1, eps=1e-6),\\n                        rtol=1e-3, atol=1e-3)\\n\\n    def test_derivatives(self):\\n        y = linspace(0.01, 2*pi-0.01, 7)\\n        x = linspace(0.01, pi-0.01, 7)\\n        z = array([[1,2,1,2,1,2,1],[1,2,1,2,1,2,1],[1,2,3,2,1,2,1],\\n                   [1,2,2,2,1,2,1],[1,2,1,2,1,2,1],[1,2,2,2,1,2,1],\\n                   [1,2,1,2,1,2,1]])\\n\\n        lut = RectSphereBivariateSpline(x,y,z)\\n\\n        y = linspace(0.02, 2*pi-0.02, 7)\\n        x = linspace(0.02, pi-0.02, 7)\\n\\n        assert_equal(lut(x, y, dtheta=1, grid=False).shape, x.shape)\\n        assert_allclose(lut(x, y, dtheta=1, grid=False),\\n                        _numdiff_2d(lambda x,y: lut(x,y,grid=False), x, y, dx=1),\\n                        rtol=1e-4, atol=1e-4)\\n        assert_allclose(lut(x, y, dphi=1, grid=False),\\n                        _numdiff_2d(lambda x,y: lut(x,y,grid=False), x, y, dy=1),\\n                        rtol=1e-4, atol=1e-4)\\n        assert_allclose(lut(x, y, dtheta=1, dphi=1, grid=False),\\n                        _numdiff_2d(lambda x,y: lut(x,y,grid=False), x, y, dx=1, dy=1, eps=1e-6),\\n                        rtol=1e-3, atol=1e-3)\\n\\n\\ndef _numdiff_2d(func, x, y, dx=0, dy=0, eps=1e-8):\\n    if dx == 0 and dy == 0:\\n        return func(x, y)\\n    elif dx == 1 and dy == 0:\\n        return (func(x + eps, y) - func(x - eps, y)) / (2*eps)\\n    elif dx == 0 and dy == 1:\\n        return (func(x, y + eps) - func(x, y - eps)) / (2*eps)\\n    elif dx == 1 and dy == 1:\\n        return (func(x + eps, y + eps) - func(x - eps, y + eps)\\n                - func(x + eps, y - eps) + func(x - eps, y - eps)) / (2*eps)**2\\n    else:\\n        raise ValueError(\\"invalid derivative order\\")\\n\\nif __name__ == \\"__main__\\":\\n    run_module_suite()\\n"}\n'
line: b'{"repo_name":"rpmcpp/Audacity","ref":"refs/heads/master","path":"lib-src/lv2/lv2/plugins/eg03-metro.lv2/waflib/Tools/vala.py","content":"#! /usr/bin/env python\\n# encoding: utf-8\\n# WARNING! Do not edit! http://waf.googlecode.com/git/docs/wafbook/single.html#_obtaining_the_waf_file\\n\\nimport os.path,shutil,re\\nfrom waflib import Context,Task,Utils,Logs,Options,Errors\\nfrom waflib.TaskGen import extension,taskgen_method\\nfrom waflib.Configure import conf\\nclass valac(Task.Task):\\n\\tvars=[\\"VALAC\\",\\"VALAC_VERSION\\",\\"VALAFLAGS\\"]\\n\\text_out=[\'.h\']\\n\\tdef run(self):\\n\\t\\tcmd=[self.env[\'VALAC\']]+self.env[\'VALAFLAGS\']\\n\\t\\tcmd.extend([a.abspath()for a in self.inputs])\\n\\t\\tret=self.exec_command(cmd,cwd=self.outputs[0].parent.abspath())\\n\\t\\tif ret:\\n\\t\\t\\treturn ret\\n\\t\\tfor x in self.outputs:\\n\\t\\t\\tif id(x.parent)!=id(self.outputs[0].parent):\\n\\t\\t\\t\\tshutil.move(self.outputs[0].parent.abspath()+os.sep+x.name,x.abspath())\\n\\t\\tif self.generator.dump_deps_node:\\n\\t\\t\\tself.generator.dump_deps_node.write(\'\\\\n\'.join(self.generator.packages))\\n\\t\\treturn ret\\nvalac=Task.update_outputs(valac)\\n@taskgen_method\\ndef init_vala_task(self):\\n\\tself.profile=getattr(self,\'profile\',\'gobject\')\\n\\tif self.profile==\'gobject\':\\n\\t\\tself.uselib=Utils.to_list(getattr(self,\'uselib\',[]))\\n\\t\\tif not\'GOBJECT\'in self.uselib:\\n\\t\\t\\tself.uselib.append(\'GOBJECT\')\\n\\tdef addflags(flags):\\n\\t\\tself.env.append_value(\'VALAFLAGS\',flags)\\n\\tif self.profile:\\n\\t\\taddflags(\'--profile=%s\'%self.profile)\\n\\tif hasattr(self,\'threading\'):\\n\\t\\tif self.profile==\'gobject\':\\n\\t\\t\\tif not\'GTHREAD\'in self.uselib:\\n\\t\\t\\t\\tself.uselib.append(\'GTHREAD\')\\n\\t\\telse:\\n\\t\\t\\tLogs.warn(\\"Profile %s means no threading support\\"%self.profile)\\n\\t\\t\\tself.threading=False\\n\\t\\tif self.threading:\\n\\t\\t\\taddflags(\'--threading\')\\n\\tvalatask=self.valatask\\n\\tself.is_lib=\'cprogram\'not in self.features\\n\\tif self.is_lib:\\n\\t\\taddflags(\'--library=%s\'%self.target)\\n\\t\\th_node=self.path.find_or_declare(\'%s.h\'%self.target)\\n\\t\\tvalatask.outputs.append(h_node)\\n\\t\\taddflags(\'--header=%s\'%h_node.name)\\n\\t\\tvalatask.outputs.append(self.path.find_or_declare(\'%s.vapi\'%self.target))\\n\\t\\tif getattr(self,\'gir\',None):\\n\\t\\t\\tgir_node=self.path.find_or_declare(\'%s.gir\'%self.gir)\\n\\t\\t\\taddflags(\'--gir=%s\'%gir_node.name)\\n\\t\\t\\tvalatask.outputs.append(gir_node)\\n\\tself.vala_target_glib=getattr(self,\'vala_target_glib\',getattr(Options.options,\'vala_target_glib\',None))\\n\\tif self.vala_target_glib:\\n\\t\\taddflags(\'--target-glib=%s\'%self.vala_target_glib)\\n\\taddflags([\'--define=%s\'%x for x in getattr(self,\'vala_defines\',[])])\\n\\tpackages_private=Utils.to_list(getattr(self,\'packages_private\',[]))\\n\\taddflags([\'--pkg=%s\'%x for x in packages_private])\\n\\tdef _get_api_version():\\n\\t\\tapi_version=\'1.0\'\\n\\t\\tif hasattr(Context.g_module,\'API_VERSION\'):\\n\\t\\t\\tversion=Context.g_module.API_VERSION.split(\\".\\")\\n\\t\\t\\tif version[0]==\\"0\\":\\n\\t\\t\\t\\tapi_version=\\"0.\\"+version[1]\\n\\t\\t\\telse:\\n\\t\\t\\t\\tapi_version=version[0]+\\".0\\"\\n\\t\\treturn api_version\\n\\tself.includes=Utils.to_list(getattr(self,\'includes\',[]))\\n\\tself.uselib=self.to_list(getattr(self,\'uselib\',[]))\\n\\tvalatask.install_path=getattr(self,\'install_path\',\'\')\\n\\tvalatask.vapi_path=getattr(self,\'vapi_path\',\'${DATAROOTDIR}/vala/vapi\')\\n\\tvalatask.pkg_name=getattr(self,\'pkg_name\',self.env[\'PACKAGE\'])\\n\\tvalatask.header_path=getattr(self,\'header_path\',\'${INCLUDEDIR}/%s-%s\'%(valatask.pkg_name,_get_api_version()))\\n\\tvalatask.install_binding=getattr(self,\'install_binding\',True)\\n\\tself.packages=packages=Utils.to_list(getattr(self,\'packages\',[]))\\n\\tself.vapi_dirs=vapi_dirs=Utils.to_list(getattr(self,\'vapi_dirs\',[]))\\n\\tincludes=[]\\n\\tif hasattr(self,\'use\'):\\n\\t\\tlocal_packages=Utils.to_list(self.use)[:]\\n\\t\\tseen=[]\\n\\t\\twhile len(local_packages)\\u003e0:\\n\\t\\t\\tpackage=local_packages.pop()\\n\\t\\t\\tif package in seen:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tseen.append(package)\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tpackage_obj=self.bld.get_tgen_by_name(package)\\n\\t\\t\\texcept Errors.WafError:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tpackage_name=package_obj.target\\n\\t\\t\\tpackage_node=package_obj.path\\n\\t\\t\\tpackage_dir=package_node.path_from(self.path)\\n\\t\\t\\tfor task in package_obj.tasks:\\n\\t\\t\\t\\tfor output in task.outputs:\\n\\t\\t\\t\\t\\tif output.name==package_name+\\".vapi\\":\\n\\t\\t\\t\\t\\t\\tvalatask.set_run_after(task)\\n\\t\\t\\t\\t\\t\\tif package_name not in packages:\\n\\t\\t\\t\\t\\t\\t\\tpackages.append(package_name)\\n\\t\\t\\t\\t\\t\\tif package_dir not in vapi_dirs:\\n\\t\\t\\t\\t\\t\\t\\tvapi_dirs.append(package_dir)\\n\\t\\t\\t\\t\\t\\tif package_dir not in includes:\\n\\t\\t\\t\\t\\t\\t\\tincludes.append(package_dir)\\n\\t\\t\\tif hasattr(package_obj,\'use\'):\\n\\t\\t\\t\\tlst=self.to_list(package_obj.use)\\n\\t\\t\\t\\tlst.reverse()\\n\\t\\t\\t\\tlocal_packages=[pkg for pkg in lst if pkg not in seen]+local_packages\\n\\taddflags([\'--pkg=%s\'%p for p in packages])\\n\\tfor vapi_dir in vapi_dirs:\\n\\t\\tv_node=self.path.find_dir(vapi_dir)\\n\\t\\tif not v_node:\\n\\t\\t\\tLogs.warn(\'Unable to locate Vala API directory: %r\'%vapi_dir)\\n\\t\\telse:\\n\\t\\t\\taddflags(\'--vapidir=%s\'%v_node.abspath())\\n\\t\\t\\taddflags(\'--vapidir=%s\'%v_node.get_bld().abspath())\\n\\tself.dump_deps_node=None\\n\\tif self.is_lib and self.packages:\\n\\t\\tself.dump_deps_node=self.path.find_or_declare(\'%s.deps\'%self.target)\\n\\t\\tvalatask.outputs.append(self.dump_deps_node)\\n\\tself.includes.append(self.bld.srcnode.abspath())\\n\\tself.includes.append(self.bld.bldnode.abspath())\\n\\tfor include in includes:\\n\\t\\ttry:\\n\\t\\t\\tself.includes.append(self.path.find_dir(include).abspath())\\n\\t\\t\\tself.includes.append(self.path.find_dir(include).get_bld().abspath())\\n\\t\\texcept AttributeError:\\n\\t\\t\\tLogs.warn(\\"Unable to locate include directory: \'%s\'\\"%include)\\n\\tif self.is_lib and valatask.install_binding:\\n\\t\\theaders_list=[o for o in valatask.outputs if o.suffix()==\\".h\\"]\\n\\t\\ttry:\\n\\t\\t\\tself.install_vheader.source=headers_list\\n\\t\\texcept AttributeError:\\n\\t\\t\\tself.install_vheader=self.bld.install_files(valatask.header_path,headers_list,self.env)\\n\\t\\tvapi_list=[o for o in valatask.outputs if(o.suffix()in(\\".vapi\\",\\".deps\\"))]\\n\\t\\ttry:\\n\\t\\t\\tself.install_vapi.source=vapi_list\\n\\t\\texcept AttributeError:\\n\\t\\t\\tself.install_vapi=self.bld.install_files(valatask.vapi_path,vapi_list,self.env)\\n\\t\\tgir_list=[o for o in valatask.outputs if o.suffix()==\'.gir\']\\n\\t\\ttry:\\n\\t\\t\\tself.install_gir.source=gir_list\\n\\t\\texcept AttributeError:\\n\\t\\t\\tself.install_gir=self.bld.install_files(getattr(self,\'gir_path\',\'${DATAROOTDIR}/gir-1.0\'),gir_list,self.env)\\n@extension(\'.vala\',\'.gs\')\\ndef vala_file(self,node):\\n\\ttry:\\n\\t\\tvalatask=self.valatask\\n\\texcept AttributeError:\\n\\t\\tvalatask=self.valatask=self.create_task(\'valac\')\\n\\t\\tself.init_vala_task()\\n\\tvalatask.inputs.append(node)\\n\\tc_node=node.change_ext(\'.c\')\\n\\tvalatask.outputs.append(c_node)\\n\\tself.source.append(c_node)\\n@conf\\ndef find_valac(self,valac_name,min_version):\\n\\tvalac=self.find_program(valac_name,var=\'VALAC\')\\n\\ttry:\\n\\t\\toutput=self.cmd_and_log(valac+\' --version\')\\n\\texcept Exception:\\n\\t\\tvalac_version=None\\n\\telse:\\n\\t\\tver=re.search(r\'\\\\d+.\\\\d+.\\\\d+\',output).group(0).split(\'.\')\\n\\t\\tvalac_version=tuple([int(x)for x in ver])\\n\\tself.msg(\'Checking for %s version \\u003e= %r\'%(valac_name,min_version),valac_version,valac_version and valac_version\\u003e=min_version)\\n\\tif valac and valac_version\\u003cmin_version:\\n\\t\\tself.fatal(\\"%s version %r is too old, need \\u003e= %r\\"%(valac_name,valac_version,min_version))\\n\\tself.env[\'VALAC_VERSION\']=valac_version\\n\\treturn valac\\n@conf\\ndef check_vala(self,min_version=(0,8,0),branch=None):\\n\\tif not branch:\\n\\t\\tbranch=min_version[:2]\\n\\ttry:\\n\\t\\tfind_valac(self,\'valac-%d.%d\'%(branch[0],branch[1]),min_version)\\n\\texcept self.errors.ConfigurationError:\\n\\t\\tfind_valac(self,\'valac\',min_version)\\n@conf\\ndef check_vala_deps(self):\\n\\tif not self.env[\'HAVE_GOBJECT\']:\\n\\t\\tpkg_args={\'package\':\'gobject-2.0\',\'uselib_store\':\'GOBJECT\',\'args\':\'--cflags --libs\'}\\n\\t\\tif getattr(Options.options,\'vala_target_glib\',None):\\n\\t\\t\\tpkg_args[\'atleast_version\']=Options.options.vala_target_glib\\n\\t\\tself.check_cfg(**pkg_args)\\n\\tif not self.env[\'HAVE_GTHREAD\']:\\n\\t\\tpkg_args={\'package\':\'gthread-2.0\',\'uselib_store\':\'GTHREAD\',\'args\':\'--cflags --libs\'}\\n\\t\\tif getattr(Options.options,\'vala_target_glib\',None):\\n\\t\\t\\tpkg_args[\'atleast_version\']=Options.options.vala_target_glib\\n\\t\\tself.check_cfg(**pkg_args)\\ndef configure(self):\\n\\tself.load(\'gnu_dirs\')\\n\\tself.check_vala_deps()\\n\\tself.check_vala()\\n\\tself.env.VALAFLAGS=[\'-C\',\'--quiet\']\\ndef options(opt):\\n\\topt.load(\'gnu_dirs\')\\n\\tvalaopts=opt.add_option_group(\'Vala Compiler Options\')\\n\\tvalaopts.add_option(\'--vala-target-glib\',default=None,dest=\'vala_target_glib\',metavar=\'MAJOR.MINOR\',help=\'Target version of glib for Vala GObject code generation\')\\n"}\n'
line: b'{"repo_name":"dvliman/jaikuengine","ref":"refs/heads/master","path":".google_appengine/lib/django-1.5/django/contrib/comments/views/moderation.py","content":"from __future__ import absolute_import\\n\\nfrom django import template\\nfrom django.conf import settings\\nfrom django.contrib import comments\\nfrom django.contrib.auth.decorators import login_required, permission_required\\nfrom django.contrib.comments import signals\\nfrom django.contrib.comments.views.utils import next_redirect, confirmation_view\\nfrom django.shortcuts import get_object_or_404, render_to_response\\nfrom django.views.decorators.csrf import csrf_protect\\n\\n\\n@csrf_protect\\n@login_required\\ndef flag(request, comment_id, next=None):\\n    \\"\\"\\"\\n    Flags a comment. Confirmation on GET, action on POST.\\n\\n    Templates: :template:`comments/flag.html`,\\n    Context:\\n        comment\\n            the flagged `comments.comment` object\\n    \\"\\"\\"\\n    comment = get_object_or_404(comments.get_model(), pk=comment_id, site__pk=settings.SITE_ID)\\n\\n    # Flag on POST\\n    if request.method == \'POST\':\\n        perform_flag(request, comment)\\n        return next_redirect(request, fallback=next or \'comments-flag-done\',\\n            c=comment.pk)\\n\\n    # Render a form on GET\\n    else:\\n        return render_to_response(\'comments/flag.html\',\\n            {\'comment\': comment, \\"next\\": next},\\n            template.RequestContext(request)\\n        )\\n\\n@csrf_protect\\n@permission_required(\\"comments.can_moderate\\")\\ndef delete(request, comment_id, next=None):\\n    \\"\\"\\"\\n    Deletes a comment. Confirmation on GET, action on POST. Requires the \\"can\\n    moderate comments\\" permission.\\n\\n    Templates: :template:`comments/delete.html`,\\n    Context:\\n        comment\\n            the flagged `comments.comment` object\\n    \\"\\"\\"\\n    comment = get_object_or_404(comments.get_model(), pk=comment_id, site__pk=settings.SITE_ID)\\n\\n    # Delete on POST\\n    if request.method == \'POST\':\\n        # Flag the comment as deleted instead of actually deleting it.\\n        perform_delete(request, comment)\\n        return next_redirect(request, fallback=next or \'comments-delete-done\',\\n            c=comment.pk)\\n\\n    # Render a form on GET\\n    else:\\n        return render_to_response(\'comments/delete.html\',\\n            {\'comment\': comment, \\"next\\": next},\\n            template.RequestContext(request)\\n        )\\n\\n@csrf_protect\\n@permission_required(\\"comments.can_moderate\\")\\ndef approve(request, comment_id, next=None):\\n    \\"\\"\\"\\n    Approve a comment (that is, mark it as public and non-removed). Confirmation\\n    on GET, action on POST. Requires the \\"can moderate comments\\" permission.\\n\\n    Templates: :template:`comments/approve.html`,\\n    Context:\\n        comment\\n            the `comments.comment` object for approval\\n    \\"\\"\\"\\n    comment = get_object_or_404(comments.get_model(), pk=comment_id, site__pk=settings.SITE_ID)\\n\\n    # Delete on POST\\n    if request.method == \'POST\':\\n        # Flag the comment as approved.\\n        perform_approve(request, comment)\\n        return next_redirect(request, fallback=next or \'comments-approve-done\',\\n            c=comment.pk)\\n\\n    # Render a form on GET\\n    else:\\n        return render_to_response(\'comments/approve.html\',\\n            {\'comment\': comment, \\"next\\": next},\\n            template.RequestContext(request)\\n        )\\n\\n# The following functions actually perform the various flag/aprove/delete\\n# actions. They\'ve been broken out into separate functions to that they\\n# may be called from admin actions.\\n\\ndef perform_flag(request, comment):\\n    \\"\\"\\"\\n    Actually perform the flagging of a comment from a request.\\n    \\"\\"\\"\\n    flag, created = comments.models.CommentFlag.objects.get_or_create(\\n        comment = comment,\\n        user    = request.user,\\n        flag    = comments.models.CommentFlag.SUGGEST_REMOVAL\\n    )\\n    signals.comment_was_flagged.send(\\n        sender  = comment.__class__,\\n        comment = comment,\\n        flag    = flag,\\n        created = created,\\n        request = request,\\n    )\\n\\ndef perform_delete(request, comment):\\n    flag, created = comments.models.CommentFlag.objects.get_or_create(\\n        comment = comment,\\n        user    = request.user,\\n        flag    = comments.models.CommentFlag.MODERATOR_DELETION\\n    )\\n    comment.is_removed = True\\n    comment.save()\\n    signals.comment_was_flagged.send(\\n        sender  = comment.__class__,\\n        comment = comment,\\n        flag    = flag,\\n        created = created,\\n        request = request,\\n    )\\n\\n\\ndef perform_approve(request, comment):\\n    flag, created = comments.models.CommentFlag.objects.get_or_create(\\n        comment = comment,\\n        user    = request.user,\\n        flag    = comments.models.CommentFlag.MODERATOR_APPROVAL,\\n    )\\n\\n    comment.is_removed = False\\n    comment.is_public = True\\n    comment.save()\\n\\n    signals.comment_was_flagged.send(\\n        sender  = comment.__class__,\\n        comment = comment,\\n        flag    = flag,\\n        created = created,\\n        request = request,\\n    )\\n\\n# Confirmation views.\\n\\nflag_done = confirmation_view(\\n    template = \\"comments/flagged.html\\",\\n    doc = \'Displays a \\"comment was flagged\\" success page.\'\\n)\\ndelete_done = confirmation_view(\\n    template = \\"comments/deleted.html\\",\\n    doc = \'Displays a \\"comment was deleted\\" success page.\'\\n)\\napprove_done = confirmation_view(\\n    template = \\"comments/approved.html\\",\\n    doc = \'Displays a \\"comment was approved\\" success page.\'\\n)\\n"}\n'
line: b'{"repo_name":"mjtamlyn/django","ref":"refs/heads/master","path":"django/contrib/auth/hashers.py","content":"import base64\\nimport binascii\\nimport functools\\nimport hashlib\\nimport importlib\\nimport warnings\\nfrom collections import OrderedDict\\n\\nfrom django.conf import settings\\nfrom django.core.exceptions import ImproperlyConfigured\\nfrom django.core.signals import setting_changed\\nfrom django.dispatch import receiver\\nfrom django.utils.crypto import (\\n    constant_time_compare, get_random_string, pbkdf2,\\n)\\nfrom django.utils.encoding import force_bytes, force_text\\nfrom django.utils.module_loading import import_string\\nfrom django.utils.translation import gettext_noop as _\\n\\nUNUSABLE_PASSWORD_PREFIX = \'!\'  # This will never be a valid encoded hash\\nUNUSABLE_PASSWORD_SUFFIX_LENGTH = 40  # number of random chars to add after UNUSABLE_PASSWORD_PREFIX\\n\\n\\ndef is_password_usable(encoded):\\n    if encoded is None or encoded.startswith(UNUSABLE_PASSWORD_PREFIX):\\n        return False\\n    try:\\n        identify_hasher(encoded)\\n    except ValueError:\\n        return False\\n    return True\\n\\n\\ndef check_password(password, encoded, setter=None, preferred=\'default\'):\\n    \\"\\"\\"\\n    Return a boolean of whether the raw password matches the three\\n    part encoded digest.\\n\\n    If setter is specified, it\'ll be called when you need to\\n    regenerate the password.\\n    \\"\\"\\"\\n    if password is None or not is_password_usable(encoded):\\n        return False\\n\\n    preferred = get_hasher(preferred)\\n    hasher = identify_hasher(encoded)\\n\\n    hasher_changed = hasher.algorithm != preferred.algorithm\\n    must_update = hasher_changed or preferred.must_update(encoded)\\n    is_correct = hasher.verify(password, encoded)\\n\\n    # If the hasher didn\'t change (we don\'t protect against enumeration if it\\n    # does) and the password should get updated, try to close the timing gap\\n    # between the work factor of the current encoded password and the default\\n    # work factor.\\n    if not is_correct and not hasher_changed and must_update:\\n        hasher.harden_runtime(password, encoded)\\n\\n    if setter and is_correct and must_update:\\n        setter(password)\\n    return is_correct\\n\\n\\ndef make_password(password, salt=None, hasher=\'default\'):\\n    \\"\\"\\"\\n    Turn a plain-text password into a hash for database storage\\n\\n    Same as encode() but generate a new random salt. If password is None then\\n    return a concatenation of UNUSABLE_PASSWORD_PREFIX and a random string,\\n    which disallows logins. Additional random string reduces chances of gaining\\n    access to staff or superuser accounts. See ticket #20079 for more info.\\n    \\"\\"\\"\\n    if password is None:\\n        return UNUSABLE_PASSWORD_PREFIX + get_random_string(UNUSABLE_PASSWORD_SUFFIX_LENGTH)\\n    hasher = get_hasher(hasher)\\n\\n    if not salt:\\n        salt = hasher.salt()\\n\\n    return hasher.encode(password, salt)\\n\\n\\n@functools.lru_cache()\\ndef get_hashers():\\n    hashers = []\\n    for hasher_path in settings.PASSWORD_HASHERS:\\n        hasher_cls = import_string(hasher_path)\\n        hasher = hasher_cls()\\n        if not getattr(hasher, \'algorithm\'):\\n            raise ImproperlyConfigured(\\"hasher doesn\'t specify an \\"\\n                                       \\"algorithm name: %s\\" % hasher_path)\\n        hashers.append(hasher)\\n    return hashers\\n\\n\\n@functools.lru_cache()\\ndef get_hashers_by_algorithm():\\n    return {hasher.algorithm: hasher for hasher in get_hashers()}\\n\\n\\n@receiver(setting_changed)\\ndef reset_hashers(**kwargs):\\n    if kwargs[\'setting\'] == \'PASSWORD_HASHERS\':\\n        get_hashers.cache_clear()\\n        get_hashers_by_algorithm.cache_clear()\\n\\n\\ndef get_hasher(algorithm=\'default\'):\\n    \\"\\"\\"\\n    Return an instance of a loaded password hasher.\\n\\n    If algorithm is \'default\', return the default hasher. Lazily import hashers\\n    specified in the project\'s settings file if needed.\\n    \\"\\"\\"\\n    if hasattr(algorithm, \'algorithm\'):\\n        return algorithm\\n\\n    elif algorithm == \'default\':\\n        return get_hashers()[0]\\n\\n    else:\\n        hashers = get_hashers_by_algorithm()\\n        try:\\n            return hashers[algorithm]\\n        except KeyError:\\n            raise ValueError(\\"Unknown password hashing algorithm \'%s\'. \\"\\n                             \\"Did you specify it in the PASSWORD_HASHERS \\"\\n                             \\"setting?\\" % algorithm)\\n\\n\\ndef identify_hasher(encoded):\\n    \\"\\"\\"\\n    Return an instance of a loaded password hasher.\\n\\n    Identify hasher algorithm by examining encoded hash, and call\\n    get_hasher() to return hasher. Raise ValueError if\\n    algorithm cannot be identified, or if hasher is not loaded.\\n    \\"\\"\\"\\n    # Ancient versions of Django created plain MD5 passwords and accepted\\n    # MD5 passwords with an empty salt.\\n    if ((len(encoded) == 32 and \'$\' not in encoded) or\\n            (len(encoded) == 37 and encoded.startswith(\'md5$$\'))):\\n        algorithm = \'unsalted_md5\'\\n    # Ancient versions of Django accepted SHA1 passwords with an empty salt.\\n    elif len(encoded) == 46 and encoded.startswith(\'sha1$$\'):\\n        algorithm = \'unsalted_sha1\'\\n    else:\\n        algorithm = encoded.split(\'$\', 1)[0]\\n    return get_hasher(algorithm)\\n\\n\\ndef mask_hash(hash, show=6, char=\\"*\\"):\\n    \\"\\"\\"\\n    Return the given hash, with only the first ``show`` number shown. The\\n    rest are masked with ``char`` for security reasons.\\n    \\"\\"\\"\\n    masked = hash[:show]\\n    masked += char * len(hash[show:])\\n    return masked\\n\\n\\nclass BasePasswordHasher:\\n    \\"\\"\\"\\n    Abstract base class for password hashers\\n\\n    When creating your own hasher, you need to override algorithm,\\n    verify(), encode() and safe_summary().\\n\\n    PasswordHasher objects are immutable.\\n    \\"\\"\\"\\n    algorithm = None\\n    library = None\\n\\n    def _load_library(self):\\n        if self.library is not None:\\n            if isinstance(self.library, (tuple, list)):\\n                name, mod_path = self.library\\n            else:\\n                mod_path = self.library\\n            try:\\n                module = importlib.import_module(mod_path)\\n            except ImportError as e:\\n                raise ValueError(\\"Couldn\'t load %r algorithm library: %s\\" %\\n                                 (self.__class__.__name__, e))\\n            return module\\n        raise ValueError(\\"Hasher %r doesn\'t specify a library attribute\\" %\\n                         self.__class__.__name__)\\n\\n    def salt(self):\\n        \\"\\"\\"Generate a cryptographically secure nonce salt in ASCII.\\"\\"\\"\\n        return get_random_string()\\n\\n    def verify(self, password, encoded):\\n        \\"\\"\\"Check if the given password is correct.\\"\\"\\"\\n        raise NotImplementedError(\'subclasses of BasePasswordHasher must provide a verify() method\')\\n\\n    def encode(self, password, salt):\\n        \\"\\"\\"\\n        Create an encoded database value.\\n\\n        The result is normally formatted as \\"algorithm$salt$hash\\" and\\n        must be fewer than 128 characters.\\n        \\"\\"\\"\\n        raise NotImplementedError(\'subclasses of BasePasswordHasher must provide an encode() method\')\\n\\n    def safe_summary(self, encoded):\\n        \\"\\"\\"\\n        Return a summary of safe values.\\n\\n        The result is a dictionary and will be used where the password field\\n        must be displayed to construct a safe representation of the password.\\n        \\"\\"\\"\\n        raise NotImplementedError(\'subclasses of BasePasswordHasher must provide a safe_summary() method\')\\n\\n    def must_update(self, encoded):\\n        return False\\n\\n    def harden_runtime(self, password, encoded):\\n        \\"\\"\\"\\n        Bridge the runtime gap between the work factor supplied in `encoded`\\n        and the work factor suggested by this hasher.\\n\\n        Taking PBKDF2 as an example, if `encoded` contains 20000 iterations and\\n        `self.iterations` is 30000, this method should run password through\\n        another 10000 iterations of PBKDF2. Similar approaches should exist\\n        for any hasher that has a work factor. If not, this method should be\\n        defined as a no-op to silence the warning.\\n        \\"\\"\\"\\n        warnings.warn(\'subclasses of BasePasswordHasher should provide a harden_runtime() method\')\\n\\n\\nclass PBKDF2PasswordHasher(BasePasswordHasher):\\n    \\"\\"\\"\\n    Secure password hashing using the PBKDF2 algorithm (recommended)\\n\\n    Configured to use PBKDF2 + HMAC + SHA256.\\n    The result is a 64 byte binary string.  Iterations may be changed\\n    safely but you must rename the algorithm if you change SHA256.\\n    \\"\\"\\"\\n    algorithm = \\"pbkdf2_sha256\\"\\n    iterations = 100000\\n    digest = hashlib.sha256\\n\\n    def encode(self, password, salt, iterations=None):\\n        assert password is not None\\n        assert salt and \'$\' not in salt\\n        if not iterations:\\n            iterations = self.iterations\\n        hash = pbkdf2(password, salt, iterations, digest=self.digest)\\n        hash = base64.b64encode(hash).decode(\'ascii\').strip()\\n        return \\"%s$%d$%s$%s\\" % (self.algorithm, iterations, salt, hash)\\n\\n    def verify(self, password, encoded):\\n        algorithm, iterations, salt, hash = encoded.split(\'$\', 3)\\n        assert algorithm == self.algorithm\\n        encoded_2 = self.encode(password, salt, int(iterations))\\n        return constant_time_compare(encoded, encoded_2)\\n\\n    def safe_summary(self, encoded):\\n        algorithm, iterations, salt, hash = encoded.split(\'$\', 3)\\n        assert algorithm == self.algorithm\\n        return OrderedDict([\\n            (_(\'algorithm\'), algorithm),\\n            (_(\'iterations\'), iterations),\\n            (_(\'salt\'), mask_hash(salt)),\\n            (_(\'hash\'), mask_hash(hash)),\\n        ])\\n\\n    def must_update(self, encoded):\\n        algorithm, iterations, salt, hash = encoded.split(\'$\', 3)\\n        return int(iterations) != self.iterations\\n\\n    def harden_runtime(self, password, encoded):\\n        algorithm, iterations, salt, hash = encoded.split(\'$\', 3)\\n        extra_iterations = self.iterations - int(iterations)\\n        if extra_iterations \\u003e 0:\\n            self.encode(password, salt, extra_iterations)\\n\\n\\nclass PBKDF2SHA1PasswordHasher(PBKDF2PasswordHasher):\\n    \\"\\"\\"\\n    Alternate PBKDF2 hasher which uses SHA1, the default PRF\\n    recommended by PKCS #5. This is compatible with other\\n    implementations of PBKDF2, such as openssl\'s\\n    PKCS5_PBKDF2_HMAC_SHA1().\\n    \\"\\"\\"\\n    algorithm = \\"pbkdf2_sha1\\"\\n    digest = hashlib.sha1\\n\\n\\nclass Argon2PasswordHasher(BasePasswordHasher):\\n    \\"\\"\\"\\n    Secure password hashing using the argon2 algorithm.\\n\\n    This is the winner of the Password Hashing Competition 2013-2015\\n    (https://password-hashing.net). It requires the argon2-cffi library which\\n    depends on native C code and might cause portability issues.\\n    \\"\\"\\"\\n    algorithm = \'argon2\'\\n    library = \'argon2\'\\n\\n    time_cost = 2\\n    memory_cost = 512\\n    parallelism = 2\\n\\n    def encode(self, password, salt):\\n        argon2 = self._load_library()\\n        data = argon2.low_level.hash_secret(\\n            force_bytes(password),\\n            force_bytes(salt),\\n            time_cost=self.time_cost,\\n            memory_cost=self.memory_cost,\\n            parallelism=self.parallelism,\\n            hash_len=argon2.DEFAULT_HASH_LENGTH,\\n            type=argon2.low_level.Type.I,\\n        )\\n        return self.algorithm + data.decode(\'ascii\')\\n\\n    def verify(self, password, encoded):\\n        argon2 = self._load_library()\\n        algorithm, rest = encoded.split(\'$\', 1)\\n        assert algorithm == self.algorithm\\n        try:\\n            return argon2.low_level.verify_secret(\\n                force_bytes(\'$\' + rest),\\n                force_bytes(password),\\n                type=argon2.low_level.Type.I,\\n            )\\n        except argon2.exceptions.VerificationError:\\n            return False\\n\\n    def safe_summary(self, encoded):\\n        (algorithm, variety, version, time_cost, memory_cost, parallelism,\\n            salt, data) = self._decode(encoded)\\n        assert algorithm == self.algorithm\\n        return OrderedDict([\\n            (_(\'algorithm\'), algorithm),\\n            (_(\'variety\'), variety),\\n            (_(\'version\'), version),\\n            (_(\'memory cost\'), memory_cost),\\n            (_(\'time cost\'), time_cost),\\n            (_(\'parallelism\'), parallelism),\\n            (_(\'salt\'), mask_hash(salt)),\\n            (_(\'hash\'), mask_hash(data)),\\n        ])\\n\\n    def must_update(self, encoded):\\n        (algorithm, variety, version, time_cost, memory_cost, parallelism,\\n            salt, data) = self._decode(encoded)\\n        assert algorithm == self.algorithm\\n        argon2 = self._load_library()\\n        return (\\n            argon2.low_level.ARGON2_VERSION != version or\\n            self.time_cost != time_cost or\\n            self.memory_cost != memory_cost or\\n            self.parallelism != parallelism\\n        )\\n\\n    def harden_runtime(self, password, encoded):\\n        # The runtime for Argon2 is too complicated to implement a sensible\\n        # hardening algorithm.\\n        pass\\n\\n    def _decode(self, encoded):\\n        \\"\\"\\"\\n        Split an encoded hash and return: (\\n            algorithm, variety, version, time_cost, memory_cost,\\n            parallelism, salt, data,\\n        ).\\n        \\"\\"\\"\\n        bits = encoded.split(\'$\')\\n        if len(bits) == 5:\\n            # Argon2 \\u003c 1.3\\n            algorithm, variety, raw_params, salt, data = bits\\n            version = 0x10\\n        else:\\n            assert len(bits) == 6\\n            algorithm, variety, raw_version, raw_params, salt, data = bits\\n            assert raw_version.startswith(\'v=\')\\n            version = int(raw_version[len(\'v=\'):])\\n        params = dict(bit.split(\'=\', 1) for bit in raw_params.split(\',\'))\\n        assert len(params) == 3 and all(x in params for x in (\'t\', \'m\', \'p\'))\\n        time_cost = int(params[\'t\'])\\n        memory_cost = int(params[\'m\'])\\n        parallelism = int(params[\'p\'])\\n        return (\\n            algorithm, variety, version, time_cost, memory_cost, parallelism,\\n            salt, data,\\n        )\\n\\n\\nclass BCryptSHA256PasswordHasher(BasePasswordHasher):\\n    \\"\\"\\"\\n    Secure password hashing using the bcrypt algorithm (recommended)\\n\\n    This is considered by many to be the most secure algorithm but you\\n    must first install the bcrypt library.  Please be warned that\\n    this library depends on native C code and might cause portability\\n    issues.\\n    \\"\\"\\"\\n    algorithm = \\"bcrypt_sha256\\"\\n    digest = hashlib.sha256\\n    library = (\\"bcrypt\\", \\"bcrypt\\")\\n    rounds = 12\\n\\n    def salt(self):\\n        bcrypt = self._load_library()\\n        return bcrypt.gensalt(self.rounds)\\n\\n    def encode(self, password, salt):\\n        bcrypt = self._load_library()\\n        # Hash the password prior to using bcrypt to prevent password\\n        # truncation as described in #20138.\\n        if self.digest is not None:\\n            # Use binascii.hexlify() because a hex encoded bytestring is str.\\n            password = binascii.hexlify(self.digest(force_bytes(password)).digest())\\n        else:\\n            password = force_bytes(password)\\n\\n        data = bcrypt.hashpw(password, salt)\\n        return \\"%s$%s\\" % (self.algorithm, force_text(data))\\n\\n    def verify(self, password, encoded):\\n        algorithm, data = encoded.split(\'$\', 1)\\n        assert algorithm == self.algorithm\\n        encoded_2 = self.encode(password, force_bytes(data))\\n        return constant_time_compare(encoded, encoded_2)\\n\\n    def safe_summary(self, encoded):\\n        algorithm, empty, algostr, work_factor, data = encoded.split(\'$\', 4)\\n        assert algorithm == self.algorithm\\n        salt, checksum = data[:22], data[22:]\\n        return OrderedDict([\\n            (_(\'algorithm\'), algorithm),\\n            (_(\'work factor\'), work_factor),\\n            (_(\'salt\'), mask_hash(salt)),\\n            (_(\'checksum\'), mask_hash(checksum)),\\n        ])\\n\\n    def must_update(self, encoded):\\n        algorithm, empty, algostr, rounds, data = encoded.split(\'$\', 4)\\n        return int(rounds) != self.rounds\\n\\n    def harden_runtime(self, password, encoded):\\n        _, data = encoded.split(\'$\', 1)\\n        salt = data[:29]  # Length of the salt in bcrypt.\\n        rounds = data.split(\'$\')[2]\\n        # work factor is logarithmic, adding one doubles the load.\\n        diff = 2**(self.rounds - int(rounds)) - 1\\n        while diff \\u003e 0:\\n            self.encode(password, force_bytes(salt))\\n            diff -= 1\\n\\n\\nclass BCryptPasswordHasher(BCryptSHA256PasswordHasher):\\n    \\"\\"\\"\\n    Secure password hashing using the bcrypt algorithm\\n\\n    This is considered by many to be the most secure algorithm but you\\n    must first install the bcrypt library.  Please be warned that\\n    this library depends on native C code and might cause portability\\n    issues.\\n\\n    This hasher does not first hash the password which means it is subject to\\n    the 72 character bcrypt password truncation, most use cases should prefer\\n    the BCryptSHA256PasswordHasher.\\n\\n    See: https://code.djangoproject.com/ticket/20138\\n    \\"\\"\\"\\n    algorithm = \\"bcrypt\\"\\n    digest = None\\n\\n\\nclass SHA1PasswordHasher(BasePasswordHasher):\\n    \\"\\"\\"\\n    The SHA1 password hashing algorithm (not recommended)\\n    \\"\\"\\"\\n    algorithm = \\"sha1\\"\\n\\n    def encode(self, password, salt):\\n        assert password is not None\\n        assert salt and \'$\' not in salt\\n        hash = hashlib.sha1(force_bytes(salt + password)).hexdigest()\\n        return \\"%s$%s$%s\\" % (self.algorithm, salt, hash)\\n\\n    def verify(self, password, encoded):\\n        algorithm, salt, hash = encoded.split(\'$\', 2)\\n        assert algorithm == self.algorithm\\n        encoded_2 = self.encode(password, salt)\\n        return constant_time_compare(encoded, encoded_2)\\n\\n    def safe_summary(self, encoded):\\n        algorithm, salt, hash = encoded.split(\'$\', 2)\\n        assert algorithm == self.algorithm\\n        return OrderedDict([\\n            (_(\'algorithm\'), algorithm),\\n            (_(\'salt\'), mask_hash(salt, show=2)),\\n            (_(\'hash\'), mask_hash(hash)),\\n        ])\\n\\n    def harden_runtime(self, password, encoded):\\n        pass\\n\\n\\nclass MD5PasswordHasher(BasePasswordHasher):\\n    \\"\\"\\"\\n    The Salted MD5 password hashing algorithm (not recommended)\\n    \\"\\"\\"\\n    algorithm = \\"md5\\"\\n\\n    def encode(self, password, salt):\\n        assert password is not None\\n        assert salt and \'$\' not in salt\\n        hash = hashlib.md5(force_bytes(salt + password)).hexdigest()\\n        return \\"%s$%s$%s\\" % (self.algorithm, salt, hash)\\n\\n    def verify(self, password, encoded):\\n        algorithm, salt, hash = encoded.split(\'$\', 2)\\n        assert algorithm == self.algorithm\\n        encoded_2 = self.encode(password, salt)\\n        return constant_time_compare(encoded, encoded_2)\\n\\n    def safe_summary(self, encoded):\\n        algorithm, salt, hash = encoded.split(\'$\', 2)\\n        assert algorithm == self.algorithm\\n        return OrderedDict([\\n            (_(\'algorithm\'), algorithm),\\n            (_(\'salt\'), mask_hash(salt, show=2)),\\n            (_(\'hash\'), mask_hash(hash)),\\n        ])\\n\\n    def harden_runtime(self, password, encoded):\\n        pass\\n\\n\\nclass UnsaltedSHA1PasswordHasher(BasePasswordHasher):\\n    \\"\\"\\"\\n    Very insecure algorithm that you should *never* use; store SHA1 hashes\\n    with an empty salt.\\n\\n    This class is implemented because Django used to accept such password\\n    hashes. Some older Django installs still have these values lingering\\n    around so we need to handle and upgrade them properly.\\n    \\"\\"\\"\\n    algorithm = \\"unsalted_sha1\\"\\n\\n    def salt(self):\\n        return \'\'\\n\\n    def encode(self, password, salt):\\n        assert salt == \'\'\\n        hash = hashlib.sha1(force_bytes(password)).hexdigest()\\n        return \'sha1$$%s\' % hash\\n\\n    def verify(self, password, encoded):\\n        encoded_2 = self.encode(password, \'\')\\n        return constant_time_compare(encoded, encoded_2)\\n\\n    def safe_summary(self, encoded):\\n        assert encoded.startswith(\'sha1$$\')\\n        hash = encoded[6:]\\n        return OrderedDict([\\n            (_(\'algorithm\'), self.algorithm),\\n            (_(\'hash\'), mask_hash(hash)),\\n        ])\\n\\n    def harden_runtime(self, password, encoded):\\n        pass\\n\\n\\nclass UnsaltedMD5PasswordHasher(BasePasswordHasher):\\n    \\"\\"\\"\\n    Incredibly insecure algorithm that you should *never* use; stores unsalted\\n    MD5 hashes without the algorithm prefix, also accepts MD5 hashes with an\\n    empty salt.\\n\\n    This class is implemented because Django used to store passwords this way\\n    and to accept such password hashes. Some older Django installs still have\\n    these values lingering around so we need to handle and upgrade them\\n    properly.\\n    \\"\\"\\"\\n    algorithm = \\"unsalted_md5\\"\\n\\n    def salt(self):\\n        return \'\'\\n\\n    def encode(self, password, salt):\\n        assert salt == \'\'\\n        return hashlib.md5(force_bytes(password)).hexdigest()\\n\\n    def verify(self, password, encoded):\\n        if len(encoded) == 37 and encoded.startswith(\'md5$$\'):\\n            encoded = encoded[5:]\\n        encoded_2 = self.encode(password, \'\')\\n        return constant_time_compare(encoded, encoded_2)\\n\\n    def safe_summary(self, encoded):\\n        return OrderedDict([\\n            (_(\'algorithm\'), self.algorithm),\\n            (_(\'hash\'), mask_hash(encoded, show=3)),\\n        ])\\n\\n    def harden_runtime(self, password, encoded):\\n        pass\\n\\n\\nclass CryptPasswordHasher(BasePasswordHasher):\\n    \\"\\"\\"\\n    Password hashing using UNIX crypt (not recommended)\\n\\n    The crypt module is not supported on all platforms.\\n    \\"\\"\\"\\n    algorithm = \\"crypt\\"\\n    library = \\"crypt\\"\\n\\n    def salt(self):\\n        return get_random_string(2)\\n\\n    def encode(self, password, salt):\\n        crypt = self._load_library()\\n        assert len(salt) == 2\\n        data = crypt.crypt(password, salt)\\n        assert data is not None  # A platform like OpenBSD with a dummy crypt module.\\n        # we don\'t need to store the salt, but Django used to do this\\n        return \\"%s$%s$%s\\" % (self.algorithm, \'\', data)\\n\\n    def verify(self, password, encoded):\\n        crypt = self._load_library()\\n        algorithm, salt, data = encoded.split(\'$\', 2)\\n        assert algorithm == self.algorithm\\n        return constant_time_compare(data, crypt.crypt(password, data))\\n\\n    def safe_summary(self, encoded):\\n        algorithm, salt, data = encoded.split(\'$\', 2)\\n        assert algorithm == self.algorithm\\n        return OrderedDict([\\n            (_(\'algorithm\'), algorithm),\\n            (_(\'salt\'), salt),\\n            (_(\'hash\'), mask_hash(data, show=3)),\\n        ])\\n\\n    def harden_runtime(self, password, encoded):\\n        pass\\n"}\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/unary/scalar/DFTest_T.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\n\\r\\nargs <- commandArgs(TRUE)\\r\\nlibrary(Matrix)\\r\\n\\r\\nqtle = qt(as.numeric(args[1]), df=as.numeric(args[2]));\\r\\np = pt(qtle, df=as.numeric(args[2]));\\r\\npl = pt(qtle, df=as.numeric(args[2]), lower.tail=F);\\r\\n\\r\\nout = matrix(0,nrow=3, ncol=1);\\r\\nout[1,1] = qtle;\\r\\nout[2,1] = p;\\r\\nout[3,1] = pl;\\r\\n\\r\\nwriteMM(as(out, \\"CsparseMatrix\\"), args[3]); \\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/external/DynProject.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nX <- as.matrix(readMM(paste(args[1], \\"X.mtx\\", sep=\\"\\")));\\r\\nc <- as.matrix(readMM(paste(args[1], \\"c.mtx\\", sep=\\"\\")));\\r\\n\\r\\nif( ncol(X)==1 )\\r\\n{\\r\\n   Y <- X[c];\\r\\n} else {\\r\\n   Y <- X[c,c];\\r\\n}\\r\\n\\r\\nwriteMM(as(Y, \\"CsparseMatrix\\"), paste(args[2], \\"Y.mtx\\", sep=\\"\\")); " }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/applications/parfor/parfor_naive-bayes.R", "content": "#-------------------------------------------------------------\\r\\n#\\r\\n# (C) Copyright IBM Corp. 2010, 2015\\r\\n#\\r\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\r\\n# you may not use this file except in compliance with the License.\\r\\n# You may obtain a copy of the License at\\r\\n#\\r\\n#     http://www.apache.org/licenses/LICENSE-2.0\\r\\n#\\r\\n# Unless required by applicable law or agreed to in writing, software\\r\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\r\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\r\\n# See the License for the specific language governing permissions and\\r\\n# limitations under the License.\\r\\n#\\r\\n#-------------------------------------------------------------\\r\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nD = as.matrix(readMM(paste(args[1], \\"D.mtx\\", sep=\\"\\")))\\r\\nC = as.matrix(readMM(paste(args[1], \\"C.mtx\\", sep=\\"\\")))\\r\\n\\r\\n# reading input args\\r\\nnumClasses = as.integer(args[2]);\\r\\nlaplace_correction = 1\\r\\n\\r\\nnumRows = nrow(D)\\r\\nnumFeatures = ncol(D)\\r\\n\\r\\n# Compute conditionals\\r\\n\\r\\n# Compute the feature counts for each class\\r\\nclassFeatureCounts = matrix(0, numClasses, numFeatures)\\r\\nfor (i in 1:numFeatures) {\\r\\n  Col = D[,i]\\r\\n  classFeatureCounts[,i] = aggregate(as.vector(Col), by=list(as.vector(C)), FUN=sum)[,2];\\r\\n}\\r\\n\\r\\n# Compute the total feature count for each class \\r\\n# and add the number of features to this sum\\r\\n# for subsequent regularization (Laplace\'s rule)\\r\\nclassSums = rowSums(classFeatureCounts) + numFeatures*laplace_correction\\r\\n\\r\\n# Compute class conditional probabilities\\r\\nrepClassSums = classSums %*% matrix(1,1,numFeatures);\\r\\nclass_conditionals = (classFeatureCounts + laplace_correction) / repClassSums;\\r\\n\\r\\n# Compute class priors\\r\\nclass_counts = aggregate(as.vector(C), by=list(as.vector(C)), FUN=length)[,2]\\r\\nclass_prior = class_counts / numRows;\\r\\n\\r\\n# write out the model\\r\\nwriteMM(as(class_prior, \\"CsparseMatrix\\"), paste(args[3], \\"class_prior\\", sep=\\"\\"));\\r\\nwriteMM(as(class_conditionals, \\"CsparseMatrix\\"), paste(args[3], \\"class_conditionals\\", sep=\\"\\"));\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/recompile/remove_empty_recompile.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nX <- readMM(paste(args[1], \\"X.mtx\\", sep=\\"\\"))\\r\\n\\r\\ntype = as.integer(args[2]);\\r\\n\\r\\nR = X;\\r\\n\\r\\nif( type==0 ){\\r\\n  R = as.matrix( sum(X) );\\r\\n}\\r\\nif( type==1 ){\\r\\n  R = round(X);\\r\\n}\\r\\nif( type==2 ){\\r\\n  R = t(X); \\r\\n}\\r\\nif( type==3 ){\\r\\n  R = X*(X-1);\\r\\n}\\r\\nif( type==4 ){\\r\\n  R = (X-1)*X;\\r\\n}\\r\\nif( type==5 ){\\r\\n  R = X+(X-1);\\r\\n}\\r\\nif( type==6 ){\\r\\n  R = (X-1)+X;\\r\\n}\\r\\nif( type==7 ){\\r\\n  R = X-(X+2);\\r\\n}\\r\\nif( type==8 ){\\r\\n  R = (X+2)-X;\\r\\n}\\r\\nif( type==9 ){\\r\\n  R = X%*%(X-1);\\r\\n}\\r\\nif( type==10 ){\\r\\n  R = (X-1)%*%X;\\r\\n}\\r\\nif( type==11 ){\\r\\n  R = X[1:(nrow(X)-1), 1:(ncol(X)-1)];\\r\\n}\\r\\nif( type==12 ){\\r\\n  X[1,] = X[2,];\\r\\n  R = X;\\r\\n}\\r\\n\\r\\nwriteMM(as(R, \\"CsparseMatrix\\"), paste(args[3], \\"R\\", sep=\\"\\")); " }'

line: b'{"repo_name":"nikolay-fedotov/tempest","ref":"refs/heads/master","path":"tempest/api/object_storage/test_account_services_negative.py","content":"# Copyright (C) 2013 eNovance SAS \\u003clicensing@enovance.com\\u003e\\n#\\n# Author: Joe H. Rahme \\u003cjoe.hakim.rahme@enovance.com\\u003e\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\nfrom tempest.api.object_storage import base\\nfrom tempest import clients\\nfrom tempest import exceptions\\nfrom tempest import test\\n\\n\\nclass AccountNegativeTest(base.BaseObjectTest):\\n\\n    @test.attr(type=[\'negative\', \'gate\'])\\n    def test_list_containers_with_non_authorized_user(self):\\n        # list containers using non-authorized user\\n\\n        # create user\\n        self.data.setup_test_user()\\n        test_os = clients.Manager(self.data.test_credentials)\\n        test_auth_provider = test_os.auth_provider\\n        # Get auth for the test user\\n        test_auth_provider.auth_data\\n\\n        # Get fresh auth for test user and set it to next auth request for\\n        # custom_account_client\\n        delattr(test_auth_provider, \'auth_data\')\\n        test_auth_new_data = test_auth_provider.auth_data\\n        self.custom_account_client.auth_provider.set_alt_auth_data(\\n            request_part=\'headers\',\\n            auth_data=test_auth_new_data\\n        )\\n\\n        params = {\'format\': \'json\'}\\n        # list containers with non-authorized user token\\n        self.assertRaises(exceptions.Unauthorized,\\n                          self.custom_account_client.list_account_containers,\\n                          params=params)\\n"}\n'
line: b'{"repo_name":"adobe/chromium","ref":"refs/heads/master","path":"third_party/closure_linter/closure_linter/error_fixer.py","content":"#!/usr/bin/env python\\n#\\n# Copyright 2007 The Closure Linter Authors. All Rights Reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#      http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS-IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\n\\"\\"\\"Main class responsible for automatically fixing simple style violations.\\"\\"\\"\\n\\n__author__ = \'robbyw@google.com (Robert Walker)\'\\n\\nimport re\\n\\nimport gflags as flags\\nfrom closure_linter import errors\\nfrom closure_linter import javascriptstatetracker\\nfrom closure_linter import javascripttokens\\nfrom closure_linter import requireprovidesorter\\nfrom closure_linter import tokenutil\\nfrom closure_linter.common import errorhandler\\n\\n# Shorthand\\nToken = javascripttokens.JavaScriptToken\\nType = javascripttokens.JavaScriptTokenType\\n\\nEND_OF_FLAG_TYPE = re.compile(r\'(}?\\\\s*)$\')\\n\\n# Regex to represent common mistake inverting author name and email as\\n# @author User Name (user@company)\\nINVERTED_AUTHOR_SPEC = re.compile(r\'(?P\\u003cleading_whitespace\\u003e\\\\s*)\'\\n                                  \'(?P\\u003cname\\u003e[^(]+)\'\\n                                  \'(?P\\u003cwhitespace_after_name\\u003e\\\\s+)\'\\n                                  \'\\\\(\'\\n                                  \'(?P\\u003cemail\\u003e[^\\\\s]+@[^)\\\\s]+)\'\\n                                  \'\\\\)\'\\n                                  \'(?P\\u003ctrailing_characters\\u003e.*)\')\\n\\nFLAGS = flags.FLAGS\\nflags.DEFINE_boolean(\'disable_indentation_fixing\', False,\\n                     \'Whether to disable automatic fixing of indentation.\')\\n\\n\\nclass ErrorFixer(errorhandler.ErrorHandler):\\n  \\"\\"\\"Object that fixes simple style errors.\\"\\"\\"\\n\\n  def __init__(self, external_file=None):\\n    \\"\\"\\"Initialize the error fixer.\\n\\n    Args:\\n      external_file: If included, all output will be directed to this file\\n          instead of overwriting the files the errors are found in.\\n    \\"\\"\\"\\n    errorhandler.ErrorHandler.__init__(self)\\n\\n    self._file_name = None\\n    self._file_token = None\\n    self._external_file = external_file\\n\\n  def HandleFile(self, filename, first_token):\\n    \\"\\"\\"Notifies this ErrorPrinter that subsequent errors are in filename.\\n\\n    Args:\\n      filename: The name of the file about to be checked.\\n      first_token: The first token in the file.\\n    \\"\\"\\"\\n    self._file_name = filename\\n    self._file_token = first_token\\n    self._file_fix_count = 0\\n    self._file_changed_lines = set()\\n\\n  def _AddFix(self, tokens):\\n    \\"\\"\\"Adds the fix to the internal count.\\n\\n    Args:\\n      tokens: The token or sequence of tokens changed to fix an error.\\n    \\"\\"\\"\\n    self._file_fix_count += 1\\n    if hasattr(tokens, \'line_number\'):\\n      self._file_changed_lines.add(tokens.line_number)\\n    else:\\n      for token in tokens:\\n        self._file_changed_lines.add(token.line_number)\\n\\n  def HandleError(self, error):\\n    \\"\\"\\"Attempts to fix the error.\\n\\n    Args:\\n      error: The error object\\n    \\"\\"\\"\\n    code = error.code\\n    token = error.token\\n\\n    if code == errors.JSDOC_PREFER_QUESTION_TO_PIPE_NULL:\\n      iterator = token.attached_object.type_start_token\\n      if iterator.type == Type.DOC_START_BRACE or iterator.string.isspace():\\n        iterator = iterator.next\\n\\n      leading_space = len(iterator.string) - len(iterator.string.lstrip())\\n      iterator.string = \'%s?%s\' % (\' \' * leading_space,\\n                                   iterator.string.lstrip())\\n\\n      # Cover the no outer brace case where the end token is part of the type.\\n      while iterator and iterator != token.attached_object.type_end_token.next:\\n        iterator.string = iterator.string.replace(\\n            \'null|\', \'\').replace(\'|null\', \'\')\\n        iterator = iterator.next\\n\\n      # Create a new flag object with updated type info.\\n      token.attached_object = javascriptstatetracker.JsDocFlag(token)\\n      self._AddFix(token)\\n\\n    elif code == errors.JSDOC_MISSING_OPTIONAL_TYPE:\\n      iterator = token.attached_object.type_end_token\\n      if iterator.type == Type.DOC_END_BRACE or iterator.string.isspace():\\n        iterator = iterator.previous\\n\\n      ending_space = len(iterator.string) - len(iterator.string.rstrip())\\n      iterator.string = \'%s=%s\' % (iterator.string.rstrip(),\\n                                   \' \' * ending_space)\\n\\n      # Create a new flag object with updated type info.\\n      token.attached_object = javascriptstatetracker.JsDocFlag(token)\\n      self._AddFix(token)\\n\\n    elif code in (errors.MISSING_SEMICOLON_AFTER_FUNCTION,\\n                  errors.MISSING_SEMICOLON):\\n      semicolon_token = Token(\';\', Type.SEMICOLON, token.line,\\n                              token.line_number)\\n      tokenutil.InsertTokenAfter(semicolon_token, token)\\n      token.metadata.is_implied_semicolon = False\\n      semicolon_token.metadata.is_implied_semicolon = False\\n      self._AddFix(token)\\n\\n    elif code in (errors.ILLEGAL_SEMICOLON_AFTER_FUNCTION,\\n                  errors.REDUNDANT_SEMICOLON,\\n                  errors.COMMA_AT_END_OF_LITERAL):\\n      tokenutil.DeleteToken(token)\\n      self._AddFix(token)\\n\\n    elif code == errors.INVALID_JSDOC_TAG:\\n      if token.string == \'@returns\':\\n        token.string = \'@return\'\\n        self._AddFix(token)\\n\\n    elif code == errors.FILE_MISSING_NEWLINE:\\n      # This error is fixed implicitly by the way we restore the file\\n      self._AddFix(token)\\n\\n    elif code == errors.MISSING_SPACE:\\n      if error.position:\\n        if error.position.IsAtBeginning():\\n          tokenutil.InsertSpaceTokenAfter(token.previous)\\n        elif error.position.IsAtEnd(token.string):\\n          tokenutil.InsertSpaceTokenAfter(token)\\n        else:\\n          token.string = error.position.Set(token.string, \' \')\\n        self._AddFix(token)\\n\\n    elif code == errors.EXTRA_SPACE:\\n      if error.position:\\n        token.string = error.position.Set(token.string, \'\')\\n        self._AddFix(token)\\n\\n    elif code == errors.JSDOC_TAG_DESCRIPTION_ENDS_WITH_INVALID_CHARACTER:\\n      token.string = error.position.Set(token.string, \'.\')\\n      self._AddFix(token)\\n\\n    elif code == errors.MISSING_LINE:\\n      if error.position.IsAtBeginning():\\n        tokenutil.InsertBlankLineAfter(token.previous)\\n      else:\\n        tokenutil.InsertBlankLineAfter(token)\\n      self._AddFix(token)\\n\\n    elif code == errors.EXTRA_LINE:\\n      tokenutil.DeleteToken(token)\\n      self._AddFix(token)\\n\\n    elif code == errors.WRONG_BLANK_LINE_COUNT:\\n      if not token.previous:\\n        # TODO(user): Add an insertBefore method to tokenutil.\\n        return\\n\\n      num_lines = error.fix_data\\n      should_delete = False\\n\\n      if num_lines \\u003c 0:\\n        num_lines *= -1\\n        should_delete = True\\n\\n      for i in xrange(1, num_lines + 1):\\n        if should_delete:\\n          # TODO(user): DeleteToken should update line numbers.\\n          tokenutil.DeleteToken(token.previous)\\n        else:\\n          tokenutil.InsertBlankLineAfter(token.previous)\\n        self._AddFix(token)\\n\\n    elif code == errors.UNNECESSARY_DOUBLE_QUOTED_STRING:\\n      end_quote = tokenutil.Search(token, Type.DOUBLE_QUOTE_STRING_END)\\n      if end_quote:\\n        single_quote_start = Token(\\n            \\"\'\\", Type.SINGLE_QUOTE_STRING_START, token.line, token.line_number)\\n        single_quote_end = Token(\\n            \\"\'\\", Type.SINGLE_QUOTE_STRING_START, end_quote.line,\\n            token.line_number)\\n\\n        tokenutil.InsertTokenAfter(single_quote_start, token)\\n        tokenutil.InsertTokenAfter(single_quote_end, end_quote)\\n        tokenutil.DeleteToken(token)\\n        tokenutil.DeleteToken(end_quote)\\n        self._AddFix([token, end_quote])\\n\\n    elif code == errors.MISSING_BRACES_AROUND_TYPE:\\n      fixed_tokens = []\\n      start_token = token.attached_object.type_start_token\\n\\n      if start_token.type != Type.DOC_START_BRACE:\\n        leading_space = (\\n            len(start_token.string) - len(start_token.string.lstrip()))\\n        if leading_space:\\n          start_token = tokenutil.SplitToken(start_token, leading_space)\\n          # Fix case where start and end token were the same.\\n          if token.attached_object.type_end_token == start_token.previous:\\n            token.attached_object.type_end_token = start_token\\n\\n        new_token = Token(\'{\', Type.DOC_START_BRACE, start_token.line,\\n                          start_token.line_number)\\n        tokenutil.InsertTokenAfter(new_token, start_token.previous)\\n        token.attached_object.type_start_token = new_token\\n        fixed_tokens.append(new_token)\\n\\n      end_token = token.attached_object.type_end_token\\n      if end_token.type != Type.DOC_END_BRACE:\\n        # If the start token was a brace, the end token will be a\\n        # FLAG_ENDING_TYPE token, if there wasn\'t a starting brace then\\n        # the end token is the last token of the actual type.\\n        last_type = end_token\\n        if not fixed_tokens:\\n          last_type = end_token.previous\\n\\n        while last_type.string.isspace():\\n          last_type = last_type.previous\\n\\n        # If there was no starting brace then a lone end brace wouldn\'t have\\n        # been type end token. Now that we\'ve added any missing start brace,\\n        # see if the last effective type token was an end brace.\\n        if last_type.type != Type.DOC_END_BRACE:\\n          trailing_space = (len(last_type.string) -\\n                            len(last_type.string.rstrip()))\\n          if trailing_space:\\n            tokenutil.SplitToken(last_type,\\n                                 len(last_type.string) - trailing_space)\\n\\n          new_token = Token(\'}\', Type.DOC_END_BRACE, last_type.line,\\n                            last_type.line_number)\\n          tokenutil.InsertTokenAfter(new_token, last_type)\\n          token.attached_object.type_end_token = new_token\\n          fixed_tokens.append(new_token)\\n\\n      self._AddFix(fixed_tokens)\\n\\n    elif code == errors.GOOG_REQUIRES_NOT_ALPHABETIZED:\\n      require_start_token = error.fix_data\\n      sorter = requireprovidesorter.RequireProvideSorter()\\n      sorter.FixRequires(require_start_token)\\n\\n      self._AddFix(require_start_token)\\n\\n    elif code == errors.GOOG_PROVIDES_NOT_ALPHABETIZED:\\n      provide_start_token = error.fix_data\\n      sorter = requireprovidesorter.RequireProvideSorter()\\n      sorter.FixProvides(provide_start_token)\\n\\n      self._AddFix(provide_start_token)\\n\\n    elif code == errors.UNNECESSARY_BRACES_AROUND_INHERIT_DOC:\\n      if token.previous.string == \'{\' and token.next.string == \'}\':\\n        tokenutil.DeleteToken(token.previous)\\n        tokenutil.DeleteToken(token.next)\\n        self._AddFix([token])\\n\\n    elif code == errors.INVALID_AUTHOR_TAG_DESCRIPTION:\\n      match = INVERTED_AUTHOR_SPEC.match(token.string)\\n      if match:\\n        token.string = \'%s%s%s(%s)%s\' % (match.group(\'leading_whitespace\'),\\n                                         match.group(\'email\'),\\n                                         match.group(\'whitespace_after_name\'),\\n                                         match.group(\'name\'),\\n                                         match.group(\'trailing_characters\'))\\n        self._AddFix(token)\\n\\n    elif (code == errors.WRONG_INDENTATION and\\n          not FLAGS.disable_indentation_fixing):\\n      token = tokenutil.GetFirstTokenInSameLine(token)\\n      actual = error.position.start\\n      expected = error.position.length\\n\\n      if token.type in (Type.WHITESPACE, Type.PARAMETERS) and actual != 0:\\n        token.string = token.string.lstrip() + (\' \' * expected)\\n        self._AddFix([token])\\n      else:\\n        # We need to add indentation.\\n        new_token = Token(\' \' * expected, Type.WHITESPACE,\\n                          token.line, token.line_number)\\n        # Note that we\'ll never need to add indentation at the first line,\\n        # since it will always not be indented.  Therefore it\'s safe to assume\\n        # token.previous exists.\\n        tokenutil.InsertTokenAfter(new_token, token.previous)\\n        self._AddFix([token])\\n\\n    elif code in [errors.MALFORMED_END_OF_SCOPE_COMMENT,\\n                  errors.MISSING_END_OF_SCOPE_COMMENT]:\\n      # Only fix cases where }); is found with no trailing content on the line\\n      # other than a comment. Value of \'token\' is set to } for this error.\\n      if (token.type == Type.END_BLOCK and\\n          token.next.type == Type.END_PAREN and\\n          token.next.next.type == Type.SEMICOLON):\\n        current_token = token.next.next.next\\n        removed_tokens = []\\n        while current_token and current_token.line_number == token.line_number:\\n          if current_token.IsAnyType(Type.WHITESPACE,\\n                                     Type.START_SINGLE_LINE_COMMENT,\\n                                     Type.COMMENT):\\n            removed_tokens.append(current_token)\\n            current_token = current_token.next\\n          else:\\n            return\\n\\n        if removed_tokens:\\n          tokenutil.DeleteTokens(removed_tokens[0], len(removed_tokens))\\n\\n        whitespace_token = Token(\'  \', Type.WHITESPACE, token.line,\\n                                 token.line_number)\\n        start_comment_token = Token(\'//\', Type.START_SINGLE_LINE_COMMENT,\\n                                    token.line, token.line_number)\\n        comment_token = Token(\' goog.scope\', Type.COMMENT, token.line,\\n                              token.line_number)\\n        insertion_tokens = [whitespace_token, start_comment_token,\\n                            comment_token]\\n\\n        tokenutil.InsertTokensAfter(insertion_tokens, token.next.next)\\n        self._AddFix(removed_tokens + insertion_tokens)\\n\\n    elif code in [errors.EXTRA_GOOG_PROVIDE, errors.EXTRA_GOOG_REQUIRE]:\\n      tokens_in_line = tokenutil.GetAllTokensInSameLine(token)\\n      tokenutil.DeleteTokens(tokens_in_line[0], len(tokens_in_line))\\n      self._AddFix(tokens_in_line)\\n\\n    elif code in [errors.MISSING_GOOG_PROVIDE, errors.MISSING_GOOG_REQUIRE]:\\n      is_provide = code == errors.MISSING_GOOG_PROVIDE\\n      is_require = code == errors.MISSING_GOOG_REQUIRE\\n\\n      missing_namespaces = error.fix_data[0]\\n      need_blank_line = error.fix_data[1]\\n\\n      if need_blank_line is None:\\n        # TODO(user): This happens when there are no existing\\n        # goog.provide or goog.require statements to position new statements\\n        # relative to. Consider handling this case with a heuristic.\\n        return\\n\\n      insert_location = token.previous\\n\\n      # If inserting a missing require with no existing requires, insert a\\n      # blank line first.\\n      if need_blank_line and is_require:\\n        tokenutil.InsertBlankLineAfter(insert_location)\\n        insert_location = insert_location.next\\n\\n      for missing_namespace in missing_namespaces:\\n        new_tokens = self._GetNewRequireOrProvideTokens(\\n            is_provide, missing_namespace, insert_location.line_number + 1)\\n        tokenutil.InsertLineAfter(insert_location, new_tokens)\\n        insert_location = new_tokens[-1]\\n        self._AddFix(new_tokens)\\n\\n      # If inserting a missing provide with no existing provides, insert a\\n      # blank line after.\\n      if need_blank_line and is_provide:\\n        tokenutil.InsertBlankLineAfter(insert_location)\\n\\n  def _GetNewRequireOrProvideTokens(self, is_provide, namespace, line_number):\\n    \\"\\"\\"Returns a list of tokens to create a goog.require/provide statement.\\n\\n    Args:\\n      is_provide: True if getting tokens for a provide, False for require.\\n      namespace: The required or provided namespaces to get tokens for.\\n      line_number: The line number the new require or provide statement will be\\n          on.\\n\\n    Returns:\\n      Tokens to create a new goog.require or goog.provide statement.\\n    \\"\\"\\"\\n    string = \'goog.require\'\\n    if is_provide:\\n      string = \'goog.provide\'\\n    line_text = string + \'(\\\\\'\' + namespace + \'\\\\\');\\\\n\'\\n    return [\\n        Token(string, Type.IDENTIFIER, line_text, line_number),\\n        Token(\'(\', Type.START_PAREN, line_text, line_number),\\n        Token(\'\\\\\'\', Type.SINGLE_QUOTE_STRING_START, line_text, line_number),\\n        Token(namespace, Type.STRING_TEXT, line_text, line_number),\\n        Token(\'\\\\\'\', Type.SINGLE_QUOTE_STRING_END, line_text, line_number),\\n        Token(\')\', Type.END_PAREN, line_text, line_number),\\n        Token(\';\', Type.SEMICOLON, line_text, line_number)\\n        ]\\n\\n  def FinishFile(self):\\n    \\"\\"\\"Called when the current file has finished style checking.\\n\\n    Used to go back and fix any errors in the file.\\n    \\"\\"\\"\\n    if self._file_fix_count:\\n      f = self._external_file\\n      if not f:\\n        print \'Fixed %d errors in %s\' % (self._file_fix_count, self._file_name)\\n        f = open(self._file_name, \'w\')\\n\\n      token = self._file_token\\n      char_count = 0\\n      while token:\\n        f.write(token.string)\\n        char_count += len(token.string)\\n\\n        if token.IsLastInLine():\\n          f.write(\'\\\\n\')\\n          if char_count \\u003e 80 and token.line_number in self._file_changed_lines:\\n            print \'WARNING: Line %d of %s is now longer than 80 characters.\' % (\\n                token.line_number, self._file_name)\\n\\n          char_count = 0\\n\\n        token = token.next\\n\\n      if not self._external_file:\\n        # Close the file if we created it\\n        f.close()\\n"}\n'
line: b'{"repo_name":"ryfeus/lambda-packs","ref":"refs/heads/master","path":"Skimage_numpy/source/PIL/JpegImagePlugin.py","content":"#\\n# The Python Imaging Library.\\n# $Id$\\n#\\n# JPEG (JFIF) file handling\\n#\\n# See \\"Digital Compression and Coding of Continuous-Tone Still Images,\\n# Part 1, Requirements and Guidelines\\" (CCITT T.81 / ISO 10918-1)\\n#\\n# History:\\n# 1995-09-09 fl   Created\\n# 1995-09-13 fl   Added full parser\\n# 1996-03-25 fl   Added hack to use the IJG command line utilities\\n# 1996-05-05 fl   Workaround Photoshop 2.5 CMYK polarity bug\\n# 1996-05-28 fl   Added draft support, JFIF version (0.1)\\n# 1996-12-30 fl   Added encoder options, added progression property (0.2)\\n# 1997-08-27 fl   Save mode 1 images as BW (0.3)\\n# 1998-07-12 fl   Added YCbCr to draft and save methods (0.4)\\n# 1998-10-19 fl   Don\'t hang on files using 16-bit DQT\'s (0.4.1)\\n# 2001-04-16 fl   Extract DPI settings from JFIF files (0.4.2)\\n# 2002-07-01 fl   Skip pad bytes before markers; identify Exif files (0.4.3)\\n# 2003-04-25 fl   Added experimental EXIF decoder (0.5)\\n# 2003-06-06 fl   Added experimental EXIF GPSinfo decoder\\n# 2003-09-13 fl   Extract COM markers\\n# 2009-09-06 fl   Added icc_profile support (from Florian Hoech)\\n# 2009-03-06 fl   Changed CMYK handling; always use Adobe polarity (0.6)\\n# 2009-03-08 fl   Added subsampling support (from Justin Huff).\\n#\\n# Copyright (c) 1997-2003 by Secret Labs AB.\\n# Copyright (c) 1995-1996 by Fredrik Lundh.\\n#\\n# See the README file for information on usage and redistribution.\\n#\\n\\nfrom __future__ import print_function\\n\\nimport array\\nimport struct\\nimport io\\nimport warnings\\nfrom struct import unpack_from\\nfrom PIL import Image, ImageFile, TiffImagePlugin, _binary\\nfrom PIL.JpegPresets import presets\\nfrom PIL._util import isStringType\\n\\ni8 = _binary.i8\\no8 = _binary.o8\\ni16 = _binary.i16be\\ni32 = _binary.i32be\\n\\n__version__ = \\"0.6\\"\\n\\n\\n#\\n# Parser\\n\\ndef Skip(self, marker):\\n    n = i16(self.fp.read(2))-2\\n    ImageFile._safe_read(self.fp, n)\\n\\n\\ndef APP(self, marker):\\n    #\\n    # Application marker.  Store these in the APP dictionary.\\n    # Also look for well-known application markers.\\n\\n    n = i16(self.fp.read(2))-2\\n    s = ImageFile._safe_read(self.fp, n)\\n\\n    app = \\"APP%d\\" % (marker \\u0026 15)\\n\\n    self.app[app] = s  # compatibility\\n    self.applist.append((app, s))\\n\\n    if marker == 0xFFE0 and s[:4] == b\\"JFIF\\":\\n        # extract JFIF information\\n        self.info[\\"jfif\\"] = version = i16(s, 5)  # version\\n        self.info[\\"jfif_version\\"] = divmod(version, 256)\\n        # extract JFIF properties\\n        try:\\n            jfif_unit = i8(s[7])\\n            jfif_density = i16(s, 8), i16(s, 10)\\n        except:\\n            pass\\n        else:\\n            if jfif_unit == 1:\\n                self.info[\\"dpi\\"] = jfif_density\\n            self.info[\\"jfif_unit\\"] = jfif_unit\\n            self.info[\\"jfif_density\\"] = jfif_density\\n    elif marker == 0xFFE1 and s[:5] == b\\"Exif\\\\0\\":\\n        # extract Exif information (incomplete)\\n        self.info[\\"exif\\"] = s  # FIXME: value will change\\n    elif marker == 0xFFE2 and s[:5] == b\\"FPXR\\\\0\\":\\n        # extract FlashPix information (incomplete)\\n        self.info[\\"flashpix\\"] = s  # FIXME: value will change\\n    elif marker == 0xFFE2 and s[:12] == b\\"ICC_PROFILE\\\\0\\":\\n        # Since an ICC profile can be larger than the maximum size of\\n        # a JPEG marker (64K), we need provisions to split it into\\n        # multiple markers. The format defined by the ICC specifies\\n        # one or more APP2 markers containing the following data:\\n        #   Identifying string      ASCII \\"ICC_PROFILE\\\\0\\"  (12 bytes)\\n        #   Marker sequence number  1, 2, etc (1 byte)\\n        #   Number of markers       Total of APP2\'s used (1 byte)\\n        #   Profile data            (remainder of APP2 data)\\n        # Decoders should use the marker sequence numbers to\\n        # reassemble the profile, rather than assuming that the APP2\\n        # markers appear in the correct sequence.\\n        self.icclist.append(s)\\n    elif marker == 0xFFEE and s[:5] == b\\"Adobe\\":\\n        self.info[\\"adobe\\"] = i16(s, 5)\\n        # extract Adobe custom properties\\n        try:\\n            adobe_transform = i8(s[1])\\n        except:\\n            pass\\n        else:\\n            self.info[\\"adobe_transform\\"] = adobe_transform\\n    elif marker == 0xFFE2 and s[:4] == b\\"MPF\\\\0\\":\\n        # extract MPO information\\n        self.info[\\"mp\\"] = s[4:]\\n        # offset is current location minus buffer size\\n        # plus constant header size\\n        self.info[\\"mpoffset\\"] = self.fp.tell() - n + 4\\n\\n\\ndef COM(self, marker):\\n    #\\n    # Comment marker.  Store these in the APP dictionary.\\n    n = i16(self.fp.read(2))-2\\n    s = ImageFile._safe_read(self.fp, n)\\n\\n    self.app[\\"COM\\"] = s  # compatibility\\n    self.applist.append((\\"COM\\", s))\\n\\n\\ndef SOF(self, marker):\\n    #\\n    # Start of frame marker.  Defines the size and mode of the\\n    # image.  JPEG is colour blind, so we use some simple\\n    # heuristics to map the number of layers to an appropriate\\n    # mode.  Note that this could be made a bit brighter, by\\n    # looking for JFIF and Adobe APP markers.\\n\\n    n = i16(self.fp.read(2))-2\\n    s = ImageFile._safe_read(self.fp, n)\\n    self.size = i16(s[3:]), i16(s[1:])\\n\\n    self.bits = i8(s[0])\\n    if self.bits != 8:\\n        raise SyntaxError(\\"cannot handle %d-bit layers\\" % self.bits)\\n\\n    self.layers = i8(s[5])\\n    if self.layers == 1:\\n        self.mode = \\"L\\"\\n    elif self.layers == 3:\\n        self.mode = \\"RGB\\"\\n    elif self.layers == 4:\\n        self.mode = \\"CMYK\\"\\n    else:\\n        raise SyntaxError(\\"cannot handle %d-layer images\\" % self.layers)\\n\\n    if marker in [0xFFC2, 0xFFC6, 0xFFCA, 0xFFCE]:\\n        self.info[\\"progressive\\"] = self.info[\\"progression\\"] = 1\\n\\n    if self.icclist:\\n        # fixup icc profile\\n        self.icclist.sort()  # sort by sequence number\\n        if i8(self.icclist[0][13]) == len(self.icclist):\\n            profile = []\\n            for p in self.icclist:\\n                profile.append(p[14:])\\n            icc_profile = b\\"\\".join(profile)\\n        else:\\n            icc_profile = None  # wrong number of fragments\\n        self.info[\\"icc_profile\\"] = icc_profile\\n        self.icclist = None\\n\\n    for i in range(6, len(s), 3):\\n        t = s[i:i+3]\\n        # 4-tuples: id, vsamp, hsamp, qtable\\n        self.layer.append((t[0], i8(t[1])//16, i8(t[1]) \\u0026 15, i8(t[2])))\\n\\n\\ndef DQT(self, marker):\\n    #\\n    # Define quantization table.  Support baseline 8-bit tables\\n    # only.  Note that there might be more than one table in\\n    # each marker.\\n\\n    # FIXME: The quantization tables can be used to estimate the\\n    # compression quality.\\n\\n    n = i16(self.fp.read(2))-2\\n    s = ImageFile._safe_read(self.fp, n)\\n    while len(s):\\n        if len(s) \\u003c 65:\\n            raise SyntaxError(\\"bad quantization table marker\\")\\n        v = i8(s[0])\\n        if v//16 == 0:\\n            self.quantization[v \\u0026 15] = array.array(\\"B\\", s[1:65])\\n            s = s[65:]\\n        else:\\n            return  # FIXME: add code to read 16-bit tables!\\n            # raise SyntaxError, \\"bad quantization table element size\\"\\n\\n\\n#\\n# JPEG marker table\\n\\nMARKER = {\\n    0xFFC0: (\\"SOF0\\", \\"Baseline DCT\\", SOF),\\n    0xFFC1: (\\"SOF1\\", \\"Extended Sequential DCT\\", SOF),\\n    0xFFC2: (\\"SOF2\\", \\"Progressive DCT\\", SOF),\\n    0xFFC3: (\\"SOF3\\", \\"Spatial lossless\\", SOF),\\n    0xFFC4: (\\"DHT\\", \\"Define Huffman table\\", Skip),\\n    0xFFC5: (\\"SOF5\\", \\"Differential sequential DCT\\", SOF),\\n    0xFFC6: (\\"SOF6\\", \\"Differential progressive DCT\\", SOF),\\n    0xFFC7: (\\"SOF7\\", \\"Differential spatial\\", SOF),\\n    0xFFC8: (\\"JPG\\", \\"Extension\\", None),\\n    0xFFC9: (\\"SOF9\\", \\"Extended sequential DCT (AC)\\", SOF),\\n    0xFFCA: (\\"SOF10\\", \\"Progressive DCT (AC)\\", SOF),\\n    0xFFCB: (\\"SOF11\\", \\"Spatial lossless DCT (AC)\\", SOF),\\n    0xFFCC: (\\"DAC\\", \\"Define arithmetic coding conditioning\\", Skip),\\n    0xFFCD: (\\"SOF13\\", \\"Differential sequential DCT (AC)\\", SOF),\\n    0xFFCE: (\\"SOF14\\", \\"Differential progressive DCT (AC)\\", SOF),\\n    0xFFCF: (\\"SOF15\\", \\"Differential spatial (AC)\\", SOF),\\n    0xFFD0: (\\"RST0\\", \\"Restart 0\\", None),\\n    0xFFD1: (\\"RST1\\", \\"Restart 1\\", None),\\n    0xFFD2: (\\"RST2\\", \\"Restart 2\\", None),\\n    0xFFD3: (\\"RST3\\", \\"Restart 3\\", None),\\n    0xFFD4: (\\"RST4\\", \\"Restart 4\\", None),\\n    0xFFD5: (\\"RST5\\", \\"Restart 5\\", None),\\n    0xFFD6: (\\"RST6\\", \\"Restart 6\\", None),\\n    0xFFD7: (\\"RST7\\", \\"Restart 7\\", None),\\n    0xFFD8: (\\"SOI\\", \\"Start of image\\", None),\\n    0xFFD9: (\\"EOI\\", \\"End of image\\", None),\\n    0xFFDA: (\\"SOS\\", \\"Start of scan\\", Skip),\\n    0xFFDB: (\\"DQT\\", \\"Define quantization table\\", DQT),\\n    0xFFDC: (\\"DNL\\", \\"Define number of lines\\", Skip),\\n    0xFFDD: (\\"DRI\\", \\"Define restart interval\\", Skip),\\n    0xFFDE: (\\"DHP\\", \\"Define hierarchical progression\\", SOF),\\n    0xFFDF: (\\"EXP\\", \\"Expand reference component\\", Skip),\\n    0xFFE0: (\\"APP0\\", \\"Application segment 0\\", APP),\\n    0xFFE1: (\\"APP1\\", \\"Application segment 1\\", APP),\\n    0xFFE2: (\\"APP2\\", \\"Application segment 2\\", APP),\\n    0xFFE3: (\\"APP3\\", \\"Application segment 3\\", APP),\\n    0xFFE4: (\\"APP4\\", \\"Application segment 4\\", APP),\\n    0xFFE5: (\\"APP5\\", \\"Application segment 5\\", APP),\\n    0xFFE6: (\\"APP6\\", \\"Application segment 6\\", APP),\\n    0xFFE7: (\\"APP7\\", \\"Application segment 7\\", APP),\\n    0xFFE8: (\\"APP8\\", \\"Application segment 8\\", APP),\\n    0xFFE9: (\\"APP9\\", \\"Application segment 9\\", APP),\\n    0xFFEA: (\\"APP10\\", \\"Application segment 10\\", APP),\\n    0xFFEB: (\\"APP11\\", \\"Application segment 11\\", APP),\\n    0xFFEC: (\\"APP12\\", \\"Application segment 12\\", APP),\\n    0xFFED: (\\"APP13\\", \\"Application segment 13\\", APP),\\n    0xFFEE: (\\"APP14\\", \\"Application segment 14\\", APP),\\n    0xFFEF: (\\"APP15\\", \\"Application segment 15\\", APP),\\n    0xFFF0: (\\"JPG0\\", \\"Extension 0\\", None),\\n    0xFFF1: (\\"JPG1\\", \\"Extension 1\\", None),\\n    0xFFF2: (\\"JPG2\\", \\"Extension 2\\", None),\\n    0xFFF3: (\\"JPG3\\", \\"Extension 3\\", None),\\n    0xFFF4: (\\"JPG4\\", \\"Extension 4\\", None),\\n    0xFFF5: (\\"JPG5\\", \\"Extension 5\\", None),\\n    0xFFF6: (\\"JPG6\\", \\"Extension 6\\", None),\\n    0xFFF7: (\\"JPG7\\", \\"Extension 7\\", None),\\n    0xFFF8: (\\"JPG8\\", \\"Extension 8\\", None),\\n    0xFFF9: (\\"JPG9\\", \\"Extension 9\\", None),\\n    0xFFFA: (\\"JPG10\\", \\"Extension 10\\", None),\\n    0xFFFB: (\\"JPG11\\", \\"Extension 11\\", None),\\n    0xFFFC: (\\"JPG12\\", \\"Extension 12\\", None),\\n    0xFFFD: (\\"JPG13\\", \\"Extension 13\\", None),\\n    0xFFFE: (\\"COM\\", \\"Comment\\", COM)\\n}\\n\\n\\ndef _accept(prefix):\\n    return prefix[0:1] == b\\"\\\\377\\"\\n\\n\\n##\\n# Image plugin for JPEG and JFIF images.\\n\\nclass JpegImageFile(ImageFile.ImageFile):\\n\\n    format = \\"JPEG\\"\\n    format_description = \\"JPEG (ISO 10918)\\"\\n\\n    def _open(self):\\n\\n        s = self.fp.read(1)\\n\\n        if i8(s) != 255:\\n            raise SyntaxError(\\"not a JPEG file\\")\\n\\n        # Create attributes\\n        self.bits = self.layers = 0\\n\\n        # JPEG specifics (internal)\\n        self.layer = []\\n        self.huffman_dc = {}\\n        self.huffman_ac = {}\\n        self.quantization = {}\\n        self.app = {}  # compatibility\\n        self.applist = []\\n        self.icclist = []\\n\\n        while True:\\n\\n            i = i8(s)\\n            if i == 0xFF:\\n                s = s + self.fp.read(1)\\n                i = i16(s)\\n            else:\\n                # Skip non-0xFF junk\\n                s = self.fp.read(1)\\n                continue\\n\\n            if i in MARKER:\\n                name, description, handler = MARKER[i]\\n                # print(hex(i), name, description)\\n                if handler is not None:\\n                    handler(self, i)\\n                if i == 0xFFDA:  # start of scan\\n                    rawmode = self.mode\\n                    if self.mode == \\"CMYK\\":\\n                        rawmode = \\"CMYK;I\\"  # assume adobe conventions\\n                    self.tile = [(\\"jpeg\\", (0, 0) + self.size, 0,\\n                                 (rawmode, \\"\\"))]\\n                    # self.__offset = self.fp.tell()\\n                    break\\n                s = self.fp.read(1)\\n            elif i == 0 or i == 0xFFFF:\\n                # padded marker or junk; move on\\n                s = b\\"\\\\xff\\"\\n            elif i == 0xFF00:  # Skip extraneous data (escaped 0xFF)\\n                s = self.fp.read(1)\\n            else:\\n                raise SyntaxError(\\"no marker found\\")\\n\\n    def draft(self, mode, size):\\n\\n        if len(self.tile) != 1:\\n            return\\n\\n        # Protect from second call\\n        if self.decoderconfig:\\n            return\\n\\n        d, e, o, a = self.tile[0]\\n        scale = 0\\n\\n        if a[0] == \\"RGB\\" and mode in [\\"L\\", \\"YCbCr\\"]:\\n            self.mode = mode\\n            a = mode, \\"\\"\\n\\n        if size:\\n            scale = min(self.size[0] // size[0], self.size[1] // size[1])\\n            for s in [8, 4, 2, 1]:\\n                if scale \\u003e= s:\\n                    break\\n            e = e[0], e[1], (e[2]-e[0]+s-1)//s+e[0], (e[3]-e[1]+s-1)//s+e[1]\\n            self.size = ((self.size[0]+s-1)//s, (self.size[1]+s-1)//s)\\n            scale = s\\n\\n        self.tile = [(d, e, o, a)]\\n        self.decoderconfig = (scale, 0)\\n\\n        return self\\n\\n    def load_djpeg(self):\\n\\n        # ALTERNATIVE: handle JPEGs via the IJG command line utilities\\n\\n        import subprocess\\n        import tempfile\\n        import os\\n        f, path = tempfile.mkstemp()\\n        os.close(f)\\n        if os.path.exists(self.filename):\\n            subprocess.check_call([\\"djpeg\\", \\"-outfile\\", path, self.filename])\\n        else:\\n            raise ValueError(\\"Invalid Filename\\")\\n\\n        try:\\n            _im = Image.open(path)\\n            _im.load()\\n            self.im = _im.im\\n        finally:\\n            try:\\n                os.unlink(path)\\n            except OSError:\\n                pass\\n\\n        self.mode = self.im.mode\\n        self.size = self.im.size\\n\\n        self.tile = []\\n\\n    def _getexif(self):\\n        return _getexif(self)\\n\\n    def _getmp(self):\\n        return _getmp(self)\\n\\n\\ndef _fixup_dict(src_dict):\\n    # Helper function for _getexif()\\n    # returns a dict with any single item tuples/lists as individual values\\n    def _fixup(value):\\n        try:\\n            if len(value) == 1 and not isinstance(value, dict):\\n                return value[0]\\n        except: pass\\n        return value\\n\\n    return {k: _fixup(v) for k, v in src_dict.items()}\\n\\n\\ndef _getexif(self):\\n    # Extract EXIF information.  This method is highly experimental,\\n    # and is likely to be replaced with something better in a future\\n    # version.\\n\\n    # The EXIF record consists of a TIFF file embedded in a JPEG\\n    # application marker (!).\\n    try:\\n        data = self.info[\\"exif\\"]\\n    except KeyError:\\n        return None\\n    file = io.BytesIO(data[6:])\\n    head = file.read(8)\\n    # process dictionary\\n    info = TiffImagePlugin.ImageFileDirectory_v1(head)\\n    info.load(file)\\n    exif = dict(_fixup_dict(info))\\n    # get exif extension\\n    try:\\n        # exif field 0x8769 is an offset pointer to the location\\n        # of the nested embedded exif ifd.\\n        # It should be a long, but may be corrupted.\\n        file.seek(exif[0x8769])\\n    except (KeyError, TypeError):\\n        pass\\n    else:\\n        info = TiffImagePlugin.ImageFileDirectory_v1(head)\\n        info.load(file)\\n        exif.update(_fixup_dict(info))\\n    # get gpsinfo extension\\n    try:\\n        # exif field 0x8825 is an offset pointer to the location\\n        # of the nested embedded gps exif ifd.\\n        # It should be a long, but may be corrupted.\\n        file.seek(exif[0x8825])\\n    except (KeyError, TypeError):\\n        pass\\n    else:\\n        info = TiffImagePlugin.ImageFileDirectory_v1(head)\\n        info.load(file)\\n        exif[0x8825] = _fixup_dict(info)\\n\\n    return exif\\n\\n\\ndef _getmp(self):\\n    # Extract MP information.  This method was inspired by the \\"highly\\n    # experimental\\" _getexif version that\'s been in use for years now,\\n    # itself based on the ImageFileDirectory class in the TIFF plug-in.\\n\\n    # The MP record essentially consists of a TIFF file embedded in a JPEG\\n    # application marker.\\n    try:\\n        data = self.info[\\"mp\\"]\\n    except KeyError:\\n        return None\\n    file_contents = io.BytesIO(data)\\n    head = file_contents.read(8)\\n    endianness = \'\\u003e\' if head[:4] == b\'\\\\x4d\\\\x4d\\\\x00\\\\x2a\' else \'\\u003c\'\\n    # process dictionary\\n    try:\\n        info = TiffImagePlugin.ImageFileDirectory_v2(head)\\n        info.load(file_contents)\\n        mp = dict(info)\\n    except:\\n        raise SyntaxError(\\"malformed MP Index (unreadable directory)\\")\\n    # it\'s an error not to have a number of images\\n    try:\\n        quant = mp[0xB001]\\n    except KeyError:\\n        raise SyntaxError(\\"malformed MP Index (no number of images)\\")\\n    # get MP entries\\n    mpentries = []\\n    try:\\n        rawmpentries = mp[0xB002]\\n        for entrynum in range(0, quant):\\n            unpackedentry = unpack_from(\\n                \'{}LLLHH\'.format(endianness), rawmpentries, entrynum * 16)\\n            labels = (\'Attribute\', \'Size\', \'DataOffset\', \'EntryNo1\',\\n                      \'EntryNo2\')\\n            mpentry = dict(zip(labels, unpackedentry))\\n            mpentryattr = {\\n                \'DependentParentImageFlag\': bool(mpentry[\'Attribute\'] \\u0026\\n                                                 (1 \\u003c\\u003c 31)),\\n                \'DependentChildImageFlag\': bool(mpentry[\'Attribute\'] \\u0026\\n                                                (1 \\u003c\\u003c 30)),\\n                \'RepresentativeImageFlag\': bool(mpentry[\'Attribute\'] \\u0026\\n                                                (1 \\u003c\\u003c 29)),\\n                \'Reserved\': (mpentry[\'Attribute\'] \\u0026 (3 \\u003c\\u003c 27)) \\u003e\\u003e 27,\\n                \'ImageDataFormat\': (mpentry[\'Attribute\'] \\u0026 (7 \\u003c\\u003c 24)) \\u003e\\u003e 24,\\n                \'MPType\': mpentry[\'Attribute\'] \\u0026 0x00FFFFFF\\n            }\\n            if mpentryattr[\'ImageDataFormat\'] == 0:\\n                mpentryattr[\'ImageDataFormat\'] = \'JPEG\'\\n            else:\\n                raise SyntaxError(\\"unsupported picture format in MPO\\")\\n            mptypemap = {\\n                0x000000: \'Undefined\',\\n                0x010001: \'Large Thumbnail (VGA Equivalent)\',\\n                0x010002: \'Large Thumbnail (Full HD Equivalent)\',\\n                0x020001: \'Multi-Frame Image (Panorama)\',\\n                0x020002: \'Multi-Frame Image: (Disparity)\',\\n                0x020003: \'Multi-Frame Image: (Multi-Angle)\',\\n                0x030000: \'Baseline MP Primary Image\'\\n            }\\n            mpentryattr[\'MPType\'] = mptypemap.get(mpentryattr[\'MPType\'],\\n                                                  \'Unknown\')\\n            mpentry[\'Attribute\'] = mpentryattr\\n            mpentries.append(mpentry)\\n        mp[0xB002] = mpentries\\n    except KeyError:\\n        raise SyntaxError(\\"malformed MP Index (bad MP Entry)\\")\\n    # Next we should try and parse the individual image unique ID list;\\n    # we don\'t because I\'ve never seen this actually used in a real MPO\\n    # file and so can\'t test it.\\n    return mp\\n\\n\\n# --------------------------------------------------------------------\\n# stuff to save JPEG files\\n\\nRAWMODE = {\\n    \\"1\\": \\"L\\",\\n    \\"L\\": \\"L\\",\\n    \\"RGB\\": \\"RGB\\",\\n    \\"RGBA\\": \\"RGB\\",\\n    \\"RGBX\\": \\"RGB\\",\\n    \\"CMYK\\": \\"CMYK;I\\",  # assume adobe conventions\\n    \\"YCbCr\\": \\"YCbCr\\",\\n}\\n\\nzigzag_index = (0,  1,  5,  6, 14, 15, 27, 28,\\n                2,  4,  7, 13, 16, 26, 29, 42,\\n                3,  8, 12, 17, 25, 30, 41, 43,\\n                9, 11, 18, 24, 31, 40, 44, 53,\\n               10, 19, 23, 32, 39, 45, 52, 54,\\n               20, 22, 33, 38, 46, 51, 55, 60,\\n               21, 34, 37, 47, 50, 56, 59, 61,\\n               35, 36, 48, 49, 57, 58, 62, 63)\\n\\nsamplings = {(1, 1, 1, 1, 1, 1): 0,\\n             (2, 1, 1, 1, 1, 1): 1,\\n             (2, 2, 1, 1, 1, 1): 2,\\n             }\\n\\n\\ndef convert_dict_qtables(qtables):\\n    qtables = [qtables[key] for key in range(len(qtables)) if key in qtables]\\n    for idx, table in enumerate(qtables):\\n        qtables[idx] = [table[i] for i in zigzag_index]\\n    return qtables\\n\\n\\ndef get_sampling(im):\\n    # There\'s no subsampling when image have only 1 layer\\n    # (grayscale images) or when they are CMYK (4 layers),\\n    # so set subsampling to default value.\\n    #\\n    # NOTE: currently Pillow can\'t encode JPEG to YCCK format.\\n    # If YCCK support is added in the future, subsampling code will have\\n    # to be updated (here and in JpegEncode.c) to deal with 4 layers.\\n    if not hasattr(im, \'layers\') or im.layers in (1, 4):\\n        return -1\\n    sampling = im.layer[0][1:3] + im.layer[1][1:3] + im.layer[2][1:3]\\n    return samplings.get(sampling, -1)\\n\\n\\ndef _save(im, fp, filename):\\n\\n    try:\\n        rawmode = RAWMODE[im.mode]\\n    except KeyError:\\n        raise IOError(\\"cannot write mode %s as JPEG\\" % im.mode)\\n\\n    if im.mode == \'RGBA\':\\n        warnings.warn(\\n            \'You are saving RGBA image as JPEG. The alpha channel will be \'\\n            \'discarded. This conversion is deprecated and will be disabled \'\\n            \'in Pillow 3.7. Please, convert the image to RGB explicitly.\',\\n            DeprecationWarning\\n        )\\n\\n    info = im.encoderinfo\\n\\n    dpi = [int(round(x)) for x in info.get(\\"dpi\\", (0, 0))]\\n\\n    quality = info.get(\\"quality\\", 0)\\n    subsampling = info.get(\\"subsampling\\", -1)\\n    qtables = info.get(\\"qtables\\")\\n\\n    if quality == \\"keep\\":\\n        quality = 0\\n        subsampling = \\"keep\\"\\n        qtables = \\"keep\\"\\n    elif quality in presets:\\n        preset = presets[quality]\\n        quality = 0\\n        subsampling = preset.get(\'subsampling\', -1)\\n        qtables = preset.get(\'quantization\')\\n    elif not isinstance(quality, int):\\n        raise ValueError(\\"Invalid quality setting\\")\\n    else:\\n        if subsampling in presets:\\n            subsampling = presets[subsampling].get(\'subsampling\', -1)\\n        if isStringType(qtables) and qtables in presets:\\n            qtables = presets[qtables].get(\'quantization\')\\n\\n    if subsampling == \\"4:4:4\\":\\n        subsampling = 0\\n    elif subsampling == \\"4:2:2\\":\\n        subsampling = 1\\n    elif subsampling == \\"4:1:1\\":\\n        subsampling = 2\\n    elif subsampling == \\"keep\\":\\n        if im.format != \\"JPEG\\":\\n            raise ValueError(\\n                \\"Cannot use \'keep\' when original image is not a JPEG\\")\\n        subsampling = get_sampling(im)\\n\\n    def validate_qtables(qtables):\\n        if qtables is None:\\n            return qtables\\n        if isStringType(qtables):\\n            try:\\n                lines = [int(num) for line in qtables.splitlines()\\n                         for num in line.split(\'#\', 1)[0].split()]\\n            except ValueError:\\n                raise ValueError(\\"Invalid quantization table\\")\\n            else:\\n                qtables = [lines[s:s+64] for s in range(0, len(lines), 64)]\\n        if isinstance(qtables, (tuple, list, dict)):\\n            if isinstance(qtables, dict):\\n                qtables = convert_dict_qtables(qtables)\\n            elif isinstance(qtables, tuple):\\n                qtables = list(qtables)\\n            if not (0 \\u003c len(qtables) \\u003c 5):\\n                raise ValueError(\\"None or too many quantization tables\\")\\n            for idx, table in enumerate(qtables):\\n                try:\\n                    if len(table) != 64:\\n                        raise\\n                    table = array.array(\'B\', table)\\n                except TypeError:\\n                    raise ValueError(\\"Invalid quantization table\\")\\n                else:\\n                    qtables[idx] = list(table)\\n            return qtables\\n\\n    if qtables == \\"keep\\":\\n        if im.format != \\"JPEG\\":\\n            raise ValueError(\\n                \\"Cannot use \'keep\' when original image is not a JPEG\\")\\n        qtables = getattr(im, \\"quantization\\", None)\\n    qtables = validate_qtables(qtables)\\n\\n    extra = b\\"\\"\\n\\n    icc_profile = info.get(\\"icc_profile\\")\\n    if icc_profile:\\n        ICC_OVERHEAD_LEN = 14\\n        MAX_BYTES_IN_MARKER = 65533\\n        MAX_DATA_BYTES_IN_MARKER = MAX_BYTES_IN_MARKER - ICC_OVERHEAD_LEN\\n        markers = []\\n        while icc_profile:\\n            markers.append(icc_profile[:MAX_DATA_BYTES_IN_MARKER])\\n            icc_profile = icc_profile[MAX_DATA_BYTES_IN_MARKER:]\\n        i = 1\\n        for marker in markers:\\n            size = struct.pack(\\"\\u003eH\\", 2 + ICC_OVERHEAD_LEN + len(marker))\\n            extra += (b\\"\\\\xFF\\\\xE2\\" + size + b\\"ICC_PROFILE\\\\0\\" + o8(i) +\\n                      o8(len(markers)) + marker)\\n            i += 1\\n\\n    # \\"progressive\\" is the official name, but older documentation\\n    # says \\"progression\\"\\n    # FIXME: issue a warning if the wrong form is used (post-1.1.7)\\n    progressive = info.get(\\"progressive\\", False) or\\\\\\n                  info.get(\\"progression\\", False)\\n\\n    optimize = info.get(\\"optimize\\", False)\\n\\n    # get keyword arguments\\n    im.encoderconfig = (\\n        quality,\\n        progressive,\\n        info.get(\\"smooth\\", 0),\\n        optimize,\\n        info.get(\\"streamtype\\", 0),\\n        dpi[0], dpi[1],\\n        subsampling,\\n        qtables,\\n        extra,\\n        info.get(\\"exif\\", b\\"\\")\\n        )\\n\\n    # if we optimize, libjpeg needs a buffer big enough to hold the whole image\\n    # in a shot. Guessing on the size, at im.size bytes. (raw pizel size is\\n    # channels*size, this is a value that\'s been used in a django patch.\\n    # https://github.com/matthewwithanm/django-imagekit/issues/50\\n    bufsize = 0\\n    if optimize or progressive:\\n        # CMYK can be bigger\\n        if im.mode == \'CMYK\':\\n            bufsize = 4 * im.size[0] * im.size[1]\\n        # keep sets quality to 0, but the actual value may be high.\\n        elif quality \\u003e= 95 or quality == 0:\\n            bufsize = 2 * im.size[0] * im.size[1]\\n        else:\\n            bufsize = im.size[0] * im.size[1]\\n\\n    # The exif info needs to be written as one block, + APP1, + one spare byte.\\n    # Ensure that our buffer is big enough\\n    bufsize = max(ImageFile.MAXBLOCK, bufsize, len(info.get(\\"exif\\", b\\"\\")) + 5)\\n\\n    ImageFile._save(im, fp, [(\\"jpeg\\", (0, 0)+im.size, 0, rawmode)], bufsize)\\n\\n\\ndef _save_cjpeg(im, fp, filename):\\n    # ALTERNATIVE: handle JPEGs via the IJG command line utilities.\\n    import os\\n    import subprocess\\n    tempfile = im._dump()\\n    subprocess.check_call([\\"cjpeg\\", \\"-outfile\\", filename, tempfile])\\n    try:\\n        os.unlink(tempfile)\\n    except OSError:\\n        pass\\n\\n\\n##\\n# Factory for making JPEG and MPO instances\\ndef jpeg_factory(fp=None, filename=None):\\n    im = JpegImageFile(fp, filename)\\n    try:\\n        mpheader = im._getmp()\\n        if mpheader[45057] \\u003e 1:\\n            # It\'s actually an MPO\\n            from .MpoImagePlugin import MpoImageFile\\n            im = MpoImageFile(fp, filename)\\n    except (TypeError, IndexError):\\n        # It is really a JPEG\\n        pass\\n    except SyntaxError:\\n        warnings.warn(\\"Image appears to be a malformed MPO file, it will be \\"\\n                      \\"interpreted as a base JPEG file\\")\\n    return im\\n\\n\\n# -------------------------------------------------------------------q-\\n# Registry stuff\\n\\nImage.register_open(JpegImageFile.format, jpeg_factory, _accept)\\nImage.register_save(JpegImageFile.format, _save)\\n\\nImage.register_extension(JpegImageFile.format, \\".jfif\\")\\nImage.register_extension(JpegImageFile.format, \\".jpe\\")\\nImage.register_extension(JpegImageFile.format, \\".jpg\\")\\nImage.register_extension(JpegImageFile.format, \\".jpeg\\")\\n\\nImage.register_mime(JpegImageFile.format, \\"image/jpeg\\")\\n"}\n'
line: b'{"repo_name":"Taketrung/betfair.py","ref":"refs/heads/master","path":"tests/fixtures.py","content":"# -*- coding: utf-8 -*-\\n\\nimport pytest\\n\\nimport os\\n\\nfrom betfair import betfair\\nfrom tests.utils import response_fixture_factory\\n\\n\\n@pytest.fixture\\ndef client():\\n    return betfair.Betfair(app_key=\'test\', cert_file=\'path/to/cert\')\\n\\n\\n@pytest.fixture\\ndef logged_in_client(client):\\n    client = betfair.Betfair(app_key=\'test\', cert_file=\'path/to/cert\')\\n    client.session_token = \'secret\'\\n    return client\\n\\nlogin_success = response_fixture_factory(\\n    os.path.join(betfair.IDENTITY_URLS[None], \'certlogin\'),\\n    {\\n        \'loginStatus\': \'SUCCESS\',\\n        \'sessionToken\': \'secret\',\\n    },\\n)\\n\\nlogin_failure = response_fixture_factory(\\n    os.path.join(betfair.IDENTITY_URLS[None], \'certlogin\'),\\n    {\'loginStatus\': \'INVALID_USERNAME_OR_PASSWORD\'},\\n)\\n\\nlogin_bad_code = response_fixture_factory(\\n    os.path.join(betfair.IDENTITY_URLS[None], \'certlogin\'),\\n    status=422,\\n)\\n\\nkeepalive_success = response_fixture_factory(\\n    os.path.join(betfair.IDENTITY_URLS[None], \'keepAlive\'),\\n    {\'status\': \'SUCCESS\'},\\n)\\n\\nkeepalive_failure = response_fixture_factory(\\n    os.path.join(betfair.IDENTITY_URLS[None], \'keepAlive\'),\\n    {\\n        \'status\': \'FAIL\',\\n        \'error\': \'NO_SESSION\',\\n    },\\n)\\n\\nlogout_success = response_fixture_factory(\\n    os.path.join(betfair.IDENTITY_URLS[None], \'logout\'),\\n    {\'status\': \'SUCCESS\'},\\n)\\n\\nlogout_failure = response_fixture_factory(\\n    os.path.join(betfair.IDENTITY_URLS[None], \'logout\'),\\n    {\\n        \'status\': \'FAIL\',\\n        \'error\': \'NO_SESSION\',\\n    },\\n)\\n\\nlogin_required_methods = [\\n    \'keep_alive\',\\n    \'logout\',\\n    \'list_event_types\',\\n    \'list_competitions\',\\n    \'list_time_ranges\',\\n    \'list_events\',\\n    \'list_market_types\',\\n    \'list_countries\',\\n    \'list_venues\',\\n    \'list_market_catalogue\',\\n    \'list_market_book\',\\n    \'list_market_profit_and_loss\',\\n    \'list_current_orders\',\\n    \'list_cleared_orders\',\\n    \'place_orders\',\\n    \'cancel_orders\',\\n    \'replace_orders\',\\n    \'update_orders\',\\n]\\n"}\n'
line: b'{"repo_name":"mjtamlyn/django","ref":"refs/heads/master","path":"tests/mail/tests.py","content":"import asyncore\\nimport base64\\nimport mimetypes\\nimport os\\nimport shutil\\nimport smtpd\\nimport socket\\nimport sys\\nimport tempfile\\nimport threading\\nfrom email import message_from_binary_file, message_from_bytes\\nfrom email.header import Header\\nfrom email.mime.text import MIMEText\\nfrom email.utils import parseaddr\\nfrom io import StringIO\\nfrom smtplib import SMTP, SMTPAuthenticationError, SMTPException\\nfrom ssl import SSLError\\n\\nfrom django.core import mail\\nfrom django.core.mail import (\\n    EmailMessage, EmailMultiAlternatives, mail_admins, mail_managers,\\n    send_mail, send_mass_mail,\\n)\\nfrom django.core.mail.backends import console, dummy, filebased, locmem, smtp\\nfrom django.core.mail.message import BadHeaderError, sanitize_address\\nfrom django.test import SimpleTestCase, override_settings\\nfrom django.test.utils import requires_tz_support\\nfrom django.utils.encoding import force_bytes, force_text\\nfrom django.utils.translation import gettext_lazy\\n\\n\\nclass HeadersCheckMixin:\\n\\n    def assertMessageHasHeaders(self, message, headers):\\n        \\"\\"\\"\\n        Asserts that the `message` has all `headers`.\\n\\n        message: can be an instance of an email.Message subclass or a string\\n                 with the contents of an email message.\\n        headers: should be a set of (header-name, header-value) tuples.\\n        \\"\\"\\"\\n        if isinstance(message, bytes):\\n            message = message_from_bytes(message)\\n        msg_headers = set(message.items())\\n        self.assertTrue(headers.issubset(msg_headers), msg=\'Message is missing \'\\n                        \'the following headers: %s\' % (headers - msg_headers),)\\n\\n\\nclass MailTests(HeadersCheckMixin, SimpleTestCase):\\n    \\"\\"\\"\\n    Non-backend specific tests.\\n    \\"\\"\\"\\n    def get_decoded_attachments(self, django_message):\\n        \\"\\"\\"\\n        Encode the specified django.core.mail.message.EmailMessage, then decode\\n        it using Python\'s email.parser module and, for each attachment of the\\n        message, return a list of tuples with (filename, content, mimetype).\\n        \\"\\"\\"\\n        msg_bytes = django_message.message().as_bytes()\\n        email_message = message_from_bytes(msg_bytes)\\n\\n        def iter_attachments():\\n            for i in email_message.walk():\\n                # Once support for Python\\u003c3.5 has been dropped, we can use\\n                # i.get_content_disposition() here instead.\\n                content_disposition = i.get(\'content-disposition\', \'\').split(\';\')[0].lower()\\n                if content_disposition == \'attachment\':\\n                    filename = i.get_filename()\\n                    content = i.get_payload(decode=True)\\n                    mimetype = i.get_content_type()\\n                    yield filename, content, mimetype\\n\\n        return list(iter_attachments())\\n\\n    def test_ascii(self):\\n        email = EmailMessage(\'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'])\\n        message = email.message()\\n        self.assertEqual(message[\'Subject\'], \'Subject\')\\n        self.assertEqual(message.get_payload(), \'Content\')\\n        self.assertEqual(message[\'From\'], \'from@example.com\')\\n        self.assertEqual(message[\'To\'], \'to@example.com\')\\n\\n    def test_multiple_recipients(self):\\n        email = EmailMessage(\'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\', \'other@example.com\'])\\n        message = email.message()\\n        self.assertEqual(message[\'Subject\'], \'Subject\')\\n        self.assertEqual(message.get_payload(), \'Content\')\\n        self.assertEqual(message[\'From\'], \'from@example.com\')\\n        self.assertEqual(message[\'To\'], \'to@example.com, other@example.com\')\\n\\n    def test_recipients_with_empty_strings(self):\\n        \\"\\"\\"\\n        Empty strings in various recipient arguments are always stripped\\n        off the final recipient list.\\n        \\"\\"\\"\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\', \'\'],\\n            cc=[\'cc@example.com\', \'\'],\\n            bcc=[\'\', \'bcc@example.com\'],\\n            reply_to=[\'\', None],\\n        )\\n        self.assertEqual(\\n            email.recipients(),\\n            [\'to@example.com\', \'cc@example.com\', \'bcc@example.com\']\\n        )\\n\\n    def test_cc(self):\\n        \\"\\"\\"Regression test for #7722\\"\\"\\"\\n        email = EmailMessage(\'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'], cc=[\'cc@example.com\'])\\n        message = email.message()\\n        self.assertEqual(message[\'Cc\'], \'cc@example.com\')\\n        self.assertEqual(email.recipients(), [\'to@example.com\', \'cc@example.com\'])\\n\\n        # Test multiple CC with multiple To\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\', \'other@example.com\'],\\n            cc=[\'cc@example.com\', \'cc.other@example.com\']\\n        )\\n        message = email.message()\\n        self.assertEqual(message[\'Cc\'], \'cc@example.com, cc.other@example.com\')\\n        self.assertEqual(\\n            email.recipients(),\\n            [\'to@example.com\', \'other@example.com\', \'cc@example.com\', \'cc.other@example.com\']\\n        )\\n\\n        # Testing with Bcc\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\', \'other@example.com\'],\\n            cc=[\'cc@example.com\', \'cc.other@example.com\'], bcc=[\'bcc@example.com\']\\n        )\\n        message = email.message()\\n        self.assertEqual(message[\'Cc\'], \'cc@example.com, cc.other@example.com\')\\n        self.assertEqual(\\n            email.recipients(),\\n            [\'to@example.com\', \'other@example.com\', \'cc@example.com\', \'cc.other@example.com\', \'bcc@example.com\']\\n        )\\n\\n    def test_reply_to(self):\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'],\\n            reply_to=[\'reply_to@example.com\'],\\n        )\\n        message = email.message()\\n        self.assertEqual(message[\'Reply-To\'], \'reply_to@example.com\')\\n\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'],\\n            reply_to=[\'reply_to1@example.com\', \'reply_to2@example.com\']\\n        )\\n        message = email.message()\\n        self.assertEqual(message[\'Reply-To\'], \'reply_to1@example.com, reply_to2@example.com\')\\n\\n    def test_recipients_as_tuple(self):\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'from@example.com\', (\'to@example.com\', \'other@example.com\'),\\n            cc=(\'cc@example.com\', \'cc.other@example.com\'), bcc=(\'bcc@example.com\',)\\n        )\\n        message = email.message()\\n        self.assertEqual(message[\'Cc\'], \'cc@example.com, cc.other@example.com\')\\n        self.assertEqual(\\n            email.recipients(),\\n            [\'to@example.com\', \'other@example.com\', \'cc@example.com\', \'cc.other@example.com\', \'bcc@example.com\']\\n        )\\n\\n    def test_recipients_as_string(self):\\n        with self.assertRaisesMessage(TypeError, \'\\"to\\" argument must be a list or tuple\'):\\n            EmailMessage(to=\'foo@example.com\')\\n        with self.assertRaisesMessage(TypeError, \'\\"cc\\" argument must be a list or tuple\'):\\n            EmailMessage(cc=\'foo@example.com\')\\n        with self.assertRaisesMessage(TypeError, \'\\"bcc\\" argument must be a list or tuple\'):\\n            EmailMessage(bcc=\'foo@example.com\')\\n        with self.assertRaisesMessage(TypeError, \'\\"reply_to\\" argument must be a list or tuple\'):\\n            EmailMessage(reply_to=\'reply_to@example.com\')\\n\\n    def test_header_injection(self):\\n        email = EmailMessage(\'Subject\\\\nInjection Test\', \'Content\', \'from@example.com\', [\'to@example.com\'])\\n        with self.assertRaises(BadHeaderError):\\n            email.message()\\n        email = EmailMessage(\\n            gettext_lazy(\'Subject\\\\nInjection Test\'), \'Content\', \'from@example.com\', [\'to@example.com\']\\n        )\\n        with self.assertRaises(BadHeaderError):\\n            email.message()\\n\\n    def test_space_continuation(self):\\n        \\"\\"\\"\\n        Test for space continuation character in long (ASCII) subject headers (#7747)\\n        \\"\\"\\"\\n        email = EmailMessage(\\n            \'Long subject lines that get wrapped should contain a space \'\\n            \'continuation character to get expected behavior in Outlook and Thunderbird\',\\n            \'Content\', \'from@example.com\', [\'to@example.com\']\\n        )\\n        message = email.message()\\n        self.assertEqual(\\n            message[\'Subject\'].encode(),\\n            b\'Long subject lines that get wrapped should contain a space continuation\\\\n\'\\n            b\' character to get expected behavior in Outlook and Thunderbird\'\\n        )\\n\\n    def test_message_header_overrides(self):\\n        \\"\\"\\"\\n        Specifying dates or message-ids in the extra headers overrides the\\n        default values (#9233)\\n        \\"\\"\\"\\n        headers = {\\"date\\": \\"Fri, 09 Nov 2001 01:08:47 -0000\\", \\"Message-ID\\": \\"foo\\"}\\n        email = EmailMessage(\'subject\', \'content\', \'from@example.com\', [\'to@example.com\'], headers=headers)\\n\\n        self.assertMessageHasHeaders(email.message(), {\\n            (\'Content-Transfer-Encoding\', \'7bit\'),\\n            (\'Content-Type\', \'text/plain; charset=\\"utf-8\\"\'),\\n            (\'From\', \'from@example.com\'),\\n            (\'MIME-Version\', \'1.0\'),\\n            (\'Message-ID\', \'foo\'),\\n            (\'Subject\', \'subject\'),\\n            (\'To\', \'to@example.com\'),\\n            (\'date\', \'Fri, 09 Nov 2001 01:08:47 -0000\'),\\n        })\\n\\n    def test_from_header(self):\\n        \\"\\"\\"\\n        Make sure we can manually set the From header (#9214)\\n        \\"\\"\\"\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        message = email.message()\\n        self.assertEqual(message[\'From\'], \'from@example.com\')\\n\\n    def test_to_header(self):\\n        \\"\\"\\"\\n        Make sure we can manually set the To header (#17444)\\n        \\"\\"\\"\\n        email = EmailMessage(\'Subject\', \'Content\', \'bounce@example.com\',\\n                             [\'list-subscriber@example.com\', \'list-subscriber2@example.com\'],\\n                             headers={\'To\': \'mailing-list@example.com\'})\\n        message = email.message()\\n        self.assertEqual(message[\'To\'], \'mailing-list@example.com\')\\n        self.assertEqual(email.to, [\'list-subscriber@example.com\', \'list-subscriber2@example.com\'])\\n\\n        # If we don\'t set the To header manually, it should default to the `to` argument to the constructor\\n        email = EmailMessage(\'Subject\', \'Content\', \'bounce@example.com\',\\n                             [\'list-subscriber@example.com\', \'list-subscriber2@example.com\'])\\n        message = email.message()\\n        self.assertEqual(message[\'To\'], \'list-subscriber@example.com, list-subscriber2@example.com\')\\n        self.assertEqual(email.to, [\'list-subscriber@example.com\', \'list-subscriber2@example.com\'])\\n\\n    def test_reply_to_header(self):\\n        \\"\\"\\"\\n        Specifying \'Reply-To\' in headers should override reply_to.\\n        \\"\\"\\"\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'bounce@example.com\', [\'to@example.com\'],\\n            reply_to=[\'foo@example.com\'], headers={\'Reply-To\': \'override@example.com\'},\\n        )\\n        message = email.message()\\n        self.assertEqual(message[\'Reply-To\'], \'override@example.com\')\\n\\n    def test_multiple_message_call(self):\\n        \\"\\"\\"\\n        Regression for #13259 - Make sure that headers are not changed when\\n        calling EmailMessage.message()\\n        \\"\\"\\"\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        message = email.message()\\n        self.assertEqual(message[\'From\'], \'from@example.com\')\\n        message = email.message()\\n        self.assertEqual(message[\'From\'], \'from@example.com\')\\n\\n    def test_unicode_address_header(self):\\n        \\"\\"\\"\\n        Regression for #11144 - When a to/from/cc header contains unicode,\\n        make sure the email addresses are parsed correctly (especially with\\n        regards to commas)\\n        \\"\\"\\"\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'from@example.com\',\\n            [\'\\"Firstname S\xc3\xbcrname\\" \\u003cto@example.com\\u003e\', \'other@example.com\'],\\n        )\\n        self.assertEqual(\\n            email.message()[\'To\'],\\n            \'=?utf-8?q?Firstname_S=C3=BCrname?= \\u003cto@example.com\\u003e, other@example.com\'\\n        )\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'from@example.com\',\\n            [\'\\"S\xc3\xbcrname, Firstname\\" \\u003cto@example.com\\u003e\', \'other@example.com\'],\\n        )\\n        self.assertEqual(\\n            email.message()[\'To\'],\\n            \'=?utf-8?q?S=C3=BCrname=2C_Firstname?= \\u003cto@example.com\\u003e, other@example.com\'\\n        )\\n\\n    def test_unicode_headers(self):\\n        email = EmailMessage(\\"G\xc5\xbceg\xc5\xbc\xc3\xb3\xc5\x82ka\\", \\"Content\\", \\"from@example.com\\", [\\"to@example.com\\"],\\n                             headers={\\"Sender\\": \'\\"Firstname S\xc3\xbcrname\\" \\u003csender@example.com\\u003e\',\\n                                      \\"Comments\\": \'My S\xc3\xbcrname is non-ASCII\'})\\n        message = email.message()\\n        self.assertEqual(message[\'Subject\'], \'=?utf-8?b?R8W8ZWfFvMOzxYJrYQ==?=\')\\n        self.assertEqual(message[\'Sender\'], \'=?utf-8?q?Firstname_S=C3=BCrname?= \\u003csender@example.com\\u003e\')\\n        self.assertEqual(message[\'Comments\'], \'=?utf-8?q?My_S=C3=BCrname_is_non-ASCII?=\')\\n\\n    def test_safe_mime_multipart(self):\\n        \\"\\"\\"\\n        Make sure headers can be set with a different encoding than utf-8 in\\n        SafeMIMEMultipart as well\\n        \\"\\"\\"\\n        headers = {\\"Date\\": \\"Fri, 09 Nov 2001 01:08:47 -0000\\", \\"Message-ID\\": \\"foo\\"}\\n        from_email, to = \'from@example.com\', \'\\"S\xc3\xbcrname, Firstname\\" \\u003cto@example.com\\u003e\'\\n        text_content = \'This is an important message.\'\\n        html_content = \'\\u003cp\\u003eThis is an \\u003cstrong\\u003eimportant\\u003c/strong\\u003e message.\\u003c/p\\u003e\'\\n        msg = EmailMultiAlternatives(\'Message from Firstname S\xc3\xbcrname\', text_content, from_email, [to], headers=headers)\\n        msg.attach_alternative(html_content, \\"text/html\\")\\n        msg.encoding = \'iso-8859-1\'\\n        self.assertEqual(msg.message()[\'To\'], \'=?iso-8859-1?q?S=FCrname=2C_Firstname?= \\u003cto@example.com\\u003e\')\\n        self.assertEqual(msg.message()[\'Subject\'], \'=?iso-8859-1?q?Message_from_Firstname_S=FCrname?=\')\\n\\n    def test_encoding(self):\\n        \\"\\"\\"\\n        Regression for #12791 - Encode body correctly with other encodings\\n        than utf-8\\n        \\"\\"\\"\\n        email = EmailMessage(\'Subject\', \'Firstname S\xc3\xbcrname is a great guy.\', \'from@example.com\', [\'other@example.com\'])\\n        email.encoding = \'iso-8859-1\'\\n        message = email.message()\\n        self.assertMessageHasHeaders(message, {\\n            (\'MIME-Version\', \'1.0\'),\\n            (\'Content-Type\', \'text/plain; charset=\\"iso-8859-1\\"\'),\\n            (\'Content-Transfer-Encoding\', \'quoted-printable\'),\\n            (\'Subject\', \'Subject\'),\\n            (\'From\', \'from@example.com\'),\\n            (\'To\', \'other@example.com\')})\\n        self.assertEqual(message.get_payload(), \'Firstname S=FCrname is a great guy.\')\\n\\n        # Make sure MIME attachments also works correctly with other encodings than utf-8\\n        text_content = \'Firstname S\xc3\xbcrname is a great guy.\'\\n        html_content = \'\\u003cp\\u003eFirstname S\xc3\xbcrname is a \\u003cstrong\\u003egreat\\u003c/strong\\u003e guy.\\u003c/p\\u003e\'\\n        msg = EmailMultiAlternatives(\'Subject\', text_content, \'from@example.com\', [\'to@example.com\'])\\n        msg.encoding = \'iso-8859-1\'\\n        msg.attach_alternative(html_content, \\"text/html\\")\\n        payload0 = msg.message().get_payload(0)\\n        self.assertMessageHasHeaders(payload0, {\\n            (\'MIME-Version\', \'1.0\'),\\n            (\'Content-Type\', \'text/plain; charset=\\"iso-8859-1\\"\'),\\n            (\'Content-Transfer-Encoding\', \'quoted-printable\')})\\n        self.assertTrue(payload0.as_bytes().endswith(b\'\\\\n\\\\nFirstname S=FCrname is a great guy.\'))\\n        payload1 = msg.message().get_payload(1)\\n        self.assertMessageHasHeaders(payload1, {\\n            (\'MIME-Version\', \'1.0\'),\\n            (\'Content-Type\', \'text/html; charset=\\"iso-8859-1\\"\'),\\n            (\'Content-Transfer-Encoding\', \'quoted-printable\')})\\n        self.assertTrue(\\n            payload1.as_bytes().endswith(b\'\\\\n\\\\n\\u003cp\\u003eFirstname S=FCrname is a \\u003cstrong\\u003egreat\\u003c/strong\\u003e guy.\\u003c/p\\u003e\')\\n        )\\n\\n    def test_attachments(self):\\n        \\"\\"\\"Regression test for #9367\\"\\"\\"\\n        headers = {\\"Date\\": \\"Fri, 09 Nov 2001 01:08:47 -0000\\", \\"Message-ID\\": \\"foo\\"}\\n        subject, from_email, to = \'hello\', \'from@example.com\', \'to@example.com\'\\n        text_content = \'This is an important message.\'\\n        html_content = \'\\u003cp\\u003eThis is an \\u003cstrong\\u003eimportant\\u003c/strong\\u003e message.\\u003c/p\\u003e\'\\n        msg = EmailMultiAlternatives(subject, text_content, from_email, [to], headers=headers)\\n        msg.attach_alternative(html_content, \\"text/html\\")\\n        msg.attach(\\"an attachment.pdf\\", b\\"%PDF-1.4.%...\\", mimetype=\\"application/pdf\\")\\n        msg_bytes = msg.message().as_bytes()\\n        message = message_from_bytes(msg_bytes)\\n        self.assertTrue(message.is_multipart())\\n        self.assertEqual(message.get_content_type(), \'multipart/mixed\')\\n        self.assertEqual(message.get_default_type(), \'text/plain\')\\n        payload = message.get_payload()\\n        self.assertEqual(payload[0].get_content_type(), \'multipart/alternative\')\\n        self.assertEqual(payload[1].get_content_type(), \'application/pdf\')\\n\\n    def test_non_ascii_attachment_filename(self):\\n        \\"\\"\\"Regression test for #14964\\"\\"\\"\\n        headers = {\\"Date\\": \\"Fri, 09 Nov 2001 01:08:47 -0000\\", \\"Message-ID\\": \\"foo\\"}\\n        subject, from_email, to = \'hello\', \'from@example.com\', \'to@example.com\'\\n        content = \'This is the message.\'\\n        msg = EmailMessage(subject, content, from_email, [to], headers=headers)\\n        # Unicode in file name\\n        msg.attach(\\"une pi\xc3\xa8ce jointe.pdf\\", b\\"%PDF-1.4.%...\\", mimetype=\\"application/pdf\\")\\n        msg_bytes = msg.message().as_bytes()\\n        message = message_from_bytes(msg_bytes)\\n        payload = message.get_payload()\\n        self.assertEqual(payload[1].get_filename(), \'une pi\xc3\xa8ce jointe.pdf\')\\n\\n    def test_attach_file(self):\\n        \\"\\"\\"\\n        Test attaching a file against different mimetypes and make sure that\\n        a file will be attached and sent properly even if an invalid mimetype\\n        is specified.\\n        \\"\\"\\"\\n        files = (\\n            # filename, actual mimetype\\n            (\'file.txt\', \'text/plain\'),\\n            (\'file.png\', \'image/png\'),\\n            (\'file_txt\', None),\\n            (\'file_png\', None),\\n            (\'file_txt.png\', \'image/png\'),\\n            (\'file_png.txt\', \'text/plain\'),\\n            (\'file.eml\', \'message/rfc822\'),\\n        )\\n        test_mimetypes = [\'text/plain\', \'image/png\', None]\\n\\n        for basename, real_mimetype in files:\\n            for mimetype in test_mimetypes:\\n                email = EmailMessage(\'subject\', \'body\', \'from@example.com\', [\'to@example.com\'])\\n                self.assertEqual(mimetypes.guess_type(basename)[0], real_mimetype)\\n                self.assertEqual(email.attachments, [])\\n                file_path = os.path.join(os.path.dirname(__file__), \'attachments\', basename)\\n                email.attach_file(file_path, mimetype=mimetype)\\n                self.assertEqual(len(email.attachments), 1)\\n                self.assertIn(basename, email.attachments[0])\\n                msgs_sent_num = email.send()\\n                self.assertEqual(msgs_sent_num, 1)\\n\\n    def test_attach_text_as_bytes(self):\\n        msg = EmailMessage(\'subject\', \'body\', \'from@example.com\', [\'to@example.com\'])\\n        msg.attach(\'file.txt\', b\'file content\')\\n        sent_num = msg.send()\\n        self.assertEqual(sent_num, 1)\\n        filename, content, mimetype = self.get_decoded_attachments(msg)[0]\\n        self.assertEqual(filename, \'file.txt\')\\n        self.assertEqual(content, b\'file content\')\\n        self.assertEqual(mimetype, \'text/plain\')\\n\\n    def test_attach_utf8_text_as_bytes(self):\\n        \\"\\"\\"\\n        Non-ASCII characters encoded as valid UTF-8 are correctly transported\\n        and decoded.\\n        \\"\\"\\"\\n        msg = EmailMessage(\'subject\', \'body\', \'from@example.com\', [\'to@example.com\'])\\n        msg.attach(\'file.txt\', b\'\\\\xc3\\\\xa4\')  # UTF-8 encoded a umlaut.\\n        filename, content, mimetype = self.get_decoded_attachments(msg)[0]\\n        self.assertEqual(filename, \'file.txt\')\\n        self.assertEqual(content, b\'\\\\xc3\\\\xa4\')\\n        self.assertEqual(mimetype, \'text/plain\')\\n\\n    def test_attach_non_utf8_text_as_bytes(self):\\n        \\"\\"\\"\\n        Binary data that can\'t be decoded as UTF-8 overrides the MIME type\\n        instead of decoding the data.\\n        \\"\\"\\"\\n        msg = EmailMessage(\'subject\', \'body\', \'from@example.com\', [\'to@example.com\'])\\n        msg.attach(\'file.txt\', b\'\\\\xff\')  # Invalid UTF-8.\\n        filename, content, mimetype = self.get_decoded_attachments(msg)[0]\\n        self.assertEqual(filename, \'file.txt\')\\n        # Content should be passed through unmodified.\\n        self.assertEqual(content, b\'\\\\xff\')\\n        self.assertEqual(mimetype, \'application/octet-stream\')\\n\\n    def test_dummy_backend(self):\\n        \\"\\"\\"\\n        Make sure that dummy backends returns correct number of sent messages\\n        \\"\\"\\"\\n        connection = dummy.EmailBackend()\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        self.assertEqual(connection.send_messages([email, email, email]), 3)\\n\\n    def test_arbitrary_keyword(self):\\n        \\"\\"\\"\\n        Make sure that get_connection() accepts arbitrary keyword that might be\\n        used with custom backends.\\n        \\"\\"\\"\\n        c = mail.get_connection(fail_silently=True, foo=\'bar\')\\n        self.assertTrue(c.fail_silently)\\n\\n    def test_custom_backend(self):\\n        \\"\\"\\"Test custom backend defined in this suite.\\"\\"\\"\\n        conn = mail.get_connection(\'mail.custombackend.EmailBackend\')\\n        self.assertTrue(hasattr(conn, \'test_outbox\'))\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        conn.send_messages([email])\\n        self.assertEqual(len(conn.test_outbox), 1)\\n\\n    def test_backend_arg(self):\\n        \\"\\"\\"Test backend argument of mail.get_connection()\\"\\"\\"\\n        self.assertIsInstance(mail.get_connection(\'django.core.mail.backends.smtp.EmailBackend\'), smtp.EmailBackend)\\n        self.assertIsInstance(\\n            mail.get_connection(\'django.core.mail.backends.locmem.EmailBackend\'),\\n            locmem.EmailBackend\\n        )\\n        self.assertIsInstance(mail.get_connection(\'django.core.mail.backends.dummy.EmailBackend\'), dummy.EmailBackend)\\n        self.assertIsInstance(\\n            mail.get_connection(\'django.core.mail.backends.console.EmailBackend\'),\\n            console.EmailBackend\\n        )\\n        with tempfile.TemporaryDirectory() as tmp_dir:\\n            self.assertIsInstance(\\n                mail.get_connection(\'django.core.mail.backends.filebased.EmailBackend\', file_path=tmp_dir),\\n                filebased.EmailBackend\\n            )\\n        self.assertIsInstance(mail.get_connection(), locmem.EmailBackend)\\n\\n    @override_settings(\\n        EMAIL_BACKEND=\'django.core.mail.backends.locmem.EmailBackend\',\\n        ADMINS=[(\'nobody\', \'nobody@example.com\')],\\n        MANAGERS=[(\'nobody\', \'nobody@example.com\')])\\n    def test_connection_arg(self):\\n        \\"\\"\\"Test connection argument to send_mail(), et. al.\\"\\"\\"\\n        mail.outbox = []\\n\\n        # Send using non-default connection\\n        connection = mail.get_connection(\'mail.custombackend.EmailBackend\')\\n        send_mail(\'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'], connection=connection)\\n        self.assertEqual(mail.outbox, [])\\n        self.assertEqual(len(connection.test_outbox), 1)\\n        self.assertEqual(connection.test_outbox[0].subject, \'Subject\')\\n\\n        connection = mail.get_connection(\'mail.custombackend.EmailBackend\')\\n        send_mass_mail([\\n            (\'Subject1\', \'Content1\', \'from1@example.com\', [\'to1@example.com\']),\\n            (\'Subject2\', \'Content2\', \'from2@example.com\', [\'to2@example.com\']),\\n        ], connection=connection)\\n        self.assertEqual(mail.outbox, [])\\n        self.assertEqual(len(connection.test_outbox), 2)\\n        self.assertEqual(connection.test_outbox[0].subject, \'Subject1\')\\n        self.assertEqual(connection.test_outbox[1].subject, \'Subject2\')\\n\\n        connection = mail.get_connection(\'mail.custombackend.EmailBackend\')\\n        mail_admins(\'Admin message\', \'Content\', connection=connection)\\n        self.assertEqual(mail.outbox, [])\\n        self.assertEqual(len(connection.test_outbox), 1)\\n        self.assertEqual(connection.test_outbox[0].subject, \'[Django] Admin message\')\\n\\n        connection = mail.get_connection(\'mail.custombackend.EmailBackend\')\\n        mail_managers(\'Manager message\', \'Content\', connection=connection)\\n        self.assertEqual(mail.outbox, [])\\n        self.assertEqual(len(connection.test_outbox), 1)\\n        self.assertEqual(connection.test_outbox[0].subject, \'[Django] Manager message\')\\n\\n    def test_dont_mangle_from_in_body(self):\\n        # Regression for #13433 - Make sure that EmailMessage doesn\'t mangle\\n        # \'From \' in message body.\\n        email = EmailMessage(\\n            \'Subject\', \'From the future\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        self.assertNotIn(b\'\\u003eFrom the future\', email.message().as_bytes())\\n\\n    def test_dont_base64_encode(self):\\n        # Ticket #3472\\n        # Shouldn\'t use Base64 encoding at all\\n        msg = EmailMessage(\\n            \'Subject\', \'UTF-8 encoded body\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        self.assertIn(b\'Content-Transfer-Encoding: 7bit\', msg.message().as_bytes())\\n\\n        # Ticket #11212\\n        # Shouldn\'t use quoted printable, should detect it can represent content with 7 bit data\\n        msg = EmailMessage(\\n            \'Subject\', \'Body with only ASCII characters.\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        s = msg.message().as_bytes()\\n        self.assertIn(b\'Content-Transfer-Encoding: 7bit\', s)\\n\\n        # Shouldn\'t use quoted printable, should detect it can represent content with 8 bit data\\n        msg = EmailMessage(\\n            \'Subject\', \'Body with latin characters: \xc3\xa0\xc3\xa1\xc3\xa4.\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        s = msg.message().as_bytes()\\n        self.assertIn(b\'Content-Transfer-Encoding: 8bit\', s)\\n        s = msg.message().as_string()\\n        self.assertIn(\'Content-Transfer-Encoding: 8bit\', s)\\n\\n        msg = EmailMessage(\\n            \'Subject\', \'Body with non latin characters: \xd0\x90 \xd0\x91 \xd0\x92 \xd0\x93 \xd0\x94 \xd0\x95 \xd0\x96 \xd0\x85 \xd0\x97 \xd0\x98 \xd0\x86 \xd0\x9a \xd0\x9b \xd0\x9c \xd0\x9d \xd0\x9e \xd0\x9f.\', \'bounce@example.com\',\\n            [\'to@example.com\'], headers={\'From\': \'from@example.com\'},\\n        )\\n        s = msg.message().as_bytes()\\n        self.assertIn(b\'Content-Transfer-Encoding: 8bit\', s)\\n        s = msg.message().as_string()\\n        self.assertIn(\'Content-Transfer-Encoding: 8bit\', s)\\n\\n    def test_dont_base64_encode_message_rfc822(self):\\n        # Ticket #18967\\n        # Shouldn\'t use base64 encoding for a child EmailMessage attachment.\\n        # Create a child message first\\n        child_msg = EmailMessage(\\n            \'Child Subject\', \'Some body of child message\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        child_s = child_msg.message().as_string()\\n\\n        # Now create a parent\\n        parent_msg = EmailMessage(\\n            \'Parent Subject\', \'Some parent body\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n\\n        # Attach to parent as a string\\n        parent_msg.attach(content=child_s, mimetype=\'message/rfc822\')\\n        parent_s = parent_msg.message().as_string()\\n\\n        # The child message header is not base64 encoded\\n        self.assertIn(\'Child Subject\', parent_s)\\n\\n        # Feature test: try attaching email.Message object directly to the mail.\\n        parent_msg = EmailMessage(\\n            \'Parent Subject\', \'Some parent body\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        parent_msg.attach(content=child_msg.message(), mimetype=\'message/rfc822\')\\n        parent_s = parent_msg.message().as_string()\\n\\n        # The child message header is not base64 encoded\\n        self.assertIn(\'Child Subject\', parent_s)\\n\\n        # Feature test: try attaching Django\'s EmailMessage object directly to the mail.\\n        parent_msg = EmailMessage(\\n            \'Parent Subject\', \'Some parent body\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        parent_msg.attach(content=child_msg, mimetype=\'message/rfc822\')\\n        parent_s = parent_msg.message().as_string()\\n\\n        # The child message header is not base64 encoded\\n        self.assertIn(\'Child Subject\', parent_s)\\n\\n    def test_sanitize_address(self):\\n        \\"\\"\\"\\n        Email addresses are properly sanitized.\\n        \\"\\"\\"\\n        # Simple ASCII address - string form\\n        self.assertEqual(sanitize_address(\'to@example.com\', \'ascii\'), \'to@example.com\')\\n        self.assertEqual(sanitize_address(\'to@example.com\', \'utf-8\'), \'to@example.com\')\\n\\n        # Simple ASCII address - tuple form\\n        self.assertEqual(\\n            sanitize_address((\'A name\', \'to@example.com\'), \'ascii\'),\\n            \'A name \\u003cto@example.com\\u003e\'\\n        )\\n        self.assertEqual(\\n            sanitize_address((\'A name\', \'to@example.com\'), \'utf-8\'),\\n            \'=?utf-8?q?A_name?= \\u003cto@example.com\\u003e\'\\n        )\\n\\n        # Unicode characters are are supported in RFC-6532.\\n        self.assertEqual(\\n            sanitize_address(\'t\xc3\xb3@example.com\', \'utf-8\'),\\n            \'=?utf-8?b?dMOz?=@example.com\'\\n        )\\n        self.assertEqual(\\n            sanitize_address((\'T\xc3\xb3 Example\', \'t\xc3\xb3@example.com\'), \'utf-8\'),\\n            \'=?utf-8?q?T=C3=B3_Example?= \\u003c=?utf-8?b?dMOz?=@example.com\\u003e\'\\n        )\\n\\n\\n@requires_tz_support\\nclass MailTimeZoneTests(SimpleTestCase):\\n\\n    @override_settings(EMAIL_USE_LOCALTIME=False, USE_TZ=True, TIME_ZONE=\'Africa/Algiers\')\\n    def test_date_header_utc(self):\\n        \\"\\"\\"\\n        EMAIL_USE_LOCALTIME=False creates a datetime in UTC.\\n        \\"\\"\\"\\n        email = EmailMessage(\'Subject\', \'Body\', \'bounce@example.com\', [\'to@example.com\'])\\n        self.assertTrue(email.message()[\'Date\'].endswith(\'-0000\'))\\n\\n    @override_settings(EMAIL_USE_LOCALTIME=True, USE_TZ=True, TIME_ZONE=\'Africa/Algiers\')\\n    def test_date_header_localtime(self):\\n        \\"\\"\\"\\n        EMAIL_USE_LOCALTIME=True creates a datetime in the local time zone.\\n        \\"\\"\\"\\n        email = EmailMessage(\'Subject\', \'Body\', \'bounce@example.com\', [\'to@example.com\'])\\n        self.assertTrue(email.message()[\'Date\'].endswith(\'+0100\'))  # Africa/Algiers is UTC+1\\n\\n\\nclass PythonGlobalState(SimpleTestCase):\\n    \\"\\"\\"\\n    Tests for #12422 -- Django smarts (#2472/#11212) with charset of utf-8 text\\n    parts shouldn\'t pollute global email Python package charset registry when\\n    django.mail.message is imported.\\n    \\"\\"\\"\\n\\n    def test_utf8(self):\\n        txt = MIMEText(\'UTF-8 encoded body\', \'plain\', \'utf-8\')\\n        self.assertIn(\'Content-Transfer-Encoding: base64\', txt.as_string())\\n\\n    def test_7bit(self):\\n        txt = MIMEText(\'Body with only ASCII characters.\', \'plain\', \'utf-8\')\\n        self.assertIn(\'Content-Transfer-Encoding: base64\', txt.as_string())\\n\\n    def test_8bit_latin(self):\\n        txt = MIMEText(\'Body with latin characters: \xc3\xa0\xc3\xa1\xc3\xa4.\', \'plain\', \'utf-8\')\\n        self.assertIn(\'Content-Transfer-Encoding: base64\', txt.as_string())\\n\\n    def test_8bit_non_latin(self):\\n        txt = MIMEText(\'Body with non latin characters: \xd0\x90 \xd0\x91 \xd0\x92 \xd0\x93 \xd0\x94 \xd0\x95 \xd0\x96 \xd0\x85 \xd0\x97 \xd0\x98 \xd0\x86 \xd0\x9a \xd0\x9b \xd0\x9c \xd0\x9d \xd0\x9e \xd0\x9f.\', \'plain\', \'utf-8\')\\n        self.assertIn(\'Content-Transfer-Encoding: base64\', txt.as_string())\\n\\n\\nclass BaseEmailBackendTests(HeadersCheckMixin):\\n    email_backend = None\\n\\n    def setUp(self):\\n        self.settings_override = override_settings(EMAIL_BACKEND=self.email_backend)\\n        self.settings_override.enable()\\n\\n    def tearDown(self):\\n        self.settings_override.disable()\\n\\n    def assertStartsWith(self, first, second):\\n        if not first.startswith(second):\\n            self.longMessage = True\\n            self.assertEqual(first[:len(second)], second, \\"First string doesn\'t start with the second.\\")\\n\\n    def get_mailbox_content(self):\\n        raise NotImplementedError(\'subclasses of BaseEmailBackendTests must provide a get_mailbox_content() method\')\\n\\n    def flush_mailbox(self):\\n        raise NotImplementedError(\'subclasses of BaseEmailBackendTests may require a flush_mailbox() method\')\\n\\n    def get_the_message(self):\\n        mailbox = self.get_mailbox_content()\\n        self.assertEqual(\\n            len(mailbox), 1,\\n            \\"Expected exactly one message, got %d.\\\\n%r\\" % (len(mailbox), [m.as_string() for m in mailbox])\\n        )\\n        return mailbox[0]\\n\\n    def test_send(self):\\n        email = EmailMessage(\'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'])\\n        num_sent = mail.get_connection().send_messages([email])\\n        self.assertEqual(num_sent, 1)\\n        message = self.get_the_message()\\n        self.assertEqual(message[\\"subject\\"], \\"Subject\\")\\n        self.assertEqual(message.get_payload(), \\"Content\\")\\n        self.assertEqual(message[\\"from\\"], \\"from@example.com\\")\\n        self.assertEqual(message.get_all(\\"to\\"), [\\"to@example.com\\"])\\n\\n    def test_send_unicode(self):\\n        email = EmailMessage(\'Ch\xc3\xa8re maman\', \'Je t\\\\\'aime tr\xc3\xa8s fort\', \'from@example.com\', [\'to@example.com\'])\\n        num_sent = mail.get_connection().send_messages([email])\\n        self.assertEqual(num_sent, 1)\\n        message = self.get_the_message()\\n        self.assertEqual(message[\\"subject\\"], \'=?utf-8?q?Ch=C3=A8re_maman?=\')\\n        self.assertEqual(force_text(message.get_payload(decode=True)), \'Je t\\\\\'aime tr\xc3\xa8s fort\')\\n\\n    def test_send_long_lines(self):\\n        \\"\\"\\"\\n        Email line length is limited to 998 chars by the RFC:\\n        https://tools.ietf.org/html/rfc5322#section-2.1.1\\n        Message body containing longer lines are converted to Quoted-Printable\\n        to avoid having to insert newlines, which could be hairy to do properly.\\n        \\"\\"\\"\\n        # Unencoded body length is \\u003c 998 (840) but \\u003e 998 when utf-8 encoded.\\n        email = EmailMessage(\'Subject\', \'\xd0\x92 \xd1\x8e\xd0\xb6\xd0\xbd\xd1\x8b\xd1\x85 \xd0\xbc\xd0\xbe\xd1\x80\xd1\x8f\xd1\x85 \' * 60, \'from@example.com\', [\'to@example.com\'])\\n        email.send()\\n        message = self.get_the_message()\\n        self.assertMessageHasHeaders(message, {\\n            (\'MIME-Version\', \'1.0\'),\\n            (\'Content-Type\', \'text/plain; charset=\\"utf-8\\"\'),\\n            (\'Content-Transfer-Encoding\', \'quoted-printable\'),\\n        })\\n\\n    def test_send_many(self):\\n        email1 = EmailMessage(\'Subject\', \'Content1\', \'from@example.com\', [\'to@example.com\'])\\n        email2 = EmailMessage(\'Subject\', \'Content2\', \'from@example.com\', [\'to@example.com\'])\\n        # send_messages() may take a list or a generator.\\n        emails_lists = ([email1, email2], (email for email in [email1, email2]))\\n        for emails_list in emails_lists:\\n            num_sent = mail.get_connection().send_messages(emails_list)\\n            self.assertEqual(num_sent, 2)\\n            messages = self.get_mailbox_content()\\n            self.assertEqual(len(messages), 2)\\n            self.assertEqual(messages[0].get_payload(), \'Content1\')\\n            self.assertEqual(messages[1].get_payload(), \'Content2\')\\n            self.flush_mailbox()\\n\\n    def test_send_verbose_name(self):\\n        email = EmailMessage(\\"Subject\\", \\"Content\\", \'\\"Firstname S\xc3\xbcrname\\" \\u003cfrom@example.com\\u003e\',\\n                             [\\"to@example.com\\"])\\n        email.send()\\n        message = self.get_the_message()\\n        self.assertEqual(message[\\"subject\\"], \\"Subject\\")\\n        self.assertEqual(message.get_payload(), \\"Content\\")\\n        self.assertEqual(message[\\"from\\"], \\"=?utf-8?q?Firstname_S=C3=BCrname?= \\u003cfrom@example.com\\u003e\\")\\n\\n    def test_plaintext_send_mail(self):\\n        \\"\\"\\"\\n        Test send_mail without the html_message\\n        regression test for adding html_message parameter to send_mail()\\n        \\"\\"\\"\\n        send_mail(\'Subject\', \'Content\', \'sender@example.com\', [\'nobody@example.com\'])\\n        message = self.get_the_message()\\n\\n        self.assertEqual(message.get(\'subject\'), \'Subject\')\\n        self.assertEqual(message.get_all(\'to\'), [\'nobody@example.com\'])\\n        self.assertFalse(message.is_multipart())\\n        self.assertEqual(message.get_payload(), \'Content\')\\n        self.assertEqual(message.get_content_type(), \'text/plain\')\\n\\n    def test_html_send_mail(self):\\n        \\"\\"\\"Test html_message argument to send_mail\\"\\"\\"\\n        send_mail(\'Subject\', \'Content\', \'sender@example.com\', [\'nobody@example.com\'], html_message=\'HTML Content\')\\n        message = self.get_the_message()\\n\\n        self.assertEqual(message.get(\'subject\'), \'Subject\')\\n        self.assertEqual(message.get_all(\'to\'), [\'nobody@example.com\'])\\n        self.assertTrue(message.is_multipart())\\n        self.assertEqual(len(message.get_payload()), 2)\\n        self.assertEqual(message.get_payload(0).get_payload(), \'Content\')\\n        self.assertEqual(message.get_payload(0).get_content_type(), \'text/plain\')\\n        self.assertEqual(message.get_payload(1).get_payload(), \'HTML Content\')\\n        self.assertEqual(message.get_payload(1).get_content_type(), \'text/html\')\\n\\n    @override_settings(MANAGERS=[(\'nobody\', \'nobody@example.com\')])\\n    def test_html_mail_managers(self):\\n        \\"\\"\\"Test html_message argument to mail_managers\\"\\"\\"\\n        mail_managers(\'Subject\', \'Content\', html_message=\'HTML Content\')\\n        message = self.get_the_message()\\n\\n        self.assertEqual(message.get(\'subject\'), \'[Django] Subject\')\\n        self.assertEqual(message.get_all(\'to\'), [\'nobody@example.com\'])\\n        self.assertTrue(message.is_multipart())\\n        self.assertEqual(len(message.get_payload()), 2)\\n        self.assertEqual(message.get_payload(0).get_payload(), \'Content\')\\n        self.assertEqual(message.get_payload(0).get_content_type(), \'text/plain\')\\n        self.assertEqual(message.get_payload(1).get_payload(), \'HTML Content\')\\n        self.assertEqual(message.get_payload(1).get_content_type(), \'text/html\')\\n\\n    @override_settings(ADMINS=[(\'nobody\', \'nobody@example.com\')])\\n    def test_html_mail_admins(self):\\n        \\"\\"\\"Test html_message argument to mail_admins \\"\\"\\"\\n        mail_admins(\'Subject\', \'Content\', html_message=\'HTML Content\')\\n        message = self.get_the_message()\\n\\n        self.assertEqual(message.get(\'subject\'), \'[Django] Subject\')\\n        self.assertEqual(message.get_all(\'to\'), [\'nobody@example.com\'])\\n        self.assertTrue(message.is_multipart())\\n        self.assertEqual(len(message.get_payload()), 2)\\n        self.assertEqual(message.get_payload(0).get_payload(), \'Content\')\\n        self.assertEqual(message.get_payload(0).get_content_type(), \'text/plain\')\\n        self.assertEqual(message.get_payload(1).get_payload(), \'HTML Content\')\\n        self.assertEqual(message.get_payload(1).get_content_type(), \'text/html\')\\n\\n    @override_settings(\\n        ADMINS=[(\'nobody\', \'nobody+admin@example.com\')],\\n        MANAGERS=[(\'nobody\', \'nobody+manager@example.com\')])\\n    def test_manager_and_admin_mail_prefix(self):\\n        \\"\\"\\"\\n        String prefix + lazy translated subject = bad output\\n        Regression for #13494\\n        \\"\\"\\"\\n        mail_managers(gettext_lazy(\'Subject\'), \'Content\')\\n        message = self.get_the_message()\\n        self.assertEqual(message.get(\'subject\'), \'[Django] Subject\')\\n\\n        self.flush_mailbox()\\n        mail_admins(gettext_lazy(\'Subject\'), \'Content\')\\n        message = self.get_the_message()\\n        self.assertEqual(message.get(\'subject\'), \'[Django] Subject\')\\n\\n    @override_settings(ADMINS=[], MANAGERS=[])\\n    def test_empty_admins(self):\\n        \\"\\"\\"\\n        mail_admins/mail_managers doesn\'t connect to the mail server\\n        if there are no recipients (#9383)\\n        \\"\\"\\"\\n        mail_admins(\'hi\', \'there\')\\n        self.assertEqual(self.get_mailbox_content(), [])\\n        mail_managers(\'hi\', \'there\')\\n        self.assertEqual(self.get_mailbox_content(), [])\\n\\n    def test_message_cc_header(self):\\n        \\"\\"\\"\\n        Regression test for #7722\\n        \\"\\"\\"\\n        email = EmailMessage(\'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'], cc=[\'cc@example.com\'])\\n        mail.get_connection().send_messages([email])\\n        message = self.get_the_message()\\n        self.assertMessageHasHeaders(message, {\\n            (\'MIME-Version\', \'1.0\'),\\n            (\'Content-Type\', \'text/plain; charset=\\"utf-8\\"\'),\\n            (\'Content-Transfer-Encoding\', \'7bit\'),\\n            (\'Subject\', \'Subject\'),\\n            (\'From\', \'from@example.com\'),\\n            (\'To\', \'to@example.com\'),\\n            (\'Cc\', \'cc@example.com\')})\\n        self.assertIn(\'\\\\nDate: \', message.as_string())\\n\\n    def test_idn_send(self):\\n        \\"\\"\\"\\n        Regression test for #14301\\n        \\"\\"\\"\\n        self.assertTrue(send_mail(\'Subject\', \'Content\', \'from@\xc3\xb6\xc3\xa4\xc3\xbc.com\', [\'to@\xc3\xb6\xc3\xa4\xc3\xbc.com\']))\\n        message = self.get_the_message()\\n        self.assertEqual(message.get(\'subject\'), \'Subject\')\\n        self.assertEqual(message.get(\'from\'), \'from@xn--4ca9at.com\')\\n        self.assertEqual(message.get(\'to\'), \'to@xn--4ca9at.com\')\\n\\n        self.flush_mailbox()\\n        m = EmailMessage(\'Subject\', \'Content\', \'from@\xc3\xb6\xc3\xa4\xc3\xbc.com\', [\'to@\xc3\xb6\xc3\xa4\xc3\xbc.com\'], cc=[\'cc@\xc3\xb6\xc3\xa4\xc3\xbc.com\'])\\n        m.send()\\n        message = self.get_the_message()\\n        self.assertEqual(message.get(\'subject\'), \'Subject\')\\n        self.assertEqual(message.get(\'from\'), \'from@xn--4ca9at.com\')\\n        self.assertEqual(message.get(\'to\'), \'to@xn--4ca9at.com\')\\n        self.assertEqual(message.get(\'cc\'), \'cc@xn--4ca9at.com\')\\n\\n    def test_recipient_without_domain(self):\\n        \\"\\"\\"\\n        Regression test for #15042\\n        \\"\\"\\"\\n        self.assertTrue(send_mail(\\"Subject\\", \\"Content\\", \\"tester\\", [\\"django\\"]))\\n        message = self.get_the_message()\\n        self.assertEqual(message.get(\'subject\'), \'Subject\')\\n        self.assertEqual(message.get(\'from\'), \\"tester\\")\\n        self.assertEqual(message.get(\'to\'), \\"django\\")\\n\\n    def test_lazy_addresses(self):\\n        \\"\\"\\"\\n        Email sending should support lazy email addresses (#24416).\\n        \\"\\"\\"\\n        _ = gettext_lazy\\n        self.assertTrue(send_mail(\'Subject\', \'Content\', _(\'tester\'), [_(\'django\')]))\\n        message = self.get_the_message()\\n        self.assertEqual(message.get(\'from\'), \'tester\')\\n        self.assertEqual(message.get(\'to\'), \'django\')\\n\\n        self.flush_mailbox()\\n        m = EmailMessage(\\n            \'Subject\', \'Content\', _(\'tester\'), [_(\'to1\'), _(\'to2\')],\\n            cc=[_(\'cc1\'), _(\'cc2\')],\\n            bcc=[_(\'bcc\')],\\n            reply_to=[_(\'reply\')],\\n        )\\n        self.assertEqual(m.recipients(), [\'to1\', \'to2\', \'cc1\', \'cc2\', \'bcc\'])\\n        m.send()\\n        message = self.get_the_message()\\n        self.assertEqual(message.get(\'from\'), \'tester\')\\n        self.assertEqual(message.get(\'to\'), \'to1, to2\')\\n        self.assertEqual(message.get(\'cc\'), \'cc1, cc2\')\\n        self.assertEqual(message.get(\'Reply-To\'), \'reply\')\\n\\n    def test_close_connection(self):\\n        \\"\\"\\"\\n        Connection can be closed (even when not explicitly opened)\\n        \\"\\"\\"\\n        conn = mail.get_connection(username=\'\', password=\'\')\\n        conn.close()\\n\\n    def test_use_as_contextmanager(self):\\n        \\"\\"\\"\\n        The connection can be used as a contextmanager.\\n        \\"\\"\\"\\n        opened = [False]\\n        closed = [False]\\n        conn = mail.get_connection(username=\'\', password=\'\')\\n\\n        def open():\\n            opened[0] = True\\n        conn.open = open\\n\\n        def close():\\n            closed[0] = True\\n        conn.close = close\\n        with conn as same_conn:\\n            self.assertTrue(opened[0])\\n            self.assertIs(same_conn, conn)\\n            self.assertFalse(closed[0])\\n        self.assertTrue(closed[0])\\n\\n\\nclass LocmemBackendTests(BaseEmailBackendTests, SimpleTestCase):\\n    email_backend = \'django.core.mail.backends.locmem.EmailBackend\'\\n\\n    def get_mailbox_content(self):\\n        return [m.message() for m in mail.outbox]\\n\\n    def flush_mailbox(self):\\n        mail.outbox = []\\n\\n    def tearDown(self):\\n        super().tearDown()\\n        mail.outbox = []\\n\\n    def test_locmem_shared_messages(self):\\n        \\"\\"\\"\\n        Make sure that the locmen backend populates the outbox.\\n        \\"\\"\\"\\n        connection = locmem.EmailBackend()\\n        connection2 = locmem.EmailBackend()\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        connection.send_messages([email])\\n        connection2.send_messages([email])\\n        self.assertEqual(len(mail.outbox), 2)\\n\\n    def test_validate_multiline_headers(self):\\n        # Ticket #18861 - Validate emails when using the locmem backend\\n        with self.assertRaises(BadHeaderError):\\n            send_mail(\'Subject\\\\nMultiline\', \'Content\', \'from@example.com\', [\'to@example.com\'])\\n\\n\\nclass FileBackendTests(BaseEmailBackendTests, SimpleTestCase):\\n    email_backend = \'django.core.mail.backends.filebased.EmailBackend\'\\n\\n    def setUp(self):\\n        super().setUp()\\n        self.tmp_dir = tempfile.mkdtemp()\\n        self.addCleanup(shutil.rmtree, self.tmp_dir)\\n        self._settings_override = override_settings(EMAIL_FILE_PATH=self.tmp_dir)\\n        self._settings_override.enable()\\n\\n    def tearDown(self):\\n        self._settings_override.disable()\\n        super().tearDown()\\n\\n    def flush_mailbox(self):\\n        for filename in os.listdir(self.tmp_dir):\\n            os.unlink(os.path.join(self.tmp_dir, filename))\\n\\n    def get_mailbox_content(self):\\n        messages = []\\n        for filename in os.listdir(self.tmp_dir):\\n            with open(os.path.join(self.tmp_dir, filename), \'rb\') as fp:\\n                session = fp.read().split(force_bytes(\'\\\\n\' + (\'-\' * 79) + \'\\\\n\', encoding=\'ascii\'))\\n            messages.extend(message_from_bytes(m) for m in session if m)\\n        return messages\\n\\n    def test_file_sessions(self):\\n        \\"\\"\\"Make sure opening a connection creates a new file\\"\\"\\"\\n        msg = EmailMessage(\\n            \'Subject\', \'Content\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        connection = mail.get_connection()\\n        connection.send_messages([msg])\\n\\n        self.assertEqual(len(os.listdir(self.tmp_dir)), 1)\\n        with open(os.path.join(self.tmp_dir, os.listdir(self.tmp_dir)[0]), \'rb\') as fp:\\n            message = message_from_binary_file(fp)\\n        self.assertEqual(message.get_content_type(), \'text/plain\')\\n        self.assertEqual(message.get(\'subject\'), \'Subject\')\\n        self.assertEqual(message.get(\'from\'), \'from@example.com\')\\n        self.assertEqual(message.get(\'to\'), \'to@example.com\')\\n\\n        connection2 = mail.get_connection()\\n        connection2.send_messages([msg])\\n        self.assertEqual(len(os.listdir(self.tmp_dir)), 2)\\n\\n        connection.send_messages([msg])\\n        self.assertEqual(len(os.listdir(self.tmp_dir)), 2)\\n\\n        msg.connection = mail.get_connection()\\n        self.assertTrue(connection.open())\\n        msg.send()\\n        self.assertEqual(len(os.listdir(self.tmp_dir)), 3)\\n        msg.send()\\n        self.assertEqual(len(os.listdir(self.tmp_dir)), 3)\\n\\n        connection.close()\\n\\n\\nclass ConsoleBackendTests(BaseEmailBackendTests, SimpleTestCase):\\n    email_backend = \'django.core.mail.backends.console.EmailBackend\'\\n\\n    def setUp(self):\\n        super().setUp()\\n        self.__stdout = sys.stdout\\n        self.stream = sys.stdout = StringIO()\\n\\n    def tearDown(self):\\n        del self.stream\\n        sys.stdout = self.__stdout\\n        del self.__stdout\\n        super().tearDown()\\n\\n    def flush_mailbox(self):\\n        self.stream = sys.stdout = StringIO()\\n\\n    def get_mailbox_content(self):\\n        messages = self.stream.getvalue().split(\'\\\\n\' + (\'-\' * 79) + \'\\\\n\')\\n        return [message_from_bytes(force_bytes(m)) for m in messages if m]\\n\\n    def test_console_stream_kwarg(self):\\n        \\"\\"\\"\\n        The console backend can be pointed at an arbitrary stream.\\n        \\"\\"\\"\\n        s = StringIO()\\n        connection = mail.get_connection(\'django.core.mail.backends.console.EmailBackend\', stream=s)\\n        send_mail(\'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'], connection=connection)\\n        message = force_bytes(s.getvalue().split(\'\\\\n\' + (\'-\' * 79) + \'\\\\n\')[0])\\n        self.assertMessageHasHeaders(message, {\\n            (\'MIME-Version\', \'1.0\'),\\n            (\'Content-Type\', \'text/plain; charset=\\"utf-8\\"\'),\\n            (\'Content-Transfer-Encoding\', \'7bit\'),\\n            (\'Subject\', \'Subject\'),\\n            (\'From\', \'from@example.com\'),\\n            (\'To\', \'to@example.com\')})\\n        self.assertIn(b\'\\\\nDate: \', message)\\n\\n\\nclass FakeSMTPChannel(smtpd.SMTPChannel):\\n\\n    def collect_incoming_data(self, data):\\n        try:\\n            smtpd.SMTPChannel.collect_incoming_data(self, data)\\n        except UnicodeDecodeError:\\n            # ignore decode error in SSL/TLS connection tests as we only care\\n            # whether the connection attempt was made\\n            pass\\n\\n    def smtp_AUTH(self, arg):\\n        if arg == \'CRAM-MD5\':\\n            # This is only the first part of the login process. But it\'s enough\\n            # for our tests.\\n            challenge = base64.b64encode(b\'somerandomstring13579\')\\n            self.push(\'334 %s\' % challenge.decode())\\n        else:\\n            self.push(\'502 Error: login \\"%s\\" not implemented\' % arg)\\n\\n\\nclass FakeSMTPServer(smtpd.SMTPServer, threading.Thread):\\n    \\"\\"\\"\\n    Asyncore SMTP server wrapped into a thread. Based on DummyFTPServer from:\\n    http://svn.python.org/view/python/branches/py3k/Lib/test/test_ftplib.py?revision=86061\\u0026view=markup\\n    \\"\\"\\"\\n    channel_class = FakeSMTPChannel\\n\\n    def __init__(self, *args, **kwargs):\\n        threading.Thread.__init__(self)\\n        # New kwarg added in Python 3.5; default switching to False in 3.6.\\n        if sys.version_info \\u003e= (3, 5):\\n            kwargs[\'decode_data\'] = True\\n        smtpd.SMTPServer.__init__(self, *args, **kwargs)\\n        self._sink = []\\n        self.active = False\\n        self.active_lock = threading.Lock()\\n        self.sink_lock = threading.Lock()\\n\\n    def process_message(self, peer, mailfrom, rcpttos, data):\\n        data = data.encode()\\n        m = message_from_bytes(data)\\n        maddr = parseaddr(m.get(\'from\'))[1]\\n\\n        if mailfrom != maddr:\\n            # According to the spec, mailfrom does not necessarily match the\\n            # From header - this is the case where the local part isn\'t\\n            # encoded, so try to correct that.\\n            lp, domain = mailfrom.split(\'@\', 1)\\n            lp = Header(lp, \'utf-8\').encode()\\n            mailfrom = \'@\'.join([lp, domain])\\n\\n        if mailfrom != maddr:\\n            return \\"553 \'%s\' != \'%s\'\\" % (mailfrom, maddr)\\n        with self.sink_lock:\\n            self._sink.append(m)\\n\\n    def get_sink(self):\\n        with self.sink_lock:\\n            return self._sink[:]\\n\\n    def flush_sink(self):\\n        with self.sink_lock:\\n            self._sink[:] = []\\n\\n    def start(self):\\n        assert not self.active\\n        self.__flag = threading.Event()\\n        threading.Thread.start(self)\\n        self.__flag.wait()\\n\\n    def run(self):\\n        self.active = True\\n        self.__flag.set()\\n        while self.active and asyncore.socket_map:\\n            with self.active_lock:\\n                asyncore.loop(timeout=0.1, count=1)\\n        asyncore.close_all()\\n\\n    def stop(self):\\n        if self.active:\\n            self.active = False\\n            self.join()\\n\\n\\nclass FakeAUTHSMTPConnection(SMTP):\\n    \\"\\"\\"\\n    A SMTP connection pretending support for the AUTH command. It does not, but\\n    at least this can allow testing the first part of the AUTH process.\\n    \\"\\"\\"\\n\\n    def ehlo(self, name=\'\'):\\n        response = SMTP.ehlo(self, name=name)\\n        self.esmtp_features.update({\\n            \'auth\': \'CRAM-MD5 PLAIN LOGIN\',\\n        })\\n        return response\\n\\n\\nclass SMTPBackendTestsBase(SimpleTestCase):\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        super().setUpClass()\\n        cls.server = FakeSMTPServer((\'127.0.0.1\', 0), None)\\n        cls._settings_override = override_settings(\\n            EMAIL_HOST=\\"127.0.0.1\\",\\n            EMAIL_PORT=cls.server.socket.getsockname()[1])\\n        cls._settings_override.enable()\\n        cls.server.start()\\n\\n    @classmethod\\n    def tearDownClass(cls):\\n        cls._settings_override.disable()\\n        cls.server.stop()\\n        super().tearDownClass()\\n\\n\\nclass SMTPBackendTests(BaseEmailBackendTests, SMTPBackendTestsBase):\\n    email_backend = \'django.core.mail.backends.smtp.EmailBackend\'\\n\\n    def setUp(self):\\n        super().setUp()\\n        self.server.flush_sink()\\n\\n    def tearDown(self):\\n        self.server.flush_sink()\\n        super().tearDown()\\n\\n    def flush_mailbox(self):\\n        self.server.flush_sink()\\n\\n    def get_mailbox_content(self):\\n        return self.server.get_sink()\\n\\n    @override_settings(\\n        EMAIL_HOST_USER=\\"not empty username\\",\\n        EMAIL_HOST_PASSWORD=\\"not empty password\\")\\n    def test_email_authentication_use_settings(self):\\n        backend = smtp.EmailBackend()\\n        self.assertEqual(backend.username, \'not empty username\')\\n        self.assertEqual(backend.password, \'not empty password\')\\n\\n    @override_settings(\\n        EMAIL_HOST_USER=\\"not empty username\\",\\n        EMAIL_HOST_PASSWORD=\\"not empty password\\")\\n    def test_email_authentication_override_settings(self):\\n        backend = smtp.EmailBackend(username=\'username\', password=\'password\')\\n        self.assertEqual(backend.username, \'username\')\\n        self.assertEqual(backend.password, \'password\')\\n\\n    @override_settings(\\n        EMAIL_HOST_USER=\\"not empty username\\",\\n        EMAIL_HOST_PASSWORD=\\"not empty password\\")\\n    def test_email_disabled_authentication(self):\\n        backend = smtp.EmailBackend(username=\'\', password=\'\')\\n        self.assertEqual(backend.username, \'\')\\n        self.assertEqual(backend.password, \'\')\\n\\n    def test_auth_attempted(self):\\n        \\"\\"\\"\\n        Opening the backend with non empty username/password tries\\n        to authenticate against the SMTP server.\\n        \\"\\"\\"\\n        backend = smtp.EmailBackend(\\n            username=\'not empty username\', password=\'not empty password\')\\n        with self.assertRaisesMessage(SMTPException, \'SMTP AUTH extension not supported by server.\'):\\n            with backend:\\n                pass\\n\\n    def test_server_open(self):\\n        \\"\\"\\"\\n        open() returns whether it opened a connection.\\n        \\"\\"\\"\\n        backend = smtp.EmailBackend(username=\'\', password=\'\')\\n        self.assertFalse(backend.connection)\\n        opened = backend.open()\\n        backend.close()\\n        self.assertTrue(opened)\\n\\n    def test_server_login(self):\\n        \\"\\"\\"\\n        Even if the Python SMTP server doesn\'t support authentication, the\\n        login process starts and the appropriate exception is raised.\\n        \\"\\"\\"\\n        class CustomEmailBackend(smtp.EmailBackend):\\n            connection_class = FakeAUTHSMTPConnection\\n\\n        backend = CustomEmailBackend(username=\'username\', password=\'password\')\\n        with self.assertRaises(SMTPAuthenticationError):\\n            with backend:\\n                pass\\n\\n    @override_settings(EMAIL_USE_TLS=True)\\n    def test_email_tls_use_settings(self):\\n        backend = smtp.EmailBackend()\\n        self.assertTrue(backend.use_tls)\\n\\n    @override_settings(EMAIL_USE_TLS=True)\\n    def test_email_tls_override_settings(self):\\n        backend = smtp.EmailBackend(use_tls=False)\\n        self.assertFalse(backend.use_tls)\\n\\n    def test_email_tls_default_disabled(self):\\n        backend = smtp.EmailBackend()\\n        self.assertFalse(backend.use_tls)\\n\\n    @override_settings(EMAIL_USE_SSL=True)\\n    def test_email_ssl_use_settings(self):\\n        backend = smtp.EmailBackend()\\n        self.assertTrue(backend.use_ssl)\\n\\n    @override_settings(EMAIL_USE_SSL=True)\\n    def test_email_ssl_override_settings(self):\\n        backend = smtp.EmailBackend(use_ssl=False)\\n        self.assertFalse(backend.use_ssl)\\n\\n    def test_email_ssl_default_disabled(self):\\n        backend = smtp.EmailBackend()\\n        self.assertFalse(backend.use_ssl)\\n\\n    @override_settings(EMAIL_SSL_CERTFILE=\'foo\')\\n    def test_email_ssl_certfile_use_settings(self):\\n        backend = smtp.EmailBackend()\\n        self.assertEqual(backend.ssl_certfile, \'foo\')\\n\\n    @override_settings(EMAIL_SSL_CERTFILE=\'foo\')\\n    def test_email_ssl_certfile_override_settings(self):\\n        backend = smtp.EmailBackend(ssl_certfile=\'bar\')\\n        self.assertEqual(backend.ssl_certfile, \'bar\')\\n\\n    def test_email_ssl_certfile_default_disabled(self):\\n        backend = smtp.EmailBackend()\\n        self.assertIsNone(backend.ssl_certfile)\\n\\n    @override_settings(EMAIL_SSL_KEYFILE=\'foo\')\\n    def test_email_ssl_keyfile_use_settings(self):\\n        backend = smtp.EmailBackend()\\n        self.assertEqual(backend.ssl_keyfile, \'foo\')\\n\\n    @override_settings(EMAIL_SSL_KEYFILE=\'foo\')\\n    def test_email_ssl_keyfile_override_settings(self):\\n        backend = smtp.EmailBackend(ssl_keyfile=\'bar\')\\n        self.assertEqual(backend.ssl_keyfile, \'bar\')\\n\\n    def test_email_ssl_keyfile_default_disabled(self):\\n        backend = smtp.EmailBackend()\\n        self.assertIsNone(backend.ssl_keyfile)\\n\\n    @override_settings(EMAIL_USE_TLS=True)\\n    def test_email_tls_attempts_starttls(self):\\n        backend = smtp.EmailBackend()\\n        self.assertTrue(backend.use_tls)\\n        with self.assertRaisesMessage(SMTPException, \'STARTTLS extension not supported by server.\'):\\n            with backend:\\n                pass\\n\\n    @override_settings(EMAIL_USE_SSL=True)\\n    def test_email_ssl_attempts_ssl_connection(self):\\n        backend = smtp.EmailBackend()\\n        self.assertTrue(backend.use_ssl)\\n        with self.assertRaises(SSLError):\\n            with backend:\\n                pass\\n\\n    def test_connection_timeout_default(self):\\n        \\"\\"\\"The connection\'s timeout value is None by default.\\"\\"\\"\\n        connection = mail.get_connection(\'django.core.mail.backends.smtp.EmailBackend\')\\n        self.assertIsNone(connection.timeout)\\n\\n    def test_connection_timeout_custom(self):\\n        \\"\\"\\"The timeout parameter can be customized.\\"\\"\\"\\n        class MyEmailBackend(smtp.EmailBackend):\\n            def __init__(self, *args, **kwargs):\\n                kwargs.setdefault(\'timeout\', 42)\\n                super().__init__(*args, **kwargs)\\n\\n        myemailbackend = MyEmailBackend()\\n        myemailbackend.open()\\n        self.assertEqual(myemailbackend.timeout, 42)\\n        self.assertEqual(myemailbackend.connection.timeout, 42)\\n        myemailbackend.close()\\n\\n    @override_settings(EMAIL_TIMEOUT=10)\\n    def test_email_timeout_override_settings(self):\\n        backend = smtp.EmailBackend()\\n        self.assertEqual(backend.timeout, 10)\\n\\n    def test_email_msg_uses_crlf(self):\\n        \\"\\"\\"#23063 -- RFC-compliant messages are sent over SMTP.\\"\\"\\"\\n        send = SMTP.send\\n        try:\\n            smtp_messages = []\\n\\n            def mock_send(self, s):\\n                smtp_messages.append(s)\\n                return send(self, s)\\n\\n            SMTP.send = mock_send\\n\\n            email = EmailMessage(\'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'])\\n            mail.get_connection().send_messages([email])\\n\\n            # Find the actual message\\n            msg = None\\n            for i, m in enumerate(smtp_messages):\\n                if m[:4] == \'data\':\\n                    msg = smtp_messages[i + 1]\\n                    break\\n\\n            self.assertTrue(msg)\\n\\n            msg = msg.decode()\\n            # The message only contains CRLF and not combinations of CRLF, LF, and CR.\\n            msg = msg.replace(\'\\\\r\\\\n\', \'\')\\n            self.assertNotIn(\'\\\\r\', msg)\\n            self.assertNotIn(\'\\\\n\', msg)\\n\\n        finally:\\n            SMTP.send = send\\n\\n    def test_send_messages_after_open_failed(self):\\n        \\"\\"\\"\\n        send_messages() shouldn\'t try to send messages if open() raises an\\n        exception after initializing the connection.\\n        \\"\\"\\"\\n        backend = smtp.EmailBackend()\\n        # Simulate connection initialization success and a subsequent\\n        # connection exception.\\n        backend.connection = True\\n        backend.open = lambda: None\\n        email = EmailMessage(\'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'])\\n        self.assertEqual(backend.send_messages([email]), None)\\n\\n\\nclass SMTPBackendStoppedServerTests(SMTPBackendTestsBase):\\n    \\"\\"\\"\\n    These tests require a separate class, because the FakeSMTPServer is shut\\n    down in setUpClass(), and it cannot be restarted (\\"RuntimeError: threads\\n    can only be started once\\").\\n    \\"\\"\\"\\n    @classmethod\\n    def setUpClass(cls):\\n        super().setUpClass()\\n        cls.backend = smtp.EmailBackend(username=\'\', password=\'\')\\n        cls.server.stop()\\n\\n    def test_server_stopped(self):\\n        \\"\\"\\"\\n        Closing the backend while the SMTP server is stopped doesn\'t raise an\\n        exception.\\n        \\"\\"\\"\\n        self.backend.close()\\n\\n    def test_fail_silently_on_connection_error(self):\\n        \\"\\"\\"\\n        A socket connection error is silenced with fail_silently=True.\\n        \\"\\"\\"\\n        with self.assertRaises(socket.error):\\n            self.backend.open()\\n        self.backend.fail_silently = True\\n        self.backend.open()\\n"}\n'
line: b'{"repo_name":"w1ll1am23/home-assistant","ref":"refs/heads/dev","path":"homeassistant/components/homematicip_cloud/weather.py","content":"\\n\\"\\"\\"Support for HomematicIP Cloud weather devices.\\"\\"\\"\\nimport logging\\n\\nfrom homematicip.aio.device import (\\n    AsyncWeatherSensor, AsyncWeatherSensorPlus, AsyncWeatherSensorPro)\\nfrom homematicip.aio.home import AsyncHome\\n\\nfrom homeassistant.components.weather import WeatherEntity\\nfrom homeassistant.config_entries import ConfigEntry\\nfrom homeassistant.const import TEMP_CELSIUS\\nfrom homeassistant.core import HomeAssistant\\n\\nfrom . import DOMAIN as HMIPC_DOMAIN, HMIPC_HAPID, HomematicipGenericDevice\\n\\n_LOGGER = logging.getLogger(__name__)\\n\\n\\nasync def async_setup_platform(\\n        hass, config, async_add_entities, discovery_info=None):\\n    \\"\\"\\"Set up the HomematicIP Cloud weather sensor.\\"\\"\\"\\n    pass\\n\\n\\nasync def async_setup_entry(hass: HomeAssistant, config_entry: ConfigEntry,\\n                            async_add_entities) -\\u003e None:\\n    \\"\\"\\"Set up the HomematicIP weather sensor from a config entry.\\"\\"\\"\\n    home = hass.data[HMIPC_DOMAIN][config_entry.data[HMIPC_HAPID]].home\\n    devices = []\\n    for device in home.devices:\\n        if isinstance(device, AsyncWeatherSensorPro):\\n            devices.append(HomematicipWeatherSensorPro(home, device))\\n        elif isinstance(device, (AsyncWeatherSensor, AsyncWeatherSensorPlus)):\\n            devices.append(HomematicipWeatherSensor(home, device))\\n\\n    if devices:\\n        async_add_entities(devices)\\n\\n\\nclass HomematicipWeatherSensor(HomematicipGenericDevice, WeatherEntity):\\n    \\"\\"\\"representation of a HomematicIP Cloud weather sensor plus \\u0026 basic.\\"\\"\\"\\n\\n    def __init__(self, home: AsyncHome, device) -\\u003e None:\\n        \\"\\"\\"Initialize the weather sensor.\\"\\"\\"\\n        super().__init__(home, device)\\n\\n    @property\\n    def name(self) -\\u003e str:\\n        \\"\\"\\"Return the name of the sensor.\\"\\"\\"\\n        return self._device.label\\n\\n    @property\\n    def temperature(self) -\\u003e float:\\n        \\"\\"\\"Return the platform temperature.\\"\\"\\"\\n        return self._device.actualTemperature\\n\\n    @property\\n    def temperature_unit(self) -\\u003e str:\\n        \\"\\"\\"Return the unit of measurement.\\"\\"\\"\\n        return TEMP_CELSIUS\\n\\n    @property\\n    def humidity(self) -\\u003e int:\\n        \\"\\"\\"Return the humidity.\\"\\"\\"\\n        return self._device.humidity\\n\\n    @property\\n    def wind_speed(self) -\\u003e float:\\n        \\"\\"\\"Return the wind speed.\\"\\"\\"\\n        return self._device.windSpeed\\n\\n    @property\\n    def attribution(self) -\\u003e str:\\n        \\"\\"\\"Return the attribution.\\"\\"\\"\\n        return \\"Powered by Homematic IP\\"\\n\\n    @property\\n    def condition(self) -\\u003e str:\\n        \\"\\"\\"Return the current condition.\\"\\"\\"\\n        if hasattr(self._device, \\"raining\\") and self._device.raining:\\n            return \'rainy\'\\n        if self._device.storm:\\n            return \'windy\'\\n        if self._device.sunshine:\\n            return \'sunny\'\\n        return \'\'\\n\\n\\nclass HomematicipWeatherSensorPro(HomematicipWeatherSensor):\\n    \\"\\"\\"representation of a HomematicIP weather sensor pro.\\"\\"\\"\\n\\n    @property\\n    def wind_bearing(self) -\\u003e float:\\n        \\"\\"\\"Return the wind bearing.\\"\\"\\"\\n        return self._device.windDirection\\n"}\n'
line: b'{"repo_name":"andybab/Impala","ref":"refs/heads/master","path":"tests/util/hdfs_util.py","content":"#!/usr/bin/env python\\n# Copyright (c) 2012 Cloudera, Inc. All rights reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n# http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n# Hdfs access utilities\\n\\nfrom xml.etree.ElementTree import parse\\nfrom pywebhdfs.webhdfs import PyWebHdfsClient, errors, _raise_pywebhdfs_exception\\nimport getpass\\nimport types\\nimport requests, httplib\\n\\nclass PyWebHdfsClientWithChmod(PyWebHdfsClient):\\n  def chmod(self, path, permission):\\n    \\"\\"\\"Set the permission of \'path\' to \'permission\' (specified as an octal string, e.g.\\n    \'775\'\\"\\"\\"\\n    uri = self._create_uri(path, \\"SETPERMISSION\\", permission=permission)\\n    response = requests.put(uri, allow_redirects=True)\\n    if not response.status_code == httplib.OK:\\n      _raise_pywebhdfs_exception(response.status_code, response.text)\\n\\n    return True\\n\\nclass HdfsConfig(object):\\n  \\"\\"\\"Reads an XML configuration file (produced by a mini-cluster) into a dictionary\\n  accessible via get()\\"\\"\\"\\n  def __init__(self, filename):\\n    self.conf = {}\\n    tree = parse(filename)\\n    for property in tree.getroot().getiterator(\'property\'):\\n      self.conf[property.find(\'name\').text] = property.find(\'value\').text\\n\\n  def get(self, key):\\n    return self.conf.get(key)\\n\\ndef get_hdfs_client_from_conf(conf):\\n  \\"\\"\\"Returns a new HTTP client for an HDFS cluster using an HdfsConfig object\\"\\"\\"\\n  hostport = conf.get(\'dfs.namenode.http-address\')\\n  if hostport is None:\\n    raise Exception(\\"dfs.namenode.http-address not found in config\\")\\n  host, port = hostport.split(\\":\\")\\n  return get_hdfs_client(host=host, port=port)\\n\\ndef __pyweb_hdfs_client_exists(self, path):\\n  \\"\\"\\"The PyWebHdfsClient doesn\'t provide an API to cleanly detect if a file or directory\\n  exists. This method is bound to each client that is created so tests can simply call\\n  hdfs_client.exists(\'path\') and get back a bool.\\n  \\"\\"\\"\\n  try:\\n    self.get_file_dir_status(path)\\n  except errors.FileNotFound:\\n    return False\\n  return True\\n\\ndef get_hdfs_client(host, port, user_name=getpass.getuser()):\\n  \\"\\"\\"Returns a new HTTP client for an HDFS cluster using an explict host:port pair\\"\\"\\"\\n  hdfs_client = PyWebHdfsClientWithChmod(host=host, port=port, user_name=user_name)\\n  # Bind our \\"exists\\" method to hdfs_client.exists\\n  hdfs_client.exists = types.MethodType(__pyweb_hdfs_client_exists, hdfs_client)\\n  return hdfs_client\\n"}\n'
line: b'{"repo_name":"doublebits/osf.io","ref":"refs/heads/develop","path":"admin_tests/factories.py","content":"import factory\\n\\nfrom admin.common_auth.models import MyUser\\n\\n\\nclass UserFactory(factory.Factory):\\n    class Meta:\\n        model = MyUser\\n\\n    id = 123\\n    email = \'cello@email.org\'\\n    first_name = \'Yo-yo\'\\n    last_name = \'Ma\'\\n    osf_id = \'abc12\'\\n\\n    @classmethod\\n    def is_in_group(cls, value):\\n        return True\\n"}\n'
line: b'{"repo_name":"sysadmind/ansible-modules-extras","ref":"refs/heads/devel","path":"monitoring/pagerduty.py","content":"#!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n#\\n# This file is part of Ansible\\n#\\n# Ansible is free software: you can redistribute it and/or modify\\n# it under the terms of the GNU General Public License as published by\\n# the Free Software Foundation, either version 3 of the License, or\\n# (at your option) any later version.\\n#\\n# Ansible is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU General Public License for more details.\\n#\\n# You should have received a copy of the GNU General Public License\\n# along with Ansible.  If not, see \\u003chttp://www.gnu.org/licenses/\\u003e.\\n\\nDOCUMENTATION = \'\'\'\\n\\nmodule: pagerduty\\nshort_description: Create PagerDuty maintenance windows\\ndescription:\\n    - This module will let you create PagerDuty maintenance windows\\nversion_added: \\"1.2\\"\\nauthor:\\n    - \\"Andrew Newdigate (@suprememoocow)\\"\\n    - \\"Dylan Silva (@thaumos)\\"\\n    - \\"Justin Johns\\"\\n    - \\"Bruce Pennypacker\\"\\nrequirements:\\n    - PagerDuty API access\\noptions:\\n    state:\\n        description:\\n            - Create a maintenance window or get a list of ongoing windows.\\n        required: true\\n        default: null\\n        choices: [ \\"running\\", \\"started\\", \\"ongoing\\", \\"absent\\" ]\\n        aliases: []\\n    name:\\n        description:\\n            - PagerDuty unique subdomain.\\n        required: true\\n        default: null\\n        choices: []\\n        aliases: []\\n    user:\\n        description:\\n            - PagerDuty user ID.\\n        required: true\\n        default: null\\n        choices: []\\n        aliases: []\\n    passwd:\\n        description:\\n            - PagerDuty user password.\\n        required: true\\n        default: null\\n        choices: []\\n        aliases: []\\n    token:\\n        description:\\n            - A pagerduty token, generated on the pagerduty site. Can be used instead of\\n              user/passwd combination.\\n        required: true\\n        default: null\\n        choices: []\\n        aliases: []\\n        version_added: \'1.8\'\\n    requester_id:\\n        description:\\n            - ID of user making the request. Only needed when using a token and creating a maintenance_window.\\n        required: true\\n        default: null\\n        choices: []\\n        aliases: []\\n        version_added: \'1.8\'\\n    service:\\n        description:\\n            - A comma separated list of PagerDuty service IDs.\\n        required: false\\n        default: null\\n        choices: []\\n        aliases: [ services ]\\n    hours:\\n        description:\\n            - Length of maintenance window in hours.\\n        required: false\\n        default: 1\\n        choices: []\\n        aliases: []\\n    minutes:\\n        description:\\n            - Maintenance window in minutes (this is added to the hours).\\n        required: false\\n        default: 0\\n        choices: []\\n        aliases: []\\n        version_added: \'1.8\'\\n    desc:\\n        description:\\n            - Short description of maintenance window.\\n        required: false\\n        default: Created by Ansible\\n        choices: []\\n        aliases: []\\n    validate_certs:\\n        description:\\n            - If C(no), SSL certificates will not be validated. This should only be used\\n              on personally controlled sites using self-signed certificates.\\n        required: false\\n        default: \'yes\'\\n        choices: [\'yes\', \'no\']\\n        version_added: 1.5.1\\n\'\'\'\\n\\nEXAMPLES=\'\'\'\\n# List ongoing maintenance windows using a user/passwd\\n- pagerduty: name=companyabc user=example@example.com passwd=password123 state=ongoing\\n\\n# List ongoing maintenance windows using a token\\n- pagerduty: name=companyabc token=xxxxxxxxxxxxxx state=ongoing\\n\\n# Create a 1 hour maintenance window for service FOO123, using a user/passwd\\n- pagerduty: name=companyabc\\n             user=example@example.com\\n             passwd=password123\\n             state=running\\n             service=FOO123\\n\\n# Create a 5 minute maintenance window for service FOO123, using a token\\n- pagerduty: name=companyabc\\n             token=xxxxxxxxxxxxxx\\n             hours=0\\n             minutes=5\\n             state=running\\n             service=FOO123\\n\\n\\n# Create a 4 hour maintenance window for service FOO123 with the description \\"deployment\\".\\n- pagerduty: name=companyabc\\n             user=example@example.com\\n             passwd=password123\\n             state=running\\n             service=FOO123\\n             hours=4\\n             desc=deployment\\n  register: pd_window\\n\\n# Delete the previous maintenance window\\n- pagerduty: name=companyabc\\n             user=example@example.com\\n             passwd=password123\\n             state=absent\\n             service={{ pd_window.result.maintenance_window.id }}\\n\'\'\'\\n\\nimport datetime\\nimport base64\\n\\ndef auth_header(user, passwd, token):\\n    if token:\\n        return \\"Token token=%s\\" % token\\n\\n    auth = base64.encodestring(\'%s:%s\' % (user, passwd)).replace(\'\\\\n\', \'\')\\n    return \\"Basic %s\\" % auth\\n\\ndef ongoing(module, name, user, passwd, token):\\n    url = \\"https://\\" + name + \\".pagerduty.com/api/v1/maintenance_windows/ongoing\\"\\n    headers = {\\"Authorization\\": auth_header(user, passwd, token)}\\n\\n    response, info = fetch_url(module, url, headers=headers)\\n    if info[\'status\'] != 200:\\n        module.fail_json(msg=\\"failed to lookup the ongoing window: %s\\" % info[\'msg\'])\\n\\n    try:\\n        json_out = json.loads(response.read())\\n    except:\\n        json_out = \\"\\"\\n\\n    return False, json_out, False\\n\\n\\ndef create(module, name, user, passwd, token, requester_id, service, hours, minutes, desc):\\n    now = datetime.datetime.utcnow()\\n    later = now + datetime.timedelta(hours=int(hours), minutes=int(minutes))\\n    start = now.strftime(\\"%Y-%m-%dT%H:%M:%SZ\\")\\n    end = later.strftime(\\"%Y-%m-%dT%H:%M:%SZ\\")\\n\\n    url = \\"https://\\" + name + \\".pagerduty.com/api/v1/maintenance_windows\\"\\n    headers = {\\n        \'Authorization\': auth_header(user, passwd, token),\\n        \'Content-Type\' : \'application/json\',\\n    }\\n    request_data = {\'maintenance_window\': {\'start_time\': start, \'end_time\': end, \'description\': desc, \'service_ids\': service}}\\n    \\n    if requester_id:\\n        request_data[\'requester_id\'] = requester_id\\n    else:\\n        if token:\\n            module.fail_json(msg=\\"requester_id is required when using a token\\")\\n\\n    data = json.dumps(request_data)\\n    response, info = fetch_url(module, url, data=data, headers=headers, method=\'POST\')\\n    if info[\'status\'] != 200:\\n        module.fail_json(msg=\\"failed to create the window: %s\\" % info[\'msg\'])\\n\\n    try:\\n        json_out = json.loads(response.read())\\n    except:\\n        json_out = \\"\\"\\n\\n    return False, json_out, True\\n\\ndef absent(module, name, user, passwd, token, requester_id, service):\\n    url = \\"https://\\" + name + \\".pagerduty.com/api/v1/maintenance_windows/\\" + service[0]\\n    headers = {\\n        \'Authorization\': auth_header(user, passwd, token),\\n        \'Content-Type\' : \'application/json\',\\n    }\\n    request_data = {}\\n    \\n    if requester_id:\\n        request_data[\'requester_id\'] = requester_id\\n    else:\\n        if token:\\n            module.fail_json(msg=\\"requester_id is required when using a token\\")\\n\\n    data = json.dumps(request_data)\\n    response, info = fetch_url(module, url, data=data, headers=headers, method=\'DELETE\')\\n    if info[\'status\'] != 200:\\n        module.fail_json(msg=\\"failed to delete the window: %s\\" % info[\'msg\'])\\n\\n    try:\\n        json_out = json.loads(response.read())\\n    except:\\n        json_out = \\"\\"\\n\\n    return False, json_out, True\\n\\n\\ndef main():\\n\\n    module = AnsibleModule(\\n        argument_spec=dict(\\n        state=dict(required=True, choices=[\'running\', \'started\', \'ongoing\', \'absent\']),\\n        name=dict(required=True),\\n        user=dict(required=False),\\n        passwd=dict(required=False),\\n        token=dict(required=False),\\n        service=dict(required=False, type=\'list\', aliases=[\\"services\\"]),\\n        requester_id=dict(required=False),\\n        hours=dict(default=\'1\', required=False),\\n        minutes=dict(default=\'0\', required=False),\\n        desc=dict(default=\'Created by Ansible\', required=False),\\n        validate_certs = dict(default=\'yes\', type=\'bool\'),\\n        )\\n    )\\n\\n    state = module.params[\'state\']\\n    name = module.params[\'name\']\\n    user = module.params[\'user\']\\n    passwd = module.params[\'passwd\']\\n    token = module.params[\'token\']\\n    service = module.params[\'service\']\\n    hours = module.params[\'hours\']\\n    minutes = module.params[\'minutes\']\\n    token = module.params[\'token\']\\n    desc = module.params[\'desc\']\\n    requester_id =  module.params[\'requester_id\']\\n\\n    if not token and not (user or passwd):\\n        module.fail_json(msg=\\"neither user and passwd nor token specified\\")\\n\\n    if state == \\"running\\" or state == \\"started\\":\\n        if not service:\\n            module.fail_json(msg=\\"service not specified\\")\\n        (rc, out, changed) = create(module, name, user, passwd, token, requester_id, service, hours, minutes, desc)\\n        if rc == 0:\\n            changed=True\\n\\n    if state == \\"ongoing\\":\\n        (rc, out, changed) = ongoing(module, name, user, passwd, token)\\n\\n    if state == \\"absent\\":\\n        (rc, out, changed) = absent(module, name, user, passwd, token, requester_id, service)\\n\\n    if rc != 0:\\n        module.fail_json(msg=\\"failed\\", result=out)\\n\\n\\n    module.exit_json(msg=\\"success\\", result=out, changed=changed)\\n\\n# import module snippets\\nfrom ansible.module_utils.basic import *\\nfrom ansible.module_utils.urls import *\\n\\nmain()\\n"}\n'
line: b'{"repo_name":"ftomassetti/intellij-community","ref":"refs/heads/master","path":"python/lib/Lib/site-packages/django/contrib/gis/geos/tests/test_geos.py","content":"import ctypes, random, unittest, sys\\nfrom django.contrib.gis.geos import *\\nfrom django.contrib.gis.geos.base import gdal, numpy, GEOSBase\\nfrom django.contrib.gis.geos.libgeos import GEOS_PREPARE\\nfrom django.contrib.gis.geometry.test_data import TestDataMixin\\n\\nclass GEOSTest(unittest.TestCase, TestDataMixin):\\n\\n    @property\\n    def null_srid(self):\\n        \\"\\"\\"\\n        Returns the proper null SRID depending on the GEOS version.\\n        See the comments in `test15_srid` for more details.\\n        \\"\\"\\"\\n        info = geos_version_info()\\n        if info[\'version\'] == \'3.0.0\' and info[\'release_candidate\']:\\n            return -1\\n        else:\\n            return None\\n\\n    def test00_base(self):\\n        \\"Tests out the GEOSBase class.\\"\\n        # Testing out GEOSBase class, which provides a `ptr` property\\n        # that abstracts out access to underlying C pointers.\\n        class FakeGeom1(GEOSBase):\\n            pass\\n\\n        # This one only accepts pointers to floats\\n        c_float_p = ctypes.POINTER(ctypes.c_float)\\n        class FakeGeom2(GEOSBase):\\n            ptr_type = c_float_p\\n\\n        # Default ptr_type is `c_void_p`.\\n        fg1 = FakeGeom1()\\n        # Default ptr_type is C float pointer\\n        fg2 = FakeGeom2()\\n\\n        # These assignments are OK -- None is allowed because\\n        # it\'s equivalent to the NULL pointer.\\n        fg1.ptr = ctypes.c_void_p()\\n        fg1.ptr = None\\n        fg2.ptr = c_float_p(ctypes.c_float(5.23))\\n        fg2.ptr = None\\n\\n        # Because pointers have been set to NULL, an exception should be\\n        # raised when we try to access it.  Raising an exception is\\n        # preferrable to a segmentation fault that commonly occurs when\\n        # a C method is given a NULL memory reference.\\n        for fg in (fg1, fg2):\\n            # Equivalent to `fg.ptr`\\n            self.assertRaises(GEOSException, fg._get_ptr)\\n\\n        # Anything that is either not None or the acceptable pointer type will\\n        # result in a TypeError when trying to assign it to the `ptr` property.\\n        # Thus, memmory addresses (integers) and pointers of the incorrect type\\n        # (in `bad_ptrs`) will not be allowed.\\n        bad_ptrs = (5, ctypes.c_char_p(\'foobar\'))\\n        for bad_ptr in bad_ptrs:\\n            # Equivalent to `fg.ptr = bad_ptr`\\n            self.assertRaises(TypeError, fg1._set_ptr, bad_ptr)\\n            self.assertRaises(TypeError, fg2._set_ptr, bad_ptr)\\n\\n    def test01a_wkt(self):\\n        \\"Testing WKT output.\\"\\n        for g in self.geometries.wkt_out:\\n            geom = fromstr(g.wkt)\\n            self.assertEqual(g.ewkt, geom.wkt)\\n\\n    def test01b_hex(self):\\n        \\"Testing HEX output.\\"\\n        for g in self.geometries.hex_wkt:\\n            geom = fromstr(g.wkt)\\n            self.assertEqual(g.hex, geom.hex)\\n\\n    def test01b_hexewkb(self):\\n        \\"Testing (HEX)EWKB output.\\"\\n        from binascii import a2b_hex\\n\\n        # For testing HEX(EWKB).\\n        ogc_hex = \'01010000000000000000000000000000000000F03F\'\\n        # `SELECT ST_AsHEXEWKB(ST_GeomFromText(\'POINT(0 1)\', 4326));`\\n        hexewkb_2d = \'0101000020E61000000000000000000000000000000000F03F\'\\n        # `SELECT ST_AsHEXEWKB(ST_GeomFromEWKT(\'SRID=4326;POINT(0 1 2)\'));`\\n        hexewkb_3d = \'01010000A0E61000000000000000000000000000000000F03F0000000000000040\'\\n\\n        pnt_2d = Point(0, 1, srid=4326)\\n        pnt_3d = Point(0, 1, 2, srid=4326)\\n\\n        # OGC-compliant HEX will not have SRID nor Z value.\\n        self.assertEqual(ogc_hex, pnt_2d.hex)\\n        self.assertEqual(ogc_hex, pnt_3d.hex)\\n\\n        # HEXEWKB should be appropriate for its dimension -- have to use an\\n        # a WKBWriter w/dimension set accordingly, else GEOS will insert\\n        # garbage into 3D coordinate if there is none.  Also, GEOS has a\\n        # a bug in versions prior to 3.1 that puts the X coordinate in\\n        # place of Z; an exception should be raised on those versions.\\n        self.assertEqual(hexewkb_2d, pnt_2d.hexewkb)\\n        if GEOS_PREPARE:\\n            self.assertEqual(hexewkb_3d, pnt_3d.hexewkb)\\n            self.assertEqual(True, GEOSGeometry(hexewkb_3d).hasz)\\n        else:\\n            try:\\n                hexewkb = pnt_3d.hexewkb\\n            except GEOSException:\\n                pass\\n            else:\\n                self.fail(\'Should have raised GEOSException.\')\\n\\n        # Same for EWKB.\\n        self.assertEqual(buffer(a2b_hex(hexewkb_2d)), pnt_2d.ewkb)\\n        if GEOS_PREPARE:\\n            self.assertEqual(buffer(a2b_hex(hexewkb_3d)), pnt_3d.ewkb)\\n        else:\\n            try:\\n                ewkb = pnt_3d.ewkb\\n            except GEOSException:\\n                pass\\n            else:\\n                self.fail(\'Should have raised GEOSException\')\\n\\n        # Redundant sanity check.\\n        self.assertEqual(4326, GEOSGeometry(hexewkb_2d).srid)\\n\\n    def test01c_kml(self):\\n        \\"Testing KML output.\\"\\n        for tg in self.geometries.wkt_out:\\n            geom = fromstr(tg.wkt)\\n            kml = getattr(tg, \'kml\', False)\\n            if kml: self.assertEqual(kml, geom.kml)\\n\\n    def test01d_errors(self):\\n        \\"Testing the Error handlers.\\"\\n        # string-based\\n        print \\"\\\\nBEGIN - expecting GEOS_ERROR; safe to ignore.\\\\n\\"\\n        for err in self.geometries.errors:\\n            try:\\n                g = fromstr(err.wkt)\\n            except (GEOSException, ValueError):\\n                pass\\n\\n        # Bad WKB\\n        self.assertRaises(GEOSException, GEOSGeometry, buffer(\'0\'))\\n\\n        print \\"\\\\nEND - expecting GEOS_ERROR; safe to ignore.\\\\n\\"\\n\\n        class NotAGeometry(object):\\n            pass\\n\\n        # Some other object\\n        self.assertRaises(TypeError, GEOSGeometry, NotAGeometry())\\n        # None\\n        self.assertRaises(TypeError, GEOSGeometry, None)\\n\\n    def test01e_wkb(self):\\n        \\"Testing WKB output.\\"\\n        from binascii import b2a_hex\\n        for g in self.geometries.hex_wkt:\\n            geom = fromstr(g.wkt)\\n            wkb = geom.wkb\\n            self.assertEqual(b2a_hex(wkb).upper(), g.hex)\\n\\n    def test01f_create_hex(self):\\n        \\"Testing creation from HEX.\\"\\n        for g in self.geometries.hex_wkt:\\n            geom_h = GEOSGeometry(g.hex)\\n            # we need to do this so decimal places get normalised\\n            geom_t = fromstr(g.wkt)\\n            self.assertEqual(geom_t.wkt, geom_h.wkt)\\n\\n    def test01g_create_wkb(self):\\n        \\"Testing creation from WKB.\\"\\n        from binascii import a2b_hex\\n        for g in self.geometries.hex_wkt:\\n            wkb = buffer(a2b_hex(g.hex))\\n            geom_h = GEOSGeometry(wkb)\\n            # we need to do this so decimal places get normalised\\n            geom_t = fromstr(g.wkt)\\n            self.assertEqual(geom_t.wkt, geom_h.wkt)\\n\\n    def test01h_ewkt(self):\\n        \\"Testing EWKT.\\"\\n        srid = 32140\\n        for p in self.geometries.polygons:\\n            ewkt = \'SRID=%d;%s\' % (srid, p.wkt)\\n            poly = fromstr(ewkt)\\n            self.assertEqual(srid, poly.srid)\\n            self.assertEqual(srid, poly.shell.srid)\\n            self.assertEqual(srid, fromstr(poly.ewkt).srid) # Checking export\\n\\n    def test01i_json(self):\\n        \\"Testing GeoJSON input/output (via GDAL).\\"\\n        if not gdal or not gdal.GEOJSON: return\\n        for g in self.geometries.json_geoms:\\n            geom = GEOSGeometry(g.wkt)\\n            if not hasattr(g, \'not_equal\'):\\n                self.assertEqual(g.json, geom.json)\\n                self.assertEqual(g.json, geom.geojson)\\n            self.assertEqual(GEOSGeometry(g.wkt), GEOSGeometry(geom.json))\\n\\n    def test01k_fromfile(self):\\n        \\"Testing the fromfile() factory.\\"\\n        from StringIO import StringIO\\n        ref_pnt = GEOSGeometry(\'POINT(5 23)\')\\n\\n        wkt_f = StringIO()\\n        wkt_f.write(ref_pnt.wkt)\\n        wkb_f = StringIO()\\n        wkb_f.write(str(ref_pnt.wkb))\\n\\n        # Other tests use `fromfile()` on string filenames so those\\n        # aren\'t tested here.\\n        for fh in (wkt_f, wkb_f):\\n            fh.seek(0)\\n            pnt = fromfile(fh)\\n            self.assertEqual(ref_pnt, pnt)\\n\\n    def test01k_eq(self):\\n        \\"Testing equivalence.\\"\\n        p = fromstr(\'POINT(5 23)\')\\n        self.assertEqual(p, p.wkt)\\n        self.assertNotEqual(p, \'foo\')\\n        ls = fromstr(\'LINESTRING(0 0, 1 1, 5 5)\')\\n        self.assertEqual(ls, ls.wkt)\\n        self.assertNotEqual(p, \'bar\')\\n        # Error shouldn\'t be raise on equivalence testing with\\n        # an invalid type.\\n        for g in (p, ls):\\n            self.assertNotEqual(g, None)\\n            self.assertNotEqual(g, {\'foo\' : \'bar\'})\\n            self.assertNotEqual(g, False)\\n\\n    def test02a_points(self):\\n        \\"Testing Point objects.\\"\\n        prev = fromstr(\'POINT(0 0)\')\\n        for p in self.geometries.points:\\n            # Creating the point from the WKT\\n            pnt = fromstr(p.wkt)\\n            self.assertEqual(pnt.geom_type, \'Point\')\\n            self.assertEqual(pnt.geom_typeid, 0)\\n            self.assertEqual(p.x, pnt.x)\\n            self.assertEqual(p.y, pnt.y)\\n            self.assertEqual(True, pnt == fromstr(p.wkt))\\n            self.assertEqual(False, pnt == prev)\\n\\n            # Making sure that the point\'s X, Y components are what we expect\\n            self.assertAlmostEqual(p.x, pnt.tuple[0], 9)\\n            self.assertAlmostEqual(p.y, pnt.tuple[1], 9)\\n\\n            # Testing the third dimension, and getting the tuple arguments\\n            if hasattr(p, \'z\'):\\n                self.assertEqual(True, pnt.hasz)\\n                self.assertEqual(p.z, pnt.z)\\n                self.assertEqual(p.z, pnt.tuple[2], 9)\\n                tup_args = (p.x, p.y, p.z)\\n                set_tup1 = (2.71, 3.14, 5.23)\\n                set_tup2 = (5.23, 2.71, 3.14)\\n            else:\\n                self.assertEqual(False, pnt.hasz)\\n                self.assertEqual(None, pnt.z)\\n                tup_args = (p.x, p.y)\\n                set_tup1 = (2.71, 3.14)\\n                set_tup2 = (3.14, 2.71)\\n\\n            # Centroid operation on point should be point itself\\n            self.assertEqual(p.centroid, pnt.centroid.tuple)\\n\\n            # Now testing the different constructors\\n            pnt2 = Point(tup_args)  # e.g., Point((1, 2))\\n            pnt3 = Point(*tup_args) # e.g., Point(1, 2)\\n            self.assertEqual(True, pnt == pnt2)\\n            self.assertEqual(True, pnt == pnt3)\\n\\n            # Now testing setting the x and y\\n            pnt.y = 3.14\\n            pnt.x = 2.71\\n            self.assertEqual(3.14, pnt.y)\\n            self.assertEqual(2.71, pnt.x)\\n\\n            # Setting via the tuple/coords property\\n            pnt.tuple = set_tup1\\n            self.assertEqual(set_tup1, pnt.tuple)\\n            pnt.coords = set_tup2\\n            self.assertEqual(set_tup2, pnt.coords)\\n\\n            prev = pnt # setting the previous geometry\\n\\n    def test02b_multipoints(self):\\n        \\"Testing MultiPoint objects.\\"\\n        for mp in self.geometries.multipoints:\\n            mpnt = fromstr(mp.wkt)\\n            self.assertEqual(mpnt.geom_type, \'MultiPoint\')\\n            self.assertEqual(mpnt.geom_typeid, 4)\\n\\n            self.assertAlmostEqual(mp.centroid[0], mpnt.centroid.tuple[0], 9)\\n            self.assertAlmostEqual(mp.centroid[1], mpnt.centroid.tuple[1], 9)\\n\\n            self.assertRaises(GEOSIndexError, mpnt.__getitem__, len(mpnt))\\n            self.assertEqual(mp.centroid, mpnt.centroid.tuple)\\n            self.assertEqual(mp.coords, tuple(m.tuple for m in mpnt))\\n            for p in mpnt:\\n                self.assertEqual(p.geom_type, \'Point\')\\n                self.assertEqual(p.geom_typeid, 0)\\n                self.assertEqual(p.empty, False)\\n                self.assertEqual(p.valid, True)\\n\\n    def test03a_linestring(self):\\n        \\"Testing LineString objects.\\"\\n        prev = fromstr(\'POINT(0 0)\')\\n        for l in self.geometries.linestrings:\\n            ls = fromstr(l.wkt)\\n            self.assertEqual(ls.geom_type, \'LineString\')\\n            self.assertEqual(ls.geom_typeid, 1)\\n            self.assertEqual(ls.empty, False)\\n            self.assertEqual(ls.ring, False)\\n            if hasattr(l, \'centroid\'):\\n                self.assertEqual(l.centroid, ls.centroid.tuple)\\n            if hasattr(l, \'tup\'):\\n                self.assertEqual(l.tup, ls.tuple)\\n\\n            self.assertEqual(True, ls == fromstr(l.wkt))\\n            self.assertEqual(False, ls == prev)\\n            self.assertRaises(GEOSIndexError, ls.__getitem__, len(ls))\\n            prev = ls\\n\\n            # Creating a LineString from a tuple, list, and numpy array\\n            self.assertEqual(ls, LineString(ls.tuple))  # tuple\\n            self.assertEqual(ls, LineString(*ls.tuple)) # as individual arguments\\n            self.assertEqual(ls, LineString([list(tup) for tup in ls.tuple])) # as list\\n            self.assertEqual(ls.wkt, LineString(*tuple(Point(tup) for tup in ls.tuple)).wkt) # Point individual arguments\\n            if numpy: self.assertEqual(ls, LineString(numpy.array(ls.tuple))) # as numpy array\\n\\n    def test03b_multilinestring(self):\\n        \\"Testing MultiLineString objects.\\"\\n        prev = fromstr(\'POINT(0 0)\')\\n        for l in self.geometries.multilinestrings:\\n            ml = fromstr(l.wkt)\\n            self.assertEqual(ml.geom_type, \'MultiLineString\')\\n            self.assertEqual(ml.geom_typeid, 5)\\n\\n            self.assertAlmostEqual(l.centroid[0], ml.centroid.x, 9)\\n            self.assertAlmostEqual(l.centroid[1], ml.centroid.y, 9)\\n\\n            self.assertEqual(True, ml == fromstr(l.wkt))\\n            self.assertEqual(False, ml == prev)\\n            prev = ml\\n\\n            for ls in ml:\\n                self.assertEqual(ls.geom_type, \'LineString\')\\n                self.assertEqual(ls.geom_typeid, 1)\\n                self.assertEqual(ls.empty, False)\\n\\n            self.assertRaises(GEOSIndexError, ml.__getitem__, len(ml))\\n            self.assertEqual(ml.wkt, MultiLineString(*tuple(s.clone() for s in ml)).wkt)\\n            self.assertEqual(ml, MultiLineString(*tuple(LineString(s.tuple) for s in ml)))\\n\\n    def test04_linearring(self):\\n        \\"Testing LinearRing objects.\\"\\n        for rr in self.geometries.linearrings:\\n            lr = fromstr(rr.wkt)\\n            self.assertEqual(lr.geom_type, \'LinearRing\')\\n            self.assertEqual(lr.geom_typeid, 2)\\n            self.assertEqual(rr.n_p, len(lr))\\n            self.assertEqual(True, lr.valid)\\n            self.assertEqual(False, lr.empty)\\n\\n            # Creating a LinearRing from a tuple, list, and numpy array\\n            self.assertEqual(lr, LinearRing(lr.tuple))\\n            self.assertEqual(lr, LinearRing(*lr.tuple))\\n            self.assertEqual(lr, LinearRing([list(tup) for tup in lr.tuple]))\\n            if numpy: self.assertEqual(lr, LinearRing(numpy.array(lr.tuple)))\\n\\n    def test05a_polygons(self):\\n        \\"Testing Polygon objects.\\"\\n\\n        # Testing `from_bbox` class method\\n        bbox = (-180, -90, 180, 90)\\n        p = Polygon.from_bbox( bbox )\\n        self.assertEqual(bbox, p.extent)\\n\\n        prev = fromstr(\'POINT(0 0)\')\\n        for p in self.geometries.polygons:\\n            # Creating the Polygon, testing its properties.\\n            poly = fromstr(p.wkt)\\n            self.assertEqual(poly.geom_type, \'Polygon\')\\n            self.assertEqual(poly.geom_typeid, 3)\\n            self.assertEqual(poly.empty, False)\\n            self.assertEqual(poly.ring, False)\\n            self.assertEqual(p.n_i, poly.num_interior_rings)\\n            self.assertEqual(p.n_i + 1, len(poly)) # Testing __len__\\n            self.assertEqual(p.n_p, poly.num_points)\\n\\n            # Area \\u0026 Centroid\\n            self.assertAlmostEqual(p.area, poly.area, 9)\\n            self.assertAlmostEqual(p.centroid[0], poly.centroid.tuple[0], 9)\\n            self.assertAlmostEqual(p.centroid[1], poly.centroid.tuple[1], 9)\\n\\n            # Testing the geometry equivalence\\n            self.assertEqual(True, poly == fromstr(p.wkt))\\n            self.assertEqual(False, poly == prev) # Should not be equal to previous geometry\\n            self.assertEqual(True, poly != prev)\\n\\n            # Testing the exterior ring\\n            ring = poly.exterior_ring\\n            self.assertEqual(ring.geom_type, \'LinearRing\')\\n            self.assertEqual(ring.geom_typeid, 2)\\n            if p.ext_ring_cs:\\n                self.assertEqual(p.ext_ring_cs, ring.tuple)\\n                self.assertEqual(p.ext_ring_cs, poly[0].tuple) # Testing __getitem__\\n\\n            # Testing __getitem__ and __setitem__ on invalid indices\\n            self.assertRaises(GEOSIndexError, poly.__getitem__, len(poly))\\n            self.assertRaises(GEOSIndexError, poly.__setitem__, len(poly), False)\\n            self.assertRaises(GEOSIndexError, poly.__getitem__, -1 * len(poly) - 1)\\n\\n            # Testing __iter__\\n            for r in poly:\\n                self.assertEqual(r.geom_type, \'LinearRing\')\\n                self.assertEqual(r.geom_typeid, 2)\\n\\n            # Testing polygon construction.\\n            self.assertRaises(TypeError, Polygon.__init__, 0, [1, 2, 3])\\n            self.assertRaises(TypeError, Polygon.__init__, \'foo\')\\n\\n            # Polygon(shell, (hole1, ... holeN))\\n            rings = tuple(r for r in poly)\\n            self.assertEqual(poly, Polygon(rings[0], rings[1:]))\\n\\n            # Polygon(shell_tuple, hole_tuple1, ... , hole_tupleN)\\n            ring_tuples = tuple(r.tuple for r in poly)\\n            self.assertEqual(poly, Polygon(*ring_tuples))\\n\\n            # Constructing with tuples of LinearRings.\\n            self.assertEqual(poly.wkt, Polygon(*tuple(r for r in poly)).wkt)\\n            self.assertEqual(poly.wkt, Polygon(*tuple(LinearRing(r.tuple) for r in poly)).wkt)\\n\\n    def test05b_multipolygons(self):\\n        \\"Testing MultiPolygon objects.\\"\\n        print \\"\\\\nBEGIN - expecting GEOS_NOTICE; safe to ignore.\\\\n\\"\\n        prev = fromstr(\'POINT (0 0)\')\\n        for mp in self.geometries.multipolygons:\\n            mpoly = fromstr(mp.wkt)\\n            self.assertEqual(mpoly.geom_type, \'MultiPolygon\')\\n            self.assertEqual(mpoly.geom_typeid, 6)\\n            self.assertEqual(mp.valid, mpoly.valid)\\n\\n            if mp.valid:\\n                self.assertEqual(mp.num_geom, mpoly.num_geom)\\n                self.assertEqual(mp.n_p, mpoly.num_coords)\\n                self.assertEqual(mp.num_geom, len(mpoly))\\n                self.assertRaises(GEOSIndexError, mpoly.__getitem__, len(mpoly))\\n                for p in mpoly:\\n                    self.assertEqual(p.geom_type, \'Polygon\')\\n                    self.assertEqual(p.geom_typeid, 3)\\n                    self.assertEqual(p.valid, True)\\n                self.assertEqual(mpoly.wkt, MultiPolygon(*tuple(poly.clone() for poly in mpoly)).wkt)\\n\\n        print \\"\\\\nEND - expecting GEOS_NOTICE; safe to ignore.\\\\n\\"\\n\\n    def test06a_memory_hijinks(self):\\n        \\"Testing Geometry __del__() on rings and polygons.\\"\\n        #### Memory issues with rings and polygons\\n\\n        # These tests are needed to ensure sanity with writable geometries.\\n\\n        # Getting a polygon with interior rings, and pulling out the interior rings\\n        poly = fromstr(self.geometries.polygons[1].wkt)\\n        ring1 = poly[0]\\n        ring2 = poly[1]\\n\\n        # These deletes should be \'harmless\' since they are done on child geometries\\n        del ring1\\n        del ring2\\n        ring1 = poly[0]\\n        ring2 = poly[1]\\n\\n        # Deleting the polygon\\n        del poly\\n\\n        # Access to these rings is OK since they are clones.\\n        s1, s2 = str(ring1), str(ring2)\\n\\n    def test08_coord_seq(self):\\n        \\"Testing Coordinate Sequence objects.\\"\\n        for p in self.geometries.polygons:\\n            if p.ext_ring_cs:\\n                # Constructing the polygon and getting the coordinate sequence\\n                poly = fromstr(p.wkt)\\n                cs = poly.exterior_ring.coord_seq\\n\\n                self.assertEqual(p.ext_ring_cs, cs.tuple) # done in the Polygon test too.\\n                self.assertEqual(len(p.ext_ring_cs), len(cs)) # Making sure __len__ works\\n\\n                # Checks __getitem__ and __setitem__\\n                for i in xrange(len(p.ext_ring_cs)):\\n                    c1 = p.ext_ring_cs[i] # Expected value\\n                    c2 = cs[i] # Value from coordseq\\n                    self.assertEqual(c1, c2)\\n\\n                    # Constructing the test value to set the coordinate sequence with\\n                    if len(c1) == 2: tset = (5, 23)\\n                    else: tset = (5, 23, 8)\\n                    cs[i] = tset\\n\\n                    # Making sure every set point matches what we expect\\n                    for j in range(len(tset)):\\n                        cs[i] = tset\\n                        self.assertEqual(tset[j], cs[i][j])\\n\\n    def test09_relate_pattern(self):\\n        \\"Testing relate() and relate_pattern().\\"\\n        g = fromstr(\'POINT (0 0)\')\\n        self.assertRaises(GEOSException, g.relate_pattern, 0, \'invalid pattern, yo\')\\n        for rg in self.geometries.relate_geoms:\\n            a = fromstr(rg.wkt_a)\\n            b = fromstr(rg.wkt_b)\\n            self.assertEqual(rg.result, a.relate_pattern(b, rg.pattern))\\n            self.assertEqual(rg.pattern, a.relate(b))\\n\\n    def test10_intersection(self):\\n        \\"Testing intersects() and intersection().\\"\\n        for i in xrange(len(self.geometries.topology_geoms)):\\n            a = fromstr(self.geometries.topology_geoms[i].wkt_a)\\n            b = fromstr(self.geometries.topology_geoms[i].wkt_b)\\n            i1 = fromstr(self.geometries.intersect_geoms[i].wkt)\\n            self.assertEqual(True, a.intersects(b))\\n            i2 = a.intersection(b)\\n            self.assertEqual(i1, i2)\\n            self.assertEqual(i1, a \\u0026 b) # __and__ is intersection operator\\n            a \\u0026= b # testing __iand__\\n            self.assertEqual(i1, a)\\n\\n    def test11_union(self):\\n        \\"Testing union().\\"\\n        for i in xrange(len(self.geometries.topology_geoms)):\\n            a = fromstr(self.geometries.topology_geoms[i].wkt_a)\\n            b = fromstr(self.geometries.topology_geoms[i].wkt_b)\\n            u1 = fromstr(self.geometries.union_geoms[i].wkt)\\n            u2 = a.union(b)\\n            self.assertEqual(u1, u2)\\n            self.assertEqual(u1, a | b) # __or__ is union operator\\n            a |= b # testing __ior__\\n            self.assertEqual(u1, a)\\n\\n    def test12_difference(self):\\n        \\"Testing difference().\\"\\n        for i in xrange(len(self.geometries.topology_geoms)):\\n            a = fromstr(self.geometries.topology_geoms[i].wkt_a)\\n            b = fromstr(self.geometries.topology_geoms[i].wkt_b)\\n            d1 = fromstr(self.geometries.diff_geoms[i].wkt)\\n            d2 = a.difference(b)\\n            self.assertEqual(d1, d2)\\n            self.assertEqual(d1, a - b) # __sub__ is difference operator\\n            a -= b # testing __isub__\\n            self.assertEqual(d1, a)\\n\\n    def test13_symdifference(self):\\n        \\"Testing sym_difference().\\"\\n        for i in xrange(len(self.geometries.topology_geoms)):\\n            a = fromstr(self.geometries.topology_geoms[i].wkt_a)\\n            b = fromstr(self.geometries.topology_geoms[i].wkt_b)\\n            d1 = fromstr(self.geometries.sdiff_geoms[i].wkt)\\n            d2 = a.sym_difference(b)\\n            self.assertEqual(d1, d2)\\n            self.assertEqual(d1, a ^ b) # __xor__ is symmetric difference operator\\n            a ^= b # testing __ixor__\\n            self.assertEqual(d1, a)\\n\\n    def test14_buffer(self):\\n        \\"Testing buffer().\\"\\n        for bg in self.geometries.buffer_geoms:\\n            g = fromstr(bg.wkt)\\n\\n            # The buffer we expect\\n            exp_buf = fromstr(bg.buffer_wkt)\\n            quadsegs = bg.quadsegs\\n            width = bg.width\\n\\n            # Can\'t use a floating-point for the number of quadsegs.\\n            self.assertRaises(ctypes.ArgumentError, g.buffer, width, float(quadsegs))\\n\\n            # Constructing our buffer\\n            buf = g.buffer(width, quadsegs)\\n            self.assertEqual(exp_buf.num_coords, buf.num_coords)\\n            self.assertEqual(len(exp_buf), len(buf))\\n\\n            # Now assuring that each point in the buffer is almost equal\\n            for j in xrange(len(exp_buf)):\\n                exp_ring = exp_buf[j]\\n                buf_ring = buf[j]\\n                self.assertEqual(len(exp_ring), len(buf_ring))\\n                for k in xrange(len(exp_ring)):\\n                    # Asserting the X, Y of each point are almost equal (due to floating point imprecision)\\n                    self.assertAlmostEqual(exp_ring[k][0], buf_ring[k][0], 9)\\n                    self.assertAlmostEqual(exp_ring[k][1], buf_ring[k][1], 9)\\n\\n    def test15_srid(self):\\n        \\"Testing the SRID property and keyword.\\"\\n        # Testing SRID keyword on Point\\n        pnt = Point(5, 23, srid=4326)\\n        self.assertEqual(4326, pnt.srid)\\n        pnt.srid = 3084\\n        self.assertEqual(3084, pnt.srid)\\n        self.assertRaises(ctypes.ArgumentError, pnt.set_srid, \'4326\')\\n\\n        # Testing SRID keyword on fromstr(), and on Polygon rings.\\n        poly = fromstr(self.geometries.polygons[1].wkt, srid=4269)\\n        self.assertEqual(4269, poly.srid)\\n        for ring in poly: self.assertEqual(4269, ring.srid)\\n        poly.srid = 4326\\n        self.assertEqual(4326, poly.shell.srid)\\n\\n        # Testing SRID keyword on GeometryCollection\\n        gc = GeometryCollection(Point(5, 23), LineString((0, 0), (1.5, 1.5), (3, 3)), srid=32021)\\n        self.assertEqual(32021, gc.srid)\\n        for i in range(len(gc)): self.assertEqual(32021, gc[i].srid)\\n\\n        # GEOS may get the SRID from HEXEWKB\\n        # \'POINT(5 23)\' at SRID=4326 in hex form -- obtained from PostGIS\\n        # using `SELECT GeomFromText(\'POINT (5 23)\', 4326);`.\\n        hex = \'0101000020E610000000000000000014400000000000003740\'\\n        p1 = fromstr(hex)\\n        self.assertEqual(4326, p1.srid)\\n\\n        # In GEOS 3.0.0rc1-4  when the EWKB and/or HEXEWKB is exported,\\n        # the SRID information is lost and set to -1 -- this is not a\\n        # problem on the 3.0.0 version (another reason to upgrade).\\n        exp_srid = self.null_srid\\n\\n        p2 = fromstr(p1.hex)\\n        self.assertEqual(exp_srid, p2.srid)\\n        p3 = fromstr(p1.hex, srid=-1) # -1 is intended.\\n        self.assertEqual(-1, p3.srid)\\n\\n    def test16_mutable_geometries(self):\\n        \\"Testing the mutability of Polygons and Geometry Collections.\\"\\n        ### Testing the mutability of Polygons ###\\n        for p in self.geometries.polygons:\\n            poly = fromstr(p.wkt)\\n\\n            # Should only be able to use __setitem__ with LinearRing geometries.\\n            self.assertRaises(TypeError, poly.__setitem__, 0, LineString((1, 1), (2, 2)))\\n\\n            # Constructing the new shell by adding 500 to every point in the old shell.\\n            shell_tup = poly.shell.tuple\\n            new_coords = []\\n            for point in shell_tup: new_coords.append((point[0] + 500., point[1] + 500.))\\n            new_shell = LinearRing(*tuple(new_coords))\\n\\n            # Assigning polygon\'s exterior ring w/the new shell\\n            poly.exterior_ring = new_shell\\n            s = str(new_shell) # new shell is still accessible\\n            self.assertEqual(poly.exterior_ring, new_shell)\\n            self.assertEqual(poly[0], new_shell)\\n\\n        ### Testing the mutability of Geometry Collections\\n        for tg in self.geometries.multipoints:\\n            mp = fromstr(tg.wkt)\\n            for i in range(len(mp)):\\n                # Creating a random point.\\n                pnt = mp[i]\\n                new = Point(random.randint(1, 100), random.randint(1, 100))\\n                # Testing the assignment\\n                mp[i] = new\\n                s = str(new) # what was used for the assignment is still accessible\\n                self.assertEqual(mp[i], new)\\n                self.assertEqual(mp[i].wkt, new.wkt)\\n                self.assertNotEqual(pnt, mp[i])\\n\\n        # MultiPolygons involve much more memory management because each\\n        # Polygon w/in the collection has its own rings.\\n        for tg in self.geometries.multipolygons:\\n            mpoly = fromstr(tg.wkt)\\n            for i in xrange(len(mpoly)):\\n                poly = mpoly[i]\\n                old_poly = mpoly[i]\\n                # Offsetting the each ring in the polygon by 500.\\n                for j in xrange(len(poly)):\\n                    r = poly[j]\\n                    for k in xrange(len(r)): r[k] = (r[k][0] + 500., r[k][1] + 500.)\\n                    poly[j] = r\\n\\n                self.assertNotEqual(mpoly[i], poly)\\n                # Testing the assignment\\n                mpoly[i] = poly\\n                s = str(poly) # Still accessible\\n                self.assertEqual(mpoly[i], poly)\\n                self.assertNotEqual(mpoly[i], old_poly)\\n\\n        # Extreme (!!) __setitem__ -- no longer works, have to detect\\n        # in the first object that __setitem__ is called in the subsequent\\n        # objects -- maybe mpoly[0, 0, 0] = (3.14, 2.71)?\\n        #mpoly[0][0][0] = (3.14, 2.71)\\n        #self.assertEqual((3.14, 2.71), mpoly[0][0][0])\\n        # Doing it more slowly..\\n        #self.assertEqual((3.14, 2.71), mpoly[0].shell[0])\\n        #del mpoly\\n\\n    def test17_threed(self):\\n        \\"Testing three-dimensional geometries.\\"\\n        # Testing a 3D Point\\n        pnt = Point(2, 3, 8)\\n        self.assertEqual((2.,3.,8.), pnt.coords)\\n        self.assertRaises(TypeError, pnt.set_coords, (1.,2.))\\n        pnt.coords = (1.,2.,3.)\\n        self.assertEqual((1.,2.,3.), pnt.coords)\\n\\n        # Testing a 3D LineString\\n        ls = LineString((2., 3., 8.), (50., 250., -117.))\\n        self.assertEqual(((2.,3.,8.), (50.,250.,-117.)), ls.tuple)\\n        self.assertRaises(TypeError, ls.__setitem__, 0, (1.,2.))\\n        ls[0] = (1.,2.,3.)\\n        self.assertEqual((1.,2.,3.), ls[0])\\n\\n    def test18_distance(self):\\n        \\"Testing the distance() function.\\"\\n        # Distance to self should be 0.\\n        pnt = Point(0, 0)\\n        self.assertEqual(0.0, pnt.distance(Point(0, 0)))\\n\\n        # Distance should be 1\\n        self.assertEqual(1.0, pnt.distance(Point(0, 1)))\\n\\n        # Distance should be ~ sqrt(2)\\n        self.assertAlmostEqual(1.41421356237, pnt.distance(Point(1, 1)), 11)\\n\\n        # Distances are from the closest vertex in each geometry --\\n        #  should be 3 (distance from (2, 2) to (5, 2)).\\n        ls1 = LineString((0, 0), (1, 1), (2, 2))\\n        ls2 = LineString((5, 2), (6, 1), (7, 0))\\n        self.assertEqual(3, ls1.distance(ls2))\\n\\n    def test19_length(self):\\n        \\"Testing the length property.\\"\\n        # Points have 0 length.\\n        pnt = Point(0, 0)\\n        self.assertEqual(0.0, pnt.length)\\n\\n        # Should be ~ sqrt(2)\\n        ls = LineString((0, 0), (1, 1))\\n        self.assertAlmostEqual(1.41421356237, ls.length, 11)\\n\\n        # Should be circumfrence of Polygon\\n        poly = Polygon(LinearRing((0, 0), (0, 1), (1, 1), (1, 0), (0, 0)))\\n        self.assertEqual(4.0, poly.length)\\n\\n        # Should be sum of each element\'s length in collection.\\n        mpoly = MultiPolygon(poly.clone(), poly)\\n        self.assertEqual(8.0, mpoly.length)\\n\\n    def test20a_emptyCollections(self):\\n        \\"Testing empty geometries and collections.\\"\\n        gc1 = GeometryCollection([])\\n        gc2 = fromstr(\'GEOMETRYCOLLECTION EMPTY\')\\n        pnt = fromstr(\'POINT EMPTY\')\\n        ls = fromstr(\'LINESTRING EMPTY\')\\n        poly = fromstr(\'POLYGON EMPTY\')\\n        mls = fromstr(\'MULTILINESTRING EMPTY\')\\n        mpoly1 = fromstr(\'MULTIPOLYGON EMPTY\')\\n        mpoly2 = MultiPolygon(())\\n\\n        for g in [gc1, gc2, pnt, ls, poly, mls, mpoly1, mpoly2]:\\n            self.assertEqual(True, g.empty)\\n\\n            # Testing len() and num_geom.\\n            if isinstance(g, Polygon):\\n                self.assertEqual(1, len(g)) # Has one empty linear ring\\n                self.assertEqual(1, g.num_geom)\\n                self.assertEqual(0, len(g[0]))\\n            elif isinstance(g, (Point, LineString)):\\n                self.assertEqual(1, g.num_geom)\\n                self.assertEqual(0, len(g))\\n            else:\\n                self.assertEqual(0, g.num_geom)\\n                self.assertEqual(0, len(g))\\n\\n            # Testing __getitem__ (doesn\'t work on Point or Polygon)\\n            if isinstance(g, Point):\\n                self.assertRaises(GEOSIndexError, g.get_x)\\n            elif isinstance(g, Polygon):\\n                lr = g.shell\\n                self.assertEqual(\'LINEARRING EMPTY\', lr.wkt)\\n                self.assertEqual(0, len(lr))\\n                self.assertEqual(True, lr.empty)\\n                self.assertRaises(GEOSIndexError, lr.__getitem__, 0)\\n            else:\\n                self.assertRaises(GEOSIndexError, g.__getitem__, 0)\\n\\n    def test20b_collections_of_collections(self):\\n        \\"Testing GeometryCollection handling of other collections.\\"\\n        # Creating a GeometryCollection WKT string composed of other\\n        # collections and polygons.\\n        coll = [mp.wkt for mp in self.geometries.multipolygons if mp.valid]\\n        coll.extend([mls.wkt for mls in self.geometries.multilinestrings])\\n        coll.extend([p.wkt for p in self.geometries.polygons])\\n        coll.extend([mp.wkt for mp in self.geometries.multipoints])\\n        gc_wkt = \'GEOMETRYCOLLECTION(%s)\' % \',\'.join(coll)\\n\\n        # Should construct ok from WKT\\n        gc1 = GEOSGeometry(gc_wkt)\\n\\n        # Should also construct ok from individual geometry arguments.\\n        gc2 = GeometryCollection(*tuple(g for g in gc1))\\n\\n        # And, they should be equal.\\n        self.assertEqual(gc1, gc2)\\n\\n    def test21_test_gdal(self):\\n        \\"Testing `ogr` and `srs` properties.\\"\\n        if not gdal.HAS_GDAL: return\\n        g1 = fromstr(\'POINT(5 23)\')\\n        self.assertEqual(True, isinstance(g1.ogr, gdal.OGRGeometry))\\n        self.assertEqual(g1.srs, None)\\n\\n        g2 = fromstr(\'LINESTRING(0 0, 5 5, 23 23)\', srid=4326)\\n        self.assertEqual(True, isinstance(g2.ogr, gdal.OGRGeometry))\\n        self.assertEqual(True, isinstance(g2.srs, gdal.SpatialReference))\\n        self.assertEqual(g2.hex, g2.ogr.hex)\\n        self.assertEqual(\'WGS 84\', g2.srs.name)\\n\\n    def test22_copy(self):\\n        \\"Testing use with the Python `copy` module.\\"\\n        import django.utils.copycompat as copy\\n        poly = GEOSGeometry(\'POLYGON((0 0, 0 23, 23 23, 23 0, 0 0), (5 5, 5 10, 10 10, 10 5, 5 5))\')\\n        cpy1 = copy.copy(poly)\\n        cpy2 = copy.deepcopy(poly)\\n        self.assertNotEqual(poly._ptr, cpy1._ptr)\\n        self.assertNotEqual(poly._ptr, cpy2._ptr)\\n\\n    def test23_transform(self):\\n        \\"Testing `transform` method.\\"\\n        if not gdal.HAS_GDAL: return\\n        orig = GEOSGeometry(\'POINT (-104.609 38.255)\', 4326)\\n        trans = GEOSGeometry(\'POINT (992385.4472045 481455.4944650)\', 2774)\\n\\n        # Using a srid, a SpatialReference object, and a CoordTransform object\\n        # for transformations.\\n        t1, t2, t3 = orig.clone(), orig.clone(), orig.clone()\\n        t1.transform(trans.srid)\\n        t2.transform(gdal.SpatialReference(\'EPSG:2774\'))\\n        ct = gdal.CoordTransform(gdal.SpatialReference(\'WGS84\'), gdal.SpatialReference(2774))\\n        t3.transform(ct)\\n\\n        # Testing use of the `clone` keyword.\\n        k1 = orig.clone()\\n        k2 = k1.transform(trans.srid, clone=True)\\n        self.assertEqual(k1, orig)\\n        self.assertNotEqual(k1, k2)\\n\\n        prec = 3\\n        for p in (t1, t2, t3, k2):\\n            self.assertAlmostEqual(trans.x, p.x, prec)\\n            self.assertAlmostEqual(trans.y, p.y, prec)\\n\\n    def test23_transform_noop(self):\\n        \\"\\"\\" Testing `transform` method (SRID match) \\"\\"\\"\\n        # transform() should no-op if source \\u0026 dest SRIDs match,\\n        # regardless of whether GDAL is available.\\n        if gdal.HAS_GDAL:\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', 4326)\\n            gt = g.tuple\\n            g.transform(4326)\\n            self.assertEqual(g.tuple, gt)\\n            self.assertEqual(g.srid, 4326)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', 4326)\\n            g1 = g.transform(4326, clone=True)\\n            self.assertEqual(g1.tuple, g.tuple)\\n            self.assertEqual(g1.srid, 4326)\\n            self.assert_(g1 is not g, \\"Clone didn\'t happen\\")\\n\\n        old_has_gdal = gdal.HAS_GDAL\\n        try:\\n            gdal.HAS_GDAL = False\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', 4326)\\n            gt = g.tuple\\n            g.transform(4326)\\n            self.assertEqual(g.tuple, gt)\\n            self.assertEqual(g.srid, 4326)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', 4326)\\n            g1 = g.transform(4326, clone=True)\\n            self.assertEqual(g1.tuple, g.tuple)\\n            self.assertEqual(g1.srid, 4326)\\n            self.assert_(g1 is not g, \\"Clone didn\'t happen\\")\\n        finally:\\n            gdal.HAS_GDAL = old_has_gdal\\n\\n    def test23_transform_nosrid(self):\\n        \\"\\"\\" Testing `transform` method (no SRID) \\"\\"\\"\\n        # raise a warning if SRID \\u003c0/None\\n        import warnings\\n        print \\"\\\\nBEGIN - expecting Warnings; safe to ignore.\\\\n\\"\\n\\n        # test for do-nothing behaviour.\\n        try:\\n            # Keeping line-noise down by only printing the relevant\\n            # warnings once.\\n            warnings.simplefilter(\'once\', UserWarning)\\n            warnings.simplefilter(\'once\', FutureWarning)    \\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', srid=None)\\n            g.transform(2774)\\n            self.assertEqual(g.tuple, (-104.609, 38.255))\\n            self.assertEqual(g.srid, None)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', srid=None)\\n            g1 = g.transform(2774, clone=True)\\n            self.assert_(g1 is None)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', srid=-1)\\n            g.transform(2774)\\n            self.assertEqual(g.tuple, (-104.609, 38.255))\\n            self.assertEqual(g.srid, -1)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', srid=-1)\\n            g1 = g.transform(2774, clone=True)\\n            self.assert_(g1 is None)\\n\\n        finally:\\n            warnings.simplefilter(\'default\', UserWarning)\\n            warnings.simplefilter(\'default\', FutureWarning)\\n\\n        print \\"\\\\nEND - expecting Warnings; safe to ignore.\\\\n\\"\\n\\n\\n        # test warning is raised\\n        try:\\n            warnings.simplefilter(\'error\', FutureWarning)\\n            warnings.simplefilter(\'ignore\', UserWarning)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', srid=None)\\n            self.assertRaises(FutureWarning, g.transform, 2774)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', srid=None)\\n            self.assertRaises(FutureWarning, g.transform, 2774, clone=True)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', srid=-1)\\n            self.assertRaises(FutureWarning, g.transform, 2774)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', srid=-1)\\n            self.assertRaises(FutureWarning, g.transform, 2774, clone=True)\\n        finally:\\n            warnings.simplefilter(\'default\', FutureWarning)\\n            warnings.simplefilter(\'default\', UserWarning)\\n\\n\\n    def test23_transform_nogdal(self):\\n        \\"\\"\\" Testing `transform` method (GDAL not available) \\"\\"\\"\\n        old_has_gdal = gdal.HAS_GDAL\\n        try:\\n            gdal.HAS_GDAL = False\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', 4326)\\n            self.assertRaises(GEOSException, g.transform, 2774)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', 4326)\\n            self.assertRaises(GEOSException, g.transform, 2774, clone=True)\\n        finally:\\n            gdal.HAS_GDAL = old_has_gdal\\n\\n    def test24_extent(self):\\n        \\"Testing `extent` method.\\"\\n        # The xmin, ymin, xmax, ymax of the MultiPoint should be returned.\\n        mp = MultiPoint(Point(5, 23), Point(0, 0), Point(10, 50))\\n        self.assertEqual((0.0, 0.0, 10.0, 50.0), mp.extent)\\n        pnt = Point(5.23, 17.8)\\n        # Extent of points is just the point itself repeated.\\n        self.assertEqual((5.23, 17.8, 5.23, 17.8), pnt.extent)\\n        # Testing on the \'real world\' Polygon.\\n        poly = fromstr(self.geometries.polygons[3].wkt)\\n        ring = poly.shell\\n        x, y = ring.x, ring.y\\n        xmin, ymin = min(x), min(y)\\n        xmax, ymax = max(x), max(y)\\n        self.assertEqual((xmin, ymin, xmax, ymax), poly.extent)\\n\\n    def test25_pickle(self):\\n        \\"Testing pickling and unpickling support.\\"\\n        # Using both pickle and cPickle -- just \'cause.\\n        import pickle, cPickle\\n\\n        # Creating a list of test geometries for pickling,\\n        # and setting the SRID on some of them.\\n        def get_geoms(lst, srid=None):\\n            return [GEOSGeometry(tg.wkt, srid) for tg in lst]\\n        tgeoms = get_geoms(self.geometries.points)\\n        tgeoms.extend(get_geoms(self.geometries.multilinestrings, 4326))\\n        tgeoms.extend(get_geoms(self.geometries.polygons, 3084))\\n        tgeoms.extend(get_geoms(self.geometries.multipolygons, 900913))\\n\\n        # The SRID won\'t be exported in GEOS 3.0 release candidates.\\n        no_srid = self.null_srid == -1\\n        for geom in tgeoms:\\n            s1, s2 = cPickle.dumps(geom), pickle.dumps(geom)\\n            g1, g2 = cPickle.loads(s1), pickle.loads(s2)\\n            for tmpg in (g1, g2):\\n                self.assertEqual(geom, tmpg)\\n                if not no_srid: self.assertEqual(geom.srid, tmpg.srid)\\n\\n    def test26_prepared(self):\\n        \\"Testing PreparedGeometry support.\\"\\n        if not GEOS_PREPARE: return\\n        # Creating a simple multipolygon and getting a prepared version.\\n        mpoly = GEOSGeometry(\'MULTIPOLYGON(((0 0,0 5,5 5,5 0,0 0)),((5 5,5 10,10 10,10 5,5 5)))\')\\n        prep = mpoly.prepared\\n\\n        # A set of test points.\\n        pnts = [Point(5, 5), Point(7.5, 7.5), Point(2.5, 7.5)]\\n        covers = [True, True, False] # No `covers` op for regular GEOS geoms.\\n        for pnt, c in zip(pnts, covers):\\n            # Results should be the same (but faster)\\n            self.assertEqual(mpoly.contains(pnt), prep.contains(pnt))\\n            self.assertEqual(mpoly.intersects(pnt), prep.intersects(pnt))\\n            self.assertEqual(c, prep.covers(pnt))\\n\\n    def test26_line_merge(self):\\n        \\"Testing line merge support\\"\\n        ref_geoms = (fromstr(\'LINESTRING(1 1, 1 1, 3 3)\'),\\n                     fromstr(\'MULTILINESTRING((1 1, 3 3), (3 3, 4 2))\'),\\n                     )\\n        ref_merged = (fromstr(\'LINESTRING(1 1, 3 3)\'),\\n                      fromstr(\'LINESTRING (1 1, 3 3, 4 2)\'),\\n                      )\\n        for geom, merged in zip(ref_geoms, ref_merged):\\n            self.assertEqual(merged, geom.merged)\\n\\n    def test27_valid_reason(self):\\n        \\"Testing IsValidReason support\\"\\n        # Skipping tests if GEOS \\u003c v3.1.\\n        if not GEOS_PREPARE: return\\n\\n        g = GEOSGeometry(\\"POINT(0 0)\\")\\n        self.assert_(g.valid)\\n        self.assert_(isinstance(g.valid_reason, basestring))\\n        self.assertEqual(g.valid_reason, \\"Valid Geometry\\")\\n\\n        print \\"\\\\nBEGIN - expecting GEOS_NOTICE; safe to ignore.\\\\n\\"\\n\\n        g = GEOSGeometry(\\"LINESTRING(0 0, 0 0)\\")\\n\\n        self.assert_(not g.valid)\\n        self.assert_(isinstance(g.valid_reason, basestring))\\n        self.assert_(g.valid_reason.startswith(\\"Too few points in geometry component\\"))\\n\\n        print \\"\\\\nEND - expecting GEOS_NOTICE; safe to ignore.\\\\n\\"\\n\\ndef suite():\\n    s = unittest.TestSuite()\\n    s.addTest(unittest.makeSuite(GEOSTest))\\n    return s\\n\\ndef run(verbosity=2):\\n    unittest.TextTestRunner(verbosity=verbosity).run(suite())\\n"}\n'
line: b'{"repo_name":"arifgursel/pyglet","ref":"refs/heads/master","path":"pyglet/gl/lib.py","content":"# ----------------------------------------------------------------------------\\n# pyglet\\n# Copyright (c) 2006-2008 Alex Holkner\\n# All rights reserved.\\n# \\n# Redistribution and use in source and binary forms, with or without\\n# modification, are permitted provided that the following conditions \\n# are met:\\n#\\n#  * Redistributions of source code must retain the above copyright\\n#    notice, this list of conditions and the following disclaimer.\\n#  * Redistributions in binary form must reproduce the above copyright \\n#    notice, this list of conditions and the following disclaimer in\\n#    the documentation and/or other materials provided with the\\n#    distribution.\\n#  * Neither the name of pyglet nor the names of its\\n#    contributors may be used to endorse or promote products\\n#    derived from this software without specific prior written\\n#    permission.\\n#\\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\\n# \\"AS IS\\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS\\n# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE\\n# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\\n# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\\n# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\n# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN\\n# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\\n# POSSIBILITY OF SUCH DAMAGE.\\n# ----------------------------------------------------------------------------\\n\\n\'\'\'\\n\'\'\'\\n\\n__docformat__ = \'restructuredtext\'\\n__version__ = \'$Id$\'\\n\\nimport ctypes\\n\\nimport pyglet\\n\\n__all__ = [\'link_GL\', \'link_GLU\', \'link_AGL\', \'link_GLX\', \'link_WGL\']\\n\\n_debug_gl = pyglet.options[\'debug_gl\']\\n_debug_gl_trace = pyglet.options[\'debug_gl_trace\']\\n_debug_gl_trace_args = pyglet.options[\'debug_gl_trace_args\']\\n\\nclass MissingFunctionException(Exception):\\n    def __init__(self, name, requires=None, suggestions=None):\\n        msg = \'%s is not exported by the available OpenGL driver.\' % name\\n        if requires:\\n            msg += \'  %s is required for this functionality.\' % requires\\n        if suggestions:\\n            msg += \'  Consider alternative(s) %s.\' % \', \'.join(suggestions)\\n        Exception.__init__(self, msg)\\n\\ndef missing_function(name, requires=None, suggestions=None):\\n    def MissingFunction(*args, **kwargs):\\n        raise MissingFunctionException(name, requires, suggestions)\\n    return MissingFunction\\n\\n_int_types = (ctypes.c_int16, ctypes.c_int32)\\nif hasattr(ctypes, \'c_int64\'):\\n    # Some builds of ctypes apparently do not have c_int64\\n    # defined; it\'s a pretty good bet that these builds do not\\n    # have 64-bit pointers.\\n    _int_types += (ctypes.c_int64,)\\nfor t in _int_types:\\n    if ctypes.sizeof(t) == ctypes.sizeof(ctypes.c_size_t):\\n        c_ptrdiff_t = t\\n\\nclass c_void(ctypes.Structure):\\n    # c_void_p is a buggy return type, converting to int, so\\n    # POINTER(None) == c_void_p is actually written as\\n    # POINTER(c_void), so it can be treated as a real pointer.\\n    _fields_ = [(\'dummy\', ctypes.c_int)]\\n\\nclass GLException(Exception):\\n    pass\\n\\ndef errcheck(result, func, arguments):\\n    if _debug_gl_trace:\\n        try:\\n            name = func.__name__\\n        except AttributeError:\\n            name = repr(func)\\n        if _debug_gl_trace_args:\\n            trace_args = \', \'.join([repr(arg)[:20] for arg in arguments])\\n            print \'%s(%s)\' % (name, trace_args)\\n        else:\\n            print name\\n\\n    from pyglet import gl\\n    context = gl.current_context\\n    if not context:\\n        raise GLException(\'No GL context; create a Window first\')\\n    if not context._gl_begin:\\n        error = gl.glGetError()\\n        if error:\\n            msg = ctypes.cast(gl.gluErrorString(error), ctypes.c_char_p).value\\n            raise GLException(msg)\\n        return result\\n\\ndef errcheck_glbegin(result, func, arguments):\\n    from pyglet import gl\\n    context = gl.current_context\\n    if not context:\\n        raise GLException(\'No GL context; create a Window first\')\\n    context._gl_begin = True\\n    return result\\n\\ndef errcheck_glend(result, func, arguments):\\n    from pyglet import gl\\n    context = gl.current_context\\n    if not context:\\n        raise GLException(\'No GL context; create a Window first\')\\n    context._gl_begin = False\\n    return errcheck(result, func, arguments)\\n\\ndef decorate_function(func, name):\\n    if _debug_gl:\\n        if name == \'glBegin\':\\n            func.errcheck = errcheck_glbegin\\n        elif name == \'glEnd\':\\n            func.errcheck = errcheck_glend\\n        elif name not in (\'glGetError\', \'gluErrorString\') and \\\\\\n             name[:3] not in (\'glX\', \'agl\', \'wgl\'):\\n            func.errcheck = errcheck\\n\\nlink_AGL = None\\nlink_GLX = None\\nlink_WGL = None\\n\\nif pyglet.compat_platform in (\'win32\', \'cygwin\'):\\n    from pyglet.gl.lib_wgl import link_GL, link_GLU, link_WGL\\nelif pyglet.compat_platform == \'darwin\':\\n    from pyglet.gl.lib_agl import link_GL, link_GLU, link_AGL\\nelse:\\n    from pyglet.gl.lib_glx import link_GL, link_GLU, link_GLX\\n\\n"}\n'
line: b'{"repo_name":"tangyiyong/odoo","ref":"refs/heads/8.0","path":"addons/mrp/wizard/stock_move.py","content":"# -*- coding: utf-8 -*-\\n##############################################################################\\n#\\n#    OpenERP, Open Source Management Solution\\n#    Copyright (C) 2004-2010 Tiny SPRL (\\u003chttp://tiny.be\\u003e).\\n#\\n#    This program is free software: you can redistribute it and/or modify\\n#    it under the terms of the GNU Affero General Public License as\\n#    published by the Free Software Foundation, either version 3 of the\\n#    License, or (at your option) any later version.\\n#\\n#    This program is distributed in the hope that it will be useful,\\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#    GNU Affero General Public License for more details.\\n#\\n#    You should have received a copy of the GNU Affero General Public License\\n#    along with this program.  If not, see \\u003chttp://www.gnu.org/licenses/\\u003e.\\n#\\n##############################################################################\\n\\nfrom openerp.osv import fields, osv\\nfrom openerp.tools import float_compare\\nfrom openerp.tools.translate import _\\nimport openerp.addons.decimal_precision as dp\\n\\nclass stock_move_consume(osv.osv_memory):\\n    _name = \\"stock.move.consume\\"\\n    _description = \\"Consume Products\\"\\n\\n    _columns = {\\n        \'product_id\': fields.many2one(\'product.product\', \'Product\', required=True, select=True),\\n        \'product_qty\': fields.float(\'Quantity\', digits_compute=dp.get_precision(\'Product Unit of Measure\'), required=True),\\n        \'product_uom\': fields.many2one(\'product.uom\', \'Product Unit of Measure\', required=True),\\n        \'location_id\': fields.many2one(\'stock.location\', \'Location\', required=True),\\n        \'restrict_lot_id\': fields.many2one(\'stock.production.lot\', \'Lot\'),\\n    }\\n\\n    #TOFIX: product_uom should not have different category of default UOM of product. Qty should be convert into UOM of original move line before going in consume and scrap\\n    def default_get(self, cr, uid, fields, context=None):\\n        if context is None:\\n            context = {}\\n        res = super(stock_move_consume, self).default_get(cr, uid, fields, context=context)\\n        move = self.pool.get(\'stock.move\').browse(cr, uid, context[\'active_id\'], context=context)\\n        if \'product_id\' in fields:\\n            res.update({\'product_id\': move.product_id.id})\\n        if \'product_uom\' in fields:\\n            res.update({\'product_uom\': move.product_uom.id})\\n        if \'product_qty\' in fields:\\n            res.update({\'product_qty\': move.product_uom_qty})\\n        if \'location_id\' in fields:\\n            res.update({\'location_id\': move.location_id.id})\\n        return res\\n\\n\\n\\n    def do_move_consume(self, cr, uid, ids, context=None):\\n        if context is None:\\n            context = {}\\n        move_obj = self.pool.get(\'stock.move\')\\n        uom_obj = self.pool.get(\'product.uom\')\\n        production_obj = self.pool.get(\'mrp.production\')\\n        move_ids = context[\'active_ids\']\\n        move = move_obj.browse(cr, uid, move_ids[0], context=context)\\n        production_id = move.raw_material_production_id.id\\n        production = production_obj.browse(cr, uid, production_id, context=context)\\n        precision = self.pool[\'decimal.precision\'].precision_get(cr, uid, \'Product Unit of Measure\')\\n\\n        for data in self.browse(cr, uid, ids, context=context):\\n            qty = uom_obj._compute_qty(cr, uid, data[\'product_uom\'].id, data.product_qty, data.product_id.uom_id.id)\\n            remaining_qty = move.product_qty - qty\\n            #check for product quantity is less than previously planned\\n            if float_compare(remaining_qty, 0, precision_digits=precision) \\u003e= 0:\\n                move_obj.action_consume(cr, uid, move_ids, qty, data.location_id.id, restrict_lot_id=data.restrict_lot_id.id, context=context)\\n            else:\\n                consumed_qty = min(move.product_qty, qty)\\n                new_moves = move_obj.action_consume(cr, uid, move_ids, consumed_qty, data.location_id.id, restrict_lot_id=data.restrict_lot_id.id, context=context)\\n                #consumed more in wizard than previously planned\\n                extra_more_qty = qty - consumed_qty\\n                #create new line for a remaining qty of the product\\n                extra_move_id = production_obj._make_consume_line_from_data(cr, uid, production, data.product_id, data.product_id.uom_id.id, extra_more_qty, False, 0, context=context)\\n                move_obj.write(cr, uid, [extra_move_id], {\'restrict_lot_id\': data.restrict_lot_id.id}, context=context)\\n                move_obj.action_done(cr, uid, [extra_move_id], context=context)\\n\\n        return {\'type\': \'ir.actions.act_window_close\'}"}\n'
line: b'{"repo_name":"txemi/ansible","ref":"refs/heads/devel","path":"test/units/parsing/yaml/test_dumper.py","content":"# coding: utf-8\\n# This file is part of Ansible\\n#\\n# Ansible is free software: you can redistribute it and/or modify\\n# it under the terms of the GNU General Public License as published by\\n# the Free Software Foundation, either version 3 of the License, or\\n# (at your option) any later version.\\n#\\n# Ansible is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU General Public License for more details.\\n#\\n# You should have received a copy of the GNU General Public License\\n# along with Ansible.  If not, see \\u003chttp://www.gnu.org/licenses/\\u003e.\\n\\n# Make coding more python3-ish\\nfrom __future__ import (absolute_import, division, print_function)\\n__metaclass__ = type\\n\\nimport io\\nimport yaml\\n\\ntry:\\n    from _yaml import ParserError\\nexcept ImportError:\\n    from yaml.parser import ParserError\\n\\nfrom ansible.parsing.yaml import dumper\\nfrom ansible.parsing.yaml.loader import AnsibleLoader\\n\\nfrom ansible.compat.tests import unittest\\nfrom ansible.parsing.yaml import objects\\nfrom ansible.parsing import vault\\n\\nfrom units.mock.yaml_helper import YamlTestUtils\\n\\nclass TestAnsibleDumper(unittest.TestCase, YamlTestUtils):\\n    def setUp(self):\\n        self.vault_password = \\"hunter42\\"\\n        self.good_vault = vault.VaultLib(self.vault_password)\\n        self.vault = self.good_vault\\n        self.stream = self._build_stream()\\n        self.dumper = dumper.AnsibleDumper\\n\\n    def _build_stream(self,yaml_text=None):\\n        text = yaml_text or u\'\'\\n        stream = io.StringIO(text)\\n        return stream\\n\\n    def _loader(self, stream):\\n        return AnsibleLoader(stream, vault_password=self.vault_password)\\n\\n    def test(self):\\n        plaintext = \'This is a string we are going to encrypt.\'\\n        avu = objects.AnsibleVaultEncryptedUnicode.from_plaintext(plaintext, vault=self.vault)\\n\\n        yaml_out = self._dump_string(avu, dumper=self.dumper)\\n        stream = self._build_stream(yaml_out)\\n        loader = self._loader(stream)\\n\\n        data_from_yaml = loader.get_single_data()\\n\\n        self.assertEquals(plaintext, data_from_yaml.data)\\n"}\n'
line: b'{"repo_name":"rjschwei/azure-sdk-for-python","ref":"refs/heads/master","path":"azure-mgmt-logic/azure/mgmt/logic/models/generate_upgraded_definition_parameters.py","content":"# coding=utf-8\\n# --------------------------------------------------------------------------\\n# Copyright (c) Microsoft Corporation. All rights reserved.\\n# Licensed under the MIT License. See License.txt in the project root for\\n# license information.\\n#\\n# Code generated by Microsoft (R) AutoRest Code Generator.\\n# Changes may cause incorrect behavior and will be lost if the code is\\n# regenerated.\\n# --------------------------------------------------------------------------\\n\\nfrom msrest.serialization import Model\\n\\n\\nclass GenerateUpgradedDefinitionParameters(Model):\\n    \\"\\"\\"GenerateUpgradedDefinitionParameters.\\n\\n    :param target_schema_version: The target schema version.\\n    :type target_schema_version: str\\n    \\"\\"\\" \\n\\n    _attribute_map = {\\n        \'target_schema_version\': {\'key\': \'targetSchemaVersion\', \'type\': \'str\'},\\n    }\\n\\n    def __init__(self, target_schema_version=None):\\n        self.target_schema_version = target_schema_version\\n"}\n'
line: b'{"repo_name":"voidcc/PCTRL","ref":"refs/heads/master","path":"tests/unit/lib/mock_socket_test.py","content":"#!/usr/bin/env python\\n#\\n# Copyright 2011-2012 Andreas Wundsam\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at:\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\nimport unittest\\nimport sys\\nimport os.path\\nfrom copy import copy\\n\\nsys.path.append(os.path.dirname(__file__) + \\"/../../..\\")\\n\\nfrom pox.lib.mock_socket import MockSocket\\n\\nclass MockSocketTest(unittest.TestCase):\\n  def setUp(self):\\n    pass\\n\\n  def test_simple_send(self):\\n    (a, b) = MockSocket.pair()\\n    a.send(\\"Hallo\\")\\n    self.assertEquals(b.recv(), \\"Hallo\\")\\n    b.send(\\"Servus\\")\\n    self.assertEquals(a.recv(), \\"Servus\\")\\n\\n  def test_ready_to_recv(self):\\n    (a, b) = MockSocket.pair()\\n    a.send(\\"Hallo\\")\\n    self.assertFalse(a.ready_to_recv())\\n    self.assertTrue(b.ready_to_recv())\\n    self.assertEquals(b.recv(), \\"Hallo\\")\\n    self.assertFalse(b.ready_to_recv())\\n\\n    self.assertFalse(a.ready_to_recv())\\n    b.send(\\"Servus\\")\\n    self.assertTrue(a.ready_to_recv())\\n    self.assertEquals(a.recv(), \\"Servus\\")\\n    self.assertFalse(a.ready_to_recv())\\n\\n  def test_on_ready_to_recv(self):\\n    self.seen_size = -1\\n    self.called = 0\\n    def ready(socket, size):\\n      self.called += 1\\n      self.seen_size = size\\n\\n    (a, b) = MockSocket.pair()\\n    b.set_on_ready_to_recv(ready)\\n    self.assertEquals(self.called, 0)\\n    a.send(\\"Hallo\\")\\n    self.assertEquals(self.called, 1)\\n    self.assertEquals(self.seen_size, 5)\\n\\n    # check that it doesn\'t get called on the other sockets data\\n    b.send(\\"Huhu\\")\\n    self.assertEquals(self.called, 1)\\n\\n  def test_empty_recv(self):\\n    \\"\\"\\" test_empty_recv: Check that empty reads on socket return \\"\\"\\n       Note that this is actually non-sockety behavior and should probably be changed. This\\n       test documents it as intended for now, though\\n    \\"\\"\\"\\n    (a, b) = MockSocket.pair()\\n    self.assertEquals(a.recv(), \\"\\")\\n\\nif __name__ == \'__main__\':\\n  unittest.main()\\n"}\n'
line: b'{"repo_name":"mluo613/osf.io","ref":"refs/heads/develop","path":"scripts/update_comments.py","content":"\\"\\"\\"\\nUpdate User.comments_viewed_timestamp field.\\n\\"\\"\\"\\nimport logging\\nimport sys\\n\\nfrom modularodm import Q\\n\\nfrom framework.auth.core import User\\nfrom framework.transactions.context import TokuTransaction\\n\\nfrom website.models import Comment\\nfrom website.app import init_app\\nfrom scripts import utils as script_utils\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\ndef main():\\n    update_comments_viewed_timestamp()\\n\\n\\ndef update_comments_viewed_timestamp():\\n    users = User.find(Q(\'comments_viewed_timestamp\', \'ne\', None) \\u0026 Q(\'comments_viewed_timestamp\', \'ne\', {}))\\n    for user in users:\\n        if user.comments_viewed_timestamp:\\n            timestamps = {}\\n            for node_id in user.comments_viewed_timestamp:\\n                node_timestamps = user.comments_viewed_timestamp[node_id]\\n\\n                # node timestamp\\n                if node_timestamps.get(\'node\', None):\\n                    timestamps[node_id] = node_timestamps[\'node\']\\n\\n                # file timestamps\\n                file_timestamps = node_timestamps.get(\'files\', None)\\n                if file_timestamps:\\n                    for file_id in file_timestamps:\\n                        timestamps[file_id] = file_timestamps[file_id]\\n\\n            user.comments_viewed_timestamp = timestamps\\n            user.save()\\n            logger.info(\'Migrated timestamp for user {0}\'.format(user._id))\\n\\n\\nif __name__ == \'__main__\':\\n    dry = \'--dry\' in sys.argv\\n    if not dry:\\n        script_utils.add_file_logger(logger, __file__)\\n    init_app(routes=False, set_backends=True)\\n    with TokuTransaction():\\n        main()\\n        if dry:\\n            raise Exception(\'Dry Run -- Aborting Transaction\')\\n"}\n'
line: b'{"repo_name":"travisdesell/csg_boinc","ref":"refs/heads/master","path":"test/cgiserver.py","content":"#!/usr/bin/env python\\n\\n# $Id$\\n# cgi/php web server\\n\\nimport BaseHTTPServer, CGIHTTPServer\\nimport sys, os, urllib, select\\nimport random, time                     # XXX\\n\\nphp_path = None\\npossible_php_paths = [ \'/usr/lib/cgi-bin/php4\',\\n                       \'PROGRAM_PATH/fake_php.py\' ]\\ndef setup_php(program_path):\\n    global php_path\\n    for p in possible_php_paths:\\n        p = p.replace(\'PROGRAM_PATH\', program_path)\\n        if os.path.exists(p):\\n            php_path = p\\n            return\\n    raise Exception(\\"No php binary found - not even fake_php.py (program_path=%s) !\\"%program_path)\\n\\nclass PHPHTTPRequestHandler(CGIHTTPServer.CGIHTTPRequestHandler):\\n    def is_cgi(self):\\n        if os.path.split(self.path)[1] == \'\':\\n            index_php = os.path.join(self.path, \'index.php\')\\n            if os.path.exists(self.translate_path(index_php)):\\n                self.path = index_php\\n        if self.path.find(\'.php\') != -1:\\n            self.cgi_info = os.path.split(self.path)\\n            return True\\n\\n        for p in self.cgi_directories:\\n            p = os.path.join(p,\'\')\\n            if self.path.startswith(p):\\n                self.cgi_info = os.path.split(self.path)\\n                return True\\n        return False\\n\\n    def run_cgi(self):\\n        \\"\\"\\"Execute a CGI script.\\"\\"\\"\\n        dir, rest = self.cgi_info\\n        i = rest.rfind(\'?\')\\n        if i \\u003e= 0:\\n            rest, query = rest[:i], rest[i+1:]\\n        else:\\n            query = \'\'\\n        i = rest.find(\'/\')\\n        if i \\u003e= 0:\\n            script, rest = rest[:i], rest[i:]\\n        else:\\n            script, rest = rest, \'\'\\n        scriptname = dir + \'/\' + script\\n        is_php = script.endswith(\'.php\')\\n        # print \\"#### cgi_info=%s,dir=%s,rest=%s,script=%s,scriptname=%s,is_php=%s\\"%(self.cgi_info,dir,rest,script,scriptname,is_php)\\n        if is_php:\\n            if not php_path: raise Exception(\'php_path not set\')\\n            scriptfile = php_path\\n            sourcefile = self.translate_path(scriptname)\\n        else:\\n            scriptfile = self.translate_path(scriptname)\\n        if not os.path.exists(scriptfile):\\n            self.send_error(404, \\"No such CGI script (%s)\\" % `scriptname`)\\n            return\\n        if not os.path.isfile(scriptfile):\\n            self.send_error(403, \\"CGI script is not a plain file (%s)\\" %\\n                            `scriptname`)\\n            return\\n        ispy = self.is_python(scriptname)\\n        if not ispy:\\n            if not (self.have_fork or self.have_popen2 or self.have_popen3):\\n                self.send_error(403, \\"CGI script is not a Python script (%s)\\" %\\n                                `scriptname`)\\n                return\\n            if not self.is_executable(scriptfile):\\n                self.send_error(403, \\"CGI script is not executable (%s)\\" %\\n                                `scriptname`)\\n                return\\n\\n        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html\\n        # XXX Much of the following could be prepared ahead of time!\\n        env = {}\\n        env[\'DOCUMENT_ROOT\'] = os.getcwd()\\n        env[\'SERVER_SOFTWARE\'] = self.version_string()\\n        env[\'SERVER_NAME\'] = self.server.server_name\\n        env[\'GATEWAY_INTERFACE\'] = \'CGI/1.1\'\\n        env[\'SERVER_PROTOCOL\'] = self.protocol_version\\n        env[\'SERVER_PORT\'] = str(self.server.server_port)\\n        env[\'REQUEST_METHOD\'] = self.command\\n        uqrest = urllib.unquote(self.cgi_info[1])\\n        env[\'REQUEST_URI\'] = self.path\\n        # env[\'PATH_INFO\'] = uqrest\\n        # env[\'PATH_TRANSLATED\'] = self.translate_path(uqrest)\\n        env[\'SCRIPT_NAME\'] = scriptname\\n        env[\'SCRIPT_FILENAME\'] = self.translate_path(scriptname)\\n        if query:\\n            env[\'QUERY_STRING\'] = query\\n        host = self.address_string()\\n        if host != self.client_address[0]:\\n            env[\'REMOTE_HOST\'] = host\\n        env[\'REMOTE_ADDR\'] = self.client_address[0]\\n        env[\'REDIRECT_STATUS\'] = \'1\'      # for php\\n        # XXX AUTH_TYPE\\n        # XXX REMOTE_USER\\n        # XXX REMOTE_IDENT\\n        if self.headers.typeheader is None:\\n            env[\'CONTENT_TYPE\'] = self.headers.type\\n        else:\\n            env[\'CONTENT_TYPE\'] = self.headers.typeheader\\n        length = self.headers.getheader(\'content-length\')\\n        if length:\\n            env[\'CONTENT_LENGTH\'] = length\\n        accept = []\\n        for line in self.headers.getallmatchingheaders(\'accept\'):\\n            if line[:1] in \\"\\\\t\\\\n\\\\r \\":\\n                accept.append(line.strip())\\n            else:\\n                accept = accept + line[7:].split(\',\')\\n        env[\'HTTP_ACCEPT\'] = \',\'.join(accept)\\n        ua = self.headers.getheader(\'user-agent\')\\n        if ua:\\n            env[\'HTTP_USER_AGENT\'] = ua\\n        co = filter(None, self.headers.getheaders(\'cookie\'))\\n        if co:\\n            env[\'HTTP_COOKIE\'] = \', \'.join(co)\\n        # XXX Other HTTP_* headers\\n        if not self.have_fork:\\n            # Since we\'re setting the env in the parent, provide empty\\n            # values to override previously set values\\n            for k in (\'QUERY_STRING\', \'REMOTE_HOST\', \'CONTENT_LENGTH\',\\n                      \'HTTP_USER_AGENT\', \'HTTP_COOKIE\'):\\n                env.setdefault(k, \\"\\")\\n        os.environ.update(env)\\n\\n        self.send_response(200, \\"Script output follows\\")\\n\\n        decoded_query = query.replace(\'+\', \' \')\\n\\n        if self.have_fork:\\n            # Unix -- fork as we should\\n            if is_php:\\n                args = [php_path, sourcefile]\\n            else:\\n                args = [script]\\n            if \'=\' not in decoded_query:\\n                args.append(decoded_query)\\n            self.wfile.flush() # Always flush before forking\\n            pid = os.fork()\\n            if pid != 0:\\n                # Parent\\n                pid, sts = os.waitpid(pid, 0)\\n                # throw away additional data [see bug #427345]\\n                while select.select([self.rfile], [], [], 0)[0]:\\n                    try:\\n                        if not self.rfile.read(1):\\n                            break\\n                    except:\\n                        break\\n                if sts:\\n                    self.log_error(\\"CGI script exit status %#x\\", sts)\\n                return\\n            # Child\\n            try:\\n                if 0:\\n                    time.sleep(.1)\\n                    fn = \'/tmp/a%d\'%random.randint(1000,10000)\\n                    f = open(fn, \'w\')\\n                    s = \'\'\\n                    while select.select([self.rfile], [], [], 0)[0]:\\n                        try:\\n                            c = self.rfile.read(1)\\n                            if not c:\\n                                break\\n                            s += c\\n                        except:\\n                            break\\n                    print \'### input:\', repr(s)\\n                    print \\u003e\\u003ef, s\\n                    f.close()\\n                    self.rfile = open(fn, \'r\')\\n                os.dup2(self.rfile.fileno(), 0)\\n                os.dup2(self.wfile.fileno(), 1)\\n                os.chdir(self.translate_path(dir)) # KC\\n                os.execve(scriptfile, args, os.environ)\\n            except:\\n                self.server.handle_error(self.request, self.client_address)\\n                os._exit(127)\\n\\n        else:\\n            raise SystemExit(\'need fork()\')\\n\\ndef serve(bind=\'localhost\', port=8000, handler=PHPHTTPRequestHandler):\\n    httpd = BaseHTTPServer.HTTPServer((bind,port), handler)\\n    httpd.serve_forever()\\n\\nif __name__ == \'__main__\':\\n    setup_php(os.path.realpath(os.path.dirname(sys.argv[0])))\\n    serve()\\n"}\n'
input_path: /home/gcloud/TransCoder/data/test_dataset/r/r.002.json.gz
language: r
output_path: /home/gcloud/TransCoder/data/test_dataset/r/r.002.with_comments.tok
line: b'{ "repo_name": "hadley/readxl", "ref": "refs/heads/master", "path": "R/excel-sheets.R", "content": "#\' List all sheets in an excel spreadsheet.\\n#\'\\n#\' @inheritParams read_excel\\n#\' @export\\n#\' @examples\\n#\' excel_sheets(system.file(\\"extdata/datasets.xlsx\\", package = \\"readxl\\"))\\n#\' excel_sheets(system.file(\\"extdata/datasets.xls\\", package = \\"readxl\\"))\\n#\'\\n#\' # To load all sheets in a workbook, use lapply\\n#\' path <- system.file(\\"extdata/datasets.xls\\", package = \\"readxl\\")\\n#\' lapply(excel_sheets(path), read_excel, path = path)\\nexcel_sheets <- function(path) {\\n  path <- check_file(path)\\n  ext <- tolower(tools::file_ext(path))\\n\\n  switch(excel_format(path),\\n    xls =  xls_sheets(path),\\n    xlsx = xlsx_sheets(path)\\n  )\\n}\\n" }\n'
line: b'{ "repo_name": "eddelbuettel/rcppredis", "ref": "refs/heads/master", "path": "inst/tests/runit.serverIssues.R", "content": "test_01_setup <- function() {\\n    suppressMessages(library(RcppRedis))\\n    # we start the Redis server for this test as a slave so we don\'t clobber the main running version of redis for the rest of the tests\\n    writeLines(\\"requirepass badPassword\\",\\"/tmp/redis.conf\\")\\n    system(\\"redis-server /tmp/redis.conf --port 7777 --slaveof localhost 6379\\", wait=FALSE)\\n    # Wait for server to come up\\n    Sys.sleep(5)\\n}\\n\\ntest_02_testUnauth <- function () {\\n  redis <<- new(RcppRedis::Redis, \\"localhost\\", 7777, auth = \\"\\", 10)\\n  # we expect an exception because we haven\'t send the password\\n  checkException(redis$ping())\\n}\\n\\ntest_03_testAuth <- function () {\\n  redis <<- new(RcppRedis::Redis, \\"localhost\\", 7777, auth = \\"badPassword\\", 10)\\n  checkEquals(redis$ping(), \\"PONG\\")\\n}\\n\\ntest_04_killServer <- function() {\\n    # confirm server is up\\n    checkEquals(redis$ping(),\\"PONG\\")\\n    # kill server\\n    checkException(redis$exec(\\"SHUTDOWN\\"))\\n}\\n\\ntest_05_cleanup <- function() {\\n    NULL\\n}\\n" }\n'
line: b'{ "repo_name": "eddelbuettel/rcppredis", "ref": "refs/heads/master", "path": "demo/spDemo.R", "content": "\\n\\nlibrary(rredis)\\nsuppressMessages(library(RcppRedis))\\nsuppressMessages(library(xts))\\nlibrary(rbenchmark)\\n\\nredisConnect()                          # default localhost\\nredis <- new(Redis)\\n\\nif (!redisExists(\\"sp500\\")) {\\n    suppressMessages(library(quantmod))\\n    options(\\"getSymbols.warning4.0\\"=FALSE)   ## suppress some line noise\\n    sp <- getSymbols(\\"^GSPC\\", auto.assign=FALSE, from=\\"1950-01-01\\", to=Sys.Date())\\n    redisSet(\\"sp500\\", sp)\\n    cat(\\"Downloaded SP500 and stored in redis\\\\n\\")\\n} else { \\n    cat(\\"Retrieving SP500 from redis\\\\n\\")\\n    sp <- redisGet(\\"sp500\\")\\n}\\n#print(summary(sp))\\n\\nrInsert <- function(x) {\\n    n <- nrow(x)\\n    key <- \\"sp500_R\\"\\n    if (redisExists(key)) redisDelete(key)\\n    M <- cbind(as.numeric(index(x)), coredata(x))\\n    for (i in 1:n) {\\n        redisRPush(key, M[i,,drop=TRUE])\\n    }\\n    invisible(NULL)\\n}\\n\\n## This is atrociously slow:\\n##\\n## R> system.time(rInsert(sp))\\n##    user  system elapsed \\n##  16.392   0.292 645.643 \\n## R> \\n\\nsystem.time(m1 <- do.call(rbind, redisLRange(\\"sp500_R\\", 0, -1)))\\nsystem.time(m2 <- do.call(rbind, redis$lrange(\\"sp500_R\\", 0, -1)))\\nidentical(m1,m2)\\nsystem.time(m3 <- redis$listToMatrix(redis$lrange(\\"sp500_R\\", 0, -1)))\\nm4 <- m1\\ndimnames(m4) <- list()\\nidentical(m4,m3)\\n\\n## approx factor 20\\nres <- benchmark(do.call(rbind, redisLRange(\\"sp500_R\\", 0, -1)),\\n                 do.call(rbind, redis$lrange(\\"sp500_R\\", 0, -1)),\\n                 redis$listToMatrix(redis$lrange(\\"sp500_R\\", 0, -1)),\\n                 order=\\"relative\\", replications=25)[,1:4]\\nprint(res)\\n\\n\\ncInsert <- function(x) {\\n    n <- nrow(x)\\n    key <- \\"sp500_C\\"\\n    if (redisExists(key)) redisDelete(key)\\n    M <- cbind(as.numeric(index(x)), coredata(x))\\n    for (i in 1:n) {\\n        redis$listRPush(key, M[i,])\\n    }\\n    invisible(NULL)\\n}\\n\\n\\n## approx factor 20\\nres <- benchmark(do.call(rbind, redisLRange(\\"sp500_R\\", 0, -1)),\\n                 do.call(rbind, redis$lrange(\\"sp500_R\\", 0, -1)),\\n                 redis$listToMatrix(redis$lrange(\\"sp500_R\\", 0, -1)),\\n                 redis$listToMatrix(redis$listRange(\\"sp500_C\\", 0, -1)),\\n                 order=\\"relative\\", replications=25)[,1:4]\\nprint(res)\\n\\n\\n## R> print(res)\\n##                                                    test replications elapsed relative\\n## 4 redis$listToMatrix(redis$listRange(\\"sp500_C\\", 0, -1))           25   0.296    1.000\\n## 3    redis$listToMatrix(redis$lrange(\\"sp500_R\\", 0, -1))           25   2.300    7.770\\n## 2        do.call(rbind, redis$lrange(\\"sp500_R\\", 0, -1))           25   2.629    8.882\\n## 1         do.call(rbind, redisLRange(\\"sp500_R\\", 0, -1))           25  48.028  162.257\\n## R> \\n\\n## redo after Bryan\'s socket/nagle update to rredis:\\n##                                                    test replications elapsed relative\\n## 4 redis$listToMatrix(redis$listRange(\\"sp500_C\\", 0, -1))           25   0.407    1.000\\n## 3    redis$listToMatrix(redis$lrange(\\"sp500_R\\", 0, -1))           25   2.112    5.189\\n## 2        do.call(rbind, redis$lrange(\\"sp500_R\\", 0, -1))           25   2.458    6.039\\n## 1         do.call(rbind, redisLRange(\\"sp500_R\\", 0, -1))           25  48.367  118.838\\n## edd@max:~/git/rhiredis/demo$ \\n" }\n'
line: b'{ "repo_name": "NCBI-Hackathons/SRA2R", "ref": "refs/heads/master", "path": "tests/runTests.R", "content": "BiocGenerics:::testPackage(\\"SRA2R\\")\\n" }\n'
line: b'{ "repo_name": "NCBI-Hackathons/SRA2R", "ref": "refs/heads/master", "path": "tests/testthat/test_getReads.R", "content": "library(SRA2R)\\ncontext(\\"getReads\\")\\n\\ntest_that(\\"getReads returns reads from a run\\", {\\n  expect_equal(getFastqCount(\'SRR000123\'),4583)\\n  expect_equal(getFastqCount(\'SRR1607152\'),78377869)\\n})\\n" }\n'
line: b'{ "repo_name": "thiloklein/matchingMarkets", "ref": "refs/heads/master", "path": "R/ttc.R", "content": "# ----------------------------------------------------------------------------\\n# R-code (www.r-project.org/) for the Top-Trading-Cycles Algorithm\\n#\\n# Copyright (c) 2013 Thilo Klein\\n#\\n# This library is distributed under the terms of the GNU Public License (GPL)\\n# for full details see the file LICENSE\\n#\\n# ----------------------------------------------------------------------------\\n\\n#\' @title Top-Trading-Cycles Algorithm for the house allocation problem\\n#\'\\n#\' @description Finds the stable matching in the \\\\href{http://en.wikipedia.org/wiki/Herbert_Scarf#8._The_Housing_Market}{house allocation problem} with existing tenants.\\n#\' Uses the Top-Trading-Cycles Algorithm proposed in Abdulkadiroglu and Sonmez (1999).\\n#\'\\n#\' @param P list of individuals\' preference rankings over objects.\\n#\' @param X 2-column-matrix of objects (\\\\code{obj}) and their owners (\\\\code{ind}).\\n#\' @return \\\\code{ttc} returns a 2-column matrix of the stable matching solution for the housing market problem based on the Top-Trading-Cycles algorithm.\\n#\' @author Thilo Klein \\n#\' @keywords algorithms\\n#\' @references Abdulkadiroglu, A. and Sonmez, T. (1999). House Allocation with Existing Tenants. \\\\emph{Journal of Economic Theory}, 88(2):233--260.\\n#\' @examples\\n#\' ## generate list of individuals\' preference rankings over objects\\n#\' P <- list()\\n#\' P[[1]] <- c(2,5,1,4,3)    # individual 1\\n#\' P[[2]] <- c(1,5,4,3,2)    # individual 2\\n#\' P[[3]] <- c(2,1,4,3,5)    # individual 3\\n#\' P[[4]] <- c(2,4,3,1,5)    # individual 4\\n#\' P[[5]] <- c(4,3,1,2,5); P # individual 5\\n#\' \\n#\' ## generate 2-column-matrix of objects (\'obj\') and their owners (\'ind\')\\n#\' X <- data.frame(ind=1:5, obj=1:5); X\\n#\' \\n#\' ## find assignment based on TTC algorithm\\n#\' ttc(P=P,X=X)\\n#\' @export\\nttc <- function(P=NULL,X=NULL){\\n  \\n  ## 2-column-matrix of home objects (\'obj\') and their owners (\'ind\')\\n  Y <- data.frame(ind=NULL, obj=NULL)\\n  \\n  for(z in 1:length(unique(X$obj))){\\n\\n    ## 1. Find cycle\\n    Cycle <- findCycle(P=P,X=X)\\n    \\n    ## 2. Add objects in this cycle to \'home territory\'\\n    Y <- rbind(Y,Cycle)\\n\\n    ## 3. Remove objects in this cycle from tradable objects\\n    X <- X[-which(X$obj %in% Cycle$obj),]\\n    for(i in 1:length(P)){\\n      P[[i]] <- P[[i]][!P[[i]] %in% Y$ind]\\n    }\\n\\n    ## 4. Process ends if no tradable objects remain\\n    if(nrow(X)==0){\\n      Y <- rbind(Y,X)\\n      return(Y)\\n      break\\n    }\\n  }\\n}\\nfindCycle <- function(P=NULL,X=NULL){\\n  Cycle   <- data.frame(ind=NA, obj=NA)\\n  thisind <- X$ind[1] # start with first individual in line\\n  for(j in 1:length(unique(X$ind))){\\n    Cycle[j,] <- c(thisind,P[[thisind]][1]) # id and top-ranked object of the individual in line\\n    thisind   <- X[X$obj == P[[thisind]][1],\\"ind\\"] # individual whose object is requested\\n    if(Cycle[j,1] == Cycle[j,2]){ # if individual points to own object\\n      return(Cycle[j,])\\n      break\\n    }\\n    if(thisind %in% Cycle$ind){ # if this individual completes a cycle\\n      return(Cycle)\\n      break\\n    } \\n  }\\n}" }\n'
line: b'{ "repo_name": "thiloklein/matchingMarkets", "ref": "refs/heads/master", "path": "R/mfx.R", "content": "# ----------------------------------------------------------------------------\\n# R-code (www.r-project.org/) to obtain marginal effects for probit and matching models\\n#\\n# Copyright (c) 2013 Thilo Klein\\n#\\n# This library is distributed under the terms of the GNU Public License (GPL)\\n# for full details see the file LICENSE\\n#\\n# ----------------------------------------------------------------------------\\n\\n#\' @title Marginal effects for probit and matching models\\n#\'\\n#\' @description Marginal effects from regression coefficients for probit \\n#\' and matching models. \\n#\'\\n#\' @param m an object returned from functions \\\\code{stabit} or \\\\code{stabit2}.\\n#\' @param toLatex logical: if \\\\code{TRUE} the result tables are printed in Latex format. The default setting is \\\\code{FALSE}.\\n#\' \\n#\' @export\\n#\' \\n#\' @import stats\\n#\' \\n#\' @author Thilo Klein \\n#\' \\n#\' @keywords summary\\n#\' \\n#\' @references Klein, T. (2015a). \\\\href{https://ideas.repec.org/p/cam/camdae/1521.html}{Does Anti-Diversification Pay? A One-Sided Matching Model of Microcredit}.\\n#\' \\\\emph{Cambridge Working Papers in Economics}, #1521.\\n#\' \\n#\' @examples\\n#\' ## 1. load results from Klein (2015a)\\n#\'  data(klein15a)\\n#\' \\n#\' ## 2. apply mfx function and print results\\n#\'  mfx(m=klein15a)\\nmfx <- function(m,toLatex=FALSE){\\n  \\n  if(!is.null(m$coefs$alpha)){ ## Selectiom and Outcome Eqns\\n\\n    ## model matrix\\n    X <- do.call(rbind.data.frame, m$model.list$X)\\n    eta <- c(m$coefs$eta, rep(0, length(m$model.list$X)-length(m$model.list$W)))\\n    X <- as.matrix(data.frame(X=X,eta=eta))\\n    \\n    ## valuation equation\\n    nrowX <- dim(do.call(rbind.data.frame, m$model.list$W))[1]\\n    sel <- mfxVal(postmean=m$coefs$alpha[,1], poststd=m$coefs$alpha[,2],\\n           nrowX=nrowX, toLatex=toLatex)\\n    \\n    ## structral model outcome\\n    out <- mfxOut(sims=10000, postmean=unlist(c(m$coefs$beta[,1], data.frame(delta=m$coefs$delta[1]))),\\n           poststd=c(m$coefs$beta[,2], m$coefs$delta[2]), X=X, toLatex=toLatex)\\n\\n    return(list(mfx.selection=sel, mfx.outcome=out))\\n\\n  } else{ ## Outcome Eqn only\\n    \\n    ## model matrix\\n    X <- do.call(rbind.data.frame, m$model.list$X)\\n    X <- as.matrix(X)\\n    \\n    ## model outcome    \\n    out <- mfxOut(sims=10000, postmean=m$coefs$beta[,1],\\n           poststd=m$coefs$beta[,2], X=X, toLatex=toLatex)\\n    \\n    return(list(mfx.outcome=out))\\n  }\\n}\\n\\n\\nmfxOut <- function(sims=10000,x.mean=TRUE,postmean,poststd,X,toLatex){\\n  ## source: http://researchrepository.ucd.ie/handle/10197/3404\\n  ## method: average of individual marginal effects at each observation\\n  ## interpretation: http://www.indiana.edu/~statmath/stat/all/cdvm/cdvm.pdf page 8\\n  set.seed(1984)\\n  if(x.mean==TRUE){\\n    ## marginal effects are calculated at the means of independent variables\\n    pdf <- dnorm(mean(X%*%postmean))\\n    pdfsd <- dnorm(sd(X%*%postmean))\\n  } else{\\n    ## marginal effects are calculated for each observation and then averaged\\n    pdf <- mean(dnorm(X%*%postmean))\\n    pdfsd <- sd(dnorm(X%*%postmean))\\n  }  \\n  mx <- pdf*postmean\\n\\n  sim <- matrix(rep(NA,sims*length(postmean)), nrow=sims)\\n  for(i in 1:length(postmean)){\\n    sim[,i] <- rnorm(sims,postmean[i],poststd[i])\\n  }\\n  pdfsim <- rnorm(sims,pdf,pdfsd)\\n  sim.se <- pdfsim*sim\\n  s.e. <- apply(sim.se,2,sd)\\n\\n  t.stat <- mx/s.e.\\n  p.val <- pt(-abs(t.stat),df=dim(X)[1]-length(postmean)+1)\\n  stars <- ifelse(p.val<0.001,\\"***\\",ifelse(p.val<0.01,\\"**\\",ifelse(p.val<0.05,\\"*\\",ifelse(p.val<0.10,\\".\\",\\"\\"))))\\n  if(toLatex==FALSE){\\n    res <- data.frame(round(cbind(mx, s.e., t.stat, p.val),3), stars)\\n  } else{\\n      sign <- ifelse(mx>0,\\"~\\",\\"\\")\\n      res <- data.frame(\\"&\\", sign, round(mx,3), se=paste(paste(\\"(\\",round(s.e.,3),sep=\\"\\"),\\")\\",sep=\\"\\"), stars, \\"\\\\\\\\\\")\\n  }\\n  return(res)\\n}\\n\\n\\nmfxVal <- function(postmean,poststd,nrowX,toLatex){\\n\\n  ## Reference: Sorensen (2007, p. 2748)\\n\\n  mx <- dnorm(0)*postmean/sqrt(2)\\n  s.e. <- dnorm(0)*poststd/sqrt(2)\\n  t.stat <- mx/s.e.\\n  p.val <- pt(-abs(t.stat),df=nrowX-length(postmean))\\n  stars <- ifelse(p.val<0.001,\\"***\\",ifelse(p.val<0.01,\\"**\\",ifelse(p.val<0.05,\\"*\\",ifelse(p.val<0.10,\\".\\",\\"\\"))))\\n  if(toLatex==FALSE){\\n    res <- data.frame( round(cbind(mx, s.e., t.stat, p.val),3), stars)\\n  } else{\\n    sign <- ifelse(mx>0,\\"~\\",\\"\\")\\n    res <- data.frame( \\"&\\", sign, round(mx,3), se=paste(paste(\\"(\\",round(s.e.,3),sep=\\"\\"),\\")\\",sep=\\"\\"), stars, \\"\\\\\\\\\\")\\n  }\\n  return(res)\\n}\\n" } \n'
line: b'{ "repo_name": "jbkunst/r-posts", "ref": "refs/heads/master", "path": "015-www-working-with-weather/translation_data.R", "content": "# Data from \'https://centroccbb.cl/clima/indexData.php\'\\nrm(list = ls())\\n\\nlibrary(\\"rio\\")\\nlibrary(\\"dplyr\\")\\nlibrary(\\"rvest\\")\\n\\n\\ndata <- import(\\"Informe parcial.xls\\")\\n\\ndata <- data %>% tbl_df()\\n\\nnames(data)\\n\\n\\n# names(data) <- c(\\"date\\", \\"time\\", \\"temperature\\", \\"humidity\\", \\"pressure\\", \\"precipitation\\",\\n#                  \\"solar_radiation\\", \\"wind_speed\\", \\"wind_direction\\",\\n#                  \\"chill\\", \\"Temperature_flushing\\", \\"htw_index\\", \\"thws_index\\", \\"dew_point\\",\\n#                  \\"rainfall_rate\\", \\"air_density\\")\\n\\nnames(data) <- c(\\"date\\", \\"time\\", \\"temperature\\", \\"humidity\\", \\"pressure\\", \\"precipitation\\",\\n                 \\"solar_radiation\\", \\"wind_speed\\", \\"wind_direction\\",\\n                 \\"thermal_sensation\\", \\"rainfall_rate\\")\\n\\nnames(data)\\n\\ndata\\n\\nstr(data)\\n\\nsave(data, file = \\"wheather_data.RData\\")\\n" }\n'
line: b'{ "repo_name": "jbkunst/r-posts", "ref": "refs/heads/master", "path": "052-motion-piramid/readme.R", "content": "#\' ---\\r\\n#\' title: \\"Motion Piramid\\"\\r\\n#\' author: \\"Joshua Kunst\\"\\r\\n#\' output:\\r\\n#\'  html_document:\\r\\n#\'    toc: true\\r\\n#\'    keep_md: yes\\r\\n#\' ---\\r\\n\\r\\n#+ echo=FALSE, message=FALSE, warning=FALSE\\r\\nrm(list = ls())\\r\\nknitr::opts_chunk$set(message = FALSE, warning = FALSE,\\r\\n                      fig.showtext = TRUE, dev = \\"CairoPNG\\")\\r\\n\\r\\n#\'\\r\\nlibrary(idbr)\\r\\nlibrary(dplyr)\\r\\nlibrary(purrr)\\r\\nlibrary(highcharter)\\r\\nrm(list = ls())\\r\\n\\r\\nidb_api_key(\\"35f116582d5a89d11a47c7ffbfc2ba309133f09d\\")\\r\\n\\r\\nyrs <-  seq(1980, 2030, by = 1)\\r\\n\\r\\ndf <- map_df(c(\\"male\\", \\"female\\"), function(sex){\\r\\n  idb1(\\"US\\", yrs, sex = sex) %>%\\r\\n    mutate(sex_label = sex)\\r\\n})\\r\\n\\r\\nnames(df) <- tolower(names(df))\\r\\n\\r\\nstr(df)\\r\\nhead(df) \\r\\n\\r\\ndf <- df %>%\\r\\n  mutate(population = pop*ifelse(sex_label == \\"male\\", -1, 1))\\r\\n\\r\\nseries <- df %>% \\r\\n  group_by(sex_label, age) %>% \\r\\n  do(data = list(sequence = .$population)) %>% \\r\\n  ungroup() %>% \\r\\n  group_by(sex_label) %>% \\r\\n  do(data = .$data) %>%\\r\\n  mutate(name = sex_label) %>% \\r\\n  list.parse3()\\r\\n\\r\\nmaxpop <- max(abs(df$population))\\r\\n\\r\\nxaxis <- list(categories = sort(unique(df$age)),\\r\\n              reversed = FALSE, tickInterval = 5,\\r\\n              labels = list(step = 5))\\r\\n\\r\\nhighchart() %>%\\r\\n  hc_chart(type = \\"bar\\") %>%\\r\\n  hc_motion(enabled = TRUE, labels = yrs, series = c(0,1), autoplay = TRUE, updateInterval = 1) %>% \\r\\n  hc_add_series_list(series) %>% \\r\\n  hc_plotOptions(\\r\\n    series = list(stacking = \\"normal\\"),\\r\\n    bar = list(groupPadding = 0, pointPadding =  0, borderWidth = 0)\\r\\n  ) %>% \\r\\n  hc_tooltip(shared = TRUE) %>% \\r\\n  hc_yAxis(\\r\\n    labels = list(\\r\\n      formatter = JS(\\"function(){ return Math.abs(this.value) / 1000000 + \'M\'; }\\") \\r\\n    ),\\r\\n    tickInterval = 0.5e6,\\r\\n    min = -maxpop,\\r\\n    max = maxpop) %>% \\r\\n  hc_xAxis(\\r\\n    xaxis,\\r\\n    rlist::list.merge(xaxis, list(opposite = TRUE, linkedTo = 0))\\r\\n  ) %>% \\r\\n  hc_tooltip(shared = FALSE,\\r\\n             formatter = JS(\\"function () { return \'<b>\' + this.series.name + \', age \' + this.point.category + \'</b><br/>\' + \'Population: \' + Highcharts.numberFormat(Math.abs(this.point.y), 0);}\\")\\r\\n  ) %>% \\r\\n  hc_add_theme(hc_theme_smpl())\\r\\n\\r\\n  \\r\\n" }\n'
line: b'{ "repo_name": "jbkunst/r-posts", "ref": "refs/heads/master", "path": "001-GTFS-Transantiago-ggplot2/script/script_02.R", "content": "# Source\\n# https://developers.google.com/transit/gtfs/reference\\nrm(list=ls())\\noptions(stringsAsFactors=FALSE)\\n\\nlibrary(devtools)\\nlibrary(ggplot2)\\nlibrary(plyr)\\nlibrary(dplyr)\\n\\nsource_url(\\"https://raw.githubusercontent.com/jbkunst/reuse/master/R/gg_themes.R\\")\\n\\nfile_temp <- tempfile(fileext = \\".zip\\")\\n\\n# http://www.mbta.com/rider_tools/developers/default.asp?id=21895\\ndownload.file(\\"http://www.mbta.com/uploadedfiles/MBTA_GTFS.zip\\", file_temp)\\n\\ndata <- read.table()\\n\\nshapes <- read.csv(unz(file_temp, \\"shapes.txt\\"))\\nroutes <- read.csv(unz(file_temp, \\"routes.txt\\"))\\ntrips <- read.csv(unz(file_temp, \\"trips.txt\\"))\\nstops <- read.csv(unz(file_temp, \\"stops.txt\\"))\\n\\nhead(routes)\\ntail(routes)\\nhead(shapes)\\n\\nunlink(file_temp)\\n\\n\\np <- ggplot(shapes) +\\n  geom_path(aes(shape_pt_lon, shape_pt_lat, group=shape_id), size=.2, alpha=.1) +\\n  xlim(-71.2, -71) + ylim(42.0,42.6) +\\n  coord_equal()\\n  \\np\\n\\n\\nstops_metro <- stops %>% filter(!grepl(\\"\\\\\\\\d\\", stop_id))\\nshapes_colors <- left_join(left_join(shapes %>% select(shape_id) %>% unique(),\\n                                     trips %>% select(shape_id, route_id) %>% unique()),\\n                           routes %>% select(route_id, route_color) %>% unique())\\nshapes_colors <- shapes_colors  %>% mutate(route_color = paste0(\\"#\\", route_color))\\nroutes_metro <- routes %>% filter(grepl(\\"^L\\\\\\\\d\\",route_id))\\nshapes_metro <- shapes %>% filter(shape_id %in% trips$shape_id[trips$route_id %in% routes_metro$route_id])\\nshapes_colors_metro <- shapes_colors %>% filter(shape_id %in% trips$shape_id[trips$route_id %in% routes_metro$route_id])\\n\\np2 <- ggplot() +\\n  geom_path(data=shapes, aes(shape_pt_lon, shape_pt_lat, group=shape_id), color=\\"white\\", size=.2, alpha=.1) +\\n  geom_path(data=shapes_metro, aes(shape_pt_lon, shape_pt_lat, group=shape_id, colour=shape_id), size = 2, alpha=.7) +\\n  scale_color_manual(values=shapes_colors_metro$route_color) +\\n  geom_point(data=stops_metro, aes(stop_lon, stop_lat), shape=21, colour=\\"white\\", alpha =.8) +\\n  coord_equal() +\\n  theme_null() +\\n  theme(plot.background = element_rect(fill = \\"black\\", colour = \\"black\\"),\\n        title = element_text(hjust=1, colour=\\"white\\")) +\\n  ggtitle(\\"TRANSANTIAGO\\\\nSantiago\'s public transport system\\")\\np2\\n" }\n'
line: b'{ "repo_name": "jbkunst/r-posts", "ref": "refs/heads/master", "path": "025-stackoverflow/get-github-language-colors.R", "content": "library(\\"httr\\")\\r\\ndfcols <- \\"https://raw.githubusercontent.com/doda/github-language-colors/master/colors.json\\" %>% \\r\\n  GET() %>%\\r\\n  content() %>% \\r\\n  jsonlite::fromJSON() %>% \\r\\n  { data_frame(language = tolower(names(.)),\\r\\n               color = unlist(.)) }\\r\\n\\r\\ndfcols\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "jbkunst/r-posts", "ref": "refs/heads/master", "path": "018-riv-woe-package/readme.R", "content": "#\' ---\\n#\' title: \\"RIV WOE Package\\"\\n#\' author: \\"Joshua Kunst\\"\\n#\' output: \\n#\'  html_document: \\n#\'    keep_md: yes\\n#\' ---\\n\\n#+ fig.width=10, fig.height=5\\n#+ echo=FALSE\\n# library(\\"printr\\")\\nlibrary(\\"knitr\\")\\noptions(digits = 3, knitr.table.format = \\"markdown\\")\\nknitr::opts_chunk$set(collapse = TRUE, comment = \\">\\", warning = FALSE,\\n                      fig.width = 10, fig.height = 6,\\n                      fig.align = \\"center\\", dpi = 72)\\n\\n#\' # Introducction\\n#\' \\n#\' - woe is data frame oriented, the functions have always a data frame argument.\\n#\' - riskr is variable oriented, the functions have always a variable (non dataframe) argument.\\n\\n#\' # Load packages and data\\n# devtools::install_github(\\"tomasgreif/woe\\")\\nlibrary(\\"woe\\")\\nlibrary(\\"riskr\\")\\n\\ndata(\\"german_data\\")\\n\\n#\' Required for riskr\\ngerman_data$gb2 <- ifelse(german_data$gb == \\"good\\", 1, 0)\\n\\n#\' # Bivariate analysis:\\n#\' \\n#\' ## 1 variable case\\n\\nlvls <- names(sort(table(german_data$purpose)))\\ngerman_data$purpose <- factor(as.character(german_data$purpose), levels = lvls)\\n\\n#\' ### woe \\niv.str(german_data,\\"purpose\\",\\"gb\\", verbose = FALSE)\\niv.plot.woe(iv.mult(german_data,\\"gb\\",vars = c(\\"purpose\\"), summary = FALSE))\\n\\n#\' ### riskr\\nbt(german_data$purpose, german_data$gb2)\\n\\nplot_ba(german_data$purpose, german_data$gb2) +\\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0))\\n  \\n\\n#\' ## data frame case\\n#\' \\n#\' ### woe\\niv.mult(german_data, \\"gb\\", vars =  c(\\"purpose\\", \\"ca_status\\", \\"credit_history\\", \\"status_sex\\"),\\n        verbose = FALSE)\\n\\n#\' ### riskr\\nlibrary(\\"tidyr\\")\\nlibrary(\\"dplyr\\")\\n\\ngerman_data %>% \\n  tbl_df() %>% \\n  select(gb2, purpose, ca_status, credit_history, status_sex) %>% \\n  gather(variable, value, -gb2) %>% \\n  group_by(variable) %>% \\n  do({ bt(.$value, .$gb2)  })\\n\\n\\n# iv.num(german_data,\\"duration\\",\\"gb\\")\\n# iv.mult(german_data,\\"gb\\",TRUE, verbose = TRUE)\\n# iv.plot.summary(iv.mult(german_data,\\"gb\\",TRUE))\\n# head(german_data$credit_amount)\\n# iv.binning.simple(german_data,\\"credit_amount\\")\\n" }\n'
line: b'{ "repo_name": "jbkunst/r-posts", "ref": "refs/heads/master", "path": "036-ezsummary-tests/readme.R", "content": "#\' ---\\r\\n#\' title: \\"\\"\\r\\n#\' author: \\"Joshua Kunst\\"\\r\\n#\' output:\\r\\n#\'  html_document:\\r\\n#\'    toc: true\\r\\n#\'    keep_md: yes\\r\\n#\' ---\\r\\n\\r\\n#+echo=FALSE\\r\\nknitr::opts_chunk$set(warning=FALSE)\\r\\n\\r\\n#+echo=TRUE\\r\\nlibrary(\\"dplyr\\")\\r\\nlibrary(\\"ezsummary\\")\\r\\n\\r\\ndata(diamonds, package = \\"ggplot2\\")\\r\\ntbl <- diamonds\\r\\n\\r\\nselect_categorical <- function(tbl, nuniques = 10){\\r\\n  \\r\\n  selections <- purrr::map_lgl(tbl, function(x) {\\r\\n    is.character(x) ||\\r\\n      is.factor(x) ||\\r\\n      is.logical(x) ||\\r\\n      (length(unique(x)) <= nuniques) # this is when you have numeric variables with few uniques (dummies)\\r\\n  })\\r\\n  tbl[, selections]\\r\\n  \\r\\n}\\r\\n\\r\\nselect_quantitative <- function(tbl){\\r\\n  \\r\\n  selections <- purrr::map_lgl(tbl, function(x) is.numeric(x))\\r\\n  tbl[, selections]\\r\\n  \\r\\n}\\r\\n\\r\\ndiamonds %>% \\r\\n  select_categorical() \\r\\n\\r\\ndiamonds %>% \\r\\n  select_quantitative()\\r\\n\\r\\n\\r\\n#### ####\\r\\n\\r\\n# tbl <- diamonds %>% select_categorical() %>%  group_by(cut)\\r\\n# tbl <- diamonds %>% select_categorical() %>%  group_by(cut, color)\\r\\n\\r\\nezsummary_categorical2 <- function(tbl){\\r\\n  \\r\\n  grp_cols <- names(attr(tbl, \\"labels\\"))\\r\\n  \\r\\n  tbl %>%\\r\\n    purrr::map_if(is.factor, as.character) %>% # avoid warning\\r\\n    as_data_frame() %>% # this ungroup the tbl\\r\\n    group_by_(.dots = lapply(grp_cols, as.symbol)) %>%  # http://stackoverflow.com/questions/21208801/\\r\\n    do({ezsum = \\r\\n      tidyr::gather(., key, value) %>% # you can use tidyr::gather(., variable, category)\\r\\n      ungroup() %>%\\r\\n      count(key, value) %>% \\r\\n      mutate(p = n/sum(n))\\r\\n    }) %>% \\r\\n    # ungroup() %>%\\r\\n    filter(!key %in% grp_cols) # not sure whiy appear as key a group col\\r\\n \\r\\n}\\r\\n\\r\\ndiamonds %>%\\r\\n  select_categorical() %>% \\r\\n  ezsummary_categorical() \\r\\n\\r\\ndiamonds %>%\\r\\n  select_categorical() %>% \\r\\n  ezsummary_categorical2() \\r\\n\\r\\nlibrary(rbenchmark)\\r\\n\\r\\ntblcat <- diamonds %>% select_categorical()\\r\\ntblcat <- diamonds %>% select_categorical() %>%  group_by(cut, color)\\r\\n\\r\\nbenchmark(\\r\\n  ezsummary_categorical(tblcat),\\r\\n  ezsummary_categorical2(tblcat),\\r\\n  replications = 100\\r\\n)\\r\\n# Mmm not so fast\\r\\n\\r\\nt1 <- diamonds %>%\\r\\n  select_categorical() %>% \\r\\n  ezsummary_categorical2() \\r\\n\\r\\nt1\\r\\n\\r\\n# check how much groups are \\r\\nsum(t1$p) == nrow(distinct(t1, key))\\r\\n\\r\\nt2 <- diamonds %>%\\r\\n  select_categorical() %>% \\r\\n  group_by(cut, color) %>% \\r\\n  ezsummary_categorical2()\\r\\n\\r\\nt2\\r\\n\\r\\n# check how much groups are \\r\\nsum(t2$p) == nrow(distinct(t2, cut, color, key))\\r\\n\\r\\nmtcars %>% \\r\\n  select_categorical(nuniques = 2) %>% \\r\\n  ezsummary_categorical2()\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "greenelab/TDM", "ref": "refs/heads/master", "path": "R/package_loader.R", "content": "#\' Loads a character vector of packages, installing them from Bioconductor if necessary.\\n#\' This will also load most CRAN packages.\\n#\'  \\n#\' @title load_it\\n#\' @name load_it\\n#\' @param pack -- A vector of character strings containing package names.\\n#\' @return nothing\\n#\' @export\\n#\'\\nload_it = function(pack, update=FALSE) {\\n\\tinvisible(sapply(pack, function(package) {\\n\\t\\tif(!require(package, quietly=T, character.only=T)){\\n\\t\\t\\t\\tif(length(find(\\"biocLite\\")) < 1) {\\n\\t\\t\\t\\t\\t\\tsource(\\"http://bioconductor.org/biocLite.R\\")\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tif(update) {\\n\\t\\t\\t\\t\\t\\tbiocLite(package)\\t\\n\\t\\t\\t\\t} else {\\n\\t\\t\\t\\t\\t\\tbiocLite(package, suppressUpdates=TRUE)\\n\\t\\t\\t\\t}\\n\\n\\t\\t\\t\\tlibrary(package, quietly=T, character.only=T)\\n\\t\\t\\t}\\n\\t\\t}))\\n} #end load_it" }\n'
line: b'{ "repo_name": "PoisonAlien/maftools", "ref": "refs/heads/master", "path": "R/oncodrive.R", "content": "#\' Detect cancer driver genes based on positional clustering of variants.\\n#\'\\n#\' @description Clusters variants based on their position to detect disease causing genes.\\n#\' @details This is the re-implimentation of algorithm defined in OncodriveCLUST article. Concept is based on the fact that most of the variants in cancer causing genes are enriched at few specific loci (aka hotspots).\\n#\' This method takes advantage of such positions to identify cancer genes. Cluster score of 1 means, a single hotspot hosts all observed variants. If you use this function, please cite OncodriveCLUST article.\\n#\' @references Tamborero D, Gonzalez-Perez A and Lopez-Bigas N. OncodriveCLUST: exploiting the positional clustering of somatic mutations to identify cancer genes. Bioinformatics. 2013; doi: 10.1093/bioinformatics/btt395s\\n#\' @param maf an \\\\code{\\\\link{MAF}} object generated by \\\\code{\\\\link{read.maf}}\\n#\' @param AACol manually specify column name for amino acid changes. Default looks for field \'AAChange\'\\n#\' @param pvalMethod either zscore (default method for oncodriveCLUST), poisson or combined (uses lowest of the two pvalues).\\n#\' @param nBgGenes minimum number of genes required to estimate background score. Default 100. Do not change this unless its necessary.\\n#\' @param minMut minimum number of mutations required for a gene to be included in analysis. Default 5.\\n#\' @param bgEstimate If FALSE skips background estimation from synonymous variants and uses predifined values estimated from COSMIC synonymous variants.\\n#\' @param ignoreGenes Ignore these genes from analysis. Default NULL. Helpful in case data contains large number of variants belonging to polymorphic genes such as mucins and TTN.\\n#\' @return data table of genes ordered according to p-values.\\n#\' @seealso \\\\code{\\\\link{plotOncodrive}}\\n#\' @examples\\n#\'\\n#\' laml.maf <- system.file(\\"extdata\\", \\"tcga_laml.maf.gz\\", package = \\"maftools\\")\\n#\' laml <- read.maf(maf = laml.maf, removeSilent = TRUE, useAll = FALSE)\\n#\' laml.sig <- oncodrive(maf = laml, AACol = \'Protein_Change\', minMut = 5)\\n#\'\\n#\'\\n#\' @importFrom dplyr filter\\n#\' @export\\n\\n\\noncodrive = function(maf, AACol = NULL, minMut = 5, pvalMethod = \'zscore\', nBgGenes = 100, bgEstimate = TRUE, ignoreGenes = NULL){\\n\\n  #Proetin Length source\\n  gl = system.file(\'extdata\', \'prot_len.txt.gz\', package = \'maftools\')\\n\\n  if(Sys.info()[[\'sysname\']] == \'Windows\'){\\n    gl.gz = gzfile(description = gl, open = \'r\')\\n    gl <- suppressWarnings( data.table(read.csv( file = gl.gz, header = TRUE, sep = \'\\\\t\', stringsAsFactors = FALSE)) )\\n    close(gl.gz)\\n  } else{\\n    gl = fread(input = paste(\'zcat <\', gl), sep = \'\\\\t\', stringsAsFactors = FALSE)\\n  }\\n\\n  pval.options = c(\'zscore\', \'poisson\', \'combined\')\\n\\n  if(!pvalMethod %in% pval.options){\\n    stop(\'pvalMethod can only be either zscore, poisson or combined\')\\n  }\\n\\n  if(length(pvalMethod) > 1){\\n    stop(\'pvalMethod can only be either zscore, poisson or combined\')\\n  }\\n\\n\\n  #syn variants for background\\n  syn.maf = maf@maf.silent\\n  #number of samples in maf\\n  numSamples = as.numeric(maf@summary[3,summary])\\n  #Perform clustering and calculate background scores.\\n  if(bgEstimate){\\n    if(nrow(syn.maf) == 0){\\n      message(\'No syn mutations found! Skipping background estimation. Using predefined values. (Mean = 0.279; SD = 0.13)\')\\n      bg.mean = 0.279\\n      bg.sd = 0.13\\n    }else{\\n      message(\'Estimating background scores from synonymous variants..\')\\n      syn.bg.scores = parse_prot(dat = syn.maf, AACol = AACol, gl, m = minMut, calBg = TRUE, nBg = nBgGenes)\\n\\n      #If number of genes to calculate background scores is not enough, use predefined scores.\\n      if(is.null(syn.bg.scores)){\\n        message(\\"Not enough genes to build background. Using predefined values. (Mean = 0.279; SD = 0.13)\\")\\n        bg.mean = 0.279\\n        bg.sd = 0.13\\n      }else {\\n        if(nrow(syn.bg.scores) < nBgGenes){\\n          message(\\"Not enough genes to build background. Using predefined values. (Mean = 0.279; SD = 0.13)\\")\\n          bg.mean = 0.279\\n          bg.sd = 0.13\\n        }else{\\n          bg.mean = mean(syn.bg.scores$clusterScores)\\n          bg.sd = sd(syn.bg.scores$clusterScores)\\n          message(paste(\'Estimated background mean: \', bg.mean))\\n          message(paste(\'Estimated background SD: \', bg.sd))\\n        }\\n      }\\n    }\\n  }else{\\n    message(\\"Using predefined values for background. (Mean = 0.279; SD = 0.13)\\")\\n    bg.mean = 0.279\\n    bg.sd = 0.13\\n  }\\n\\n\\n\\n  #non-syn variants\\n  non.syn.maf = maf@data\\n  #in case user read maf without removing silent variants, remove theme here.\\n  silent = c(\\"3\'UTR\\", \\"5\'UTR\\", \\"3\'Flank\\", \\"Targeted_Region\\", \\"Silent\\", \\"Intron\\",\\n             \\"RNA\\", \\"IGR\\", \\"Splice_Region\\", \\"5\'Flank\\", \\"lincRNA\\")\\n  non.syn.maf = non.syn.maf[!Variant_Classification %in% silent] #Remove silent variants from main table\\n\\n  #Remove genes to ignore\\n  if(!is.null(ignoreGenes)){\\n    ignoreGenes.count = nrow(non.syn.maf[Hugo_Symbol %in% ignoreGenes])\\n    message(paste(\'Removed\', ignoreGenes.count, \'variants belonging to\', paste(ignoreGenes, collapse = \', \', sep=\',\')))\\n    non.syn.maf = non.syn.maf[!Hugo_Symbol %in% ignoreGenes]\\n  }\\n\\n  #Perform clustering and calculate cluster scores for nonsyn variants.\\n  message(\'Estimating cluster scores from non-syn variants..\')\\n  nonsyn.scores = parse_prot(dat = non.syn.maf, AACol = AACol, gl = gl, m = minMut, calBg = FALSE, nBg = nBgGenes)\\n\\n  if(pvalMethod == \'combined\'){\\n    message(\'Comapring with background model and estimating p-values..\')\\n    nonsyn.scores$zscore = (nonsyn.scores$clusterScores - bg.mean) / bg.sd\\n    nonsyn.scores$tPval = 1- pnorm(nonsyn.scores$zscore)\\n    nonsyn.scores$tFdr = p.adjust(nonsyn.scores$tPval, method = \'fdr\')\\n\\n    nonsyn.scores = merge(getGeneSummary(maf), nonsyn.scores, by = \'Hugo_Symbol\')\\n    nonsyn.scores[,fract_muts_in_clusters := muts_in_clusters/total]\\n\\n    counts.glm = glm(formula = total ~ protLen+clusters, family = poisson(link = identity), data = nonsyn.scores) #Poisson model\\n    nonsyn.scores$Expected = counts.glm$fitted.values #Get expected number of events (mutations) from the model\\n\\n    observed_mut_colIndex = which(colnames(nonsyn.scores) == \'total\')\\n    expected_mut_colIndex = which(colnames(nonsyn.scores) == \'Expected\')\\n\\n    #Poisson test to caluclate difference (p-value)\\n    nonsyn.scores$poissonPval = apply(nonsyn.scores, 1, function(x) {\\n      poisson.test(as.numeric(x[observed_mut_colIndex]), as.numeric(x[expected_mut_colIndex]))$p.value\\n    })\\n\\n    nonsyn.scores$poissonFdr = p.adjust(nonsyn.scores$poissonPval)\\n    nonsyn.scores = nonsyn.scores[order(poissonFdr)]\\n\\n    nonsyn.scores$fdr = apply(nonsyn.scores[,.(tFdr, poissonFdr)], MARGIN = 1, FUN = min)\\n\\n  } else if(pvalMethod == \'zscore\'){\\n    #Oncodrive clust way of caluclating pvalues\\n    #Calculate z scores; compare it to bg scores and estimate z-score, pvalues, corrected pvalues (fdr) (assumes normal distribution)\\n    message(\'Comapring with background model and estimating p-values..\')\\n    nonsyn.scores$zscore = (nonsyn.scores$clusterScores - bg.mean) / bg.sd\\n    nonsyn.scores$pval = 1- pnorm(nonsyn.scores$zscore)\\n    nonsyn.scores$fdr = p.adjust(nonsyn.scores$pval, method = \'fdr\')\\n\\n    nonsyn.scores = merge(getGeneSummary(maf), nonsyn.scores, by = \'Hugo_Symbol\')\\n    nonsyn.scores[,fract_muts_in_clusters := muts_in_clusters/total]\\n    #nonsyn.scores[,fract_MutatedSamples := MutatedSamples/numSamples]\\n    nonsyn.scores = nonsyn.scores[order(fdr)]\\n  }else{\\n    #Assuming poisson distribution of mutation counts\\n    #Now model observed number of mutations as a function of number of clusters and protein length. Calculate expected number of events based on poisson distribution.\\n    nonsyn.scores = merge(getGeneSummary(maf), nonsyn.scores, by = \'Hugo_Symbol\')\\n    nonsyn.scores[,fract_muts_in_clusters := muts_in_clusters/total]\\n\\n    counts.glm = glm(formula = total ~ protLen+clusters, family = poisson(link = identity), data = nonsyn.scores) #Poisson model\\n    nonsyn.scores$Expected = counts.glm$fitted.values #Get expected number of events (mutations) from the model\\n\\n    observed_mut_colIndex = which(colnames(nonsyn.scores) == \'total\')\\n    expected_mut_colIndex = which(colnames(nonsyn.scores) == \'Expected\')\\n\\n    #Poisson test to caluclate difference (p-value)\\n    nonsyn.scores$pval = apply(nonsyn.scores, 1, function(x) {\\n      poisson.test(as.numeric(x[observed_mut_colIndex]), as.numeric(x[expected_mut_colIndex]))$p.value\\n    })\\n\\n    nonsyn.scores$fdr = p.adjust(nonsyn.scores$pval)\\n    nonsyn.scores = nonsyn.scores[order(fdr)]\\n  }\\n  message(\'Done !\')\\n  return(nonsyn.scores)\\n}\\n" }\n'
line: b'{ "repo_name": "PoisonAlien/maftools", "ref": "refs/heads/master", "path": "R/titv.R", "content": "#\' Classifies SNPs into transitions and transversions\\n#\' @description takes output generated by read.maf and classifies Single Nucleotide Variants into Transitions and Transversions.\\n#\'\\n#\' @param maf an \\\\code{\\\\link{MAF}} object generated by \\\\code{\\\\link{read.maf}}\\n#\' @param useSyn Logical. Whether to include synonymous variants in analysis. Defaults to FALSE.\\n#\' @param plot plots a titv fractions. default TRUE.\\n#\' @param file basename for output file name. If given writes summaries to output file. Default NULL.\\n#\' @return list of \\\\code{data.frame}s with Transitions and Transversions summary.\\n#\' @seealso  \\\\code{\\\\link{plotTiTv}}\\n#\' @examples\\n#\' laml.maf <- system.file(\\"extdata\\", \\"tcga_laml.maf.gz\\", package = \\"maftools\\")\\n#\' laml <- read.maf(maf = laml.maf, removeSilent = TRUE, useAll = FALSE)\\n#\' laml.titv = titv(maf = laml, useSyn = TRUE)\\n#\'\\n#\' @export\\n\\n\\ntitv = function(maf, useSyn = FALSE, plot = TRUE, file = NULL)\\n{\\n\\n  #Synonymous variants\\n  maf.silent = maf@maf.silent\\n  #Main data\\n  maf = maf@data\\n\\n  #in case user read maf without removing silent variants, remove theme here.\\n  silent = c(\\"3\'UTR\\", \\"5\'UTR\\", \\"3\'Flank\\", \\"Targeted_Region\\", \\"Silent\\", \\"Intron\\",\\n             \\"RNA\\", \\"IGR\\", \\"Splice_Region\\", \\"5\'Flank\\", \\"lincRNA\\")\\n  maf = maf[!Variant_Classification %in% silent] #Remove silent variants from main table\\n\\n  if(useSyn){\\n    maf = rbind(maf, maf.silent, fill = TRUE)\\n  }\\n\\n  maf = maf[Variant_Type == \'SNP\']\\n\\n  #Some TCGA studies have Start_Position set to as \'position\'. Change if so.\\n  if(length(grep(pattern = \'Start_position\', x = colnames(maf))) > 0){\\n    colnames(maf)[which(colnames(maf) == \'Start_position\')] = \'Start_Position\'\\n  }\\n\\n  if(length(grep(pattern = \'End_position\', x = colnames(maf))) > 0){\\n    colnames(maf)[which(colnames(maf) == \'End_position\')] = \'End_Position\'\\n  }\\n\\n  maf = maf[,.(Hugo_Symbol, Start_Position, End_Position, Reference_Allele, Tumor_Seq_Allele2, Tumor_Sample_Barcode)]\\n  maf$con = paste(maf[,Reference_Allele], maf[,Tumor_Seq_Allele2], sep = \'-\')\\n\\n  maf.con.summary = maf[,.N, by = .(Tumor_Sample_Barcode, con)]\\n  maf.con.summary$con.class = suppressWarnings(as.character(factor(maf.con.summary$con, levels = c(\\"A-G\\", \\"T-C\\", \\"C-T\\", \\"G-A\\", \\"A-T\\", \\"T-A\\", \\"A-C\\", \\"T-G\\", \\"C-A\\", \\"G-T\\", \\"C-G\\", \\"G-C\\"),\\n                                                                   labels = c(\\"A-G\\", \\"A-G\\", \\"C-T\\", \\"C-T\\", \\"A-T\\", \\"A-T\\", \\"A-C\\", \\"A-C\\", \\"C-A\\", \\"C-A\\", \\"C-G\\", \\"C-G\\"))))\\n\\n\\n  maf.con.class.summary = maf.con.summary[,sum(N), by = .(Tumor_Sample_Barcode, con.class)]\\n  colnames(maf.con.class.summary)[ncol(maf.con.class.summary)] = \'nVars\'\\n  suppressWarnings(maf.con.class.summary[,fract := (nVars/sum(nVars))*100, by = .(Tumor_Sample_Barcode)])\\n\\n  maf.con.class.summary$con.class = factor(x = maf.con.class.summary$con.class,\\n                                           levels = c(\\"A-G\\", \\"C-T\\", \\"A-T\\", \\"A-C\\", \\"C-A\\", \\"C-G\\"))\\n  maf.con.class.summary$TiTv = suppressWarnings(as.character(factor(x = maf.con.class.summary$con.class,\\n                                                                    levels = c(\\"A-G\\", \\"C-T\\", \\"A-T\\", \\"A-C\\", \\"C-A\\", \\"C-G\\"), labels = c(\'Ti\', \'Ti\', \'Tv\', \'Tv\', \'Tv\', \'Tv\'))))\\n\\n  fract.classes = data.table::dcast(data = maf.con.class.summary, formula = Tumor_Sample_Barcode ~ con.class, value.var = \'fract\', fill = 0)\\n  raw.classes = data.table::dcast(data = maf.con.class.summary, formula = Tumor_Sample_Barcode ~ con.class, value.var = \'nVars\', fill = 0)\\n  titv.summary = maf.con.class.summary[,sum(fract), by = .(Tumor_Sample_Barcode, TiTv)]\\n  titv.summary = data.table::dcast(data = titv.summary, Tumor_Sample_Barcode ~ TiTv, value.var = \'V1\', fill = 0)\\n\\n  titv.res = list(fraction.contribution = fract.classes, raw.counts = raw.classes, TiTv.fractions = titv.summary)\\n\\n  if(plot){\\n    plotTiTv(res = titv.res)\\n  }\\n\\n  if(!is.null(file)){\\n    write.table(x = fract.classes,file = paste(file, \'_fraction_contribution.txt\', sep = \'\'), quote = FALSE, row.names = FALSE, sep = \'\\\\t\')\\n    write.table(x = raw.classes,file = paste(file, \'_fraction_counts.txt\', sep = \'\'), quote = FALSE, row.names = FALSE, sep = \'\\\\t\')\\n    write.table(x = titv.summary,file = paste(file, \'_TiTv_fractions.txt\', sep = \'\'), quote = FALSE, row.names = FALSE, sep = \'\\\\t\')\\n  }\\n\\n  return(titv.res)\\n}\\n" }\n'
line: b'{ "repo_name": "joyent/Rbunyan", "ref": "refs/heads/master", "path": "R/bunyanTracebackErrors.R", "content": "# Roxygen Comments bunyanTracebackErrors\\n#\' Count of errors above level threshold of 50 after setpoint\\n#\'\\n#\' Returns the number of ERROR/FATAL log messages\\n#\' encountered since bunyanSetpoint first called. Note that\\n#\' only the first call to bunyanSetpoint is used, subsequent\\n#\' calls are ignored. Use bunyanClearSetpoint to clear before\\n#\' setting a new setpoint.\\n#\'\\n#\'\\n#\' @keywords bunyan, setpoint\\n#\'\\n#\' @export\\nbunyanTracebackErrors <-\\nfunction() {\\n    return(bunyan_globals$errorssincemark)\\n}\\n" }\n'
line: b'{ "repo_name": "henkelstone/NPEL.Classification", "ref": "refs/heads/master", "path": "data-raw/constants.R", "content": "# Create package constants\\n\\n##### Scope Constants #####\\nsuppModels <- c(\'randomForest\',\'rfsrc\',\'fnn.FNN\',\'fnn.class\',\'kknn\',\'gbm\')          # Which packages (model classes) are supported by NPEL.Classification\\nprobModels <- c(\'randomForest\',\'rfsrc\',\'fnn.FNN\',\'fnn.class\',\'kknn\',\'gbm\')          # Which packages generate probabilities when passed categorical data\\ncontModels <- c(\'randomForest\',\'rfsrc\',\'kknn\',\'gbm\')                                # Which packages are able to handle continuous data }\\n\\n##### Generate Ecotype Groupings #####\\n# Create a shell to fill with data\\necoGroup <- list(); length(ecoGroup) <- 12;\\ndim(ecoGroup) <- c(4,3)                                                   # Three rows (different classifiers), and three columns (type of data we could extract)\\nrownames(ecoGroup)=c(\'identity\',\'domSpecies\',\'maxGranularity\',\'domGroup\') # Rows are different grouping scenarios\\ncolnames(ecoGroup)=c(\'transform\',\'labels\',\'colours\')                      # Cols are different things we could look up\\n\\n# Fill with data\\necoGroup[\'identity\',] <- list(c(1:27), paste0(\'BS\',1:27), 1:27)\\necoGroup[\'domSpecies\',] <- list( c(1,1,2,2,2,2,3,3,3,3,4,4,5,5,6,7,8,8,8,8,9,9,9,9,9,10,10),\\n                                 c(\'1\'=\\"Barren\\",\'2\'=\\"Pine\\",\'3\'=\\"Black Sp\\",\'4\'=\\"White Sp\\",\'5\'=\\"Birch\\",\'6\'=\\"Aspen\\",\'7\'=\\"Swamp\\",\'8\'=\\"Bog\\",\'9\'=\\"Fen\\",\'10\'=\\"Shore\\"),\\n                                 c(\'1\'=\\"gray\\",\'2\'=\\"tan1\\",\'3\'=\\"green4\\",\'4\'=\\"seagreen\\",\'5\'=\\"green3\\",\'6\'=\\"green2\\",\'7\'=\\"steelblue\\",\'8\'=\\"steelblue2\\",\'9\'=\\"steelblue4\\",\'10\'=\\"azure\\") )\\necoGroup[\'maxGranularity\',] <- list( c(1,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,15,16,17,17,18,18,19,20,20,21,21), paste0(\'BS\',1:21), 1:21)\\necoGroup[\'domGroup\',] <- list( c(1,1,2,2,2,2,2,2,2,2,2,2,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4),\\n                               c(\\"Barren\\",\\"Conifer\\",\\"Decid\\",\\"Wetland\\"),\\n                               c(\'1\'=\\"gray\\",\'2\'=\\"green4\\",\'3\'=\\"green2\\",\'4\'=\\"steelblue\\") )\\n" }\n'
line: b'{ "repo_name": "henkelstone/NPEL.Classification", "ref": "refs/heads/master", "path": "tests/testthat/test_package.R", "content": "# Automated testing of NPEL.Classification package\\n# Created by Jonathan Henkelman 16.Feb.2016\\n\\nlibrary(NPEL.Classification)\\ncontext(\\"Package\\")\\ncat(\'\\\\n\')\\n\\ntest_examples(\'../../man\')\\n# testthat::test_examples(\'./man\')\\n" }\n'
line: b'{ "repo_name": "henkelstone/NPEL.Classification", "ref": "refs/heads/master", "path": "tests/testthat/test_util.R", "content": " # Automated testing of the \'util.R\' file in the NPEL package\\n# Created by Jonathan Henkelman 27.Jan.2016\\n\\nlibrary(NPEL.Classification)\\nlibrary(testthat)\\ncontext(\\"Util Functions\\")\\ncat(\'\\\\n\')\\n\\n# Setup testing environment\\nfacA <- c(2,3,5,1);     lvlA <- c(3:5,1:2)\\nfacB <- c(12,13,15,11); lvlB <- c(13:15,11:12)\\ntFacA <- factor (facA, levels=lvlA)\\ntFacB <- factor (facB, levels=lvlB)\\nfacC <- factor(NULL)\\n\\ntest_that(\\"factorValues\\", {\\n  expect_equal(factorValues(tFacA), facA)\\n  expect_equal(factorValues(tFacB), facB)\\n  expect_equal(class(factorValues(facC)), \'numeric\')\\n  expect_equal(length(factorValues(facC)), 0)\\n})\\n\\ntest_that(\\"sortLevels\\", {\\n  sFac <- sortLevels(tFacA)\\n  expect_equal(factorValues(sFac), facA)\\n  expect_equal(as.numeric(levels(sFac)), c(1:5))\\n  sFac <- sortLevels(facC)\\n  expect_equal(class(sFac), \'factor\')\\n  expect_equal(length(sFac), 0)\\n})\\n\\ntest_that(\\"trimLevels\\", {\\n  sFac <- trimLevels(tFacA)\\n  expect_equal(factorValues(sFac), facA)\\n  expect_equal(as.numeric(levels(sFac)), c(3,5,1,2))\\n  sFac <- trimLevels(facC)\\n  expect_equal(class(sFac), \'factor\')\\n  expect_equal(length(sFac), 0)\\n})\\n\\ntest_that(\\"mergeLevels\\", {\\n  sFac <- mergeLevels(tFacA,tFacB)\\n  expect_equal(factorValues(sFac), facA)\\n  expect_equal(as.numeric(levels(sFac)), sort(c(lvlA,lvlB)))\\n  sFac <- mergeLevels(tFacA,facC)\\n  expect_equal(sFac, sortLevels(tFacA))\\n})\\n\\ntest_that(\\"fx2vars\\", {\\n  expect_error(fx2vars(),\\"fx2vars: needs some data; supply either a formula, x and y, and/or a list of names.\\")\\n  fx <- x <- y <- NULL; fx2vars(formula(\'a~b+c\'),x,y);                                   expect_equal(x,c(\'b\',\'c\')); expect_equal(y,\'a\')\\n  fx <- x <- y <- NULL; fx2vars(fx,c(\'b\',\'c\'),\'a\');                                      expect_equal(fx,formula(\'a~b+c\'))\\n  fx <- x <- y <- NULL; fx2vars(fx,x,y,names=c(\'a\',\'b\',\'c\'));                            expect_equal(fx,formula(\'a~b+c\')); expect_equal(x,c(\'b\',\'c\')); expect_equal(y,\'a\')\\n\\n  fx <- x <- y <- NULL; expect_error(fx2vars(fx,c(\'b\',\'c\'),\'a\',names=c(\'a\',\'b\')),        \\"fx2vars: a column specified for x does not occur in the dataset.\\")\\n  fx <- x <- y <- NULL; expect_error(fx2vars(formula(\'a~b+c\'),x,y,names=c(\'a\',\'b\')),     \\"fx2vars: a column specified for x does not occur in the dataset.\\")\\n  fx <- x <- y <- NULL; expect_error(fx2vars(formula(\'a+d~b+c\'),x,y),                    \\"fx2vars: multivariate analysis not supported; specify only a single variable for y.\\")\\n  fx <- x <- y <- NULL; expect_error(fx2vars(fx,c(\'b\',\'c\'),c(\'a\',\'d\')),                  \\"fx2vars: multivariate analysis not supported; specify only a single variable for y.\\")\\n  fx <- x <- y <- NULL; expect_error(fx2vars(formula(\'d~b+c\'),x,y,names=c(\'a\',\'b\',\'c\')), \\"fx2vars: variable specified for y does not occur in the dataset; check the column names.\\")\\n  fx <- x <- y <- NULL; expect_error(fx2vars(fx,c(\'b\',\'c\'),\'d\',names=c(\'a\',\'b\',\'c\')),    \\"fx2vars: variable specified for y does not occur in the dataset; check the column names.\\")\\n  fx <- x <- y <- NULL; expect_error(fx2vars(fx,x,\'a\'),                                  \\"fx2vars: cannot find a suitable default for x variables; provide fx and/or names.\\")\\n  fx <- x <- y <- NULL; expect_error(fx2vars(fx,c(\'b\',\'d\'),\'a\',names=c(\'a\',\'b\',\'c\')),    \\"fx2vars: a column specified for x does not occur in the dataset.\\")\\n  fx <- x <- y <- NULL; expect_error(fx2vars(fx,c(\'a\',\'b\',\'c\'),\'a\'),                     \\"fx2vars: x cannot contain the y variable.\\")\\n  fx <- x <- y <- NULL; expect_error(fx2vars(formula(\'a~a+b+c\'),x,y),                    \\"fx2vars: x cannot contain the y variable.\\")\\n})\\n" }\n'
line: b'{ "repo_name": "henkelstone/NPEL.Classification", "ref": "refs/heads/master", "path": "R/Plot.R", "content": "# Functions used to plot tiles and output\\n# Created 9.Oct.2015 from pre-existing code file started 6.Apr.2015\\n\\n##### plotTile.base #####\\n#\' Creates a base ggplot object of the raster object\\n#\'\\n#\' @param data a raster* object to plot\\n#\' @param layers (optional) the layers to include as facets\\n#\' @param title (optional) a title for the plot\\n#\' @param maxpixels (optional) the maximum number of pixels to output\\n#\' @param reduction (optional) is a reducing scale factor applied to both x and y axis; will use whichever is less: ncell/(reduction^2) or maxpixels\\n#\' @param ... other parameters for ggplot\\n#\' @return a ggplot object which can have geoms, scales, etc. added to it (see PlotTile).\\n#\' @export\\nplotTile.base <- function(data, layers=NULL, title=\\"\\", maxpixels=500000, reduction=1, ...) {\\n  if (!requireNamespace(\'ggplot2\')) stop(\\"ggplot2 package required for plotting\\")\\n  if (!is.null(layers)) data <- subset(data,layers)\\n\\n  # Theres a glitch in sampleRegular: we can\'t just simply ask for sR(data, number of cells, xy=TRUE) as it doesn\'t return the same values\\n  # for xy as if we do it step by step: x <- sR(data,#cells,xy=FALSE,raster=TRUE); xyFromCell(x,1:#cells). I don\'t know why this is, but it\\n  # causes artifacts in the final plot so it is necessary to do the work around shown here... It just means we have to extract the data\\n  # twice so it\'s a bit slower.\\n  colNames <- names(data)\\n  data <- raster::sampleRegular(data, size=min(raster::ncell(data)/(reduction*reduction),maxpixels),asRaster=TRUE)   # Return a raster so we can extract the coords afterwards.\\n  data <- data.frame(raster::xyFromCell(data,1:raster::ncell(data)), raster::getValues(data))\\n  names(data) <- c(\'x\',\'y\',colNames)\\n  dat <- reshape(data=data,direction=\'long\',idvar=1:2,varying=3:dim(data)[2],v.names=\'value\',timevar=\'type\',times=names(data)[3:dim(data)[2]]) # Massage into long form; over a GB for a full tile!\\n\\n  # Create a base plot object with some useful aesthetics and that holds the data; then if this is not the only plot the user wants, they should be able to output more without needing to resample the data\\n  return( ggplot2::ggplot(ggplot2::aes(x = \'x\', y = \'y\'), data = \'dat\', ...) +\\n            ggplot2::theme(axis.text.y=ggplot2::element_text(angle=90,hjust=0.5)) + ggplot2::coord_equal() +\\n            ggplot2::labs(title=title,x=\'Latitude (UTM)\',y=\'Longitude (UTM)\',fill=\'Value\') )\\n}\\n\\n##### plotTile #####\\n#\' Given a ggplot base object create a finished plot\\n#\' @param gp the base ggplot to which to add a colour scale and aesthetic (see plotTile.base)\\n#\' @param layers a character vector of the levels which to plot\\n#\' @param discrete force the plot to attempt a discrete fill axis\\n#\' @param colours specify the colours use; a list if discrete is TRUE, or, 2 or 3 colours for the gradient if discrete is FALSE\\n#\' @param labels if desired, specify the names of the labels for each class\\n#\' @param ... other parameters to pass\\n#\' @return the ggplot object with additional aes and scale\\nplotTile <- function(gp, layers, discrete, colours, labels=NULL,...){\\n\\n  # Note: ggplot saves the data in the object which both allows this, but also makes for a large object... like over a GB for a full tile -- I recommend against\\n  # saving the R database on exit without being sure you have time for this file to save/load.\\n\\n  if (!requireNamespace(\'ggplot2\')) stop(\\"ggplot2 package required for plotting\\")\\n  if (discrete) { if (is.null(colours)) stop (\\"plotTile: if discrete is specified, so must be colours\\") }\\n  else          { if (is.null(colours) || !sum(length(colours) == c(2,3))) stop(\\"plotTile: either 2 or 3 colours need to be specified for continuous gradient scales\\") }\\n\\n  gp <- ggplot2::`%+%`(gp, subset(gp$data,gp$data$type %in% layers))   # Subset the data. Necessary to do it this way so we don\'t have to specify the subset to every item, i.e. scale, geom, facet, etc.\\n  if (length(list(...))) gp <- gp + ...                   # Add any extra parameters the user may have supplied\\n  if (discrete) {                                         # If it\'s discrete then the user should have supplied the colours\\n    if (is.null(labels)) return( gp + ggplot2::aes(fill=\'factor(value)\') + ggplot2::geom_raster() + ggplot2::scale_fill_manual(values=colours) )\\n    else                 return( gp + ggplot2::aes(fill=\'factor(value)\') + ggplot2::geom_raster() + ggplot2::scale_fill_manual(values=colours, labels=labels) )\\n  } else {\\n    if (length (colours) == 2) return( gp + ggplot2::aes(fill=\'value\') + ggplot2::geom_raster() + ggplot2::scale_fill_gradient (low=colours[1], high=colours[2]) + ggplot2::facet_wrap(\'~type\') )\\n    if (length (colours) == 3) return( gp + ggplot2::aes(fill=\'value\') + ggplot2::geom_raster() + ggplot2::scale_fill_gradient2(low=colours[1], mid=colours[2], high=colours[3], midpoint=(max(gp$data$value,na.rm=TRUE)-min(gp$data$value,na.rm=TRUE))/2) + ggplot2::facet_wrap(\'~type\') )\\n  }\\n}\\n\\n##### plotTiles #####\\n#\' Outputs plots of multiple data files into a single folder\\n#\' @param path the path of the folder\\n#\' @param base the base part of the filename\\n#\' @param extension the filename \'extension\'; may contain more than the strict extension\\n#\' @param models a list of model names\\n#\' @param type (optional) type out output to use\\n#\' @param ... other parameters to pass to the device function\\nplotTiles <- function(path, base, extension, models, type=\'pdf\',...) {\\n  for (fType in models) {\\n    fName <- paste0(path,base,fType,extension)\\n    Tile <- raster::brick(paste0(fName,\'.tif\'))\\n    if (raster::nlayers(Tile) > 1) { names(Tile) <- c(\'Class\', \'Prob\', paste0(\'Prob.\',1:(raster::nlayers(Tile)-2))) } else { names(Tile) <- c(\'Class\') }\\n\\n    if (type==\'png\') png(paste0(fName,\'.png\'),...)\\n    else pdf(paste0(fName,\'.pdf\'),width=10.0,height=7.5,onefile=TRUE)\\n\\n    gp <- plotTile.base(Tile, layers=names(Tile), maxpixels=1000000)\\n    plotTile(gp, layers=\'Class\', discrete=TRUE, colours=NPEL.Classification::ecoGroup[[\'domSpecies\',\'colours\']]) #,aes(alpha=\'Prob\'))\\n    if (fType != \'fnn\') plotTile(gp, layers=c(\'Prob\',paste(\'Prob\',1:7,sep=\'.\')), discrete=FALSE, colours=c(\'grey17\',\'red\',\'yellow\'))\\n    #    Sys.sleep(15)\\n    dev.off(dev.cur())\\n  }\\n}\\n" }\n'
line: b'{ "repo_name": "rgriff23/btw", "ref": "refs/heads/master", "path": "R/killbt.R", "content": "killbt = function() {\\n\\tjobs = system(\\"pgrep BayesTraits\\", intern=T)\\n\\tfor (n in 1:length(jobs)) {system(paste(\\"kill\\", jobs[n]))}\\n}\\n\\n" }\n'
line: b'{ "repo_name": "fcharte/CursoCienciaDatosR", "ref": "refs/heads/master", "path": "instalaPaquetes.R", "content": "install.packages(\\"caret\\")\\n# LINUX: system(\'wajig install libgtk2.0-dev\') # Use \'sudo apt-get install wajig\' at the command line if needed, then install Gtk headers\\ninstall.packages(\'RGtk2\')\\ninstall.packages(\\"rattle\\")\\ninstall.packages(\\"neuralnet\\")\\ninstall.packages(\\"autoencoder\\")\\ninstall.packages(\\"ggplot2\\")\\ninstall.packages(\\"h2o\\", dependencies = c(\\"Depends\\", \\"Suggests\\"))\\ninstall.packages(\\"pROC\\")\\ninstall.packages(\\"arules\\")\\ninstall.packages(\\"arulesViz\\")\\ninstall.packages(\\"fpc\\")\\n\\nlibrary(rattle)  # WINDOWS: Choose to install Gtk\\nrattle()\\n\\nlibrary(h2o)\\nlocalH2O <- h2o.init() # It should indicate that H2O is initialized, as well as the Java VM version\\n\\n# Download https://github.com/fcharte/CursoCienciaDatosR/blob/master/data/datosTrabajo.RData and save it in working directory\\n\\nload(\'datosTrabajo.RData\')\\n# Look for the previous file at the \\"Files\\" panel, and click it to load it. All the data should appear in the Environment tab\\n\\nlibrary(caret)\\nlibrary(neuralnet)\\nlibrary(autoencoder)\\nlibrary(ggplot2)\\n" }\n'
line: b'{ "repo_name": "rich-iannone/PuffR", "ref": "refs/heads/master", "path": "R/calmet_get_ncdc_history.R", "content": "#\' Retrieve the NCDC history data file\\n#\' @description This function initiates a download of the NCDC surface station history file.\\n#\' @param replace.file selecting \'yes\' will overwrite history file if it exists in the working directory.\\n#\' @export calmet_get_ncdc_history\\n#\' @examples\\n#\' \\\\dontrun{\\n#\' # Obtain the NCDC history file\\n#\' calmet_get_ncdc_history()\\n#\'}\\n\\ncalmet_get_ncdc_history <- function(replace.file = FALSE) {\\n  \\n  # Get hourly surface data history CSV from NOAA/NCDC FTP\\n  file <- \\"ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-history.csv\\"\\n  \\n  if (replace.file == TRUE) {\\n    repeat {\\n      \\n      try(download.file(file, \\"ish-history.csv\\", quiet = TRUE))\\n      \\n      if (file.info(\\"ish-history.csv\\")$size > 0) { break }\\n      \\n    }\\n    \\n  } else { }\\n  \\n  # Check if file exists in working directory\\n  if (file.exists(\\"ish-history.csv\\") &\\n        file.info(\\"ish-history.csv\\")$size > 0 &\\n        replace.file == FALSE) { } else { \\n          \\n          repeat {\\n            \\n            try(download.file(file, \\"ish-history.csv\\", quiet = TRUE))\\n            \\n            if (file.info(\\"ish-history.csv\\")$size > 0) { break }\\n            \\n          }\\n          \\n        }\\n  \\n  # Read in the \\"ish-history\\" CSV file\\n  st <- read.csv(\\"ish-history.csv\\")\\n  \\n}\\n" }\n'
line: b'{ "repo_name": "rich-iannone/PuffR", "ref": "refs/heads/master", "path": "R/calpuff_add_area_sources.R", "content": "#\' Add area sources to a list for later use in CALPUFF\\n#\' @description Add area sources to a list for later use in CALPUFF\\n#\' @param src_name the name of the source emitting the species.\\n#\' @param species_name the name of the species undergoing emissions.\\n#\' @param lat_dec_deg a vector of 4 latitude values for the area source in units of decimal degrees.\\n#\' @param lon_dec_deg a vector of 4 longitude values for the area source in units of decimal degrees.\\n#\' @param x_coord_km a vector of 4 UTM easting values for the area source in km units.\\n#\' @param y_coord_km a vector of 4 UTM northing values for the area source in km units.\\n#\' @param UTM_zone the UTM zone for the area source.\\n#\' @param UTM_hemisphere the UTM hemisphere for the area source.\\n#\' @param effective_height the effective height of the area source in meters above ground level (m AGL).\\n#\' @param base_elev the ground elevation at the location of the area source in meters above sea level (m ASL).\\n#\' @param init_sigma_z the initial sigma z value for the area source in meters.\\n#\' @param emission_rate the rate of constant emissions from the area source; units are defined in the \'emission_units\' argument.\\n#\' @param emission_units the units applied to the value defined in the \'emission_rate\' argument. The possible selections are: (1) \\"g/m2/s\\", (2) \\"kg/m2/hr\\", (3) \\"lb/m2/hr\\", (4) \\"tons/m2/yr\\", (5) \\"Odour Unit * m/s\\", (6) \\"Odour Unit * m/min\\", (7) \\"metric tons/m2/yr\\", (8) \\"Bq/m2/s\\", and (9) \\"GBq/m2/yr\\".\\n#\' @export calpuff_add_area_sources\\n\\ncalpuff_add_area_sources <- function(src_name,\\n                                     species_name,\\n                                     lat_dec_deg = NULL,\\n                                     lon_dec_deg = NULL,\\n                                     x_coord_km = NULL,\\n                                     y_coord_km = NULL,\\n                                     UTM_zone = NULL,\\n                                     UTM_hemisphere = NULL,\\n                                     effective_height,\\n                                     base_elev,\\n                                     init_sigma_z,\\n                                     emission_rate,\\n                                     emission_units){\\n  \\n  # Add require statements\\n  require(rgdal)\\n  require(raster)\\n  require(stringr)\\n  require(plyr)\\n  \\n  # Get expected filename for area sources\\n  area_sources_filename <-\\n    paste0(unlist(str_split(getwd(),\\n                            pattern = \\"/\\"))[length(unlist(str_split(getwd(),\\n                                                                    pattern = \\"/\\")))],\\n           \\"--area_sources.txt\\")\\n  \\n  # Create area sources text file with header if it doesn\'t exist\\n  if (file.exists(area_sources_filename) == FALSE){\\n    \\n    # Create empty file in working folder\\n    file.create(area_sources_filename)\\n    \\n    # Add header row to new area sources file\\n    cat(paste0(\\"src_name\\", \\",\\",\\n               \\"species_name\\", \\",\\",\\n               \\"lat_dec_deg_1\\", \\",\\",\\n               \\"lon_dec_deg_1\\", \\",\\",\\n               \\"lat_dec_deg_2\\", \\",\\",\\n               \\"lon_dec_deg_2\\", \\",\\",\\n               \\"lat_dec_deg_3\\", \\",\\",\\n               \\"lon_dec_deg_3\\", \\",\\",\\n               \\"lat_dec_deg_4\\", \\",\\",\\n               \\"lon_dec_deg_4\\", \\",\\",\\n               \\"x_coord_km_1\\", \\",\\",\\n               \\"y_coord_km_1\\", \\",\\",\\n               \\"x_coord_km_2\\", \\",\\",\\n               \\"y_coord_km_2\\", \\",\\",\\n               \\"x_coord_km_3\\", \\",\\",\\n               \\"y_coord_km_3\\", \\",\\",\\n               \\"x_coord_km_4\\", \\",\\",\\n               \\"y_coord_km_4\\", \\",\\",\\n               \\"UTM_zone\\", \\",\\",\\n               \\"UTM_hemisphere\\", \\",\\",\\n               \\"effective_height\\", \\",\\",\\n               \\"base_elev\\", \\",\\",\\n               \\"init_sigma_z\\", \\",\\",\\n               \\"emission_rate\\", \\",\\",\\n               \\"emission_units\\"),\\n        sep = \\"\\\\n\\",\\n        file = area_sources_filename,\\n        append = TRUE)\\n    \\n  }\\n  \\n  # Determine whether lon/lat provided\\n  if (!is.null(lat_dec_deg) & !is.null(lon_dec_deg)){\\n    lon_lat_provided <- TRUE\\n  } else {\\n    lon_lat_provided <- FALSE\\n  }\\n  \\n  # Determine whether UTM coordinates and zone information provided\\n  if (!is.null(x_coord_km) & !is.null(y_coord_km)\\n      & !is.null(UTM_zone) & !is.null(UTM_hemisphere)){\\n    UTM_provided <- TRUE\\n  } else {\\n    UTM_provided <- FALSE\\n  }  \\n  \\n  # If both lon/lat provided, convert to UTM\\n  if (lon_lat_provided == TRUE & UTM_provided == FALSE){\\n    \\n    # Get matrix of longitude and latitude for source location\\n    lat_lon_dec_deg <- cbind(lon_dec_deg, lat_dec_deg)\\n    \\n    # Determine the UTM zone\\n    UTM_zone <- unique((floor((lon_dec_deg + 180)/6) %% 60) + 1)[1]\\n    \\n    # Determine whether source is in the Northern Hemisphere or the Southern Hemisphere\\n    UTM_hemisphere <- unique(ifelse(lat_dec_deg >= 0, \\"N\\", \\"S\\"))[1]\\n    \\n    # Define a PROJ.4 projection string for a lat/lon projection\\n    proj_string_longlat <- \\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\\"\\n    \\n    # Define a PROJ.4 projection string for a UTM projection\\n    proj_string_UTM <- paste0(\\"+proj=utm +zone=\\",\\n                              UTM_zone,\\n                              \\" +ellps=WGS84 +datum=WGS84 +units=m +no_defs\\")\\n    \\n    # Project as UTM coordinates from the determined UTM zone\\n    UTM_location <- project(lat_lon_dec_deg, proj_string_UTM)\\n    \\n    # Define the UTM x coordinates in km units\\n    x_coord_km <- UTM_location[,1] / 1000\\n    \\n    # Define the UTM y coordinates in km units\\n    y_coord_km <- UTM_location[,2] / 1000\\n    \\n  }\\n  \\n  # If UTM coordinates provided, convert to lon/lat\\n  if (lon_lat_provided == FALSE & UTM_provided == TRUE){\\n    \\n    # Define a PROJ.4 projection string for a lat/lon projection\\n    proj_string_longlat <- \\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\\"\\n    \\n    # Define a PROJ.4 projection string for a UTM projection\\n    proj_string_UTM <- paste0(\\"+proj=utm +zone=\\",\\n                              UTM_zone,\\n                              \\" +ellps=WGS84 +datum=WGS84 +units=m +no_defs\\")\\n    \\n    # Create a SpatialPoints object for the UTM coordinates\\n    UTM_m_SP <- SpatialPoints(matrix(c(x_coord_km * 1000,\\n                                       y_coord_km * 1000),\\n                                     nrow = 4,\\n                                     ncol = 2),\\n                              proj4string = CRS(proj_string_UTM))\\n    \\n    # Project as UTM coordinates from the determined UTM zone\\n    latlon_SP <- spTransform(UTM_m_SP, CRS(proj_string_longlat))\\n    \\n    # Extract the latitude values in decimal degrees from the SpatialPoints object\\n    lat_dec_deg <- latlon_SP@coords[,2]\\n    \\n    # Extract the longitude values in decimal degrees from the SpatialPoints object\\n    lon_dec_deg <- latlon_SP@coords[,1]\\n    \\n  }\\n  \\n  # Write the values to the file\\n  cat(paste0(src_name, \\",\\",\\n             species_name, \\",\\",\\n             format(lat_dec_deg[1], small.interval = 6), \\",\\",\\n             format(lon_dec_deg[1], small.interval = 6), \\",\\",\\n             format(lat_dec_deg[2], small.interval = 6), \\",\\",\\n             format(lon_dec_deg[2], small.interval = 6), \\",\\",\\n             format(lat_dec_deg[3], small.interval = 6), \\",\\",\\n             format(lon_dec_deg[3], small.interval = 6), \\",\\",\\n             format(lat_dec_deg[4], small.interval = 6), \\",\\",\\n             format(lon_dec_deg[4], small.interval = 6), \\",\\",\\n             format(x_coord_km[1], small.interval = 3), \\",\\",\\n             format(y_coord_km[1], small.interval = 3), \\",\\",\\n             format(x_coord_km[2], small.interval = 3), \\",\\",\\n             format(y_coord_km[2], small.interval = 3), \\",\\",\\n             format(x_coord_km[3], small.interval = 3), \\",\\",\\n             format(y_coord_km[3], small.interval = 3), \\",\\",\\n             format(x_coord_km[4], small.interval = 3), \\",\\",\\n             format(y_coord_km[4], small.interval = 3), \\",\\",\\n             UTM_zone, \\",\\",\\n             UTM_hemisphere, \\",\\",\\n             effective_height, \\",\\",\\n             base_elev, \\",\\",\\n             init_sigma_z, \\",\\",\\n             emission_rate, \\",\\",\\n             emission_units),\\n      sep = \\"\\\\n\\",\\n      file = area_sources_filename,\\n      append = TRUE)\\n  \\n}\\n" }\n'
line: b'{ "repo_name": "sfr/RStudio-Addin-Snippets", "ref": "refs/heads/master", "path": "tests/testthat/data/.foobar.R", "content": "# dummy method for copy.data unit testing\\n.foobar <- function( .x_y_z, a )\\n{\\na <- F # not indented on purpose\\n\\n    if (missing(.x_y_z)) {\\n        print(\'Please set a list .x_y_z.\')\\n    } else {\\n        print(.x_y_z$pi)\\n    }\\n\\n    a\\n}\\n\\n# dummy method for insert.pipe unit testing\\n.foobar2 <- function()\\n{\\n    select <- function()\\n    {\\n        NULL\\n    }\\n\\n    filter <- function()\\n    {\\n        NULL\\n    }\\n\\n    # test here\\n    a <- select() %   >  %\\n\\n\\n\\n\\n        filter()\\n\\n\\n    # expected\\n    a <- select() %>%\\n        filter() %>%\\n            as.data.frame()\\n\\n    a\\n}\\n\\ntop.context <- rstudioapi::getActiveDocumentContext()\\ntop.context[[\'selection\']][[1]] <- NULL\\nsave(top.context, file=\'.\\\\\\\\tests\\\\\\\\testthat\\\\\\\\.foobar.Rdata\')\\nrm(top.context)\\n" }\n'
line: b'{ "repo_name": "ellisp/ggseas", "ref": "refs/heads/master", "path": "pkg/tests/testthat.R", "content": "library(testthat)\\nlibrary(ggseas)\\n\\ntest_check(\\"ggseas\\")\\n" }\n'
line: b'{ "repo_name": "ellisp/ggseas", "ref": "refs/heads/master", "path": "prep/create_ldeaths_df.R", "content": "library(tidyr)\\nlibrary(dplyr)\\n\\nldeaths_df <- cbind(fdeaths, mdeaths, YearMon = time(fdeaths)) %>%\\n   as.data.frame() %>%\\n   gather(sex, deaths, -YearMon) %>%\\n   mutate(sex = ifelse(sex == \\"fdeaths\\", \\"female\\", \\"male\\"))\\n\\n\\nsave(ldeaths_df, file = \\"data/ldeaths_df.rda\\")" }\n'
line: b'{ "repo_name": "woobe/rApps", "ref": "refs/heads/master", "path": "oddsimiser/server.R", "content": "suppressMessages(library(shiny))\\nsuppressMessages(library(combinat))\\nsuppressMessages(library(GA))\\nsuppressMessages(library(MCMCpack))\\n\\nshinyServer(function(input, output) {\\n  \\n  ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n  ## Optimiser (Single)\\n  ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n  optimise_single <- reactive({\\n    \\n    odds_all <- c()  \\n    if (input$odds1 > 1) odds_all <- c(odds_all, input$odds1)\\n    if (input$odds2 > 1) odds_all <- c(odds_all, input$odds2)\\n    if (input$odds3 > 1) odds_all <- c(odds_all, input$odds3)\\n    if (input$odds4 > 1) odds_all <- c(odds_all, input$odds4)\\n    if (input$odds5 > 1) odds_all <- c(odds_all, input$odds5)\\n    if (input$odds6 > 1) odds_all <- c(odds_all, input$odds6)\\n    if (input$odds7 > 1) odds_all <- c(odds_all, input$odds7)\\n    if (input$odds8 > 1) odds_all <- c(odds_all, input$odds8)\\n    if (input$odds9 > 1) odds_all <- c(odds_all, input$odds9)\\n    if (input$odds10 > 1) odds_all <- c(odds_all, input$odds10)\\n    as.matrix(odds_all)\\n    \\n        \\n    ## Evaluate Function\\n    eval_odds <- function(inp_p) {\\n      total_p <- sum(inp_p)\\n      rtn <- odds_all * as.matrix(inp_p) - total_p\\n      #return(min(rtn))\\n      return(1-(max(rtn)-min(rtn))/max(rtn))\\n    }\\n    \\n    ## Define parameters for GA\\n    para_ga_min <- rep(0, length(odds_all))\\n    para_ga_max <- rep(1, length(odds_all))\\n    \\n    ## Optimise with GA\\n    set.seed(1234)\\n    model <- ga(type = \\"real-valued\\",\\n                fitness = eval_odds, monitor = FALSE,\\n                min = para_ga_min, max = para_ga_max,\\n                popSize = 10, maxiter = 1000)\\n    \\n    ## Normalise and save results\\n    best_p <- summary(model)$solution / sum(summary(model)$solution)\\n    \\n    ## Create summary df\\n    sum_df <- data.frame(matrix(NA, nrow = 4, ncol = length(odds_all)))\\n    colnames(sum_df) <- paste0(\\"\xe8\xb3\xbd\xe4\xba\x8b\\", 1:length(odds_all))\\n    rownames(sum_df) <- c(\\"\xe8\xb3\xa0\xe7\x8e\x87\\", \\"\xe6\xb3\xa8\xe7\xa2\xbc\\", \\"\xe5\x9b\x9e\xe5\xa0\xb1\\", \\"\xe5\x88\xa9\xe6\xbd\xa4\\")  \\n    sum_df[1,] <- odds_all\\n    sum_df[2,] <- best_p * input$stake\\n    sum_df[3,] <- odds_all * best_p * input$stake\\n    sum_df[4,] <- (odds_all * best_p - 1) * input$stake\\n    \\n    ## Return\\n    t(sum_df)\\n    \\n  })\\n  \\n  \\n  ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n  ## Optimiser Double\\n  ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n  optimise_double <- reactive({\\n    \\n    odds_all <- c()  \\n    if (input$odds1 > 1) odds_all <- c(odds_all, input$odds1)\\n    if (input$odds2 > 1) odds_all <- c(odds_all, input$odds2)\\n    if (input$odds3 > 1) odds_all <- c(odds_all, input$odds3)\\n    if (input$odds4 > 1) odds_all <- c(odds_all, input$odds4)\\n    if (input$odds5 > 1) odds_all <- c(odds_all, input$odds5)\\n    if (input$odds6 > 1) odds_all <- c(odds_all, input$odds6)\\n    if (input$odds7 > 1) odds_all <- c(odds_all, input$odds7)\\n    if (input$odds8 > 1) odds_all <- c(odds_all, input$odds8)\\n    if (input$odds9 > 1) odds_all <- c(odds_all, input$odds9)\\n    if (input$odds10 > 1) odds_all <- c(odds_all, input$odds10)\\n    as.matrix(odds_all)\\n      \\n    games_all <- paste0(\\"\xe8\xb3\xbd\xe4\xba\x8b\\", 1:length(odds_all))\\n    games_double <- combn(games_all, 2)\\n    \\n    cbn_double <- combn(odds_all, 2)\\n    odds_all <- cbn_double[1, ] * cbn_double[2, ]\\n    \\n    ## Evaluate Function\\n    eval_odds <- function(inp_p) {\\n      total_p <- sum(inp_p)\\n      rtn <- odds_all * as.matrix(inp_p) - total_p\\n      #return(min(rtn))\\n      return(1-(max(rtn)-min(rtn))/max(rtn))\\n    }\\n    \\n    ## Define parameters for GA\\n    para_ga_min <- rep(0, length(odds_all))\\n    para_ga_max <- rep(1, length(odds_all))\\n    \\n    ## Optimise with GA\\n    set.seed(1234)\\n    model <- ga(type = \\"real-valued\\",\\n                fitness = eval_odds, monitor = FALSE,\\n                min = para_ga_min, max = para_ga_max,\\n                popSize = 20, maxiter = 2000)\\n    \\n    ## Normalise and save results\\n    best_p <- summary(model)$solution / sum(summary(model)$solution)\\n    \\n    ## Create summary df\\n    sum_df <- data.frame(matrix(NA, nrow = 4, ncol = length(odds_all)))\\n    \\n    names_double <- c()\\n    for (n in 1:length(odds_all)) {\\n      names_double <- c(names_double, paste0(games_double[1,n], \\"x\\", games_double[2,n]))\\n    }\\n    colnames(sum_df) <- names_double\\n    \\n    rownames(sum_df) <- c(\\"\xe8\xb3\xa0\xe7\x8e\x87\\", \\"\xe6\xb3\xa8\xe7\xa2\xbc\\", \\"\xe5\x9b\x9e\xe5\xa0\xb1\\", \\"\xe5\x88\xa9\xe6\xbd\xa4\\")  \\n    sum_df[1,] <- odds_all\\n    sum_df[2,] <- best_p * input$stake\\n    sum_df[3,] <- odds_all * best_p * input$stake\\n    sum_df[4,] <- (odds_all * best_p - 1) * input$stake\\n    \\n    ## Return\\n    t(sum_df)\\n    \\n  })\\n  \\n  \\n  \\n  ## Outputs\\n  \\n  output$single <- renderTable({\\n    print(as.data.frame(optimise_single()))\\n  })\\n  \\n  output$double <- renderTable({\\n    print(as.data.frame(optimise_double()))\\n  })\\n  \\n  \\n})" }\n'
line: b'{ "repo_name": "fcocquemas/rdatastream", "ref": "refs/heads/master", "path": "R/help.R", "content": "#\' A R interface for Datastream and Thomson Dataworks Enterprise. \\n#\'\\n#\' @docType package\\n#\' @name RDatastream\\n#\' @aliases RDatastream\\n#\' @import XML RCurl\\nNULL" }\n'
line: b'{ "repo_name": "Robinlovelace/learning-shiny", "ref": "refs/heads/master", "path": "hi/server.R", "content": "library(shiny)\\n\\n# Define server logic required to draw a histogram\\nshinyServer(function(input, output) {\\n\\n  # Expression that generates a histogram. The expression is\\n  # wrapped in a call to renderPlot to indicate that:\\n  #\\n  #  1) It is \\"reactive\\" and therefore should be automatically\\n  #     re-executed when inputs change\\n  #  2) Its output type is a plot\\n\\n  output$distPlot <- renderPlot({\\n    x    <- faithful[, 2]  # Old Faithful Geyser data\\n    bins <- seq(min(x), max(x), length.out = input$bins + 1)\\n\\n    # draw the histogram with the specified number of bins\\n    hist(x, breaks = bins, col = \'darkgray\', border = \'white\')\\n  })\\n\\n})" }\n'
line: b'{ "repo_name": "zachmayer/cv.ts", "ref": "refs/heads/master", "path": "tests/testthat.R", "content": "library(testthat)\\nlibrary(cv.ts)\\n\\ntest_check(\\"cv.ts\\")\\n" }\n'
line: b'{ "repo_name": "RGLab/MAST", "ref": "refs/heads/summarizedExpt", "path": "R/ZlmFit-bootstrap.R", "content": "##\' Bootstrap a zlmfit\\n##\'\\n##\' Sample cells with replacement to find bootstrapped distribution of coefficients\\n##\' @param cl a \\\\code{cluster} object created by \\\\code{makeCluster}\\n##\' @param zlmfit class \\\\code{ZlmFit}\\n##\' @param R number of bootstrap replicates\\n##\' @return array of bootstrapped coefficients\\n##\' @export\\npbootVcov1<-function (cl,zlmfit, R = 99)\\n{\\n    sca <- zlmfit@sca\\n    N <- ncol(sca)\\n    LMlike <- zlmfit@LMlike\\n    parallel::clusterEvalQ(cl,require(MAST))\\n    ## clusterEvalQ(cl,require(abind))\\n    parallel::clusterExport(cl,\\"N\\",envir=environment())\\n    parallel::clusterExport(cl,\\"LMlike\\",envir=environment())\\n    parallel::clusterExport(cl,\\"sca\\",envir=environment())\\n    manyvc <- parallel::parSapply(cl,1:R, function(i,...){\\n        s <- sample(N, replace = TRUE)\\n        newsca <- sca[, s]\\n        LMlike <- update(LMlike, design=colData(newsca))\\n        zlm.SingleCellAssay(sca = newsca, LMlike = LMlike, onlyCoef=TRUE)\\n    })\\n  \\n    d<-dim(coef(zlmfit,\\"D\\"))\\n    manyvc<-aperm(array(manyvc,c(d,2,R)),c(4,1,2,3))\\n    dimnames(manyvc)<-c(list(NULL),dimnames(coef(zlmfit,\\"D\\")),list(c(\\"C\\",\\"D\\")))\\n    manyvc\\n}\\n\\n##\' Bootstrap a zlmfit\\n##\'\\n##\' Sample cells with replacement to find bootstrapped distribution of coefficients\\n##\' @param zlmfit class \\\\code{ZlmFit}\\n##\' @param R number of bootstrap replicates\\n##\' @return array of bootstrapped coefficients\\n##\' @importFrom plyr raply\\n##\' @export\\nbootVcov1 <- function(zlmfit, R=99){\\n    sca <- zlmfit@sca\\n    N <- ncol(sca)\\n    LMlike <- zlmfit@LMlike\\n    manyvc <- raply(R, {\\n        s <- sample(N, replace=TRUE)\\n        newsca <- sca[,s]\\n        LMlike <- update(LMlike, design=colData(newsca))\\n        zlm.SingleCellAssay(sca=newsca, LMlike=LMlike, onlyCoef=TRUE)\\n    })\\n\\n   manyvc\\n    \\n}\\n\\n" }\n'
line: b'{ "repo_name": "RGLab/MAST", "ref": "refs/heads/summarizedExpt", "path": "tests/testthat/test-lmWrapper-glmer.R", "content": "obj <- new(\'LMERlike\', design=colData(fd), formula=~Stim.Condition + (1|Subject.ID))\\n\\ncontext(\'LMERlike\')\\nif(require(lme4)){\\n    obj <- fit(obj, response=exprs(fd)[,2])\\nobjC <- lmer(obj@response ~Stim.Condition +  (1|Subject.ID), data=as.data.frame(obj@design), subset=obj@response>0, REML=FALSE)\\nobjD <- glmer(obj@response>0 ~Stim.Condition + (1|Subject.ID), data=as.data.frame(obj@design), family=binomial())\\nsource(\'common-lmWrapper-tests.R\', local=TRUE)\\n    try(detach(\'package:lme4\'), silent=TRUE)\\n}\\n\\n" }\n'
line: b'{ "repo_name": "RGLab/MAST", "ref": "refs/heads/summarizedExpt", "path": "tests/testthat/helper-vbeta-init.R", "content": "geneid=\\"Gene\\"\\nprimerid=\'Gene\'\\nmeasurement=\'et\'\\nidvars=c(\'Subject.ID\', \'Chip.Number\', \'Stim.Condition\', \'Population\', \'Well\', \'Number.of.Cells\')\\nphenovars=NULL\\ncellvars=\'Experiment.Number\'\\nfeaturevars=NULL\\nncells <- \'Number.of.Cells\'\\n\\n## Currently needed because devtools 1.11.0 has broken data()\\n## See https://github.com/mlr-org/mlr/pull/835\\nload(system.file(\'data/vbeta.RData\', package=\'MAST\'))\\ndata(vbeta)\\n\\nvbeta$et <- ifelse(is.na(vbeta$Ct), 0, 40-vbeta$Ct)\\n\\n\\nfd <- FromFlatDF(vbeta, idvars=idvars, primerid=primerid, measurement=measurement,cellvars=cellvars, geneid=geneid, ncells=\'Number.of.Cells\', class=\'FluidigmAssay\')\\n" }\n'
line: b'{ "repo_name": "terrytangyuan/dml", "ref": "refs/heads/master", "path": "tests/testthat/test_helper_functions.R", "content": "context(\'helper functions\')\\n\\ntest_that(\'package set up successfully\', {\\n  expect_that(sum(1,2), not(throws_error()))\\n})\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Getting_and_Cleaning_Data/Grouping_and_Chaining_with_dplyr/scripts/chain3.R", "content": "# Use filter() to select all rows for which size_mb is\\n# less than or equal to (<=) 0.5.\\n#\\n# If you want your results printed to the console, add\\n# print to the end of your chain.\\n\\ncran %>%\\n  select(ip_id, country, package, size) %>%\\n  mutate(size_mb = size / 2^20) %>%\\n  # Your call to filter() goes here\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Data_Analysis/Dispersion/initLesson.R", "content": "assign(\\"cars\\", openintro::cars, envir=globalenv())\\nassign(\\"mpg.midsize\\", cars[cars$type==\\"midsize\\",\\"mpgCity\\"], envir=globalenv())\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Exploratory_Data_Analysis/Dimension_Reduction/showRanMat.R", "content": "#par(mar=rep(0.2,4))\\nimage(1:10,1:40,t(dataMatrix)[,nrow(dataMatrix):1])\\n#par(mar=rep(0.2,4))\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Writing_swirl_Courses/R/yamlWriter.R", "content": "#\' DIRECTIONS: \\n#\' 1. Source this file and invoke newLesson(courseName, lessonName), where\\n#\' courseName and lessonName are strings of your choice such as \\"My Course\\" and \\n#\' \\"My Lesson 1\\". Spaces are allowed in course and lesson names.\\n#\' This will create a file such as My_Course/My_Lesson_1/lesson.yaml.\\n#\' The file should appear in the editor automatically. If not, open it manually.\\n#\' \\n#\' 2. Fill in the course meta-information, Author, Organization, etc. as indicated in \\n#\' lesson.yaml. \\n#\' \\n#\' 3. Append instructional units such as multiple-choice questions to lesson.yaml by\\n#\' invoking appropriate helper functions. Type hlp() for a brief list. Typing qmult(),\\n#\' for instance would append the following template to lesson.yaml:\\n#\' - Class: mult_question  \\n#\'   Output: ask the multiple choice question here\\n#\'   AnswerChoices: ANS;2;3\\n#\'   CorrectAnswer: ANS\\n#\'   AnswerTests: omnitest(correctVal= \'ANS\')\\n#\'   Hint: hint\\n#\' which might be manually edited as follows.\\n#\' - Class: mult_question  \\n#\'   Output: This demonstrates a multiple choice question. Which Scooby-Doo character wears an ascot?\\n#\'   AnswerChoices: Fred Jones;Velma Dinkley;Daphne Blake;Shaggy Rogers\\n#\'   CorrectAnswer: Fred Jones\\n#\'   AnswerTests: omnitest(correctVal= \'Fred Jones\')\\n#\'   Hint: This person doesn\'t say \\"Jinkies.\\"\\n#\'   \\n#\' 4. Save lesson.yaml whenever you append and edit a unit of instruction.\\n#\' \\n#\' 5. Strings may take more than one line provided they are surrounded by double quotes. For instance,\\n#\' in the following question the Output: field takes up two lines:\\n#\' - Class: text_question\\n#\'   Output: \\"This illustrates a question requiring a single phrase text answer. \\n#\' What is the name of the van which carries Scooby\'s gang?\\"\\n#\'   CorrectAnswer: Mystery Machine\\n#\'   AnswerTests: omnitest(correctVal=\'Mystery Machine\')\\n#\'   Hint: The gang generally solves a M------? It\'s the M------ Machine.\\n\\nnewLesson <- function(course, lesson){\\n  courseDir <- file.path(gsub(\\" \\", \\"_\\", course))\\n  lessonDir <<- file.path(courseDir, gsub(\\" \\", \\"_\\", lesson))\\n  if(!file.exists(lessonDir))dir.create(lessonDir, recursive=TRUE)\\n  # Check for existence of a manifest\\n  manifest <- file.path(courseDir, \\"MANIFEST\\")\\n  if(!file.exists(manifest)){\\n    file.create(manifest)\\n  }\\n  # Append the current lesson to the manifest\\n  cat(paste0(gsub(\\" \\", \\"_\\", lesson), \\"\\\\n\\"), file=manifest, append=TRUE)\\n  # The yaml faq, http://www.yaml.org/faq.html, encourages\\n  # use of the .yaml (as opposed to .yml) file extension\\n  # whenever possible.\\n  lessonFile <<- file.path(lessonDir, \\"lesson.yaml\\")\\n  writeLines(c(\\"- Class: meta\\", \\n               paste(\\"  Course:\\", course),\\n               paste(\\"  Lesson:\\", lesson),\\n               \\"  Author: your name goes here\\",\\n               \\"  Type: Standard\\",\\n               \\"  Organization: your organization\\",\\n               paste(\\"  Version: \\", packageDescription(\\"swirl\\")$Version)),\\n             lessonFile)\\n  # Create supporting files\\n  depends <- file.path(lessonDir, \\"dependson.txt\\")\\n  file.create(depends)\\n  init <- file.path(lessonDir, \\"initLesson.R\\")\\n  file.create(init)\\n  cat(\\"# Code placed in this file will be executed every time the\\n# lesson is started. Any variables created here will show up in\\n# the user\'s working directory and thus be accessible to them\\n# throughout the lesson.\\\\n\\", file=init)\\n  custom <- file.path(lessonDir, \\"customTests.R\\")\\n  file.create(custom)\\n  cat(\\"# Put custom tests in this file.\\n\\n# Uncommenting the following line of code will disable\\n# auto-detection of new variables and thus prevent swirl from\\n# executing every command twice, which can slow things down.\\n\\n# AUTO_DETECT_NEWVAR <- FALSE\\n\\n# However, this means that you should detect user-created\\n# variables when appropriate. The answer test, expr_creates_var()\\n# can be used for for the purpose, but it also re-evaluates the\\n# expression which the user entered, so care must be taken.\\\\n\\", \\n      file=custom)\\n  file.edit(lessonFile)\\n}\\n\\nsetLesson <- function(course, lesson){\\n  courseDir <- file.path(gsub(\\" \\", \\"_\\", course))\\n  lessonDir <- file.path(courseDir, gsub(\\" \\", \\"_\\", lesson))\\n  lessonFile <- file.path(lessonDir, \\"lesson.yaml\\")\\n  if(!file.exists(lessonFile)){\\n    stop(paste(\\"Sorry!\\", lessonFile, \\"doesn\'t exist. Check the path.\\"))\\n  } else {\\n    lessonDir <<- lessonDir\\n    lessonFile <<- lessonFile\\n    file.edit(lessonFile)\\n  }\\n}\\n\\n# yaml writer help\\nhlp <- function(){\\n  print(\\"txt -- just text, no question\\")\\n  print(\\"qmult -- multiple choice question\\")\\n  print(\\"qcmd -- command line question\\")\\n  print(\\"vid -- video\\")\\n  print(\\"fig -- figure\\")\\n  print(\\"qx -- question requiring exact numerical answer\\")\\n  print(\\"qtxt -- question requiring a short text answer\\")\\n  print(\\"qrng -- question requiring a numerical answer within a certain range.\\")\\n}\\n\\n# template for presentation without a question\\ntxt <- function(){\\n  cat(\\"\\\\n- Class: text\\n  Output: put your exposition here.\\\\n\\", file=lessonFile, append=TRUE)\\n  invisible()\\n}\\n\\n# template for multiple choice question\\nqmult <- function(){\\n  cat(\\"\\\\n- Class: mult_question  \\n  Output: ask the multiple choice question here\\n  AnswerChoices: ANS;2;3\\n  CorrectAnswer: ANS\\n  AnswerTests: omnitest(correctVal= \'ANS\')\\n  Hint: hint\\\\n\\", file=lessonFile, append=TRUE)\\n  invisible()\\n}\\n\\nqcmd <- function(){\\n  cat(\\"\\\\n- Class: cmd_question\\n  Output: explain what the user must do here\\n  CorrectAnswer: EXPR or VAL\\n  AnswerTests: omnitest(correctExpr=\'EXPR\', correctVal=VAL)\\n  Hint: hint\\\\n\\", file=lessonFile, append=TRUE)\\n  invisible()\\n}\\n\\nvid <- function(){\\n  cat(\\"\\\\n- Class: video\\n  Output: Would you like to watch a short video about ___?\\n  VideoLink: \'http://address.of.video\'\\\\n\\", file=lessonFile, append=TRUE)\\n  invisible()\\n}\\n\\nfig <- function(){\\n  cat(\\"\\\\n- Class: figure\\n  Output: explain the figure here\\n  Figure: sourcefile.R\\n  FigureType: new or add\\\\n\\", file=lessonFile, append=TRUE)\\n  invisible()\\n}\\n\\nqx<- function(){\\n  cat(\\"\\\\n- Class: exact_question\\n  Output: explain the question here\\n  CorrectAnswer: n\\n  AnswerTests: omnitest(correctVal=n)\\n  Hint: hint\\\\n\\", file=lessonFile, append=TRUE)\\ninvisible()\\n}\\n\\nqtxt <- function(){\\n  cat(\\"\\\\n- Class: text_question\\n  Output: explain the question here\\n  CorrectAnswer: answer\\n  AnswerTests: val_matches(\'regular_expression_which_matches_answer\')\\n  Hint: hint\\\\n\\", file=lessonFile, append=TRUE)\\ninvisible()\\n}\\n\\nqrng <- function(){\\n  cat(\\"\\\\n- Class: range_question\\n  Output: explain the question here\\n      CorrectAnswer: answer\\n      AnswerTests: requires a custom test\\n      Hint: hint\\\\n\\", file=lessonFile, append=TRUE)\\n  invisible()\\n}\\n\\n\\nreinstall <- function(){\\n  course <- dirname(lessonDir)\\n  # Uninstall_course\\n  try(swirl::uninstall_course(course), silent=TRUE)\\n  # Install course\\n  swirl::install_course_directory(gsub(\\" \\", \\"_\\", course))\\n}\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/Binary_Outcomes/initLesson.R", "content": "# For compatibility with 2.2.21\\n.get_course_path <- function(){\\n  tryCatch(swirl:::swirl_courses_dir(),\\n           error = function(c) {file.path(find.package(\\"swirl\\"),\\"Courses\\")}\\n  )\\n}\\n\\n# ravens data\\nravenData <- read.csv(file.path(.get_course_path(), \\n                                 \\"Regression_Models\\", \\"Binary_Outcomes\\", \\"ravens_data.csv\\"))\\nravenData <- ravenData[order(ravenData$ravenScore), 1:3]\\nrownames(ravenData) <- NULL\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/Overfitting_and_Underfitting/initLesson.R", "content": "# For compatibility with 2.2.21\\n.get_course_path <- function(){\\n  tryCatch(swirl:::swirl_courses_dir(),\\n           error = function(c) {file.path(find.package(\\"swirl\\"),\\"Courses\\")}\\n  )\\n}\\n\\nswiss <- datasets::swiss\\nfile.copy(from=file.path(.get_course_path(),\\n\\t\\"Regression_Models\\", \\"Overfitting_and_Underfitting\\",\\"fitting.R\\"), \\n          to=\\"fitting.R\\")\\nfile.edit(\\"fitting.R\\")\\nsource(\\"fitting.R\\", local=TRUE)\\nfit5 <- lm(Fertility ~ Agriculture + Examination + Education + Catholic, swiss)\\nfit6 <- lm(Fertility ~ ., swiss)\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Exploratory_Data_Analysis/Principles_of_Analytic_Graphs/MVD2.R", "content": "fname <- paste(path_to_course,\\"MVData2.jpeg\\",sep=\\"/\\")\\ntry(dev.off(),silent=TRUE)\\nplot.new()\\nplotArea=par(\'fig\')\\nrasterImage(readJPEG(fname),plotArea[1],plotArea[3],plotArea[2],plotArea[4],interpolate=FALSE)\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Getting_and_Cleaning_Data/Tidying_Data_with_tidyr/scripts/script8.R", "content": "# Accomplish the following three goals:\\n#\\n# 1. select() all columns that do NOT contain the word \\"total\\",\\n# since if we have the male and female data, we can always\\n# recreate the total count in a separate column, if we want it.\\n# Hint: Use the contains() function, which you\'ll\\n# find detailed in \'Special functions\' section of ?select.\\n#\\n# 2. gather() all columns EXCEPT score_range, using\\n# key = part_sex and value = count.\\n#\\n# 3. separate() part_sex into two separate variables (columns),\\n# called \\"part\\" and \\"sex\\", respectively. You may need to check\\n# the \'Examples\' section of ?separate to remember how the \'into\'\\n# argument should be phrased.\\n#\\nsat %>%\\n  select(-contains(###)) %>%\\n  gather(###, ###, -###) %>%\\n  ### <Your call to separate()> %>%\\n  print\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Getting_and_Cleaning_Data/Grouping_and_Chaining_with_dplyr/scripts/chain4.R", "content": "# arrange() the result by size_mb, in descending order.\\n#\\n# If you want your results printed to the console, add\\n# print to the end of your chain.\\n\\ncran %>%\\n  select(ip_id, country, package, size) %>%\\n  mutate(size_mb = size / 2^20) %>%\\n  filter(size_mb <= 0.5) %>%\\n  # Your call to arrange() goes here\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/Introduction_to_Multivariable_Regression/customTests.R", "content": "# So swirl does not repeat execution of plot commands\\nAUTO_DETECT_NEWVAR <- FALSE\\n\\n# Returns TRUE if e$expr matches any of the expressions given\\n# (as characters) in the argument.\\nANY_of_exprs <- function(...){\\n  e <- get(\\"e\\", parent.frame())\\n  any(sapply(c(...), function(expr)omnitest(expr)))\\n}\\n\\n# Returns TRUE if the user has created a specified lm model\\n# with a specified name.\\ncreates_lm_model <- function(correctExpr){\\n  e <- get(\\"e\\", parent.frame())\\n  # Do what the user should have done\\n  eSw <- cleanEnv(e$snapshot)\\n  mdlSw <- eval(parse(text=correctExpr), eSw)\\n  # Recreate what the user has done\\n  eUsr <- cleanEnv(e$snapshot)\\n  mdlUsr <- eval(e$expr, eUsr)\\n  # If the correct model is named:\\n  if(length(ls(eSw))>0){\\n    # Check whether the model\'s name is correct\\n    nameGood <- sum(ls(eUsr) %in% ls(eSw)) & sum(ls(eSw) %in% ls(eUsr))\\n    # If not, highlight the misspelling\\n    if(!nameGood){\\n      swirl_out(paste0(\\"You seem to have misspelled the model\'s name. I was expecting \\", ls(eSw), \\n                       \\" but you apparently typed \\", ls(eUsr), \\".\\"))\\n      return(FALSE)\\n    } else {\\n      # Append the result, as a list to e$delta for progress restoration\\n      e$delta <- c(e$delta, as.list(eUsr))\\n    }\\n  }\\n  # Check for effective equality of the models\\n  isTRUE(all.equal(sort(as.vector(mdlUsr$coefficients)), sort(as.vector(mdlSw$coefficients)))) &\\n    isTRUE(all.equal(mdlUsr$fitted.values, mdlSw$fitted.values))\\n}\\n\\n# Returns TRUE if the user has calculated a value equal to that calculated by the given expression.\\ncalculates_same_value <- function(expr){\\n  e <- get(\\"e\\", parent.frame())\\n  # Calculate what the user should have done.\\n  eSnap <- cleanEnv(e$snapshot)\\n  val <- eval(parse(text=expr), eSnap)\\n  isTRUE(all.equal(val, e$val))\\n}\\n\\n# Returns TRUE of the user has calculated a value equal to any of those computed by the given\\n# expressions.\\ncalculates_ANY_value <- function(...){\\n  e <- get(\\"e\\", parent.frame())\\n  any(sapply(c(...), function(expr)calculates_same_value(expr)))\\n}\\n\\n# Get the swirl state\\ngetState <- function(){\\n  # Whenever swirl is running, its callback is at the top of its call stack.\\n  # Swirl\'s state, named e, is stored in the environment of the callback.\\n  environment(sys.function(1))$e\\n}\\n\\n# Get the value which a user either entered directly or was computed\\n# by the command he or she entered.\\ngetVal <- function(){\\n  getState()$val\\n}\\n\\n# Get the last expression which the user entered at the R console.\\ngetExpr <- function(){\\n  getState()$expr\\n}\\n\\ncoursera_on_demand <- function(){\\n  selection <- getState()$val\\n  if(selection == \\"Yes\\"){\\n    email <- readline(\\"What is your email address? \\")\\n    token <- readline(\\"What is your assignment token? \\")\\n    \\n    payload <- sprintf(\'{  \\n      \\"assignmentKey\\": \\"hHsdF68wEeWxaw7Jay15BQ\\",\\n      \\"submitterEmail\\": \\"%s\\",  \\n      \\"secret\\": \\"%s\\",  \\n      \\"parts\\": {  \\n        \\"3wpmw\\": {  \\n          \\"output\\": \\"correct\\"  \\n        }  \\n      }  \\n    }\', email, token)\\n    url <- \'https://www.coursera.org/api/onDemandProgrammingScriptSubmissions.v1\'\\n  \\n    respone <- httr::POST(url, body = payload)\\n    if(respone$status_code >= 200 && respone$status_code < 300){\\n      message(\\"Grade submission succeeded!\\")\\n    } else {\\n      message(\\"Grade submission failed.\\")\\n      message(\\"Press ESC if you want to exit this lesson and you\\")\\n      message(\\"want to try to submit your grade at a later time.\\")\\n      return(FALSE)\\n    }\\n  }\\n  TRUE\\n}" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/Introduction/restore_1.R", "content": "plot(child ~ parent, galton)\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Statistical_Inference/Probability1/customTests.R", "content": "# So swirl does not repeat execution of plot commands\\nAUTO_DETECT_NEWVAR <- FALSE\\n\\n# Returns TRUE if e$expr matches any of the expressions given\\n# (as characters) in the argument.\\nANY_of_exprs <- function(...){\\n  e <- get(\\"e\\", parent.frame())\\n  any(sapply(c(...), function(expr)omnitest(expr)))\\n}\\n\\nequiv_val <- function(correctVal){\\n  e <- get(\\"e\\", parent.frame()) \\n  #print(paste(\\"User val is \\",e$val,\\"Correct ans is \\",correctVal))\\n  isTRUE(all.equal(correctVal,e$val))\\n  \\n}\\n\\n# Get the swirl state\\ngetState <- function(){\\n  # Whenever swirl is running, its callback is at the top of its call stack.\\n  # Swirl\'s state, named e, is stored in the environment of the callback.\\n  environment(sys.function(1))$e\\n}\\n\\n# Get the value which a user either entered directly or was computed\\n# by the command he or she entered.\\ngetVal <- function(){\\n  getState()$val\\n}\\n\\n# Get the last expression which the user entered at the R console.\\ngetExpr <- function(){\\n  getState()$expr\\n}\\n\\ncoursera_on_demand <- function(){\\n  selection <- getState()$val\\n  if(selection == \\"Yes\\"){\\n    email <- readline(\\"What is your email address? \\")\\n    token <- readline(\\"What is your assignment token? \\")\\n    \\n    payload <- sprintf(\'{  \\n      \\"assignmentKey\\": \\"BOLw8680EeWRRQpQejjiSw\\",\\n      \\"submitterEmail\\": \\"%s\\",  \\n      \\"secret\\": \\"%s\\",  \\n      \\"parts\\": {  \\n        \\"jwsCv\\": {  \\n          \\"output\\": \\"correct\\"  \\n        }  \\n      }  \\n    }\', email, token)\\n    url <- \'https://www.coursera.org/api/onDemandProgrammingScriptSubmissions.v1\'\\n  \\n    respone <- httr::POST(url, body = payload)\\n    if(respone$status_code >= 200 && respone$status_code < 300){\\n      message(\\"Grade submission succeeded!\\")\\n    } else {\\n      message(\\"Grade submission failed.\\")\\n      message(\\"Press ESC if you want to exit this lesson and you\\")\\n      message(\\"want to try to submit your grade at a later time.\\")\\n      return(FALSE)\\n    }\\n  }\\n  TRUE\\n}" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/MultiVar_Examples/customTests.R", "content": "# So swirl does not repeat execution of plot commands\\n#AUTO_DETECT_NEWVAR <- FALSE\\n\\n# Returns TRUE if e$expr matches any of the expressions given\\n# (as characters) in the argument.\\nANY_of_exprs <- function(...){\\n  e <- get(\\"e\\", parent.frame())\\n  any(sapply(c(...), function(expr)omnitest(expr)))\\n}\\n\\n# Returns TRUE if the user has created a specified lm model\\n# with a specified name.\\ncreates_lm_model <- function(correctExpr){\\n  e <- get(\\"e\\", parent.frame())\\n  # Do what the user should have done\\n  eSw <- cleanEnv(e$snapshot)\\n  mdlSw <- eval(parse(text=correctExpr), eSw)\\n  # Recreate what the user has done\\n  eUsr <- cleanEnv(e$snapshot)\\n  mdlUsr <- eval(e$expr, eUsr)\\n  # If the correct model is named:\\n  if(length(ls(eSw))>0){\\n    # Check whether the model\'s name is correct\\n    nameGood <- sum(ls(eUsr) %in% ls(eSw)) & sum(ls(eSw) %in% ls(eUsr))\\n    # If not, highlight the misspelling\\n    if(!nameGood){\\n      swirl_out(paste0(\\"You seem to have misspelled the model\'s name. I was expecting \\", ls(eSw), \\n                       \\" but you apparently typed \\", ls(eUsr), \\".\\"))\\n      return(FALSE)\\n    } else {\\n      # Append the result, as a list to e$delta for progress restoration\\n      e$delta <- c(e$delta, as.list(eUsr))\\n    }\\n  }\\n  # Check for effective equality of the models\\n  isTRUE(all.equal(sort(as.vector(mdlUsr$coefficients)), sort(as.vector(mdlSw$coefficients)))) &\\n    isTRUE(all.equal(mdlUsr$fitted.values, mdlSw$fitted.values))\\n}\\n\\n# Get the swirl state\\ngetState <- function(){\\n  # Whenever swirl is running, its callback is at the top of its call stack.\\n  # Swirl\'s state, named e, is stored in the environment of the callback.\\n  environment(sys.function(1))$e\\n}\\n\\n# Get the value which a user either entered directly or was computed\\n# by the command he or she entered.\\ngetVal <- function(){\\n  getState()$val\\n}\\n\\n# Get the last expression which the user entered at the R console.\\ngetExpr <- function(){\\n  getState()$expr\\n}\\n\\ncoursera_on_demand <- function(){\\n  selection <- getState()$val\\n  if(selection == \\"Yes\\"){\\n    email <- readline(\\"What is your email address? \\")\\n    token <- readline(\\"What is your assignment token? \\")\\n    \\n    payload <- sprintf(\'{  \\n      \\"assignmentKey\\": \\"iGMF3K8wEeWVdAqQVb1YyQ\\",\\n      \\"submitterEmail\\": \\"%s\\",  \\n      \\"secret\\": \\"%s\\",  \\n      \\"parts\\": {  \\n        \\"Dxjqj\\": {  \\n          \\"output\\": \\"correct\\"  \\n        }  \\n      }  \\n    }\', email, token)\\n    url <- \'https://www.coursera.org/api/onDemandProgrammingScriptSubmissions.v1\'\\n  \\n    respone <- httr::POST(url, body = payload)\\n    if(respone$status_code >= 200 && respone$status_code < 300){\\n      message(\\"Grade submission succeeded!\\")\\n    } else {\\n      message(\\"Grade submission failed.\\")\\n      message(\\"Press ESC if you want to exit this lesson and you\\")\\n      message(\\"want to try to submit your grade at a later time.\\")\\n      return(FALSE)\\n    }\\n  }\\n  TRUE\\n}" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Statistical_Inference/CommonDistros/stddev1.R", "content": "g <- ggplot(dat, aes(x = x, y = y)) + geom_line(size = 1.5)\\ng <- g + geom_ribbon(aes(x = ifelse(x > -1 & x < 1, x, 0), ymin = 0, ymax = dat$y), fill = \\"red\\", alpha = 1)\\ng <- g +  geom_ribbon(aes(x = ifelse(x > -2 & x < 2, x, 0), ymin = 0, ymax = dat$y), fill = \\"red\\", alpha = 0.5)\\ng <- g +  geom_ribbon(aes(x = ifelse(x > -3 & x < 3, x, 0), ymin = 0, ymax = dat$y), fill = \\"red\\", alpha = 0.35)\\nprint(g)\\n\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Getting_and_Cleaning_Data/Grouping_and_Chaining_with_dplyr/scripts/summarize2-correct.R", "content": "# Don\'t change any of the code below. Just type submit()\\n# when you think you understand it.\\n\\n# We\'ve already done this part, but we\'re repeating it\\n# here for clarity.\\n\\nby_package <- group_by(cran, package)\\npack_sum <- summarize(by_package,\\n                      count = n(),\\n                      unique = n_distinct(ip_id),\\n                      countries = n_distinct(country),\\n                      avg_bytes = mean(size))\\n\\n# Here\'s the new bit, but using the same approach we\'ve\\n# been using this whole time.\\n\\ntop_countries <- filter(pack_sum, countries > 60)\\nresult1 <- arrange(top_countries, desc(countries), avg_bytes)\\n\\n# Print the results to the console.\\nprint(result1)\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Data_Analysis/Central_Tendency/initLesson.R", "content": "  assign(\\"cars\\", openintro::cars, envir=globalenv())\\n  assign(\\"mpg.midsize\\", cars[cars$type==\\"midsize\\",\\"mpgCity\\"], envir=globalenv())\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/Residuals_Diagnostics_and_Variation/restore_4.R", "content": "plot(fit, which=2)" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "R_Programming_Alt/Functions/customTests.R", "content": "test_func1 <- function() {\\n  try({\\n    func <- get(\'boring_function\', globalenv())\\n    t1 <- identical(func(9), 9)\\n    t2 <- identical(func(4), 4)\\n    t3 <- identical(func(0), 0)\\n    ok <- all(t1, t2, t3)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}\\n\\ntest_func2 <- function() {\\n  try({\\n    func <- get(\'my_mean\', globalenv())\\n    t1 <- identical(func(9), mean(9))\\n    t2 <- identical(func(1:10), mean(1:10))\\n    t3 <- identical(func(c(-5, -2, 4, 10)), mean(c(-5, -2, 4, 10)))\\n    ok <- all(t1, t2, t3)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}\\n\\ntest_func3 <- function() {\\n  try({\\n    func <- get(\'remainder\', globalenv())\\n    t1 <- identical(func(9, 4), 9 %% 4)\\n    t2 <- identical(func(divisor = 5, num = 2), 2 %% 5)\\n    t3 <- identical(func(5), 5 %% 2)\\n    ok <- all(t1, t2, t3)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}\\n\\ntest_func4 <- function() {\\n  try({\\n    func <- get(\'evaluate\', globalenv())\\n    t1 <- identical(func(sum, c(2, 4, 7)), 13)\\n    t2 <- identical(func(median, c(9, 200, 100)), 100)\\n    t3 <- identical(func(floor, 12.1), 12)\\n    ok <- all(t1, t2, t3)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}\\n\\ntest_func5 <- function() {\\n  try({\\n    func <- get(\'telegram\', globalenv())\\n    t1 <- identical(func(\\"Good\\", \\"morning\\"), \\"START Good morning STOP\\")\\n    t2 <- identical(func(\\"hello\\", \\"there\\", \\"sir\\"), \\"START hello there sir STOP\\")\\n    t3 <- identical(func(), \\"START STOP\\")\\n    ok <- all(t1, t2, t3)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}\\n\\ntest_func6 <- function() {\\n  try({\\n    func <- get(\'mad_libs\', globalenv())\\n    t1 <- identical(func(place = \\"Baltimore\\", adjective = \\"smelly\\", noun = \\"Roger Peng statue\\"), \\"News from Baltimore today where smelly students took to the streets in protest of the new Roger Peng statue being installed on campus.\\")\\n    t2 <- identical(func(place = \\"Washington\\", adjective = \\"angry\\", noun = \\"Shake Shack\\"), \\"News from Washington today where angry students took to the streets in protest of the new Shake Shack being installed on campus.\\")\\n    ok <- all(t1, t2)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}\\n\\ntest_func7 <- function() {\\n  try({\\n    func <- get(\'%p%\', globalenv())\\n    t1 <- identical(func(\\"Good\\", \\"job!\\"), \\"Good job!\\")\\n    t2 <- identical(func(\\"one\\", func(\\"two\\", \\"three\\")), \\"one two three\\")\\n    ok <- all(t1, t2)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}\\n\\ntest_eval1 <- function(){\\n  try({\\n    e <- get(\\"e\\", parent.frame())\\n    expr <- e$expr\\n    t1 <- identical(expr[[3]], 6)\\n    expr[[3]] <- 7\\n    t2 <- identical(eval(expr), 8)\\n    ok <- all(t1, t2)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}\\n\\ntest_eval2 <- function(){\\n  try({\\n    e <- get(\\"e\\", parent.frame())\\n    expr <- e$expr\\n    t1 <- identical(expr[[3]], quote(c(8, 4, 0)))\\n    t2 <- identical(expr[[1]], quote(evaluate))\\n    expr[[3]] <- c(5, 6)\\n    t3 <- identical(eval(expr), 5)\\n    ok <- all(t1, t2, t3)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}\\n\\ntest_eval3 <- function(){\\n  try({\\n    e <- get(\\"e\\", parent.frame())\\n    expr <- e$expr\\n    t1 <- identical(expr[[3]], quote(c(8, 4, 0)))\\n    t2 <- identical(expr[[1]], quote(evaluate))\\n    expr[[3]] <- c(5, 6)\\n    t3 <- identical(eval(expr), 6)\\n    ok <- all(t1, t2, t3)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Exploratory_Data_Analysis/Working_with_Colors/customTests.R", "content": "# So swirl does not repeat execution of plot commands\\nAUTO_DETECT_NEWVAR <- FALSE\\n\\n# Returns TRUE if e$expr matches any of the expressions given\\n# (as characters) in the argument.\\nANY_of_exprs <- function(...){\\n  e <- get(\\"e\\", parent.frame())\\n  any(sapply(c(...), function(expr)omnitest(expr)))\\n}\\n\\nequiv_val <- function(correctVal){\\n  e <- get(\\"e\\", parent.frame()) \\n  #print(paste(\\"User val is \\",e$val,\\"Correct ans is \\",correctVal))\\n  isTRUE(all.equal(correctVal,e$val))\\n  \\n}\\n\\n# Get the swirl state\\ngetState <- function(){\\n  # Whenever swirl is running, its callback is at the top of its call stack.\\n  # Swirl\'s state, named e, is stored in the environment of the callback.\\n  environment(sys.function(1))$e\\n}\\n\\n# Get the value which a user either entered directly or was computed\\n# by the command he or she entered.\\ngetVal <- function(){\\n  getState()$val\\n}\\n\\n# Get the last expression which the user entered at the R console.\\ngetExpr <- function(){\\n  getState()$expr\\n}\\n\\ncoursera_on_demand <- function(){\\n  selection <- getState()$val\\n  if(selection == \\"Yes\\"){\\n    email <- readline(\\"What is your email address? \\")\\n    token <- readline(\\"What is your assignment token? \\")\\n    \\n    payload <- sprintf(\'{  \\n      \\"assignmentKey\\": \\"jbRkna8dEeWxIhKVGQB0WQ\\",\\n      \\"submitterEmail\\": \\"%s\\",  \\n      \\"secret\\": \\"%s\\",  \\n      \\"parts\\": {  \\n        \\"Lxfoi\\": {  \\n          \\"output\\": \\"correct\\"  \\n        }  \\n      }  \\n    }\', email, token)\\n    url <- \'https://www.coursera.org/api/onDemandProgrammingScriptSubmissions.v1\'\\n  \\n    respone <- httr::POST(url, body = payload)\\n    if(respone$status_code >= 200 && respone$status_code < 300){\\n      message(\\"Grade submission succeeded!\\")\\n    } else {\\n      message(\\"Grade submission failed.\\")\\n      message(\\"Press ESC if you want to exit this lesson and you\\")\\n      message(\\"want to try to submit your grade at a later time.\\")\\n      return(FALSE)\\n    }\\n  }\\n  TRUE\\n}" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Exploratory_Data_Analysis/Hierarchical_Clustering/dendro.R", "content": "try(dev.off(),silent=TRUE)\\nplot.new()\\nplot(as.dendrogram(hc))" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Exploratory_Data_Analysis/CaseStudy/initLesson.R", "content": "library(fields)\\n\\n# For compatibility with 2.2.21\\n.get_course_path <- function(){\\n  tryCatch(swirl:::swirl_courses_dir(),\\n           error = function(c) {file.path(find.package(\\"swirl\\"),\\"Courses\\")}\\n  )\\n}\\n\\n# Put initialization code in this file.\\npath_to_course <- file.path(.get_course_path(),\\n  \\"Exploratory_Data_Analysis\\",\\"CaseStudy\\")\\ntry(dev.off(),silent=TRUE)\\nplot.new()\\n\\npathtofile <- function(fileName){\\n  mypath <- file.path(.get_course_path(),\\n    \\"Exploratory_Data_Analysis\\",\\"CaseStudy\\",\\n                      fileName)\\n}\\nfxfer <- function(fileName){\\n  mypath <- pathtofile(fileName)\\n  file.copy(mypath,fileName)\\n}\\n\\nmyImage <- function(iname){\\n  par(mfrow=c(1,1))\\n  par(mar=c(8,10,8,10))\\n  image(t(iname)[,nrow(iname):1])\\n}\\nmyedit <- function(fname){\\n   #fxfer(fname)\\n   #file.edit(fname)\\n   mypath <- pathtofile(fname)\\n   file.edit(mypath)\\n}\\n\\nmdist <- function(x,y,cx,cy){\\n  distTmp <- matrix(NA,nrow=3,ncol=12)\\n  distTmp[1,] <- (x-cx[1])^2 + (y-cy[1])^2\\n  distTmp[2,] <- (x-cx[2])^2 + (y-cy[2])^2\\n  distTmp[3,] <- (x-cx[3])^2 + (y-cy[3])^2  \\n  return(distTmp)\\n}\\n\\nshowMe <- function(cv){\\n  myarg <- deparse(substitute(cv))\\n  z<- outer( 1:20,1:20, \\"+\\")\\n  obj<- list( x=1:20,y=1:20,z=z )\\n  image(obj, col=cv, main=myarg  )\\n}\\n#my1999 <- pathtofile(\\"RD_501_88101_1999-0.txt.gz\\")\\nmy1999 <- pathtofile(\\"airData1999.txt.gz\\")\\n#my2012 <- pathtofile(\\"RD_501_88101_2012-0.txt.gz\\")\\nmy2012 <- pathtofile(\\"airData2012.txt.gz\\")\\ncnames <- \\"# RD|Action Code|State Code|County Code|Site ID|Parameter|POC|Sample Duration|Unit|Method|Date|Start Time|Sample Value|Null Data Code|Sampling Frequency|Monitor Protocol (MP) ID|Qualifier - 1|Qualifier - 2|Qualifier - 3|Qualifier - 4|Qualifier - 5|Qualifier - 6|Qualifier - 7|Qualifier - 8|Qualifier - 9|Qualifier - 10|Alternate Method Detectable Limit|Uncertainty\\"\\npm0 <- read.table(my1999, comment.char = \\"#\\", header = FALSE, sep = \\"|\\", na.strings = \\"\\")\\npm1 <- read.table(my2012, comment.char = \\"#\\", header = FALSE, sep = \\"|\\", na.strings = \\"\\")\\nwcol <- c(3,4,5,11,13)\\n#pm0 <- pm0[,wcol]\\n#pm1 <- pm1[,wcol]\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Writing_swirl_Courses/Custom_Tests/initLesson.R", "content": "# Code placed in this file will be executed every time the\\n# lesson is started. Any variables created here will show up in\\n# the user\'s working directory and thus be accessible to them\\n# throughout the lesson.\\n\\n# Source utilities.R\\nsource(file.path(find.package(\\"swirl\\"), \\"Courses\\", \\"Writing_swirl_Courses\\", \\"R\\", \\"utilities.R\\"))\\n\\n# Display the customTests.R file.\\ndisplay_swirl_file(\\"customTests.R\\", \\"Writing_swirl_Courses\\", \\"Custom_Tests\\")\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/Least_Squares_Estimation/demofile.R", "content": "file.edit(fname)" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/MultiVar_Examples3/interactplot.R", "content": "plot(hunger$Year,hunger$Numeric,pch=19)\\npoints(hunger$Year,hunger$Numeric,pch=19,col=((hunger$Sex==\\"Male\\")*1+125))\\nabline(c(lmInter$coeff[1],lmInter$coeff[2]),col=\\"red\\",lwd=3)\\nabline(c(lmInter$coeff[1] + lmInter$coeff[3],lmInter$coeff[2] +lmInter$coeff[4]),col=\\"blue\\",lwd=3)" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "R_Programming/Functions/scripts/remainder.R", "content": "# Let me show you an example of a function I\'m going to make up called\\n# increment(). Most of the time I want to use this function to increase the\\n# value of a number by one. This function will take two arguments: \\"number\\" and\\n# \\"by\\" where \\"number\\" is the digit I want to increment and \\"by\\" is the amount I\\n# want to increment \\"number\\" by. I\'ve written the function below. \\n#\\n# increment <- function(number, by = 1){\\n#     number + by\\n# }\\n#\\n# If you take a look in between the parentheses you can see that I\'ve set\\n# \\"by\\" equal to 1. This means that the \\"by\\" argument will have the default\\n# value of 1.\\n#\\n# I can now use the increment function without providing a value for \\"by\\": \\n# increment(5) will evaluate to 6. \\n#\\n# However if I want to provide a value for the \\"by\\" argument I still can! The\\n# expression: increment(5, 2) will evaluate to 7. \\n# \\n# You\'re going to write a function called \\"remainder.\\" remainder() will take\\n# two arguments: \\"num\\" and \\"divisor\\" where \\"num\\" is divided by \\"divisor\\" and\\n# the remainder is returned. Imagine that you usually want to know the remainder\\n# when you divide by 2, so set the default value of \\"divisor\\" to 2. Please be\\n# sure that \\"num\\" is the first argument and \\"divisor\\" is the second argument.\\n#\\n# Hint #1: You can use the modulus operator %% to find the remainder.\\n#   Ex: 7 %% 4 evaluates to 3. \\n#\\n# Remember to set appropriate default values! Be sure to save this \\n# script and type submit() in the console after you write the function.\\n\\nremainder <- function(num, divisor) {\\n  # Write your code here!\\n  # Remember: the last expression evaluated will be returned! \\n}\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Statistical_Inference/T_Confidence_Intervals/sleepPlot.R", "content": "g <- ggplot(sleep, aes(x = group, y = extra, group = factor(ID)))\\ng <- g + geom_line(size = 1, aes(colour = ID)) + geom_point(size =10, pch = 21, fill = \\"salmon\\", alpha = .5)\\nprint(g)" }\n'
line: b'{ "repo_name": "mariodeng/FirebrowseR", "ref": "refs/heads/master", "path": "R/Samples.Clinical.R", "content": "#\' Retrieve TCGA CDEs verbatim, i.e. not normalized by Firehose.\\n#\' \\n#\' This service returns patient clinical data from TCGA, verbatim. It differs from the Samples/Clinical_FH method by providing access to all TCGA CDEs in their original form, not merely the subset of CDEs normalized by Firehose for analyses.  Results may be selected by disease cohort, patient barcode or CDE name, but at least one cohort, barcode, or CDE must be provided. When filtering by CDE note that only when a patient record contains one or more of the selected CDEs will it be returned. Visit the Metadata/ClinicalNames api function to see the entire list of TCGA CDEs that may be queried via this method. For more information on how clinical data are processed, see our <a href=\\"https://confluence.broadinstitute.org/display/GDAC/Documentation#Documentation-ClinicalPipeline\\">pipeline documentation</a>.\\n#\'\\n#\' @param format Format of result. Default value is json. While json,tsv,csv are available. \\n#\' @param cohort Narrow search to one or more TCGA disease cohorts from the scrollable list. Multiple values are allowed ACC,BLCA,BRCA,CESC,CHOL,COAD,COADREAD,DLBC,ESCA,FPPP,GBM,GBMLGG,HNSC,KICH,KIPAN,KIRC,KIRP,LAML,LGG,LIHC,LUAD,LUSC,MESO,OV,PAAD,PCPG,PRAD,READ,SARC,SKCM,STAD,STES,TGCT,THCA,THYM,UCEC,UCS,UVM.\\n#\' @param tcga_participant_barcode Comma separated list of TCGA participant barcodes (e.g. TCGA-GF-A4EO). Multiple values are allowed .\\n#\' @param cde_name Retrieve results only for specified CDEs, per the Metadata/ClinicalNames function Multiple values are allowed .\\n#\' @param page Which page (slice) of entire results set should be returned.  Multiple values are allowed . Default value is 1.  \\n#\' @param page_size Number of records per page of results.  Max is 2000. Multiple values are allowed . Default value is 150.  \\n#\' @param sort_by Which column in the results should be used for sorting paginated results? Default value is cohort. While tcga_participant_barcode,cohort,cde_name are available. \\n#\' \\n#\' @export\\nSamples.Clinical = function(format = \\"json\\",\\n                             cohort = \\"\\",\\n                             tcga_participant_barcode = \\"\\",\\n                             cde_name = \\"\\",\\n                             page = \\"1\\",\\n                             page_size = \\"150\\",\\n                             sort_by = \\"cohort\\"\\n                             ){\\n                             \\n  parameters = list(format = format,\\n                    cohort = cohort,\\n                    tcga_participant_barcode = tcga_participant_barcode,\\n                    cde_name = cde_name,\\n                    page = page,\\n                    page_size = page_size,\\n                    sort_by = sort_by)\\n  to.Validate = c(\\"cohort\\",\\"tcga_participant_barcode\\",\\"cde_name\\")\\n  validate.Parameters(params = parameters, to.Validate = to.Validate)\\n\\n  url = build.Query(parameters = parameters,\\n                    invoker = \\"Samples\\",\\n                    method = \\"Clinical\\")\\n  ret = download.Data(url, format, page)\\n\\n  return(ret)\\n\\n}\\n" }\n'
line: b'{ "repo_name": "mariodeng/FirebrowseR", "ref": "refs/heads/master", "path": "R/Metadata.SampleTypes.R", "content": "#\' Return all TCGA sample type codes, both numeric and symbolic.\\n#\' \\n#\' \\n#\'\\n#\' @param format Format of result. Default value is json. While json,tsv,csv are available. \\n#\' \\n#\' @export\\nMetadata.SampleTypes = function(format = \\"json\\"\\n                             ){\\n                             \\n  parameters = list(format = format)\\n  \\n  validate.Parameters(params = parameters)\\n\\n  url = build.Query(parameters = parameters,\\n                    invoker = \\"Metadata\\",\\n                    method = \\"SampleTypes\\")\\n  ret = download.Data(url, format)\\n\\n  return(ret)\\n\\n}\\n" }\n'
line: b'{ "repo_name": "mariodeng/FirebrowseR", "ref": "refs/heads/master", "path": "tests/testthat/test.Samples.miRSeq.R", "content": "#library(FirebrowseR)\\ncontext(\\"Samples.miRSeq\\")\\n\\ntest_that(\\"miRSeq data is retrieved correctly\\", {\\n\\n  format = \\"json\\"\\n  mir = c(\\"hsa-mir-1285-3p\\",\\"hsa-mir-125a-5p\\",\\"hsa-mir-221-3p\\",\\"hsa-mir-10b-5p\\",\\"hsa-mir-608\\",\\"hsa-mir-324-5p\\")\\n  cohort = \\"BRCA\\"\\n  tcga_participant_barcode = \\"\\"\\n  tool = \\"miRseq_Mature_Preprocess\\"\\n  sample_type = \\"NT\\"\\n  page = 1\\n  page_size = 250\\n  sort_by = \\"mir\\"\\n\\n  obj = Samples.miRSeq(format = format,\\n                          mir = mir,\\n                          cohort = cohort,\\n                          tcga_participant_barcode = tcga_participant_barcode,\\n                          tool = tool,\\n                          sample_type = sample_type,\\n                          page = page,\\n                          page_size = page_size,\\n                          sort_by = sort_by)\\n  test.q = \\"http://firebrowse.org/api/v1/Samples/miRSeq?format=csv&mir=hsa-mir-1285-3p%2Chsa-mir-125a-5p%2Chsa-mir-221-3p%2Chsa-mir-10b-5p%2Chsa-mir-608%2Chsa-mir-324-5p&cohort=BRCA&tool=miRseq_Mature_Preprocess&sample_type=NT&page=1&page_size=250&sort_by=cohort\\"\\n  test.obj = read.table(test.q, header = T, sep = \\",\\", quote = \\"\\\\\\"\\")\\n  expect_is(obj, \\"list\\")\\n  expect_equal(length(obj[[1]]), nrow(test.obj))\\n  expect_equal(length(obj[[1]][[1]]), ncol(test.obj))\\n  \\n\\n  format = \\"csv\\"\\n  mir = c(\\"hsa-mir-1285-3p\\", \\"hsa-mir-125a-5p\\")\\n  obj = Samples.miRSeq(format = format,\\n                          mir = mir,\\n                          cohort = cohort,\\n                          tcga_participant_barcode = tcga_participant_barcode,\\n                          tool = tool,\\n                          sample_type = sample_type,\\n                          page = page,\\n                          page_size = page_size,\\n                          sort_by = sort_by)\\n  test.q = \\"http://firebrowse.org/api/v1/Samples/miRSeq?format=csv&mir=hsa-mir-1285-3p%2Chsa-mir-125a-5p&cohort=BRCA&tool=miRseq_Mature_Preprocess&sample_type=NT&page=1&page_size=250&sort_by=cohort\\"\\n  test.obj = read.table(test.q, header = T, sep = \\",\\", quote = \\"\\\\\\"\\")\\n  expect_equal(nrow(obj), nrow(test.obj))\\n  expect_equal(ncol(obj), ncol(test.obj))\\n\\n})\\n" }\n'
line: b'{ "repo_name": "mariodeng/FirebrowseR", "ref": "refs/heads/master", "path": "R/Metadata.Counts.R", "content": "#\' Retrieve sample counts.\\n#\' \\n#\' Returns the aliquot counts for each disease cohort, per sample type and data type.  The sample type designation of \\"Tumor\\" may be used to aggregate the count of all tumor aliquots into a single number per disease and data type. See the SampleTypes function for a complete description of sample types.\\n#\'\\n#\' @param format Format of result. Default value is json. While json,tsv,csv are available. \\n#\' @param date Select one or more date stamps. Multiple values are allowed 2016_01_28,2015_11_01,2015_08_21,2015_06_01,2015_04_02,2015_02_04,2014_12_06,2014_10_17,2014_09_02,2014_07_15,2014_05_18,2014_04_16,2014_03_16. Default value is 2016_01_28.  \\n#\' @param cohort Narrow search to one or more TCGA disease cohorts from the scrollable list. Multiple values are allowed ACC,BLCA,BRCA,CESC,CHOL,COAD,COADREAD,DLBC,ESCA,FPPP,GBM,GBMLGG,HNSC,KICH,KIPAN,KIRC,KIRP,LAML,LGG,LIHC,LUAD,LUSC,MESO,OV,PAAD,PCPG,PRAD,READ,SARC,SKCM,STAD,STES,TGCT,THCA,THYM,UCEC,UCS,UVM.\\n#\' @param sample_type Narrow search to one or more TCGA sample types from the scrollable list. Multiple values are allowed FFPE,NB,NBC,NBM,NT,TAM,TAP,TB,TM,TP,TR,Tumor.\\n#\' @param data_type Narrow search to one or more TCGA data types from the scrollable list. Multiple values are allowed bcr,clinical,cn,lowp,methylation,mrna,mrnaseq,mir,mirseq,rppa,maf,rawmaf.\\n#\' @param totals Output an entry providing the totals for each data type. Default value is TRUE. While  are available. \\n#\' @param sort_by Which column in the results should be used for sorting paginated results? Default value is cohort. While cohort are available. \\n#\' \\n#\' @export\\nMetadata.Counts = function(format = \\"json\\",\\n                             date = \\"2016_01_28\\",\\n                             cohort = \\"\\",\\n                             sample_type = \\"\\",\\n                             data_type = \\"\\",\\n                             totals = \\"TRUE\\",\\n                             sort_by = \\"cohort\\"\\n                             ){\\n                             \\n  parameters = list(format = format,\\n                    date = date,\\n                    cohort = cohort,\\n                    sample_type = sample_type,\\n                    data_type = data_type,\\n                    totals = totals,\\n                    sort_by = sort_by)\\n  \\n  validate.Parameters(params = parameters)\\n\\n  url = build.Query(parameters = parameters,\\n                    invoker = \\"Metadata\\",\\n                    method = \\"Counts\\")\\n  ret = download.Data(url, format)\\n\\n  return(ret)\\n\\n}\\n" }\n'
line: b'{ "repo_name": "mariodeng/FirebrowseR", "ref": "refs/heads/master", "path": "R/Metadata.Platforms.R", "content": "#\' Translate TCGA platform codes to full platform names.\\n#\' \\n#\' By default this function returns a table of all of the technology platforms used to sequence or characterize samples in TCGA--both their short platform codes and full names.  A subset of this table may be obtained by explicitly specifying one or more platform codes.\\n#\'\\n#\' @param format Format of result. Default value is json. While json,tsv,csv are available. \\n#\' @param platform Narrow search to one or more TCGA data generation platforms from the scrollable list. Multiple values are allowed 454,ABI,AgilentG4502A_07,AgilentG4502A_07_1,AgilentG4502A_07_2,AgilentG4502A_07_3,bio,biotab,CGH-1x1M_G4447A,diagnostic_images,fh_analyses,fh_reports,fh_stddata,Genome_Wide_SNP_6,GenomeWideSNP_5,H-miRNA_8x15K,H-miRNA_8x15Kv2,H-miRNA_EarlyAccess,H-miRNA_G4470A,HG-CGH-244A,HG-CGH-415K_G4124A,HG-U133_Plus_2,HG-U133A_2,HT_HG-U133A,HuEx-1_0-st-v2,Human1MDuo,HumanHap550,HumanMethylation27,HumanMethylation450,IlluminaDNAMethylation_OMA002_CPI,IlluminaDNAMethylation_OMA003_CPI,IlluminaGA_DNASeq,IlluminaGA_DNASeq_automated,IlluminaGA_DNASeq_Cont,IlluminaGA_DNASeq_Cont_automated,IlluminaGA_DNASeq_Cont_curated,IlluminaGA_DNASeq_curated,IlluminaGA_miRNASeq,IlluminaGA_mRNA_DGE,IlluminaGA_RNASeq,IlluminaGA_RNASeqV2,IlluminaGG,IlluminaHiSeq_DNASeq,IlluminaHiSeq_DNASeq_automated,IlluminaHiSeq_DNASeq_Cont,IlluminaHiSeq_DNASeq_Cont_automated,IlluminaHiSeq_DNASeq_Cont_curated,IlluminaHiSeq_DNASeq_curated,IlluminaHiSeq_DNASeqC,IlluminaHiSeq_miRNASeq,IlluminaHiSeq_mRNA_DGE,IlluminaHiSeq_RNASeq,IlluminaHiSeq_RNASeqV2,IlluminaHiSeq_TotalRNASeqV2,IlluminaHiSeq_WGBS,Mapping250K_Nsp,Mapping250K_Sty,MDA_RPPA_Core,microsat_i,minbio,minbiotab,Mixed_DNASeq,Mixed_DNASeq_automated,Mixed_DNASeq_Cont,Mixed_DNASeq_Cont_automated,Mixed_DNASeq_Cont_curated,Mixed_DNASeq_curated,pathology_reports,SOLiD_DNASeq,SOLiD_DNASeq_automated,SOLiD_DNASeq_Cont,SOLiD_DNASeq_Cont_automated,SOLiD_DNASeq_Cont_curated,SOLiD_DNASeq_curated,tissue_images,WHG-1x44K_G4112A,WHG-4x44K_G4112F,WHG-CGH_4x44B.\\n#\' \\n#\' @export\\nMetadata.Platforms = function(format = \\"json\\",\\n                             platform = \\"\\"\\n                             ){\\n                             \\n  parameters = list(format = format,\\n                    platform = platform)\\n  \\n  validate.Parameters(params = parameters)\\n\\n  url = build.Query(parameters = parameters,\\n                    invoker = \\"Metadata\\",\\n                    method = \\"Platforms\\")\\n  ret = download.Data(url, format)\\n\\n  return(ret)\\n\\n}\\n" }\n'
line: b'{ "repo_name": "alexholmes/hiped2", "ref": "refs/heads/master", "path": "src/main/R/wordcount_rhipe.R", "content": "#! /usr/bin/env Rscript\\nlibrary(Rhipe)\\n\\nrhinit(TRUE,TRUE)\\n\\nm <- expression({\\n  for(x in map.values){\\n    y <- strsplit(x,\\" +\\")[[1]]\\n    for(w in y) rhcollect(w,T)\\n  }\\n})\\n\\nz <- rhmr(map=m,inout=c(\\"text\\",\\"sequence\\"),\\n    ifolder=\\"stocks.txt\\",ofolder=\'/output\',mapred=list(mapred.reduce.tasks=5))\\n\\nrhex(z)" }\n'
line: b'{ "repo_name": "hredestig/pcaMethods", "ref": "refs/heads/master", "path": "R/llsImpute.R", "content": "##\' Missing value estimation using local least squares (LLS).  First,\\n##\' k variables (for Microarrya data usually the genes)  are selected\\n##\' by pearson, spearman or kendall correlation coefficients.  Then\\n##\' missing values are imputed by a linear combination of the k\\n##\' selected variables. The optimal combination is found by LLS\\n##\' regression.  The method was first described by Kim et al,\\n##\' Bioinformatics, 21(2),2005.\\n##\'\\n##\' Missing values are denoted as \\\\code{NA}\\\\cr It is not recommended\\n##\' to use this function directely but rather to use the nni() wrapper\\n##\' function. The methods provides two ways for missing value\\n##\' estimation, selected by the \\\\code{allVariables} option. The first\\n##\' one is to use only complete variables for the  regression. This is\\n##\' preferable when the number of incomplete variables is relatively\\n##\' small.\\n##\' \\n##\' The second way is to consider all variables as candidates for the\\n##\' regression.  Hereby missing values are initially replaced by the\\n##\' columns wise mean.  The method then iterates, using the current\\n##\' estimate as input for the regression until the change between new\\n##\' and old estimate falls below a threshold (0.001).\\n##\' \\n##\' @title LLSimpute algorithm\\n##\' @param Matrix \\\\code{matrix} -- Data containing the variables\\n##\' (genes) in columns and observations (samples) in rows. The data\\n##\' may contain missing values, denoted as \\\\code{NA}.\\n##\' @param k \\\\code{numeric} -- Cluster size, this is the number of\\n##\' similar genes used for regression.\\n##\' @param center \\\\code{boolean} -- Mean center the data if TRUE\\n##\' @param completeObs \\\\code{boolean} -- Return the estimated complete\\n##\' observations if  TRUE. This is the input data with NA values\\n##\' replaced by the estimated values.\\n##\' @param correlation \\\\code{character} -- How to calculate the\\n##\' distance between genes.  One out of pearson | kendall | spearman ,\\n##\' see also help(\\"cor\\").\\n##\' @param allVariables \\\\code{boolean} -- Use only complete genes to\\n##\' do the regression if TRUE, all genes if FALSE.\\n##\' @param maxSteps \\\\code{numeric} -- Maximum number of iteration\\n##\' steps if allGenes = TRUE.\\n##\' @param xval \\\\code{numeric} Use LLSimpute for cross\\n##\' validation. xval is the index of the gene to estimate, all other\\n##\' incomplete genes will be ignored if this parameter is set. We do\\n##\' not consider them in the cross-validation.\\n##\' @param verbose \\\\code{boolean} -- Print step number and relative\\n##\' change if TRUE and  allVariables = TRUE\\n##\' @param ... Reserved for parameters used in future version of the\\n##\' algorithm\\n##\' @note Each step the generalized inverse of a \\\\code{miss} x k\\n##\' matrix is calculated. Where \\\\code{miss} is the number of missing\\n##\' values in  variable j and \\\\code{k} the number of neighbours. This\\n##\' may be slow for large values of k and / or many missing\\n##\' values. See also help(\\"ginv\\").\\n##\' @return   \\\\item{nniRes}{Standard nni (nearest neighbour\\n##\' imputation) result object of this package. See\\n##\' \\\\code{\\\\link{nniRes}} for details.}\\n##\' @seealso \\\\code{\\\\link{pca}, \\\\link{nniRes}, \\\\link{nni}}.\\n##\' @examples\\n##\' ## Load a sample metabolite dataset (metaboliteData) with already 5\\\\% of\\n##\' ## data missing\\n##\' data(metaboliteData)\\n##\' ## Perform llsImpute using k = 10\\n##\' ## Set allVariables TRUE because there are very few complete variables\\n##\' result <- llsImpute(metaboliteData, k = 10, correlation=\\"pearson\\", allVariables=TRUE)\\n##\' ## Get the estimated complete observations\\n##\' cObs <- completeObs(result)\\n##\' @keywords multivariate\\n##\' @export\\n##\' @references Kim, H. and Golub, G.H. and Park, H.  - Missing value\\n##\' estimation for DNA microarray gene expression data: local least\\n##\' squares imputation.  \\\\emph{Bioinformatics, 2005; 21(2):187-198.}\\n##\' \\n##\' Troyanskaya O. and Cantor M. and Sherlock G. and Brown P. and\\n##\' Hastie T. and Tibshirani R. and Botstein D. and Altman RB.  -\\n##\' Missing value estimation methods for DNA microarrays.\\n##\' \\\\emph{Bioinformatics. 2001 Jun;17(6):520-525.}\\n##\' @author Wolfram Stacklies\\nllsImpute <- function(Matrix, k=10, center=FALSE, completeObs=TRUE,\\n                      correlation=\\"pearson\\", \\n                      allVariables=FALSE, maxSteps=100, xval=NULL,\\n                      verbose=FALSE, ...) {\\n\\n    threshold <- 0.001\\n\\n    correlation <- match.arg(correlation, c(\\"pearson\\", \\"kendall\\", \\"spearman\\"))\\n\\n    ## If the data is a data frame, convert it into a matrix\\n    Matrix <- as.matrix(Matrix, rownames.force=TRUE)\\n    ## And now check if everything is right...\\n    if ( !checkData(Matrix, verbose = interactive()) ) {\\n        stop(\\"Invalid data format! Use checkData(Matrix, verbose = TRUE) for details.\\\\n\\")\\n    }\\n\\n    ## Exit if number of neighbours exceeds number of columns\\n    if (k > ncol(Matrix))\\n        stop(\\"Cluster size larger than the number of columns, choose a k < ncol(Matrix)!\\")\\n \\n    ## Set allVariables TRUE if k exceeds number of complete genes\\n    ## Print warning messages in the first case and when less than 50% of all genes are complete\\n    ## and allVariables == FALSE\\n    cg <- sum( apply(is.na(Matrix), 2, sum) == 0)\\n    if ( (k > cg) && (!allVariables) ) {\\n        warning(\\"Cluster size larger than number of complete genes, using allVariables = TRUE\\")\\n        allVariables <- TRUE\\n    } else if ( (cg < (ncol(Matrix) / 2)) && (!allVariables) ) {\\n        warning(\\"Less than 50% of the genes are complete, consider using allVariables = TRUE\\")\\n    } else if (sum(is.na(Matrix)) == 0)\\n        stop(\\"No missing values, no need for missing value imputation :))\\")\\n\\n    ## Find all genes with missing values\\n    missing <- apply(is.na(Matrix), 2, sum) > 0\\n    missIx <- which(missing == TRUE)\\n    # For cross validation we want to only estimate one variable, the others\\n    # are not considered in the cross validation anyway\\n    if (!is.null(xval))\\n        missIx = xval\\n    obs <- Matrix    ## working copy of the data\\n    Ye <- Matrix     ## Estimated complete observations\\n\\n    ## Center the data column wise\\n    if (center) {\\n        obs   <- scale(Matrix, center = TRUE, scale = FALSE)\\n        Ye    <- obs\\n        means <- attr(Ye, \\"scaled:center\\")\\n    }\\n\\n    if (allVariables) {\\n        compIx <- 1:ncol(obs)\\n        ## Impute the row average\\n        rowMeans <- apply(obs, 1, mean, na.rm = TRUE)\\n        for (i in 1:nrow(obs)) {\\n            obs[i, is.na(Matrix[i,])] <- rowMeans[i]\\n        }\\n        ## distances between all genes, ignore the diagonal (correlation to itself)\\n        distance = abs(cor(obs, obs, method = correlation))\\n    } else {\\n        compIx <- which(missing == FALSE)\\n        ## missing genes are the rows, complete genes the columns\\n        distance = abs(cor(obs[,missIx, drop=FALSE], obs[,compIx, drop=FALSE], use=\\"pairwise.complete.obs\\",\\n                       method = correlation))\\n    }\\n\\n    change <- Inf\\n    step <- 0\\n    while ( (change > threshold) && (step < maxSteps) ) {\\n        step <- step + 1\\n        iteration <- 0\\n        \\n        ## Do the regression and imputation\\n        for (index in missIx) {\\n            iteration <- iteration + 1\\n\\t    if (allVariables) {\\n                similar <- sort(distance[iteration,], index.return = TRUE, decreasing = TRUE)\\n                simIx <- compIx[ similar$ix[similar$ix != iteration][1:k] ]\\n            } else {\\n                similar <- sort(distance[iteration,], index.return = TRUE, decreasing = TRUE)\\n                simIx <- compIx[ similar$ix[1:k] ]\\n            }\\n\\n            ##\\n            ## Do a regression against the k most similar genes\\n            ## See Kim et. al 2005 for details\\n            ##\\n            target <- obs[, index, drop = FALSE]\\n            tMiss <- is.na(Matrix[, index, drop = FALSE])\\n\\n            Apart <- obs[!tMiss, simIx, drop = FALSE]\\n            Bpart <- obs[tMiss, simIx, drop = FALSE]\\n            targetComplete <- target[!tMiss, , drop = FALSE]\\n            X <- MASS::ginv(Apart) %*% targetComplete\\n            estimate <- Bpart %*% X\\n\\n            ## Impute the estimate\\n            Ye[tMiss, index] <- estimate\\n        }\\n\\n        ## We do not want to iterate if allVariables == FALSE\\n        if (!allVariables || !is.null(xval)) {\\n            break\\n        } else {\\n            ## relative change in estimation\\n            change <- sqrt(sum( (obs - Ye)^2 ) / sum(obs^2))\\n            obs <- Ye\\n            if (verbose) {\\n                cat(\\"Step number     : \\", step, \'\\\\n\')\\n                cat(\\"Relative change : \\", change, \'\\\\n\')\\n                cat(\\"---------------\\", \'\\\\n\')\\n            }\\n        }\\n    }\\n\\n    ## Add the original mean\\n    if (center) {\\n        for(i in 1:ncol(Ye)) {\\n            Ye[,i] <- Ye[,i] + means[i]\\n        }\\n    }\\n\\n    ## Build the nniRes object\\n    ##\\n    result <- new(\\"nniRes\\")\\n\\n    if(completeObs) {\\n        Ye[!is.na(Matrix)] <- Matrix[!is.na(Matrix)]\\n        result@completeObs <- Ye\\n    }\\n    result@centered        <- center\\n    result@center          <- attr(scale(Matrix, center = TRUE, scale = FALSE), \\"scaled:center\\")\\n    result@nObs            <- nrow(Matrix)\\n    result@nVar            <- ncol(Matrix)\\n    result@method          <- \\"llsImpute\\"\\n    result@correlation     <- correlation\\n    result@k               <- k\\n    result@missing         <- sum(is.na(Matrix))\\n\\n    return(result)        \\n} \\n\\n" }\n'
line: b'{ "repo_name": "anqif/cvxr", "ref": "refs/heads/master", "path": "tests/testthat.R", "content": "library(testthat)\\nlibrary(cvxr)\\n\\ntest_check(\\"cvxr\\")\\n" }\n'
line: b'{ "repo_name": "anqif/cvxr", "ref": "refs/heads/master", "path": "tests/testthat/test_convolution.R", "content": "test_that(\\"test 1D convolution\\", {\\n  n <- 3\\n  x <- Variable(n)\\n  f <- as.matrix(c(1, 2, 3))\\n  g <- as.matrix(c(0, 1, 0.5))\\n  f_conv_g <- as.matrix(c(0, 1, 2.5, 4, 1.5))\\n  expr <- Conv(f, g)\\n  expect_true(is_constant(expr))\\n  expect_equal(size(expr), c(5, 1))\\n  \\n  expr <- Conv(f, x)\\n  expect_true(is_affine(expr))\\n  expect_equal(size(expr), c(5, 1))\\n  \\n  t <- Variable()\\n  prob <- Problem(Minimize(Pnorm(expr, 1)), list(x == g))\\n})\\n\\ntest_that(\\"test a problem with convolution\\", {\\n  N <- 5\\n  y <- matrix(rnorm(N), nrow = N, ncol = 1)\\n  h <- matrix(rnorm(2), nrow = 2, ncol = 1)\\n  x <- Variable(N)\\n  v <- Conv(h, x)\\n  # obj <- Minimize(SumEntries(MulElemwise(y, v[1:N])))\\n})\\n" }\n'
line: b'{ "repo_name": "philchalmers/mirtCAT", "ref": "refs/heads/master", "path": "R/PersonClass.R", "content": "Person <- setRefClass(\\"Person\\", \\n                      \\n                      fields = list(raw_responses = \'character\',\\n                                    responses = \'integer\',\\n                                    items_answered = \'integer\',\\n                                    thetas = \'matrix\',\\n                                    thetas_history = \'matrix\',\\n                                    thetas_SE_history = \'matrix\',\\n                                    info_thetas = \'matrix\',\\n                                    demographics = \'data.frame\',\\n                                    item_time = \'numeric\',\\n                                    valid_item = \'logical\',\\n                                    score = \'logical\'),\\n                      \\n                      methods = list(\\n                         initialize = function(nfact, nitems, thetas.start_in, score,\\n                                               theta_SEs){\\n                             \'Initialize the person object given background information\'\\n                             raw_responses <<- as.character(rep(NA, nitems))\\n                             responses <<- as.integer(rep(NA, nitems))\\n                             valid_item <<- rep(TRUE, nitems)\\n                             items_answered <<- as.integer(rep(NA, nitems))\\n                             thetas <<- matrix(numeric(nfact), nrow=1L)\\n                             thetas_SE_history <<- matrix(theta_SEs, 1L)\\n                             score <<- score\\n                             item_time <<- numeric(nitems)\\n                             if(!is.null(thetas.start_in))\\n                                thetas <<- matrix(thetas.start_in, nrow=1L)\\n                             thetas_history <<- matrix(thetas, 1L, nfact)\\n                             info_thetas <<- matrix(0, nfact, nfact)\\n                         })\\n                      \\n)\\n\\nPerson$methods(\\n    \\n    # Update thetas\\n    Update.thetas = function(design, test){\\n        \'Update the latent trait (theta) values using information \\n        from the design and test objects\'\\n        responses2 <- responses\\n        responses2[design@items_not_scored] <- NA\\n        if(score){\\n            method <- design@method\\n            if(last_item(items_answered) %in% design@items_not_scored)\\n                method <- \'fixed\'\\n            if(method == \'ML\'){\\n                if(length(unique(na.omit(responses2))) < 2L) method <- \'MAP\'\\n            }\\n            if(method != \'fixed\'){\\n                suppressWarnings(tmp <- fscores(test@mo, method=method, response.pattern=responses2,\\n                                   theta_lim=test@fscores_args$theta_lim,\\n                                   MI = test@fscores_args$MI, quadpts = test@quadpts, \\n                                   mean = test@fscores_args$mean, cov = test@fscores_args$cov,\\n                                   QMC=test@fscores_args$QMC, custom_den=test@fscores_args$custom_den))\\n                thetas <<- tmp[,paste0(\'F\', 1L:test@nfact), drop=FALSE]\\n                thetas_SE_history <<- rbind(thetas_SE_history, \\n                                            tmp[,paste0(\'SE_F\', 1L:test@nfact), drop=FALSE])\\n            } else {\\n                thetas_SE_history <<- rbind(thetas_SE_history, \\n                                            thetas_SE_history[nrow(thetas_SE_history),])\\n            }\\n            thetas_history <<- rbind(thetas_history, thetas)\\n            set <- c(\'Drule\', \'Trule\', \'Erule\', \'Wrule\', \'Arule\', \'APrule\',\\n                     \'DPrule\', \'TPrule\', \'EPrule\', \'WPrule\')\\n            if(test@nfact > 1L && design@criteria %in% set){\\n                pick <- which(!is.na(responses2))\\n                infos <- lapply(pick, function(x, thetas)\\n                    FI(extract.item(test@mo, x), Theta=thetas), thetas=thetas)\\n                tmp <- matrix(0, nrow(infos[[1L]]), ncol(infos[[1L]]))\\n                for(i in 1L:length(infos))\\n                    tmp <- tmp + infos[[i]]\\n                if(design@criteria %in% c(\'DPrule\', \'TPrule\', \'EPrule\', \'WPrule\', \'APrule\'))\\n                    tmp <- tmp + solve(test@gp$gcov)\\n                info_thetas <<- tmp\\n            }\\n        }\\n    }\\n)\\n" }\n'
line: b'{ "repo_name": "philchalmers/mirtCAT", "ref": "refs/heads/master", "path": "R/ShinyGUIClass.R", "content": "ShinyGUI <- setRefClass(\\"ShinyGUI\\", \\n                      \\n                      fields = list(title = \'character\',\\n                                    author = \'character\',\\n                                    questions = \'list\',\\n                                    df = \'list\',\\n                                    firstpage = \'list\',\\n                                    demographics = \'list\',\\n                                    lastpage = \'function\',\\n                                    instructions = \'character\',\\n                                    begin_message = \'character\',\\n                                    stem_locations = \'character\',\\n                                    demographic_inputIDs = \'character\',\\n                                    temp_file = \'character\',\\n                                    width = \'numeric\',\\n                                    height = \'numeric\',\\n                                    forced_choice = \'logical\',\\n                                    css = \'character\',\\n                                    stopApp = \'logical\',\\n                                    ui = \'function\'),\\n                      \\n                      methods = list(\\n                          initialize = function(questions, df, shinyGUI){\\n                              \'Initialize the shiny GUI given questions, df, and shinyGUI list\'\\n                              ui <<- default_UI\\n                              questions <<- questions\\n                              df <<- df\\n                              forced_choice <<- TRUE\\n                              stopApp <<- TRUE\\n                              if(is.null(shinyGUI$stem_locations)){\\n                                  stem_locations <<- as.character(rep(NA, length(questions)))\\n                              } else {\\n                                  stem_locations <<- as.character(sapply(shinyGUI$stem_locations, \\n                                    function(x){                                        \\n                                        ret <- if(!is.na(x)){\\n                                            org <- x\\n                                            exsts <- file.exists(x)\\n                                            if(!exsts){\\n                                                x <- paste0(getwd(), \'/\', x)\\n                                                exsts <- file.exists(x)\\n                                            }\\n                                            if(!exsts) \\n                                                stop(sprintf(\'The following file cannot be located: %s\', org), call.=FALSE)\\n                                            normalizePath(x, mustWork = TRUE)\\n                                        } else NA\\n                                        return(ret)\\n                                  }))\\n                              }\\n                              title <<- \'mirtCAT\'\\n                              author <<- \'Author information\'\\n                              instructions <<- c(\\"Instructions:\\",\\n                                                 \\"To progress through the interface, click on the action button below.\\",\\n                                                 \\"Next\\")\\n                              demographic_inputIDs <<- character(0)\\n                              begin_message <<- \\"Click the action button to begin.\\"\\n                              firstpage <<- list(h1(\'Welcome to the mirtCAT interface\'),\\n                                                 \'The following interface was created using the mirtCAT package. \\n                                                 To cite the package use citation(\\\\\'mirtCAT\\\\\') in R.\')\\n                              demographics <<- list()\\n                              lastpage <<- function(person) \\n                                            return(list(h5(\\"You have successfully completed the interface.\\n                                                   Click the action button to terminate the application.\\")))\\n                              if(!is.null(shinyGUI$stopApp) && !shinyGUI$stopApp)\\n                                  lastpage <<- function(person) \\n                                      return(list(h5(\\"You have successfully completed the interface.\\n                                                   Please close the tab/web browser to terminate the application.\\")))\\n                              temp_file <<- \'\'\\n                              css <<- \'\'\\n                                                 \\n                              if(length(shinyGUI)){\\n                                  dnames <- names(shinyGUI)\\n                                  gnames <- c(\'title\', \'authors\', \'instructions\', \'firstpage\', \'demographics\',\\n                                              \'demographics_inputIDs\', \'max_time\', \'temp_file\', \\n                                              \'lastpage\', \'css\', \'stem_dims\', \'forced_choice\', \'stem_locations\',\\n                                              \'begin_message\', \'stopApp\', \'ui\')\\n                                  if(!all(dnames %in% gnames))\\n                                      stop(\'The following inputs to shinyGUI are invalid: \',\\n                                           paste0(dnames[!(dnames %in% gnames)], \' \'), call.=FALSE)\\n                                  if(!is.null(shinyGUI$ui))\\n                                      ui <<- shinyGUI$ui\\n                                  if(!is.null(shinyGUI$instructions))\\n                                      instructions <<- shinyGUI$instructions\\n                                  if(!is.null(shinyGUI$begin_message))\\n                                      begin_message <<- shinyGUI$begin_message\\n                                  if(!is.null(shinyGUI$title))\\n                                      title <<- shinyGUI$title\\n                                  if(!is.null(shinyGUI$authors))\\n                                      author <<- shinyGUI$authors\\n                                  if(!is.null(shinyGUI$firstpage)) \\n                                      firstpage <<- shinyGUI$firstpage\\n                                  if(!is.null(shinyGUI$demographics)){\\n                                      demographics <<- shinyGUI$demographics\\n                                      demographic_inputIDs <<- shinyGUI$demographics_inputIDs\\n                                  }\\n                                  if(!is.null(shinyGUI$forced_choice))\\n                                      forced_choice <<- shinyGUI$forced_choice\\n                                  if(!is.null(shinyGUI$stopApp))\\n                                      stopApp <<- shinyGUI$stopApp\\n                                  if(!is.null(shinyGUI$lastpage)) \\n                                      lastpage <<- shinyGUI$lastpage\\n                                  if(!is.null(shinyGUI$temp_file))\\n                                      temp_file <<- shinyGUI$temp_file\\n                                  if(!is.null(shinyGUI$css))\\n                                      css <<- shinyGUI$css\\n                              }\\n                          })\\n                      \\n)" }\n'
line: b'{ "repo_name": "Netflix/Surus", "ref": "refs/heads/master", "path": "resources/R/RAD/R/anomaly_detection.R", "content": "#\' Time Series Anomaly Detection\\n#\' \\n#\' Fast C++ implementation of time series anomaly detection using Robust Principal Component Pursuit\\n#\' @param X a vector representing a time series, or a data frame where columns are time series.\\n#\' The length of this vector should be divisible by frequency.\\n#\' If X is a vector it will be cast to a matrix of dimension frequency by length(X)/frequency\\n#\' @param frequency the frequency of the seasonality of X\\n#\' @param dates optional vector of dates to be used as a time index in the output\\n#\' @param autodiff boolean. If true, use the Augmented Dickey Fuller Test to determine\\n#\' if differencing is needed to make X stationary\\n#\' @param forcediff boolean. If true, always compute differences\\n#\' @param scale boolean. If true normalize the time series to zero mean and unit variance\\n#\' @param L.penalty a scalar for the amount of thresholding in determining the low rank approximation for X.\\n#\' The default values are chosen to correspond to the smart thresholding values described in Candes\'\\n#\' Stable Principal Component Pursuit\\n#\' @param s.penalty a scalar for the amount of thresholding in determining the separation between noise and sparse outliers\\n#\' The default values are chosen to correspond to the smart thresholding values described in Zhou\'s\\n#\' Stable Principal Component Pursuit\\n#\' @param verbose boolean. If true print status updates while running optimization program\\n#\' @useDynLib RAD\\n#\' @importFrom tseries adf.test\\n#\' @details Robust Principal Component Pursuit is a matrix decomposition algorithm that seeks\\n#\' to separate a matrix X into the sum of three parts X = L + S + E. L is a low rank matrix representing\\n#\' a smooth X, S is a sparse matrix containing corrupted data, and E is noise. To convert a time series\\n#\' into the matrix X we take advantage of seasonality so that each column represents one full period, for\\n#\' example for weekly seasonality each row is a day of week and one column is one full week.\\n#\' \\n#\' While computing the low rank matrix L we take an SVD of X and soft threshold the singular values.\\n#\' This approach allows us to dampen all anomalies across the board simultaneously making the method\\n#\' robust to multiple anomalies. Most techniques such as time series regression and moving averages\\n#\' are not robust when there are two or more anomalies present.\\n#\' \\n#\' Empirical tests show that identifying anomalies is easier if X is stationary.\\n#\' The Augmented Dickey Fuller Test is used to test for stationarity - if X is not stationary\\n#\' then the time series is differenced before calling RPCP. While this test is abstracted away\\n#\' from the user differencing can be forced by setting the forcediff parameter.\\n#\' \\n#\' The thresholding values can be tuned for different applications, however we strongly\\n#\' recommend using the defaults which were proposed by Zhou.\\n#\' For more details on the choice of L.penalty and s.penalty\\n#\' please refer to Zhou\'s 2010 paper on Stable Principal Component Pursuit.\\n#\' \\n#\' The implementation of RPCP is done in C++ for high performance through RCpp.\\n#\' This function simply preprocesses the time series and calls RcppRPCP. \\n#\' @return \\n#\' \\\\itemize{\\n#\'   \\\\item X_transform. The transformation applied to the time series,\\n#\'   can be the identity or could be differencing\\n#\'   \\\\item L_transform. The low rank component in the transformed space\\n#\'   \\\\item S_transform. The sparse outliers in the transformed space\\n#\'   \\\\item E_transform. The noise in the transformed space\\n#\'   \\\\item X_original. The original time series\\n#\'   \\\\item time. The time index\\n#\'   \\\\item name. The name of the time series if X was a named data frame\\n#\' }\\n#\' @references\\n#\' The following are recommended educational material:\\n#\' \\\\itemize{\\n#\'   \\\\item Candes\' paper on RPCP \\\\url{http://statweb.stanford.edu/~candes/papers/RobustPCA.pdf}\\n#\'   \\\\item Zhou\'s follow up paper on Stable PCP \\\\url{http://arxiv.org/abs/1001.2363}\\n#\'   \\\\item Metamarkets Tech Blog on anomalies in time \\\\url{https://metamarkets.com/2012/algorithmic-trendspotting-the-meaning-of-interesting/}\\n#\' }\\n#\' @export\\n#\' @examples\\n#\' frequency = 7\\n#\' numPeriods = 10\\n#\' ts.sinusoidal = sin((2 * pi / frequency ) * 1:(numPeriods * frequency))\\n#\' ts = ts.sinusoidal\\n#\' ts = sin((2 * pi / frequency ) * 1:(numPeriods * frequency))\\n#\' ts[58:60] = 100\\n#\' ggplot_AnomalyDetection.rpca(AnomalyDetection.rpca(ts)) + ggplot2::theme_grey(base_size = 25)\\nAnomalyDetection.rpca = function(X, frequency=7, dates=NULL,\\n                                 autodiff = T,\\n                                 forcediff = F,\\n                                 scale = T,\\n                                 L.penalty = 1,\\n                                 s.penalty=1.4 / sqrt(max(frequency, ifelse(is.data.frame(X), nrow(X), length(X)) / frequency)),\\n                                 verbose=F) {\\n  if (is.vector(X) & !is.data.frame(X)) X = data.frame(y=X)\\n  time = if (is.null(dates)) 1:nrow(X) else dates\\n  \\n  #look through columns which are separate time series\\n  #transform each column vector into a matrix with nrow = observations per period\\n  #the number of columns will be equal to the number of periods\\n  rpca.ts = apply(X, 2, function(j) {\\n    j.init = j[1]\\n    useddiff = F\\n    if (forcediff) {\\n      useddiff = T\\n      j = c(0, diff(j))\\n    }\\n    else if (autodiff) {\\n      adf = suppressWarnings(tseries::adf.test(j))\\n      if (adf$p.value > .05) {useddiff = T; j = c(0, diff(j))}\\n    } \\n    \\n    if (scale) {\\n      j.global.mean = mean(j)\\n      j.global.sd = sd(j)\\n      j.matrix.standard.global = matrix((j - j.global.mean) / j.global.sd, nrow = frequency)\\n      j.matrix = j.matrix.standard.global  \\n    } else {\\n      j.global.mean = 0\\n      j.global.sd = 1\\n      j.matrix = matrix(j, nrow = frequency)\\n    }\\n    \\n    list(rpca = RcppRPCA(j.matrix, \\n                         Lpenalty = L.penalty, Spenalty = s.penalty, \\n                         verbose=verbose),\\n         mean = j.global.mean,\\n         sd = j.global.sd,\\n         diff = useddiff,\\n         j.init = j.init\\n    )\\n  })\\n  rpca.ts.stacked = lapply(rpca.ts, function(i) {\\n    if (i$diff) {\\n      X.orig = c(i$j.init + cumsum((as.vector(i$rpca$X)) * i$sd + i$mean))\\n      X.transform = (as.vector(i$rpca$X)) * i$sd + i$mean\\n      L.transform = (as.vector(i$rpca$L)) * i$sd + i$mean\\n      S.transform = (as.vector(i$rpca$S)) * i$sd\\n      E.transform = (as.vector(i$rpca$E)) * i$sd\\n      \\n      L.orig = cumsum(L.transform) + i$j.init\\n      X.rough = X.orig - L.orig\\n      \\n      #S.orig = cumsum(S.transform)\\n      #E.orig = X.orig - L.orig - S.orig\\n      \\n      ###       \\n      #\\n      #X.rough.rpca = RcppRPCA(matrix(X.rough, nrow(i$rpca$X), ncol(i$rpca$X)),\\n      #                        Lpenalty = 10,\\n      #                        Spenalty = 2 / sqrt(10))\\n      #S.orig = as.numeric(X.rough.rpca$S)\\n      #E.orig = X.orig - L.orig - S.orig\\n      \\n      ###\\n      S.orig = softThreshold(X.rough, 3 * (1/sqrt(2)) * sd(E.transform))\\n      E.orig = X.orig - (L.orig) - S.orig\\n      \\n      data.frame(X.transform = X.transform,\\n                 L.transform = L.transform,\\n                 S.transform = S.transform,\\n                 E.transform = E.transform,\\n                 X.orig = X.orig,\\n                 time = time)[-1,]\\n    }\\n    else {\\n      data.frame(X.transform = (as.vector(i$rpca$X)) * i$sd + i$mean,\\n                 L.transform = (as.vector(i$rpca$L)) * i$sd + i$mean,\\n                 S.transform = (as.vector(i$rpca$S)) * i$sd,\\n                 E.transform = (as.vector(i$rpca$E)) * i$sd,\\n                 X.orig = (as.vector(i$rpca$X)) * i$sd + i$mean,\\n                 time = time)\\n    }\\n  })\\n  names = unlist((mapply(function(df, name) { rep(name, nrow(df)) }, rpca.ts.stacked, names(rpca.ts))))\\n  #build a report containing anomaly data for all the columns found in X\\n  rpca.ts.stacked = cbind(do.call(\'rbind\', rpca.ts.stacked), name = as.vector(names))\\n  names(rpca.ts.stacked) = c(\\"X_transform\\", \\"L_transform\\", \\"S_transform\\", \\"E_transform\\",\\n                             \\"X_original\\",\\n                             \\"time\\", \\"name\\")\\n  \\n  return (rpca.ts.stacked)\\n}\\n\\n#\' ggplot for AnomalyDetection\\n#\' \\n#\' ggplot function which shows the low rank signal in blue, the random noise in green,\\n#\' and any outliers in red. If a transformation was applied, these signals will be plotted\\n#\' in the transformed space, along with the original time series\\n#\' @param anomalyDetection output from AnomalyDetection.rpca\\n#\' @import ggplot2\\n#\' @export\\n#\' @examples\\n#\' frequency = 7\\n#\' numPeriods = 10\\n#\' ts.sinusoidal = sin((2 * pi / frequency ) * 1:(numPeriods * frequency))\\n#\' ts = ts.sinusoidal\\n#\' ts = sin((2 * pi / frequency ) * 1:(numPeriods * frequency))\\n#\' ts[58:60] = 100\\n#\' ggplot_AnomalyDetection.rpca(AnomalyDetection.rpca(ts)) + ggplot2::theme_grey(base_size = 25)\\nggplot_AnomalyDetection.rpca = function(anomalyDetection) {\\n  ggplot2::ggplot(anomalyDetection, ggplot2::aes(time, X_original)) +\\n    ggplot2::geom_line(size = 1) +\\n    ggplot2::geom_line(ggplot2::aes(y = X_transform), size = 1, color = \\"black\\", linetype = \'dashed\') +\\n    ggplot2::geom_line(ggplot2::aes(y = L_transform), size = .5, color = \\"blue\\") +\\n    ggplot2::geom_line(ggplot2::aes(y = E_transform), size = .5, color = \\"green\\") +\\n    ggplot2::geom_point(data = subset(anomalyDetection, abs(S_transform) > 0), color = \\"red\\",\\n               ggplot2::aes(size = abs(S_transform))) +\\n    ggplot2::scale_size_continuous(range=c(4,6)) +\\n    ggplot2::facet_wrap(~name, scale = \\"free\\")    \\n}\\n\\nsoftThreshold = function(x, penalty) {\\n  sign(x) * pmax(abs(x) - penalty,0)\\n}\\n" }\n'
line: b'{ "repo_name": "Netflix/Surus", "ref": "refs/heads/master", "path": "resources/R/RAD/R/anomaly_detection_ma.R", "content": "AnomalyDetection.ma = function(X, frequency=7) {\\n  if (is.vector(X) & !is.data.frame(X)) X = data.frame(y=X)\\n  \\n  ma.ts = do.call(\'rbind\', apply(X, 2, function(j) {\\n    j.matrix = matrix(j, nrow= frequency)\\n    means = apply(j.matrix[,1:(ncol(j.matrix)-1)], 1, mean)\\n    sds = apply(j.matrix[,1:(ncol(j.matrix)-1)],1,sd)\\n    upperbounds = means + 1.6*sds\\n    lowerbounds = means - 1.6*sds\\n    anomalous = t(apply(cbind(upperbounds, lowerbounds, j.matrix), 1, function(i) {\\n      i[-(1:2)] > i[1] | i[-(1:2)] < i[2]\\n    }))\\n    data.frame(X = j,\\n               time = 1:length(j),\\n               anomaly = as.vector(anomalous))\\n  }))\\n  ma.ts = cbind(ma.ts, name = rep(names(X), each = nrow(X)))\\n  \\n  return (ma.ts)\\n}\\n\\nggplot_AnomalyDetection.ma = function(anomalyDetection) {\\n  ggplot(anomalyDetection,\\n         aes(x = time, y=X)) +\\n    geom_line(size = 1) +\\n    geom_point(data = subset(anomalyDetection, anomaly == T), color = \'red\', size = 6) +\\n    facet_wrap(~name, scale = \'free\')\\n}" }\n'
line: b'{ "repo_name": "AgResearch/KGD", "ref": "refs/heads/master", "path": "GBSRun.R", "content": "#!/bin/env Rscript\\n\\ngenofile <- \\"Example/HapMap.hmc.txt.gz\\"\\ngform <- \\"uneak\\"   # uneak (default), Tassel or chip\\n\\nsource(\\"GBS-Chip-Gmatrix.R\\")\\n\\nGfull      <- calcG()\\nGHWdgm.05  <- calcG(which(HWdis > -0.05), \\"HWdgm.05\\", npc = 4)  # recalculate using Hardy-Weinberg disequilibrium cut-off at -0.05\\n\\npedfile    <- \\"Example/Ped-GBS.csv\\"\\ngroupsfile <- \\"Example/Ped-Groups.csv\\"\\n\\nrel.thresh <- 0.2\\nGCheck     <- \\"GHWdgm.05$G5\\"\\nsource(\\"GBSPedAssign.R\\")\\n\\n# G5 <- GHWdgm.05$G5 save(G5,file=\'G5.RData\')\\n" }\n'
line: b'{ "repo_name": "gabraham/flashpca", "ref": "refs/heads/master", "path": "flashpcaR/tests/testthat/test_pca.R", "content": "context(\\"Checking PCA\\")\\n\\nn <- 200\\np <- 1000\\nndim <- 50\\nnextra <- 50\\n\\ntest_that(\\"Testing PCA with stand=\'binom\'\\", {\\n   X <- matrix(sample(0:2, n * p, replace=TRUE), n, p)\\n   q <- colMeans(X) / 2\\n   S <- scale(X, center=TRUE, scale=sqrt(q * (1 - q)))\\n\\n   f1 <- prcomp(S, center=FALSE, scale.=FALSE)\\n   f2 <- flashpca(X, ndim=ndim, mem=\\"low\\", nextra=nextra)\\n   f3 <- flashpca(X, ndim=ndim, mem=\\"high\\", nextra=nextra)\\n\\n   expect_equal(attr(S, \\"scaled:center\\"), f2$center)\\n   expect_equal(attr(S, \\"scaled:scale\\"), f2$scale)\\n   expect_equal(attr(S, \\"scaled:center\\"), f3$center)\\n   expect_equal(attr(S, \\"scaled:scale\\"), f3$scale)\\n\\n   r0 <- abs(diag(cor(f2$projection, f3$projection)))\\n   expect_equal(r0, rep(1.0, ndim), tolerance=1e-3)\\n\\n   r1 <- abs(diag(cor(f1$x[, 1:ndim], f2$projection)))\\n   expect_equal(r1, rep(1.0, ndim), tolerance=1e-3)\\n\\n   r2 <- abs(diag(cor(f1$x[, 1:ndim], f3$projection)))\\n   expect_equal(r2, rep(1.0, ndim), tolerance=1e-3)\\n})\\n\\n\\ntest_that(\\"Testing PCA with stand=\'sd\'\\", {\\n   X <- matrix(rnorm(n * p), n, p)\\n   S <- scale(X, center=TRUE, scale=TRUE)\\n\\n   f1 <- prcomp(S, center=FALSE, scale.=FALSE)\\n   f2 <- flashpca(X, ndim=ndim, stand=\\"sd\\", mem=\\"low\\", nextra=nextra)\\n   f3 <- flashpca(X, ndim=ndim, stand=\\"sd\\", mem=\\"high\\", nextra=nextra)\\n\\n   expect_equal(attr(S, \\"scaled:center\\"), f2$center)\\n   expect_equal(attr(S, \\"scaled:scale\\"), f2$scale)\\n   expect_equal(attr(S, \\"scaled:center\\"), f3$center)\\n   expect_equal(attr(S, \\"scaled:scale\\"), f3$scale)\\n\\n   r1 <- abs(diag(cor(f1$x[, 1:ndim], f2$projection)))\\n   expect_equal(r1, rep(1.0, ndim), tolerance=1e-3)\\n\\n   r2 <- abs(diag(cor(f1$x[, 1:ndim], f3$projection)))\\n   expect_equal(r2, rep(1.0, ndim), tolerance=1e-3)\\n})\\n\\ntest_that(\\"Testing PCA with stand=\'none\'\\", {\\n   X <- matrix(rnorm(n * p), n, p)\\n\\n   f1 <- prcomp(X, center=FALSE, scale.=FALSE)\\n   f2 <- flashpca(X, ndim=ndim, stand=\\"none\\", mem=\\"low\\", nextra=nextra)\\n   f3 <- flashpca(X, ndim=ndim, stand=\\"none\\", mem=\\"high\\", nextra=nextra)\\n\\n   expect_identical(numeric(0), f2$center)\\n   expect_identical(numeric(0), f2$scale)\\n   expect_identical(numeric(0), f3$center)\\n   expect_identical(numeric(0), f3$scale)\\n\\n   r1 <- abs(diag(cor(f1$x[, 1:ndim], f2$projection)))\\n   expect_equal(r1, rep(1.0, ndim), tolerance=1e-3)\\n\\n   r2 <- abs(diag(cor(f1$x[, 1:ndim], f3$projection)))\\n   expect_equal(r2, rep(1.0, ndim), tolerance=1e-3)\\n})\\n\\ntest_that(\\"Testing PCA with stand=\'center\'\\", {\\n   X <- matrix(rnorm(n * p), n, p)\\n   S <- scale(X, center=TRUE, scale=FALSE)\\n\\n   f1 <- prcomp(S, center=FALSE, scale.=FALSE)\\n   f2 <- flashpca(X, ndim=ndim, mem=\\"low\\", stand=\\"center\\", nextra=nextra)\\n   f3 <- flashpca(X, ndim=ndim, mem=\\"high\\", stand=\\"center\\", nextra=nextra)\\n\\n   expect_equal(attr(S, \\"scaled:center\\"), f2$center)\\n   expect_equal(rep(1, p), f2$scale)\\n   expect_equal(attr(S, \\"scaled:center\\"), f3$center)\\n   expect_equal(rep(1, p), f3$scale)\\n\\n   r1 <- abs(diag(cor(f1$x[, 1:ndim], f2$projection)))\\n   expect_equal(r1, rep(1.0, ndim), tolerance=1e-3)\\n\\n   r2 <- abs(diag(cor(f1$x[, 1:ndim], f3$projection)))\\n   expect_equal(r2, rep(1.0, ndim), tolerance=1e-3)\\n})\\n\\n" }\n'
line: b'{ "repo_name": "gabraham/flashpca", "ref": "refs/heads/master", "path": "test.R", "content": "\\nset.seed(32312)\\n\\nlibrary(flashpcaR)\\n\\nn <- 1000\\np <- 5000\\nX <- matrix(rnorm(n * p), n, p)\\nX <- scale(X, center=TRUE, scale=FALSE)\\n\\nsystem.time({\\n   S <- tcrossprod(X) / (nrow(X) - 1)\\n   e <- eigen(S)\\n})\\n\\nsystem.time({\\n   s <- svd(X)\\n})\\n\\nnthr <- 1\\ntol <- 1e-9\\nk <- 20\\nsystem.time({\\n   f1 <- flashpca(X, ndim=k, stand=\\"center\\", transpose=FALSE, tol=tol,\\n      num_threads=nthr, maxiter=100)\\n})\\nf2 <- flashpca(t(X), ndim=k, stand=\\"center\\", transpose=TRUE, tol=tol,\\n   num_threads=nthr, maxiter=100)\\nf3 <- flashpca(X, ndim=k, stand=\\"none\\", transpose=FALSE, tol=tol,\\n   num_threads=nthr, maxiter=100)\\nf4 <- flashpca(t(X), ndim=k, stand=\\"none\\", transpose=TRUE, tol=tol,\\n   num_threads=nthr, maxiter=100)\\n\\n(r <- cbind(\\n   e=e$val[1:k],\\n   f1=f1$val,\\n   f2=f2$val,\\n   f3=f3$val,\\n   f4=f4$val,\\n   s=s$d[1:k]^2 / (nrow(X) - 1)\\n))\\n\\ncor(r)\\n\\n" }\n'
line: b'{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "inst/ShinyStan/server_files/pages/diagnose/server/multitrace.R", "content": "# \\n# # multiparameter traceplots -----------------------------------------------\\n# calc_height_trace_plot <- reactive({\\n#   params <- input$multitrace_params\\n#   grid <- FALSE\\n#   if (!is.null(input$multitrace_layout)) {\\n#     if (input$multitrace_layout == \\"Grid\\") grid <- TRUE\\n#   }\\n#   params <- .update_params_with_groups(params, param_names)\\n#   LL <- length(params)\\n#   if (LL == 0) LL <- 4\\n#   if (LL == 1) LL <- 2\\n#   if (grid) {\\n#     if (LL > 5) return(30*LL)\\n#     if (LL < 5) return(60*LL)\\n#   }\\n#   round(100*LL)\\n# })\\n# \\n# # multitrace_plot\\n# multitrace_plot <- reactive({\\n#   validate(need(!is.null(input$multitrace_rect), message = \\"Loading...\\"))\\n#   x1 <- input$multi_xzoom[1]\\n#   x2 <- input$multi_xzoom[2]\\n#   dat <- samps_all[x1:x2,,,drop=FALSE]\\n#   # zoom <- \\"On\\"\\n#   do.call(\\".param_trace_multi\\", args = list(\\n#     params      = input$multitrace_params,\\n#     all_param_names = param_names,\\n#     dat         = dat,\\n#     chain       = input$multitrace_chain,\\n#     warmup_val  = warmup_val,\\n#     palette     = input$multitrace_palette ,\\n#     rect        = input$multitrace_rect,\\n#     rect_color  = \\"skyblue\\",\\n#     rect_alpha  = input$multitrace_rect_alpha,\\n#     layout      = input$multitrace_layout,\\n#     x1          = x1,\\n#     x2          = x2\\n#   ))\\n# })\\n# \\n# output$multitrace_plot_out <- renderPlot({\\n#   x <- multitrace_plot()\\n#   suppressWarnings(print(x)) # this avoids warnings about removing rows when using tracezoom feature\\n# }, height = calc_height_trace_plot, bg = \\"transparent\\")\\n# \\n# # download the plot\\n# output$download_multitrace <- downloadHandler(\\n#   filename = paste0(\'shinystan_multitrace.RData\'),\\n#   content = function(file) {\\n#     shinystan_multitrace <- multitrace_plot()\\n#     save(shinystan_multitrace, file = file)\\n#   }\\n# )\\n" }\n'
line: b'{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "R/generate_quantity.R", "content": "# shinystan is free software; you can redistribute it and/or modify it under the\\n# terms of the GNU General Public License as published by the Free Software\\n# Foundation; either version 3 of the License, or (at your option) any later\\n# version.\\n# \\n# shinystan is distributed in the hope that it will be useful, but WITHOUT ANY\\n# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR\\n# A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\\n# \\n# You should have received a copy of the GNU General Public License along with\\n# this program; if not, see <http://www.gnu.org/licenses/>.\\n\\n\\n#\' Add new quantity to shinystan object\\n#\' \\n#\' Add to shinystan object a new parameter as a function of one or two existing\\n#\' parameters.\\n#\' \\n#\' @export\\n#\' @template args-sso\\n#\' @param fun Function to call, i.e. \\\\code{function(param1)} or \\n#\'   \\\\code{function(param1,param2)}. See Examples, below.\\n#\' @param param1 Name of first parameter as character string.\\n#\' @param param2 Optional. Name of second paramter as character string.\\n#\' @param new_name Name for the new parameter as character string.\\n#\'   \\n#\' @return sso, updated. See Examples.\\n#\' \\n#\' @template seealso-drop_parameters\\n#\'\\n#\' @examples\\n#\' # Using example shinystan object \'eight_schools\'\\n#\' sso <- eight_schools\\n#\' sso <- generate_quantity(sso, fun = function(x) x^2, \\n#\'                          param1 = \\"tau\\", new_name = \\"tau_sq\\")\\n#\' sso <- generate_quantity(sso, fun = \\"-\\", \\n#\'                          param1 = \\"theta[1]\\", param2 = \\"theta[2]\\", \\n#\'                          new_name = \\"theta1minus2\\")\\n#\'                          \\ngenerate_quantity <- function(sso, param1, param2, fun, new_name) {\\n  sso_check(sso)\\n  if (isTRUE(new_name %in% slot(sso, \\"param_names\\")))\\n    stop(paste(\\"There is already a parameter named\\", new_name))\\n  \\n  message(\\"\\\\nThis might take a moment for large shinystan objects...\\")\\n  \\n  two_params <- !missing(param2)\\n  posterior <- slot(sso, \\"posterior_sample\\")\\n  dims <- dim(posterior)\\n  ndim <- length(dims)\\n  if (ndim == 3) {\\n    # i.e. multiple chains\\n    x_samp <- posterior[, , param1]\\n    if (two_params)\\n      y_samp <- posterior[, , param2]\\n  }\\n  if (ndim == 2) {\\n    # i.e. only 1 chain\\n    x_samp <- posterior[, param1]\\n    if (two_params)\\n      y_samp <- posterior[, param2]\\n  }\\n  \\n  arglist <- if (two_params)\\n    list(x_samp, y_samp) else list(x_samp)\\n  temp <- do.call(fun, args = arglist)\\n  \\n  new_dim <- dims\\n  new_dim[[ndim]] <- new_dim[[ndim]] + 1\\n  new_dim_names <- dimnames(posterior)\\n  new_dim_names[[ndim]] <- c(new_dim_names[[ndim]], new_name)\\n  posterior <-\\n    array(data = c(posterior, temp),\\n          dim = new_dim,\\n          dimnames = new_dim_names)\\n  \\n  param_dims_new <- slot(sso, \\"param_dims\\")\\n  param_dims_new[[new_name]] <- numeric(0)\\n  sso_new <- as.shinystan(\\n    posterior,\\n    model_name = slot(sso, \\"model_name\\"),\\n    burnin = slot(sso, \\"n_warmup\\"),\\n    param_dims = param_dims_new\\n  )\\n  slot(sso_new, \\"summary\\") <-\\n    shinystan_monitor(posterior, warmup = slot(sso, \\"n_warmup\\"))\\n  \\n  slot_names <- c(\\"sampler_params\\", \\"model_code\\", \\"user_model_info\\", \\"misc\\")\\n  for (sn in slot_names)\\n    slot(sso_new, sn) <- slot(sso, sn)\\n  \\n  sso_new\\n}\\n" }\n'
line: b'{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "R/sso-metadata.R", "content": "# shinystan is free software; you can redistribute it and/or modify it under the\\n# terms of the GNU General Public License as published by the Free Software\\n# Foundation; either version 3 of the License, or (at your option) any later\\n# version.\\n# \\n# shinystan is distributed in the hope that it will be useful, but WITHOUT ANY\\n# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR\\n# A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\\n# \\n# You should have received a copy of the GNU General Public License along with\\n# this program; if not, see <http://www.gnu.org/licenses/>.\\n\\n\\n#\' View or change metadata associated with a shinystan object\\n#\' \\n#\' @name shinystan-metadata\\n#\' @template args-sso\\n#\' \\n#\' @template seealso-as.shinystan\\n#\' @template seealso-drop_parameters\\n#\' @template seealso-generate_quantity\\n#\' \\n#\' @examples \\n#\' # use eight_schools example object\\n#\' sso <- eight_schools\\n#\' \\nNULL\\n\\n# sso_info ----------------------------------------------------------------\\n#\' @rdname shinystan-metadata\\n#\' @export\\n#\' \\n#\' @return \\\\code{sso_info} prints basic metadata including number of parameters, \\n#\'   chains, iterations, warmup iterations, etc. It does not return anything.\\n#\' \\n#\' @examples \\n#\' ################\\n#\' ### sso_info ###\\n#\' ################\\n#\' \\n#\' sso_info(sso)\\n#\'\\nsso_info <- function(sso) {\\n  sso_check(sso)\\n  sso_name <- deparse(substitute(sso))\\n  has_notes <-\\n    sso@user_model_info != \\"Use this space to store notes about your model\\"\\n  has_code <-\\n    sso@model_code != \\"Use this space to store your model code\\"\\n  \\n  cat(\\n    sso_name,\\n    \\"---------------------\\",\\n    paste(\\"Model name:\\", sso@model_name),\\n    paste(\\"Parameters:\\", length(sso@param_names)),\\n    paste(\\"Parameter groups:\\", length(names(sso@param_dims))),\\n    paste(\\"Chains:\\", sso@n_chain),\\n    paste(\\"Iterations:\\", sso@n_iter),\\n    paste(\\"Warmup:\\", sso@n_warmup),\\n    paste(\\"Has model code:\\", has_code),\\n    paste(\\"Has user notes:\\", has_notes),\\n    sep = \\"\\\\n\\"\\n  )\\n}\\n\\n\\n\\n# model_code --------------------------------------------------------------\\n#\' @rdname shinystan-metadata\\n#\' @export\\n#\' @param code A string, containing model code to be added, that can be\\n#\'   used as an argument to \\\\code{\\\\link{cat}}. See \\\\strong{Examples}.\\n#\'   \\n#\' @return \\\\code{model_code} returns or replaces model code stored in a \\n#\'   shinystan object. If \\\\code{code} is \\\\code{NULL} then any existing model\\n#\'   code stored in \\\\code{sso} is returned as a character string. If \\\\code{code}\\n#\'   is specified then an updated shinystan object is returned with \\\\code{code}\\n#\'   added. For shinystan objects created from stanfit (\\\\pkg{rstan}) and stanreg\\n#\'   (\\\\pkg{rstanarm}) objects, model code is automatically taken from that\\n#\'   object and does not need to be added manually. From within the ShinyStan\\n#\'   interface model code can be viewed on the \\\\strong{Model Code} page.\\n#\'\\n#\' @examples\\n#\' ##################\\n#\' ### model_code ###\\n#\' ##################\\n#\' \\n#\' # view model code in example shinystan object \'eight_schools\'\\n#\' cat(model_code(sso))\\n#\' \\n#\' # change the model code in sso \\n#\' # some jags style code\\n#\' my_code <- \\"\\n#\'  model {\\n#\'    for (i in 1:length(Y)) {\\n#\'      Y[i] ~ dpois(lambda[i])\\n#\'      log(lambda[i]) <- inprod(X[i,], theta[])\\n#\'    }\\n#\'    for (j in 1:J) {\\n#\'      theta[j] ~ dt(0.0, 1.0, 1.0)\\n#\'    }\\n#\'  }\\n#\' \\"\\n#\' sso <- model_code(sso, my_code)\\n#\' cat(model_code(sso))\\n#\'\\nmodel_code <- function(sso, code = NULL) {\\n  sso_check(sso)\\n  validate_model_code(code)\\n  \\n  if (is.null(code))\\n    return(slot(sso, \\"model_code\\"))\\n  \\n  slot(sso, \\"model_code\\") <- code\\n  message(\\n    paste0(\\n      \\"Successfully added code.\\",\\n      \\"\\\\nYou can view the code in the\\",\\n      \\"ShinyStan GUI on the \'Model Code\' page.\\"\\n    )\\n  )\\n  sso\\n}\\n\\nvalidate_model_code <- function(code) {\\n  if (is.null(code) || is.character(code)) {\\n    invisible(TRUE)\\n  } else {\\n    stop(\\"Model code should be NULL or a string\\", call. = FALSE)\\n  }\\n}\\n\\n\\n\\n# notes -------------------------------------------------------------------\\n#\' @rdname shinystan-metadata\\n#\' @export\\n#\' @param note A string containing a note to add to any existing notes\\n#\'   or replace existing notes, depending on the value of \\\\code{replace}.\\n#\' @param replace If \\\\code{TRUE} the existing notes are overwritten by \\n#\'   \\\\code{note} if \\\\code{note} is specified. If \\\\code{FALSE} (the default) \\n#\'   if \\\\code{note} is specified then its content is appended to the existing\\n#\'   notes.\\n#\'   \\n#\' @return \\\\code{notes} returns, amends, or replaces notes stored in a shinystan\\n#\'   object. If \\\\code{note} is \\\\code{NULL} then any existing notes stored in \\n#\'   \\\\code{sso} are returned as a character string. If \\\\code{note} is specified \\n#\'   then an updated shinystan object is returned with either \\\\code{note} added \\n#\'   to the previous notes (if \\\\code{replace=FALSE}) or overwritten by \\n#\'   \\\\code{note} (if \\\\code{replace = TRUE}). From within the ShinyStan\\n#\'   interface, notes are viewable on the \\\\strong{Notepad} page.\\n#\'   \\n#\' @examples \\n#\' #############\\n#\' ### notes ###\\n#\' #############\\n#\' \\n#\' # view existing notes\\n#\' notes(sso)\\n#\' \\n#\' # add a note to the existing notes\\n#\' sso <- notes(sso, \\"New note\\")\\n#\' notes(sso)\\n#\' cat(notes(sso))\\n#\' \\n#\' # replace existing notes\\n#\' sso <- notes(sso, \\"replacement note\\", replace = TRUE)\\n#\' notes(sso)\\n#\'  \\nnotes <- function(sso, note = NULL, replace = FALSE) {\\n  sso_check(sso)\\n  if (is.null(note))\\n    return(slot(sso, \\"user_model_info\\"))\\n  \\n  if (!is.character(note) || !isTRUE(length(note) == 1))\\n    stop(\\"\'note\' should be a single string\\")\\n  \\n  slot(sso, \\"user_model_info\\") <- if (replace)\\n    note else c(slot(sso, \\"user_model_info\\"), paste0(\\"\\\\n\\\\n\\", note))\\n  \\n  message(\\n    paste(\\n      \\"Successfully added note.\\",\\n      \\"\\\\nYou can view the notes in the\\",\\n      \\"ShinyStan GUI on the \'Notepad\' page.\\"\\n    )\\n  )\\n  sso\\n}\\n\\n\\n\\n# model_name (renaming) -----------------------------------------------------#\' \\n#\' @rdname shinystan-metadata\\n#\' @export\\n#\' @param name A string giving the new model name to use.\\n#\'   \\n#\' @return \\\\code{model_name} returns or replaces the model name associated with \\n#\'   a shinystan object. If \\\\code{name} is \\\\code{NULL} then the current model\\n#\'   name is returned. If \\\\code{name} is specified then \\\\code{sso} is returned\\n#\'   with an updated model name.\\n#\' \\n#\' @examples\\n#\' ##################\\n#\' ### model_name ###\\n#\' ##################\\n#\' \\n#\' # view model name\\n#\' model_name(sso)\\n#\' \\n#\' # change model name\\n#\' sso <- model_name(sso, \\"some other name\\")\\n#\' identical(model_name(sso), \\"some other name\\")\\n#\' \\nmodel_name <- function(sso, name = NULL) {\\n  sso_check(sso)\\n  if (is.null(name))\\n    return(slot(sso, \\"model_name\\"))\\n  \\n  if (!is.character(name) || !isTRUE(length(name) == 1))\\n    stop(\\"\'name\' should be a single string\\")\\n  \\n  slot(sso, \\"model_name\\") <- name\\n  message(paste(\\"Successfully changed model name to\\", name))\\n  sso\\n}\\n\\n\\n# nocov start\\n#\' rename_model (deprecated)\\n#\' \\n#\' This function is deprecated and will be removed in a future release. Please \\n#\' use the \\\\code{\\\\link{model_name}} function instead.\\n#\' \\n#\' @export\\n#\' @keywords internal\\n#\' @param sso,new_model_name Use the \\\\code{\\\\link{model_name}} function instead.\\n#\' \\nrename_model <- function(sso, new_model_name) {\\n  .Deprecated(\\"model_name()\\")\\n  model_name(sso, new_model_name)\\n}\\n# nocov end\\n" }\n'
line: b'{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "R/launch_shinystan.R", "content": "# shinystan is free software; you can redistribute it and/or modify it under the\\n# terms of the GNU General Public License as published by the Free Software\\n# Foundation; either version 3 of the License, or (at your option) any later\\n# version.\\n# \\n# shinystan is distributed in the hope that it will be useful, but WITHOUT ANY\\n# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR\\n# A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\\n# \\n# You should have received a copy of the GNU General Public License along with\\n# this program; if not, see <http://www.gnu.org/licenses/>.\\n\\n\\n#\' Launch the ShinyStan app\\n#\' \\n#\' Launch the ShinyStan app in the default web browser. RStudio users also have\\n#\' the option of launching the app in RStudio\'s pop-up Viewer.\\n#\' \\n#\' @export\\n#\' @param object An object of class shinystan, stanfit, or stanreg. To use other\\n#\'   types of objects first create a shinystan object using \\n#\'   \\\\code{\\\\link{as.shinystan}}.\\n#\' @param rstudio Only relevant for RStudio users. The default (\\\\code{FALSE}) is\\n#\'   to launch the app in the user\'s default web browser rather than RStudio\'s\\n#\'   pop-up Viewer. Users can change the default to \\\\code{TRUE} by setting the\\n#\'   global option \\\\code{options(shinystan.rstudio = TRUE)}.\\n#\' @param ... Optional arguments passed to \\\\code{\\\\link[shiny]{runApp}}.\\n#\' \\n#\' @return The \\\\code{launch_shinystan} function is used for the side effect of \\n#\'   starting the ShinyStan app, but it also returns a shinystan object, an\\n#\'   instance of S4 class \\\\code{\\"shinystan\\"}.\\n#\'   \\n#\' @template seealso-as.shinystan \\n#\' @template seealso-update_sso \\n#\' @template seealso-demo\\n#\'   \\n#\'   \\n#\' @examples\\n#\' \\\\dontrun{\\n#\' #######################################\\n#\' # Example 1: \'sso\' is a shinystan object\\n#\' #######################################\\n#\' \\n#\' # Just launch shinystan\\n#\' launch_shinystan(sso)\\n#\' \\n#\' # Launch shinystan and replace sso with an updated version of itself\\n#\' # if any changes are made to sso while using the app\\n#\' sso <- launch_shinystan(sso)\\n#\' \\n#\' # Launch shinystan but save any changes made to sso while running the app\\n#\' # in a new shinystan object sso2. sso will remained unchanged. \\n#\' sso2 <- launch_shinystan(sso) \\n#\' \\n#\' #######################################\\n#\' # Example 2: \'sf\' is a stanfit object\\n#\' #######################################\\n#\' \\n#\' # Just launch shinystan\\n#\' launch_shinystan(sf)\\n#\' \\n#\' # Launch shinystan and save the resulting shinystan object\\n#\' sf_sso <- launch_shinystan(sf)\\n#\' \\n#\' # Now sf_sso is a shinystan object and so Example 1 (above) applies when\\n#\' # using sf_sso. \\n#\' \\n#\' #######################################\\n#\' # Example 3: \'fit\' is an mcmc.list, array or list of matrices\\n#\' #######################################\\n#\'\\n#\' # First create shinystan object (see ?as.shinystan for full details)\\n#\' fit_sso <- as.shinystan(fit, model_name = \\"Example\\")\\n#\' \\n#\' # Now fit_sso is a shinystan object and so Example 1 (above) applies.\\n#\' }\\n#\'\\nlaunch_shinystan <- function(object, \\n                             rstudio = getOption(\\"shinystan.rstudio\\"), \\n                             ...) {\\n  if (is.shinystan(object)) {\\n    sso_check(object)\\n  } else if (is.stanreg(object) || is.stanfit(object)) {\\n    message(\\"\\\\nCreating shinystan object...\\")\\n    object <- as.shinystan(object)\\n  }\\n  if (!is.shinystan(object))\\n    stop(\\"\'object\' is not a valid input. See help(\'launch_shinystan\').\\")\\n  \\n  message(\\"\\\\nLaunching ShinyStan interface... \\",\\n          \\"for large models this  may take some time.\\")\\n  invisible(launch(object, rstudio, ...))\\n}\\n\\n\\n#\' ShinyStan demo\\n#\'\\n#\' @aliases eight_schools\\n#\' @export\\n#\' @inheritParams launch_shinystan\\n#\' @param demo_name The name of the demo. Currently \\\\code{\\"eight_schools\\"} is \\n#\'   the only option, but additional demos may be available in future releases.\\n#\'   \\\\describe{\\n#\'   \\\\item{\\\\code{eight_schools}}{Hierarchical meta-analysis model. See \\n#\'    \\\\emph{Meta Analysis} chapter of the Stan manual (chapter 11.2 in version\\n#\'    2.9), \\\\url{http://mc-stan.org/documentation/}.}\\n#\'   }\\n#\' @return An S4 shinystan object.\\n#\'   \\n#\' @template seealso-launch\\n#\' @template seealso-as.shinystan\\n#\' \\n#\' @examples\\n#\' \\\\dontrun{\\n#\' # launch demo but don\'t save a shinystan object\\n#\' launch_shinystan_demo() \\n#\' \\n#\' # launch demo and save the shinystan object for the demo \\n#\' sso_demo <- launch_shinystan_demo()\\n#\' }\\n#\'\\nlaunch_shinystan_demo <- function(demo_name = \\"eight_schools\\",\\n                                  rstudio = getOption(\\"shinystan.rstudio\\"),\\n                                  ...) {\\n  demo_name <- match.arg(demo_name)\\n  demo_object <- get(demo_name)\\n  invisible(launch(demo_object, rstudio = rstudio, ...))\\n}\\n\\n# Internal launch function \\n# @param sso shinystan object\\n# @param rstudio launch in rstudio viewer instead of web browser? \\n# @param ... passed to shiny::runApp\\nlaunch <- function(sso, rstudio = FALSE, ...) {\\n  launch.browser <- if (!rstudio) \\n    TRUE else getOption(\\"shiny.launch.browser\\", interactive())\\n  \\n  .sso_env$.SHINYSTAN_OBJECT <- sso  # see zzz.R for .sso_env\\n  on.exit(.sso_env$.SHINYSTAN_OBJECT <- NULL, add = TRUE)\\n  shiny::runApp(system.file(\\"ShinyStan\\", package = \\"shinystan\\"), \\n                launch.browser = launch.browser, ...)\\n}\\n" }\n'
line: b'{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "inst/ShinyStan/ui_files/model_code.R", "content": "sidebarLayout(\\n  sidebarPanel(\\n    width = 3,\\n    style = \\"height: 550px;\\",\\n    br(),\\n    h4(\\"Model Code\\"),\\n    helpText(\\n      style = \\"font-size: 12px;\\",\\n      p(\\n        \\"Model code will be displayed here each\\",\\n        \\"time you launch ShinyStan with this shinystan object.\\"\\n      )\\n    ),\\n    br(),\\n    actionButton(\\n      \\"save_user_model_code\\",\\n      label = \\"Save code\\",\\n      icon = icon(\\"save\\")\\n    ),\\n    div(style = \\"font-size: 11px;\\", textOutput(\\"user_code_saved\\")),\\n    conditionalPanel(\\n      condition = \\"input.save_user_model_code > 0\\",\\n      br(),\\n      save_and_close_reminder(\\"save_user_model_code_safe_quit\\")\\n    )\\n  ),\\n  mainPanel(\\n    width = 9,\\n    br(), br(),\\n    tags$textarea(\\n      id = \\"user_model_code\\",\\n      wrap = \\"off\\",\\n      cols = 80,\\n      rows = 20,\\n      .model_code\\n    )\\n  )\\n)\\n" }\n'
line: b'{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "inst/ShinyStan/server_utils.R", "content": "# function to suppress unnecessary warnings and messages generated by ggplot \\nsuppress_and_print <- function(x) {\\n  suppressMessages(suppressWarnings(print(x)))\\n}\\n" }\n'
line: b'{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "inst/ShinyStan/ui_files/help.R", "content": "div(\\n  class = \\"help-glossary-div\\",\\n  br(), br(),\\n  div(\\n    class = \\"help-glossary-nav-container\\",\\n    navlistPanel(\\n      well = TRUE,\\n      id = \\"help_navlist\\",\\n      \\"Topics\\",\\n      tabPanel(\\n        \\"Questions, bugs, and new features\\",\\n        div(\\n          class = \\"glossary-entry\\",\\n          h4(\\"Stan users group\\"),\\n          p(\\n            \\"To ask a question or suggest a new feature visit the\\",\\n            a(\\n              \\"Stan users message board.\\", \\n              href = \\"https://groups.google.com/forum/?fromgroups#!forum/stan-users\\"\\n            )\\n          ),\\n          br(),\\n          h4(\\"GitHub issue tracker\\"),\\n          p(\\n            \\"To report a bug  or suggest a new feature visit the\\",\\n            a(\\n              \\"GitHub issue tracker.\\", \\n              href = \\"https://github.com/stan-dev/shinystan/issues\\"\\n            )\\n          )\\n        )\\n      ),\\n      tabPanel(\\n        \\"Saving plots\\",\\n        div(\\n          class = \\"glossary-entry\\",\\n          h4(\\"Saving plots as ggplot2 objects\\"),\\n          p(\\n            \\"Clicking on a \'Save ggplot2 object\' button will be save an .RData\\n            file that you can load into your Global Environment using the\\",\\n            code(\\"load\\"),\\n            \\"function in R.\\n            You can then make changes to the plot using the functions in the\\n            ggplot2 package.\\"\\n          ),\\n          p(\\n            \\"Any plot that can be saved as a ggplot2 object can also be saved\\n            as a PDF.\\"\\n          )\\n      )), \\n      tabPanel(\\n        \\"Large models and launch speed\\",\\n        div(\\n          class = \\"glossary-entry\\",\\n          h4(\\"Launching ShinyStan faster\\"),\\n          p(\\n            \\"The\\", code(\\"drop_parameters\\"), \\"function in the\\", \\n            strong(\\"shinystan\\"), \\"R package will allow you to reduce the size\\", \\n            \\"of a shinystan object by removing parameters.\\", \\n            \\"See\\", code(\\"help(\'drop_parameters\', \'shinystan\')\\"), \\n            \\"for the documentation.\\"\\n          ),\\n          p(\\n            \\"Additionally, for large models, the\\", code(\\"launch_shinystan\\"),\\n            \\"function will launch the app faster when used with a\\",\\n            \\"shinystan object rather than a stanfit object\\",\\n            \\"(because no conversion is required).\\",\\n            \\"If ShinyStan takes a long time to launch for your\\",\\n            \\"model then it can help to first create a\\",\\n            \\"shinystan object using the\\", code(\\"as.shinystan\\"), \\"function.\\",\\n            \\"Alternatively, the first time you launch\\",\\n            \\"ShinyStan using a stanfit object, a shinystan\\",\\n            \\"object will be returned if you assign the value of\\",\\n            code(\\"launch_shinystan\\"),\\n            \\"to a name, e.g.\\"\\n          ),\\n          p(code(\\"sso <- launch_shinystan(stanfit)\\")),\\n          p(\\"rather than just\\"),\\n          p(code(\\"launch_shinystan(stanfit)\\")),\\n          p(\\n            \\"The next time you launch ShinyStan for the same\\",\\n            \\"model you can launch it using\\", code(\\"sso\\"), \\"rather than\\",\\n            code(\\"stanfit\\"), \\"and it should be quicker to launch.\\",\\n            \\"If it is still too slow then dropping some large parameters\\", \\n            \\"from the shinystan object is the best solution.\\"\\n          )\\n        )\\n      )\\n    )\\n  ),\\n  br(), br()\\n)\\n" }\n'
line: b'{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "inst/ShinyStan/ui_files/diagnostics_treedepth.R", "content": "# treedepth\\ndiv(\\n  class = \\"diagnostics-navlist-tabpanel\\",\\n  fluidRow(\\n    column(\\n      width = 7,\\n      help_dynamic,\\n      dygraphOutput_175px(\\"dynamic_trace_diagnostic_treedepth_out\\"),\\n      br(), br(),\\n      plotOutput(\\"treedepth_vs_lp_out\\", height = \\"150px\\")\\n    ),\\n    column(width = 5, plotOutput_400px(\\"treedepth_vs_accept_stat_out\\"))\\n  ),\\n  splitLayout(\\n    plotOutput(\\"treedepth_ndivergent_hist_out\\", height = \\"125px\\"),\\n    plotOutput(\\"treedepth_ndivergent0_hist_out\\", height = \\"125px\\"),\\n    plotOutput(\\"treedepth_ndivergent1_hist_out\\", height = \\"125px\\")\\n  ),\\n  br()\\n)\\n" }\n'
line: b'{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "man-roxygen/seealso-generate_quantity.R", "content": "#\' @seealso \\\\code{\\\\link{generate_quantity}} to add a new quantity to a shinystan\\n#\'   object.\\n" }\n'
line: b'{ "repo_name": "data-steve/data-steve.github.io", "ref": "refs/heads/master", "path": "_backlog/ggdumbbell2.R", "content": "pacman::p_load(dplyr, ggplot2, tidyr)\\nmkdat <- function(x,y,cat){\\n\\n}\\n\\ndat <-data.frame(\\"Jessica\\" = c(8.6, 9.3, 9.34, 9.1, 9),\\n                 \\"Francine\\" = c(9.4, 8.88, 9.2, 9.24, 9.34),\\n            \\"Events\\" = factor(1:5,labels=c(\\"sprint\\", \\"hurdles\\",\\"javelin\\", \\"discus\\", \\"hi-jump\\")))\\n\\nggdumbbell <- function(d\\n                       ,colors=c(\\"green4\\",\\"red3\\",\\"brown\\")\\n                       ,sz=17){\\n    nms <- names(d)\\n    names(d) <- c(\\"xvar\\",\\"yvar\\", \\"fvar\\")\\n    df <- d %>%\\n        dplyr::mutate(\\n               abs_diff = abs(xvar-yvar),\\n               pos_neg = sign(xvar-yvar),\\n               midpoint = abs_diff/2,\\n               clr = ifelse(pos_neg>0, colors[1], colors[2])\\n          ) %>%\\n      dplyr::rowwise() %>%\\n      dplyr::mutate(xmin = min(xvar,yvar),\\n             xmax = max(xvar,yvar)) %>% ungroup\\n\\n    df_tall <- df %>%\\n        tidyr::gather(keys, values, -c(fvar, xmin, xmax, midpoint, pos_neg, abs_diff, clr)) %>%\\n        mutate(keys=ifelse(keys==\\"xvar\\", nms[1], nms[2]))\\n\\n    l <-  as.character(df[[\\"fvar\\"]][as.integer(df[[\\"fvar\\"]])==max(as.integer(df[[\\"fvar\\"]]))])\\n    ll <- if(d[1,1]<d[1,2]) nms[1:2] else nms[2:1]\\nggplot(df_tall, aes(y=fvar, x=values) )+\\n  geom_segment(data=df, aes(x=xmin, xend=xmax, y=fvar, yend=fvar, alpha=abs_diff), color=df$clr, size=3) +\\n      geom_point(color = colors[3], size=5.5)  +\\n      geom_point(shape=21, color = colors[3], size=4.8, aes(fill=keys))  +\\n\\n      scale_fill_manual(values = c(\\"white\\", colors[3]), guide=FALSE) +\\n\\n      theme_bw()  +\\n  theme(text=element_text(size=sz),legend.position=\\"none\\", panel.grid = element_blank() )  +\\n  annotate(\\"text\\"\\n           , x = c(df[df$\\"fvar\\"==l,]$xmin,df[df$\\"fvar\\"==l,]$xmax)\\n           , y= c(df[df$\\"fvar\\"==l,]$fvar,df[df$\\"fvar\\"==l,]$fvar)\\n           , label=ll\\n           , color = colors[3], size=4.8 , vjust = 2.5) +\\n  labs(list(x=\\"Values\\",y=nms[3])) +\\n  annotate(\\"text\\", x=8.75, y=5, label=\\"Green/Red signifies\\\\nhigher/lower performance.\\\\n\\\\n Color intensity signifies\\\\nimporance of difference.\\", color=colors[3], size=7, vjust = .75)\\n}\\n\\nggdumbbell(dat) +\\n  # ggtitle(\\"Jessica\'s performance against Francine\\") +\\n  ggsave(\\"~/Documents/repos/data-steve.github.io/images/dumbbell.png\\")\\n" }\n'
line: b'{ "repo_name": "langcog/wordbank", "ref": "refs/heads/master", "path": "incoming_data/TEDS/convert_TEDS.R", "content": "# DEMOGRAPHIC INFORMATION\\n# Id_twin (unique twin identifier)\\n# Id_fam (family identifier, unique to each twin pair)\\n# twin   (twin birth order, 1=elder 2=younger)\\n# Random  (permits selecting one twin at random from each pair, to give independence of data, coded 0/1)\\n# Aethnic (ethnic category: 1=white, 0=other)\\n# Sex1 (gender of twin: 0=female, 1=male)\\n# Zygos (twin pair zygosity, 1=MZ 2=DZ)\\n# Amohqual (maternal educational qualifications, 8-point ordinal scale)\\n# Ases  (composite social class based on parental education and occupation, standardised with mean 0.0 and SD 1.0)\\n# \\n# AGE 2 MEASURES\\n# Brepage1  (age of twin on completion of parent-reported measures),\\n# converted to months and rounded down\\n# Bvc0011 to bvc1001  (100 item vocabulary checklist)\\n# Bvocab1 (vocabulary total on this scale)\\n# Bwu061  (does child combine words)\\n# Bs01s1 through bs12s1  (12 sentence complexity pairs)\\n# \\n# AGE 3 MEASURES\\n# Crepage1 (age of twin on completion of parent-reported measures),\\n# converted to months and rounded down\\n# Cvc0001 (not yet talking)\\n# Cvc0011 to cvc1001  (100 item vocabulary checklist)\\n# Cvocab1   (vocabulary total on this scale)\\n# Cs00s1 (does child combine words)\\n# Cs01s1 through cs12s1) (12 sentence complexity pairs)\\n# \\n# AGE 4 MEASURES\\n# Drepage1 (age of twin on completion of parent-reported measures),\\n# converted to months and rounded down\\n# Dvd011 through dvc481 (48 item vocabulary checklist)\\n# Dsay011  (overall evaluation of child\xe2\x80\x99s language/sentences, 6 point scale)\\n\\nlibrary(dplyr)\\nlibrary(haven)\\nlibrary(readr)\\nlibrary(foreign)\\nlibrary(tidyr)\\n\\nteds_raw <- read_spss(\\"TEDS dataset for WordBank 110516.sav\\")\\n\\nwrite_csv(teds_raw, \\"teds.csv\\")\\n\\n# make long form\\nteds_long <- teds_raw %>% \\n  gather(variable, value, btwoyear:dsay011) %>%\\n  separate(variable, into=c(\\"age\\", \\"variable\\"), sep = 1) %>%\\n  mutate(age = ifelse(age == \\"b\\", 2, ifelse(age == \\"c\\", 3, 4))) %>%\\n  filter(!(variable %in% c(\\"twoyear\\",\\"threeyr\\",\\"fouryr\\", \\"vocab1\\"))) %>%\\n  mutate(sex = ifelse(sex1 == 1, \\"male\\", \\"female\\"), \\n         ethnicity = ifelse(aethnic == 1, \\"white\\", \\"other\\"),\\n         zygosity = ifelse(zygos == 1, \\"MZ\\",\\"DZ\\"),\\n         ses = ases, \\n         mom_ed = amohqual) %>%\\n  select(-sex1, -aethnic, -zygos, -ases, -amohqual)\\n\\n# split out ages for later merging\\nages <- filter(teds_long, variable == \\"repagem1\\") %>%\\n  rename(age_months = value) %>%\\n  select(id_twin, age, age_months)\\n\\n# fours <- filter(d, variable != \\"repagem1\\", age == 4)\\n\\n# take only the two- and three-year-olds\\n# drop NAs for age, as this is missing data\\ntwos <- filter(teds_long, \\n               age == 2, \\n               variable != \\"repagem1\\") %>%\\n  spread(variable, value) %>%\\n  mutate(id_twin = factor(id_twin)) %>%\\n  left_join(ages %>% mutate(id_twin = factor(id_twin))) %>%\\n  filter(!is.na(age_months)) %>%\\n  select(-twin,-random, -age, -ses)\\n\\nthrees <- filter(teds_long, \\n               age == 3, \\n               variable != \\"repagem1\\") %>%\\n  spread(variable, value) %>%\\n  mutate(id_twin = factor(id_twin)) %>%\\n  left_join(ages %>% mutate(id_twin = factor(id_twin))) %>%\\n  filter(!is.na(age_months)) %>%\\n  select(-twin,-random, -age, -ses)\\n\\n# Either a single excel file (by convention called WugeseWS_Dax, but doesn\'t have to be), with sheets named data, fields, and values;\\n# Or three csv files, with the names foo_data, foo_fields, and foo_values (where foo is WugeseWS_Dax by convention, but doesn\'t have to be).\\n# For the data sheet/file:\\n#   The first row should be column labels (whatever they might be in this dataset).\\n# Each other row should be a single CDI administration.\\n\\nwrite_csv(twos, \\"../../raw_data/English_British_TEDS_2s/EnglishBritishTEDS2s_data.csv\\")\\nwrite_csv(threes, \\"../../raw_data/English_British_TEDS_3s/EnglishBritishTEDS3s_data.csv\\")\\n\\n# The fields sheet/file is a mapping from the dataset\'s column labels to Wordbank\'s fields, and should have the following columns:\\n#   column: column labels from the data sheet/file (modulo case sensitivity) that will be extracted\\n# field: what Wordbank field to map the column label to\\n# MUST include study_id and at least one of data_age and (date_of_birth and date_of_test)\\n# can also optionally have any of birth_order, ethnicity, mom_ed, sex\\n# the rest (everything in group=item) MUST be in this dataset\'s instrument definition file\'s itemID column\\n# this is how the dataset\'s fields get mapped \xe2\x80\x94 it\'s tricky and important to get right\\n# group: whether this field should be associated with the administration, the child, or the data table for the instrument\\n# one of admin, child, or item\\n# type: how to treat the value(s) of this field\\n# study_id, study_momed: value as is\\n# birth_order, data_age: value is made into an integer\\n# date_of_birth, date_of_test: value is made into date\\n# ethnicity, sex, mom_ed, any type in group=item: value is mapped using value mapping\\n\\nwrite_csv(data.frame(column = names(twos), \\n           field = names(twos), \\n           group = \\"item\\",\\n           type = \\"word\\"), \\n          \\"../../raw_data/English_British_TEDS_2s/EnglishBritishTEDS2s_fields.csv\\")\\n\\nwrite_csv(data.frame(column = names(threes), \\n                     field = names(threes), \\n                     group = \\"item\\",\\n                     type = \\"word\\"), \\n          \\"../../raw_data/English_British_TEDS_3s/EnglishBritishTEDS3s_fields.csv\\")\\n\\n# The values sheet/file is a mapping from the dataset\'s value to Wordbank\'s values, split by type, and should have the following columns:\\n#   type: one of the types in the field mapping sheet/file\\n# data_value: the value option in the dataset\\n# value: the short form (e.g. M) of the corresponding value option in Wordbank. The sets of value options in Wordbank are:\\n#   For ethnicity, defined in common/models.py\\n# ((\'A\', \'Asian\'), (\'B\', \'Black\'), (\'H\', \'Hispanic\'), (\'W\', \'White\'), (\'O\', \'Other/Mixed\'))\\n# For sex, defined in common/models.py\\n# ((\'M\', \'Male\'), (\'F\', \'Female\'), (\'O\', \'Other\'))\\n# For mom_ed, defined in common/management/commands/populate_momed.py\\n# {(1, \'None\'), (2, \'Primary\'), (3, \'Some Secondary\'), (4, \'Secondary\'), (5, \'Some College\'), (6, \'College\'), (7, \'Some Graduate\'), (8, \'Graduate\')}\\n# For all types in group=item, defined in e.g. instruments/schemas/Wugese_WS.py and equal to the choices for that type of item as given in the instrument definition file, e.g.\\n# [(u\'understands\', u\'understands\'), (u\'produces\', u\'produces\')]\\n\\n\\n\\n" }'
input_path: /home/gcloud/TransCoder/data/test_dataset/python/python.000.json.gz
language: python
output_path: /home/gcloud/TransCoder/data/test_dataset/python/python.000.with_comments.tok
line: b'{"repo_name":"coronary/RandomEpisode","ref":"refs/heads/master","path":"depends/Lib/encodings/cp1006.py","content":"\\"\\"\\" Python Character Mapping Codec cp1006 generated from \'MAPPINGS/VENDORS/MISC/CP1006.TXT\' with gencodec.py.\\n\\n\\"\\"\\"#\\"\\n\\nimport codecs\\n\\n### Codec APIs\\n\\nclass Codec(codecs.Codec):\\n\\n    def encode(self,input,errors=\'strict\'):\\n        return codecs.charmap_encode(input,errors,encoding_table)\\n\\n    def decode(self,input,errors=\'strict\'):\\n        return codecs.charmap_decode(input,errors,decoding_table)\\n\\nclass IncrementalEncoder(codecs.IncrementalEncoder):\\n    def encode(self, input, final=False):\\n        return codecs.charmap_encode(input,self.errors,encoding_table)[0]\\n\\nclass IncrementalDecoder(codecs.IncrementalDecoder):\\n    def decode(self, input, final=False):\\n        return codecs.charmap_decode(input,self.errors,decoding_table)[0]\\n\\nclass StreamWriter(Codec,codecs.StreamWriter):\\n    pass\\n\\nclass StreamReader(Codec,codecs.StreamReader):\\n    pass\\n\\n### encodings module API\\n\\ndef getregentry():\\n    return codecs.CodecInfo(\\n        name=\'cp1006\',\\n        encode=Codec().encode,\\n        decode=Codec().decode,\\n        incrementalencoder=IncrementalEncoder,\\n        incrementaldecoder=IncrementalDecoder,\\n        streamreader=StreamReader,\\n        streamwriter=StreamWriter,\\n    )\\n\\n\\n### Decoding Table\\n\\ndecoding_table = (\\n    \'\\\\x00\'     #  0x00 -\\u003e NULL\\n    \'\\\\x01\'     #  0x01 -\\u003e START OF HEADING\\n    \'\\\\x02\'     #  0x02 -\\u003e START OF TEXT\\n    \'\\\\x03\'     #  0x03 -\\u003e END OF TEXT\\n    \'\\\\x04\'     #  0x04 -\\u003e END OF TRANSMISSION\\n    \'\\\\x05\'     #  0x05 -\\u003e ENQUIRY\\n    \'\\\\x06\'     #  0x06 -\\u003e ACKNOWLEDGE\\n    \'\\\\x07\'     #  0x07 -\\u003e BELL\\n    \'\\\\x08\'     #  0x08 -\\u003e BACKSPACE\\n    \'\\\\t\'       #  0x09 -\\u003e HORIZONTAL TABULATION\\n    \'\\\\n\'       #  0x0A -\\u003e LINE FEED\\n    \'\\\\x0b\'     #  0x0B -\\u003e VERTICAL TABULATION\\n    \'\\\\x0c\'     #  0x0C -\\u003e FORM FEED\\n    \'\\\\r\'       #  0x0D -\\u003e CARRIAGE RETURN\\n    \'\\\\x0e\'     #  0x0E -\\u003e SHIFT OUT\\n    \'\\\\x0f\'     #  0x0F -\\u003e SHIFT IN\\n    \'\\\\x10\'     #  0x10 -\\u003e DATA LINK ESCAPE\\n    \'\\\\x11\'     #  0x11 -\\u003e DEVICE CONTROL ONE\\n    \'\\\\x12\'     #  0x12 -\\u003e DEVICE CONTROL TWO\\n    \'\\\\x13\'     #  0x13 -\\u003e DEVICE CONTROL THREE\\n    \'\\\\x14\'     #  0x14 -\\u003e DEVICE CONTROL FOUR\\n    \'\\\\x15\'     #  0x15 -\\u003e NEGATIVE ACKNOWLEDGE\\n    \'\\\\x16\'     #  0x16 -\\u003e SYNCHRONOUS IDLE\\n    \'\\\\x17\'     #  0x17 -\\u003e END OF TRANSMISSION BLOCK\\n    \'\\\\x18\'     #  0x18 -\\u003e CANCEL\\n    \'\\\\x19\'     #  0x19 -\\u003e END OF MEDIUM\\n    \'\\\\x1a\'     #  0x1A -\\u003e SUBSTITUTE\\n    \'\\\\x1b\'     #  0x1B -\\u003e ESCAPE\\n    \'\\\\x1c\'     #  0x1C -\\u003e FILE SEPARATOR\\n    \'\\\\x1d\'     #  0x1D -\\u003e GROUP SEPARATOR\\n    \'\\\\x1e\'     #  0x1E -\\u003e RECORD SEPARATOR\\n    \'\\\\x1f\'     #  0x1F -\\u003e UNIT SEPARATOR\\n    \' \'        #  0x20 -\\u003e SPACE\\n    \'!\'        #  0x21 -\\u003e EXCLAMATION MARK\\n    \'\\"\'        #  0x22 -\\u003e QUOTATION MARK\\n    \'#\'        #  0x23 -\\u003e NUMBER SIGN\\n    \'$\'        #  0x24 -\\u003e DOLLAR SIGN\\n    \'%\'        #  0x25 -\\u003e PERCENT SIGN\\n    \'\\u0026\'        #  0x26 -\\u003e AMPERSAND\\n    \\"\'\\"        #  0x27 -\\u003e APOSTROPHE\\n    \'(\'        #  0x28 -\\u003e LEFT PARENTHESIS\\n    \')\'        #  0x29 -\\u003e RIGHT PARENTHESIS\\n    \'*\'        #  0x2A -\\u003e ASTERISK\\n    \'+\'        #  0x2B -\\u003e PLUS SIGN\\n    \',\'        #  0x2C -\\u003e COMMA\\n    \'-\'        #  0x2D -\\u003e HYPHEN-MINUS\\n    \'.\'        #  0x2E -\\u003e FULL STOP\\n    \'/\'        #  0x2F -\\u003e SOLIDUS\\n    \'0\'        #  0x30 -\\u003e DIGIT ZERO\\n    \'1\'        #  0x31 -\\u003e DIGIT ONE\\n    \'2\'        #  0x32 -\\u003e DIGIT TWO\\n    \'3\'        #  0x33 -\\u003e DIGIT THREE\\n    \'4\'        #  0x34 -\\u003e DIGIT FOUR\\n    \'5\'        #  0x35 -\\u003e DIGIT FIVE\\n    \'6\'        #  0x36 -\\u003e DIGIT SIX\\n    \'7\'        #  0x37 -\\u003e DIGIT SEVEN\\n    \'8\'        #  0x38 -\\u003e DIGIT EIGHT\\n    \'9\'        #  0x39 -\\u003e DIGIT NINE\\n    \':\'        #  0x3A -\\u003e COLON\\n    \';\'        #  0x3B -\\u003e SEMICOLON\\n    \'\\u003c\'        #  0x3C -\\u003e LESS-THAN SIGN\\n    \'=\'        #  0x3D -\\u003e EQUALS SIGN\\n    \'\\u003e\'        #  0x3E -\\u003e GREATER-THAN SIGN\\n    \'?\'        #  0x3F -\\u003e QUESTION MARK\\n    \'@\'        #  0x40 -\\u003e COMMERCIAL AT\\n    \'A\'        #  0x41 -\\u003e LATIN CAPITAL LETTER A\\n    \'B\'        #  0x42 -\\u003e LATIN CAPITAL LETTER B\\n    \'C\'        #  0x43 -\\u003e LATIN CAPITAL LETTER C\\n    \'D\'        #  0x44 -\\u003e LATIN CAPITAL LETTER D\\n    \'E\'        #  0x45 -\\u003e LATIN CAPITAL LETTER E\\n    \'F\'        #  0x46 -\\u003e LATIN CAPITAL LETTER F\\n    \'G\'        #  0x47 -\\u003e LATIN CAPITAL LETTER G\\n    \'H\'        #  0x48 -\\u003e LATIN CAPITAL LETTER H\\n    \'I\'        #  0x49 -\\u003e LATIN CAPITAL LETTER I\\n    \'J\'        #  0x4A -\\u003e LATIN CAPITAL LETTER J\\n    \'K\'        #  0x4B -\\u003e LATIN CAPITAL LETTER K\\n    \'L\'        #  0x4C -\\u003e LATIN CAPITAL LETTER L\\n    \'M\'        #  0x4D -\\u003e LATIN CAPITAL LETTER M\\n    \'N\'        #  0x4E -\\u003e LATIN CAPITAL LETTER N\\n    \'O\'        #  0x4F -\\u003e LATIN CAPITAL LETTER O\\n    \'P\'        #  0x50 -\\u003e LATIN CAPITAL LETTER P\\n    \'Q\'        #  0x51 -\\u003e LATIN CAPITAL LETTER Q\\n    \'R\'        #  0x52 -\\u003e LATIN CAPITAL LETTER R\\n    \'S\'        #  0x53 -\\u003e LATIN CAPITAL LETTER S\\n    \'T\'        #  0x54 -\\u003e LATIN CAPITAL LETTER T\\n    \'U\'        #  0x55 -\\u003e LATIN CAPITAL LETTER U\\n    \'V\'        #  0x56 -\\u003e LATIN CAPITAL LETTER V\\n    \'W\'        #  0x57 -\\u003e LATIN CAPITAL LETTER W\\n    \'X\'        #  0x58 -\\u003e LATIN CAPITAL LETTER X\\n    \'Y\'        #  0x59 -\\u003e LATIN CAPITAL LETTER Y\\n    \'Z\'        #  0x5A -\\u003e LATIN CAPITAL LETTER Z\\n    \'[\'        #  0x5B -\\u003e LEFT SQUARE BRACKET\\n    \'\\\\\\\\\'       #  0x5C -\\u003e REVERSE SOLIDUS\\n    \']\'        #  0x5D -\\u003e RIGHT SQUARE BRACKET\\n    \'^\'        #  0x5E -\\u003e CIRCUMFLEX ACCENT\\n    \'_\'        #  0x5F -\\u003e LOW LINE\\n    \'`\'        #  0x60 -\\u003e GRAVE ACCENT\\n    \'a\'        #  0x61 -\\u003e LATIN SMALL LETTER A\\n    \'b\'        #  0x62 -\\u003e LATIN SMALL LETTER B\\n    \'c\'        #  0x63 -\\u003e LATIN SMALL LETTER C\\n    \'d\'        #  0x64 -\\u003e LATIN SMALL LETTER D\\n    \'e\'        #  0x65 -\\u003e LATIN SMALL LETTER E\\n    \'f\'        #  0x66 -\\u003e LATIN SMALL LETTER F\\n    \'g\'        #  0x67 -\\u003e LATIN SMALL LETTER G\\n    \'h\'        #  0x68 -\\u003e LATIN SMALL LETTER H\\n    \'i\'        #  0x69 -\\u003e LATIN SMALL LETTER I\\n    \'j\'        #  0x6A -\\u003e LATIN SMALL LETTER J\\n    \'k\'        #  0x6B -\\u003e LATIN SMALL LETTER K\\n    \'l\'        #  0x6C -\\u003e LATIN SMALL LETTER L\\n    \'m\'        #  0x6D -\\u003e LATIN SMALL LETTER M\\n    \'n\'        #  0x6E -\\u003e LATIN SMALL LETTER N\\n    \'o\'        #  0x6F -\\u003e LATIN SMALL LETTER O\\n    \'p\'        #  0x70 -\\u003e LATIN SMALL LETTER P\\n    \'q\'        #  0x71 -\\u003e LATIN SMALL LETTER Q\\n    \'r\'        #  0x72 -\\u003e LATIN SMALL LETTER R\\n    \'s\'        #  0x73 -\\u003e LATIN SMALL LETTER S\\n    \'t\'        #  0x74 -\\u003e LATIN SMALL LETTER T\\n    \'u\'        #  0x75 -\\u003e LATIN SMALL LETTER U\\n    \'v\'        #  0x76 -\\u003e LATIN SMALL LETTER V\\n    \'w\'        #  0x77 -\\u003e LATIN SMALL LETTER W\\n    \'x\'        #  0x78 -\\u003e LATIN SMALL LETTER X\\n    \'y\'        #  0x79 -\\u003e LATIN SMALL LETTER Y\\n    \'z\'        #  0x7A -\\u003e LATIN SMALL LETTER Z\\n    \'{\'        #  0x7B -\\u003e LEFT CURLY BRACKET\\n    \'|\'        #  0x7C -\\u003e VERTICAL LINE\\n    \'}\'        #  0x7D -\\u003e RIGHT CURLY BRACKET\\n    \'~\'        #  0x7E -\\u003e TILDE\\n    \'\\\\x7f\'     #  0x7F -\\u003e DELETE\\n    \'\\\\x80\'     #  0x80 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x81\'     #  0x81 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x82\'     #  0x82 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x83\'     #  0x83 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x84\'     #  0x84 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x85\'     #  0x85 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x86\'     #  0x86 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x87\'     #  0x87 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x88\'     #  0x88 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x89\'     #  0x89 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x8a\'     #  0x8A -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x8b\'     #  0x8B -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x8c\'     #  0x8C -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x8d\'     #  0x8D -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x8e\'     #  0x8E -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x8f\'     #  0x8F -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x90\'     #  0x90 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x91\'     #  0x91 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x92\'     #  0x92 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x93\'     #  0x93 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x94\'     #  0x94 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x95\'     #  0x95 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x96\'     #  0x96 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x97\'     #  0x97 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x98\'     #  0x98 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x99\'     #  0x99 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x9a\'     #  0x9A -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x9b\'     #  0x9B -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x9c\'     #  0x9C -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x9d\'     #  0x9D -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x9e\'     #  0x9E -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x9f\'     #  0x9F -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\xa0\'     #  0xA0 -\\u003e NO-BREAK SPACE\\n    \'\\\\u06f0\'   #  0xA1 -\\u003e EXTENDED ARABIC-INDIC DIGIT ZERO\\n    \'\\\\u06f1\'   #  0xA2 -\\u003e EXTENDED ARABIC-INDIC DIGIT ONE\\n    \'\\\\u06f2\'   #  0xA3 -\\u003e EXTENDED ARABIC-INDIC DIGIT TWO\\n    \'\\\\u06f3\'   #  0xA4 -\\u003e EXTENDED ARABIC-INDIC DIGIT THREE\\n    \'\\\\u06f4\'   #  0xA5 -\\u003e EXTENDED ARABIC-INDIC DIGIT FOUR\\n    \'\\\\u06f5\'   #  0xA6 -\\u003e EXTENDED ARABIC-INDIC DIGIT FIVE\\n    \'\\\\u06f6\'   #  0xA7 -\\u003e EXTENDED ARABIC-INDIC DIGIT SIX\\n    \'\\\\u06f7\'   #  0xA8 -\\u003e EXTENDED ARABIC-INDIC DIGIT SEVEN\\n    \'\\\\u06f8\'   #  0xA9 -\\u003e EXTENDED ARABIC-INDIC DIGIT EIGHT\\n    \'\\\\u06f9\'   #  0xAA -\\u003e EXTENDED ARABIC-INDIC DIGIT NINE\\n    \'\\\\u060c\'   #  0xAB -\\u003e ARABIC COMMA\\n    \'\\\\u061b\'   #  0xAC -\\u003e ARABIC SEMICOLON\\n    \'\\\\xad\'     #  0xAD -\\u003e SOFT HYPHEN\\n    \'\\\\u061f\'   #  0xAE -\\u003e ARABIC QUESTION MARK\\n    \'\\\\ufe81\'   #  0xAF -\\u003e ARABIC LETTER ALEF WITH MADDA ABOVE ISOLATED FORM\\n    \'\\\\ufe8d\'   #  0xB0 -\\u003e ARABIC LETTER ALEF ISOLATED FORM\\n    \'\\\\ufe8e\'   #  0xB1 -\\u003e ARABIC LETTER ALEF FINAL FORM\\n    \'\\\\ufe8e\'   #  0xB2 -\\u003e ARABIC LETTER ALEF FINAL FORM\\n    \'\\\\ufe8f\'   #  0xB3 -\\u003e ARABIC LETTER BEH ISOLATED FORM\\n    \'\\\\ufe91\'   #  0xB4 -\\u003e ARABIC LETTER BEH INITIAL FORM\\n    \'\\\\ufb56\'   #  0xB5 -\\u003e ARABIC LETTER PEH ISOLATED FORM\\n    \'\\\\ufb58\'   #  0xB6 -\\u003e ARABIC LETTER PEH INITIAL FORM\\n    \'\\\\ufe93\'   #  0xB7 -\\u003e ARABIC LETTER TEH MARBUTA ISOLATED FORM\\n    \'\\\\ufe95\'   #  0xB8 -\\u003e ARABIC LETTER TEH ISOLATED FORM\\n    \'\\\\ufe97\'   #  0xB9 -\\u003e ARABIC LETTER TEH INITIAL FORM\\n    \'\\\\ufb66\'   #  0xBA -\\u003e ARABIC LETTER TTEH ISOLATED FORM\\n    \'\\\\ufb68\'   #  0xBB -\\u003e ARABIC LETTER TTEH INITIAL FORM\\n    \'\\\\ufe99\'   #  0xBC -\\u003e ARABIC LETTER THEH ISOLATED FORM\\n    \'\\\\ufe9b\'   #  0xBD -\\u003e ARABIC LETTER THEH INITIAL FORM\\n    \'\\\\ufe9d\'   #  0xBE -\\u003e ARABIC LETTER JEEM ISOLATED FORM\\n    \'\\\\ufe9f\'   #  0xBF -\\u003e ARABIC LETTER JEEM INITIAL FORM\\n    \'\\\\ufb7a\'   #  0xC0 -\\u003e ARABIC LETTER TCHEH ISOLATED FORM\\n    \'\\\\ufb7c\'   #  0xC1 -\\u003e ARABIC LETTER TCHEH INITIAL FORM\\n    \'\\\\ufea1\'   #  0xC2 -\\u003e ARABIC LETTER HAH ISOLATED FORM\\n    \'\\\\ufea3\'   #  0xC3 -\\u003e ARABIC LETTER HAH INITIAL FORM\\n    \'\\\\ufea5\'   #  0xC4 -\\u003e ARABIC LETTER KHAH ISOLATED FORM\\n    \'\\\\ufea7\'   #  0xC5 -\\u003e ARABIC LETTER KHAH INITIAL FORM\\n    \'\\\\ufea9\'   #  0xC6 -\\u003e ARABIC LETTER DAL ISOLATED FORM\\n    \'\\\\ufb84\'   #  0xC7 -\\u003e ARABIC LETTER DAHAL ISOLATED FORMN\\n    \'\\\\ufeab\'   #  0xC8 -\\u003e ARABIC LETTER THAL ISOLATED FORM\\n    \'\\\\ufead\'   #  0xC9 -\\u003e ARABIC LETTER REH ISOLATED FORM\\n    \'\\\\ufb8c\'   #  0xCA -\\u003e ARABIC LETTER RREH ISOLATED FORM\\n    \'\\\\ufeaf\'   #  0xCB -\\u003e ARABIC LETTER ZAIN ISOLATED FORM\\n    \'\\\\ufb8a\'   #  0xCC -\\u003e ARABIC LETTER JEH ISOLATED FORM\\n    \'\\\\ufeb1\'   #  0xCD -\\u003e ARABIC LETTER SEEN ISOLATED FORM\\n    \'\\\\ufeb3\'   #  0xCE -\\u003e ARABIC LETTER SEEN INITIAL FORM\\n    \'\\\\ufeb5\'   #  0xCF -\\u003e ARABIC LETTER SHEEN ISOLATED FORM\\n    \'\\\\ufeb7\'   #  0xD0 -\\u003e ARABIC LETTER SHEEN INITIAL FORM\\n    \'\\\\ufeb9\'   #  0xD1 -\\u003e ARABIC LETTER SAD ISOLATED FORM\\n    \'\\\\ufebb\'   #  0xD2 -\\u003e ARABIC LETTER SAD INITIAL FORM\\n    \'\\\\ufebd\'   #  0xD3 -\\u003e ARABIC LETTER DAD ISOLATED FORM\\n    \'\\\\ufebf\'   #  0xD4 -\\u003e ARABIC LETTER DAD INITIAL FORM\\n    \'\\\\ufec1\'   #  0xD5 -\\u003e ARABIC LETTER TAH ISOLATED FORM\\n    \'\\\\ufec5\'   #  0xD6 -\\u003e ARABIC LETTER ZAH ISOLATED FORM\\n    \'\\\\ufec9\'   #  0xD7 -\\u003e ARABIC LETTER AIN ISOLATED FORM\\n    \'\\\\ufeca\'   #  0xD8 -\\u003e ARABIC LETTER AIN FINAL FORM\\n    \'\\\\ufecb\'   #  0xD9 -\\u003e ARABIC LETTER AIN INITIAL FORM\\n    \'\\\\ufecc\'   #  0xDA -\\u003e ARABIC LETTER AIN MEDIAL FORM\\n    \'\\\\ufecd\'   #  0xDB -\\u003e ARABIC LETTER GHAIN ISOLATED FORM\\n    \'\\\\ufece\'   #  0xDC -\\u003e ARABIC LETTER GHAIN FINAL FORM\\n    \'\\\\ufecf\'   #  0xDD -\\u003e ARABIC LETTER GHAIN INITIAL FORM\\n    \'\\\\ufed0\'   #  0xDE -\\u003e ARABIC LETTER GHAIN MEDIAL FORM\\n    \'\\\\ufed1\'   #  0xDF -\\u003e ARABIC LETTER FEH ISOLATED FORM\\n    \'\\\\ufed3\'   #  0xE0 -\\u003e ARABIC LETTER FEH INITIAL FORM\\n    \'\\\\ufed5\'   #  0xE1 -\\u003e ARABIC LETTER QAF ISOLATED FORM\\n    \'\\\\ufed7\'   #  0xE2 -\\u003e ARABIC LETTER QAF INITIAL FORM\\n    \'\\\\ufed9\'   #  0xE3 -\\u003e ARABIC LETTER KAF ISOLATED FORM\\n    \'\\\\ufedb\'   #  0xE4 -\\u003e ARABIC LETTER KAF INITIAL FORM\\n    \'\\\\ufb92\'   #  0xE5 -\\u003e ARABIC LETTER GAF ISOLATED FORM\\n    \'\\\\ufb94\'   #  0xE6 -\\u003e ARABIC LETTER GAF INITIAL FORM\\n    \'\\\\ufedd\'   #  0xE7 -\\u003e ARABIC LETTER LAM ISOLATED FORM\\n    \'\\\\ufedf\'   #  0xE8 -\\u003e ARABIC LETTER LAM INITIAL FORM\\n    \'\\\\ufee0\'   #  0xE9 -\\u003e ARABIC LETTER LAM MEDIAL FORM\\n    \'\\\\ufee1\'   #  0xEA -\\u003e ARABIC LETTER MEEM ISOLATED FORM\\n    \'\\\\ufee3\'   #  0xEB -\\u003e ARABIC LETTER MEEM INITIAL FORM\\n    \'\\\\ufb9e\'   #  0xEC -\\u003e ARABIC LETTER NOON GHUNNA ISOLATED FORM\\n    \'\\\\ufee5\'   #  0xED -\\u003e ARABIC LETTER NOON ISOLATED FORM\\n    \'\\\\ufee7\'   #  0xEE -\\u003e ARABIC LETTER NOON INITIAL FORM\\n    \'\\\\ufe85\'   #  0xEF -\\u003e ARABIC LETTER WAW WITH HAMZA ABOVE ISOLATED FORM\\n    \'\\\\ufeed\'   #  0xF0 -\\u003e ARABIC LETTER WAW ISOLATED FORM\\n    \'\\\\ufba6\'   #  0xF1 -\\u003e ARABIC LETTER HEH GOAL ISOLATED FORM\\n    \'\\\\ufba8\'   #  0xF2 -\\u003e ARABIC LETTER HEH GOAL INITIAL FORM\\n    \'\\\\ufba9\'   #  0xF3 -\\u003e ARABIC LETTER HEH GOAL MEDIAL FORM\\n    \'\\\\ufbaa\'   #  0xF4 -\\u003e ARABIC LETTER HEH DOACHASHMEE ISOLATED FORM\\n    \'\\\\ufe80\'   #  0xF5 -\\u003e ARABIC LETTER HAMZA ISOLATED FORM\\n    \'\\\\ufe89\'   #  0xF6 -\\u003e ARABIC LETTER YEH WITH HAMZA ABOVE ISOLATED FORM\\n    \'\\\\ufe8a\'   #  0xF7 -\\u003e ARABIC LETTER YEH WITH HAMZA ABOVE FINAL FORM\\n    \'\\\\ufe8b\'   #  0xF8 -\\u003e ARABIC LETTER YEH WITH HAMZA ABOVE INITIAL FORM\\n    \'\\\\ufef1\'   #  0xF9 -\\u003e ARABIC LETTER YEH ISOLATED FORM\\n    \'\\\\ufef2\'   #  0xFA -\\u003e ARABIC LETTER YEH FINAL FORM\\n    \'\\\\ufef3\'   #  0xFB -\\u003e ARABIC LETTER YEH INITIAL FORM\\n    \'\\\\ufbb0\'   #  0xFC -\\u003e ARABIC LETTER YEH BARREE WITH HAMZA ABOVE ISOLATED FORM\\n    \'\\\\ufbae\'   #  0xFD -\\u003e ARABIC LETTER YEH BARREE ISOLATED FORM\\n    \'\\\\ufe7c\'   #  0xFE -\\u003e ARABIC SHADDA ISOLATED FORM\\n    \'\\\\ufe7d\'   #  0xFF -\\u003e ARABIC SHADDA MEDIAL FORM\\n)\\n\\n### Encoding table\\nencoding_table=codecs.charmap_build(decoding_table)\\n"}\n'
line: b'{"repo_name":"fernandog/Medusa","ref":"refs/heads/optimized","path":"ext/click/termui.py","content":"import os\\nimport sys\\nimport struct\\n\\nfrom ._compat import raw_input, text_type, string_types, \\\\\\n     isatty, strip_ansi, get_winterm_size, DEFAULT_COLUMNS, WIN\\nfrom .utils import echo\\nfrom .exceptions import Abort, UsageError\\nfrom .types import convert_type\\nfrom .globals import resolve_color_default\\n\\n\\n# The prompt functions to use.  The doc tools currently override these\\n# functions to customize how they work.\\nvisible_prompt_func = raw_input\\n\\n_ansi_colors = (\'black\', \'red\', \'green\', \'yellow\', \'blue\', \'magenta\',\\n                \'cyan\', \'white\', \'reset\')\\n_ansi_reset_all = \'\\\\033[0m\'\\n\\n\\ndef hidden_prompt_func(prompt):\\n    import getpass\\n    return getpass.getpass(prompt)\\n\\n\\ndef _build_prompt(text, suffix, show_default=False, default=None):\\n    prompt = text\\n    if default is not None and show_default:\\n        prompt = \'%s [%s]\' % (prompt, default)\\n    return prompt + suffix\\n\\n\\ndef prompt(text, default=None, hide_input=False,\\n           confirmation_prompt=False, type=None,\\n           value_proc=None, prompt_suffix=\': \',\\n           show_default=True, err=False):\\n    \\"\\"\\"Prompts a user for input.  This is a convenience function that can\\n    be used to prompt a user for input later.\\n\\n    If the user aborts the input by sending a interrupt signal, this\\n    function will catch it and raise a :exc:`Abort` exception.\\n\\n    .. versionadded:: 6.0\\n       Added unicode support for cmd.exe on Windows.\\n\\n    .. versionadded:: 4.0\\n       Added the `err` parameter.\\n\\n    :param text: the text to show for the prompt.\\n    :param default: the default value to use if no input happens.  If this\\n                    is not given it will prompt until it\'s aborted.\\n    :param hide_input: if this is set to true then the input value will\\n                       be hidden.\\n    :param confirmation_prompt: asks for confirmation for the value.\\n    :param type: the type to use to check the value against.\\n    :param value_proc: if this parameter is provided it\'s a function that\\n                       is invoked instead of the type conversion to\\n                       convert a value.\\n    :param prompt_suffix: a suffix that should be added to the prompt.\\n    :param show_default: shows or hides the default value in the prompt.\\n    :param err: if set to true the file defaults to ``stderr`` instead of\\n                ``stdout``, the same as with echo.\\n    \\"\\"\\"\\n    result = None\\n\\n    def prompt_func(text):\\n        f = hide_input and hidden_prompt_func or visible_prompt_func\\n        try:\\n            # Write the prompt separately so that we get nice\\n            # coloring through colorama on Windows\\n            echo(text, nl=False, err=err)\\n            return f(\'\')\\n        except (KeyboardInterrupt, EOFError):\\n            # getpass doesn\'t print a newline if the user aborts input with ^C.\\n            # Allegedly this behavior is inherited from getpass(3).\\n            # A doc bug has been filed at https://bugs.python.org/issue24711\\n            if hide_input:\\n                echo(None, err=err)\\n            raise Abort()\\n\\n    if value_proc is None:\\n        value_proc = convert_type(type, default)\\n\\n    prompt = _build_prompt(text, prompt_suffix, show_default, default)\\n\\n    while 1:\\n        while 1:\\n            value = prompt_func(prompt)\\n            if value:\\n                break\\n            # If a default is set and used, then the confirmation\\n            # prompt is always skipped because that\'s the only thing\\n            # that really makes sense.\\n            elif default is not None:\\n                return default\\n        try:\\n            result = value_proc(value)\\n        except UsageError as e:\\n            echo(\'Error: %s\' % e.message, err=err)\\n            continue\\n        if not confirmation_prompt:\\n            return result\\n        while 1:\\n            value2 = prompt_func(\'Repeat for confirmation: \')\\n            if value2:\\n                break\\n        if value == value2:\\n            return result\\n        echo(\'Error: the two entered values do not match\', err=err)\\n\\n\\ndef confirm(text, default=False, abort=False, prompt_suffix=\': \',\\n            show_default=True, err=False):\\n    \\"\\"\\"Prompts for confirmation (yes/no question).\\n\\n    If the user aborts the input by sending a interrupt signal this\\n    function will catch it and raise a :exc:`Abort` exception.\\n\\n    .. versionadded:: 4.0\\n       Added the `err` parameter.\\n\\n    :param text: the question to ask.\\n    :param default: the default for the prompt.\\n    :param abort: if this is set to `True` a negative answer aborts the\\n                  exception by raising :exc:`Abort`.\\n    :param prompt_suffix: a suffix that should be added to the prompt.\\n    :param show_default: shows or hides the default value in the prompt.\\n    :param err: if set to true the file defaults to ``stderr`` instead of\\n                ``stdout``, the same as with echo.\\n    \\"\\"\\"\\n    prompt = _build_prompt(text, prompt_suffix, show_default,\\n                           default and \'Y/n\' or \'y/N\')\\n    while 1:\\n        try:\\n            # Write the prompt separately so that we get nice\\n            # coloring through colorama on Windows\\n            echo(prompt, nl=False, err=err)\\n            value = visible_prompt_func(\'\').lower().strip()\\n        except (KeyboardInterrupt, EOFError):\\n            raise Abort()\\n        if value in (\'y\', \'yes\'):\\n            rv = True\\n        elif value in (\'n\', \'no\'):\\n            rv = False\\n        elif value == \'\':\\n            rv = default\\n        else:\\n            echo(\'Error: invalid input\', err=err)\\n            continue\\n        break\\n    if abort and not rv:\\n        raise Abort()\\n    return rv\\n\\n\\ndef get_terminal_size():\\n    \\"\\"\\"Returns the current size of the terminal as tuple in the form\\n    ``(width, height)`` in columns and rows.\\n    \\"\\"\\"\\n    # If shutil has get_terminal_size() (Python 3.3 and later) use that\\n    if sys.version_info \\u003e= (3, 3):\\n        import shutil\\n        shutil_get_terminal_size = getattr(shutil, \'get_terminal_size\', None)\\n        if shutil_get_terminal_size:\\n            sz = shutil_get_terminal_size()\\n            return sz.columns, sz.lines\\n\\n    if get_winterm_size is not None:\\n        return get_winterm_size()\\n\\n    def ioctl_gwinsz(fd):\\n        try:\\n            import fcntl\\n            import termios\\n            cr = struct.unpack(\\n                \'hh\', fcntl.ioctl(fd, termios.TIOCGWINSZ, \'1234\'))\\n        except Exception:\\n            return\\n        return cr\\n\\n    cr = ioctl_gwinsz(0) or ioctl_gwinsz(1) or ioctl_gwinsz(2)\\n    if not cr:\\n        try:\\n            fd = os.open(os.ctermid(), os.O_RDONLY)\\n            try:\\n                cr = ioctl_gwinsz(fd)\\n            finally:\\n                os.close(fd)\\n        except Exception:\\n            pass\\n    if not cr or not cr[0] or not cr[1]:\\n        cr = (os.environ.get(\'LINES\', 25),\\n              os.environ.get(\'COLUMNS\', DEFAULT_COLUMNS))\\n    return int(cr[1]), int(cr[0])\\n\\n\\ndef echo_via_pager(text, color=None):\\n    \\"\\"\\"This function takes a text and shows it via an environment specific\\n    pager on stdout.\\n\\n    .. versionchanged:: 3.0\\n       Added the `color` flag.\\n\\n    :param text: the text to page.\\n    :param color: controls if the pager supports ANSI colors or not.  The\\n                  default is autodetection.\\n    \\"\\"\\"\\n    color = resolve_color_default(color)\\n    if not isinstance(text, string_types):\\n        text = text_type(text)\\n    from ._termui_impl import pager\\n    return pager(text + \'\\\\n\', color)\\n\\n\\ndef progressbar(iterable=None, length=None, label=None, show_eta=True,\\n                show_percent=None, show_pos=False,\\n                item_show_func=None, fill_char=\'#\', empty_char=\'-\',\\n                bar_template=\'%(label)s  [%(bar)s]  %(info)s\',\\n                info_sep=\'  \', width=36, file=None, color=None):\\n    \\"\\"\\"This function creates an iterable context manager that can be used\\n    to iterate over something while showing a progress bar.  It will\\n    either iterate over the `iterable` or `length` items (that are counted\\n    up).  While iteration happens, this function will print a rendered\\n    progress bar to the given `file` (defaults to stdout) and will attempt\\n    to calculate remaining time and more.  By default, this progress bar\\n    will not be rendered if the file is not a terminal.\\n\\n    The context manager creates the progress bar.  When the context\\n    manager is entered the progress bar is already displayed.  With every\\n    iteration over the progress bar, the iterable passed to the bar is\\n    advanced and the bar is updated.  When the context manager exits,\\n    a newline is printed and the progress bar is finalized on screen.\\n\\n    No printing must happen or the progress bar will be unintentionally\\n    destroyed.\\n\\n    Example usage::\\n\\n        with progressbar(items) as bar:\\n            for item in bar:\\n                do_something_with(item)\\n\\n    Alternatively, if no iterable is specified, one can manually update the\\n    progress bar through the `update()` method instead of directly\\n    iterating over the progress bar.  The update method accepts the number\\n    of steps to increment the bar with::\\n\\n        with progressbar(length=chunks.total_bytes) as bar:\\n            for chunk in chunks:\\n                process_chunk(chunk)\\n                bar.update(chunks.bytes)\\n\\n    .. versionadded:: 2.0\\n\\n    .. versionadded:: 4.0\\n       Added the `color` parameter.  Added a `update` method to the\\n       progressbar object.\\n\\n    :param iterable: an iterable to iterate over.  If not provided the length\\n                     is required.\\n    :param length: the number of items to iterate over.  By default the\\n                   progressbar will attempt to ask the iterator about its\\n                   length, which might or might not work.  If an iterable is\\n                   also provided this parameter can be used to override the\\n                   length.  If an iterable is not provided the progress bar\\n                   will iterate over a range of that length.\\n    :param label: the label to show next to the progress bar.\\n    :param show_eta: enables or disables the estimated time display.  This is\\n                     automatically disabled if the length cannot be\\n                     determined.\\n    :param show_percent: enables or disables the percentage display.  The\\n                         default is `True` if the iterable has a length or\\n                         `False` if not.\\n    :param show_pos: enables or disables the absolute position display.  The\\n                     default is `False`.\\n    :param item_show_func: a function called with the current item which\\n                           can return a string to show the current item\\n                           next to the progress bar.  Note that the current\\n                           item can be `None`!\\n    :param fill_char: the character to use to show the filled part of the\\n                      progress bar.\\n    :param empty_char: the character to use to show the non-filled part of\\n                       the progress bar.\\n    :param bar_template: the format string to use as template for the bar.\\n                         The parameters in it are ``label`` for the label,\\n                         ``bar`` for the progress bar and ``info`` for the\\n                         info section.\\n    :param info_sep: the separator between multiple info items (eta etc.)\\n    :param width: the width of the progress bar in characters, 0 means full\\n                  terminal width\\n    :param file: the file to write to.  If this is not a terminal then\\n                 only the label is printed.\\n    :param color: controls if the terminal supports ANSI colors or not.  The\\n                  default is autodetection.  This is only needed if ANSI\\n                  codes are included anywhere in the progress bar output\\n                  which is not the case by default.\\n    \\"\\"\\"\\n    from ._termui_impl import ProgressBar\\n    color = resolve_color_default(color)\\n    return ProgressBar(iterable=iterable, length=length, show_eta=show_eta,\\n                       show_percent=show_percent, show_pos=show_pos,\\n                       item_show_func=item_show_func, fill_char=fill_char,\\n                       empty_char=empty_char, bar_template=bar_template,\\n                       info_sep=info_sep, file=file, label=label,\\n                       width=width, color=color)\\n\\n\\ndef clear():\\n    \\"\\"\\"Clears the terminal screen.  This will have the effect of clearing\\n    the whole visible space of the terminal and moving the cursor to the\\n    top left.  This does not do anything if not connected to a terminal.\\n\\n    .. versionadded:: 2.0\\n    \\"\\"\\"\\n    if not isatty(sys.stdout):\\n        return\\n    # If we\'re on Windows and we don\'t have colorama available, then we\\n    # clear the screen by shelling out.  Otherwise we can use an escape\\n    # sequence.\\n    if WIN:\\n        os.system(\'cls\')\\n    else:\\n        sys.stdout.write(\'\\\\033[2J\\\\033[1;1H\')\\n\\n\\ndef style(text, fg=None, bg=None, bold=None, dim=None, underline=None,\\n          blink=None, reverse=None, reset=True):\\n    \\"\\"\\"Styles a text with ANSI styles and returns the new string.  By\\n    default the styling is self contained which means that at the end\\n    of the string a reset code is issued.  This can be prevented by\\n    passing ``reset=False``.\\n\\n    Examples::\\n\\n        click.echo(click.style(\'Hello World!\', fg=\'green\'))\\n        click.echo(click.style(\'ATTENTION!\', blink=True))\\n        click.echo(click.style(\'Some things\', reverse=True, fg=\'cyan\'))\\n\\n    Supported color names:\\n\\n    * ``black`` (might be a gray)\\n    * ``red``\\n    * ``green``\\n    * ``yellow`` (might be an orange)\\n    * ``blue``\\n    * ``magenta``\\n    * ``cyan``\\n    * ``white`` (might be light gray)\\n    * ``reset`` (reset the color code only)\\n\\n    .. versionadded:: 2.0\\n\\n    :param text: the string to style with ansi codes.\\n    :param fg: if provided this will become the foreground color.\\n    :param bg: if provided this will become the background color.\\n    :param bold: if provided this will enable or disable bold mode.\\n    :param dim: if provided this will enable or disable dim mode.  This is\\n                badly supported.\\n    :param underline: if provided this will enable or disable underline.\\n    :param blink: if provided this will enable or disable blinking.\\n    :param reverse: if provided this will enable or disable inverse\\n                    rendering (foreground becomes background and the\\n                    other way round).\\n    :param reset: by default a reset-all code is added at the end of the\\n                  string which means that styles do not carry over.  This\\n                  can be disabled to compose styles.\\n    \\"\\"\\"\\n    bits = []\\n    if fg:\\n        try:\\n            bits.append(\'\\\\033[%dm\' % (_ansi_colors.index(fg) + 30))\\n        except ValueError:\\n            raise TypeError(\'Unknown color %r\' % fg)\\n    if bg:\\n        try:\\n            bits.append(\'\\\\033[%dm\' % (_ansi_colors.index(bg) + 40))\\n        except ValueError:\\n            raise TypeError(\'Unknown color %r\' % bg)\\n    if bold is not None:\\n        bits.append(\'\\\\033[%dm\' % (1 if bold else 22))\\n    if dim is not None:\\n        bits.append(\'\\\\033[%dm\' % (2 if dim else 22))\\n    if underline is not None:\\n        bits.append(\'\\\\033[%dm\' % (4 if underline else 24))\\n    if blink is not None:\\n        bits.append(\'\\\\033[%dm\' % (5 if blink else 25))\\n    if reverse is not None:\\n        bits.append(\'\\\\033[%dm\' % (7 if reverse else 27))\\n    bits.append(text)\\n    if reset:\\n        bits.append(_ansi_reset_all)\\n    return \'\'.join(bits)\\n\\n\\ndef unstyle(text):\\n    \\"\\"\\"Removes ANSI styling information from a string.  Usually it\'s not\\n    necessary to use this function as Click\'s echo function will\\n    automatically remove styling if necessary.\\n\\n    .. versionadded:: 2.0\\n\\n    :param text: the text to remove style information from.\\n    \\"\\"\\"\\n    return strip_ansi(text)\\n\\n\\ndef secho(text, file=None, nl=True, err=False, color=None, **styles):\\n    \\"\\"\\"This function combines :func:`echo` and :func:`style` into one\\n    call.  As such the following two calls are the same::\\n\\n        click.secho(\'Hello World!\', fg=\'green\')\\n        click.echo(click.style(\'Hello World!\', fg=\'green\'))\\n\\n    All keyword arguments are forwarded to the underlying functions\\n    depending on which one they go with.\\n\\n    .. versionadded:: 2.0\\n    \\"\\"\\"\\n    return echo(style(text, **styles), file=file, nl=nl, err=err, color=color)\\n\\n\\ndef edit(text=None, editor=None, env=None, require_save=True,\\n         extension=\'.txt\', filename=None):\\n    r\\"\\"\\"Edits the given text in the defined editor.  If an editor is given\\n    (should be the full path to the executable but the regular operating\\n    system search path is used for finding the executable) it overrides\\n    the detected editor.  Optionally, some environment variables can be\\n    used.  If the editor is closed without changes, `None` is returned.  In\\n    case a file is edited directly the return value is always `None` and\\n    `require_save` and `extension` are ignored.\\n\\n    If the editor cannot be opened a :exc:`UsageError` is raised.\\n\\n    Note for Windows: to simplify cross-platform usage, the newlines are\\n    automatically converted from POSIX to Windows and vice versa.  As such,\\n    the message here will have ``\\\\n`` as newline markers.\\n\\n    :param text: the text to edit.\\n    :param editor: optionally the editor to use.  Defaults to automatic\\n                   detection.\\n    :param env: environment variables to forward to the editor.\\n    :param require_save: if this is true, then not saving in the editor\\n                         will make the return value become `None`.\\n    :param extension: the extension to tell the editor about.  This defaults\\n                      to `.txt` but changing this might change syntax\\n                      highlighting.\\n    :param filename: if provided it will edit this file instead of the\\n                     provided text contents.  It will not use a temporary\\n                     file as an indirection in that case.\\n    \\"\\"\\"\\n    from ._termui_impl import Editor\\n    editor = Editor(editor=editor, env=env, require_save=require_save,\\n                    extension=extension)\\n    if filename is None:\\n        return editor.edit(text)\\n    editor.edit_file(filename)\\n\\n\\ndef launch(url, wait=False, locate=False):\\n    \\"\\"\\"This function launches the given URL (or filename) in the default\\n    viewer application for this file type.  If this is an executable, it\\n    might launch the executable in a new session.  The return value is\\n    the exit code of the launched application.  Usually, ``0`` indicates\\n    success.\\n\\n    Examples::\\n\\n        click.launch(\'http://click.pocoo.org/\')\\n        click.launch(\'/my/downloaded/file\', locate=True)\\n\\n    .. versionadded:: 2.0\\n\\n    :param url: URL or filename of the thing to launch.\\n    :param wait: waits for the program to stop.\\n    :param locate: if this is set to `True` then instead of launching the\\n                   application associated with the URL it will attempt to\\n                   launch a file manager with the file located.  This\\n                   might have weird effects if the URL does not point to\\n                   the filesystem.\\n    \\"\\"\\"\\n    from ._termui_impl import open_url\\n    return open_url(url, wait=wait, locate=locate)\\n\\n\\n# If this is provided, getchar() calls into this instead.  This is used\\n# for unittesting purposes.\\n_getchar = None\\n\\n\\ndef getchar(echo=False):\\n    \\"\\"\\"Fetches a single character from the terminal and returns it.  This\\n    will always return a unicode character and under certain rare\\n    circumstances this might return more than one character.  The\\n    situations which more than one character is returned is when for\\n    whatever reason multiple characters end up in the terminal buffer or\\n    standard input was not actually a terminal.\\n\\n    Note that this will always read from the terminal, even if something\\n    is piped into the standard input.\\n\\n    .. versionadded:: 2.0\\n\\n    :param echo: if set to `True`, the character read will also show up on\\n                 the terminal.  The default is to not show it.\\n    \\"\\"\\"\\n    f = _getchar\\n    if f is None:\\n        from ._termui_impl import getchar as f\\n    return f(echo)\\n\\n\\ndef pause(info=\'Press any key to continue ...\', err=False):\\n    \\"\\"\\"This command stops execution and waits for the user to press any\\n    key to continue.  This is similar to the Windows batch \\"pause\\"\\n    command.  If the program is not run through a terminal, this command\\n    will instead do nothing.\\n\\n    .. versionadded:: 2.0\\n\\n    .. versionadded:: 4.0\\n       Added the `err` parameter.\\n\\n    :param info: the info string to print before pausing.\\n    :param err: if set to message goes to ``stderr`` instead of\\n                ``stdout``, the same as with echo.\\n    \\"\\"\\"\\n    if not isatty(sys.stdin) or not isatty(sys.stdout):\\n        return\\n    try:\\n        if info:\\n            echo(info, nl=False, err=err)\\n        try:\\n            getchar()\\n        except (KeyboardInterrupt, EOFError):\\n            pass\\n    finally:\\n        if info:\\n            echo(err=err)\\n"}\n'
line: b'{"repo_name":"robertglen/flask","ref":"refs/heads/master","path":"tests/test_instance_config.py","content":"# -*- coding: utf-8 -*-\\n\\"\\"\\"\\n    tests.test_instance\\n    ~~~~~~~~~~~~~~~~~~~\\n\\n    :copyright: (c) 2015 by the Flask Team, see AUTHORS for more details.\\n    :license: BSD, see LICENSE for more details.\\n\\"\\"\\"\\nimport os\\nimport sys\\n\\nimport pytest\\nimport flask\\nfrom flask._compat import PY2\\n\\n\\ndef test_explicit_instance_paths(modules_tmpdir):\\n    with pytest.raises(ValueError) as excinfo:\\n        flask.Flask(__name__, instance_path=\'instance\')\\n    assert \'must be absolute\' in str(excinfo.value)\\n\\n    app = flask.Flask(__name__, instance_path=str(modules_tmpdir))\\n    assert app.instance_path == str(modules_tmpdir)\\n\\n\\ndef test_main_module_paths(modules_tmpdir, purge_module):\\n    app = modules_tmpdir.join(\'main_app.py\')\\n    app.write(\'import flask\\\\n\\\\napp = flask.Flask(\\"__main__\\")\')\\n    purge_module(\'main_app\')\\n\\n    from main_app import app\\n    here = os.path.abspath(os.getcwd())\\n    assert app.instance_path == os.path.join(here, \'instance\')\\n\\n\\ndef test_uninstalled_module_paths(modules_tmpdir, purge_module):\\n    app = modules_tmpdir.join(\'config_module_app.py\').write(\\n        \'import os\\\\n\'\\n        \'import flask\\\\n\'\\n        \'here = os.path.abspath(os.path.dirname(__file__))\\\\n\'\\n        \'app = flask.Flask(__name__)\\\\n\'\\n    )\\n    purge_module(\'config_module_app\')\\n\\n    from config_module_app import app\\n    assert app.instance_path == str(modules_tmpdir.join(\'instance\'))\\n\\n\\ndef test_uninstalled_package_paths(modules_tmpdir, purge_module):\\n    app = modules_tmpdir.mkdir(\'config_package_app\')\\n    init = app.join(\'__init__.py\')\\n    init.write(\\n        \'import os\\\\n\'\\n        \'import flask\\\\n\'\\n        \'here = os.path.abspath(os.path.dirname(__file__))\\\\n\'\\n        \'app = flask.Flask(__name__)\\\\n\'\\n    )\\n    purge_module(\'config_package_app\')\\n\\n    from config_package_app import app\\n    assert app.instance_path == str(modules_tmpdir.join(\'instance\'))\\n\\n\\ndef test_installed_module_paths(modules_tmpdir, modules_tmpdir_prefix,\\n                                purge_module, site_packages, limit_loader):\\n    site_packages.join(\'site_app.py\').write(\\n        \'import flask\\\\n\'\\n        \'app = flask.Flask(__name__)\\\\n\'\\n    )\\n    purge_module(\'site_app\')\\n\\n    from site_app import app\\n    assert app.instance_path == \\\\\\n        modules_tmpdir.join(\'var\').join(\'site_app-instance\')\\n\\n\\ndef test_installed_package_paths(limit_loader, modules_tmpdir,\\n                                 modules_tmpdir_prefix, purge_module,\\n                                 monkeypatch):\\n    installed_path = modules_tmpdir.mkdir(\'path\')\\n    monkeypatch.syspath_prepend(installed_path)\\n\\n    app = installed_path.mkdir(\'installed_package\')\\n    init = app.join(\'__init__.py\')\\n    init.write(\'import flask\\\\napp = flask.Flask(__name__)\')\\n    purge_module(\'installed_package\')\\n\\n    from installed_package import app\\n    assert app.instance_path == \\\\\\n        modules_tmpdir.join(\'var\').join(\'installed_package-instance\')\\n\\n\\ndef test_prefix_package_paths(limit_loader, modules_tmpdir,\\n                              modules_tmpdir_prefix, purge_module,\\n                              site_packages):\\n    app = site_packages.mkdir(\'site_package\')\\n    init = app.join(\'__init__.py\')\\n    init.write(\'import flask\\\\napp = flask.Flask(__name__)\')\\n    purge_module(\'site_package\')\\n\\n    import site_package\\n    assert site_package.app.instance_path == \\\\\\n        modules_tmpdir.join(\'var\').join(\'site_package-instance\')\\n\\n\\ndef test_egg_installed_paths(install_egg, modules_tmpdir,\\n                             modules_tmpdir_prefix):\\n    modules_tmpdir.mkdir(\'site_egg\').join(\'__init__.py\').write(\\n        \'import flask\\\\n\\\\napp = flask.Flask(__name__)\'\\n    )\\n    install_egg(\'site_egg\')\\n    try:\\n        import site_egg\\n        assert site_egg.app.instance_path == \\\\\\n            str(modules_tmpdir.join(\'var/\').join(\'site_egg-instance\'))\\n    finally:\\n        if \'site_egg\' in sys.modules:\\n            del sys.modules[\'site_egg\']\\n\\n\\n@pytest.mark.skipif(not PY2, reason=\'This only works under Python 2.\')\\ndef test_meta_path_loader_without_is_package(request, modules_tmpdir):\\n    app = modules_tmpdir.join(\'unimportable.py\')\\n    app.write(\'import flask\\\\napp = flask.Flask(__name__)\')\\n\\n    class Loader(object):\\n        def find_module(self, name, path=None):\\n            return self\\n\\n    sys.meta_path.append(Loader())\\n    request.addfinalizer(sys.meta_path.pop)\\n\\n    with pytest.raises(AttributeError):\\n        import unimportable\\n"}\n'
line: b'{"repo_name":"thomasgilgenast/gilgistatus-nonrel","ref":"refs/heads/master","path":"django/db/backends/creation.py","content":"import sys\\nimport time\\n\\nfrom django.conf import settings\\nfrom django.utils.datastructures import DictWrapper\\n\\n# The prefix to put on the default database name when creating\\n# the test database.\\nTEST_DATABASE_PREFIX = \'test_\'\\n\\nclass BaseDatabaseCreation(object):\\n    \\"\\"\\"\\n    This class encapsulates all backend-specific differences that pertain to\\n    database *creation*, such as the column types to use for particular Django\\n    Fields, the SQL used to create and destroy tables, and the creation and\\n    destruction of test databases.\\n    \\"\\"\\"\\n    data_types = {}\\n\\n    def __init__(self, connection):\\n        self.connection = connection\\n\\n    def _digest(self, *args):\\n        \\"\\"\\"\\n        Generates a 32-bit digest of a set of arguments that can be used to\\n        shorten identifying names.\\n        \\"\\"\\"\\n        return \'%x\' % (abs(hash(args)) % 4294967296L)  # 2**32\\n    \\n    def db_type(self, field):\\n        return self._db_type(field, field.get_internal_type())\\n\\n    def related_db_type(self, field):\\n        return self._db_type(field, field.get_related_internal_type())\\n\\n    def _db_type(self, field, internal_type):\\n        data = DictWrapper(field.__dict__, self.connection.ops.quote_name, \\"qn_\\")\\n        try:\\n            return self.connection.creation.data_types[internal_type] % data\\n        except KeyError:\\n            return None\\n\\n    def sql_create_model(self, model, style, known_models=set()):\\n        \\"\\"\\"\\n        Returns the SQL required to create a single model, as a tuple of:\\n            (list_of_sql, pending_references_dict)\\n        \\"\\"\\"\\n        opts = model._meta\\n        if not opts.managed or opts.proxy:\\n            return [], {}\\n        final_output = []\\n        table_output = []\\n        pending_references = {}\\n        qn = self.connection.ops.quote_name\\n        for f in opts.local_fields:\\n            col_type = f.db_type(connection=self.connection)\\n            tablespace = f.db_tablespace or opts.db_tablespace\\n            if col_type is None:\\n                # Skip ManyToManyFields, because they\'re not represented as\\n                # database columns in this table.\\n                continue\\n            # Make the definition (e.g. \'foo VARCHAR(30)\') for this field.\\n            field_output = [style.SQL_FIELD(qn(f.column)),\\n                style.SQL_COLTYPE(col_type)]\\n            if not f.null:\\n                field_output.append(style.SQL_KEYWORD(\'NOT NULL\'))\\n            if f.primary_key:\\n                field_output.append(style.SQL_KEYWORD(\'PRIMARY KEY\'))\\n            elif f.unique:\\n                field_output.append(style.SQL_KEYWORD(\'UNIQUE\'))\\n            if tablespace and f.unique:\\n                # We must specify the index tablespace inline, because we\\n                # won\'t be generating a CREATE INDEX statement for this field.\\n                field_output.append(self.connection.ops.tablespace_sql(tablespace, inline=True))\\n            if f.rel:\\n                ref_output, pending = self.sql_for_inline_foreign_key_references(f, known_models, style)\\n                if pending:\\n                    pr = pending_references.setdefault(f.rel.to, []).append((model, f))\\n                else:\\n                    field_output.extend(ref_output)\\n            table_output.append(\' \'.join(field_output))\\n        for field_constraints in opts.unique_together:\\n            table_output.append(style.SQL_KEYWORD(\'UNIQUE\') + \' (%s)\' % \\\\\\n                \\", \\".join([style.SQL_FIELD(qn(opts.get_field(f).column)) for f in field_constraints]))\\n\\n        full_statement = [style.SQL_KEYWORD(\'CREATE TABLE\') + \' \' + style.SQL_TABLE(qn(opts.db_table)) + \' (\']\\n        for i, line in enumerate(table_output): # Combine and add commas.\\n            full_statement.append(\'    %s%s\' % (line, i \\u003c len(table_output)-1 and \',\' or \'\'))\\n        full_statement.append(\')\')\\n        if opts.db_tablespace:\\n            full_statement.append(self.connection.ops.tablespace_sql(opts.db_tablespace))\\n        full_statement.append(\';\')\\n        final_output.append(\'\\\\n\'.join(full_statement))\\n\\n        if opts.has_auto_field:\\n            # Add any extra SQL needed to support auto-incrementing primary keys.\\n            auto_column = opts.auto_field.db_column or opts.auto_field.name\\n            autoinc_sql = self.connection.ops.autoinc_sql(opts.db_table, auto_column)\\n            if autoinc_sql:\\n                for stmt in autoinc_sql:\\n                    final_output.append(stmt)\\n\\n        return final_output, pending_references\\n\\n    def sql_for_inline_foreign_key_references(self, field, known_models, style):\\n        \\"Return the SQL snippet defining the foreign key reference for a field\\"\\n        qn = self.connection.ops.quote_name\\n        if field.rel.to in known_models:\\n            output = [style.SQL_KEYWORD(\'REFERENCES\') + \' \' + \\\\\\n                style.SQL_TABLE(qn(field.rel.to._meta.db_table)) + \' (\' + \\\\\\n                style.SQL_FIELD(qn(field.rel.to._meta.get_field(field.rel.field_name).column)) + \')\' +\\n                self.connection.ops.deferrable_sql()\\n            ]\\n            pending = False\\n        else:\\n            # We haven\'t yet created the table to which this field\\n            # is related, so save it for later.\\n            output = []\\n            pending = True\\n\\n        return output, pending\\n\\n    def sql_for_pending_references(self, model, style, pending_references):\\n        \\"Returns any ALTER TABLE statements to add constraints after the fact.\\"\\n        from django.db.backends.util import truncate_name\\n\\n        if not model._meta.managed or model._meta.proxy:\\n            return []\\n        qn = self.connection.ops.quote_name\\n        final_output = []\\n        opts = model._meta\\n        if model in pending_references:\\n            for rel_class, f in pending_references[model]:\\n                rel_opts = rel_class._meta\\n                r_table = rel_opts.db_table\\n                r_col = f.column\\n                table = opts.db_table\\n                col = opts.get_field(f.rel.field_name).column\\n                # For MySQL, r_name must be unique in the first 64 characters.\\n                # So we are careful with character usage here.\\n                r_name = \'%s_refs_%s_%s\' % (r_col, col, self._digest(r_table, table))\\n                final_output.append(style.SQL_KEYWORD(\'ALTER TABLE\') + \' %s ADD CONSTRAINT %s FOREIGN KEY (%s) REFERENCES %s (%s)%s;\' % \\\\\\n                    (qn(r_table), qn(truncate_name(r_name, self.connection.ops.max_name_length())),\\n                    qn(r_col), qn(table), qn(col),\\n                    self.connection.ops.deferrable_sql()))\\n            del pending_references[model]\\n        return final_output\\n\\n    def sql_for_many_to_many(self, model, style):\\n        \\"Return the CREATE TABLE statments for all the many-to-many tables defined on a model\\"\\n        import warnings\\n        warnings.warn(\\n            \'Database creation API for m2m tables has been deprecated. M2M models are now automatically generated\',\\n            DeprecationWarning\\n        )\\n\\n        output = []\\n        for f in model._meta.local_many_to_many:\\n            if model._meta.managed or f.rel.to._meta.managed:\\n                output.extend(self.sql_for_many_to_many_field(model, f, style))\\n        return output\\n\\n    def sql_for_many_to_many_field(self, model, f, style):\\n        \\"Return the CREATE TABLE statements for a single m2m field\\"\\n        import warnings\\n        warnings.warn(\\n            \'Database creation API for m2m tables has been deprecated. M2M models are now automatically generated\',\\n            DeprecationWarning\\n        )\\n\\n        from django.db import models\\n        from django.db.backends.util import truncate_name\\n\\n        output = []\\n        if f.auto_created:\\n            opts = model._meta\\n            qn = self.connection.ops.quote_name\\n            tablespace = f.db_tablespace or opts.db_tablespace\\n            if tablespace:\\n                sql = self.connection.ops.tablespace_sql(tablespace, inline=True)\\n                if sql:\\n                    tablespace_sql = \' \' + sql\\n                else:\\n                    tablespace_sql = \'\'\\n            else:\\n                tablespace_sql = \'\'\\n            table_output = [style.SQL_KEYWORD(\'CREATE TABLE\') + \' \' + \\\\\\n                style.SQL_TABLE(qn(f.m2m_db_table())) + \' (\']\\n            table_output.append(\'    %s %s %s%s,\' %\\n                (style.SQL_FIELD(qn(\'id\')),\\n                style.SQL_COLTYPE(models.AutoField(primary_key=True).db_type(connection=self.connection)),\\n                style.SQL_KEYWORD(\'NOT NULL PRIMARY KEY\'),\\n                tablespace_sql))\\n\\n            deferred = []\\n            inline_output, deferred = self.sql_for_inline_many_to_many_references(model, f, style)\\n            table_output.extend(inline_output)\\n\\n            table_output.append(\'    %s (%s, %s)%s\' %\\n                (style.SQL_KEYWORD(\'UNIQUE\'),\\n                style.SQL_FIELD(qn(f.m2m_column_name())),\\n                style.SQL_FIELD(qn(f.m2m_reverse_name())),\\n                tablespace_sql))\\n            table_output.append(\')\')\\n            if opts.db_tablespace:\\n                # f.db_tablespace is only for indices, so ignore its value here.\\n                table_output.append(self.connection.ops.tablespace_sql(opts.db_tablespace))\\n            table_output.append(\';\')\\n            output.append(\'\\\\n\'.join(table_output))\\n\\n            for r_table, r_col, table, col in deferred:\\n                r_name = \'%s_refs_%s_%s\' % (r_col, col, self._digest(r_table, table))\\n                output.append(style.SQL_KEYWORD(\'ALTER TABLE\') + \' %s ADD CONSTRAINT %s FOREIGN KEY (%s) REFERENCES %s (%s)%s;\' %\\n                (qn(r_table),\\n                qn(truncate_name(r_name, self.connection.ops.max_name_length())),\\n                qn(r_col), qn(table), qn(col),\\n                self.connection.ops.deferrable_sql()))\\n\\n            # Add any extra SQL needed to support auto-incrementing PKs\\n            autoinc_sql = self.connection.ops.autoinc_sql(f.m2m_db_table(), \'id\')\\n            if autoinc_sql:\\n                for stmt in autoinc_sql:\\n                    output.append(stmt)\\n        return output\\n\\n    def sql_for_inline_many_to_many_references(self, model, field, style):\\n        \\"Create the references to other tables required by a many-to-many table\\"\\n        import warnings\\n        warnings.warn(\\n            \'Database creation API for m2m tables has been deprecated. M2M models are now automatically generated\',\\n            DeprecationWarning\\n        )\\n\\n        from django.db import models\\n        opts = model._meta\\n        qn = self.connection.ops.quote_name\\n\\n        table_output = [\\n            \'    %s %s %s %s (%s)%s,\' %\\n                (style.SQL_FIELD(qn(field.m2m_column_name())),\\n                style.SQL_COLTYPE(models.ForeignKey(model).db_type(connection=self.connection)),\\n                style.SQL_KEYWORD(\'NOT NULL REFERENCES\'),\\n                style.SQL_TABLE(qn(opts.db_table)),\\n                style.SQL_FIELD(qn(opts.pk.column)),\\n                self.connection.ops.deferrable_sql()),\\n            \'    %s %s %s %s (%s)%s,\' %\\n                (style.SQL_FIELD(qn(field.m2m_reverse_name())),\\n                style.SQL_COLTYPE(models.ForeignKey(field.rel.to).db_type(connection=self.connection)),\\n                style.SQL_KEYWORD(\'NOT NULL REFERENCES\'),\\n                style.SQL_TABLE(qn(field.rel.to._meta.db_table)),\\n                style.SQL_FIELD(qn(field.rel.to._meta.pk.column)),\\n                self.connection.ops.deferrable_sql())\\n        ]\\n        deferred = []\\n\\n        return table_output, deferred\\n\\n    def sql_indexes_for_model(self, model, style):\\n        \\"Returns the CREATE INDEX SQL statements for a single model\\"\\n        if not model._meta.managed or model._meta.proxy:\\n            return []\\n        output = []\\n        for f in model._meta.local_fields:\\n            output.extend(self.sql_indexes_for_field(model, f, style))\\n        return output\\n\\n    def sql_indexes_for_field(self, model, f, style):\\n        \\"Return the CREATE INDEX SQL statements for a single model field\\"\\n        from django.db.backends.util import truncate_name\\n\\n        if f.db_index and not f.unique:\\n            qn = self.connection.ops.quote_name\\n            tablespace = f.db_tablespace or model._meta.db_tablespace\\n            if tablespace:\\n                sql = self.connection.ops.tablespace_sql(tablespace)\\n                if sql:\\n                    tablespace_sql = \' \' + sql\\n                else:\\n                    tablespace_sql = \'\'\\n            else:\\n                tablespace_sql = \'\'\\n            i_name = \'%s_%s\' % (model._meta.db_table, self._digest(f.column))\\n            output = [style.SQL_KEYWORD(\'CREATE INDEX\') + \' \' +\\n                style.SQL_TABLE(qn(truncate_name(i_name, self.connection.ops.max_name_length()))) + \' \' +\\n                style.SQL_KEYWORD(\'ON\') + \' \' +\\n                style.SQL_TABLE(qn(model._meta.db_table)) + \' \' +\\n                \\"(%s)\\" % style.SQL_FIELD(qn(f.column)) +\\n                \\"%s;\\" % tablespace_sql]\\n        else:\\n            output = []\\n        return output\\n\\n    def sql_destroy_model(self, model, references_to_delete, style):\\n        \\"Return the DROP TABLE and restraint dropping statements for a single model\\"\\n        if not model._meta.managed or model._meta.proxy:\\n            return []\\n        # Drop the table now\\n        qn = self.connection.ops.quote_name\\n        output = [\'%s %s;\' % (style.SQL_KEYWORD(\'DROP TABLE\'),\\n                              style.SQL_TABLE(qn(model._meta.db_table)))]\\n        if model in references_to_delete:\\n            output.extend(self.sql_remove_table_constraints(model, references_to_delete, style))\\n\\n        if model._meta.has_auto_field:\\n            ds = self.connection.ops.drop_sequence_sql(model._meta.db_table)\\n            if ds:\\n                output.append(ds)\\n        return output\\n\\n    def sql_remove_table_constraints(self, model, references_to_delete, style):\\n        from django.db.backends.util import truncate_name\\n\\n        if not model._meta.managed or model._meta.proxy:\\n            return []\\n        output = []\\n        qn = self.connection.ops.quote_name\\n        for rel_class, f in references_to_delete[model]:\\n            table = rel_class._meta.db_table\\n            col = f.column\\n            r_table = model._meta.db_table\\n            r_col = model._meta.get_field(f.rel.field_name).column\\n            r_name = \'%s_refs_%s_%s\' % (col, r_col, self._digest(table, r_table))\\n            output.append(\'%s %s %s %s;\' % \\\\\\n                (style.SQL_KEYWORD(\'ALTER TABLE\'),\\n                style.SQL_TABLE(qn(table)),\\n                style.SQL_KEYWORD(self.connection.ops.drop_foreignkey_sql()),\\n                style.SQL_FIELD(qn(truncate_name(r_name, self.connection.ops.max_name_length())))))\\n        del references_to_delete[model]\\n        return output\\n\\n    def sql_destroy_many_to_many(self, model, f, style):\\n        \\"Returns the DROP TABLE statements for a single m2m field\\"\\n        import warnings\\n        warnings.warn(\\n            \'Database creation API for m2m tables has been deprecated. M2M models are now automatically generated\',\\n            DeprecationWarning\\n        )\\n\\n        qn = self.connection.ops.quote_name\\n        output = []\\n        if f.auto_created:\\n            output.append(\\"%s %s;\\" % (style.SQL_KEYWORD(\'DROP TABLE\'),\\n                style.SQL_TABLE(qn(f.m2m_db_table()))))\\n            ds = self.connection.ops.drop_sequence_sql(\\"%s_%s\\" % (model._meta.db_table, f.column))\\n            if ds:\\n                output.append(ds)\\n        return output\\n\\n    def create_test_db(self, verbosity=1, autoclobber=False):\\n        \\"\\"\\"\\n        Creates a test database, prompting the user for confirmation if the\\n        database already exists. Returns the name of the test database created.\\n        \\"\\"\\"\\n        # Don\'t import django.core.management if it isn\'t needed.\\n        from django.core.management import call_command\\n\\n        test_database_name = self._get_test_db_name()\\n\\n        if verbosity \\u003e= 1:\\n            test_db_repr = \'\'\\n            if verbosity \\u003e= 2:\\n                test_db_repr = \\" (\'%s\')\\" % test_database_name\\n            print \\"Creating test database for alias \'%s\'%s...\\" % (self.connection.alias, test_db_repr)\\n\\n        self._create_test_db(verbosity, autoclobber)\\n\\n        self.connection.close()\\n        self.connection.settings_dict[\\"NAME\\"] = test_database_name\\n\\n        # Confirm the feature set of the test database\\n        self.connection.features.confirm()\\n\\n        # Report syncdb messages at one level lower than that requested.\\n        # This ensures we don\'t get flooded with messages during testing\\n        # (unless you really ask to be flooded)\\n        call_command(\'syncdb\',\\n            verbosity=max(verbosity - 1, 0),\\n            interactive=False,\\n            database=self.connection.alias,\\n            load_initial_data=False)\\n\\n        # We need to then do a flush to ensure that any data installed by\\n        # custom SQL has been removed. The only test data should come from\\n        # test fixtures, or autogenerated from post_syncdb triggers.\\n        # This has the side effect of loading initial data (which was\\n        # intentionally skipped in the syncdb).\\n        call_command(\'flush\',\\n            verbosity=max(verbosity - 1, 0),\\n            interactive=False,\\n            database=self.connection.alias)\\n\\n        from django.core.cache import get_cache\\n        from django.core.cache.backends.db import BaseDatabaseCache\\n        for cache_alias in settings.CACHES:\\n            cache = get_cache(cache_alias)\\n            if isinstance(cache, BaseDatabaseCache):\\n                from django.db import router\\n                if router.allow_syncdb(self.connection.alias, cache.cache_model_class):\\n                    call_command(\'createcachetable\', cache._table, database=self.connection.alias)\\n\\n        # Get a cursor (even though we don\'t need one yet). This has\\n        # the side effect of initializing the test database.\\n        cursor = self.connection.cursor()\\n\\n        return test_database_name\\n\\n    def _get_test_db_name(self):\\n        \\"\\"\\"\\n        Internal implementation - returns the name of the test DB that will be\\n        created. Only useful when called from create_test_db() and\\n        _create_test_db() and when no external munging is done with the \'NAME\'\\n        or \'TEST_NAME\' settings.\\n        \\"\\"\\"\\n        if self.connection.settings_dict[\'TEST_NAME\']:\\n            return self.connection.settings_dict[\'TEST_NAME\']\\n        return TEST_DATABASE_PREFIX + self.connection.settings_dict[\'NAME\']\\n\\n    def _create_test_db(self, verbosity, autoclobber):\\n        \\"Internal implementation - creates the test db tables.\\"\\n        suffix = self.sql_table_creation_suffix()\\n\\n        test_database_name = self._get_test_db_name()\\n\\n        qn = self.connection.ops.quote_name\\n\\n        # Create the test database and connect to it. We need to autocommit\\n        # if the database supports it because PostgreSQL doesn\'t allow\\n        # CREATE/DROP DATABASE statements within transactions.\\n        cursor = self.connection.cursor()\\n        self.set_autocommit()\\n        try:\\n            cursor.execute(\\"CREATE DATABASE %s %s\\" % (qn(test_database_name), suffix))\\n        except Exception, e:\\n            sys.stderr.write(\\"Got an error creating the test database: %s\\\\n\\" % e)\\n            if not autoclobber:\\n                confirm = raw_input(\\"Type \'yes\' if you would like to try deleting the test database \'%s\', or \'no\' to cancel: \\" % test_database_name)\\n            if autoclobber or confirm == \'yes\':\\n                try:\\n                    if verbosity \\u003e= 1:\\n                        print \\"Destroying old test database \'%s\'...\\" % self.connection.alias\\n                    cursor.execute(\\"DROP DATABASE %s\\" % qn(test_database_name))\\n                    cursor.execute(\\"CREATE DATABASE %s %s\\" % (qn(test_database_name), suffix))\\n                except Exception, e:\\n                    sys.stderr.write(\\"Got an error recreating the test database: %s\\\\n\\" % e)\\n                    sys.exit(2)\\n            else:\\n                print \\"Tests cancelled.\\"\\n                sys.exit(1)\\n\\n        return test_database_name\\n\\n    def destroy_test_db(self, old_database_name, verbosity=1):\\n        \\"\\"\\"\\n        Destroy a test database, prompting the user for confirmation if the\\n        database already exists. Returns the name of the test database created.\\n        \\"\\"\\"\\n        self.connection.close()\\n        test_database_name = self.connection.settings_dict[\'NAME\']\\n        if verbosity \\u003e= 1:\\n            test_db_repr = \'\'\\n            if verbosity \\u003e= 2:\\n                test_db_repr = \\" (\'%s\')\\" % test_database_name\\n            print \\"Destroying test database for alias \'%s\'%s...\\" % (self.connection.alias, test_db_repr)\\n        self.connection.settings_dict[\'NAME\'] = old_database_name\\n\\n        self._destroy_test_db(test_database_name, verbosity)\\n\\n    def _destroy_test_db(self, test_database_name, verbosity):\\n        \\"Internal implementation - remove the test db tables.\\"\\n        # Remove the test database to clean up after\\n        # ourselves. Connect to the previous database (not the test database)\\n        # to do so, because it\'s not allowed to delete a database while being\\n        # connected to it.\\n        cursor = self.connection.cursor()\\n        self.set_autocommit()\\n        time.sleep(1) # To avoid \\"database is being accessed by other users\\" errors.\\n        cursor.execute(\\"DROP DATABASE %s\\" % self.connection.ops.quote_name(test_database_name))\\n        self.connection.close()\\n\\n    def set_autocommit(self):\\n        \\"Make sure a connection is in autocommit mode.\\"\\n        if hasattr(self.connection.connection, \\"autocommit\\"):\\n            if callable(self.connection.connection.autocommit):\\n                self.connection.connection.autocommit(True)\\n            else:\\n                self.connection.connection.autocommit = True\\n        elif hasattr(self.connection.connection, \\"set_isolation_level\\"):\\n            self.connection.connection.set_isolation_level(0)\\n\\n    def sql_table_creation_suffix(self):\\n        \\"SQL to append to the end of the test table creation statements\\"\\n        return \'\'\\n\\n    def test_db_signature(self):\\n        \\"\\"\\"\\n        Returns a tuple with elements of self.connection.settings_dict (a\\n        DATABASES setting value) that uniquely identify a database\\n        accordingly to the RDBMS particularities.\\n        \\"\\"\\"\\n        settings_dict = self.connection.settings_dict\\n        return (\\n            settings_dict[\'HOST\'],\\n            settings_dict[\'PORT\'],\\n            settings_dict[\'ENGINE\'],\\n            settings_dict[\'NAME\']\\n        )\\n"}\n'
line: b'{"repo_name":"moijes12/oh-mainline","ref":"refs/heads/master","path":"vendor/packages/sphinx/tests/test_intersphinx.py","content":"# -*- coding: utf-8 -*-\\n\\"\\"\\"\\n    test_intersphinx\\n    ~~~~~~~~~~~~~~~~\\n\\n    Test the intersphinx extension.\\n\\n    :copyright: Copyright 2007-2013 by the Sphinx team, see AUTHORS.\\n    :license: BSD, see LICENSE for details.\\n\\"\\"\\"\\n\\nimport zlib\\nimport posixpath\\ntry:\\n    from io import BytesIO\\nexcept ImportError:\\n    from cStringIO import StringIO as BytesIO\\n\\nfrom docutils import nodes\\n\\nfrom sphinx import addnodes\\nfrom sphinx.ext.intersphinx import read_inventory_v1, read_inventory_v2, \\\\\\n     load_mappings, missing_reference\\n\\nfrom util import with_app, with_tempdir, write_file\\n\\n\\ninventory_v1 = \'\'\'\\\\\\n# Sphinx inventory version 1\\n# Project: foo\\n# Version: 1.0\\nmodule mod foo.html\\nmodule.cls class foo.html\\n\'\'\'.encode(\'utf-8\')\\n\\ninventory_v2 = \'\'\'\\\\\\n# Sphinx inventory version 2\\n# Project: foo\\n# Version: 2.0\\n# The remainder of this file is compressed with zlib.\\n\'\'\'.encode(\'utf-8\') + zlib.compress(\'\'\'\\\\\\nmodule1 py:module 0 foo.html#module-module1 Long Module desc\\nmodule2 py:module 0 foo.html#module-$ -\\nmodule1.func py:function 1 sub/foo.html#$ -\\nCFunc c:function 2 cfunc.html#CFunc -\\na term std:term -1 glossary.html#term-a-term -\\n\'\'\'.encode(\'utf-8\'))\\n\\n\\ndef test_read_inventory_v1():\\n    f = BytesIO(inventory_v1)\\n    f.readline()\\n    invdata = read_inventory_v1(f, \'/util\', posixpath.join)\\n    assert invdata[\'py:module\'][\'module\'] == \\\\\\n           (\'foo\', \'1.0\', \'/util/foo.html#module-module\', \'-\')\\n    assert invdata[\'py:class\'][\'module.cls\'] == \\\\\\n           (\'foo\', \'1.0\', \'/util/foo.html#module.cls\', \'-\')\\n\\n\\ndef test_read_inventory_v2():\\n    f = BytesIO(inventory_v2)\\n    f.readline()\\n    invdata1 = read_inventory_v2(f, \'/util\', posixpath.join)\\n\\n    # try again with a small buffer size to test the chunking algorithm\\n    f = BytesIO(inventory_v2)\\n    f.readline()\\n    invdata2 = read_inventory_v2(f, \'/util\', posixpath.join, bufsize=5)\\n\\n    assert invdata1 == invdata2\\n\\n    assert len(invdata1[\'py:module\']) == 2\\n    assert invdata1[\'py:module\'][\'module1\'] == \\\\\\n           (\'foo\', \'2.0\', \'/util/foo.html#module-module1\', \'Long Module desc\')\\n    assert invdata1[\'py:module\'][\'module2\'] == \\\\\\n           (\'foo\', \'2.0\', \'/util/foo.html#module-module2\', \'-\')\\n    assert invdata1[\'py:function\'][\'module1.func\'][2] == \\\\\\n           \'/util/sub/foo.html#module1.func\'\\n    assert invdata1[\'c:function\'][\'CFunc\'][2] == \'/util/cfunc.html#CFunc\'\\n    assert invdata1[\'std:term\'][\'a term\'][2] == \\\\\\n           \'/util/glossary.html#term-a-term\'\\n\\n\\n@with_app(confoverrides={\'extensions\': \'sphinx.ext.intersphinx\'})\\n@with_tempdir\\ndef test_missing_reference(tempdir, app):\\n    inv_file = tempdir / \'inventory\'\\n    write_file(inv_file, inventory_v2)\\n    app.config.intersphinx_mapping = {\\n        \'http://docs.python.org/\': inv_file,\\n        \'py3k\': (\'http://docs.python.org/py3k/\', inv_file),\\n    }\\n    app.config.intersphinx_cache_limit = 0\\n\\n    # load the inventory and check if it\'s done correctly\\n    load_mappings(app)\\n    inv = app.env.intersphinx_inventory\\n\\n    assert inv[\'py:module\'][\'module2\'] == \\\\\\n           (\'foo\', \'2.0\', \'http://docs.python.org/foo.html#module-module2\', \'-\')\\n\\n    # create fake nodes and check referencing\\n\\n    def fake_node(domain, type, target, content, **attrs):\\n        contnode = nodes.emphasis(content, content)\\n        node = addnodes.pending_xref(\'\')\\n        node[\'reftarget\'] = target\\n        node[\'reftype\'] = type\\n        node[\'refdomain\'] = domain\\n        node.attributes.update(attrs)\\n        node += contnode\\n        return node, contnode\\n\\n    def reference_check(*args, **kwds):\\n        node, contnode = fake_node(*args, **kwds)\\n        return missing_reference(app, app.env, node, contnode)\\n\\n    # check resolution when a target is found\\n    rn = reference_check(\'py\', \'func\', \'module1.func\', \'foo\')\\n    assert isinstance(rn, nodes.reference)\\n    assert rn[\'refuri\'] == \'http://docs.python.org/sub/foo.html#module1.func\'\\n    assert rn[\'reftitle\'] == \'(in foo v2.0)\'\\n    assert rn[0].astext() == \'foo\'\\n\\n    # create unresolvable nodes and check None return value\\n    assert reference_check(\'py\', \'foo\', \'module1.func\', \'foo\') is None\\n    assert reference_check(\'py\', \'func\', \'foo\', \'foo\') is None\\n    assert reference_check(\'py\', \'func\', \'foo\', \'foo\') is None\\n\\n    # check handling of prefixes\\n\\n    # prefix given, target found: prefix is stripped\\n    rn = reference_check(\'py\', \'mod\', \'py3k:module2\', \'py3k:module2\')\\n    assert rn[0].astext() == \'module2\'\\n\\n    # prefix given, but not in title: nothing stripped\\n    rn = reference_check(\'py\', \'mod\', \'py3k:module2\', \'module2\')\\n    assert rn[0].astext() == \'module2\'\\n\\n    # prefix given, but explicit: nothing stripped\\n    rn = reference_check(\'py\', \'mod\', \'py3k:module2\', \'py3k:module2\',\\n                         refexplicit=True)\\n    assert rn[0].astext() == \'py3k:module2\'\\n\\n    # prefix given, target not found and nonexplicit title: prefix is stripped\\n    node, contnode = fake_node(\'py\', \'mod\', \'py3k:unknown\', \'py3k:unknown\',\\n                               refexplicit=False)\\n    rn = missing_reference(app, app.env, node, contnode)\\n    assert rn is None\\n    assert contnode[0].astext() == \'unknown\'\\n\\n    # prefix given, target not found and explicit title: nothing is changed\\n    node, contnode = fake_node(\'py\', \'mod\', \'py3k:unknown\', \'py3k:unknown\',\\n                               refexplicit=True)\\n    rn = missing_reference(app, app.env, node, contnode)\\n    assert rn is None\\n    assert contnode[0].astext() == \'py3k:unknown\'\\n\\n\\n@with_app(confoverrides={\'extensions\': \'sphinx.ext.intersphinx\'})\\n@with_tempdir\\ndef test_load_mappings_warnings(tempdir, app):\\n    \\"\\"\\"\\n    load_mappings issues a warning if new-style mapping\\n    identifiers are not alphanumeric\\n    \\"\\"\\"\\n    inv_file = tempdir / \'inventory\'\\n    write_file(inv_file, inventory_v2)\\n    app.config.intersphinx_mapping = {\\n        \'http://docs.python.org/\': inv_file,\\n        \'py3k\': (\'http://docs.python.org/py3k/\', inv_file),\\n        \'repoze.workflow\': (\'http://docs.repoze.org/workflow/\', inv_file),\\n        \'django-taggit\': (\'http://django-taggit.readthedocs.org/en/latest/\',\\n                          inv_file)\\n    }\\n\\n    app.config.intersphinx_cache_limit = 0\\n    # load the inventory and check if it\'s done correctly\\n    load_mappings(app)\\n    assert len(app._warning.content) == 2\\n"}\n'
line: b'{"repo_name":"xianjunzhengbackup/Cloud-Native-Python","ref":"refs/heads/master","path":"env/lib/python3.6/site-packages/pip/_vendor/progress/__init__.py","content":"# Copyright (c) 2012 Giorgos Verigakis \\u003cverigak@gmail.com\\u003e\\n#\\n# Permission to use, copy, modify, and distribute this software for any\\n# purpose with or without fee is hereby granted, provided that the above\\n# copyright notice and this permission notice appear in all copies.\\n#\\n# THE SOFTWARE IS PROVIDED \\"AS IS\\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\\n# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\\n# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\\n# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\\n# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\\n# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\\n# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\\n\\nfrom __future__ import division\\n\\nfrom collections import deque\\nfrom datetime import timedelta\\nfrom math import ceil\\nfrom sys import stderr\\nfrom time import time\\n\\n\\n__version__ = \'1.2\'\\n\\n\\nclass Infinite(object):\\n    file = stderr\\n    sma_window = 10\\n\\n    def __init__(self, *args, **kwargs):\\n        self.index = 0\\n        self.start_ts = time()\\n        self._ts = self.start_ts\\n        self._dt = deque(maxlen=self.sma_window)\\n        for key, val in kwargs.items():\\n            setattr(self, key, val)\\n\\n    def __getitem__(self, key):\\n        if key.startswith(\'_\'):\\n            return None\\n        return getattr(self, key, None)\\n\\n    @property\\n    def avg(self):\\n        return sum(self._dt) / len(self._dt) if self._dt else 0\\n\\n    @property\\n    def elapsed(self):\\n        return int(time() - self.start_ts)\\n\\n    @property\\n    def elapsed_td(self):\\n        return timedelta(seconds=self.elapsed)\\n\\n    def update(self):\\n        pass\\n\\n    def start(self):\\n        pass\\n\\n    def finish(self):\\n        pass\\n\\n    def next(self, n=1):\\n        if n \\u003e 0:\\n            now = time()\\n            dt = (now - self._ts) / n\\n            self._dt.append(dt)\\n            self._ts = now\\n\\n        self.index = self.index + n\\n        self.update()\\n\\n    def iter(self, it):\\n        for x in it:\\n            yield x\\n            self.next()\\n        self.finish()\\n\\n\\nclass Progress(Infinite):\\n    def __init__(self, *args, **kwargs):\\n        super(Progress, self).__init__(*args, **kwargs)\\n        self.max = kwargs.get(\'max\', 100)\\n\\n    @property\\n    def eta(self):\\n        return int(ceil(self.avg * self.remaining))\\n\\n    @property\\n    def eta_td(self):\\n        return timedelta(seconds=self.eta)\\n\\n    @property\\n    def percent(self):\\n        return self.progress * 100\\n\\n    @property\\n    def progress(self):\\n        return min(1, self.index / self.max)\\n\\n    @property\\n    def remaining(self):\\n        return max(self.max - self.index, 0)\\n\\n    def start(self):\\n        self.update()\\n\\n    def goto(self, index):\\n        incr = index - self.index\\n        self.next(incr)\\n\\n    def iter(self, it):\\n        try:\\n            self.max = len(it)\\n        except TypeError:\\n            pass\\n\\n        for x in it:\\n            yield x\\n            self.next()\\n        self.finish()\\n"}\n'
line: b'{"repo_name":"thierry1985/project-1022","ref":"refs/heads/master","path":"MISC/TFD-0.2.2/translate/pddl/parser.py","content":"__all__ = [\\"ParseError\\", \\"parse_nested_list\\"]\\n\\nclass ParseError(Exception):\\n  pass\\n\\n# Basic functions for parsing PDDL (Lisp) files.\\ndef parse_nested_list(input_file):\\n  tokens = tokenize(input_file)\\n  next_token = tokens.next()\\n  if next_token != \\"(\\":\\n    raise ParseError(\\"Expected \'(\', got %s.\\" % next_token)\\n  result = list(parse_list_aux(tokens))\\n  for tok in tokens:  # Check that generator is exhausted.\\n    raise ParseError(\\"Unexpected token: %s.\\" % tok)\\n  return result\\n  \\ndef tokenize(input):\\n  for line in input:\\n    line = line.split(\\";\\", 1)[0]  # Strip comments.\\n    line = line.replace(\\"(\\", \\" ( \\").replace(\\")\\", \\" ) \\").replace(\\"?\\", \\" ?\\")\\n    for token in line.split():\\n      yield token.lower()\\n\\ndef parse_list_aux(tokenstream):\\n  # Leading \\"(\\" has already been swallowed.\\n  while True:\\n    try:\\n      token = tokenstream.next()\\n    except StopIteration:\\n      raise ParseError()\\n    if token == \\")\\":\\n      return\\n    elif token == \\"(\\":\\n      yield list(parse_list_aux(tokenstream))\\n    else:\\n      yield token\\n\\n"}\n'
line: b'{"repo_name":"andrewburnheimer/ptpop","ref":"refs/heads/master","path":"ptpop/Console.py","content":"#!/usr/local/bin/python\\n\'\'\'\\nConsole Class\\n\'\'\'\\n\'\'\'\\nTo Do:\\n    -\\n\'\'\'\\n\\nfrom Listener import Listener\\nfrom _version import __version__\\nimport time\\n\\n# =============================================================================\\n# Console\\n# \\n# Inheriting from `object` (top-level class)\\n# =============================================================================\\nclass Console(object):\\n    def __init__(self, args=None):\\n        \'\'\'\\n        Console Initialization\\n        Input Attributes:\\n        -----------------\\n        self.args -\\u003e argparse.Namespace: object holding attributes set\\n                                         on command-line.\\n        \'\'\'\\n\\n        # Default Values\\n        delay = 3.0\\n        number = 1 # XXX should be = 0\\n        command = [ ]\\n        interface = \'eth0\'\\n        listen = False\\n        host = \'localhost\'\\n        if args:\\n            delay = float(args.delay) if args.delay else delay\\n            number = args.number if (args.number != None) else number\\n            command = args.command if args.command else command\\n            interface = args.interface if args.interface else interface\\n            listen = args.listen if args.listen else listen\\n            host = args.host if args.host else host\\n\\n        # Input Checks\\n        if command != [ ]:\\n            raise NotImplementedError(\'Issuing commands to hosts has \' +\\n                    \'not been implemented yet\')\\n\\n        # init ...\\n        if listen:\\n            self.listener = Listener(interface)\\n            key = \'\'\'\\nremote          Dly St Dom Pr1  Cl Acc   Var  Pr2       Uniq       SyncT  DlyT  AnnT\\n====================================================================================\'\'\'.strip()\\n\\n\\n            while number \\u003e 0:\\n                # Report output directly to console\\n                fmt=\'%a %b %d %Y %H:%M:%S\'\\n                t = time.time()\\n                time_str = time.strftime(fmt, time.localtime(t))\\n                time_msecs = int((t - int(t)) * 1000)\\n\\n                print time_str + \'.%03d \' % (time_msecs) + time.tzname[0]\\n                print key\\n\\n                # output data seen in since last iteration\\n                neighbor_stats = self.listener.ptp_neighbors\\n                for neighbor in neighbor_stats:\\n                    print self.listener.ptp_neighbors[neighbor]\\n\\n                print\\n                number -= 1\\n                if number \\u003c= 0:\\n                    exit(0)\\n                    # No need to wait after the last iteration\\n                time.sleep(delay)\\n\\n            # Enter into the interactive environment, exit when q is\\n            # issued\\n\\n        else:\\n            for supplied_command in command:\\n                command = supplied_command.lower()\\n\\n                if command == \'rv\' or command == \'readvar\':\\n                    None\\n                    # Assuming to be similar to NTPQ\\n            # root@raspberrypi:/home/puppet# ntpq -n -c rv -c peers\\n            #associd=0 status=0615 leap_none, sync_ntp, 1 event, clock_sync,\\n            #version=\\"ntpd 4.2.6p5@1.2349-o Mon Nov  2 04:29:47 UTC 2015 (1)\\",\\n            #processor=\\"armv6l\\", system=\\"Linux/4.1.17+\\", leap=00, stratum=3,\\n            #precision=-20, rootdelay=2.916, rootdisp=60.561,\\n            #refid=3.44.174.43,\\n            #reftime=da7f3666.54078831  Mon, Feb 29 2016 21:28:06.328,\\n            #clock=da7f38cd.57a72dc6  Mon, Feb 29 2016 21:38:21.342,\\n            #peer=7185, tc=8,\\n            #mintc=3, offset=9.208, frequency=-48.954, sys_jitter=0.000,\\n            #clk_jitter=16.919, clk_wander=4.216\\n                elif command == \'peers\':\\n                    None\\n                    # Assuming to be similar to NTPQ\\n            #     remote           refid      st t when poll reach   delay   offset  jitter\\n            #==============================================================================\\n            #*useclsifl158.tf 3.199.96.254     2 u   14 1024  377    1.582    0.186   0.919\\n                else:\\n                    raise NotImplementedError(\'Unknown command, \\\\\'\' +\\n                            command + \'\\\\\'\')\\n\\n# __main__.py is executed when the package is instantiated\\nimport argparse\\n\\ndef main():\\n    parser = argparse.ArgumentParser(prog=\'ptpop\', description=\'Gain \' +\\n        \'insight into the operations of IEEE 1588 Precision Time Protocol \' +\\n        \'domains on a network. Press the \\\\\'q\\\\\' key to quit.\')\\n\\n    command_choices=[\'readvar\', \'rv\', \'peers\']\\n    parser.add_argument(\'host\', type=str, nargs=\'?\', help=\'each of the \' +\\n                        \'commands will be sent to the PTP servers \' +\\n                        \'running on the host provided, localhost by \' +\\n                        \'default.\')\\n    parser.add_argument(\'-c\', \'--command\', type=str, action=\'append\',\\n                        help=\'a command to run on the provided host, \' +\\n                        \'i.e. \' + str(command_choices) + \', \\\\\'readvar\\\\\' \' +\\n                        \'by default. Multiple commands can be issued.\')\\n    parser.add_argument(\'-i\', \'--interface\', type=str,\\n                        help=\'interface to issue commands on or to \' +\\n                        \'observe on in listen mode.\')\\n    parser.add_argument(\'-l\', \'--listen\', action=\'store_true\',\\n                        help=\'don\\\\\'t contact any PTP servers, but \' +\\n                        \'report on any services currently observed \' +\\n                        \'on the network, instead.\')\\n    parser.add_argument(\'-d\', \'--delay\', metavar=\'SECS.TENTHS\', type=str,\\n                        help=\'Specifies the delay between screen \' +\\n                        \'updates when interactive. Can be changed while \' +\\n                        \'running using the \\\\\'d\\\\\' key. Negative \' +\\n                        \'numbers are not allowed. Setting this value \' +\\n                        \'to 0 is the same as issuing the \\\\\'-n 1\\\\\' \' +\\n                        \'option.\')\\n    parser.add_argument(\'-n\', \'--number\', metavar=\'COUNT\', type=int,\\n                        help=\'Specifies the maximum number of iterations \' +\\n                        \'in interactive mode before ending.\')\\n    parser.add_argument(\'-v\', \'--version\', action=\'version\',\\n                        version=\'%(prog)s \' + __version__)\\n\\n    args = parser.parse_args()\\n    try:\\n        c = Console(args)\\n\\n    except Exception as e:\\n        print type(e).__name__ + \\": \\" + str(e.message)\\n        exit(-1)\\n\\nif __name__ == \'__main__\':\\n    main()\\n"}\n'
line: b'{"repo_name":"achoy/cwapi","ref":"refs/heads/master","path":"backend/py-server/flask/lib/python3.6/site-packages/six.py","content":"\\"\\"\\"Utilities for writing code that runs on Python 2 and 3\\"\\"\\"\\n\\n# Copyright (c) 2010-2015 Benjamin Peterson\\n#\\n# Permission is hereby granted, free of charge, to any person obtaining a copy\\n# of this software and associated documentation files (the \\"Software\\"), to deal\\n# in the Software without restriction, including without limitation the rights\\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n# copies of the Software, and to permit persons to whom the Software is\\n# furnished to do so, subject to the following conditions:\\n#\\n# The above copyright notice and this permission notice shall be included in all\\n# copies or substantial portions of the Software.\\n#\\n# THE SOFTWARE IS PROVIDED \\"AS IS\\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n# SOFTWARE.\\n\\nfrom __future__ import absolute_import\\n\\nimport functools\\nimport itertools\\nimport operator\\nimport sys\\nimport types\\n\\n__author__ = \\"Benjamin Peterson \\u003cbenjamin@python.org\\u003e\\"\\n__version__ = \\"1.10.0\\"\\n\\n\\n# Useful for very coarse version differentiation.\\nPY2 = sys.version_info[0] == 2\\nPY3 = sys.version_info[0] == 3\\nPY34 = sys.version_info[0:2] \\u003e= (3, 4)\\n\\nif PY3:\\n    string_types = str,\\n    integer_types = int,\\n    class_types = type,\\n    text_type = str\\n    binary_type = bytes\\n\\n    MAXSIZE = sys.maxsize\\nelse:\\n    string_types = basestring,\\n    integer_types = (int, long)\\n    class_types = (type, types.ClassType)\\n    text_type = unicode\\n    binary_type = str\\n\\n    if sys.platform.startswith(\\"java\\"):\\n        # Jython always uses 32 bits.\\n        MAXSIZE = int((1 \\u003c\\u003c 31) - 1)\\n    else:\\n        # It\'s possible to have sizeof(long) != sizeof(Py_ssize_t).\\n        class X(object):\\n\\n            def __len__(self):\\n                return 1 \\u003c\\u003c 31\\n        try:\\n            len(X())\\n        except OverflowError:\\n            # 32-bit\\n            MAXSIZE = int((1 \\u003c\\u003c 31) - 1)\\n        else:\\n            # 64-bit\\n            MAXSIZE = int((1 \\u003c\\u003c 63) - 1)\\n        del X\\n\\n\\ndef _add_doc(func, doc):\\n    \\"\\"\\"Add documentation to a function.\\"\\"\\"\\n    func.__doc__ = doc\\n\\n\\ndef _import_module(name):\\n    \\"\\"\\"Import module, returning the module after the last dot.\\"\\"\\"\\n    __import__(name)\\n    return sys.modules[name]\\n\\n\\nclass _LazyDescr(object):\\n\\n    def __init__(self, name):\\n        self.name = name\\n\\n    def __get__(self, obj, tp):\\n        result = self._resolve()\\n        setattr(obj, self.name, result)  # Invokes __set__.\\n        try:\\n            # This is a bit ugly, but it avoids running this again by\\n            # removing this descriptor.\\n            delattr(obj.__class__, self.name)\\n        except AttributeError:\\n            pass\\n        return result\\n\\n\\nclass MovedModule(_LazyDescr):\\n\\n    def __init__(self, name, old, new=None):\\n        super(MovedModule, self).__init__(name)\\n        if PY3:\\n            if new is None:\\n                new = name\\n            self.mod = new\\n        else:\\n            self.mod = old\\n\\n    def _resolve(self):\\n        return _import_module(self.mod)\\n\\n    def __getattr__(self, attr):\\n        _module = self._resolve()\\n        value = getattr(_module, attr)\\n        setattr(self, attr, value)\\n        return value\\n\\n\\nclass _LazyModule(types.ModuleType):\\n\\n    def __init__(self, name):\\n        super(_LazyModule, self).__init__(name)\\n        self.__doc__ = self.__class__.__doc__\\n\\n    def __dir__(self):\\n        attrs = [\\"__doc__\\", \\"__name__\\"]\\n        attrs += [attr.name for attr in self._moved_attributes]\\n        return attrs\\n\\n    # Subclasses should override this\\n    _moved_attributes = []\\n\\n\\nclass MovedAttribute(_LazyDescr):\\n\\n    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):\\n        super(MovedAttribute, self).__init__(name)\\n        if PY3:\\n            if new_mod is None:\\n                new_mod = name\\n            self.mod = new_mod\\n            if new_attr is None:\\n                if old_attr is None:\\n                    new_attr = name\\n                else:\\n                    new_attr = old_attr\\n            self.attr = new_attr\\n        else:\\n            self.mod = old_mod\\n            if old_attr is None:\\n                old_attr = name\\n            self.attr = old_attr\\n\\n    def _resolve(self):\\n        module = _import_module(self.mod)\\n        return getattr(module, self.attr)\\n\\n\\nclass _SixMetaPathImporter(object):\\n\\n    \\"\\"\\"\\n    A meta path importer to import six.moves and its submodules.\\n\\n    This class implements a PEP302 finder and loader. It should be compatible\\n    with Python 2.5 and all existing versions of Python3\\n    \\"\\"\\"\\n\\n    def __init__(self, six_module_name):\\n        self.name = six_module_name\\n        self.known_modules = {}\\n\\n    def _add_module(self, mod, *fullnames):\\n        for fullname in fullnames:\\n            self.known_modules[self.name + \\".\\" + fullname] = mod\\n\\n    def _get_module(self, fullname):\\n        return self.known_modules[self.name + \\".\\" + fullname]\\n\\n    def find_module(self, fullname, path=None):\\n        if fullname in self.known_modules:\\n            return self\\n        return None\\n\\n    def __get_module(self, fullname):\\n        try:\\n            return self.known_modules[fullname]\\n        except KeyError:\\n            raise ImportError(\\"This loader does not know module \\" + fullname)\\n\\n    def load_module(self, fullname):\\n        try:\\n            # in case of a reload\\n            return sys.modules[fullname]\\n        except KeyError:\\n            pass\\n        mod = self.__get_module(fullname)\\n        if isinstance(mod, MovedModule):\\n            mod = mod._resolve()\\n        else:\\n            mod.__loader__ = self\\n        sys.modules[fullname] = mod\\n        return mod\\n\\n    def is_package(self, fullname):\\n        \\"\\"\\"\\n        Return true, if the named module is a package.\\n\\n        We need this method to get correct spec objects with\\n        Python 3.4 (see PEP451)\\n        \\"\\"\\"\\n        return hasattr(self.__get_module(fullname), \\"__path__\\")\\n\\n    def get_code(self, fullname):\\n        \\"\\"\\"Return None\\n\\n        Required, if is_package is implemented\\"\\"\\"\\n        self.__get_module(fullname)  # eventually raises ImportError\\n        return None\\n    get_source = get_code  # same as get_code\\n\\n_importer = _SixMetaPathImporter(__name__)\\n\\n\\nclass _MovedItems(_LazyModule):\\n\\n    \\"\\"\\"Lazy loading of moved objects\\"\\"\\"\\n    __path__ = []  # mark as package\\n\\n\\n_moved_attributes = [\\n    MovedAttribute(\\"cStringIO\\", \\"cStringIO\\", \\"io\\", \\"StringIO\\"),\\n    MovedAttribute(\\"filter\\", \\"itertools\\", \\"builtins\\", \\"ifilter\\", \\"filter\\"),\\n    MovedAttribute(\\"filterfalse\\", \\"itertools\\", \\"itertools\\", \\"ifilterfalse\\", \\"filterfalse\\"),\\n    MovedAttribute(\\"input\\", \\"__builtin__\\", \\"builtins\\", \\"raw_input\\", \\"input\\"),\\n    MovedAttribute(\\"intern\\", \\"__builtin__\\", \\"sys\\"),\\n    MovedAttribute(\\"map\\", \\"itertools\\", \\"builtins\\", \\"imap\\", \\"map\\"),\\n    MovedAttribute(\\"getcwd\\", \\"os\\", \\"os\\", \\"getcwdu\\", \\"getcwd\\"),\\n    MovedAttribute(\\"getcwdb\\", \\"os\\", \\"os\\", \\"getcwd\\", \\"getcwdb\\"),\\n    MovedAttribute(\\"range\\", \\"__builtin__\\", \\"builtins\\", \\"xrange\\", \\"range\\"),\\n    MovedAttribute(\\"reload_module\\", \\"__builtin__\\", \\"importlib\\" if PY34 else \\"imp\\", \\"reload\\"),\\n    MovedAttribute(\\"reduce\\", \\"__builtin__\\", \\"functools\\"),\\n    MovedAttribute(\\"shlex_quote\\", \\"pipes\\", \\"shlex\\", \\"quote\\"),\\n    MovedAttribute(\\"StringIO\\", \\"StringIO\\", \\"io\\"),\\n    MovedAttribute(\\"UserDict\\", \\"UserDict\\", \\"collections\\"),\\n    MovedAttribute(\\"UserList\\", \\"UserList\\", \\"collections\\"),\\n    MovedAttribute(\\"UserString\\", \\"UserString\\", \\"collections\\"),\\n    MovedAttribute(\\"xrange\\", \\"__builtin__\\", \\"builtins\\", \\"xrange\\", \\"range\\"),\\n    MovedAttribute(\\"zip\\", \\"itertools\\", \\"builtins\\", \\"izip\\", \\"zip\\"),\\n    MovedAttribute(\\"zip_longest\\", \\"itertools\\", \\"itertools\\", \\"izip_longest\\", \\"zip_longest\\"),\\n    MovedModule(\\"builtins\\", \\"__builtin__\\"),\\n    MovedModule(\\"configparser\\", \\"ConfigParser\\"),\\n    MovedModule(\\"copyreg\\", \\"copy_reg\\"),\\n    MovedModule(\\"dbm_gnu\\", \\"gdbm\\", \\"dbm.gnu\\"),\\n    MovedModule(\\"_dummy_thread\\", \\"dummy_thread\\", \\"_dummy_thread\\"),\\n    MovedModule(\\"http_cookiejar\\", \\"cookielib\\", \\"http.cookiejar\\"),\\n    MovedModule(\\"http_cookies\\", \\"Cookie\\", \\"http.cookies\\"),\\n    MovedModule(\\"html_entities\\", \\"htmlentitydefs\\", \\"html.entities\\"),\\n    MovedModule(\\"html_parser\\", \\"HTMLParser\\", \\"html.parser\\"),\\n    MovedModule(\\"http_client\\", \\"httplib\\", \\"http.client\\"),\\n    MovedModule(\\"email_mime_multipart\\", \\"email.MIMEMultipart\\", \\"email.mime.multipart\\"),\\n    MovedModule(\\"email_mime_nonmultipart\\", \\"email.MIMENonMultipart\\", \\"email.mime.nonmultipart\\"),\\n    MovedModule(\\"email_mime_text\\", \\"email.MIMEText\\", \\"email.mime.text\\"),\\n    MovedModule(\\"email_mime_base\\", \\"email.MIMEBase\\", \\"email.mime.base\\"),\\n    MovedModule(\\"BaseHTTPServer\\", \\"BaseHTTPServer\\", \\"http.server\\"),\\n    MovedModule(\\"CGIHTTPServer\\", \\"CGIHTTPServer\\", \\"http.server\\"),\\n    MovedModule(\\"SimpleHTTPServer\\", \\"SimpleHTTPServer\\", \\"http.server\\"),\\n    MovedModule(\\"cPickle\\", \\"cPickle\\", \\"pickle\\"),\\n    MovedModule(\\"queue\\", \\"Queue\\"),\\n    MovedModule(\\"reprlib\\", \\"repr\\"),\\n    MovedModule(\\"socketserver\\", \\"SocketServer\\"),\\n    MovedModule(\\"_thread\\", \\"thread\\", \\"_thread\\"),\\n    MovedModule(\\"tkinter\\", \\"Tkinter\\"),\\n    MovedModule(\\"tkinter_dialog\\", \\"Dialog\\", \\"tkinter.dialog\\"),\\n    MovedModule(\\"tkinter_filedialog\\", \\"FileDialog\\", \\"tkinter.filedialog\\"),\\n    MovedModule(\\"tkinter_scrolledtext\\", \\"ScrolledText\\", \\"tkinter.scrolledtext\\"),\\n    MovedModule(\\"tkinter_simpledialog\\", \\"SimpleDialog\\", \\"tkinter.simpledialog\\"),\\n    MovedModule(\\"tkinter_tix\\", \\"Tix\\", \\"tkinter.tix\\"),\\n    MovedModule(\\"tkinter_ttk\\", \\"ttk\\", \\"tkinter.ttk\\"),\\n    MovedModule(\\"tkinter_constants\\", \\"Tkconstants\\", \\"tkinter.constants\\"),\\n    MovedModule(\\"tkinter_dnd\\", \\"Tkdnd\\", \\"tkinter.dnd\\"),\\n    MovedModule(\\"tkinter_colorchooser\\", \\"tkColorChooser\\",\\n                \\"tkinter.colorchooser\\"),\\n    MovedModule(\\"tkinter_commondialog\\", \\"tkCommonDialog\\",\\n                \\"tkinter.commondialog\\"),\\n    MovedModule(\\"tkinter_tkfiledialog\\", \\"tkFileDialog\\", \\"tkinter.filedialog\\"),\\n    MovedModule(\\"tkinter_font\\", \\"tkFont\\", \\"tkinter.font\\"),\\n    MovedModule(\\"tkinter_messagebox\\", \\"tkMessageBox\\", \\"tkinter.messagebox\\"),\\n    MovedModule(\\"tkinter_tksimpledialog\\", \\"tkSimpleDialog\\",\\n                \\"tkinter.simpledialog\\"),\\n    MovedModule(\\"urllib_parse\\", __name__ + \\".moves.urllib_parse\\", \\"urllib.parse\\"),\\n    MovedModule(\\"urllib_error\\", __name__ + \\".moves.urllib_error\\", \\"urllib.error\\"),\\n    MovedModule(\\"urllib\\", __name__ + \\".moves.urllib\\", __name__ + \\".moves.urllib\\"),\\n    MovedModule(\\"urllib_robotparser\\", \\"robotparser\\", \\"urllib.robotparser\\"),\\n    MovedModule(\\"xmlrpc_client\\", \\"xmlrpclib\\", \\"xmlrpc.client\\"),\\n    MovedModule(\\"xmlrpc_server\\", \\"SimpleXMLRPCServer\\", \\"xmlrpc.server\\"),\\n]\\n# Add windows specific modules.\\nif sys.platform == \\"win32\\":\\n    _moved_attributes += [\\n        MovedModule(\\"winreg\\", \\"_winreg\\"),\\n    ]\\n\\nfor attr in _moved_attributes:\\n    setattr(_MovedItems, attr.name, attr)\\n    if isinstance(attr, MovedModule):\\n        _importer._add_module(attr, \\"moves.\\" + attr.name)\\ndel attr\\n\\n_MovedItems._moved_attributes = _moved_attributes\\n\\nmoves = _MovedItems(__name__ + \\".moves\\")\\n_importer._add_module(moves, \\"moves\\")\\n\\n\\nclass Module_six_moves_urllib_parse(_LazyModule):\\n\\n    \\"\\"\\"Lazy loading of moved objects in six.moves.urllib_parse\\"\\"\\"\\n\\n\\n_urllib_parse_moved_attributes = [\\n    MovedAttribute(\\"ParseResult\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"SplitResult\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"parse_qs\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"parse_qsl\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"urldefrag\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"urljoin\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"urlparse\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"urlsplit\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"urlunparse\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"urlunsplit\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"quote\\", \\"urllib\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"quote_plus\\", \\"urllib\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"unquote\\", \\"urllib\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"unquote_plus\\", \\"urllib\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"urlencode\\", \\"urllib\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"splitquery\\", \\"urllib\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"splittag\\", \\"urllib\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"splituser\\", \\"urllib\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"uses_fragment\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"uses_netloc\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"uses_params\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"uses_query\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"uses_relative\\", \\"urlparse\\", \\"urllib.parse\\"),\\n]\\nfor attr in _urllib_parse_moved_attributes:\\n    setattr(Module_six_moves_urllib_parse, attr.name, attr)\\ndel attr\\n\\nModule_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes\\n\\n_importer._add_module(Module_six_moves_urllib_parse(__name__ + \\".moves.urllib_parse\\"),\\n                      \\"moves.urllib_parse\\", \\"moves.urllib.parse\\")\\n\\n\\nclass Module_six_moves_urllib_error(_LazyModule):\\n\\n    \\"\\"\\"Lazy loading of moved objects in six.moves.urllib_error\\"\\"\\"\\n\\n\\n_urllib_error_moved_attributes = [\\n    MovedAttribute(\\"URLError\\", \\"urllib2\\", \\"urllib.error\\"),\\n    MovedAttribute(\\"HTTPError\\", \\"urllib2\\", \\"urllib.error\\"),\\n    MovedAttribute(\\"ContentTooShortError\\", \\"urllib\\", \\"urllib.error\\"),\\n]\\nfor attr in _urllib_error_moved_attributes:\\n    setattr(Module_six_moves_urllib_error, attr.name, attr)\\ndel attr\\n\\nModule_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes\\n\\n_importer._add_module(Module_six_moves_urllib_error(__name__ + \\".moves.urllib.error\\"),\\n                      \\"moves.urllib_error\\", \\"moves.urllib.error\\")\\n\\n\\nclass Module_six_moves_urllib_request(_LazyModule):\\n\\n    \\"\\"\\"Lazy loading of moved objects in six.moves.urllib_request\\"\\"\\"\\n\\n\\n_urllib_request_moved_attributes = [\\n    MovedAttribute(\\"urlopen\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"install_opener\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"build_opener\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"pathname2url\\", \\"urllib\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"url2pathname\\", \\"urllib\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"getproxies\\", \\"urllib\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"Request\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"OpenerDirector\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPDefaultErrorHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPRedirectHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPCookieProcessor\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"ProxyHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"BaseHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPPasswordMgr\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPPasswordMgrWithDefaultRealm\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"AbstractBasicAuthHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPBasicAuthHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"ProxyBasicAuthHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"AbstractDigestAuthHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPDigestAuthHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"ProxyDigestAuthHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPSHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"FileHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"FTPHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"CacheFTPHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"UnknownHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPErrorProcessor\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"urlretrieve\\", \\"urllib\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"urlcleanup\\", \\"urllib\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"URLopener\\", \\"urllib\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"FancyURLopener\\", \\"urllib\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"proxy_bypass\\", \\"urllib\\", \\"urllib.request\\"),\\n]\\nfor attr in _urllib_request_moved_attributes:\\n    setattr(Module_six_moves_urllib_request, attr.name, attr)\\ndel attr\\n\\nModule_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes\\n\\n_importer._add_module(Module_six_moves_urllib_request(__name__ + \\".moves.urllib.request\\"),\\n                      \\"moves.urllib_request\\", \\"moves.urllib.request\\")\\n\\n\\nclass Module_six_moves_urllib_response(_LazyModule):\\n\\n    \\"\\"\\"Lazy loading of moved objects in six.moves.urllib_response\\"\\"\\"\\n\\n\\n_urllib_response_moved_attributes = [\\n    MovedAttribute(\\"addbase\\", \\"urllib\\", \\"urllib.response\\"),\\n    MovedAttribute(\\"addclosehook\\", \\"urllib\\", \\"urllib.response\\"),\\n    MovedAttribute(\\"addinfo\\", \\"urllib\\", \\"urllib.response\\"),\\n    MovedAttribute(\\"addinfourl\\", \\"urllib\\", \\"urllib.response\\"),\\n]\\nfor attr in _urllib_response_moved_attributes:\\n    setattr(Module_six_moves_urllib_response, attr.name, attr)\\ndel attr\\n\\nModule_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes\\n\\n_importer._add_module(Module_six_moves_urllib_response(__name__ + \\".moves.urllib.response\\"),\\n                      \\"moves.urllib_response\\", \\"moves.urllib.response\\")\\n\\n\\nclass Module_six_moves_urllib_robotparser(_LazyModule):\\n\\n    \\"\\"\\"Lazy loading of moved objects in six.moves.urllib_robotparser\\"\\"\\"\\n\\n\\n_urllib_robotparser_moved_attributes = [\\n    MovedAttribute(\\"RobotFileParser\\", \\"robotparser\\", \\"urllib.robotparser\\"),\\n]\\nfor attr in _urllib_robotparser_moved_attributes:\\n    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\\ndel attr\\n\\nModule_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes\\n\\n_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + \\".moves.urllib.robotparser\\"),\\n                      \\"moves.urllib_robotparser\\", \\"moves.urllib.robotparser\\")\\n\\n\\nclass Module_six_moves_urllib(types.ModuleType):\\n\\n    \\"\\"\\"Create a six.moves.urllib namespace that resembles the Python 3 namespace\\"\\"\\"\\n    __path__ = []  # mark as package\\n    parse = _importer._get_module(\\"moves.urllib_parse\\")\\n    error = _importer._get_module(\\"moves.urllib_error\\")\\n    request = _importer._get_module(\\"moves.urllib_request\\")\\n    response = _importer._get_module(\\"moves.urllib_response\\")\\n    robotparser = _importer._get_module(\\"moves.urllib_robotparser\\")\\n\\n    def __dir__(self):\\n        return [\'parse\', \'error\', \'request\', \'response\', \'robotparser\']\\n\\n_importer._add_module(Module_six_moves_urllib(__name__ + \\".moves.urllib\\"),\\n                      \\"moves.urllib\\")\\n\\n\\ndef add_move(move):\\n    \\"\\"\\"Add an item to six.moves.\\"\\"\\"\\n    setattr(_MovedItems, move.name, move)\\n\\n\\ndef remove_move(name):\\n    \\"\\"\\"Remove item from six.moves.\\"\\"\\"\\n    try:\\n        delattr(_MovedItems, name)\\n    except AttributeError:\\n        try:\\n            del moves.__dict__[name]\\n        except KeyError:\\n            raise AttributeError(\\"no such move, %r\\" % (name,))\\n\\n\\nif PY3:\\n    _meth_func = \\"__func__\\"\\n    _meth_self = \\"__self__\\"\\n\\n    _func_closure = \\"__closure__\\"\\n    _func_code = \\"__code__\\"\\n    _func_defaults = \\"__defaults__\\"\\n    _func_globals = \\"__globals__\\"\\nelse:\\n    _meth_func = \\"im_func\\"\\n    _meth_self = \\"im_self\\"\\n\\n    _func_closure = \\"func_closure\\"\\n    _func_code = \\"func_code\\"\\n    _func_defaults = \\"func_defaults\\"\\n    _func_globals = \\"func_globals\\"\\n\\n\\ntry:\\n    advance_iterator = next\\nexcept NameError:\\n    def advance_iterator(it):\\n        return it.next()\\nnext = advance_iterator\\n\\n\\ntry:\\n    callable = callable\\nexcept NameError:\\n    def callable(obj):\\n        return any(\\"__call__\\" in klass.__dict__ for klass in type(obj).__mro__)\\n\\n\\nif PY3:\\n    def get_unbound_function(unbound):\\n        return unbound\\n\\n    create_bound_method = types.MethodType\\n\\n    def create_unbound_method(func, cls):\\n        return func\\n\\n    Iterator = object\\nelse:\\n    def get_unbound_function(unbound):\\n        return unbound.im_func\\n\\n    def create_bound_method(func, obj):\\n        return types.MethodType(func, obj, obj.__class__)\\n\\n    def create_unbound_method(func, cls):\\n        return types.MethodType(func, None, cls)\\n\\n    class Iterator(object):\\n\\n        def next(self):\\n            return type(self).__next__(self)\\n\\n    callable = callable\\n_add_doc(get_unbound_function,\\n         \\"\\"\\"Get the function out of a possibly unbound function\\"\\"\\")\\n\\n\\nget_method_function = operator.attrgetter(_meth_func)\\nget_method_self = operator.attrgetter(_meth_self)\\nget_function_closure = operator.attrgetter(_func_closure)\\nget_function_code = operator.attrgetter(_func_code)\\nget_function_defaults = operator.attrgetter(_func_defaults)\\nget_function_globals = operator.attrgetter(_func_globals)\\n\\n\\nif PY3:\\n    def iterkeys(d, **kw):\\n        return iter(d.keys(**kw))\\n\\n    def itervalues(d, **kw):\\n        return iter(d.values(**kw))\\n\\n    def iteritems(d, **kw):\\n        return iter(d.items(**kw))\\n\\n    def iterlists(d, **kw):\\n        return iter(d.lists(**kw))\\n\\n    viewkeys = operator.methodcaller(\\"keys\\")\\n\\n    viewvalues = operator.methodcaller(\\"values\\")\\n\\n    viewitems = operator.methodcaller(\\"items\\")\\nelse:\\n    def iterkeys(d, **kw):\\n        return d.iterkeys(**kw)\\n\\n    def itervalues(d, **kw):\\n        return d.itervalues(**kw)\\n\\n    def iteritems(d, **kw):\\n        return d.iteritems(**kw)\\n\\n    def iterlists(d, **kw):\\n        return d.iterlists(**kw)\\n\\n    viewkeys = operator.methodcaller(\\"viewkeys\\")\\n\\n    viewvalues = operator.methodcaller(\\"viewvalues\\")\\n\\n    viewitems = operator.methodcaller(\\"viewitems\\")\\n\\n_add_doc(iterkeys, \\"Return an iterator over the keys of a dictionary.\\")\\n_add_doc(itervalues, \\"Return an iterator over the values of a dictionary.\\")\\n_add_doc(iteritems,\\n         \\"Return an iterator over the (key, value) pairs of a dictionary.\\")\\n_add_doc(iterlists,\\n         \\"Return an iterator over the (key, [values]) pairs of a dictionary.\\")\\n\\n\\nif PY3:\\n    def b(s):\\n        return s.encode(\\"latin-1\\")\\n\\n    def u(s):\\n        return s\\n    unichr = chr\\n    import struct\\n    int2byte = struct.Struct(\\"\\u003eB\\").pack\\n    del struct\\n    byte2int = operator.itemgetter(0)\\n    indexbytes = operator.getitem\\n    iterbytes = iter\\n    import io\\n    StringIO = io.StringIO\\n    BytesIO = io.BytesIO\\n    _assertCountEqual = \\"assertCountEqual\\"\\n    if sys.version_info[1] \\u003c= 1:\\n        _assertRaisesRegex = \\"assertRaisesRegexp\\"\\n        _assertRegex = \\"assertRegexpMatches\\"\\n    else:\\n        _assertRaisesRegex = \\"assertRaisesRegex\\"\\n        _assertRegex = \\"assertRegex\\"\\nelse:\\n    def b(s):\\n        return s\\n    # Workaround for standalone backslash\\n\\n    def u(s):\\n        return unicode(s.replace(r\'\\\\\\\\\', r\'\\\\\\\\\\\\\\\\\'), \\"unicode_escape\\")\\n    unichr = unichr\\n    int2byte = chr\\n\\n    def byte2int(bs):\\n        return ord(bs[0])\\n\\n    def indexbytes(buf, i):\\n        return ord(buf[i])\\n    iterbytes = functools.partial(itertools.imap, ord)\\n    import StringIO\\n    StringIO = BytesIO = StringIO.StringIO\\n    _assertCountEqual = \\"assertItemsEqual\\"\\n    _assertRaisesRegex = \\"assertRaisesRegexp\\"\\n    _assertRegex = \\"assertRegexpMatches\\"\\n_add_doc(b, \\"\\"\\"Byte literal\\"\\"\\")\\n_add_doc(u, \\"\\"\\"Text literal\\"\\"\\")\\n\\n\\ndef assertCountEqual(self, *args, **kwargs):\\n    return getattr(self, _assertCountEqual)(*args, **kwargs)\\n\\n\\ndef assertRaisesRegex(self, *args, **kwargs):\\n    return getattr(self, _assertRaisesRegex)(*args, **kwargs)\\n\\n\\ndef assertRegex(self, *args, **kwargs):\\n    return getattr(self, _assertRegex)(*args, **kwargs)\\n\\n\\nif PY3:\\n    exec_ = getattr(moves.builtins, \\"exec\\")\\n\\n    def reraise(tp, value, tb=None):\\n        if value is None:\\n            value = tp()\\n        if value.__traceback__ is not tb:\\n            raise value.with_traceback(tb)\\n        raise value\\n\\nelse:\\n    def exec_(_code_, _globs_=None, _locs_=None):\\n        \\"\\"\\"Execute code in a namespace.\\"\\"\\"\\n        if _globs_ is None:\\n            frame = sys._getframe(1)\\n            _globs_ = frame.f_globals\\n            if _locs_ is None:\\n                _locs_ = frame.f_locals\\n            del frame\\n        elif _locs_ is None:\\n            _locs_ = _globs_\\n        exec(\\"\\"\\"exec _code_ in _globs_, _locs_\\"\\"\\")\\n\\n    exec_(\\"\\"\\"def reraise(tp, value, tb=None):\\n    raise tp, value, tb\\n\\"\\"\\")\\n\\n\\nif sys.version_info[:2] == (3, 2):\\n    exec_(\\"\\"\\"def raise_from(value, from_value):\\n    if from_value is None:\\n        raise value\\n    raise value from from_value\\n\\"\\"\\")\\nelif sys.version_info[:2] \\u003e (3, 2):\\n    exec_(\\"\\"\\"def raise_from(value, from_value):\\n    raise value from from_value\\n\\"\\"\\")\\nelse:\\n    def raise_from(value, from_value):\\n        raise value\\n\\n\\nprint_ = getattr(moves.builtins, \\"print\\", None)\\nif print_ is None:\\n    def print_(*args, **kwargs):\\n        \\"\\"\\"The new-style print function for Python 2.4 and 2.5.\\"\\"\\"\\n        fp = kwargs.pop(\\"file\\", sys.stdout)\\n        if fp is None:\\n            return\\n\\n        def write(data):\\n            if not isinstance(data, basestring):\\n                data = str(data)\\n            # If the file has an encoding, encode unicode with it.\\n            if (isinstance(fp, file) and\\n                    isinstance(data, unicode) and\\n                    fp.encoding is not None):\\n                errors = getattr(fp, \\"errors\\", None)\\n                if errors is None:\\n                    errors = \\"strict\\"\\n                data = data.encode(fp.encoding, errors)\\n            fp.write(data)\\n        want_unicode = False\\n        sep = kwargs.pop(\\"sep\\", None)\\n        if sep is not None:\\n            if isinstance(sep, unicode):\\n                want_unicode = True\\n            elif not isinstance(sep, str):\\n                raise TypeError(\\"sep must be None or a string\\")\\n        end = kwargs.pop(\\"end\\", None)\\n        if end is not None:\\n            if isinstance(end, unicode):\\n                want_unicode = True\\n            elif not isinstance(end, str):\\n                raise TypeError(\\"end must be None or a string\\")\\n        if kwargs:\\n            raise TypeError(\\"invalid keyword arguments to print()\\")\\n        if not want_unicode:\\n            for arg in args:\\n                if isinstance(arg, unicode):\\n                    want_unicode = True\\n                    break\\n        if want_unicode:\\n            newline = unicode(\\"\\\\n\\")\\n            space = unicode(\\" \\")\\n        else:\\n            newline = \\"\\\\n\\"\\n            space = \\" \\"\\n        if sep is None:\\n            sep = space\\n        if end is None:\\n            end = newline\\n        for i, arg in enumerate(args):\\n            if i:\\n                write(sep)\\n            write(arg)\\n        write(end)\\nif sys.version_info[:2] \\u003c (3, 3):\\n    _print = print_\\n\\n    def print_(*args, **kwargs):\\n        fp = kwargs.get(\\"file\\", sys.stdout)\\n        flush = kwargs.pop(\\"flush\\", False)\\n        _print(*args, **kwargs)\\n        if flush and fp is not None:\\n            fp.flush()\\n\\n_add_doc(reraise, \\"\\"\\"Reraise an exception.\\"\\"\\")\\n\\nif sys.version_info[0:2] \\u003c (3, 4):\\n    def wraps(wrapped, assigned=functools.WRAPPER_ASSIGNMENTS,\\n              updated=functools.WRAPPER_UPDATES):\\n        def wrapper(f):\\n            f = functools.wraps(wrapped, assigned, updated)(f)\\n            f.__wrapped__ = wrapped\\n            return f\\n        return wrapper\\nelse:\\n    wraps = functools.wraps\\n\\n\\ndef with_metaclass(meta, *bases):\\n    \\"\\"\\"Create a base class with a metaclass.\\"\\"\\"\\n    # This requires a bit of explanation: the basic idea is to make a dummy\\n    # metaclass for one level of class instantiation that replaces itself with\\n    # the actual metaclass.\\n    class metaclass(meta):\\n\\n        def __new__(cls, name, this_bases, d):\\n            return meta(name, bases, d)\\n    return type.__new__(metaclass, \'temporary_class\', (), {})\\n\\n\\ndef add_metaclass(metaclass):\\n    \\"\\"\\"Class decorator for creating a class with a metaclass.\\"\\"\\"\\n    def wrapper(cls):\\n        orig_vars = cls.__dict__.copy()\\n        slots = orig_vars.get(\'__slots__\')\\n        if slots is not None:\\n            if isinstance(slots, str):\\n                slots = [slots]\\n            for slots_var in slots:\\n                orig_vars.pop(slots_var)\\n        orig_vars.pop(\'__dict__\', None)\\n        orig_vars.pop(\'__weakref__\', None)\\n        return metaclass(cls.__name__, cls.__bases__, orig_vars)\\n    return wrapper\\n\\n\\ndef python_2_unicode_compatible(klass):\\n    \\"\\"\\"\\n    A decorator that defines __unicode__ and __str__ methods under Python 2.\\n    Under Python 3 it does nothing.\\n\\n    To support Python 2 and 3 with a single code base, define a __str__ method\\n    returning text and apply this decorator to the class.\\n    \\"\\"\\"\\n    if PY2:\\n        if \'__str__\' not in klass.__dict__:\\n            raise ValueError(\\"@python_2_unicode_compatible cannot be applied \\"\\n                             \\"to %s because it doesn\'t define __str__().\\" %\\n                             klass.__name__)\\n        klass.__unicode__ = klass.__str__\\n        klass.__str__ = lambda self: self.__unicode__().encode(\'utf-8\')\\n    return klass\\n\\n\\n# Complete the moves implementation.\\n# This code is at the end of this module to speed up module loading.\\n# Turn this module into a package.\\n__path__ = []  # required for PEP 302 and PEP 451\\n__package__ = __name__  # see PEP 366 @ReservedAssignment\\nif globals().get(\\"__spec__\\") is not None:\\n    __spec__.submodule_search_locations = []  # PEP 451 @UndefinedVariable\\n# Remove other six meta path importers, since they cause problems. This can\\n# happen if six is removed from sys.modules and then reloaded. (Setuptools does\\n# this for some reason.)\\nif sys.meta_path:\\n    for i, importer in enumerate(sys.meta_path):\\n        # Here\'s some real nastiness: Another \\"instance\\" of the six module might\\n        # be floating around. Therefore, we can\'t use isinstance() to check for\\n        # the six meta path importer, since the other six instance will have\\n        # inserted an importer with different class.\\n        if (type(importer).__name__ == \\"_SixMetaPathImporter\\" and\\n                importer.name == __name__):\\n            del sys.meta_path[i]\\n            break\\n    del i, importer\\n# Finally, add the importer to the meta path import hook.\\nsys.meta_path.append(_importer)\\n"}\n'
line: b'{"repo_name":"sbstp/streamlink","ref":"refs/heads/master","path":"src/streamlink/plugins/alieztv.py","content":"import re\\n\\nfrom os.path import splitext\\n\\nfrom streamlink.compat import urlparse, unquote\\nfrom streamlink.plugin import Plugin\\nfrom streamlink.plugin.api import http, validate\\nfrom streamlink.stream import HTTPStream, RTMPStream\\n\\n_url_re = re.compile(\\"\\"\\"\\n    http(s)?://(\\\\w+\\\\.)?aliez.tv\\n    (?:\\n        /live/[^/]+\\n    )?\\n    (?:\\n        /video/\\\\d+/[^/]+\\n    )?\\n\\"\\"\\", re.VERBOSE)\\n_file_re = re.compile(\\"\\\\\\"?file\\\\\\"?:\\\\s+[\'\\\\\\"]([^\'\\\\\\"]+)[\'\\\\\\"]\\")\\n_swf_url_re = re.compile(\\"swfobject.embedSWF\\\\(\\\\\\"([^\\\\\\"]+)\\\\\\",\\")\\n\\n_schema = validate.Schema(\\n    validate.union({\\n        \\"urls\\": validate.all(\\n            validate.transform(_file_re.findall),\\n            validate.map(unquote),\\n            [validate.url()]\\n        ),\\n        \\"swf\\": validate.all(\\n            validate.transform(_swf_url_re.search),\\n            validate.any(\\n                None,\\n                validate.all(\\n                    validate.get(1),\\n                    validate.url(\\n                        scheme=\\"http\\",\\n                        path=validate.endswith(\\"swf\\")\\n                    )\\n                )\\n            )\\n        )\\n    })\\n)\\n\\n\\nclass Aliez(Plugin):\\n    @classmethod\\n    def can_handle_url(self, url):\\n        return _url_re.match(url)\\n\\n    def _get_streams(self):\\n        res = http.get(self.url, schema=_schema)\\n        streams = {}\\n        for url in res[\\"urls\\"]:\\n            parsed = urlparse(url)\\n            if parsed.scheme.startswith(\\"rtmp\\"):\\n                params = {\\n                    \\"rtmp\\": url,\\n                    \\"pageUrl\\": self.url,\\n                    \\"live\\": True\\n                }\\n                if res[\\"swf\\"]:\\n                    params[\\"swfVfy\\"] = res[\\"swf\\"]\\n\\n                stream = RTMPStream(self.session, params)\\n                streams[\\"live\\"] = stream\\n            elif parsed.scheme.startswith(\\"http\\"):\\n                name = splitext(parsed.path)[1][1:]\\n                stream = HTTPStream(self.session, url)\\n                streams[name] = stream\\n\\n        return streams\\n\\n__plugin__ = Aliez\\n"}\n'
line: b'{"repo_name":"astronaut1712/taiga-back","ref":"refs/heads/master","path":"taiga/projects/wiki/models.py","content":"# Copyright (C) 2014 Andrey Antukh \\u003cniwi@niwi.be\\u003e\\n# Copyright (C) 2014 Jes\xc3\xbas Espino \\u003cjespinog@gmail.com\\u003e\\n# Copyright (C) 2014 David Barrag\xc3\xa1n \\u003cbameda@dbarragan.com\\u003e\\n# This program is free software: you can redistribute it and/or modify\\n# it under the terms of the GNU Affero General Public License as\\n# published by the Free Software Foundation, either version 3 of the\\n# License, or (at your option) any later version.\\n#\\n# This program is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU Affero General Public License for more details.\\n#\\n# You should have received a copy of the GNU Affero General Public License\\n# along with this program.  If not, see \\u003chttp://www.gnu.org/licenses/\\u003e.\\n\\nfrom django.db import models\\nfrom django.contrib.contenttypes import generic\\nfrom django.conf import settings\\nfrom django.utils.translation import ugettext_lazy as _\\nfrom django.utils import timezone\\nfrom taiga.projects.notifications.mixins import WatchedModelMixin\\nfrom taiga.projects.occ import OCCModelMixin\\n\\n\\nclass WikiPage(OCCModelMixin, WatchedModelMixin, models.Model):\\n    project = models.ForeignKey(\\"projects.Project\\", null=False, blank=False,\\n                                related_name=\\"wiki_pages\\", verbose_name=_(\\"project\\"))\\n    slug = models.SlugField(max_length=500, db_index=True, null=False, blank=False,\\n                            verbose_name=_(\\"slug\\"))\\n    content = models.TextField(null=False, blank=True,\\n                               verbose_name=_(\\"content\\"))\\n    owner = models.ForeignKey(settings.AUTH_USER_MODEL, null=True, blank=True,\\n                              related_name=\\"owned_wiki_pages\\", verbose_name=_(\\"owner\\"))\\n    last_modifier = models.ForeignKey(settings.AUTH_USER_MODEL, null=True, blank=True,\\n                              related_name=\\"last_modified_wiki_pages\\", verbose_name=_(\\"last modifier\\"))\\n    created_date = models.DateTimeField(null=False, blank=False,\\n                                        verbose_name=_(\\"created date\\"),\\n                                        default=timezone.now)\\n    modified_date = models.DateTimeField(null=False, blank=False,\\n                                         verbose_name=_(\\"modified date\\"))\\n    attachments = generic.GenericRelation(\\"attachments.Attachment\\")\\n    _importing = None\\n\\n    class Meta:\\n        verbose_name = \\"wiki page\\"\\n        verbose_name_plural = \\"wiki pages\\"\\n        ordering = [\\"project\\", \\"slug\\"]\\n        unique_together = (\\"project\\", \\"slug\\",)\\n        permissions = (\\n            (\\"view_wikipage\\", \\"Can view wiki page\\"),\\n        )\\n\\n    def __str__(self):\\n        return \\"project {0} - {1}\\".format(self.project_id, self.slug)\\n\\n    def save(self, *args, **kwargs):\\n        if not self._importing or not self.modified_date:\\n            self.modified_date = timezone.now()\\n\\n        return super().save(*args, **kwargs)\\n\\n\\nclass WikiLink(models.Model):\\n    project = models.ForeignKey(\\"projects.Project\\", null=False, blank=False,\\n                                related_name=\\"wiki_links\\", verbose_name=_(\\"project\\"))\\n    title = models.CharField(max_length=500, null=False, blank=False)\\n    href = models.SlugField(max_length=500, db_index=True, null=False, blank=False,\\n                            verbose_name=_(\\"href\\"))\\n    order = models.PositiveSmallIntegerField(default=1, null=False, blank=False,\\n                                             verbose_name=_(\\"order\\"))\\n\\n    class Meta:\\n        verbose_name = \\"wiki link\\"\\n        verbose_name_plural = \\"wiki links\\"\\n        ordering = [\\"project\\", \\"order\\"]\\n        unique_together = (\\"project\\", \\"href\\")\\n\\n    def __str__(self):\\n        return self.title\\n"}\n'
line: b'{"repo_name":"helenst/django","ref":"refs/heads/master","path":"django/contrib/gis/db/backends/mysql/introspection.py","content":"from MySQLdb.constants import FIELD_TYPE\\n\\nfrom django.contrib.gis.gdal import OGRGeomType\\nfrom django.db.backends.mysql.introspection import DatabaseIntrospection\\n\\n\\nclass MySQLIntrospection(DatabaseIntrospection):\\n    # Updating the data_types_reverse dictionary with the appropriate\\n    # type for Geometry fields.\\n    data_types_reverse = DatabaseIntrospection.data_types_reverse.copy()\\n    data_types_reverse[FIELD_TYPE.GEOMETRY] = \'GeometryField\'\\n\\n    def get_geometry_type(self, table_name, geo_col):\\n        cursor = self.connection.cursor()\\n        try:\\n            # In order to get the specific geometry type of the field,\\n            # we introspect on the table definition using `DESCRIBE`.\\n            cursor.execute(\'DESCRIBE %s\' %\\n                           self.connection.ops.quote_name(table_name))\\n            # Increment over description info until we get to the geometry\\n            # column.\\n            for column, typ, null, key, default, extra in cursor.fetchall():\\n                if column == geo_col:\\n                    # Using OGRGeomType to convert from OGC name to Django field.\\n                    # MySQL does not support 3D or SRIDs, so the field params\\n                    # are empty.\\n                    field_type = OGRGeomType(typ).django\\n                    field_params = {}\\n                    break\\n        finally:\\n            cursor.close()\\n\\n        return field_type, field_params\\n\\n    def supports_spatial_index(self, cursor, table_name):\\n        # Supported with MyISAM, or InnoDB on MySQL 5.7.5+\\n        storage_engine = self.get_storage_engine(cursor, table_name)\\n        return (\\n            (storage_engine == \'InnoDB\' and self.connection.mysql_version \\u003e= (5, 7, 5)) or\\n            storage_engine == \'MyISAM\'\\n        )\\n"}\n'
line: b'{"repo_name":"PaulWay/insights-core","ref":"refs/heads/master","path":"insights/parsers/tests/test_foreman_log.py","content":"from insights.tests import context_wrap\\nfrom insights.parsers.foreman_log import SatelliteLog, ProductionLog\\nfrom insights.parsers.foreman_log import CandlepinLog, ProxyLog\\n\\n\\nPRODUCTION_LOG = \\"\\"\\"\\n2015-11-13 03:30:07 [I] Completed 200 OK in 1783ms (Views: 0.2ms | ActiveRecord: 172.9ms)\\n2015-11-13 03:30:07 [I] Processing by Katello::Api::V2::RepositoriesController#sync_complete as JSON\\n2015-11-13 03:30:07 [I]   Parameters: {\\"call_report\\"=\\u003e\\"[FILTERED]\\", \\"event_type\\"=\\u003e\\"repo.sync.finish\\", \\"payload\\"=\\u003e{\\"importer_id\\"=\\u003e\\"yum_importer\\", \\"exception\\"=\\u003enil, \\"repo_id\\"=\\u003e\\"1-Gulfstream_Aerospace_Corp_-Red_Hat_Enterprise_Linux_Server-Red_Hat_Satellite_Tools_6_1_for_RHEL_6_Server_RPMs_i386\\", \\"traceback\\"=\\u003enil, \\"started\\"=\\u003e\\"2015-11-13T08:30:00Z\\", \\"_ns\\"=\\u003e\\"repo_sync_results\\", \\"completed\\"=\\u003e\\"2015-11-13T08:30:06Z\\", \\"importer_type_id\\"=\\u003e\\"yum_importer\\", \\"error_message\\"=\\u003enil, \\"summary\\"=\\u003e{\\"content\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"comps\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"distribution\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"errata\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"metadata\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}}, \\"added_count\\"=\\u003e0, \\"result\\"=\\u003e\\"success\\", \\"updated_count\\"=\\u003e3, \\"details\\"=\\u003e{\\"content\\"=\\u003e{\\"size_total\\"=\\u003e0, \\"items_left\\"=\\u003e0, \\"items_total\\"=\\u003e0, \\"state\\"=\\u003e\\"FINISHED\\", \\"size_left\\"=\\u003e0, \\"details\\"=\\u003e{\\"rpm_total\\"=\\u003e0, \\"rpm_done\\"=\\u003e0, \\"drpm_total\\"=\\u003e0, \\"drpm_done\\"=\\u003e0}, \\"error_details\\"=\\u003e[]}, \\"comps\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"distribution\\"=\\u003e{\\"items_total\\"=\\u003e0, \\"state\\"=\\u003e\\"FINISHED\\", \\"error_details\\"=\\u003e[], \\"items_left\\"=\\u003e0}, \\"errata\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"metadata\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}}, \\"id\\"=\\u003e\\"56459f8ef301a213bbfd87bb\\", \\"removed_count\\"=\\u003e0}, \\"token\\"=\\u003e\\"oQumn3XsKrdRkijuvpCNhKF2PDWZt6az\\", \\"api_version\\"=\\u003e\\"v2\\", \\"repository\\"=\\u003e{}}\\n2015-11-13 03:30:07 [I] Sync_complete called for Red Hat Satellite Tools 6.1 for RHEL 6 Server RPMs i386, running after_sync.\\n2015-11-13 03:30:09 [I] Completed 200 OK in 1995ms (Views: 0.2ms | ActiveRecord: 81.5ms)\\n2015-11-13 03:30:10 [I] Processing by Katello::Api::V2::RepositoriesController#sync_complete as JSON\\n2015-11-13 03:30:10 [I]   Parameters: {\\"call_report\\"=\\u003e\\"[FILTERED]\\", \\"event_type\\"=\\u003e\\"repo.sync.finish\\", \\"payload\\"=\\u003e{\\"importer_id\\"=\\u003e\\"yum_importer\\", \\"exception\\"=\\u003enil, \\"repo_id\\"=\\u003e\\"1-Gulfstream_Aerospace_Corp_-Red_Hat_Enterprise_Linux_Server-Red_Hat_Satellite_Tools_6_1_for_RHEL_5_Server_RPMs_i386\\", \\"traceback\\"=\\u003enil, \\"started\\"=\\u003e\\"2015-11-13T08:30:05Z\\", \\"_ns\\"=\\u003e\\"repo_sync_results\\", \\"completed\\"=\\u003e\\"2015-11-13T08:30:10Z\\", \\"importer_type_id\\"=\\u003e\\"yum_importer\\", \\"error_message\\"=\\u003enil, \\"summary\\"=\\u003e{\\"content\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"comps\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"distribution\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"errata\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"metadata\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}}, \\"added_count\\"=\\u003e0, \\"result\\"=\\u003e\\"success\\", \\"updated_count\\"=\\u003e3, \\"details\\"=\\u003e{\\"content\\"=\\u003e{\\"size_total\\"=\\u003e0, \\"items_left\\"=\\u003e0, \\"items_total\\"=\\u003e0, \\"state\\"=\\u003e\\"FINISHED\\", \\"size_left\\"=\\u003e0, \\"details\\"=\\u003e{\\"rpm_total\\"=\\u003e0, \\"rpm_done\\"=\\u003e0, \\"drpm_total\\"=\\u003e0, \\"drpm_done\\"=\\u003e0}, \\"error_details\\"=\\u003e[]}, \\"comps\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"distribution\\"=\\u003e{\\"items_total\\"=\\u003e0, \\"state\\"=\\u003e\\"FINISHED\\", \\"error_details\\"=\\u003e[], \\"items_left\\"=\\u003e0}, \\"errata\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"metadata\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}}, \\"id\\"=\\u003e\\"56459f92f301a2137cd6b802\\", \\"removed_count\\"=\\u003e0}, \\"token\\"=\\u003e\\"oQumn3XsKrdRkijuvpCNhKF2PDWZt6az\\", \\"api_version\\"=\\u003e\\"v2\\", \\"repository\\"=\\u003e{}}\\n2015-11-13 03:30:10 [I] Sync_complete called for Red Hat Satellite Tools 6.1 for RHEL 5 Server RPMs i386, running after_sync.\\n2015-11-13 03:30:11 [I] Connecting to database specified by database.yml\\n2015-11-13 03:30:11 [I] Connecting to database specified by database.yml\\n2015-11-13 03:30:11 [I] Completed 200 OK in 818ms (Views: 0.2ms | ActiveRecord: 77.2ms)\\n2015-11-13 03:30:17 [I] Connecting to database specified by database.yml\\n2015-11-13 03:30:26 [I] Sync_complete called for RHN Tools for Red Hat Enterprise Linux 5 Server RPMs x86_64 5Server, running after_sync.\\n2015-11-13 03:50:46 [I] Completed 200 OK in 2583ms (Views: 2.7ms | ActiveRecord: 0.3ms)\\n2015-11-13 06:58:25 [I]   Parameters: {\\"id\\"=\\u003e\\"cfd7275b-8cce-4323-8d1f-55ef85eca883\\"}\\n2015-11-13 06:58:25 [I] Completed 200 OK in 249ms (Views: 3.1ms | ActiveRecord: 0.3ms)\\n2015-11-13 06:59:26 [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#consumer_show as JSON\\n2015-11-13 06:59:26 [I]   Parameters: {\\"id\\"=\\u003e\\"cfd7275b-8cce-4323-8d1f-55ef85eca883\\"}\\n2015-11-13 06:59:26 [I] Completed 200 OK in 84ms (Views: 3.1ms | ActiveRecord: 0.3ms)\\n2015-11-13 07:00:12 [I] Connecting to database specified by database.yml\\n2015-11-13 07:00:12 [I] Connecting to database specified by database.yml\\n2015-11-13 07:00:12 [I] Connecting to database specified by database.yml\\n2015-11-13 07:00:18 [W] Creating scope :completer_scope. Overwriting existing method Organization.completer_scope.\\n2015-11-13 07:00:18 [W] Creating scope :completer_scope. Overwriting existing method Organization.completer_scope.\\n2015-11-13 07:00:18 [W] Creating scope :completer_scope. Overwriting existing method Location.completer_scope.\\n2015-11-13 07:00:18 [W] Creating scope :completer_scope. Overwriting existing method Location.completer_scope.\\n2015-11-13 07:00:18 [W] Creating scope :completer_scope. Overwriting existing method Organization.completer_scope.\\n2015-11-13 07:09:22 [I]   Parameters: {\\"facts\\"=\\u003e\\"[FILTERED]\\", \\"name\\"=\\u003e\\"infrhnpl002.gac.gulfaero.com\\", \\"certname\\"=\\u003e\\"infrhnpl002.gac.gulfaero.com\\", \\"apiv\\"=\\u003e\\"v2\\", :host=\\u003e{\\"name\\"=\\u003e\\"infrhnpl002.gac.gulfaero.com\\", \\"certname\\"=\\u003e\\"infrhnpl002.gac.gulfaero.com\\"}}\\n2015-11-13 07:09:22 [I] Import facts for \'infrhnpl002.gac.gulfaero.com\' completed. Added: 0, Updated: 6, Deleted 0 facts\\n2015-11-13 07:09:22 [I] Completed 201 Created in 251ms (Views: 179.3ms | ActiveRecord: 0.0ms)\\n2015-11-13 07:09:22 [I] Processing by HostsController#externalNodes as YML\\n2015-11-13 07:09:22 [I]   Parameters: {\\"name\\"=\\u003e\\"infrhnpl002.gac.gulfaero.com\\"}\\n2015-11-13 07:09:22 [I]   Rendered text template (0.0ms)\\n2015-11-13 07:09:22 [I] Completed 200 OK in 48ms (Views: 0.5ms | ActiveRecord: 6.6ms)\\n2015-11-13 07:09:22 [I] Processing by Api::V2::ReportsController#create as JSON\\n2015-11-13 07:09:22 [I]   Parameters: {\\"report\\"=\\u003e\\"[FILTERED]\\", \\"apiv\\"=\\u003e\\"v2\\"}\\n2015-11-13 07:09:22 [I]   Rendered text template (0.0ms)\\n2015-11-13 07:09:22 [I] processing report for infrhnpl002.gac.gulfaero.com\\n2015-11-13 07:09:22 [I] Imported report for infrhnpl002.gac.gulfaero.com in 0.02 seconds\\n2015-11-13 07:09:22 [I] Completed 201 Created in 28ms (Views: 1.2ms | ActiveRecord: 0.0ms)\\n2015-11-13 07:30:17 [W] Creating scope :completer_scope. Overwriting existing method Organization.completer_scope.\\n2015-11-13 07:30:18 [W] Creating scope :completer_scope. Overwriting existing method Location.completer_scope.\\n2015-11-13 07:30:18 [W] Creating scope :completer_scope. Overwriting existing method Location.completer_scope.\\n2015-11-13 07:30:18 [W] Creating scope :completer_scope. Overwriting existing method Location.completer_scope.\\n2015-11-13 07:30:25 [I] Client connected.\\n2015-11-13 07:30:25 [I] Connected to server.\\n2015-11-13 07:30:25 [I] Client connected.\\n2015-11-13 07:30:25 [I] Connected to server.\\n2015-11-13 07:30:25 [I] Client connected.\\n2015-11-13 07:30:25 [I] Connected to server.\\n2015-11-13 07:30:30 [I] init config for SecureHeaders::Configuration\\n2015-11-13 07:30:30 [I] init config for SecureHeaders::Configuration\\n2015-11-13 07:30:30 [I] init config for SecureHeaders::Configuration\\n2015-11-13 07:30:32 [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#consumer_show as JSON\\n2015-11-13 07:30:32 [I]   Parameters: {\\"id\\"=\\u003e\\"cfd7275b-8cce-4323-8d1f-55ef85eca883\\"}\\n2015-11-13 07:30:32 [I] Completed 200 OK in 110ms (Views: 2.7ms | ActiveRecord: 0.3ms)\\n2015-11-13 07:30:33 [I] 2015-11-13 07:30:33 -0500: Expired 48 Reports\\n2015-11-13 07:30:33 [I] Client disconnected.\\n2015-11-13 09:41:58 [I] Completed 200 OK in 93ms (Views: 2.9ms | ActiveRecord: 0.3ms)\\n2015-11-13 09:42:58 [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#consumer_show as JSON\\n2015-11-13 09:42:58 [I]   Parameters: {\\"id\\"=\\u003e\\"cfd7275b-8cce-4323-8d1f-55ef85eca883\\"}\\n2015-11-13 09:42:58 [I] Completed 200 OK in 80ms (Views: 3.6ms | ActiveRecord: 0.3ms)\\n2015-11-13 09:43:58 [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#consumer_show as JSON\\n2015-11-13 09:43:58 [I]   Parameters: {\\"id\\"=\\u003e\\"cfd7275b-8cce-4323-8d1f-55ef85eca883\\"}\\n2015-11-13 09:43:59 [I] Completed 200 OK in 80ms (Views: 2.9ms | ActiveRecord: 0.3ms)\\n\\"\\"\\".strip()\\n\\n\\nSATELLITE_OUT = \\"\\"\\"\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config::Pulp_client/Foreman_config_entry[pulp_client_cert]/require: requires Class[Certs::Pulp_client]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config::Pulp_client/Foreman_config_entry[pulp_client_cert]/require: requires Exec[foreman-rake-db:seed]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config::Pulp_client/Foreman_config_entry[pulp_client_key]/require: requires Class[Certs::Pulp_client]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config::Pulp_client/Foreman_config_entry[pulp_client_key]/require: requires Exec[foreman-rake-db:seed]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config/File[/etc/foreman/plugins/katello.yaml]/before: requires Class[Foreman::Database]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config/File[/etc/foreman/plugins/katello.yaml]/before: requires Exec[foreman-rake-db:migrate]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config/File[/etc/foreman/plugins/katello.yaml]/notify: subscribes to Service[foreman-tasks]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config/File[/etc/foreman/plugins/katello.yaml]/notify: subscribes to Class[Foreman::Service]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config/Foreman::Config::Passenger::Fragment[katello]/require: requires Class[Foreman::Config::Passenger]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/notify: subscribes to Class[Certs::Candlepin]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Cert[kam1opapp999.connex.bclc.com-qpid-broker]/notify: subscribes to Pubkey[/etc/pki/katello/certs/kam1opapp999.connex.bclc.com-qpid-broker.crt]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Pubkey[/etc/pki/katello/certs/kam1opapp999.connex.bclc.com-qpid-broker.crt]/notify: subscribes to Privkey[/etc/pki/katello/private/kam1opapp999.connex.bclc.com-qpid-broker.key]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Privkey[/etc/pki/katello/private/kam1opapp999.connex.bclc.com-qpid-broker.key]/notify: subscribes to File[/etc/pki/katello/private/kam1opapp999.connex.bclc.com-qpid-broker.key]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/File[/etc/pki/katello/private/kam1opapp999.connex.bclc.com-qpid-broker.key]/notify: subscribes to File[/etc/pki/katello/nssdb]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/File[/etc/pki/katello/nssdb]/notify: subscribes to Exec[generate-nss-password]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[generate-nss-password]/before: requires File[/etc/pki/katello/nssdb/nss_db_password-file]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/File[/etc/pki/katello/nssdb/nss_db_password-file]/notify: subscribes to Exec[create-nss-db]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[create-nss-db]/before: requires Exec[delete ca]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[create-nss-db]/before: requires Exec[delete broker]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[create-nss-db]/before: requires Exec[delete amqp-client]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[create-nss-db]/notify: subscribes to Certs::Ssltools::Certutil[ca]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Certs::Ssltools::Certutil[ca]/notify: subscribes to File[/etc/pki/katello/nssdb/cert8.db]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Certs::Ssltools::Certutil[ca]/notify: subscribes to File[/etc/pki/katello/nssdb/key3.db]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Certs::Ssltools::Certutil[ca]/notify: subscribes to File[/etc/pki/katello/nssdb/secmod.db]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/File[/etc/pki/katello/nssdb/cert8.db]/notify: subscribes to Certs::Ssltools::Certutil[broker]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/File[/etc/pki/katello/nssdb/key3.db]/notify: subscribes to Certs::Ssltools::Certutil[broker]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/File[/etc/pki/katello/nssdb/secmod.db]/notify: subscribes to Certs::Ssltools::Certutil[broker]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Certs::Ssltools::Certutil[broker]/notify: subscribes to Exec[generate-pfx-for-nss-db]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[generate-pfx-for-nss-db]/notify: subscribes to Exec[add-private-key-to-nss-db]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[add-private-key-to-nss-db]/notify: subscribes to Service[qpidd]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/notify: subscribes to Class[Candlepin]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Cert[java-client]/notify: subscribes to Pubkey[/etc/pki/katello/certs/java-client.crt]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/File[/etc/pki/katello/keystore_password-file]/notify: subscribes to Exec[candlepin-generate-ssl-keystore]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Exec[candlepin-generate-ssl-keystore]/notify: subscribes to File[/usr/share/tomcat/conf/keystore]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/File[/usr/share/tomcat/conf/keystore]/notify: subscribes to Service[tomcat]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Pubkey[/etc/pki/katello/certs/java-client.crt]/notify: subscribes to Privkey[/etc/pki/katello/private/java-client.key]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Privkey[/etc/pki/katello/private/java-client.key]/notify: subscribes to Certs::Ssltools::Certutil[amqp-client]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Certs::Ssltools::Certutil[amqp-client]/subscribe: subscribes to Exec[create-nss-db]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Certs::Ssltools::Certutil[amqp-client]/notify: subscribes to Service[qpidd]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Certs::Ssltools::Certutil[amqp-client]/notify: subscribes to File[/etc/candlepin/certs/amqp]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/File[/etc/candlepin/certs/amqp]/notify: subscribes to Exec[create candlepin qpid exchange]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Exec[create candlepin qpid exchange]/require: requires Service[qpidd]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Exec[create candlepin qpid exchange]/notify: subscribes to Exec[import CA into Candlepin truststore]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Exec[import CA into Candlepin truststore]/notify: subscribes to Exec[import client certificate into Candlepin keystore]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Exec[import client certificate into Candlepin keystore]/notify: subscribes to File[/etc/candlepin/certs/amqp/candlepin.jks]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/File[/etc/candlepin/certs/amqp/candlepin.jks]/notify: subscribes to Service[tomcat]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Candlepin/notify: subscribes to Class[Qpid]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Candlepin::Install/notify: subscribes to Class[Candlepin::Config]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Candlepin::Config/notify: subscribes to Class[Candlepin::Database]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Candlepin::Database/notify: subscribes to Class[Candlepin::Service]\\n\\"\\"\\".strip()\\n\\nCANDLEPIN_LOG = \\"\\"\\"\\n2016-09-09 13:45:52,650 [req=bd5a4284-d280-4fc5-a3d5-fc976b7aa5cc, org=] INFO org.candlepin.common.filter.LoggingFilter - Request: verb=GET, uri=/candlepin/consumers/f7677b4b-c470-4626-86a4-2fdf2546af4b\\n2016-09-09 13:45:52,784 [req=bd5a4284-d280-4fc5-a3d5-fc976b7aa5cc, org=ING_Luxembourg_SA] INFO  org.candlepin.common.filter.LoggingFilter - Response: status=200, content-type=\\"application/json\\", time=134\\n2016-09-09 13:45:52,947 [req=909ca4c5-f24e-4212-8f23-cc754d06ac57, org=] INFO org.candlepin.common.filter.LoggingFilter - Request: verb=GET, uri=/candlepin/consumers/f7677b4b-c470-4626-86a4-2fdf2546af4b/content_overrides\\n2016-09-09 13:45:52,976 [req=909ca4c5-f24e-4212-8f23-cc754d06ac57, org=] INFO org.candlepin.common.filter.LoggingFilter - Response: status=200, content-type=\\"application/json\\", time=29\\n2016-09-09 13:45:53,072 [req=49becd26-5dfe-4d2f-8667-470519230d88, org=] INFO org.candlepin.common.filter.LoggingFilter - Request: verb=GET, uri=/candlepin/consumers/f7677b4b-c470-4626-86a4-2fdf2546af4b/release\\n2016-09-09 13:45:53,115 [req=49becd26-5dfe-4d2f-8667-470519230d88, org=ING_Luxembourg_SA] INFO  org.candlepin.common.filter.LoggingFilter - Response: status=200, content-type=\\"application/json\\", time=43\\n\\"\\"\\".strip()\\n\\nPROXY_LOG = \\"\\"\\"\\n127.0.0.1 - - [31/May/2016:09:42:28 -0400] \\"GET /puppet/environments/KT_Encore_Library_RHEL_6_5/classes HTTP/1.1\\" 200 76785 6.1205\\n127.0.0.1 - - [31/May/2016:09:42:38 -0400] \\"GET /puppet/environments/KT_Encore_Library_RHEL_7_6/classes HTTP/1.1\\" 200 76785 4.4754\\n127.0.0.1 - - [31/May/2016:09:42:49 -0400] \\"GET /puppet/environments/KT_Encore_Library_RHEL6_8/classes HTTP/1.1\\" 200 76785 4.5776\\n127.0.0.1 - - [31/May/2016:09:57:34 -0400] \\"GET /tftp/serverName HTTP/1.1\\" 200 38 0.0014\\nE, [2016-05-31T09:57:34.884636 #4494] ERROR -- : Record 172.16.100.0/172.16.100.17 not found ]\\n\\"\\"\\".strip()\\n\\n\\ndef test_production_log():\\n    fm_log = ProductionLog(context_wrap(PRODUCTION_LOG))\\n    assert 2 == len(fm_log.get(\\"Rendered text template\\"))\\n    assert \\"Expired 48 Reports\\" in fm_log\\n    assert fm_log.get(\\"Completed 200 OK in 93\\")[0] == \\\\\\n        \\"2015-11-13 09:41:58 [I] Completed 200 OK in 93ms (Views: 2.9ms | ActiveRecord: 0.3ms)\\"\\n\\n\\ndef test_proxy_log():\\n    px_log = ProxyLog(context_wrap(PROXY_LOG))\\n    assert \\"ERROR -- \\" in px_log\\n    assert len(px_log.get(\\"KT_Encore_Library_RHEL\\")) == 3\\n\\n\\ndef test_candlepin_log():\\n    cp_log = CandlepinLog(context_wrap(CANDLEPIN_LOG))\\n    assert \\"req=49becd26-5dfe-4d2f-8667-470519230d88\\" in cp_log\\n    assert len(cp_log.get(\\"req=bd5a4284-d280-4fc5-a3d5-fc976b7aa5cc\\")) == 2\\n\\n\\ndef test_satellite_log():\\n    sat_log = SatelliteLog(context_wrap(SATELLITE_OUT))\\n    assert \\"subscribes to Class[Qpid]\\" in sat_log\\n    assert len(sat_log.get(\\"notify: subscribes to Class[\\")) == 7\\n"}\n'
line: b'{"repo_name":"Thraxis/pymedusa","ref":"refs/heads/master","path":"lib/html5lib/treewalkers/etree.py","content":"from __future__ import absolute_import, division, unicode_literals\\n\\ntry:\\n    from collections import OrderedDict\\nexcept ImportError:\\n    try:\\n        from ordereddict import OrderedDict\\n    except ImportError:\\n        OrderedDict = dict\\n\\nimport re\\n\\nfrom six import string_types\\n\\nfrom . import _base\\nfrom ..utils import moduleFactoryFactory\\n\\ntag_regexp = re.compile(\\"{([^}]*)}(.*)\\")\\n\\n\\ndef getETreeBuilder(ElementTreeImplementation):\\n    ElementTree = ElementTreeImplementation\\n    ElementTreeCommentType = ElementTree.Comment(\\"asd\\").tag\\n\\n    class TreeWalker(_base.NonRecursiveTreeWalker):\\n        \\"\\"\\"Given the particular ElementTree representation, this implementation,\\n        to avoid using recursion, returns \\"nodes\\" as tuples with the following\\n        content:\\n\\n        1. The current element\\n\\n        2. The index of the element relative to its parent\\n\\n        3. A stack of ancestor elements\\n\\n        4. A flag \\"text\\", \\"tail\\" or None to indicate if the current node is a\\n           text node; either the text or tail of the current element (1)\\n        \\"\\"\\"\\n        def getNodeDetails(self, node):\\n            if isinstance(node, tuple):  # It might be the root Element\\n                elt, key, parents, flag = node\\n                if flag in (\\"text\\", \\"tail\\"):\\n                    return _base.TEXT, getattr(elt, flag)\\n                else:\\n                    node = elt\\n\\n            if not(hasattr(node, \\"tag\\")):\\n                node = node.getroot()\\n\\n            if node.tag in (\\"DOCUMENT_ROOT\\", \\"DOCUMENT_FRAGMENT\\"):\\n                return (_base.DOCUMENT,)\\n\\n            elif node.tag == \\"\\u003c!DOCTYPE\\u003e\\":\\n                return (_base.DOCTYPE, node.text,\\n                        node.get(\\"publicId\\"), node.get(\\"systemId\\"))\\n\\n            elif node.tag == ElementTreeCommentType:\\n                return _base.COMMENT, node.text\\n\\n            else:\\n                assert isinstance(node.tag, string_types), type(node.tag)\\n                # This is assumed to be an ordinary element\\n                match = tag_regexp.match(node.tag)\\n                if match:\\n                    namespace, tag = match.groups()\\n                else:\\n                    namespace = None\\n                    tag = node.tag\\n                attrs = OrderedDict()\\n                for name, value in list(node.attrib.items()):\\n                    match = tag_regexp.match(name)\\n                    if match:\\n                        attrs[(match.group(1), match.group(2))] = value\\n                    else:\\n                        attrs[(None, name)] = value\\n                return (_base.ELEMENT, namespace, tag,\\n                        attrs, len(node) or node.text)\\n\\n        def getFirstChild(self, node):\\n            if isinstance(node, tuple):\\n                element, key, parents, flag = node\\n            else:\\n                element, key, parents, flag = node, None, [], None\\n\\n            if flag in (\\"text\\", \\"tail\\"):\\n                return None\\n            else:\\n                if element.text:\\n                    return element, key, parents, \\"text\\"\\n                elif len(element):\\n                    parents.append(element)\\n                    return element[0], 0, parents, None\\n                else:\\n                    return None\\n\\n        def getNextSibling(self, node):\\n            if isinstance(node, tuple):\\n                element, key, parents, flag = node\\n            else:\\n                return None\\n\\n            if flag == \\"text\\":\\n                if len(element):\\n                    parents.append(element)\\n                    return element[0], 0, parents, None\\n                else:\\n                    return None\\n            else:\\n                if element.tail and flag != \\"tail\\":\\n                    return element, key, parents, \\"tail\\"\\n                elif key \\u003c len(parents[-1]) - 1:\\n                    return parents[-1][key + 1], key + 1, parents, None\\n                else:\\n                    return None\\n\\n        def getParentNode(self, node):\\n            if isinstance(node, tuple):\\n                element, key, parents, flag = node\\n            else:\\n                return None\\n\\n            if flag == \\"text\\":\\n                if not parents:\\n                    return element\\n                else:\\n                    return element, key, parents, None\\n            else:\\n                parent = parents.pop()\\n                if not parents:\\n                    return parent\\n                else:\\n                    return parent, list(parents[-1]).index(parent), parents, None\\n\\n    return locals()\\n\\ngetETreeModule = moduleFactoryFactory(getETreeBuilder)\\n"}\n'
line: b'{"repo_name":"xxxIsaacPeralxxx/anim-studio-tools","ref":"refs/heads/master","path":"kip/houdini/code/kip_houdini/convert.py","content":"#                 Dr. D Studios - Software Disclaimer\\n#\\n# Copyright 2009 Dr D Studios Pty Limited (ACN 127 184 954) (Dr. D Studios), its\\n# affiliates and/or its licensors.\\n#\\n###############################################################################\\n\\"\\"\\"\\nThis module will help TD\'s to convert houdini animation curve to nuke or maya\\nthe other way around also.\\n\\n.. note::\\n\\n    Please make sure you are running in proper kipHoudini environment\\n\\n.. warning::\\n\\n    Dont import this module as standalone , use this module with kip project\\n\\n\\"\\"\\"\\n\\n__authors__ = [\\"kurian.os\\"]\\n__version__ = \\"$Revision: 104961 $\\".split()[1]\\n__revision__ = __version__\\n__date__ = \\"$Date:  July 19, 2011 12:00:00 PM$\\".split()[1]\\n__copyright__ = \\"2011\\"\\n__license__ = \\"Copyright 2011 Dr D Studios Pty Limited\\"\\n__contact__ = \\"kurian.os@drdstudios.com\\"\\n__status__ = \\"Development\\"\\n\\n\\nimport os\\nimport traceback\\n#import hou\\nimport napalm.core as nap_core\\nimport node_curves as node_curves\\nimport kip.kip_reader as kip_reader\\nreload(node_curves)\\nreload(kip_reader)\\nfrom rodin import logging\\nfrom kip.kip_curve_class import *\\nfrom kip.kip_napalm_class import *\\nfrom kip.utils.kipError import *\\nfrom kip.template import *\\n\\n\\nrodin_logger = logging.get_logger(\'kipHoudini\')\\nnapalm_func = Napalm()\\n\\nGLOBAL_FPS = 24\\nGLOBAL_TIME = 1\\n\\n\\nclass HoudiniWriter(object):\\n\\n    \\"\\"\\"\\n    Creating houdini curve writer class\\n\\n    *Parents:*\\n\\n        None\\n\\n    *Children:*\\n\\n        * :func:`writeOutCurves`\\n\\n    \\"\\"\\"\\n\\n    def __init__(self):\\n        \\"\\"\\"\\n        Base init function for houdini convert.write Class.\\n        \\"\\"\\"\\n        rodin_logger.info(\\"kip houdini writing class initialized\\")\\n        self.houdini_version = \\"houdini,%s\\" % hou.applicationVersionString()\\n        self.kip_houdini_version = \\"kipHoudini%s\\" % os.getenv(\\"DRD_KIPHOUDINI_VERSION\\")\\n\\n    def writeOutCurves(self, nap_file_name = None , houdini_nodes = [],\\n                            houdini_node_attributes = [], start_frame = None,\\n                            end_frame = None, write_xml = False, silent = False,\\n                            left_eyes = [], right_eyes = [], map_file_name = None):\\n\\n        \\"\\"\\"\\n        This function will create a curve class object first and then it will write out the napalm file.\\n\\n        .. warning::\\n\\n            If you are unable to write out napalm file or write_status=False that means napalm failed to write out.\\n\\n        :param  nap_file_name: User must pass a file where he want to write out curves and make sure you supply a .nap or .xml file format(strict)\\n\\n        :type nap_file_name: string\\n\\n        :param houdini_nodes: list of houdini objects(strict)\\n\\n        :type houdini_nodes: list\\n\\n        :param houdini_node_attribute: if you want to replace attribute from the map file then you can specify the override attribute here\\n\\n        :type houdini_node_attribute: list\\n\\n        :param start_frame: start frame to capture\\n\\n        :type start_frame: int\\n\\n        :param end_frame: end frame to capture\\n\\n        :type end_frame: int\\n\\n        :param write_xml: If you want to write out a xml file instead of napalm file then this should be true\\n\\n        :type end_frame: string\\n\\n        :param left_eyes: Left eye objects\\n\\n        :type left_eyes: list\\n\\n        :param right_eyes: Right eye objects\\n\\n        :type right_eyes: list\\n\\n        :param map_file_name: Filepath of napalm channel data\\n\\n        :type map_file_name: string\\n\\n        :return: Status,channel file , map file\\n\\n        :rtype: boot,string,string\\n\\n        Example\\n\\n            \\u003e\\u003e\\u003e import kip_houdini.convert as kh\\n            \\u003e\\u003e\\u003e reload(kh)\\n            \\u003e\\u003e\\u003e khcw = kh.HoudiniWriter()\\n            \\u003e\\u003e\\u003e status,nap_file,map_file=khcw.writeOutCurves(nap_file_name = \\"/tmp/houdini_kip_test_s.nap\\",map_file_name= \\"/tmp/houdini_kip_test_m.nap\\",houdini_nodes = [\\"/obj/geo/xform_1\\",\\"/obj/geo/xform_2\\"],left_eyes=[\\"/obj/geo/xform_1\\"],right_eyes=[\\"/obj/geo/xform_2\\"])\\n\\n        \\"\\"\\"\\n        if nap_file_name:\\n            node_curv = node_curves.NodeCurves()\\n            get_all_curves = node_curv.getCurves(houdini_node_curves = houdini_nodes, \\\\\\n                                houdini_attribute_curves = houdini_node_attributes, \\\\\\n                                start_frame = start_frame, end_frame = end_frame, \\\\\\n                                silent = silent, left_eye_curves = left_eyes, \\\\\\n                                right_eye_curves = right_eyes)\\n            if write_xml:\\n                if not nap_file_name.endswith(\\".xml\\"):\\n                    split_base_ext = os.path.splitext(nap_file_name)\\n                    if split_base_ext[-1]:\\n                        nap_file_name = \\"%s/.xml\\" % (split_base_ext[0])\\n                    else:\\n                        nap_file_name = \\"%s/.xml\\" % (nap_file_name)\\n            else:\\n                if not nap_file_name.endswith(\\".nap\\"):\\n                    raise KipBaseError(\\"Unknown file extension found in %s !\\" % nap_file_name)\\n\\n            write_status, map_file, nap_file = napalm_func.write(nap_file_name, get_all_curves, \\\\\\n                                        debug = True, map_file_name = map_file_name, \\\\\\n                                        software = self.houdini_version, \\\\\\n                                        app_version = self.kip_houdini_version)\\n\\n            rodin_logger.info(\\"%s %s %s\\" % (write_status, map_file, nap_file))\\n            return (write_status, map_file, nap_file)\\n        else:\\n            raise KipBaseError(\\"Expected napalm file name for write curve !\\")\\n\\nclass HoudiniReader(object):\\n    \\"\\"\\"\\n\\n    Creating houdini curve reader class\\n\\n    *Parents:*\\n\\n        None\\n\\n    *Children:*\\n\\n        * :func:`houSetAttr`\\n\\n    \\"\\"\\"\\n    def __init__(self):\\n        \\"\\"\\"\\n        Base init function for houdini convert.write Class.\\n        \\"\\"\\"\\n        rodin_logger.info(\\"kip houdini read class initialized\\")\\n        self.nuke_tan_types = {\\"spline\\":\\"spline()\\", \\"linear\\":\\"linear()\\", \\\\\\n                                    \\"constant\\":\\"constant()\\", \\"cubic\\":\\"bezier()\\"}\\n\\n        self.channel_match = {\'translateX\':\'tx\', \'translateY\':\'ty\', \'translateZ\':\'tz\', \\\\\\n                                \'rotateX\':\'rx\', \'rotateY\':\'ry\', \'rotateZ\':\'rz\', \\\\\\n                                \'scaleX\':\'sx\', \'scaleY\':\'sy\', \'scaleZ\':\'sz\'}\\n\\n    def houSetAttr(self, nap_file_name = None, houdini_nodes = [], houdini_node_attribute = None,\\n                            map_file_name = None, offset_value = 0, start_frame = None,\\n                            end_frame = None, attribute_map = None):\\n        \\"\\"\\"\\n        This function will get all curve data from a map and channel file then those data will be applied to proper nodes\\n\\n        :param  nap_file_name: User must pass a file where he want to write out curves and make sure you supply a .nap or .xml file format\\n\\n        :type nap_file_name: string\\n\\n        :param houdini_nodes: list of houdini objects\\n\\n        :type houdini_nodes: list\\n\\n        :param houdini_node_attribute: if you want to replace attribute from the map file then you can specify the override attribute here\\n\\n        :type houdini_node_attribute: string\\n\\n        :param map_file_name: Filepath of napalm channel data\\n\\n        :type map_file_name: string\\n\\n        :param offset_value: Animation key offset value\\n\\n        :type offset_value: int\\n\\n        :param start_frame: start frame to capture\\n\\n        :type start_frame: int\\n\\n        :param end_frame: end frame to capture\\n\\n        :type end_frame: int\\n\\n        :param attribute_map: This a template object from template module\\n\\n        :type attribute_map: list of tuple\\n\\n        Example\\n\\n            \\u003e\\u003e\\u003e import kip_houdini.convert as kh\\n            \\u003e\\u003e\\u003e reload(kh)\\n            \\u003e\\u003e\\u003e khpr=kh.HoudiniReader()\\n            \\u003e\\u003e\\u003e import kip.template as template\\n            \\u003e\\u003e\\u003e attr_mp = template.KipTemplates()\\n            \\u003e\\u003e\\u003e attr_mp.ATTRMAP={\\"t1.cutatt1\\":\\"/obj/geo1/xform1.ottr_1\\",\\"t1.cutatt2\\":\\"/obj/geo1/xform1.ottr_2\\",\\"t2.cutatt1\\":\\"/obj/geo1/xform1.ottr_3\\",\\"t2.cutatt2\\":\\"/obj/geo1/xform1.ottr_4\\"}\\n            \\u003e\\u003e\\u003e a = attr_mp.ATTRMAP\\n            \\u003e\\u003e\\u003e khpr.houSetAttr(nap_file_name=\\"/tmp/single_maya_test.nap\\",houdini_nodes=\\"/obj/geo1/xform1\\",attribute_map=a)\\n\\n        \\"\\"\\"\\n        if nap_file_name:\\n\\n            if not map_file_name:\\n                map_file_name = kip_reader.build_map_file_name(nap_file_name)\\n            header_info = kip_reader.header(map_file_name)\\n            array_index = kip_reader.find_software_index(header_info[\\"client_software\\"])\\n\\n            houdini_node_list = houdini_nodes\\n            knob_read = kip_reader.ReadCurve()\\n            get_curve_class = knob_read.getCurves(nap_file_name = nap_file_name, \\\\\\n                                map_file_name = map_file_name, offset_value = offset_value)\\n\\n        for each_node in get_curve_class:\\n            node_key = get_curve_class.index(each_node)\\n            current_node_curve = each_node[2]\\n            curent_source_node = each_node[0]\\n            for each_curve in current_node_curve:\\n                curve_attr\\t\\t= each_curve[1]\\n                current_key_dict = each_curve[2]\\n                time_keys \\t\\t= current_key_dict[\\"time\\"]\\n                key_value\\t\\t= current_key_dict[\\"key_value\\"]\\n                in_angle\\t\\t= current_key_dict[\\"in_angle\\"]\\n                out_angle\\t\\t= current_key_dict[\\"out_angle\\"]\\n                in_weight\\t\\t= current_key_dict[\\"in_weight\\"]\\n                out_weight\\t\\t= current_key_dict[\\"out_weight\\"]\\n                in_tan_type\\t\\t= current_key_dict[\\"in_tan_type\\"]\\n                out_tan_type\\t= current_key_dict[\\"out_tan_type\\"]\\n                in_slope\\t\\t= current_key_dict[\\"in_slope\\"]\\n                out_slope\\t\\t= current_key_dict[\\"out_slope\\"]\\n                try:\\n                    for time in time_keys:\\n\\n                        if houdini_node_attribute:\\n                            curve_attr = houdini_node_attribute\\n                        else:\\n                            if attribute_map:\\n                                temp_attr_keys = attribute_map.keys()\\n                                for each_template in temp_attr_keys:\\n                                    source_details = each_template.split(\\".\\")\\n                                    current_node_attr = \\"%s.%s\\" % (curent_source_node, \\\\\\n                                                                        each_curve[1])\\n                                    if current_node_attr == each_template:\\n                                        destenation_details = attribute_map[each_template]\\\\\\n                                                                                .split(\\".\\")\\n                                        curve_attr = destenation_details[1]\\n                                        current_houdini_node = destenation_details[0]\\n                                        current_houdini_node = hou.node(current_houdini_node)\\n                                        break\\n                            else:\\n                                current_houdini_node = hou.node(houdini_node_list[node_key])\\n                        if start_frame and end_frame:\\n                            if time in range(start_frame, end_frame + 1):\\n                                key_index = time_keys.index(time)\\n                            else:\\n                                print \\"%s not in range not applying the key\\" % time\\n                                continue\\n                        else:\\n                            key_index = time_keys.index(time)\\n                        in_tan_v = in_tan_type[key_index]\\n                        if self.nuke_tan_types.has_key(in_tan_v):\\n                            in_tan_v = self.nuke_tan_types[in_tan_v]\\n                        else:\\n                            in_tan_v = \\"bezier()\\"\\n                        hkey = hou.Keyframe()\\n                        hkey.setTime((time_keys[key_index]/GLOBAL_FPS))\\n                        hkey.setValue(key_value[key_index])\\n                        hkey.setExpression(\\"bezier()\\")\\n                        hkey.setExpression(\\"spline()\\")\\n                        hkey.setInAccel(in_weight[key_index])\\n                        hkey.setAccel(out_weight[key_index])\\n                        hkey.setInSlope(in_slope[key_index])\\n                        hkey.setSlope(out_slope[key_index])\\n                        this_node_attr = curve_attr\\n                        if self.channel_match.has_key(curve_attr):\\n                            this_node_attr = self.channel_match[curve_attr]\\n                        hou_nod = current_houdini_node.parm(this_node_attr).setKeyframe(hkey)\\n                except:\\n                    traceback.print_exc()\\n                    raise KipBaseError(\\"No objects found in node list!\\")\\n        rodin_logger.info(\\"Aniamtion curve trasfer is finished !\\")\\n        return True\\n\\ndef header(map_file_name):\\n    \\"\\"\\"\\n\\n    This function will return a dict of header details from map file\\n\\n    :param map_file_name: Filepath of napalm channel data\\n\\n    :type map_file_name: string\\n\\n    :return: header details\\n\\n    :rtype: dict\\n\\n    \\"\\"\\"\\n    if os.path.exists(map_file_name):\\n        nap_header = kip_reader.header(map_file_name)\\n        return nap_header\\n    return None\\n\\n# Copyright 2008-2012 Dr D Studios Pty Limited (ACN 127 184 954) (Dr. D Studios)\\n#\\n# This file is part of anim-studio-tools.\\n#\\n# anim-studio-tools is free software: you can redistribute it and/or modify\\n# it under the terms of the GNU Lesser General Public License as published by\\n# the Free Software Foundation, either version 3 of the License, or\\n# (at your option) any later version.\\n#\\n# anim-studio-tools is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU General Public License for more details.\\n#\\n# You should have received a copy of the GNU Lesser General Public License\\n# along with anim-studio-tools.  If not, see \\u003chttp://www.gnu.org/licenses/\\u003e.\\n\\n"}\n'
line: b'{"repo_name":"dentaku65/plugin.video.italyalacarta","ref":"refs/heads/master","path":"lib/gdata/tlslite/utils/PyCrypto_RSAKey.py","content":"\\"\\"\\"PyCrypto RSA implementation.\\"\\"\\"\\n\\nfrom cryptomath import *\\n\\nfrom RSAKey import *\\nfrom Python_RSAKey import Python_RSAKey\\n\\nif pycryptoLoaded:\\n\\n    from Crypto.PublicKey import RSA\\n\\n    class PyCrypto_RSAKey(RSAKey):\\n        def __init__(self, n=0, e=0, d=0, p=0, q=0, dP=0, dQ=0, qInv=0):\\n            if not d:\\n                self.rsa = RSA.construct( (n, e) )\\n            else:\\n                self.rsa = RSA.construct( (n, e, d, p, q) )\\n\\n        def __getattr__(self, name):\\n            return getattr(self.rsa, name)\\n\\n        def hasPrivateKey(self):\\n            return self.rsa.has_private()\\n\\n        def hash(self):\\n            return Python_RSAKey(self.n, self.e).hash()\\n\\n        def _rawPrivateKeyOp(self, m):\\n            s = numberToString(m)\\n            byteLength = numBytes(self.n)\\n            if len(s)== byteLength:\\n                pass\\n            elif len(s) == byteLength-1:\\n                s = \'\\\\0\' + s\\n            else:\\n                raise AssertionError()\\n            c = stringToNumber(self.rsa.decrypt((s,)))\\n            return c\\n\\n        def _rawPublicKeyOp(self, c):\\n            s = numberToString(c)\\n            byteLength = numBytes(self.n)\\n            if len(s)== byteLength:\\n                pass\\n            elif len(s) == byteLength-1:\\n                s = \'\\\\0\' + s\\n            else:\\n                raise AssertionError()\\n            m = stringToNumber(self.rsa.encrypt(s, None)[0])\\n            return m\\n\\n        def writeXMLPublicKey(self, indent=\'\'):\\n            return Python_RSAKey(self.n, self.e).write(indent)\\n\\n        def generate(bits):\\n            key = PyCrypto_RSAKey()\\n            def f(numBytes):\\n                return bytesToString(getRandomBytes(numBytes))\\n            key.rsa = RSA.generate(bits, f)\\n            return key\\n        generate = staticmethod(generate)\\n"}\n'
line: b'{"repo_name":"itsjeyd/edx-platform","ref":"refs/heads/master","path":"cms/djangoapps/contentstore/tests/test_import_draft_order.py","content":"\\"\\"\\"\\nTests Draft import order.\\n\\"\\"\\"\\nfrom xmodule.modulestore.xml_importer import import_course_from_xml\\n\\nfrom xmodule.modulestore.tests.django_utils import ModuleStoreTestCase\\nfrom xmodule.modulestore.django import modulestore\\nfrom django.conf import settings\\n\\nTEST_DATA_DIR = settings.COMMON_TEST_DATA_ROOT\\n\\n\\n# This test is in the CMS module because the test configuration to use a draft\\n# modulestore is dependent on django.\\nclass DraftReorderTestCase(ModuleStoreTestCase):\\n\\n    def test_order(self):\\n        \\"\\"\\"\\n        Verify that drafts are imported in the correct order.\\n        \\"\\"\\"\\n        store = modulestore()\\n        course_items = import_course_from_xml(\\n            store, self.user.id, TEST_DATA_DIR, [\'import_draft_order\'], create_if_not_present=True\\n        )\\n        course_key = course_items[0].id\\n        sequential = store.get_item(course_key.make_usage_key(\'sequential\', \'0f4f7649b10141b0bdc9922dcf94515a\'))\\n        verticals = sequential.children\\n\\n        # The order that files are read in from the file system is not guaranteed (cannot rely on\\n        # alphabetical ordering, for example). Therefore, I have added a lot of variation in filename and desired\\n        # ordering so that the test reliably failed with the bug, at least on Linux.\\n        #\\n        # \'a\', \'b\', \'c\', \'d\', and \'z\' are all drafts, with \'index_in_children_list\' of\\n        #  2 ,  4 ,  6 ,  5 , and  0  respectively.\\n        #\\n        # \'5a05be9d59fc4bb79282c94c9e6b88c7\' and \'second\' are public verticals.\\n        self.assertEqual(7, len(verticals))\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'z\'), verticals[0])\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'5a05be9d59fc4bb79282c94c9e6b88c7\'), verticals[1])\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'a\'), verticals[2])\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'second\'), verticals[3])\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'b\'), verticals[4])\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'d\'), verticals[5])\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'c\'), verticals[6])\\n\\n        # Now also test that the verticals in a second sequential are correct.\\n        sequential = store.get_item(course_key.make_usage_key(\'sequential\', \'secondseq\'))\\n        verticals = sequential.children\\n        # \'asecond\' and \'zsecond\' are drafts with \'index_in_children_list\' 0 and 2, respectively.\\n        # \'secondsubsection\' is a public vertical.\\n        self.assertEqual(3, len(verticals))\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'asecond\'), verticals[0])\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'secondsubsection\'), verticals[1])\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'zsecond\'), verticals[2])\\n"}\n'
line: b'{"repo_name":"Baumelbi/IntroPython2016","ref":"refs/heads/master","path":"Solutions/Session06/test_mailroom2.py","content":"#!/usr/bin/env python\\n\\n\\"\\"\\"\\nunit tests for the mailroom program\\n\\"\\"\\"\\nimport os\\n\\nimport mailroom2 as mailroom\\n\\n# so that it\'s there for the tests\\nmailroom.donor_db = mailroom.get_donor_db()\\n\\n\\ndef test_list_donors():\\n    listing = mailroom.list_donors()\\n\\n    # hard to test this throughly -- better not to hard code the entire\\n    # thing. But check for a few aspects -- this will catch the likely\\n    # errors\\n    assert listing.startswith(\\"Donor list:\\\\n\\")\\n    assert \\"Jeff Bezos\\" in listing\\n    assert \\"William Gates III\\" in listing\\n    assert len(listing.split(\'\\\\n\')) == 5\\n\\n\\ndef test_find_donor():\\n    \\"\\"\\" checks a donor that is there, but with odd case and spaces\\"\\"\\"\\n    donor = mailroom.find_donor(\\"jefF beZos \\")\\n\\n    assert donor[0] == \\"Jeff Bezos\\"\\n\\n\\ndef test_find_donor_not():\\n    \\"test one that\'s not there\\"\\n    donor = mailroom.find_donor(\\"Jeff Bzos\\")\\n\\n    assert donor is None\\n\\n\\ndef test_gen_letter():\\n    \\"\\"\\" test the donor letter \\"\\"\\"\\n\\n    # create a sample donor\\n    donor = (\\"Fred Flintstone\\", [432.45, 65.45, 230.0])\\n    letter = mailroom.gen_letter(donor)\\n    # what to test? tricky!\\n    assert letter.startswith(\\"Dear Fred Flintstone\\")\\n    assert letter.endswith(\\"-The Team\\\\n\\")\\n    assert \\"donation of $230.00\\" in letter\\n\\n\\ndef test_add_donor():\\n    name = \\"Fred Flintstone  \\"\\n\\n    donor = mailroom.add_donor(name)\\n    donor[1].append(300)\\n    assert donor[0] == \\"Fred Flintstone\\"\\n    assert donor[1] == [300]\\n    assert mailroom.find_donor(name) == donor\\n\\n\\ndef test_generate_donor_report():\\n\\n    report = mailroom.generate_donor_report()\\n\\n    print(report)  # printing so you can see it if it fails\\n    # this is pretty tough to test\\n    # these are not great, because they will fail if unimportant parts of the\\n    # report are changed.\\n    # but at least you know that codes working now.\\n    assert report.startswith(\\"Donor Name                | Total Given | Num Gifts | Average Gift\\")\\n\\n    assert \\"Jeff Bezos                  $    877.33           1   $     877.33\\" in report\\n\\n\\ndef test_save_letters_to_disk():\\n    \\"\\"\\"\\n    This only tests that the files get created, but that\'s a start\\n\\n    Note that the contents of the letter was already\\n    tested with test_gen_letter\\n    \\"\\"\\"\\n\\n    mailroom.save_letters_to_disk()\\n\\n    assert os.path.isfile(\'Jeff_Bezos.txt\')\\n    assert os.path.isfile(\'William_Gates_III.txt\')\\n    # check that it\'snot empty:\\n    with open(\'William_Gates_III.txt\') as f:\\n        size = len(f.read())\\n    assert size \\u003e 0\\n\\n\\nif __name__ == \\"__main__\\":\\n    # this is best run with a test runner, like pytest\\n    # But if not, at least this will run them all.\\n    test_list_donors()\\n    test_find_donor()\\n    test_find_donor_not()\\n    test_gen_letter()\\n    test_add_donor()\\n    test_generate_donor_report()\\n    test_save_letters_to_disk()\\n    print(\\"All tests Passed\\")\\n"}\n'
line: b'{"repo_name":"odoousers2014/odoo","ref":"refs/heads/master","path":"addons/website_sale_delivery/models/sale_order.py","content":"# -*- coding: utf-8 -*-\\n\\nfrom openerp.osv import orm, fields\\nfrom openerp import SUPERUSER_ID\\nfrom openerp.addons import decimal_precision\\n\\n\\nclass delivery_carrier(orm.Model):\\n    _name = \'delivery.carrier\'\\n    _inherit = [\'delivery.carrier\', \'website.published.mixin\']\\n\\n    _columns = {\\n        \'website_description\': fields.text(\'Description for the website\'),\\n    }\\n    _defaults = {\\n        \'website_published\': True\\n    }\\n\\n\\nclass SaleOrder(orm.Model):\\n    _inherit = \'sale.order\'\\n\\n    def _amount_all_wrapper(self, cr, uid, ids, field_name, arg, context=None):        \\n        \\"\\"\\" Wrapper because of direct method passing as parameter for function fields \\"\\"\\"\\n        return self._amount_all(cr, uid, ids, field_name, arg, context=context)\\n\\n    def _amount_all(self, cr, uid, ids, field_name, arg, context=None):\\n        res = super(SaleOrder, self)._amount_all(cr, uid, ids, field_name, arg, context=context)\\n        currency_pool = self.pool.get(\'res.currency\')\\n        for order in self.browse(cr, uid, ids, context=context):\\n            line_amount = sum([line.price_subtotal for line in order.order_line if line.is_delivery])\\n            currency = order.pricelist_id.currency_id\\n            res[order.id][\'amount_delivery\'] = currency_pool.round(cr, uid, currency, line_amount)\\n        return res\\n\\n    def _get_order(self, cr, uid, ids, context=None):\\n        result = {}\\n        for line in self.pool.get(\'sale.order.line\').browse(cr, uid, ids, context=context):\\n            result[line.order_id.id] = True\\n        return result.keys()\\n\\n    _columns = {\\n        \'amount_delivery\': fields.function(\\n            _amount_all_wrapper, type=\'float\', digits_compute=decimal_precision.get_precision(\'Account\'),\\n            string=\'Delivery Amount\',\\n            store={\\n                \'sale.order\': (lambda self, cr, uid, ids, c={}: ids, [\'order_line\'], 10),\\n                \'sale.order.line\': (_get_order, [\'price_unit\', \'tax_id\', \'discount\', \'product_uom_qty\'], 10),\\n            },\\n            multi=\'sums\', help=\\"The amount without tax.\\", track_visibility=\'always\'\\n        ),\\n        \'website_order_line\': fields.one2many(\\n            \'sale.order.line\', \'order_id\',\\n            string=\'Order Lines displayed on Website\', readonly=True,\\n            domain=[(\'is_delivery\', \'=\', False)],\\n            help=\'Order Lines to be displayed on the website. They should not be used for computation purpose.\',\\n        ),\\n    }\\n\\n    def _check_carrier_quotation(self, cr, uid, order, force_carrier_id=None, context=None):\\n        carrier_obj = self.pool.get(\'delivery.carrier\')\\n\\n        # check to add or remove carrier_id\\n        if not order:\\n            return False\\n        if all(line.product_id.type == \\"service\\" for line in order.website_order_line):\\n            order.write({\'carrier_id\': None})\\n            self.pool[\'sale.order\']._delivery_unset(cr, SUPERUSER_ID, [order.id], context=context)\\n            return True\\n        else: \\n            carrier_id = force_carrier_id or order.carrier_id.id\\n            carrier_ids = self._get_delivery_methods(cr, uid, order, context=context)\\n            if carrier_id:\\n                if carrier_id not in carrier_ids:\\n                    carrier_id = False\\n                else:\\n                    carrier_ids.remove(carrier_id)\\n                    carrier_ids.insert(0, carrier_id)\\n            if force_carrier_id or not carrier_id or not carrier_id in carrier_ids:\\n                for delivery_id in carrier_ids:\\n                    grid_id = carrier_obj.grid_get(cr, SUPERUSER_ID, [delivery_id], order.partner_shipping_id.id)\\n                    if grid_id:\\n                        carrier_id = delivery_id\\n                        break\\n                order.write({\'carrier_id\': carrier_id})\\n            if carrier_id:\\n                order.delivery_set()\\n            else:\\n                order._delivery_unset()                    \\n\\n        return bool(carrier_id)\\n\\n    def _get_delivery_methods(self, cr, uid, order, context=None):\\n        carrier_obj = self.pool.get(\'delivery.carrier\')\\n        delivery_ids = carrier_obj.search(cr, uid, [(\'website_published\',\'=\',True)], context=context)\\n        # Following loop is done to avoid displaying delivery methods who are not available for this order\\n        # This can surely be done in a more efficient way, but at the moment, it mimics the way it\'s\\n        # done in delivery_set method of sale.py, from delivery module\\n        for delivery_id in carrier_obj.browse(cr, SUPERUSER_ID, delivery_ids, context=dict(context, order_id=order.id)):\\n            if not delivery_id.available:\\n                delivery_ids.remove(delivery_id.id)\\n        return delivery_ids\\n\\n    def _get_errors(self, cr, uid, order, context=None):\\n        errors = super(SaleOrder, self)._get_errors(cr, uid, order, context=context)\\n        if not self._get_delivery_methods(cr, uid, order, context=context):\\n            errors.append((\'No delivery method available\', \'There is no available delivery method for your order\'))            \\n        return errors\\n\\n    def _get_website_data(self, cr, uid, order, context=None):\\n        \\"\\"\\" Override to add delivery-related website data. \\"\\"\\"\\n        values = super(SaleOrder, self)._get_website_data(cr, uid, order, context=context)\\n        # We need a delivery only if we have stockable products\\n        has_stockable_products = False\\n        for line in order.order_line:\\n            if line.product_id.type in (\'consu\', \'product\'):\\n                has_stockable_products = True\\n        if not has_stockable_products:\\n            return values\\n\\n        delivery_ctx = dict(context, order_id=order.id)\\n        DeliveryCarrier = self.pool.get(\'delivery.carrier\')\\n        delivery_ids = self._get_delivery_methods(cr, uid, order, context=context)\\n\\n        values[\'deliveries\'] = DeliveryCarrier.browse(cr, SUPERUSER_ID, delivery_ids, context=delivery_ctx)\\n        return values\\n"}\n'
line: b'{"repo_name":"bollu/sandhi","ref":"refs/heads/master","path":"modules/gr36/grc/gui/Connection.py","content":"\\"\\"\\"\\nCopyright 2007, 2008, 2009 Free Software Foundation, Inc.\\nThis file is part of GNU Radio\\n\\nGNU Radio Companion is free software; you can redistribute it and/or\\nmodify it under the terms of the GNU General Public License\\nas published by the Free Software Foundation; either version 2\\nof the License, or (at your option) any later version.\\n\\nGNU Radio Companion is distributed in the hope that it will be useful,\\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\nGNU General Public License for more details.\\n\\nYou should have received a copy of the GNU General Public License\\nalong with this program; if not, write to the Free Software\\nFoundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA\\n\\"\\"\\"\\n\\nimport Utils\\nfrom Element import Element\\nimport Colors\\nfrom Constants import CONNECTOR_ARROW_BASE, CONNECTOR_ARROW_HEIGHT\\n\\nclass Connection(Element):\\n\\t\\"\\"\\"\\n\\tA graphical connection for ports.\\n\\tThe connection has 2 parts, the arrow and the wire.\\n\\tThe coloring of the arrow and wire exposes the status of 3 states:\\n\\t\\tenabled/disabled, valid/invalid, highlighted/non-highlighted.\\n\\tThe wire coloring exposes the enabled and highlighted states.\\n\\tThe arrow coloring exposes the enabled and valid states.\\n\\t\\"\\"\\"\\n\\n\\tdef __init__(self): Element.__init__(self)\\n\\n\\tdef get_coordinate(self):\\n\\t\\t\\"\\"\\"\\n\\t\\tGet the 0,0 coordinate.\\n\\t\\tCoordinates are irrelevant in connection.\\n\\t\\t@return 0, 0\\n\\t\\t\\"\\"\\"\\n\\t\\treturn (0, 0)\\n\\n\\tdef get_rotation(self):\\n\\t\\t\\"\\"\\"\\n\\t\\tGet the 0 degree rotation.\\n\\t\\tRotations are irrelevant in connection.\\n\\t\\t@return 0\\n\\t\\t\\"\\"\\"\\n\\t\\treturn 0\\n\\n\\tdef create_shapes(self):\\n\\t\\t\\"\\"\\"Precalculate relative coordinates.\\"\\"\\"\\n\\t\\tElement.create_shapes(self)\\n\\t\\tself._sink_rot = None\\n\\t\\tself._source_rot = None\\n\\t\\tself._sink_coor = None\\n\\t\\tself._source_coor = None\\n\\t\\t#get the source coordinate\\n\\t\\tconnector_length = self.get_source().get_connector_length()\\n\\t\\tself.x1, self.y1 = Utils.get_rotated_coordinate((connector_length, 0), self.get_source().get_rotation())\\n\\t\\t#get the sink coordinate\\n\\t\\tconnector_length = self.get_sink().get_connector_length() + CONNECTOR_ARROW_HEIGHT\\n\\t\\tself.x2, self.y2 = Utils.get_rotated_coordinate((-connector_length, 0), self.get_sink().get_rotation())\\n\\t\\t#build the arrow\\n\\t\\tself.arrow = [(0, 0),\\n\\t\\t\\tUtils.get_rotated_coordinate((-CONNECTOR_ARROW_HEIGHT, -CONNECTOR_ARROW_BASE/2), self.get_sink().get_rotation()),\\n\\t\\t\\tUtils.get_rotated_coordinate((-CONNECTOR_ARROW_HEIGHT, CONNECTOR_ARROW_BASE/2), self.get_sink().get_rotation()),\\n\\t\\t]\\n\\t\\tself._update_after_move()\\n\\t\\tif not self.get_enabled(): self._arrow_color = Colors.CONNECTION_DISABLED_COLOR\\n\\t\\telif not self.is_valid(): self._arrow_color = Colors.CONNECTION_ERROR_COLOR\\n\\t\\telse: self._arrow_color = Colors.CONNECTION_ENABLED_COLOR\\n\\n\\tdef _update_after_move(self):\\n\\t\\t\\"\\"\\"Calculate coordinates.\\"\\"\\"\\n\\t\\tself.clear() #FIXME do i want this here?\\n\\t\\t#source connector\\n\\t\\tsource = self.get_source()\\n\\t\\tX, Y = source.get_connector_coordinate()\\n\\t\\tx1, y1 = self.x1 + X, self.y1 + Y\\n\\t\\tself.add_line((x1, y1), (X, Y))\\n\\t\\t#sink connector\\n\\t\\tsink = self.get_sink()\\n\\t\\tX, Y = sink.get_connector_coordinate()\\n\\t\\tx2, y2 = self.x2 + X, self.y2 + Y\\n\\t\\tself.add_line((x2, y2), (X, Y))\\n\\t\\t#adjust arrow\\n\\t\\tself._arrow = [(x+X, y+Y) for x,y in self.arrow]\\n\\t\\t#add the horizontal and vertical lines in this connection\\n\\t\\tif abs(source.get_connector_direction() - sink.get_connector_direction()) == 180:\\n\\t\\t\\t#2 possible point sets to create a 3-line connector\\n\\t\\t\\tmid_x, mid_y = (x1 + x2)/2.0, (y1 + y2)/2.0\\n\\t\\t\\tpoints = [((mid_x, y1), (mid_x, y2)), ((x1, mid_y), (x2, mid_y))]\\n\\t\\t\\t#source connector -\\u003e points[0][0] should be in the direction of source (if possible)\\n\\t\\t\\tif Utils.get_angle_from_coordinates((x1, y1), points[0][0]) != source.get_connector_direction(): points.reverse()\\n\\t\\t\\t#points[0][0] -\\u003e sink connector should not be in the direction of sink\\n\\t\\t\\tif Utils.get_angle_from_coordinates(points[0][0], (x2, y2)) == sink.get_connector_direction(): points.reverse()\\n\\t\\t\\t#points[0][0] -\\u003e source connector should not be in the direction of source\\n\\t\\t\\tif Utils.get_angle_from_coordinates(points[0][0], (x1, y1)) == source.get_connector_direction(): points.reverse()\\n\\t\\t\\t#create 3-line connector\\n\\t\\t\\tp1, p2 = map(int, points[0][0]), map(int, points[0][1])\\n\\t\\t\\tself.add_line((x1, y1), p1)\\n\\t\\t\\tself.add_line(p1, p2)\\n\\t\\t\\tself.add_line((x2, y2), p2)\\n\\t\\telse:\\n\\t\\t\\t#2 possible points to create a right-angled connector\\n\\t\\t\\tpoints = [(x1, y2), (x2, y1)]\\n\\t\\t\\t#source connector -\\u003e points[0] should be in the direction of source (if possible)\\n\\t\\t\\tif Utils.get_angle_from_coordinates((x1, y1), points[0]) != source.get_connector_direction(): points.reverse()\\n\\t\\t\\t#points[0] -\\u003e sink connector should not be in the direction of sink\\n\\t\\t\\tif Utils.get_angle_from_coordinates(points[0], (x2, y2)) == sink.get_connector_direction(): points.reverse()\\n\\t\\t\\t#points[0] -\\u003e source connector should not be in the direction of source\\n\\t\\t\\tif Utils.get_angle_from_coordinates(points[0], (x1, y1)) == source.get_connector_direction(): points.reverse()\\n\\t\\t\\t#create right-angled connector\\n\\t\\t\\tself.add_line((x1, y1), points[0])\\n\\t\\t\\tself.add_line((x2, y2), points[0])\\n\\n\\tdef draw(self, gc, window):\\n\\t\\t\\"\\"\\"\\n\\t\\tDraw the connection.\\n\\t\\t@param gc the graphics context\\n\\t\\t@param window the gtk window to draw on\\n\\t\\t\\"\\"\\"\\n\\t\\tsink = self.get_sink()\\n\\t\\tsource = self.get_source()\\n\\t\\t#check for changes\\n\\t\\tif self._sink_rot != sink.get_rotation() or self._source_rot != source.get_rotation(): self.create_shapes()\\n\\t\\telif self._sink_coor != sink.get_coordinate() or self._source_coor != source.get_coordinate(): self._update_after_move()\\n\\t\\t#cache values\\n\\t\\tself._sink_rot = sink.get_rotation()\\n\\t\\tself._source_rot = source.get_rotation()\\n\\t\\tself._sink_coor = sink.get_coordinate()\\n\\t\\tself._source_coor = source.get_coordinate()\\n\\t\\t#draw\\n\\t\\tif self.is_highlighted(): border_color = Colors.HIGHLIGHT_COLOR\\n\\t\\telif self.get_enabled(): border_color = Colors.CONNECTION_ENABLED_COLOR\\n\\t\\telse: border_color = Colors.CONNECTION_DISABLED_COLOR\\n\\t\\tElement.draw(self, gc, window, bg_color=None, border_color=border_color)\\n\\t\\t#draw arrow on sink port\\n\\t\\tgc.set_foreground(self._arrow_color)\\n\\t\\twindow.draw_polygon(gc, True, self._arrow)\\n"}\n'
line: b'{"repo_name":"agabrown/PyGaia","ref":"refs/heads/master","path":"pygaia/utils.py","content":"__all__ = [\'enum\', \'degreesToRadians\', \'radiansToDegrees\']\\n\\nimport numpy as np\\n\\nfrom pygaia.astrometry.constants import auKmYearPerSec\\n\\ndef enum(typename, field_names):\\n    \\"\\"\\"\\n    Create a new enumeration type.\\n  \\n    Code is copyright (c) Gabriel Genellina, 2010, MIT License.\\n\\n    Parameters\\n    ----------\\n\\n    typename - Name of the enumerated type\\n    field_names - Names of the fields of the enumerated type\\n    \\"\\"\\"\\n\\n    if isinstance(field_names, str):\\n        field_names = field_names.replace(\',\', \' \').split()\\n    d = dict((reversed(nv) for nv in enumerate(field_names)), __slots__ = ())\\n    return type(typename, (object,), d)()\\n\\ndef degreesToRadians(angle):\\n    \\"\\"\\"\\n    Convert from degrees to radians.\\n\\n    Parameters\\n    ----------\\n\\n    angle - angle in degrees\\n\\n    Returns\\n    -------\\n\\n    Angle in radians.\\n    \\"\\"\\"\\n    return angle/180.0*np.pi\\n\\ndef radiansToDegrees(angle):\\n    \\"\\"\\"\\n    Convert from radians to degrees.\\n\\n    Parameters\\n    ----------\\n\\n    angle - angle in radians.\\n\\n    Returns\\n    -------\\n \\n    Angle in degrees.\\n    \\"\\"\\"\\n    return angle/np.pi*180.0\\n\\ndef construct_covariance_matrix(cvec, parallax, radial_velocity, radial_velocity_error):\\n    \\"\\"\\"\\n    Take the astrometric parameter standard uncertainties and the uncertainty correlations as quoted in\\n    the Gaia catalogue and construct the covariance matrix.\\n\\n    Parameters\\n    ----------\\n\\n    cvec : array_like\\n        Array of shape (15,) (1 source) or (n,15) (n sources) for the astrometric parameter standard\\n        uncertainties and their correlations, as listed in the Gaia catalogue [ra_error, dec_error,\\n        parallax_error, pmra_error, pmdec_error, ra_dec_corr, ra_parallax_corr, ra_pmra_corr,\\n        ra_pmdec_corr, dec_parallax_corr, dec_pmra_corr, dec_pmdec_corr, parallax_pmra_corr,\\n        parallax_pmdec_corr, pmra_pmdec_corr]. Units are (mas^2, mas^2/yr, mas^2/yr^2).\\n    \\n    parallax : array_like (n elements)\\n        Source parallax (mas).\\n    \\n    radial_velocity : array_like (n elements)\\n        Source radial velocity (km/s, does not have to be from Gaia RVS!). If the radial velocity is not\\n        known it can be set to zero.\\n\\n    radial_velocity_error : array_like (n elements)\\n        Source radial velocity  uncertainty (km/s). If the radial velocity is not know this can be set to\\n        the radial velocity dispersion for the population the source was drawn from.\\n\\n    Returns\\n    -------\\n\\n    Covariance matrix as a 6x6 array.\\n    \\"\\"\\"\\n\\n    if np.ndim(cvec)==1:\\n        cmat = np.zeros((1,6,6))\\n        nsources = 1\\n        cv = np.atleast_2d(cvec)\\n    else:\\n        nsources = cvec.shape[0]\\n        cmat = np.zeros((nsources,6,6))\\n        cv = cvec\\n    for k in range(nsources):\\n        cmat[k,0:5,0:5] = cv[k,0:5]**2\\n\\n    iu = np.triu_indices(5,k=1)\\n    for k in range(10):\\n        i = iu[0][k]\\n        j = iu[1][k]\\n        cmat[:,i,j] = cv[:,i]*cv[:,j]*cv[:,k+5]\\n        cmat[:,j,i] = cmat[:,i,j]\\n\\n    for k in range(nsources):\\n        cmat[k,0:5,5] = cmat[k,0:5,2]*np.atleast_1d(radial_velocity)[k]/auKmYearPerSec\\n    cmat[:,5,0:5] = cmat[:,0:5,5]\\n    cmat[:,5,5] = cmat[:,2,2]*(radial_velocity**2 + radial_velocity_error**2)/auKmYearPerSec**2 + \\\\\\n            (parallax*radial_velocity_error/auKmYearPerSec)**2\\n\\n    return np.squeeze(cmat)\\n"}\n'
line: b'{"repo_name":"brandonivey/django-marimo","ref":"refs/heads/master","path":"marimo/templatetags/writecapture.py","content":"import json\\nimport random\\n\\nfrom django import template\\n\\nimport logging\\nlogger = logging.getLogger(__name__)\\n\\nregister = template.Library()\\n\\ndef jsescape(string):\\n    \\"\\"\\" escaping so that javascript can be safely put into json dicts\\n        for some reason json newline escaping isn\'t enough??\\n    \\"\\"\\"\\n    return string.replace(\'\\u003cscript\',\'$BEGINSCRIPT\').replace(\'\\u003c/script\\u003e\',\'$ENDSCRIPT\').replace(\'\\\\n\', \'$NEWLINE\').replace(\'\\\\r\',\'\')\\n\\n@register.tag(name=\'writecapture\')\\ndef write_capture(parser, token):\\n    \\"\\"\\"\\n        Syntax::\\n            {% writecapture [filter] [\\"prototype\\"] [\\"widget_id\\"] %}\\n                \\u003cscript src=\\"evil.js\\"\\u003e\\n                    document.write(\'this is evil\')\\n                \\u003cscript\\u003e\\n            {% endwritecapture %}\\n\\n        Wraps the enclosed HTML inside of a marimo writecapture widget.\\n\\n        The ``filter`` argument is a boolean (default False) that turns on a\\n        writecapture feature called writeOnGetElementById. This fixes some\\n        extra-bad scripts.\\n\\n        The ``prototype`` argument defaults to \'writecapture.\' You will only\\n        need to use this if you have subclassed marimo\'s built-in writecapture\\n        widget and want to use that instead.\\n\\n        The ``widget_id`` argument defaults to a \'writecapture_\\u003crandomnumber\\u003e.\'\\n        Use this only if you need to specify an alternate element id in the DOM\\n        to write to (otherwise one will be created for you at the site of the\\n        {%writecapture%} invocation)..\\n\\n    \\"\\"\\"\\n    # TODO should work with marimo fast and widget_id should be resolved maybe\\n    tokens = token.split_contents()\\n    if len(tokens) \\u003e 4:\\n        raise template.TemplateSyntaxError(\\"writecapture block takes at most 3 arguments\\")\\n    nodelist = parser.parse((\'endwritecapture\',))\\n    parser.delete_first_token()\\n\\n    if len(tokens) \\u003e 1:\\n        script_filter = tokens[1]\\n        if script_filter == \'False\':\\n            script_filter = False\\n        elif script_filter == \'True\':\\n            script_filter = True\\n        else:\\n            script_filter = template.Variable(script_filter)\\n    else:\\n        script_filter = False\\n\\n    return WriteCaptureNode(nodelist, script_filter, *tokens[2:])\\n\\nclass WriteCaptureNode(template.Node):\\n    def __init__(self, nodelist, script_filter=False, prototype=\'writecapture_widget\', widget_id=None):\\n        self.nodelist = nodelist\\n        self.script_filter = script_filter\\n        self.prototype = prototype\\n        self.widget_id = widget_id\\n        if not self.widget_id:\\n            self.widget_id = \'writecapture\' + str(random.randint(0,99999999))\\n\\n    def render(self, context):\\n        eviloutput = jsescape(self.nodelist.render(context))\\n        if isinstance(self.script_filter, template.Variable):\\n            self.script_filter = bool(self.script_filter.resolve(context))\\n        # Set this flag in your template tag for advanced write capture widget sanitation.\\n        # Source: https://github.com/iamnoah/writeCapture/wiki/Usage\\n\\n        global_compatibility_mode = context.get(\'wc_compatibility_mode\', None)\\n        if global_compatibility_mode is None:\\n            wc_compatibility_mode = self.script_filter\\n        else:\\n            wc_compatibility_mode = global_compatibility_mode\\n\\n        widget_dict = dict(widget_prototype=self.prototype,\\n                            id=self.widget_id,\\n                            html=eviloutput,\\n                            wc_compatibility_mode = wc_compatibility_mode,\\n                         )\\n        output = \\"\\"\\"\\u003cdiv id=\\"{widget_id}\\"\\u003e\\u003c/div\\u003e\\n\\u003cscript type=\\"text/javascript\\"\\u003e\\n    marimo.emit(\'{widget_id}_ready\');\\n    marimo.add_widget({widget_json});\\n\\u003c/script\\u003e\\"\\"\\"\\n        output = output.format(\\n            widget_id=self.widget_id,\\n            widget_json=json.dumps(widget_dict),\\n        )\\n        return output\\n\\n@register.tag(name=\'writecapture_delay\')\\ndef write_capture_delay(parser, token):\\n    \\"\\"\\"\\n        Syntax::\\n            {% writecapture_delay [event_name] %}\\n    \\"\\"\\"\\n    tokens = token.split_contents()\\n    if len(tokens) \\u003e 2:\\n        raise template.TemplateSyntaxError(\\"writecapture_delay takes at most 1 argument\\")\\n    if len(tokens) == 2:\\n        return WriteCaptureDelayNode(tokens[1])\\n    return WriteCaptureDelayNode()\\n\\nclass WriteCaptureDelayNode(template.Node):\\n    def __init__(self, event=None):\\n        self.event = event\\n\\n    def render(self, context):\\n        output = \'\'\\n        if self.event is None:\\n            self.event = \'write_\' + str(random.randint(0,999999))\\n            output = \\"\\"\\"\\u003cscript type=\\"text/javascript\\"\\u003emarimo.emit(\'%s\');\\u003c/script\\u003e\\"\\"\\" % self.event\\n\\n        # this should only be used once per page if it\'s uses a second time\\n        # overwrite but log an error\\n        wc_delay = context.get(\'marimo_writecapture_delay\', None)\\n        if not wc_delay:\\n            logger.error(\\"The writecapture_delay was called but didn\'t find \\"\\n                         \\"marimo_writecapture_delay in the context. The tag \\"\\n                         \\"depends on the Marimo middleware and context_processor.\\")\\n            return output\\n        if wc_delay.marimo_event:\\n            logger.error(\'Overwriting the marimo event delay %s with %s\' %\\n                         (wc_delay.marimo_event, self.event))\\n        wc_delay.marimo_event = self.event\\n        return output\\n\\n@register.tag(name=\'writecapture_delay\')\\ndef write_capture_delay(parser, token):\\n    \\"\\"\\"\\n        Syntax::\\n            {% writecapture_delay [event_name] %}\\n    \\"\\"\\"\\n    tokens = token.split_contents()\\n    if len(tokens) \\u003e 2:\\n        raise template.TemplateSyntaxError(\\"writecapture_delay takes at most 1 argument\\")\\n    if len(tokens) == 2:\\n        return WriteCaptureDelayNode(tokens[1])\\n    return WriteCaptureDelayNode()\\n\\nclass WriteCaptureDelayNode(template.Node):\\n    def __init__(self, event=None):\\n        self.event = event\\n\\n    def render(self, context):\\n        output = \'\'\\n        if self.event is None:\\n            self.event = \'write_\' + str(random.randint(0,999999))\\n            output = \\"\\"\\"\\u003cscript type=\\"text/javascript\\"\\u003emarimo.emit(\'%s\');\\u003c/script\\u003e\\"\\"\\" % self.event\\n\\n        # this should only be used once per page if it\'s uses a second time\\n        # overwrite but log an error\\n        wc_delay = context.get(\'marimo_writecapture_delay\', None)\\n        if not wc_delay:\\n            logger.error(\\"The writecapture_delay was called but didn\'t find \\"\\n                         \\"marimo_writecapture_delay in the context. The tag \\"\\n                         \\"depends on the Marimo middleware and context_processor.\\")\\n            return output\\n        if wc_delay.marimo_event:\\n            logger.error(\'Overwriting the marimo event delay %s with %s\' %\\n                         (wc_delay.marimo_event, self.event))\\n        wc_delay.marimo_event = self.event\\n        return output\\n\\n@register.tag(name=\'writecapture_delay\')\\ndef write_capture_delay(parser, token):\\n    \\"\\"\\"\\n        Syntax::\\n            {% writecapture_delay [event_name] %}\\n    \\"\\"\\"\\n    tokens = token.split_contents()\\n    if len(tokens) \\u003e 2:\\n        raise template.TemplateSyntaxError(\\"writecapture_delay takes at most 1 argument\\")\\n    if len(tokens) == 2:\\n        return WriteCaptureDelayNode(tokens[1])\\n    return WriteCaptureDelayNode()\\n\\nclass WriteCaptureDelayNode(template.Node):\\n    def __init__(self, event=None):\\n        self.event = event\\n\\n    def render(self, context):\\n        output = \'\'\\n        if self.event is None:\\n            self.event = \'write_\' + str(random.randint(0,999999))\\n            output = \\"\\"\\"\\u003cscript type=\\"text/javascript\\"\\u003emarimo.emit(\'%s\');\\u003c/script\\u003e\\"\\"\\" % self.event\\n\\n        # this should only be used once per page if it\'s uses a second time\\n        # overwrite but log an error\\n        wc_delay = context.get(\'marimo_writecapture_delay\', None)\\n        if not wc_delay:\\n            logger.error(\\"The writecapture_delay was called but didn\'t find \\"\\n                         \\"marimo_writecapture_delay in the context. The tag \\"\\n                         \\"depends on the Marimo middleware and context_processor.\\")\\n            return output\\n        if wc_delay.marimo_event:\\n            logger.error(\'Overwriting the marimo event delay %s with %s\' %\\n                         (wc_delay.marimo_event, self.event))\\n        wc_delay.marimo_event = self.event\\n        return output\\n"}\n'
line: b'{"repo_name":"jonathonwalz/ansible","ref":"refs/heads/devel","path":"test/units/modules/network/f5/test_bigip_iapp_service.py","content":"# -*- coding: utf-8 -*-\\n#\\n# Copyright 2017 F5 Networks Inc.\\n#\\n# This file is part of Ansible\\n#\\n# Ansible is free software: you can redistribute it and/or modify\\n# it under the terms of the GNU General Public License as published by\\n# the Free Software Foundation, either version 3 of the License, or\\n# (at your option) any later version.\\n#\\n# Ansible is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU General Public License for more details.\\n#\\n# You should have received a copy of the GNU General Public License\\n# along with Ansible.  If not, see \\u003chttp://www.gnu.org/licenses/\\u003e.\\n\\nfrom __future__ import (absolute_import, division, print_function)\\n__metaclass__ = type\\n\\nimport os\\nimport json\\nimport sys\\n\\nfrom nose.plugins.skip import SkipTest\\nif sys.version_info \\u003c (2, 7):\\n    raise SkipTest(\\"F5 Ansible modules require Python \\u003e= 2.7\\")\\n\\nfrom ansible.compat.tests import unittest\\nfrom ansible.compat.tests.mock import patch, Mock\\nfrom ansible.module_utils import basic\\nfrom ansible.module_utils._text import to_bytes\\nfrom ansible.module_utils.f5_utils import AnsibleF5Client\\n\\ntry:\\n    from library.bigip_iapp_service import Parameters\\n    from library.bigip_iapp_service import ModuleManager\\n    from library.bigip_iapp_service import ArgumentSpec\\nexcept ImportError:\\n    try:\\n        from ansible.modules.network.f5.bigip_iapp_service import Parameters\\n        from ansible.modules.network.f5.bigip_iapp_service import ModuleManager\\n        from ansible.modules.network.f5.bigip_iapp_service import ArgumentSpec\\n    except ImportError:\\n        raise SkipTest(\\"F5 Ansible modules require the f5-sdk Python library\\")\\n\\nfixture_path = os.path.join(os.path.dirname(__file__), \'fixtures\')\\nfixture_data = {}\\n\\n\\ndef set_module_args(args):\\n    args = json.dumps({\'ANSIBLE_MODULE_ARGS\': args})\\n    basic._ANSIBLE_ARGS = to_bytes(args)\\n\\n\\ndef load_fixture(name):\\n    path = os.path.join(fixture_path, name)\\n\\n    if path in fixture_data:\\n        return fixture_data[path]\\n\\n    with open(path) as f:\\n        data = f.read()\\n\\n    try:\\n        data = json.loads(data)\\n    except Exception:\\n        pass\\n\\n    fixture_data[path] = data\\n    return data\\n\\n\\nclass TestParameters(unittest.TestCase):\\n\\n    def test_module_parameters_keys(self):\\n        args = load_fixture(\'create_iapp_service_parameters_f5_http.json\')\\n        p = Parameters(args)\\n\\n        # Assert the top-level keys\\n        assert p.name == \'http_example\'\\n        assert p.partition == \'Common\'\\n        assert p.template == \'/Common/f5.http\'\\n\\n    def test_module_parameters_lists(self):\\n        args = load_fixture(\'create_iapp_service_parameters_f5_http.json\')\\n        p = Parameters(args)\\n\\n        assert \'lists\' in p._values\\n\\n        assert p.lists[0][\'name\'] == \'irules__irules\'\\n        assert p.lists[0][\'encrypted\'] == \'no\'\\n        assert len(p.lists[0][\'value\']) == 1\\n        assert p.lists[0][\'value\'][0] == \'/Common/lgyft\'\\n\\n        assert p.lists[1][\'name\'] == \'net__client_vlan\'\\n        assert p.lists[1][\'encrypted\'] == \'no\'\\n        assert len(p.lists[1][\'value\']) == 1\\n        assert p.lists[1][\'value\'][0] == \'/Common/net2\'\\n\\n    def test_module_parameters_tables(self):\\n        args = load_fixture(\'create_iapp_service_parameters_f5_http.json\')\\n        p = Parameters(args)\\n\\n        assert \'tables\' in p._values\\n\\n        assert \'columnNames\' in p.tables[0]\\n        assert len(p.tables[0][\'columnNames\']) == 1\\n        assert p.tables[0][\'columnNames\'][0] == \'name\'\\n\\n        assert \'name\' in p.tables[0]\\n        assert p.tables[0][\'name\'] == \'pool__hosts\'\\n\\n        assert \'rows\' in p.tables[0]\\n        assert len(p.tables[0][\'rows\']) == 1\\n        assert \'row\' in p.tables[0][\'rows\'][0]\\n        assert len(p.tables[0][\'rows\'][0][\'row\']) == 1\\n        assert p.tables[0][\'rows\'][0][\'row\'][0] == \'demo.example.com\'\\n\\n        assert len(p.tables[1][\'rows\']) == 2\\n        assert \'row\' in p.tables[0][\'rows\'][0]\\n        assert len(p.tables[1][\'rows\'][0][\'row\']) == 2\\n        assert p.tables[1][\'rows\'][0][\'row\'][0] == \'10.1.1.1\'\\n        assert p.tables[1][\'rows\'][0][\'row\'][1] == \'0\'\\n        assert p.tables[1][\'rows\'][1][\'row\'][0] == \'10.1.1.2\'\\n        assert p.tables[1][\'rows\'][1][\'row\'][1] == \'0\'\\n\\n    def test_module_parameters_variables(self):\\n        args = load_fixture(\'create_iapp_service_parameters_f5_http.json\')\\n        p = Parameters(args)\\n\\n        assert \'variables\' in p._values\\n        assert len(p.variables) == 34\\n\\n        # Assert one configuration value\\n        assert \'name\' in p.variables[0]\\n        assert \'value\' in p.variables[0]\\n        assert p.variables[0][\'name\'] == \'afm__dos_security_profile\'\\n        assert p.variables[0][\'value\'] == \'/#do_not_use#\'\\n\\n        # Assert a second configuration value\\n        assert \'name\' in p.variables[1]\\n        assert \'value\' in p.variables[1]\\n        assert p.variables[1][\'name\'] == \'afm__policy\'\\n        assert p.variables[1][\'value\'] == \'/#do_not_use#\'\\n\\n    def test_api_parameters_variables(self):\\n        args = dict(\\n            variables=[\\n                dict(\\n                    name=\\"client__http_compression\\",\\n                    encrypted=\\"no\\",\\n                    value=\\"/#create_new#\\"\\n                )\\n            ]\\n        )\\n        p = Parameters(args)\\n        assert p.variables[0][\'name\'] == \'client__http_compression\'\\n\\n    def test_api_parameters_tables(self):\\n        args = dict(\\n            tables=[\\n                {\\n                    \\"name\\": \\"pool__members\\",\\n                    \\"columnNames\\": [\\n                        \\"addr\\",\\n                        \\"port\\",\\n                        \\"connection_limit\\"\\n                    ],\\n                    \\"rows\\": [\\n                        {\\n                            \\"row\\": [\\n                                \\"12.12.12.12\\",\\n                                \\"80\\",\\n                                \\"0\\"\\n                            ]\\n                        },\\n                        {\\n                            \\"row\\": [\\n                                \\"13.13.13.13\\",\\n                                \\"443\\",\\n                                10\\n                            ]\\n                        }\\n                    ]\\n                }\\n            ]\\n        )\\n        p = Parameters(args)\\n        assert p.tables[0][\'name\'] == \'pool__members\'\\n        assert p.tables[0][\'columnNames\'] == [\'addr\', \'port\', \'connection_limit\']\\n        assert len(p.tables[0][\'rows\']) == 2\\n        assert \'row\' in p.tables[0][\'rows\'][0]\\n        assert \'row\' in p.tables[0][\'rows\'][1]\\n        assert p.tables[0][\'rows\'][0][\'row\'] == [\'12.12.12.12\', \'80\', \'0\']\\n        assert p.tables[0][\'rows\'][1][\'row\'] == [\'13.13.13.13\', \'443\', \'10\']\\n\\n    def test_module_template_same_partition(self):\\n        args = dict(\\n            template=\'foo\',\\n            partition=\'bar\'\\n        )\\n        p = Parameters(args)\\n        assert p.template == \'/bar/foo\'\\n\\n    def test_module_template_same_partition_full_path(self):\\n        args = dict(\\n            template=\'/bar/foo\',\\n            partition=\'bar\'\\n        )\\n        p = Parameters(args)\\n        assert p.template == \'/bar/foo\'\\n\\n    def test_module_template_different_partition_full_path(self):\\n        args = dict(\\n            template=\'/Common/foo\',\\n            partition=\'bar\'\\n        )\\n        p = Parameters(args)\\n        assert p.template == \'/Common/foo\'\\n\\n\\n@patch(\'ansible.module_utils.f5_utils.AnsibleF5Client._get_mgmt_root\',\\n       return_value=True)\\nclass TestManager(unittest.TestCase):\\n\\n    def setUp(self):\\n        self.spec = ArgumentSpec()\\n\\n    def test_create_service(self, *args):\\n        parameters = load_fixture(\'create_iapp_service_parameters_f5_http.json\')\\n        set_module_args(dict(\\n            name=\'foo\',\\n            template=\'f5.http\',\\n            parameters=parameters,\\n            state=\'present\',\\n            password=\'passsword\',\\n            server=\'localhost\',\\n            user=\'admin\'\\n        ))\\n\\n        client = AnsibleF5Client(\\n            argument_spec=self.spec.argument_spec,\\n            supports_check_mode=self.spec.supports_check_mode,\\n            f5_product_name=self.spec.f5_product_name\\n        )\\n        mm = ModuleManager(client)\\n\\n        # Override methods to force specific logic in the module to happen\\n        mm.exists = Mock(return_value=False)\\n        mm.create_on_device = Mock(return_value=True)\\n\\n        results = mm.exec_module()\\n        assert results[\'changed\'] is True\\n\\n    def test_update_agent_status_traps(self, *args):\\n        parameters = load_fixture(\'update_iapp_service_parameters_f5_http.json\')\\n        set_module_args(dict(\\n            name=\'foo\',\\n            template=\'f5.http\',\\n            parameters=parameters,\\n            state=\'present\',\\n            password=\'passsword\',\\n            server=\'localhost\',\\n            user=\'admin\'\\n        ))\\n\\n        # Configure the parameters that would be returned by querying the\\n        # remote device\\n        parameters = load_fixture(\'create_iapp_service_parameters_f5_http.json\')\\n        current = Parameters(parameters)\\n\\n        client = AnsibleF5Client(\\n            argument_spec=self.spec.argument_spec,\\n            supports_check_mode=self.spec.supports_check_mode,\\n            f5_product_name=self.spec.f5_product_name\\n        )\\n        mm = ModuleManager(client)\\n\\n        # Override methods to force specific logic in the module to happen\\n        mm.exists = Mock(return_value=True)\\n        mm.update_on_device = Mock(return_value=True)\\n        mm.read_current_from_device = Mock(return_value=current)\\n\\n        results = mm.exec_module()\\n        assert results[\'changed\'] is True\\n"}\n'
line: b'{"repo_name":"eaglexmw/seascope","ref":"refs/heads/master","path":"src/view/filecontext/plugins/ctags_view/CtagsManager.py","content":"# Copyright (c) 2010 Anil Kumar\\n# All rights reserved.\\n#\\n# License: BSD \\n\\nimport subprocess\\nimport re, os\\n\\ndef _eintr_retry_call(func, *args):\\n\\twhile True:\\n\\t\\ttry:\\n\\t\\t\\treturn func(*args)\\n\\t\\texcept OSError, e:\\n\\t\\t\\tif e.errno == errno.EINTR:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\traise\\n\\ndef cmdForFile(f):\\n\\tsuffix_cmd_map = []\\n\\tcustom_map = os.getenv(\'SEASCOPE_CTAGS_SUFFIX_CMD_MAP\')\\n\\tif custom_map:\\n\\t\\tcustom_map = eval(custom_map)\\n\\t\\tsuffix_cmd_map += custom_map\\n\\t#args = \'ctags -n -u --fields=+K -f - --extra=+q\'\\n\\t#args = \'ctags -n -u --fields=+Ki -f -\'\\n\\targs = \'ctags -n -u --fields=+K -f -\'\\n\\tsuffix_cmd_map.append( [\'\', args] )\\n\\tfor (suffix, cmd) in suffix_cmd_map:\\n\\t\\tif f.endswith(suffix):\\n\\t\\t\\treturn cmd\\n\\treturn None\\n\\ndef ct_query(filename):\\n\\targs = cmdForFile(filename)\\n\\targs = args.split()\\n\\targs.append(filename)\\n\\ttry:\\n\\t\\tproc = subprocess.Popen(args, stdout=subprocess.PIPE)\\n\\t\\t(out_data, err_data) = _eintr_retry_call(proc.communicate)\\n\\t\\tout_data = out_data.split(\'\\\\n\')\\n\\texcept Exception as e:\\n\\t\\tout_data =  [\\n\\t\\t\\t\\t\'Failed to run ctags cmd\\\\tignore\\\\t0;\\\\t \',\\n\\t\\t\\t\\t\'cmd: %s\\\\tignore\\\\t0;\\\\t \' % \' \'.join(args),\\n\\t\\t\\t\\t\'error: %s\\\\tignore\\\\t0;\\\\t \' % str(e),\\n\\t\\t\\t\\t\'ctags not installed ?\\\\tignore\\\\t0;\\\\t \',\\n\\t\\t\\t]\\n\\tres = []\\n\\tfor line in out_data:\\n\\t\\tif (line == \'\'):\\n\\t\\t\\tbreak\\n\\t\\tline = line.split(\'\\\\t\')\\n\\t\\tnum = line[2].split(\';\', 1)[0]\\n\\t\\tline = [line[0], num, line[3]]\\n\\t\\tres.append(line)\\n\\treturn res\\n\\nis_OrderedDict_available = False\\ntry:\\n\\t# OrderedDict available only in python \\u003e= 2.7\\n\\tfrom collections import OrderedDict\\n\\tis_OrderedDict_available = True\\nexcept:\\n\\tpass\\n\\ndef emptyOrderedDict():\\n\\tif is_OrderedDict_available:\\n\\t\\treturn OrderedDict({})\\n\\treturn {}\\n\\nclass CtagsTreeBuilder:\\n\\tdef __init__(self):\\n\\t\\tself.symTree = emptyOrderedDict()\\n\\n\\tdef cmdForFile(self, f):\\n\\t\\tsuffix_cmd_map = []\\n\\t\\tcustom_map = os.getenv(\'SEASCOPE_CTAGS_SUFFIX_CMD_MAP\')\\n\\t\\tif custom_map:\\n\\t\\t\\tcustom_map = eval(custom_map)\\n\\t\\t\\tsuffix_cmd_map += custom_map\\n\\t\\t#args = \'ctags -n -u --fields=+K -f - --extra=+q\'\\n\\t\\t#args = \'ctags -n -u --fields=+Ki -f -\'\\n\\t\\targs = \'ctags -n -u --fields=+K-f-t -f -\'\\n\\t\\tsuffix_cmd_map.append( [\'\', args] )\\n\\t\\tfor (suffix, cmd) in suffix_cmd_map:\\n\\t\\t\\tif f.endswith(suffix):\\n\\t\\t\\t\\treturn cmd\\n\\t\\treturn None\\n\\n\\tdef runCtags(self, f):\\n\\t\\targs = self.cmdForFile(f)\\n\\t\\targs = args.split()\\n\\t\\targs.append(f)\\n\\t\\t# In python \\u003e= 2.7 can use subprocess.check_output\\n\\t\\t# output = subprocess.check_output(args)\\n\\t\\t# return output\\n\\t\\tproc = subprocess.Popen(args, stdout=subprocess.PIPE)\\n\\t\\t(out_data, err_data) = proc.communicate()\\n\\t\\treturn out_data\\n\\n\\tdef parseCtagsOutput(self, data):\\n\\t\\tdata = re.split(\'\\\\r?\\\\n\', data)\\n\\t\\tres = []\\n\\t\\tfor line in data:\\n\\t\\t\\tif line == \'\':\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tline = line.split(\'\\\\t\', 4)\\n\\t\\t\\t\\tres.append(line)\\n\\t\\t\\texcept:\\n\\t\\t\\t\\tprint \'bad line:\', line\\n\\t\\treturn res\\n\\n\\n\\tdef addToSymLayout(self, sc):\\n\\t\\tt = self.symTree\\n\\t\\tif sc and sc != \'\':\\n\\t\\t\\tfor s in re.split(\'::|\\\\.\', sc):\\n\\t\\t\\t\\tif s not in t:\\n\\t\\t\\t\\t\\tt[s] = emptyOrderedDict()\\n\\t\\t\\t\\tt = t[s]\\n\\n\\tdef addToSymTree(self, sc, line):\\n\\t\\tt = self.symTree\\n\\t\\tif sc and sc != \'\':\\n\\t\\t\\tfor s in re.split(\'::|\\\\.\', sc):\\n\\t\\t\\t\\tassert s in t\\n\\t\\t\\t\\tt = t[s]\\n\\n\\t\\tcline = [line[0], line[2].split(\';\')[0], line[3]]\\n\\t\\tif line[0] in t:\\n\\t\\t\\t#print line[0], \'in\', t\\n\\t\\t\\tx = t[line[0]]\\n\\t\\t\\tif \'+\' not in x:\\n\\t\\t\\t\\tx[\'+\'] = cline\\n\\t\\t\\t\\treturn\\n\\t\\tif \'*\' not in t:\\n\\t\\t\\tt[\'*\'] = []\\n\\t\\tt[\'*\'].append(cline)\\n\\t\\t#print \'...\', t, line\\n\\n\\tdef buildTree(self, data):\\n\\t\\ttype_list = [ \'namespace\', \'class\', \'interface\', \'struct\', \'union\', \'enum\', \'function\' ]\\n\\t\\t# build layout using 5th field\\n\\t\\tfor line in data:\\n\\t\\t\\tif len(line) == 4:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tsd = dict([ x.split(\':\', 1) for x in line[4].split(\'\\\\t\')])\\n\\t\\t\\texcept:\\n\\t\\t\\t\\tprint \'bad line\', line\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tline[4] = sd\\n\\t\\t\\tcount = 0\\n\\t\\t\\tfor t in type_list:\\n\\t\\t\\t\\tif t in sd:\\n\\t\\t\\t\\t\\tself.addToSymLayout(sd[t])\\n\\t\\t\\t\\t\\tcount = count + 1\\n\\t\\t\\tif count != 1:\\n\\t\\t\\t\\tprint \'******** count == 1 *********\'\\n\\t\\t\\t\\tprint data\\n\\t\\t\\t\\tprint line\\n\\t\\t\\t#assert count == 1\\n\\t\\t\\n\\t\\tif len(self.symTree) == 0:\\n\\t\\t\\treturn (data, False)\\n\\t\\t\\n\\t\\tfor line in data:\\n\\t\\t\\tif len(line) == 4:\\n\\t\\t\\t\\tself.addToSymTree(None, line)\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tsd = line[4]\\n\\t\\t\\tcount = 0\\n\\t\\t\\tfor t in type_list:\\n\\t\\t\\t\\tif t in sd:\\n\\t\\t\\t\\t\\tself.addToSymTree(sd[t], line)\\n\\t\\t\\t\\t\\tcount = count + 1\\n\\t\\t\\tif count != 1:\\n\\t\\t\\t\\tprint \'******** count == 1 *********\'\\n\\t\\t\\t\\tprint data\\n\\t\\t\\t\\tprint line\\n\\t\\t\\t#assert count == 1\\n\\n\\t\\treturn (self.symTree, True)\\n\\n\\tdef doQuery(self, filename):\\n\\t\\ttry:\\n\\t\\t\\toutput = self.runCtags(filename)\\n\\t\\t\\toutput = self.parseCtagsOutput(output)\\n\\t\\t\\toutput = self.buildTree(output)\\n\\t\\texcept Exception as e:\\n\\t\\t\\tprint str(e)\\n\\t\\t\\toutput = [None, False]\\n\\t\\treturn output\\n\\n\\ndef ct_tree_query(filename):\\n\\tct = CtagsTreeBuilder()\\n\\toutput = ct.doQuery(filename)\\n\\treturn output\\n\\nif __name__ == \'__main__\':\\n\\timport optparse\\n\\timport sys\\n\\tdepth = 0\\n\\tdef recursePrint(t):\\n\\t\\tglobal depth\\n\\t\\tfor k, v in t.items():\\n\\t\\t\\tif k == \'*\':\\n\\t\\t\\t\\tfor line in v:\\n\\t\\t\\t\\t\\tprint \'%s%s\' % (\' \' * depth, line)\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tif k == \'+\':\\n\\t\\t\\t\\tcontinue\\n\\n\\t\\t\\tif \'+\' in v:\\n\\t\\t\\t\\tk = v[\'+\']\\n\\t\\t\\tprint \'%s%s\' % (\' \' * depth, k)\\n\\t\\t\\t\\t\\n\\t\\t\\tdepth = depth + 4\\n\\t\\t\\trecursePrint(v)\\n\\t\\t\\tdepth = depth - 4\\n\\n\\top = optparse.OptionParser()\\n\\t(options, args) = op.parse_args()\\n\\tif len(args) != 1:\\n\\t\\tprint \'Please specify a file\'\\n\\t\\tsys.exit(-1)\\n\\n\\t(output, isTree) = ct_tree_query(args[0])\\n\\tif isTree:\\n\\t\\trecursePrint(output)\\n\\telse:\\n\\t\\tfor line in output:\\n\\t\\t\\tprint line\\n\\n"}\n'
line: b'{"repo_name":"Thhhza/XlsxWriter","ref":"refs/heads/master","path":"xlsxwriter/test/comparison/test_chart_column04.py","content":"###############################################################################\\n#\\n# Tests for XlsxWriter.\\n#\\n# Copyright (c), 2013-2015, John McNamara, jmcnamara@cpan.org\\n#\\n\\nfrom ..excel_comparsion_test import ExcelComparisonTest\\nfrom ...workbook import Workbook\\n\\n\\nclass TestCompareXLSXFiles(ExcelComparisonTest):\\n    \\"\\"\\"\\n    Test file created by XlsxWriter against a file created by Excel.\\n\\n    \\"\\"\\"\\n\\n    def setUp(self):\\n        self.maxDiff = None\\n\\n        filename = \'chart_column04.xlsx\'\\n\\n        test_dir = \'xlsxwriter/test/comparison/\'\\n        self.got_filename = test_dir + \'_test_\' + filename\\n        self.exp_filename = test_dir + \'xlsx_files/\' + filename\\n\\n        self.ignore_files = []\\n        self.ignore_elements = {\'xl/workbook.xml\': [\'\\u003cfileVersion\', \'\\u003ccalcPr\']}\\n\\n    def test_create_file(self):\\n        \\"\\"\\"Test the creation of a simple XlsxWriter file.\\"\\"\\"\\n\\n        workbook = Workbook(self.got_filename)\\n\\n        worksheet = workbook.add_worksheet()\\n        chart = workbook.add_chart({\'type\': \'column\'})\\n\\n        chart.axis_ids = [63591936, 63593856]\\n        chart.axis2_ids = [63613568, 63612032]\\n\\n        data = [[1, 2, 3, 4, 5],\\n                [6, 8, 6, 4, 2]]\\n\\n        worksheet.write_column(\'A1\', data[0])\\n        worksheet.write_column(\'B1\', data[1])\\n\\n        chart.add_series({\'values\': \'=Sheet1!$A$1:$A$5\'})\\n        chart.add_series({\'values\': \'=Sheet1!$B$1:$B$5\', \'y2_axis\': 1})\\n\\n        worksheet.insert_chart(\'E9\', chart)\\n\\n        workbook.close()\\n\\n        self.assertExcelEqual()\\n"}\n'
line: b'{"repo_name":"mbrukman/libcloud","ref":"refs/heads/trunk","path":"libcloud/test/compute/test_gogrid.py","content":"# Licensed to the Apache Software Foundation (ASF) under one or more\\n# contributor license agreements.  See the NOTICE file distributed with\\n# this work for additional information regarding copyright ownership.\\n# The ASF licenses this file to You under the Apache License, Version 2.0\\n# (the \\"License\\"); you may not use this file except in compliance with\\n# the License.  You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\nimport sys\\nimport unittest\\n\\nfrom libcloud.utils.py3 import httplib\\nfrom libcloud.utils.py3 import urlparse\\nfrom libcloud.utils.py3 import parse_qs\\n\\nfrom libcloud.compute.base import NodeState, NodeLocation\\nfrom libcloud.common.types import LibcloudError, InvalidCredsError\\nfrom libcloud.common.gogrid import GoGridIpAddress\\nfrom libcloud.compute.drivers.gogrid import GoGridNodeDriver\\nfrom libcloud.compute.base import Node, NodeImage, NodeSize\\n\\nfrom libcloud.test import MockHttp               # pylint: disable-msg=E0611\\nfrom libcloud.test.compute import TestCaseMixin  # pylint: disable-msg=E0611\\nfrom libcloud.test.file_fixtures import ComputeFileFixtures  # pylint: disable-msg=E0611\\n\\n\\nclass GoGridTests(unittest.TestCase, TestCaseMixin):\\n\\n    def setUp(self):\\n        GoGridNodeDriver.connectionCls.conn_classes = (None, GoGridMockHttp)\\n        GoGridMockHttp.type = None\\n        self.driver = GoGridNodeDriver(\\"foo\\", \\"bar\\")\\n\\n    def _get_test_512Mb_node_size(self):\\n        return NodeSize(id=\'512Mb\',\\n                        name=None,\\n                        ram=None,\\n                        disk=None,\\n                        bandwidth=None,\\n                        price=None,\\n                        driver=self.driver)\\n\\n    def test_create_node(self):\\n        image = NodeImage(1531, None, self.driver)\\n        node = self.driver.create_node(\\n            name=\'test1\',\\n            image=image,\\n            size=self._get_test_512Mb_node_size())\\n        self.assertEqual(node.name, \'test1\')\\n        self.assertTrue(node.id is not None)\\n        self.assertEqual(node.extra[\'password\'], \'bebebe\')\\n\\n    def test_list_nodes(self):\\n        node = self.driver.list_nodes()[0]\\n\\n        self.assertEqual(node.id, \'90967\')\\n        self.assertEqual(node.extra[\'password\'], \'bebebe\')\\n        self.assertEqual(node.extra[\'description\'], \'test server\')\\n\\n    def test_reboot_node(self):\\n        node = Node(90967, None, None, None, None, self.driver)\\n        ret = self.driver.reboot_node(node)\\n        self.assertTrue(ret)\\n\\n    def test_reboot_node_not_successful(self):\\n        GoGridMockHttp.type = \'FAIL\'\\n        node = Node(90967, None, None, None, None, self.driver)\\n\\n        try:\\n            self.driver.reboot_node(node)\\n        except Exception:\\n            pass\\n        else:\\n            self.fail(\'Exception was not thrown\')\\n\\n    def test_destroy_node(self):\\n        node = Node(90967, None, None, None, None, self.driver)\\n        ret = self.driver.destroy_node(node)\\n        self.assertTrue(ret)\\n\\n    def test_list_images(self):\\n        images = self.driver.list_images()\\n        image = images[0]\\n        self.assertEqual(len(images), 4)\\n        self.assertEqual(image.name, \'CentOS 5.3 (32-bit) w/ None\')\\n        self.assertEqual(image.id, \'1531\')\\n\\n        location = NodeLocation(\\n            id=\'gogrid/GSI-939ef909-84b8-4a2f-ad56-02ccd7da05ff.img\',\\n            name=\'test location\', country=\'Slovenia\',\\n            driver=self.driver)\\n        images = self.driver.list_images(location=location)\\n        image = images[0]\\n        self.assertEqual(len(images), 4)\\n        self.assertEqual(image.name, \'CentOS 5.3 (32-bit) w/ None\')\\n        self.assertEqual(image.id, \'1531\')\\n\\n    def test_malformed_reply(self):\\n        GoGridMockHttp.type = \'FAIL\'\\n        try:\\n            self.driver.list_images()\\n        except LibcloudError:\\n            e = sys.exc_info()[1]\\n            self.assertTrue(isinstance(e, LibcloudError))\\n        else:\\n            self.fail(\\"test should have thrown\\")\\n\\n    def test_invalid_creds(self):\\n        GoGridMockHttp.type = \'FAIL\'\\n        try:\\n            self.driver.list_nodes()\\n        except InvalidCredsError:\\n            e = sys.exc_info()[1]\\n            self.assertTrue(e.driver is not None)\\n            self.assertEqual(e.driver.name, self.driver.name)\\n        else:\\n            self.fail(\\"test should have thrown\\")\\n\\n    def test_node_creation_without_free_public_ips(self):\\n        GoGridMockHttp.type = \'NOPUBIPS\'\\n        try:\\n            image = NodeImage(1531, None, self.driver)\\n            self.driver.create_node(\\n                name=\'test1\',\\n                image=image,\\n                size=self._get_test_512Mb_node_size())\\n        except LibcloudError:\\n            e = sys.exc_info()[1]\\n            self.assertTrue(isinstance(e, LibcloudError))\\n            self.assertTrue(e.driver is not None)\\n            self.assertEqual(e.driver.name, self.driver.name)\\n        else:\\n            self.fail(\\"test should have thrown\\")\\n\\n    def test_list_locations(self):\\n        locations = self.driver.list_locations()\\n        location_names = [location.name for location in locations]\\n\\n        self.assertEqual(len(locations), 2)\\n        for i in 0, 1:\\n            self.assertTrue(isinstance(locations[i], NodeLocation))\\n        self.assertTrue(\\"US-West-1\\" in location_names)\\n        self.assertTrue(\\"US-East-1\\" in location_names)\\n\\n    def test_ex_save_image(self):\\n        node = self.driver.list_nodes()[0]\\n        image = self.driver.ex_save_image(node, \\"testimage\\")\\n        self.assertEqual(image.name, \\"testimage\\")\\n\\n    def test_ex_edit_image(self):\\n        image = self.driver.list_images()[0]\\n        ret = self.driver.ex_edit_image(image=image, public=False,\\n                                        ex_description=\\"test\\", name=\\"testname\\")\\n\\n        self.assertTrue(isinstance(ret, NodeImage))\\n\\n    def test_ex_edit_node(self):\\n        node = Node(id=90967, name=None, state=None,\\n                    public_ips=None, private_ips=None, driver=self.driver)\\n        ret = self.driver.ex_edit_node(node=node,\\n                                       size=self._get_test_512Mb_node_size())\\n\\n        self.assertTrue(isinstance(ret, Node))\\n\\n    def test_ex_list_ips(self):\\n        ips = self.driver.ex_list_ips()\\n\\n        expected_ips = {\\"192.168.75.66\\": GoGridIpAddress(id=\\"5348099\\",\\n                                                         ip=\\"192.168.75.66\\", public=True, state=\\"Unassigned\\",\\n                                                         subnet=\\"192.168.75.64/255.255.255.240\\"),\\n                        \\"192.168.75.67\\": GoGridIpAddress(id=\\"5348100\\",\\n                                                         ip=\\"192.168.75.67\\", public=True, state=\\"Assigned\\",\\n                                                         subnet=\\"192.168.75.64/255.255.255.240\\"),\\n                        \\"192.168.75.68\\": GoGridIpAddress(id=\\"5348101\\",\\n                                                         ip=\\"192.168.75.68\\", public=False, state=\\"Unassigned\\",\\n                                                         subnet=\\"192.168.75.64/255.255.255.240\\")}\\n\\n        self.assertEqual(len(expected_ips), 3)\\n\\n        for ip in ips:\\n            self.assertTrue(ip.ip in expected_ips)\\n            self.assertEqual(ip.public, expected_ips[ip.ip].public)\\n            self.assertEqual(ip.state, expected_ips[ip.ip].state)\\n            self.assertEqual(ip.subnet, expected_ips[ip.ip].subnet)\\n\\n            del expected_ips[ip.ip]\\n\\n        self.assertEqual(len(expected_ips), 0)\\n\\n    def test_get_state_invalid(self):\\n        state = self.driver._get_state(\'invalid\')\\n        self.assertEqual(state, NodeState.UNKNOWN)\\n\\n\\nclass GoGridMockHttp(MockHttp):\\n\\n    fixtures = ComputeFileFixtures(\'gogrid\')\\n\\n    def _api_grid_image_list(self, method, url, body, headers):\\n        body = self.fixtures.load(\'image_list.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    def _api_grid_image_list_FAIL(self, method, url, body, headers):\\n        body = \\"\\u003ch3\\u003esome non valid json here\\u003c/h3\\u003e\\"\\n        return (httplib.SERVICE_UNAVAILABLE, body, {},\\n                httplib.responses[httplib.SERVICE_UNAVAILABLE])\\n\\n    def _api_grid_server_list(self, method, url, body, headers):\\n        body = self.fixtures.load(\'server_list.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    _api_grid_server_list_NOPUBIPS = _api_grid_server_list\\n\\n    def _api_grid_server_list_FAIL(self, method, url, body, headers):\\n        return (httplib.FORBIDDEN,\\n                \\"123\\", {}, httplib.responses[httplib.FORBIDDEN])\\n\\n    def _api_grid_ip_list(self, method, url, body, headers):\\n        body = self.fixtures.load(\'ip_list.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    def _api_grid_ip_list_NOPUBIPS(self, method, url, body, headers):\\n        body = self.fixtures.load(\'ip_list_empty.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    def _api_grid_server_power(self, method, url, body, headers):\\n        body = self.fixtures.load(\'server_power.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    def _api_grid_server_power_FAIL(self, method, url, body, headers):\\n        body = self.fixtures.load(\'server_power_fail.json\')\\n        return (httplib.NOT_FOUND, body, {}, httplib.responses[httplib.OK])\\n\\n    def _api_grid_server_add(self, method, url, body, headers):\\n        body = self.fixtures.load(\'server_add.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    _api_grid_server_add_NOPUBIPS = _api_grid_server_add\\n\\n    def _api_grid_server_delete(self, method, url, body, headers):\\n        body = self.fixtures.load(\'server_delete.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    def _api_grid_server_edit(self, method, url, body, headers):\\n        body = self.fixtures.load(\'server_edit.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    def _api_support_password_list(self, method, url, body, headers):\\n        body = self.fixtures.load(\'password_list.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    _api_support_password_list_NOPUBIPS = _api_support_password_list\\n\\n    def _api_grid_image_save(self, method, url, body, headers):\\n        body = self.fixtures.load(\'image_save.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    def _api_grid_image_edit(self, method, url, body, headers):\\n        # edit method is quite similar to save method from the response\\n        # perspective\\n        body = self.fixtures.load(\'image_save.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    def _api_common_lookup_list(self, method, url, body, headers):\\n        _valid_lookups = (\\"ip.datacenter\\",)\\n\\n        lookup = parse_qs(urlparse.urlparse(url).query)[\\"lookup\\"][0]\\n        if lookup in _valid_lookups:\\n            fixture_path = \\"lookup_list_%s.json\\" % \\\\\\n                (lookup.replace(\\".\\", \\"_\\"))\\n        else:\\n            raise NotImplementedError\\n        body = self.fixtures.load(fixture_path)\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\nif __name__ == \'__main__\':\\n    sys.exit(unittest.main())\\n"}\n'
line: b'{"repo_name":"s40523220/2016fallcp_hw","ref":"refs/heads/gh-pages","path":"plugin/liquid_tags/test_generation.py","content":"# -*- coding: utf-8 -*-\\r\\nfrom __future__ import print_function\\r\\n\\r\\nimport filecmp\\r\\nimport os\\r\\nimport unittest\\r\\nfrom shutil import rmtree\\r\\nfrom tempfile import mkdtemp\\r\\n\\r\\nimport pytest\\r\\nfrom pelican import Pelican\\r\\nfrom pelican.settings import read_settings\\r\\n\\r\\nfrom .notebook import IPYTHON_VERSION\\r\\n\\r\\nPLUGIN_DIR = os.path.dirname(__file__)\\r\\nTEST_DATA_DIR = os.path.join(PLUGIN_DIR, \'test_data\')\\r\\n\\r\\n\\r\\nclass TestFullRun(unittest.TestCase):\\r\\n    \'\'\'Test running Pelican with the Plugin\'\'\'\\r\\n\\r\\n    def setUp(self):\\r\\n        \'\'\'Create temporary output and cache folders\'\'\'\\r\\n        self.temp_path = mkdtemp(prefix=\'pelicantests.\')\\r\\n        self.temp_cache = mkdtemp(prefix=\'pelican_cache.\')\\r\\n        os.chdir(TEST_DATA_DIR)\\r\\n\\r\\n    def tearDown(self):\\r\\n        \'\'\'Remove output and cache folders\'\'\'\\r\\n        rmtree(self.temp_path)\\r\\n        rmtree(self.temp_cache)\\r\\n        os.chdir(PLUGIN_DIR)\\r\\n\\r\\n    @pytest.mark.skipif(IPYTHON_VERSION \\u003e= 3,\\r\\n                        reason=\\"output must be created with ipython version 2\\")\\r\\n    def test_generate_with_ipython3(self):\\r\\n        \'\'\'Test generation of site with the plugin.\'\'\'\\r\\n\\r\\n        base_path = os.path.dirname(os.path.abspath(__file__))\\r\\n        base_path = os.path.join(base_path, \'test_data\')\\r\\n        content_path = os.path.join(base_path, \'content\')\\r\\n        output_path = os.path.join(base_path, \'output\')\\r\\n        settings_path = os.path.join(base_path, \'pelicanconf.py\')\\r\\n        settings = read_settings(path=settings_path,\\r\\n                                 override={\'PATH\': content_path,\\r\\n                                           \'OUTPUT_PATH\': self.temp_path,\\r\\n                                           \'CACHE_PATH\': self.temp_cache,\\r\\n                                           }\\r\\n                                 )\\r\\n\\r\\n        pelican = Pelican(settings)\\r\\n        pelican.run()\\r\\n\\r\\n        # test existence\\r\\n        assert os.path.exists(os.path.join(self.temp_path,\\r\\n                                           \'test-ipython-notebook-nb-format-3.html\'))\\r\\n        assert os.path.exists(os.path.join(self.temp_path,\\r\\n                                           \'test-ipython-notebook-nb-format-4.html\'))\\r\\n\\r\\n        # test differences\\r\\n        #assert filecmp.cmp(os.path.join(output_path,\\r\\n        #                                \'test-ipython-notebook-v2.html\'),\\r\\n        #                   os.path.join(self.temp_path,\\r\\n        #                                \'test-ipython-notebook.html\'))\\r\\n\\r\\n    @pytest.mark.skipif(IPYTHON_VERSION \\u003c 3,\\r\\n                        reason=\\"output must be created with ipython version 3\\")\\r\\n    def test_generate_with_ipython2(self):\\r\\n        \'\'\'Test generation of site with the plugin.\'\'\'\\r\\n\\r\\n        base_path = os.path.dirname(os.path.abspath(__file__))\\r\\n        base_path = os.path.join(base_path, \'test_data\')\\r\\n        content_path = os.path.join(base_path, \'content\')\\r\\n        output_path = os.path.join(base_path, \'output\')\\r\\n        settings_path = os.path.join(base_path, \'pelicanconf.py\')\\r\\n        settings = read_settings(path=settings_path,\\r\\n                                 override={\'PATH\': content_path,\\r\\n                                           \'OUTPUT_PATH\': self.temp_path,\\r\\n                                           \'CACHE_PATH\': self.temp_cache,\\r\\n                                           }\\r\\n                                 )\\r\\n\\r\\n        pelican = Pelican(settings)\\r\\n        pelican.run()\\r\\n\\r\\n        # test existence\\r\\n        assert os.path.exists(os.path.join(self.temp_path,\\r\\n                                           \'test-ipython-notebook-nb-format-3.html\'))\\r\\n        assert os.path.exists(os.path.join(self.temp_path,\\r\\n                                           \'test-ipython-notebook-nb-format-4.html\'))\\r\\n\\r\\n        # test differences\\r\\n        #assert filecmp.cmp(os.path.join(output_path,\\r\\n        #                                \'test-ipython-notebook-v3.html\'),\\r\\n        #                   os.path.join(self.temp_path,\\r\\n        #                                \'test-ipython-notebook.html\'))\\r\\n"}\n'
line: b'{"repo_name":"ffantast/magnum","ref":"refs/heads/master","path":"magnum/tests/unit/objects/test_objects.py","content":"#    Copyright 2015 IBM Corp.\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\nimport datetime\\nimport gettext\\n\\nimport iso8601\\nimport netaddr\\nfrom oslo_utils import timeutils\\nfrom oslo_versionedobjects import fields\\n\\nfrom magnum.common import context as magnum_context\\nfrom magnum.common import exception\\nfrom magnum.objects import base\\nfrom magnum.objects import utils\\nfrom magnum.tests import base as test_base\\n\\ngettext.install(\'magnum\')\\n\\n\\n@base.MagnumObjectRegistry.register\\nclass MyObj(base.MagnumObject):\\n    VERSION = \'1.0\'\\n\\n    fields = {\'foo\': fields.IntegerField(),\\n              \'bar\': fields.StringField(),\\n              \'missing\': fields.StringField(),\\n              }\\n\\n    def obj_load_attr(self, attrname):\\n        setattr(self, attrname, \'loaded!\')\\n\\n    @base.remotable_classmethod\\n    def query(cls, context):\\n        obj = cls(context)\\n        obj.foo = 1\\n        obj.bar = \'bar\'\\n        obj.obj_reset_changes()\\n        return obj\\n\\n    @base.remotable\\n    def marco(self, context):\\n        return \'polo\'\\n\\n    @base.remotable\\n    def update_test(self, context):\\n        if context.project_id == \'alternate\':\\n            self.bar = \'alternate-context\'\\n        else:\\n            self.bar = \'updated\'\\n\\n    @base.remotable\\n    def save(self, context):\\n        self.obj_reset_changes()\\n\\n    @base.remotable\\n    def refresh(self, context):\\n        self.foo = 321\\n        self.bar = \'refreshed\'\\n        self.obj_reset_changes()\\n\\n    @base.remotable\\n    def modify_save_modify(self, context):\\n        self.bar = \'meow\'\\n        self.save()\\n        self.foo = 42\\n\\n\\nclass MyObj2(object):\\n    @classmethod\\n    def obj_name(cls):\\n        return \'MyObj\'\\n\\n    @base.remotable_classmethod\\n    def get(cls, *args, **kwargs):\\n        pass\\n\\n\\nclass TestSubclassedObject(MyObj):\\n    fields = {\'new_field\': fields.StringField()}\\n\\n\\nclass TestUtils(test_base.TestCase):\\n\\n    def test_datetime_or_none(self):\\n        naive_dt = datetime.datetime.now()\\n        dt = timeutils.parse_isotime(timeutils.isotime(naive_dt))\\n        self.assertEqual(utils.datetime_or_none(dt), dt)\\n        self.assertEqual(utils.datetime_or_none(dt),\\n                         naive_dt.replace(tzinfo=iso8601.iso8601.Utc(),\\n                                          microsecond=0))\\n        self.assertIsNone(utils.datetime_or_none(None))\\n        self.assertRaises(ValueError, utils.datetime_or_none, \'foo\')\\n\\n    def test_datetime_or_str_or_none(self):\\n        dts = timeutils.isotime()\\n        dt = timeutils.parse_isotime(dts)\\n        self.assertEqual(utils.datetime_or_str_or_none(dt), dt)\\n        self.assertIsNone(utils.datetime_or_str_or_none(None))\\n        self.assertEqual(utils.datetime_or_str_or_none(dts), dt)\\n        self.assertRaises(ValueError, utils.datetime_or_str_or_none, \'foo\')\\n\\n    def test_int_or_none(self):\\n        self.assertEqual(utils.int_or_none(1), 1)\\n        self.assertEqual(utils.int_or_none(\'1\'), 1)\\n        self.assertIsNone(utils.int_or_none(None))\\n        self.assertRaises(ValueError, utils.int_or_none, \'foo\')\\n\\n    def test_str_or_none(self):\\n        class Obj(object):\\n            pass\\n        self.assertEqual(utils.str_or_none(\'foo\'), \'foo\')\\n        self.assertEqual(utils.str_or_none(1), \'1\')\\n        self.assertIsNone(utils.str_or_none(None))\\n\\n    def test_ip_or_none(self):\\n        ip4 = netaddr.IPAddress(\'1.2.3.4\', 4)\\n        ip6 = netaddr.IPAddress(\'1::2\', 6)\\n        self.assertEqual(utils.ip_or_none(4)(\'1.2.3.4\'), ip4)\\n        self.assertEqual(utils.ip_or_none(6)(\'1::2\'), ip6)\\n        self.assertIsNone(utils.ip_or_none(4)(None))\\n        self.assertIsNone(utils.ip_or_none(6)(None))\\n        self.assertRaises(netaddr.AddrFormatError, utils.ip_or_none(4), \'foo\')\\n        self.assertRaises(netaddr.AddrFormatError, utils.ip_or_none(6), \'foo\')\\n\\n    def test_dt_serializer(self):\\n        class Obj(object):\\n            foo = utils.dt_serializer(\'bar\')\\n\\n        obj = Obj()\\n        obj.bar = timeutils.parse_isotime(\'1955-11-05T00:00:00Z\')\\n        self.assertEqual(\'1955-11-05T00:00:00Z\', obj.foo())\\n        obj.bar = None\\n        self.assertIsNone(obj.foo())\\n        obj.bar = \'foo\'\\n        self.assertRaises(AttributeError, obj.foo)\\n\\n    def test_dt_deserializer(self):\\n        dt = timeutils.parse_isotime(\'1955-11-05T00:00:00Z\')\\n        self.assertEqual(utils.dt_deserializer(None, timeutils.isotime(dt)),\\n                         dt)\\n        self.assertIsNone(utils.dt_deserializer(None, None))\\n        self.assertRaises(ValueError, utils.dt_deserializer, None, \'foo\')\\n\\n\\nclass _TestObject(object):\\n    def test_hydration_type_error(self):\\n        primitive = {\'magnum_object.name\': \'MyObj\',\\n                     \'magnum_object.namespace\': \'magnum\',\\n                     \'magnum_object.version\': \'1.5\',\\n                     \'magnum_object.data\': {\'foo\': \'a\'}}\\n        self.assertRaises(ValueError, MyObj.obj_from_primitive, primitive)\\n\\n    def test_hydration(self):\\n        primitive = {\'magnum_object.name\': \'MyObj\',\\n                     \'magnum_object.namespace\': \'magnum\',\\n                     \'magnum_object.version\': \'1.5\',\\n                     \'magnum_object.data\': {\'foo\': 1}}\\n        obj = MyObj.obj_from_primitive(primitive)\\n        self.assertEqual(1, obj.foo)\\n\\n    def test_hydration_bad_ns(self):\\n        primitive = {\'magnum_object.name\': \'MyObj\',\\n                     \'magnum_object.namespace\': \'foo\',\\n                     \'magnum_object.version\': \'1.5\',\\n                     \'magnum_object.data\': {\'foo\': 1}}\\n        self.assertRaises(exception.UnsupportedObjectError,\\n                          MyObj.obj_from_primitive, primitive)\\n\\n    def test_dehydration(self):\\n        expected = {\'magnum_object.name\': \'MyObj\',\\n                    \'magnum_object.namespace\': \'magnum\',\\n                    \'magnum_object.version\': \'1.5\',\\n                    \'magnum_object.data\': {\'foo\': 1}}\\n        obj = MyObj(self.context)\\n        obj.foo = 1\\n        obj.obj_reset_changes()\\n        self.assertEqual(expected, obj.obj_to_primitive())\\n\\n    def test_get_updates(self):\\n        obj = MyObj(self.context)\\n        self.assertEqual({}, obj.obj_get_changes())\\n        obj.foo = 123\\n        self.assertEqual({\'foo\': 123}, obj.obj_get_changes())\\n        obj.bar = \'test\'\\n        self.assertEqual({\'foo\': 123, \'bar\': \'test\'}, obj.obj_get_changes())\\n        obj.obj_reset_changes()\\n        self.assertEqual({}, obj.obj_get_changes())\\n\\n    def test_object_property(self):\\n        obj = MyObj(self.context, foo=1)\\n        self.assertEqual(1, obj.foo)\\n\\n    def test_object_property_type_error(self):\\n        obj = MyObj(self.context)\\n\\n        def fail():\\n            obj.foo = \'a\'\\n        self.assertRaises(ValueError, fail)\\n\\n    def test_load(self):\\n        obj = MyObj(self.context)\\n        self.assertEqual(\'loaded!\', obj.bar)\\n\\n    def test_load_in_base(self):\\n        class Foo(base.MagnumObject):\\n            fields = {\'foobar\': fields.IntegerField()}\\n        obj = Foo(self.context)\\n        # NOTE(danms): Can\'t use assertRaisesRegexp() because of py26\\n        raised = False\\n        try:\\n            obj.foobar\\n        except NotImplementedError as ex:\\n            raised = True\\n        self.assertTrue(raised)\\n        self.assertTrue(\'foobar\' in str(ex))\\n\\n    def test_loaded_in_primitive(self):\\n        obj = MyObj(self.context)\\n        obj.foo = 1\\n        obj.obj_reset_changes()\\n        self.assertEqual(\'loaded!\', obj.bar)\\n        expected = {\'magnum_object.name\': \'MyObj\',\\n                    \'magnum_object.namespace\': \'magnum\',\\n                    \'magnum_object.version\': \'1.0\',\\n                    \'magnum_object.changes\': [\'bar\'],\\n                    \'magnum_object.data\': {\'foo\': 1,\\n                                           \'bar\': \'loaded!\'}}\\n        self.assertEqual(expected, obj.obj_to_primitive())\\n\\n    def test_changes_in_primitive(self):\\n        obj = MyObj(self.context)\\n        obj.foo = 123\\n        self.assertEqual(set([\'foo\']), obj.obj_what_changed())\\n        primitive = obj.obj_to_primitive()\\n        self.assertTrue(\'magnum_object.changes\' in primitive)\\n        obj2 = MyObj.obj_from_primitive(primitive)\\n        self.assertEqual(set([\'foo\']), obj2.obj_what_changed())\\n        obj2.obj_reset_changes()\\n        self.assertEqual(set(), obj2.obj_what_changed())\\n\\n    def test_unknown_objtype(self):\\n        self.assertRaises(exception.UnsupportedObjectError,\\n                          base.MagnumObject.obj_class_from_name, \'foo\', \'1.0\')\\n\\n    def test_with_alternate_context(self):\\n        context1 = magnum_context.RequestContext(\'foo\', \'foo\')\\n        context2 = magnum_context.RequestContext(\'bar\', project_id=\'alternate\')\\n        obj = MyObj.query(context1)\\n        obj.update_test(context2)\\n        self.assertEqual(\'alternate-context\', obj.bar)\\n        self.assertRemotes()\\n\\n    def test_orphaned_object(self):\\n        obj = MyObj.query(self.context)\\n        obj._context = None\\n        self.assertRaises(exception.OrphanedObjectError,\\n                          obj.update_test)\\n        self.assertRemotes()\\n\\n    def test_changed_1(self):\\n        obj = MyObj.query(self.context)\\n        obj.foo = 123\\n        self.assertEqual(set([\'foo\']), obj.obj_what_changed())\\n        obj.update_test(self.context)\\n        self.assertEqual(set([\'foo\', \'bar\']), obj.obj_what_changed())\\n        self.assertEqual(123, obj.foo)\\n        self.assertRemotes()\\n\\n    def test_changed_2(self):\\n        obj = MyObj.query(self.context)\\n        obj.foo = 123\\n        self.assertEqual(set([\'foo\']), obj.obj_what_changed())\\n        obj.save()\\n        self.assertEqual(set([]), obj.obj_what_changed())\\n        self.assertEqual(123, obj.foo)\\n        self.assertRemotes()\\n\\n    def test_changed_3(self):\\n        obj = MyObj.query(self.context)\\n        obj.foo = 123\\n        self.assertEqual(set([\'foo\']), obj.obj_what_changed())\\n        obj.refresh()\\n        self.assertEqual(set([]), obj.obj_what_changed())\\n        self.assertEqual(321, obj.foo)\\n        self.assertEqual(\'refreshed\', obj.bar)\\n        self.assertRemotes()\\n\\n    def test_changed_4(self):\\n        obj = MyObj.query(self.context)\\n        obj.bar = \'something\'\\n        self.assertEqual(set([\'bar\']), obj.obj_what_changed())\\n        obj.modify_save_modify(self.context)\\n        self.assertEqual(set([\'foo\']), obj.obj_what_changed())\\n        self.assertEqual(42, obj.foo)\\n        self.assertEqual(\'meow\', obj.bar)\\n        self.assertRemotes()\\n\\n    def test_static_result(self):\\n        obj = MyObj.query(self.context)\\n        self.assertEqual(\'bar\', obj.bar)\\n        result = obj.marco()\\n        self.assertEqual(\'polo\', result)\\n        self.assertRemotes()\\n\\n    def test_updates(self):\\n        obj = MyObj.query(self.context)\\n        self.assertEqual(1, obj.foo)\\n        obj.update_test()\\n        self.assertEqual(\'updated\', obj.bar)\\n        self.assertRemotes()\\n\\n    def test_base_attributes(self):\\n        dt = datetime.datetime(1955, 11, 5)\\n        obj = MyObj(self.context)\\n        obj.created_at = dt\\n        obj.updated_at = dt\\n        expected = {\'magnum_object.name\': \'MyObj\',\\n                    \'magnum_object.namespace\': \'magnum\',\\n                    \'magnum_object.version\': \'1.0\',\\n                    \'magnum_object.changes\':\\n                        [\'created_at\', \'updated_at\'],\\n                    \'magnum_object.data\':\\n                        {\'created_at\': timeutils.isotime(dt),\\n                         \'updated_at\': timeutils.isotime(dt)}\\n                    }\\n        actual = obj.obj_to_primitive()\\n        # magnum_object.changes is built from a set and order is undefined\\n        self.assertEqual(sorted(expected[\'magnum_object.changes\']),\\n                         sorted(actual[\'magnum_object.changes\']))\\n        del expected[\'magnum_object.changes\'], actual[\'magnum_object.changes\']\\n        self.assertEqual(expected, actual)\\n\\n    def test_contains(self):\\n        obj = MyObj(self.context)\\n        self.assertFalse(\'foo\' in obj)\\n        obj.foo = 1\\n        self.assertTrue(\'foo\' in obj)\\n        self.assertFalse(\'does_not_exist\' in obj)\\n\\n    def test_obj_attr_is_set(self):\\n        obj = MyObj(self.context, foo=1)\\n        self.assertTrue(obj.obj_attr_is_set(\'foo\'))\\n        self.assertFalse(obj.obj_attr_is_set(\'bar\'))\\n        self.assertRaises(AttributeError, obj.obj_attr_is_set, \'bang\')\\n\\n    def test_get(self):\\n        obj = MyObj(self.context, foo=1)\\n        # Foo has value, should not get the default\\n        self.assertEqual(obj.get(\'foo\', 2), 1)\\n        # Foo has value, should return the value without error\\n        self.assertEqual(obj.get(\'foo\'), 1)\\n        # Bar is not loaded, so we should get the default\\n        self.assertEqual(obj.get(\'bar\', \'not-loaded\'), \'not-loaded\')\\n        # Bar without a default should lazy-load\\n        self.assertEqual(obj.get(\'bar\'), \'loaded!\')\\n        # Bar now has a default, but loaded value should be returned\\n        self.assertEqual(obj.get(\'bar\', \'not-loaded\'), \'loaded!\')\\n        # Invalid attribute should raise AttributeError\\n        self.assertRaises(AttributeError, obj.get, \'nothing\')\\n        # ...even with a default\\n        self.assertRaises(AttributeError, obj.get, \'nothing\', 3)\\n\\n    def test_object_inheritance(self):\\n        base_fields = list(base.MagnumObject.fields.keys())\\n        myobj_fields = [\'foo\', \'bar\', \'missing\'] + base_fields\\n        myobj3_fields = [\'new_field\']\\n        self.assertTrue(issubclass(TestSubclassedObject, MyObj))\\n        self.assertEqual(len(myobj_fields), len(MyObj.fields))\\n        self.assertEqual(set(myobj_fields), set(MyObj.fields.keys()))\\n        self.assertEqual(len(myobj_fields) + len(myobj3_fields),\\n                         len(TestSubclassedObject.fields))\\n        self.assertEqual(set(myobj_fields) | set(myobj3_fields),\\n                         set(TestSubclassedObject.fields.keys()))\\n\\n    def test_get_changes(self):\\n        obj = MyObj(self.context)\\n        self.assertEqual({}, obj.obj_get_changes())\\n        obj.foo = 123\\n        self.assertEqual({\'foo\': 123}, obj.obj_get_changes())\\n        obj.bar = \'test\'\\n        self.assertEqual({\'foo\': 123, \'bar\': \'test\'}, obj.obj_get_changes())\\n        obj.obj_reset_changes()\\n        self.assertEqual({}, obj.obj_get_changes())\\n\\n    def test_obj_fields(self):\\n        class TestObj(base.MagnumObject):\\n            fields = {\'foo\': fields.IntegerField()}\\n            obj_extra_fields = [\'bar\']\\n\\n            @property\\n            def bar(self):\\n                return \'this is bar\'\\n\\n        obj = TestObj(self.context)\\n        self.assertEqual(set([\'created_at\', \'updated_at\', \'foo\', \'bar\']),\\n                         set(obj.obj_fields))\\n\\n    def test_obj_constructor(self):\\n        obj = MyObj(self.context, foo=123, bar=\'abc\')\\n        self.assertEqual(123, obj.foo)\\n        self.assertEqual(\'abc\', obj.bar)\\n        self.assertEqual(set([\'foo\', \'bar\']), obj.obj_what_changed())\\n\\n\\nclass TestObjectSerializer(test_base.TestCase):\\n\\n    def test_object_serialization(self):\\n        ser = base.MagnumObjectSerializer()\\n        obj = MyObj(self.context)\\n        primitive = ser.serialize_entity(self.context, obj)\\n        self.assertTrue(\'magnum_object.name\' in primitive)\\n        obj2 = ser.deserialize_entity(self.context, primitive)\\n        self.assertIsInstance(obj2, MyObj)\\n        self.assertEqual(self.context, obj2._context)\\n\\n    def test_object_serialization_iterables(self):\\n        ser = base.MagnumObjectSerializer()\\n        obj = MyObj(self.context)\\n        for iterable in (list, tuple, set):\\n            thing = iterable([obj])\\n            primitive = ser.serialize_entity(self.context, thing)\\n            self.assertEqual(1, len(primitive))\\n            for item in primitive:\\n                self.assertFalse(isinstance(item, base.MagnumObject))\\n            thing2 = ser.deserialize_entity(self.context, primitive)\\n            self.assertEqual(1, len(thing2))\\n            for item in thing2:\\n                self.assertIsInstance(item, MyObj)\\n"}\n'
line: b'{"repo_name":"xboxfanj/android_kernel_htc_msm8974","ref":"refs/heads/lp5.0","path":"tools/perf/scripts/python/check-perf-trace.py","content":"# perf script event handlers, generated by perf script -g python\\n# (c) 2010, Tom Zanussi \\u003ctzanussi@gmail.com\\u003e\\n# Licensed under the terms of the GNU GPL License version 2\\n#\\n# This script tests basic functionality such as flag and symbol\\n# strings, common_xxx() calls back into perf, begin, end, unhandled\\n# events, etc.  Basically, if this script runs successfully and\\n# displays expected results, Python scripting support should be ok.\\n\\nimport os\\nimport sys\\n\\nsys.path.append(os.environ[\'PERF_EXEC_PATH\'] + \\\\\\n\\t\'/scripts/python/Perf-Trace-Util/lib/Perf/Trace\')\\n\\nfrom Core import *\\nfrom perf_trace_context import *\\n\\nunhandled = autodict()\\n\\ndef trace_begin():\\n\\tprint \\"trace_begin\\"\\n\\tpass\\n\\ndef trace_end():\\n        print_unhandled()\\n\\ndef irq__softirq_entry(event_name, context, common_cpu,\\n\\tcommon_secs, common_nsecs, common_pid, common_comm,\\n\\tvec):\\n\\t\\tprint_header(event_name, common_cpu, common_secs, common_nsecs,\\n\\t\\t\\tcommon_pid, common_comm)\\n\\n                print_uncommon(context)\\n\\n\\t\\tprint \\"vec=%s\\\\n\\" % \\\\\\n\\t\\t(symbol_str(\\"irq__softirq_entry\\", \\"vec\\", vec)),\\n\\ndef kmem__kmalloc(event_name, context, common_cpu,\\n\\tcommon_secs, common_nsecs, common_pid, common_comm,\\n\\tcall_site, ptr, bytes_req, bytes_alloc,\\n\\tgfp_flags):\\n\\t\\tprint_header(event_name, common_cpu, common_secs, common_nsecs,\\n\\t\\t\\tcommon_pid, common_comm)\\n\\n                print_uncommon(context)\\n\\n\\t\\tprint \\"call_site=%u, ptr=%u, bytes_req=%u, \\" \\\\\\n\\t\\t\\"bytes_alloc=%u, gfp_flags=%s\\\\n\\" % \\\\\\n\\t\\t(call_site, ptr, bytes_req, bytes_alloc,\\n\\n\\t\\tflag_str(\\"kmem__kmalloc\\", \\"gfp_flags\\", gfp_flags)),\\n\\ndef trace_unhandled(event_name, context, event_fields_dict):\\n    try:\\n        unhandled[event_name] += 1\\n    except TypeError:\\n        unhandled[event_name] = 1\\n\\ndef print_header(event_name, cpu, secs, nsecs, pid, comm):\\n\\tprint \\"%-20s %5u %05u.%09u %8u %-20s \\" % \\\\\\n\\t(event_name, cpu, secs, nsecs, pid, comm),\\n\\n# print trace fields not included in handler args\\ndef print_uncommon(context):\\n    print \\"common_preempt_count=%d, common_flags=%s, common_lock_depth=%d, \\" \\\\\\n        % (common_pc(context), trace_flag_str(common_flags(context)), \\\\\\n               common_lock_depth(context))\\n\\ndef print_unhandled():\\n    keys = unhandled.keys()\\n    if not keys:\\n        return\\n\\n    print \\"\\\\nunhandled events:\\\\n\\\\n\\",\\n\\n    print \\"%-40s  %10s\\\\n\\" % (\\"event\\", \\"count\\"),\\n    print \\"%-40s  %10s\\\\n\\" % (\\"----------------------------------------\\", \\\\\\n                                 \\"-----------\\"),\\n\\n    for event_name in keys:\\n\\tprint \\"%-40s  %10d\\\\n\\" % (event_name, unhandled[event_name])\\n"}\n'
line: b'{"repo_name":"Affix/CouchPotatoServer","ref":"refs/heads/master","path":"couchpotato/core/media/_base/providers/torrent/sceneaccess.py","content":"import traceback\\n\\nfrom bs4 import BeautifulSoup\\nfrom couchpotato.core.helpers.encoding import toUnicode\\nfrom couchpotato.core.helpers.variable import tryInt\\nfrom couchpotato.core.logger import CPLog\\nfrom couchpotato.core.media._base.providers.torrent.base import TorrentProvider\\n\\n\\nlog = CPLog(__name__)\\n\\n\\nclass Base(TorrentProvider):\\n\\n    urls = {\\n        \'test\': \'https://www.sceneaccess.eu/\',\\n        \'login\': \'https://www.sceneaccess.eu/login\',\\n        \'login_check\': \'https://www.sceneaccess.eu/inbox\',\\n        \'detail\': \'https://www.sceneaccess.eu/details?id=%s\',\\n        \'search\': \'https://www.sceneaccess.eu/browse?c%d=%d\',\\n        \'archive\': \'https://www.sceneaccess.eu/archive?\\u0026c%d=%d\',\\n        \'download\': \'https://www.sceneaccess.eu/%s\',\\n    }\\n\\n    http_time_between_calls = 1  # Seconds\\n\\n    def _searchOnTitle(self, title, media, quality, results):\\n\\n        url = self.buildUrl(title, media, quality)\\n        data = self.getHTMLData(url)\\n\\n        if data:\\n            html = BeautifulSoup(data)\\n\\n            try:\\n                resultsTable = html.find(\'table\', attrs = {\'id\': \'torrents-table\'})\\n                if resultsTable is None:\\n                    return\\n\\n                entries = resultsTable.find_all(\'tr\', attrs = {\'class\': \'tt_row\'})\\n                for result in entries:\\n\\n                    link = result.find(\'td\', attrs = {\'class\': \'ttr_name\'}).find(\'a\')\\n                    url = result.find(\'td\', attrs = {\'class\': \'td_dl\'}).find(\'a\')\\n                    leechers = result.find(\'td\', attrs = {\'class\': \'ttr_leechers\'}).find(\'a\')\\n                    torrent_id = link[\'href\'].replace(\'details?id=\', \'\')\\n\\n                    results.append({\\n                        \'id\': torrent_id,\\n                        \'name\': link[\'title\'],\\n                        \'url\': self.urls[\'download\'] % url[\'href\'],\\n                        \'detail_url\': self.urls[\'detail\'] % torrent_id,\\n                        \'size\': self.parseSize(result.find(\'td\', attrs = {\'class\': \'ttr_size\'}).contents[0]),\\n                        \'seeders\': tryInt(result.find(\'td\', attrs = {\'class\': \'ttr_seeders\'}).find(\'a\').string),\\n                        \'leechers\': tryInt(leechers.string) if leechers else 0,\\n                        \'get_more_info\': self.getMoreInfo,\\n                    })\\n\\n            except:\\n                log.error(\'Failed getting results from %s: %s\', (self.getName(), traceback.format_exc()))\\n\\n    def getMoreInfo(self, item):\\n        full_description = self.getCache(\'sceneaccess.%s\' % item[\'id\'], item[\'detail_url\'], cache_timeout = 25920000)\\n        html = BeautifulSoup(full_description)\\n        nfo_pre = html.find(\'div\', attrs = {\'id\': \'details_table\'})\\n        description = toUnicode(nfo_pre.text) if nfo_pre else \'\'\\n\\n        item[\'description\'] = description\\n        return item\\n\\n    # Login\\n    def getLoginParams(self):\\n        return {\\n            \'username\': self.conf(\'username\'),\\n            \'password\': self.conf(\'password\'),\\n            \'submit\': \'come on in\',\\n        }\\n\\n    def loginSuccess(self, output):\\n        return \'/inbox\' in output.lower()\\n\\n    loginCheckSuccess = loginSuccess\\n\\n\\nconfig = [{\\n    \'name\': \'sceneaccess\',\\n    \'groups\': [\\n        {\\n            \'tab\': \'searcher\',\\n            \'list\': \'torrent_providers\',\\n            \'name\': \'SceneAccess\',\\n            \'description\': \'\\u003ca href=\\"https://sceneaccess.eu/\\"\\u003eSceneAccess\\u003c/a\\u003e\',\\n            \'wizard\': True,\\n            \'icon\': \'iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAIAAACQkWg2AAAABnRSTlMAAAAAAABupgeRAAACT0lEQVR4AYVQS0sbURidO3OTmajJ5FElTTOkPmZ01GhHrIq0aoWAj1Vc+A/cuRMXbl24V9SlCGqrLhVFCrooEhCp2BAx0mobTY2kaR7qmOm87EXL1EWxh29xL+c7nPMdgGHYO5bF/gdbefnr6WlbWRnxluMwAB4Z0uEgXa7nwaDL7+/RNPzxbYvb/XJ0FBYVfd/ayh0fQ4qCGEHcm0KLRZUk7Pb2YRJPRwcsKMidnKD3t9VVT3s7BDh+z5FOZ3Vfn3h+Hltfx00mRRSRWFcUmmVNhYVqPn8dj3va2oh+txvcQRVF9ebm1fi4k+dRFbosY5rm4Hk7xxULQnJnx93S4g0EIEEQRoDLo6PrWEw8Pc0eHLwYGopMTDirqlJ7eyhYYGHhfgfHCcKYksZGVB/NcXI2mw6HhZERqrjYTNPHi4tFPh8aJIYIhgPlcCRDoZLW1s75+Z/7+59nZ/OJhLWigqAoKZX6Mjf3dXkZ3pydGYLc4aEoCCkInzQ1fRobS2xuvllaonkedfArnY5OTdGVldBkOADgqq2Nr6z8CIWaJietDHOhKB+HhwFKC6Gnq4ukKJvP9zcSbjYDXbeVlkKzuZBhnnV3e3t6UOmaJO0ODibW1hB1GYkg8R/gup7Z3TVZLJ5AILW9LcZiVpYtYBhw16O3t7cauckyeF9Tgz0ATpL2+nopmWycmbnY2LiKRjFk6/d7+/vRJfl4HGzV1T0UIM43MGBvaIBWK/YvwM5w+IMgGH8tkyEgvIpE7M3Nt6qqZrNyOq1kMmouh455Ggz+BhKY4GEc2CfwAAAAAElFTkSuQmCC\',\\n            \'options\': [\\n                {\\n                    \'name\': \'enabled\',\\n                    \'type\': \'enabler\',\\n                    \'default\': False,\\n                },\\n                {\\n                    \'name\': \'username\',\\n                    \'default\': \'\',\\n                },\\n                {\\n                    \'name\': \'password\',\\n                    \'default\': \'\',\\n                    \'type\': \'password\',\\n                },\\n                {\\n                    \'name\': \'seed_ratio\',\\n                    \'label\': \'Seed ratio\',\\n                    \'type\': \'float\',\\n                    \'default\': 1,\\n                    \'description\': \'Will not be (re)moved until this seed ratio is met.\',\\n                },\\n                {\\n                    \'name\': \'seed_time\',\\n                    \'label\': \'Seed time\',\\n                    \'type\': \'int\',\\n                    \'default\': 40,\\n                    \'description\': \'Will not be (re)moved until this seed time (in hours) is met.\',\\n                },\\n                {\\n                    \'name\': \'extra_score\',\\n                    \'advanced\': True,\\n                    \'label\': \'Extra Score\',\\n                    \'type\': \'int\',\\n                    \'default\': 20,\\n                    \'description\': \'Starting score for each release found via this provider.\',\\n                }\\n            ],\\n        },\\n    ],\\n}]\\n"}\n'
line: b'{"repo_name":"vqw/frappe","ref":"refs/heads/develop","path":"frappe/model/delete_doc.py","content":"# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\\n# MIT License. See license.txt\\n\\nfrom __future__ import unicode_literals\\n\\nimport frappe\\nimport frappe.model.meta\\nfrom frappe.model.dynamic_links import get_dynamic_link_map\\nimport frappe.defaults\\nfrom frappe.utils.file_manager import remove_all\\nfrom frappe.utils.password import delete_all_passwords_for\\nfrom frappe import _\\nfrom frappe.model.naming import revert_series_if_last\\n\\ndef delete_doc(doctype=None, name=None, force=0, ignore_doctypes=None, for_reload=False,\\n\\tignore_permissions=False, flags=None, ignore_on_trash=False):\\n\\t\\"\\"\\"\\n\\t\\tDeletes a doc(dt, dn) and validates if it is not submitted and not linked in a live record\\n\\t\\"\\"\\"\\n\\tif not ignore_doctypes: ignore_doctypes = []\\n\\n\\t# get from form\\n\\tif not doctype:\\n\\t\\tdoctype = frappe.form_dict.get(\'dt\')\\n\\t\\tname = frappe.form_dict.get(\'dn\')\\n\\n\\tnames = name\\n\\tif isinstance(name, basestring):\\n\\t\\tnames = [name]\\n\\n\\tfor name in names or []:\\n\\n\\t\\t# already deleted..?\\n\\t\\tif not frappe.db.exists(doctype, name):\\n\\t\\t\\treturn\\n\\n\\t\\t# delete attachments\\n\\t\\tremove_all(doctype, name)\\n\\n\\t\\t# delete passwords\\n\\t\\tdelete_all_passwords_for(doctype, name)\\n\\n\\t\\tdoc = None\\n\\t\\tif doctype==\\"DocType\\":\\n\\t\\t\\tif for_reload:\\n\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tdoc = frappe.get_doc(doctype, name)\\n\\t\\t\\t\\texcept frappe.DoesNotExistError:\\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tdoc.run_method(\\"before_reload\\")\\n\\n\\t\\t\\telse:\\n\\t\\t\\t\\tdoc = frappe.get_doc(doctype, name)\\n\\n\\t\\t\\t\\tupdate_flags(doc, flags, ignore_permissions)\\n\\t\\t\\t\\tcheck_permission_and_not_submitted(doc)\\n\\n\\t\\t\\t\\tfrappe.db.sql(\\"delete from `tabCustom Field` where dt = %s\\", name)\\n\\t\\t\\t\\tfrappe.db.sql(\\"delete from `tabCustom Script` where dt = %s\\", name)\\n\\t\\t\\t\\tfrappe.db.sql(\\"delete from `tabProperty Setter` where doc_type = %s\\", name)\\n\\t\\t\\t\\tfrappe.db.sql(\\"delete from `tabReport` where ref_doctype=%s\\", name)\\n\\n\\t\\t\\tdelete_from_table(doctype, name, ignore_doctypes, None)\\n\\n\\t\\telse:\\n\\t\\t\\tdoc = frappe.get_doc(doctype, name)\\n\\n\\t\\t\\tif not for_reload:\\n\\t\\t\\t\\tupdate_flags(doc, flags, ignore_permissions)\\n\\t\\t\\t\\tcheck_permission_and_not_submitted(doc)\\n\\n\\t\\t\\t\\tif not ignore_on_trash:\\n\\t\\t\\t\\t\\tdoc.run_method(\\"on_trash\\")\\n\\t\\t\\t\\t\\tdoc.run_method(\'on_change\')\\n\\n\\t\\t\\t\\tdynamic_linked_doctypes = [df.parent for df in get_dynamic_link_map().get(doc.doctype, [])]\\n\\t\\t\\t\\tif \\"ToDo\\" in dynamic_linked_doctypes:\\n\\t\\t\\t\\t\\tdelete_linked_todos(doc)\\n\\n\\t\\t\\t\\tif \\"Communication\\" in dynamic_linked_doctypes:\\n\\t\\t\\t\\t\\tdelete_linked_communications(doc)\\n\\n\\t\\t\\t\\tif \\"DocShare\\" in dynamic_linked_doctypes:\\n\\t\\t\\t\\t\\tdelete_shared(doc)\\n\\n\\t\\t\\t\\tif \\"Email Unsubscribe\\" in dynamic_linked_doctypes:\\n\\t\\t\\t\\t\\tdelete_email_subscribe(doc)\\n\\n\\t\\t\\t\\t# check if links exist\\n\\t\\t\\t\\tif not force:\\n\\t\\t\\t\\t\\tcheck_if_doc_is_linked(doc)\\n\\t\\t\\t\\t\\tcheck_if_doc_is_dynamically_linked(doc)\\n\\n\\t\\t\\tupdate_naming_series(doc)\\n\\t\\t\\tdelete_from_table(doctype, name, ignore_doctypes, doc)\\n\\t\\t\\tdoc.run_method(\\"after_delete\\")\\n\\n\\t\\tif doc and not frappe.flags.in_patch:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tdoc.notify_update()\\n\\t\\t\\t\\tinsert_feed(doc)\\n\\t\\t\\texcept ImportError:\\n\\t\\t\\t\\tpass\\n\\n\\t\\t# delete user_permissions\\n\\t\\tfrappe.defaults.clear_default(parenttype=\\"User Permission\\", key=doctype, value=name)\\n\\ndef update_naming_series(doc):\\n\\tif doc.meta.autoname:\\n\\t\\tif doc.meta.autoname.startswith(\\"naming_series:\\") \\\\\\n\\t\\t\\tand getattr(doc, \\"naming_series\\", None):\\n\\t\\t\\trevert_series_if_last(doc.naming_series, doc.name)\\n\\n\\t\\telif doc.meta.autoname.split(\\":\\")[0] not in (\\"Prompt\\", \\"field\\", \\"hash\\"):\\n\\t\\t\\trevert_series_if_last(doc.meta.autoname, doc.name)\\n\\ndef delete_from_table(doctype, name, ignore_doctypes, doc):\\n\\tif doctype!=\\"DocType\\" and doctype==name:\\n\\t\\tfrappe.db.sql(\\"delete from `tabSingles` where doctype=%s\\", name)\\n\\telse:\\n\\t\\tfrappe.db.sql(\\"delete from `tab%s` where name=%s\\" % (frappe.db.escape(doctype), \\"%s\\"), (name,))\\n\\n\\t# get child tables\\n\\tif doc:\\n\\t\\ttables = [d.options for d in doc.meta.get_table_fields()]\\n\\n\\telse:\\n\\t\\tdef get_table_fields(field_doctype):\\n\\t\\t\\treturn frappe.db.sql_list(\\"\\"\\"select options from `tab{}` where fieldtype=\'Table\'\\n\\t\\t\\t\\tand parent=%s\\"\\"\\".format(field_doctype), doctype)\\n\\n\\t\\ttables = get_table_fields(\\"DocField\\")\\n\\t\\tif not frappe.flags.in_install==\\"frappe\\":\\n\\t\\t\\ttables += get_table_fields(\\"Custom Field\\")\\n\\n\\t# delete from child tables\\n\\tfor t in list(set(tables)):\\n\\t\\tif t not in ignore_doctypes:\\n\\t\\t\\tfrappe.db.sql(\\"delete from `tab%s` where parenttype=%s and parent = %s\\" % (t, \'%s\', \'%s\'), (doctype, name))\\n\\ndef update_flags(doc, flags=None, ignore_permissions=False):\\n\\tif ignore_permissions:\\n\\t\\tif not flags: flags = {}\\n\\t\\tflags[\\"ignore_permissions\\"] = ignore_permissions\\n\\n\\tif flags:\\n\\t\\tdoc.flags.update(flags)\\n\\ndef check_permission_and_not_submitted(doc):\\n\\t# permission\\n\\tif not doc.flags.ignore_permissions and frappe.session.user!=\\"Administrator\\" and (not doc.has_permission(\\"delete\\") or (doc.doctype==\\"DocType\\" and not doc.custom)):\\n\\t\\tfrappe.msgprint(_(\\"User not allowed to delete {0}: {1}\\").format(doc.doctype, doc.name), raise_exception=True)\\n\\n\\t# check if submitted\\n\\tif doc.docstatus == 1:\\n\\t\\tfrappe.msgprint(_(\\"{0} {1}: Submitted Record cannot be deleted.\\").format(doc.doctype, doc.name),\\n\\t\\t\\traise_exception=True)\\n\\ndef check_if_doc_is_linked(doc, method=\\"Delete\\"):\\n\\t\\"\\"\\"\\n\\t\\tRaises excption if the given doc(dt, dn) is linked in another record.\\n\\t\\"\\"\\"\\n\\tfrom frappe.model.rename_doc import get_link_fields\\n\\tlink_fields = get_link_fields(doc.doctype)\\n\\tlink_fields = [[lf[\'parent\'], lf[\'fieldname\'], lf[\'issingle\']] for lf in link_fields]\\n\\n\\tfor link_dt, link_field, issingle in link_fields:\\n\\t\\tif not issingle:\\n\\t\\t\\titem = frappe.db.get_value(link_dt, {link_field:doc.name},\\n\\t\\t\\t\\t[\\"name\\", \\"parent\\", \\"parenttype\\", \\"docstatus\\"], as_dict=True)\\n\\t\\t\\tif item and ((item.parent or item.name) != doc.name) \\\\\\n\\t\\t\\t\\t\\tand ((method==\\"Delete\\" and item.docstatus\\u003c2) or (method==\\"Cancel\\" and item.docstatus==1)):\\n\\t\\t\\t\\t# raise exception only if\\n\\t\\t\\t\\t# linked to an non-cancelled doc when deleting\\n\\t\\t\\t\\t# or linked to a submitted doc when cancelling\\n\\t\\t\\t\\tfrappe.throw(_(\\"Cannot delete or cancel because {0} {1} is linked with {2} {3}\\")\\n\\t\\t\\t\\t\\t.format(doc.doctype, doc.name, item.parenttype if item.parent else link_dt,\\n\\t\\t\\t\\t\\titem.parent or item.name), frappe.LinkExistsError)\\n\\ndef check_if_doc_is_dynamically_linked(doc, method=\\"Delete\\"):\\n\\t\'\'\'Raise `frappe.LinkExistsError` if the document is dynamically linked\'\'\'\\n\\tfor df in get_dynamic_link_map().get(doc.doctype, []):\\n\\t\\tif df.parent in (\\"Communication\\", \\"ToDo\\", \\"DocShare\\", \\"Email Unsubscribe\\"):\\n\\t\\t\\t# don\'t check for communication and todo!\\n\\t\\t\\tcontinue\\n\\n\\t\\tmeta = frappe.get_meta(df.parent)\\n\\t\\tif meta.issingle:\\n\\t\\t\\t# dynamic link in single doc\\n\\t\\t\\trefdoc = frappe.db.get_singles_dict(df.parent)\\n\\t\\t\\tif (refdoc.get(df.options)==doc.doctype\\n\\t\\t\\t\\tand refdoc.get(df.fieldname)==doc.name\\n\\t\\t\\t\\tand ((method==\\"Delete\\" and refdoc.docstatus \\u003c 2)\\n\\t\\t\\t\\t\\tor (method==\\"Cancel\\" and refdoc.docstatus==1))\\n\\t\\t\\t\\t):\\n\\t\\t\\t\\t# raise exception only if\\n\\t\\t\\t\\t# linked to an non-cancelled doc when deleting\\n\\t\\t\\t\\t# or linked to a submitted doc when cancelling\\n\\t\\t\\t\\tfrappe.throw(_(\\"Cannot delete or cancel because {0} {1} is linked with {2} {3}\\").format(doc.doctype,\\n\\t\\t\\t\\t\\tdoc.name, df.parent, \\"\\"), frappe.LinkExistsError)\\n\\t\\telse:\\n\\t\\t\\t# dynamic link in table\\n\\t\\t\\tfor refdoc in frappe.db.sql(\\"\\"\\"select name, docstatus from `tab{parent}` where\\n\\t\\t\\t\\t{options}=%s and {fieldname}=%s\\"\\"\\".format(**df), (doc.doctype, doc.name), as_dict=True):\\n\\n\\t\\t\\t\\tif ((method==\\"Delete\\" and refdoc.docstatus \\u003c 2) or (method==\\"Cancel\\" and refdoc.docstatus==1)):\\n\\t\\t\\t\\t\\t# raise exception only if\\n\\t\\t\\t\\t\\t# linked to an non-cancelled doc when deleting\\n\\t\\t\\t\\t\\t# or linked to a submitted doc when cancelling\\n\\t\\t\\t\\t\\tfrappe.throw(_(\\"Cannot delete or cancel because {0} {1} is linked with {2} {3}\\")\\\\\\n\\t\\t\\t\\t\\t\\t.format(doc.doctype, doc.name, df.parent, refdoc.name), frappe.LinkExistsError)\\n\\ndef delete_linked_todos(doc):\\n\\tdelete_doc(\\"ToDo\\", frappe.db.sql_list(\\"\\"\\"select name from `tabToDo`\\n\\t\\twhere reference_type=%s and reference_name=%s\\"\\"\\", (doc.doctype, doc.name)),\\n\\t\\tignore_permissions=True)\\n\\ndef delete_email_subscribe(doc):\\n\\tfrappe.db.sql(\'\'\'delete from `tabEmail Unsubscribe`\\n\\t\\twhere reference_doctype=%s and reference_name=%s\'\'\', (doc.doctype, doc.name))\\n\\ndef delete_linked_communications(doc):\\n\\t# delete comments\\n\\tfrappe.db.sql(\\"\\"\\"delete from `tabCommunication`\\n\\t\\twhere\\n\\t\\t\\tcommunication_type = \'Comment\'\\n\\t\\t\\tand reference_doctype=%s and reference_name=%s\\"\\"\\", (doc.doctype, doc.name))\\n\\n\\t# make communications orphans\\n\\tfrappe.db.sql(\\"\\"\\"update `tabCommunication`\\n\\t\\tset reference_doctype=null, reference_name=null\\n\\t\\twhere\\n\\t\\t\\tcommunication_type = \'Communication\'\\n\\t\\t\\tand reference_doctype=%s\\n\\t\\t\\tand reference_name=%s\\"\\"\\", (doc.doctype, doc.name))\\n\\n\\t# make secondary references orphans\\n\\tfrappe.db.sql(\\"\\"\\"update `tabCommunication`\\n\\t\\tset link_doctype=null, link_name=null\\n\\t\\twhere link_doctype=%s and link_name=%s\\"\\"\\", (doc.doctype, doc.name))\\n\\n\\tfrappe.db.sql(\\"\\"\\"update `tabCommunication`\\n\\t\\tset timeline_doctype=null, timeline_name=null\\n\\t\\twhere timeline_doctype=%s and timeline_name=%s\\"\\"\\", (doc.doctype, doc.name))\\n\\ndef insert_feed(doc):\\n\\tfrom frappe.utils import get_fullname\\n\\n\\tif frappe.flags.in_install or frappe.flags.in_import or getattr(doc, \\"no_feed_on_delete\\", False):\\n\\t\\treturn\\n\\n\\tfrappe.get_doc({\\n\\t\\t\\"doctype\\": \\"Communication\\",\\n\\t\\t\\"communication_type\\": \\"Comment\\",\\n\\t\\t\\"comment_type\\": \\"Deleted\\",\\n\\t\\t\\"reference_doctype\\": doc.doctype,\\n\\t\\t\\"subject\\": \\"{0} {1}\\".format(_(doc.doctype), doc.name),\\n\\t\\t\\"full_name\\": get_fullname(doc.owner)\\n\\t}).insert(ignore_permissions=True)\\n\\ndef delete_shared(doc):\\n\\tdelete_doc(\\"DocShare\\", frappe.db.sql_list(\\"\\"\\"select name from `tabDocShare`\\n\\t\\twhere share_doctype=%s and share_name=%s\\"\\"\\", (doc.doctype, doc.name)), ignore_on_trash=True)\\n"}\n'
line: b'{"repo_name":"kcompher/BuildingMachineLearningSystemsWithPython","ref":"refs/heads/master","path":"ch03/rel_post_01.py","content":"# This code is supporting material for the book\\n# Building Machine Learning Systems with Python\\n# by Willi Richert and Luis Pedro Coelho\\n# published by PACKT Publishing\\n#\\n# It is made available under the MIT License\\n\\nimport os\\nimport sys\\n\\nimport scipy as sp\\n\\nfrom sklearn.feature_extraction.text import CountVectorizer\\n\\nDIR = r\\"../data/toy\\"\\nposts = [open(os.path.join(DIR, f)).read() for f in os.listdir(DIR)]\\n\\nnew_post = \\"imaging databases\\"\\n\\nimport nltk.stem\\nenglish_stemmer = nltk.stem.SnowballStemmer(\'english\')\\n\\n\\nclass StemmedCountVectorizer(CountVectorizer):\\n\\n    def build_analyzer(self):\\n        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\\n        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\\n\\n# vectorizer = CountVectorizer(min_df=1, stop_words=\'english\',\\n# preprocessor=stemmer)\\nvectorizer = StemmedCountVectorizer(min_df=1, stop_words=\'english\')\\n\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\n\\nclass StemmedTfidfVectorizer(TfidfVectorizer):\\n\\n    def build_analyzer(self):\\n        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\\n        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\\n\\nvectorizer = StemmedTfidfVectorizer(\\n    min_df=1, stop_words=\'english\', charset_error=\'ignore\')\\nprint(vectorizer)\\n\\nX_train = vectorizer.fit_transform(posts)\\n\\nnum_samples, num_features = X_train.shape\\nprint(\\"#samples: %d, #features: %d\\" % (num_samples, num_features))\\n\\nnew_post_vec = vectorizer.transform([new_post])\\nprint(new_post_vec, type(new_post_vec))\\nprint(new_post_vec.toarray())\\nprint(vectorizer.get_feature_names())\\n\\n\\ndef dist_raw(v1, v2):\\n    delta = v1 - v2\\n    return sp.linalg.norm(delta.toarray())\\n\\n\\ndef dist_norm(v1, v2):\\n    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\\n    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\\n\\n    delta = v1_normalized - v2_normalized\\n\\n    return sp.linalg.norm(delta.toarray())\\n\\ndist = dist_norm\\n\\nbest_dist = sys.maxsize\\nbest_i = None\\n\\nfor i in range(0, num_samples):\\n    post = posts[i]\\n    if post == new_post:\\n        continue\\n    post_vec = X_train.getrow(i)\\n    d = dist(post_vec, new_post_vec)\\n\\n    print(\\"=== Post %i with dist=%.2f: %s\\" % (i, d, post))\\n\\n    if d \\u003c best_dist:\\n        best_dist = d\\n        best_i = i\\n\\nprint(\\"Best post is %i with dist=%.2f\\" % (best_i, best_dist))\\n"}\n'
line: b'{"repo_name":"petecummings/django-cms","ref":"refs/heads/develop","path":"cms/south_migrations/0015_modified_by_added.py","content":"# -*- coding: utf-8 -*-\\nimport datetime\\nfrom south.db import db\\nfrom south.v2 import SchemaMigration\\nfrom django.db import models\\n\\n\\ntry:\\n    from django.contrib.auth import get_user_model\\nexcept ImportError: # django \\u003c 1.5\\n    from django.contrib.auth.models import User\\nelse:\\n    User = get_user_model()\\n\\nuser_orm_label = \'%s.%s\' % (User._meta.app_label, User._meta.object_name)\\nuser_model_label = \'%s.%s\' % (User._meta.app_label, User._meta.model_name)\\nuser_ptr_name = \'%s_ptr\' % User._meta.object_name.lower()\\n\\nclass Migration(SchemaMigration):\\n\\n    def forwards(self, orm):\\n        # Dummy migration\\n        pass\\n\\n\\n\\n    def backwards(self, orm):\\n    # Dummy migration\\n        pass\\n\\n\\n    models = {\\n        \'auth.group\': {\\n            \'Meta\': {\'object_name\': \'Group\'},\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [],\\n                     {\'unique\': \'True\', \'max_length\': \'80\'}),\\n            \'permissions\': (\'django.db.models.fields.related.ManyToManyField\', [],\\n                            {\'to\': \\"orm[\'auth.Permission\']\\", \'symmetrical\': \'False\',\\n                             \'blank\': \'True\'})\\n        },\\n        \'auth.permission\': {\\n            \'Meta\': {\\n                \'ordering\': \\"(\'content_type__app_label\', \'content_type__model\', \'codename\')\\",\\n                \'unique_together\': \\"((\'content_type\', \'codename\'),)\\",\\n                \'object_name\': \'Permission\'},\\n            \'codename\': (\\n                \'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'content_type\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                             {\'to\': \\"orm[\'contenttypes.ContentType\']\\"}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\'})\\n        },\\n        user_model_label: {\\n            \'Meta\': {\'object_name\': User.__name__, \'db_table\': \\"\'%s\'\\" % User._meta.db_table},\\n            \'date_joined\': (\'django.db.models.fields.DateTimeField\', [],\\n                            {\'default\': \'datetime.datetime.now\'}),\\n            \'email\': (\'django.db.models.fields.EmailField\', [],\\n                      {\'max_length\': \'75\', \'blank\': \'True\'}),\\n            \'first_name\': (\'django.db.models.fields.CharField\', [],\\n                           {\'max_length\': \'30\', \'blank\': \'True\'}),\\n            \'groups\': (\'django.db.models.fields.related.ManyToManyField\', [],\\n                       {\'to\': \\"orm[\'auth.Group\']\\", \'symmetrical\': \'False\',\\n                        \'blank\': \'True\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'is_active\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'is_staff\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'is_superuser\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'last_login\': (\'django.db.models.fields.DateTimeField\', [],\\n                           {\'default\': \'datetime.datetime.now\'}),\\n            \'last_name\': (\'django.db.models.fields.CharField\', [],\\n                          {\'max_length\': \'30\', \'blank\': \'True\'}),\\n            \'password\': (\\n                \'django.db.models.fields.CharField\', [], {\'max_length\': \'128\'}),\\n            \'user_permissions\': (\\n                \'django.db.models.fields.related.ManyToManyField\', [],\\n                {\'to\': \\"orm[\'auth.Permission\']\\", \'symmetrical\': \'False\',\\n                 \'blank\': \'True\'}),\\n            \'username\': (\'django.db.models.fields.CharField\', [],\\n                         {\'unique\': \'True\', \'max_length\': \'30\'})\\n        },\\n        \'cms.cmsplugin\': {\\n            \'Meta\': {\'object_name\': \'CMSPlugin\'},\\n            \'changed_date\': (\'django.db.models.fields.DateTimeField\', [],\\n                             {\'auto_now\': \'True\', \'blank\': \'True\'}),\\n            \'creation_date\': (\'django.db.models.fields.DateTimeField\', [],\\n                              {\'default\': \'datetime.datetime.now\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'language\': (\'django.db.models.fields.CharField\', [],\\n                         {\'max_length\': \'15\', \'db_index\': \'True\'}),\\n            \'level\': (\'django.db.models.fields.PositiveIntegerField\', [],\\n                      {\'db_index\': \'True\'}),\\n            \'lft\': (\'django.db.models.fields.PositiveIntegerField\', [],\\n                    {\'db_index\': \'True\'}),\\n            \'parent\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                       {\'to\': \\"orm[\'cms.CMSPlugin\']\\", \'null\': \'True\',\\n                        \'blank\': \'True\'}),\\n            \'placeholder\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                            {\'to\': \\"orm[\'cms.Placeholder\']\\", \'null\': \'True\'}),\\n            \'plugin_type\': (\'django.db.models.fields.CharField\', [],\\n                            {\'max_length\': \'50\', \'db_index\': \'True\'}),\\n            \'position\': (\'django.db.models.fields.PositiveSmallIntegerField\', [],\\n                         {\'null\': \'True\', \'blank\': \'True\'}),\\n            \'rght\': (\'django.db.models.fields.PositiveIntegerField\', [],\\n                     {\'db_index\': \'True\'}),\\n            \'tree_id\': (\'django.db.models.fields.PositiveIntegerField\', [],\\n                        {\'db_index\': \'True\'})\\n        },\\n        \'cms.globalpagepermission\': {\\n            \'Meta\': {\'object_name\': \'GlobalPagePermission\'},\\n            \'can_add\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_change\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_change_advanced_settings\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'can_change_permissions\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'can_delete\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_moderate\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_move_page\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_publish\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_recover_page\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_view\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'group\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                      {\'to\': \\"orm[\'auth.Group\']\\", \'null\': \'True\', \'blank\': \'True\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'sites\': (\'django.db.models.fields.related.ManyToManyField\', [],\\n                      {\'symmetrical\': \'False\', \'to\': \\"orm[\'sites.Site\']\\",\\n                       \'null\': \'True\', \'blank\': \'True\'}),\\n            \'user\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                     {\'to\': \\"orm[\'%s\']\\" % user_orm_label, \'null\': \'True\', \'blank\': \'True\'})\\n        },\\n        \'cms.page\': {\\n            \'Meta\': {\'ordering\': \\"(\'site\', \'tree_id\', \'lft\')\\",\\n                     \'object_name\': \'Page\'},\\n            \'changed_by\': (\\n                \'django.db.models.fields.CharField\', [], {\'max_length\': \'70\'}),\\n            \'changed_date\': (\'django.db.models.fields.DateTimeField\', [],\\n                             {\'auto_now\': \'True\', \'blank\': \'True\'}),\\n            \'created_by\': (\\n                \'django.db.models.fields.CharField\', [], {\'max_length\': \'70\'}),\\n            \'creation_date\': (\'django.db.models.fields.DateTimeField\', [],\\n                              {\'auto_now_add\': \'True\', \'blank\': \'True\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'in_navigation\': (\'django.db.models.fields.BooleanField\', [],\\n                              {\'default\': \'True\', \'db_index\': \'True\'}),\\n            \'level\': (\'django.db.models.fields.PositiveIntegerField\', [],\\n                      {\'db_index\': \'True\'}),\\n            \'lft\': (\'django.db.models.fields.PositiveIntegerField\', [],\\n                    {\'db_index\': \'True\'}),\\n            \'limit_visibility_in_menu\': (\\n                \'django.db.models.fields.SmallIntegerField\', [],\\n                {\'default\': \'None\', \'null\': \'True\', \'db_index\': \'True\',\\n                 \'blank\': \'True\'}),\\n            \'login_required\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'moderator_state\': (\'django.db.models.fields.SmallIntegerField\', [],\\n                                {\'default\': \'1\', \'blank\': \'True\'}),\\n            \'navigation_extenders\': (\'django.db.models.fields.CharField\', [],\\n                                     {\'db_index\': \'True\', \'max_length\': \'80\',\\n                                      \'null\': \'True\', \'blank\': \'True\'}),\\n            \'parent\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                       {\'blank\': \'True\', \'related_name\': \\"\'children\'\\",\\n                        \'null\': \'True\', \'to\': \\"orm[\'cms.Page\']\\"}),\\n            \'placeholders\': (\'django.db.models.fields.related.ManyToManyField\', [],\\n                             {\'to\': \\"orm[\'cms.Placeholder\']\\",\\n                              \'symmetrical\': \'False\'}),\\n            \'publication_date\': (\'django.db.models.fields.DateTimeField\', [],\\n                                 {\'db_index\': \'True\', \'null\': \'True\',\\n                                  \'blank\': \'True\'}),\\n            \'publication_end_date\': (\'django.db.models.fields.DateTimeField\', [],\\n                                     {\'db_index\': \'True\', \'null\': \'True\',\\n                                      \'blank\': \'True\'}),\\n            \'published\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'publisher_is_draft\': (\'django.db.models.fields.BooleanField\', [],\\n                                   {\'default\': \'True\', \'db_index\': \'True\'}),\\n            \'publisher_public\': (\\n                \'django.db.models.fields.related.OneToOneField\', [],\\n                {\'related_name\': \\"\'publisher_draft\'\\", \'unique\': \'True\', \'null\': \'True\',\\n                 \'to\': \\"orm[\'cms.Page\']\\"}),\\n            \'publisher_state\': (\'django.db.models.fields.SmallIntegerField\', [],\\n                                {\'default\': \'0\', \'db_index\': \'True\'}),\\n            \'reverse_id\': (\'django.db.models.fields.CharField\', [],\\n                           {\'db_index\': \'True\', \'max_length\': \'40\', \'null\': \'True\',\\n                            \'blank\': \'True\'}),\\n            \'rght\': (\'django.db.models.fields.PositiveIntegerField\', [],\\n                     {\'db_index\': \'True\'}),\\n            \'site\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                     {\'to\': \\"orm[\'sites.Site\']\\"}),\\n            \'soft_root\': (\'django.db.models.fields.BooleanField\', [],\\n                          {\'default\': \'False\', \'db_index\': \'True\'}),\\n            \'template\': (\\n                \'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'tree_id\': (\'django.db.models.fields.PositiveIntegerField\', [],\\n                        {\'db_index\': \'True\'})\\n        },\\n        \'cms.pagemoderator\': {\\n            \'Meta\': {\'object_name\': \'PageModerator\'},\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'moderate_children\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'moderate_descendants\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'moderate_page\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'page\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                     {\'to\': \\"orm[\'cms.Page\']\\"}),\\n            \'user\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                     {\'to\': \\"orm[\'%s\']\\" % user_orm_label})\\n        },\\n        \'cms.pagemoderatorstate\': {\\n            \'Meta\': {\'ordering\': \\"(\'page\', \'action\', \'-created\')\\",\\n                     \'object_name\': \'PageModeratorState\'},\\n            \'action\': (\'django.db.models.fields.CharField\', [],\\n                       {\'max_length\': \'3\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'created\': (\'django.db.models.fields.DateTimeField\', [],\\n                        {\'auto_now_add\': \'True\', \'blank\': \'True\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'message\': (\'django.db.models.fields.TextField\', [],\\n                        {\'default\': \\"\'\'\\", \'max_length\': \'1000\', \'blank\': \'True\'}),\\n            \'page\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                     {\'to\': \\"orm[\'cms.Page\']\\"}),\\n            \'user\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                     {\'to\': \\"orm[\'%s\']\\" % user_orm_label, \'null\': \'True\'})\\n        },\\n        \'cms.pagepermission\': {\\n            \'Meta\': {\'object_name\': \'PagePermission\'},\\n            \'can_add\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_change\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_change_advanced_settings\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'can_change_permissions\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'can_delete\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_moderate\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_move_page\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_publish\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_view\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'grant_on\': (\\n                \'django.db.models.fields.IntegerField\', [], {\'default\': \'5\'}),\\n            \'group\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                      {\'to\': \\"orm[\'auth.Group\']\\", \'null\': \'True\', \'blank\': \'True\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'page\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                     {\'to\': \\"orm[\'cms.Page\']\\", \'null\': \'True\', \'blank\': \'True\'}),\\n            \'user\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                     {\'to\': \\"orm[\'%s\']\\" % user_orm_label, \'null\': \'True\', \'blank\': \'True\'})\\n        },\\n        \'cms.pageuser\': {\\n            \'Meta\': {\'object_name\': \'PageUser\', \'_ormbases\': [user_orm_label]},\\n            \'created_by\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                           {\'related_name\': \\"\'created_users\'\\",\\n                            \'to\': \\"orm[\'%s\']\\" % user_orm_label}),\\n            \'user_ptr\': (\'django.db.models.fields.related.OneToOneField\', [],\\n                         {\'to\': \\"orm[\'%s\']\\" % user_orm_label, \'unique\': \'True\',\\n                          \'primary_key\': \'True\'})\\n        },\\n        \'cms.pageusergroup\': {\\n            \'Meta\': {\'object_name\': \'PageUserGroup\', \'_ormbases\': [\'auth.Group\']},\\n            \'created_by\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                           {\'related_name\': \\"\'created_usergroups\'\\",\\n                            \'to\': \\"orm[\'%s\']\\" % user_orm_label}),\\n            \'group_ptr\': (\'django.db.models.fields.related.OneToOneField\', [],\\n                          {\'to\': \\"orm[\'auth.Group\']\\", \'unique\': \'True\',\\n                           \'primary_key\': \'True\'})\\n        },\\n        \'cms.placeholder\': {\\n            \'Meta\': {\'object_name\': \'Placeholder\'},\\n            \'default_width\': (\\n                \'django.db.models.fields.PositiveSmallIntegerField\', [],\\n                {\'null\': \'True\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'slot\': (\'django.db.models.fields.CharField\', [],\\n                     {\'max_length\': \'50\', \'db_index\': \'True\'})\\n        },\\n        \'cms.title\': {\\n            \'Meta\': {\'unique_together\': \\"((\'language\', \'page\'),)\\",\\n                     \'object_name\': \'Title\'},\\n            \'application_urls\': (\'django.db.models.fields.CharField\', [],\\n                                 {\'db_index\': \'True\', \'max_length\': \'200\',\\n                                  \'null\': \'True\', \'blank\': \'True\'}),\\n            \'creation_date\': (\'django.db.models.fields.DateTimeField\', [],\\n                              {\'default\': \'datetime.datetime.now\'}),\\n            \'has_url_overwrite\': (\'django.db.models.fields.BooleanField\', [],\\n                                  {\'default\': \'False\', \'db_index\': \'True\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'language\': (\'django.db.models.fields.CharField\', [],\\n                         {\'max_length\': \'15\', \'db_index\': \'True\'}),\\n            \'menu_title\': (\'django.db.models.fields.CharField\', [],\\n                           {\'max_length\': \'255\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'meta_description\': (\'django.db.models.fields.TextField\', [],\\n                                 {\'max_length\': \'255\', \'null\': \'True\',\\n                                  \'blank\': \'True\'}),\\n            \'meta_keywords\': (\'django.db.models.fields.CharField\', [],\\n                              {\'max_length\': \'255\', \'null\': \'True\',\\n                               \'blank\': \'True\'}),\\n            \'page\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                     {\'related_name\': \\"\'title_set\'\\", \'to\': \\"orm[\'cms.Page\']\\"}),\\n            \'page_title\': (\'django.db.models.fields.CharField\', [],\\n                           {\'max_length\': \'255\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'path\': (\'django.db.models.fields.CharField\', [],\\n                     {\'max_length\': \'255\', \'db_index\': \'True\'}),\\n            \'redirect\': (\'django.db.models.fields.CharField\', [],\\n                         {\'max_length\': \'255\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'slug\': (\\n                \'django.db.models.fields.SlugField\', [], {\'max_length\': \'255\'}),\\n            \'title\': (\\n                \'django.db.models.fields.CharField\', [], {\'max_length\': \'255\'})\\n        },\\n        \'contenttypes.contenttype\': {\\n            \'Meta\': {\'ordering\': \\"(\'name\',)\\",\\n                     \'unique_together\': \\"((\'app_label\', \'model\'),)\\",\\n                     \'object_name\': \'ContentType\',\\n                     \'db_table\': \\"\'django_content_type\'\\"},\\n            \'app_label\': (\\n                \'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'model\': (\\n                \'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'})\\n        },\\n        \'sites.site\': {\\n            \'Meta\': {\'ordering\': \\"(\'domain\',)\\", \'object_name\': \'Site\',\\n                     \'db_table\': \\"\'django_site\'\\"},\\n            \'domain\': (\\n                \'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\'})\\n        }\\n    }\\n\\n    complete_apps = [\'cms\']\\n"}\n'
line: b'{"repo_name":"chouseknecht/ansible","ref":"refs/heads/devel","path":"lib/ansible/module_utils/network/eos/argspec/facts/facts.py","content":"# -*- coding: utf-8 -*-\\n# Copyright 2019 Red Hat\\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\\n\\"\\"\\"\\nThe arg spec for the eos facts module.\\n\\"\\"\\"\\n\\nfrom __future__ import (absolute_import, division, print_function)\\n__metaclass__ = type\\n\\n\\nclass FactsArgs(object):\\n    \\"\\"\\" The arg spec for the eos facts module\\n    \\"\\"\\"\\n\\n    def __init__(self, **kwargs):\\n        pass\\n\\n    argument_spec = {\\n        \'gather_subset\': dict(default=[\'!config\'], type=\'list\'),\\n        \'gather_network_resources\': dict(type=\'list\'),\\n    }\\n"}\n'
line: b'{"repo_name":"alokotosh/mm-master","ref":"refs/heads/master","path":"mm/commands/misc.py","content":"import os\\nimport json\\nimport mm.util as util\\nimport mm.config as config\\nfrom mm.exceptions import *\\nfrom mm.basecommand import Command\\nfrom mm.sfdc_client import MavensMateClient\\n\\nclass GetActiveSessionCommand(Command):\\n    def execute(self):\\n        if \'username\' not in self.params or self.params[\'username\'] == None or self.params[\'username\'] == \'\':\\n            raise MMException(\'Please enter a Salesforce.com username\')\\n        if \'password\' not in self.params or self.params[\'password\'] == None or self.params[\'password\'] == \'\':\\n            raise MMException(\'Please enter a Salesforce.com password\')\\n        if \'org_type\' not in self.params or self.params[\'org_type\'] == None or self.params[\'org_type\'] == \'\':\\n            raise MMException(\'Please select an org type\')\\n        if \'org_type\' in self.params and self.params[\'org_type\'] == \\"custom\\" and \\"org_url\\" not in self.params:\\n            raise MMException(\'To use a custom org type, please include a org_url parameter\') \\n        if \'org_type\' in self.params and self.params[\'org_type\'] == \\"custom\\" and \\"org_url\\" in self.params and self.params[\\"org_url\\"] == \\"\\":\\n            raise MMException(\'Please specify the org url\')    \\n\\n        config.logger.debug(\'=================\\u003e\')\\n        config.logger.debug(self.params)\\n\\n        client = MavensMateClient(credentials={\\n            \\"username\\" : self.params[\'username\'],\\n            \\"password\\" : self.params[\'password\'],\\n            \\"org_type\\" : self.params[\'org_type\'],\\n            \\"org_url\\"  : self.params.get(\'org_url\', None)\\n        }) \\n        \\n        response = {\\n            \\"sid\\"                   : client.sid,\\n            \\"user_id\\"               : client.user_id,\\n            \\"metadata_server_url\\"   : client.metadata_server_url,\\n            \\"server_url\\"            : client.server_url,\\n            \\"metadata\\"              : client.get_org_metadata(subscription=self.params.get(\'subscription\', None)),\\n            \\"org_metadata_types\\"    : util.metadata_types(),\\n            \\"success\\"               : True\\n        }\\n        return util.generate_response(response)\\n\\nclass IndexApexSymbolsCommand(Command):\\n    aliases=[\\"index_apex\\",\\"index_apex_file_properties\\"]\\n    \\"\\"\\"\\n        Updates symbol index for one or more Apex Classes. If files is not included or empty, will force a full refresh\\n    \\"\\"\\"\\n    def execute(self):\\n        return config.project.index_apex_symbols(self.params.get(\\"files\\", None))\\n\\nclass ResetMetadataContainerCommand(Command):\\n    def execute(self):\\n        return config.project.reset_metadata_container(accept=\\"json\\")\\n\\nclass OpenFileInClientCommand(Command):\\n    \\"\\"\\"\\n        Opens the requested files in the plugin client (Sublime Text, etc.)\\n    \\"\\"\\"\\n    def execute(self):\\n        file_name = self.params[\\"file_name\\"]\\n        extension = util.get_file_extension_no_period(file_name)\\n        mtype = util.get_meta_type_by_suffix(extension)\\n        full_file_path = os.path.join(config.project.location, \\"src\\", mtype[\\"directoryName\\"], file_name)\\n        params = {\\n            \\"project_name\\"  : config.project.project_name,\\n            \\"file_name\\"     : full_file_path,\\n            \\"line_number\\"   : self.params.get(\\"line_number\\", 0)\\n        } \\n        config.connection.run_subl_command(\\"open_file_in_project\\", json.dumps(params))\\n        return util.generate_success_response(\\"ok\\")\\n\\nclass ExecuteApexCommand(Command):\\n    aliases=[\\"run_apex_script\\"]\\n    \\"\\"\\"\\n        executes a string of apex\\n    \\"\\"\\"\\n    def execute(self):\\n        if \'script_name\' in self.params: #running an apex script\\n            self.params[\\"body\\"] = util.get_file_as_string(os.path.join(config.project.location,\\"apex-scripts\\",self.params[\\"script_name\\"]))\\n        if \'debug_categories\' not in self.params and not os.path.isfile(os.path.join(config.project.location,\\"config\\",\\".apex_script\\")):\\n            self.params[\\"debug_categories\\"] = [\\n                {\\n                    \\"category\\"  : \\"Apex_code\\",\\n                    \\"level\\"     :  \\"DEBUG\\"\\n                }\\n            ]\\n        elif os.path.isfile(os.path.join(config.project.location,\\"config\\",\\".apex_script\\")):\\n            log_settings = util.parse_json_from_file(os.path.join(config.project.location,\\"config\\",\\".apex_script\\"))\\n            categories = []\\n            levels = log_settings[\\"levels\\"]\\n            for category in levels.keys():\\n                categories.append({\\n                    \\"category\\"  : category,\\n                    \\"level\\"     : levels[category]\\n                })\\n            self.params[\\"debug_categories\\"] = categories\\n        elif \'debug_categories\' not in self.params:\\n            self.params[\\"debug_categories\\"] = [\\n                {\\n                    \\"category\\"  : \\"Apex_code\\",\\n                    \\"level\\"     :  \\"DEBUG\\"\\n                }\\n            ]\\n        return_log = self.params.get(\\"return_log\\", True)\\n\\n        execute_result = config.sfdc_client.execute_apex(self.params)\\n        result = {\\n            \'column\'                : execute_result[\'column\'],\\n            \'compileProblem\'        : execute_result[\'compileProblem\'],\\n            \'compiled\'              : execute_result[\'compiled\'],\\n            \'exceptionMessage\'      : execute_result[\'exceptionMessage\'],\\n            \'exceptionStackTrace\'   : execute_result[\'exceptionStackTrace\'],\\n            \'line\'                  : execute_result[\'line\'],\\n            \'success\'               : execute_result[\'success\'],\\n        }\\n        if \'log\' in execute_result and return_log:\\n            result[\'log\'] = execute_result[\'log\']\\n        if result[\'success\']:\\n            log_apex = config.connection.get_plugin_client_setting(\'mm_log_anonymous_apex\', False)\\n            if log_apex:\\n                location = config.project.log_anonymous_apex(self.params[\'body\'], execute_result[\'log\'], self.params.get(\\"script_name\\", None))\\n                result[\\"log_location\\"] = location\\n        return util.generate_response(result)\\n\\nclass SignInWithGithubCommand(Command):\\n    def execute(self):\\n        return config.connection.sign_in_with_github(self.params)"}\n'
line: b'{"repo_name":"nischalsheth/contrail-controller","ref":"refs/heads/master","path":"src/vnsw/provisioning/contrail_vrouter_provisioning/network.py","content":"#!/usr/bin/env python\\n#\\n# Copyright (c) 2013 Juniper Networks, Inc. All rights reserved.\\n#\\n\\nimport os\\nimport re\\nimport glob\\nimport struct\\nimport socket\\nimport logging\\nimport netifaces\\n\\nfrom contrail_vrouter_provisioning import local\\n\\n\\nlog = logging.getLogger(\'contrail_vrouter_provisioning.network\')\\n\\n\\nclass ComputeNetworkSetup(object):\\n    def find_gateway(self, dev):\\n        gateway = \'\'\\n        cmd = \\"sudo netstat -rn | sudo grep ^\\\\\\"0.0.0.0\\\\\\" | \\"\\n        cmd += \\"sudo head -n 1 | sudo grep %s | sudo awk \'{ print $2 }\'\\" % dev\\n        gateway = local(cmd, capture=True).strip()\\n        return gateway\\n    # end find_gateway\\n\\n    def get_dns_servers(self, dev):\\n        cmd = \\"sudo grep \\\\\\"^nameserver\\\\\\\\\\u003e\\\\\\" /etc/resolv.conf | \\"\\n        cmd += \\"sudo awk  \'{print $2}\'\\"\\n        dns_list = local(cmd, capture=True)\\n        return dns_list.split()\\n    # end get_dns_servers\\n\\n    def get_domain_search_list(self):\\n        domain_list = \'\'\\n        cmd = \\"sudo grep ^\\\\\\"search\\\\\\" /etc/resolv.conf | \\"\\n        cmd += \\"sudo awk \'{$1=\\\\\\"\\\\\\";print $0}\'\\"\\n        domain_list = local(cmd, capture=True).strip()\\n        if not domain_list:\\n            cmd = \\"sudo grep ^\\\\\\"domain\\\\\\" /etc/resolv.conf | \\"\\n            cmd += \\"sudo awk \'{$1=\\\\\\"\\\\\\"; print $0}\'\\"\\n            domain_list = local(cmd, capture=True).strip()\\n        return domain_list\\n\\n    def get_if_mtu(self, dev):\\n        cmd = \\"sudo ifconfig %s | sudo grep mtu | sudo awk \'{ print $NF }\'\\" %\\\\\\n               dev\\n        mtu = local(cmd, capture=True).strip()\\n        if not mtu:\\n            # for debian\\n            cmd = r\\"sudo ifconfig %s | sudo grep MTU | \\" % dev\\n            cmd += r\\"sudo sed \'s/.*MTU.\\\\([0-9]\\\\+\\\\).*/\\\\1/g\'\\"\\n            mtu = local(cmd, capture=True).strip()\\n        if (mtu and mtu != \'1500\'):\\n            return mtu\\n        return \'\'\\n    # end if_mtu\\n\\n    def get_device_by_ip(self, ip):\\n        for i in netifaces.interfaces():\\n            try:\\n                if i == \'pkt1\':\\n                    continue\\n                if netifaces.AF_INET in netifaces.ifaddresses(i):\\n                    interfaces = netifaces.ifaddresses(i)[netifaces.AF_INET]\\n                    for interface in interfaces:\\n                        if ip == interface[\'addr\']:\\n                            if i == \'vhost0\':\\n                                log.info(\\"vhost0 is already present!\\")\\n                            return i\\n            except ValueError:\\n                log.info(\\"Skipping interface %s\\", i)\\n        raise RuntimeError(\'%s not configured, rerun w/ --physical_interface\' %\\n                           ip)\\n    # end get_device_by_ip\\n\\n    def get_device_info(self, ip):\\n        reprov = False\\n        cfg_file = \\"/etc/contrail/contrail-vrouter-agent.conf\\"\\n        try:\\n            dev = self.get_device_by_ip(ip)\\n            if dev == \\"vhost0\\":\\n                dev = self.get_config(cfg_file,\\n                                      \\"VIRTUAL-HOST-INTERFACE\\",\\n                                      \\"physical_interface\\")\\n                log.info(\\"Re-provision. vhost0 present\\")\\n                reprov = True\\n            else:\\n                log.info(\\"Fresh Install. vhost0 not present\\")\\n        except RuntimeError:\\n            dev = self.get_config(cfg_file,\\n                                  \\"VIRTUAL-HOST-INTERFACE\\",\\n                                  \\"physical_interface\\")\\n            if not dev.succeeded:\\n                raise\\n            log.info(\\"vhost0 not present, vrouter not running\\")\\n            reprov = True\\n        return (dev.strip(), reprov)\\n    # end get_device_info\\n\\n    def get_secondary_device(self, primary):\\n        for i in netifaces.interfaces():\\n            try:\\n                if i == \'pkt1\':\\n                    continue\\n                if i == primary:\\n                    continue\\n                if i == \'vhost0\':\\n                    continue\\n                if netifaces.AF_INET not in netifaces.ifaddresses(i):\\n                    return i\\n            except ValueError:\\n                log.info(\\"Skipping interface %s\\" % i)\\n        raise RuntimeError(\'Secondary interace  not configured,\',\\n                           \'rerun w/ --physical_interface\')\\n    # end get_secondary_device\\n\\n    def get_if_mac(self, dev):\\n        iface_addr = netifaces.ifaddresses(dev)\\n        link_info = iface_addr[netifaces.AF_LINK]\\n        mac_addr = link_info[0][\'addr\']\\n        return mac_addr\\n    # end get_if_mac\\n\\n    @staticmethod\\n    def is_interface_vlan(interface):\\n        iface = local(\\"sudo ip link show %s | head -1\\" % interface +\\n                      \\"| cut -f2 -d\':\' | grep \'@\'\\",\\n                      capture=True, warn_only=True)\\n        if iface.succeeded:\\n            return True\\n        else:\\n            return False\\n\\n    @staticmethod\\n    def get_physical_interface_of_vlan(interface):\\n        iface = local(\\"sudo ip link show %s | head -1 | cut -f2 -d\':\'\\" %\\n                      interface + \\"| cut -f2 -d\'@\'\\", capture=True)\\n        return iface\\n\\n    def _rewrite_ifcfg_file(self, filename, dev, prsv_cfg):\\n        bond = False\\n        mac = \'\'\\n        temp_dir_name = self._temp_dir_name\\n\\n        vlan = False\\n        if os.path.isfile(\'/proc/net/vlan/%s\' % dev):\\n            vlan_info = open(\'/proc/net/vlan/config\').readlines()\\n            match = re.search(\'^%s.*\\\\|\\\\s+(\\\\S+)$\' % dev, \\"\\\\n\\".join(vlan_info),\\n                              flags=re.M | re.I)\\n            if not match:\\n                raise RuntimeError(\\"Configured vlan %s is not found in\\",\\n                                   \\"/proc/net/vlan/config\\" % dev)\\n            vlan = True\\n\\n        if os.path.isdir(\'/sys/class/net/%s/bonding\' % dev):\\n            bond = True\\n        # end if os.path.isdir...\\n\\n        mac = netifaces.ifaddresses(dev)[netifaces.AF_LINK][0][\'addr\']\\n        ifcfg_file = \'/etc/sysconfig/network-scripts/ifcfg-%s\' % dev\\n        if not os.path.isfile(ifcfg_file):\\n            ifcfg_file = temp_dir_name + \'ifcfg-\' + dev\\n            with open(ifcfg_file, \'w\') as f:\\n                f.write(\'\'\'#Contrail %s\\nTYPE=Ethernet\\nONBOOT=yes\\nDEVICE=\\"%s\\"\\nUSERCTL=yes\\nNM_CONTROLLED=no\\nHWADDR=%s\\n\'\'\' % (dev, dev, mac))\\n                for dcfg in prsv_cfg:\\n                    f.write(dcfg+\'\\\\n\')\\n                if vlan:\\n                    f.write(\'VLAN=yes\\\\n\')\\n        fd = open(ifcfg_file)\\n        f_lines = fd.readlines()\\n        fd.close()\\n        local(\\"sudo rm -f %s\\" % ifcfg_file)\\n        new_f_lines = []\\n        remove_items = [\'IPADDR\', \'NETMASK\', \'PREFIX\', \'GATEWAY\', \'HWADDR\',\\n                        \'DNS1\', \'DNS2\', \'BOOTPROTO\', \'NM_CONTROLLED\',\\n                        \'#Contrail\']\\n\\n        remove_items.append(\'DEVICE\')\\n        new_f_lines.append(\'#Contrail %s\\\\n\' % dev)\\n        new_f_lines.append(\'DEVICE=%s\\\\n\' % dev)\\n\\n        for line in f_lines:\\n            found = False\\n            for text in remove_items:\\n                if text in line:\\n                    found = True\\n            if not found:\\n                new_f_lines.append(line)\\n\\n        new_f_lines.append(\'NM_CONTROLLED=no\\\\n\')\\n        if bond:\\n            new_f_lines.append(\'SUBCHANNELS=1,2,3\\\\n\')\\n        elif not vlan:\\n            new_f_lines.append(\'HWADDR=%s\\\\n\' % mac)\\n\\n        fdw = open(filename, \'w\')\\n        fdw.writelines(new_f_lines)\\n        fdw.close()\\n\\n    def migrate_routes(self, device):\\n        \'\'\'\\n        add route entries in /proc/net/route\\n        \'\'\'\\n        temp_dir_name = self._temp_dir_name\\n        cfg_file = \'/etc/sysconfig/network-scripts/route-vhost0\'\\n        tmp_file = \'%s/route-vhost0\' % temp_dir_name\\n        with open(tmp_file, \'w\') as route_cfg_file:\\n            for route in open(\'/proc/net/route\', \'r\').readlines():\\n                if route.startswith(device):\\n                    route_fields = route.split()\\n                    destination = int(route_fields[1], 16)\\n                    gateway = int(route_fields[2], 16)\\n                    flags = int(route_fields[3], 16)\\n                    mask = int(route_fields[7], 16)\\n                    if flags \\u0026 0x2:\\n                        if destination != 0:\\n                            route_cfg_file.write(socket.inet_ntoa(\\n                                struct.pack(\'I\', destination)))\\n                            route_cfg_file.write(\\n                                    \'/\' + str(bin(mask).count(\'1\')) + \' \')\\n                            route_cfg_file.write(\'via \')\\n                            route_cfg_file.write(socket.inet_ntoa(\\n                                struct.pack(\'I\', gateway)) + \' \')\\n                            route_cfg_file.write(\'dev vhost0\')\\n                        # end if detination...\\n                    # end if flags \\u0026...\\n                # end if route.startswith...\\n            # end for route...\\n        # end with open...\\n        local(\\"sudo mv -f %s %s\\" % (tmp_file, cfg_file))\\n        # delete the route-dev file\\n        if os.path.isfile(\'/etc/sysconfig/network-scripts/route-%s\' % device):\\n            os.unlink(\'/etc/sysconfig/network-scripts/route-%s\' % device)\\n    # end def migrate_routes\\n\\n    def get_cfgfile_for_dev(self, iface, cfg_files):\\n        if not cfg_files:\\n            return None\\n        mapped_intf_cfgfile = None\\n        for file in cfg_files:\\n            with open(file, \'r\') as fd:\\n                contents = fd.read()\\n                regex = \'(?:^|\\\\n)\\\\s*iface\\\\s+%s\\\\s+\' % iface\\n                if re.search(regex, contents):\\n                    mapped_intf_cfgfile = file\\n        return mapped_intf_cfgfile\\n\\n    def get_sourced_files(self):\\n        \'\'\'Get all sourced config files\'\'\'\\n        files = self.get_valid_files(self.get_source_entries())\\n        files += self.get_source_directory_files()\\n        return list(set(files))\\n\\n    def get_source_directory_files(self):\\n        \'\'\'Get source-directory entry and make list of valid files\'\'\'\\n        regex = \'(?:^|\\\\n)\\\\s*source-directory\\\\s+(\\\\S+)\'\\n        files = list()\\n        with open(self.default_cfg_file, \'r\') as fd:\\n            entries = re.findall(regex, fd.read())\\n        dirs = [d for d in self.get_valid_files(entries) if os.path.isdir(d)]\\n        for dir in dirs:\\n            files.extend([os.path.join(dir, f) for f in os.listdir(dir)\\n                          if os.path.isfile(os.path.join(dir, f)) and\\n                          re.match(\'^[a-zA-Z0-9_-]+$\', f)])\\n        return files\\n\\n    def get_source_entries(self):\\n        \'\'\'\\n        Get entries matching source keyword from\\n        /etc/network/interfaces file.\\n        \'\'\'\\n        regex = \'(?:^|\\\\n)\\\\s*source\\\\s+(\\\\S+)\'\\n        with open(self.default_cfg_file, \'r\') as fd:\\n            return re.findall(regex, fd.read())\\n\\n    def get_valid_files(self, entries):\\n        \'\'\'Provided a list of glob\'d strings, return matching file names\'\'\'\\n        files = list()\\n        prepend = os.path.join(os.path.sep, \'etc\', \'network\') + os.path.sep\\n        for entry in entries:\\n            entry = entry.lstrip(\'./\') if entry.startswith(\'./\') else entry\\n            if entry.startswith(os.path.sep):\\n                entry = entry\\n            else:\\n                entry = prepend+entry\\n            files.extend(glob.glob(entry))\\n        return files\\n\\n    def _rewrite_net_interfaces_file(self, dev, mac, vhost_ip,\\n                                     netmask, gateway_ip, esxi_vm,\\n                                     vmpg_mtu, datapg_mtu):\\n        self.default_cfg_file = \'/etc/network/interfaces\'\\n        cfg_files = self.get_sourced_files()\\n        cfg_files.append(self.default_cfg_file)\\n        intf_cfgfile = self.get_cfgfile_for_dev(\'vhost0\', cfg_files)\\n        if intf_cfgfile:\\n            log.info(\\"Interface vhost0 is already present in\\" +\\n                     \\"/etc/network/interfaces\\")\\n            log.info(\\"Skipping rewrite of this file\\")\\n            return\\n        # endif\\n\\n        vlan = False\\n        if os.path.isfile(\'/proc/net/vlan/%s\' % dev):\\n            vlan_info = open(\'/proc/net/vlan/config\').readlines()\\n            match = re.search(\'^%s.*\\\\|\\\\s+(\\\\S+)$\' % dev, \\"\\\\n\\".join(vlan_info),\\n                              flags=re.M | re.I)\\n            if not match:\\n                raise RuntimeError(\'Configured vlan %s is not found in\',\\n                                   \'/proc/net/vlan/config\' % dev)\\n            phydev = match.group(1)\\n            vlan = True\\n\\n        # Replace strings matching dev to vhost0 in ifup and ifdown parts file\\n        # Any changes to the file/logic with static routes has to be\\n        # reflected in setup-vnc-static-routes.py too\\n        ifup_parts_file = os.path.join(os.path.sep, \'etc\', \'network\',\\n                                       \'if-up.d\', \'routes\')\\n        ifdown_parts_file = os.path.join(os.path.sep, \'etc\', \'network\',\\n                                         \'if-down.d\', \'routes\')\\n\\n        if (os.path.isfile(ifup_parts_file) and\\n                os.path.isfile(ifdown_parts_file)):\\n            local(\\"sudo sed -i \'s/%s/vhost0/g\' %s\\" % (dev, ifup_parts_file),\\n                  warn_only=True)\\n            local(\\"sudo sed -i \'s/%s/vhost0/g\' %s\\" % (dev, ifdown_parts_file),\\n                  warn_only=True)\\n\\n        dev_cfgfile = self.get_cfgfile_for_dev(dev, cfg_files)\\n        temp_intf_file = \'%s/interfaces\' % self._temp_dir_name\\n        local(\\"sudo cp %s %s\\" % (dev_cfgfile, temp_intf_file))\\n        with open(dev_cfgfile, \'r\') as fd:\\n            cfg_file = fd.read()\\n\\n        if not self._args.non_mgmt_ip:\\n            # remove entry from auto \\u003cdev\\u003e to auto excluding these pattern\\n            # then delete specifically auto \\u003cdev\\u003e\\n            local(\\"sudo sed -i \'/auto %s/,/auto/{/auto/!d}\' %s\\" %\\n                  (dev, temp_intf_file))\\n            local(\\"sudo sed -i \'/auto %s/d\' %s\\" % (dev, temp_intf_file))\\n            # add manual entry for dev\\n            local(\\"sudo echo \'auto %s\' \\u003e\\u003e %s\\" % (dev, temp_intf_file))\\n            local(\\"sudo echo \'iface %s inet manual\' \\u003e\\u003e %s\\" %\\n                  (dev, temp_intf_file))\\n            if vlan:\\n                local(\\"sudo echo \'    post-up ifconfig %s up\' \\u003e\\u003e %s\\" %\\n                      (dev, temp_intf_file))\\n                local(\\"sudo echo \'    pre-down ifconfig %s down\' \\u003e\\u003e %s\\" %\\n                      (dev, temp_intf_file))\\n            else:\\n                local(\\"sudo echo \'    pre-up ifconfig %s up\' \\u003e\\u003e %s\\" %\\n                      (dev, temp_intf_file))\\n                local(\\"sudo echo \'    post-down ifconfig %s down\' \\u003e\\u003e %s\\" %\\n                      (dev, temp_intf_file))\\n            if esxi_vm:\\n                    local(\\"sudo echo \'    pre-up ifconfig %s up mtu %s\' \\u003e\\u003e %s\\"\\n                          % (dev, datapg_mtu, temp_intf_file))\\n                    cmd = \\"sudo ethtool -i %s | grep driver | cut -f 2 -d \' \'\\"\\\\\\n                          % dev\\n                    device_driver = local(cmd, capture=True)\\n                    if (device_driver == \\"vmxnet3\\"):\\n                        cmd = \\"sudo echo \'    pre-up ethtool --offload \\"\\n                        rx_cmd = (cmd +\\n                                  \\"%s rx off\' \\u003e\\u003e %s\\" % (dev, temp_intf_file))\\n                        tx_cmd = (cmd +\\n                                  \\"%s tx off\' \\u003e\\u003e %s\\" % (dev, temp_intf_file))\\n                        local(rx_cmd)\\n                        local(tx_cmd)\\n            if vlan:\\n                local(\\"sudo echo \'    vlan-raw-device %s\' \\u003e\\u003e %s\\" %\\n                      (phydev, temp_intf_file))\\n            if \'bond\' in dev.lower():\\n                iters = re.finditer(\'^\\\\s*auto\\\\s\', cfg_file, re.M)\\n                indices = [pat_match.start() for pat_match in iters]\\n                matches = map(cfg_file.__getslice__, indices,\\n                              indices[1:] + [len(cfg_file)])\\n                for each in matches:\\n                    each = each.strip()\\n                    if re.match(\'^auto\\\\s+%s\' % dev, each):\\n                        string = \'\'\\n                        for lines in each.splitlines():\\n                            if \'bond-\' in lines:\\n                                string += lines+os.linesep\\n                        local(\\"sudo echo \'%s\' \\u003e\\u003e %s\\" % (string,\\n                                                        temp_intf_file))\\n                    else:\\n                        continue\\n            local(\\"sudo echo \'\' \\u003e\\u003e %s\\" % temp_intf_file)\\n        else:\\n            # remove ip address and gateway\\n            local(\\"sudo sed -i \'/iface %s inet static/, +2d\' %s\\" %\\n                  (dev, temp_intf_file), warn_only=True)\\n            if esxi_vm:\\n                local(\\"sudo echo \'    pre-up ifconfig %s up mtu %s\' \\u003e\\u003e %s\\" %\\n                      (dev, datapg_mtu, temp_intf_file), warn_only=True)\\n                cmd = \\"sudo ethtool -i %s | \\" % dev\\n                cmd += \\"sudo grep driver | sudo cut -f 2 -d \' \'\\"\\n                device_driver = local(cmd, capture=True, warn_only=True)\\n                if (device_driver == \\"vmxnet3\\"):\\n                    cmd = \\"sudo echo \'    pre-up ethtool --offload \\"\\n                    rx_cmd = cmd + \\"%s rx off\' \\u003e\\u003e %s\\" % (dev, temp_intf_file)\\n                    tx_cmd = cmd + \\"%s tx off\' \\u003e\\u003e %s\\" % (dev, temp_intf_file)\\n                    local(rx_cmd, warn_only=True)\\n                    local(tx_cmd, warn_only=True)\\n            if vlan:\\n                cmd = \\"sudo sed -i \'/auto %s/ a\\\\iface %s inet manual\\\\\\\\n    \\" %\\\\\\n                       (dev, dev)\\n                cmd += \\"post-up ifconfig %s up\\\\\\\\n    \\" % dev\\n                cmd += \\"pre-down ifconfig %s down\\\\\' %s\\" % (dev, temp_intf_file)\\n                local(cmd)\\n            else:\\n                cmd = \\"sudo sed -i \'/auto %s/ a\\\\iface %s inet manual\\\\\\\\n    \\" %\\\\\\n                       (dev, dev)\\n                cmd += \\"pre-up ifconfig %s up\\\\\\\\n    \\" % dev\\n                cmd += \\"post-down ifconfig %s down\\\\\' %s\\" %\\\\\\n                       (dev, temp_intf_file)\\n                local(cmd)\\n        if esxi_vm and vmpg_mtu:\\n            intf = self.get_secondary_device(self.dev)\\n            mac_addr = self.get_if_mac(intf)\\n            udev_net_file = \'/etc/udev/rules.d/70-persistent-net.rules\'\\n            temp_udev_net_file = \'%s/70-persistent-net.rules\' %\\\\\\n                                 (self._temp_dir_name)\\n            local(\\"sudo touch %s\\" % temp_udev_net_file)\\n            local(\\"sudo cp %s %s\\" % (udev_net_file, temp_udev_net_file))\\n            cmd = \\"sudo echo \'SUBSYSTEM==\\\\\\"net\\\\\\", ACTION==\\\\\\"add\\\\\\",\\"\\n            cmd += \\" DRIVERS==\\\\\\"?*\\\\\\",\\"\\n            cmd += \\" ATTR{address}==\\\\\\"%s\\\\\\", ATTR{dev_id}==\\\\\\"0x0\\\\\\", \\" % mac_addr\\n            cmd += \\"ATTR{type}==\\\\\\"1\\\\\\", KERNEL==\\\\\\"eth*\\\\\\", NAME=\\\\\\"%s\\\\\\"\' \\u003e\\u003e %s\\" %\\\\\\n                   (intf, temp_udev_net_file)\\n            local(cmd)\\n            local(\\"sudo mv -f %s %s\\" % (temp_udev_net_file, udev_net_file))\\n            local(\\"sudo sed -i \'/auto %s/,/down/d\' %s\\" % (intf,\\n                                                          temp_intf_file))\\n            local(\\"sudo echo \'\\\\nauto %s\' \\u003e\\u003e %s\\" % (intf, temp_intf_file))\\n            local(\\"sudo echo \'iface %s inet manual\' \\u003e\\u003e %s\\" % (intf,\\n                                                              temp_intf_file))\\n            local(\\"sudo echo \'    pre-up ifconfig %s up mtu %s\' \\u003e\\u003e %s\\" %\\n                  (intf, vmpg_mtu, temp_intf_file))\\n            local(\\"sudo echo \'    post-down ifconfig %s down\' \\u003e\\u003e %s\\" %\\n                  (intf, temp_intf_file))\\n            local(\\"sudo echo \'    pre-up ethtool --offload %s lro off\' \\u003e\\u003e %s\\" %\\n                  (intf, temp_intf_file))\\n\\n        # populte vhost0 as static\\n        local(\\"sudo echo \'\' \\u003e\\u003e %s\\" % (temp_intf_file))\\n        local(\\"sudo echo \'auto vhost0\' \\u003e\\u003e %s\\" % (temp_intf_file))\\n        local(\\"sudo echo \'iface vhost0 inet static\' \\u003e\\u003e %s\\" % (temp_intf_file))\\n        local(\\"sudo echo \'    pre-up %s/if-vhost0\' \\u003e\\u003e %s\\" %\\n              (self.contrail_bin_dir, temp_intf_file))\\n        local(\\"sudo echo \'    netmask %s\' \\u003e\\u003e %s\\" % (netmask, temp_intf_file))\\n        local(\\"sudo echo \'    network_name application\' \\u003e\\u003e %s\\" %\\n              temp_intf_file)\\n        if esxi_vm and datapg_mtu:\\n            local(\\"sudo echo \'    mtu %s\' \\u003e\\u003e %s\\" % (datapg_mtu,\\n                                                    temp_intf_file))\\n        if vhost_ip:\\n            local(\\"sudo echo \'    address %s\' \\u003e\\u003e %s\\" % (vhost_ip,\\n                                                        temp_intf_file))\\n        if (not self._args.non_mgmt_ip) and gateway_ip:\\n            local(\\"sudo echo \'    gateway %s\' \\u003e\\u003e %s\\" % (gateway_ip,\\n                                                        temp_intf_file))\\n\\n        domain = self.get_domain_search_list()\\n        if domain:\\n            local(\\"sudo echo \'    dns-search %s\' \\u003e\\u003e %s\\" % (domain,\\n                                                           temp_intf_file))\\n        dns_list = self.get_dns_servers(dev)\\n        if dns_list:\\n            local(\\"sudo echo -n \'    dns-nameservers\' \\u003e\\u003e %s\\" %\\n                  temp_intf_file)\\n            for dns in dns_list:\\n                local(\\"sudo echo -n \' %s\' \\u003e\\u003e %s\\" % (dns, temp_intf_file))\\n            local(\\"sudo echo \'\' \\u003e\\u003e %s\\" % temp_intf_file)\\n        local(\\"sudo echo \'    post-up ip link set vhost0 address %s\' \\u003e\\u003e %s\\" %\\n              (mac, temp_intf_file))\\n\\n        # move it to right place\\n        local(\\"sudo mv -f %s %s\\" % (temp_intf_file, dev_cfgfile))\\n"}\n'
line: b'{"repo_name":"mcrowson/django","ref":"refs/heads/master","path":"tests/choices/tests.py","content":"from django.test import TestCase\\n\\nfrom .models import Person\\n\\n\\nclass ChoicesTests(TestCase):\\n    def test_display(self):\\n        a = Person.objects.create(name=\'Adrian\', gender=\'M\')\\n        s = Person.objects.create(name=\'Sara\', gender=\'F\')\\n        self.assertEqual(a.gender, \'M\')\\n        self.assertEqual(s.gender, \'F\')\\n\\n        self.assertEqual(a.get_gender_display(), \'Male\')\\n        self.assertEqual(s.get_gender_display(), \'Female\')\\n\\n        # If the value for the field doesn\'t correspond to a valid choice,\\n        # the value itself is provided as a display value.\\n        a.gender = \'\'\\n        self.assertEqual(a.get_gender_display(), \'\')\\n\\n        a.gender = \'U\'\\n        self.assertEqual(a.get_gender_display(), \'U\')\\n"}\n'
line: b'{"repo_name":"RPi-Distro/python-gpiozero","ref":"refs/heads/master","path":"gpiozerocli/pinout.py","content":"# GPIO Zero: a library for controlling the Raspberry Pi\'s GPIO pins\\n# Copyright (c) 2017-2019 Dave Jones \\u003cdave@waveform.org.uk\\u003e\\n# Copyright (c) 2017 Ben Nuttall \\u003cben@bennuttall.com\\u003e\\n#\\n# Redistribution and use in source and binary forms, with or without\\n# modification, are permitted provided that the following conditions are met:\\n#\\n# * Redistributions of source code must retain the above copyright notice,\\n#   this list of conditions and the following disclaimer.\\n#\\n# * Redistributions in binary form must reproduce the above copyright notice,\\n#   this list of conditions and the following disclaimer in the documentation\\n#   and/or other materials provided with the distribution.\\n#\\n# * Neither the name of the copyright holder nor the names of its contributors\\n#   may be used to endorse or promote products derived from this software\\n#   without specific prior written permission.\\n#\\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \\"AS IS\\"\\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\\n# POSSIBILITY OF SUCH DAMAGE.\\n\\n\\"\\"\\"\\nA utility for querying Raspberry Pi GPIO pin-out information.\\n\\"\\"\\"\\n\\nfrom __future__ import (\\n    unicode_literals,\\n    absolute_import,\\n    print_function,\\n    division,\\n)\\n\\nimport argparse\\nimport sys\\nimport textwrap\\nimport warnings\\nimport webbrowser\\n\\nfrom gpiozero import pi_info\\n\\n\\nclass PinoutTool(object):\\n    def __init__(self):\\n        self.parser = argparse.ArgumentParser(\\n            description=__doc__\\n        )\\n        self.parser.add_argument(\\n            \'-r\', \'--revision\',\\n            dest=\'revision\',\\n            default=\'\',\\n            help=\'RPi revision. Default is to autodetect revision of current device\'\\n        )\\n        self.parser.add_argument(\\n            \'-c\', \'--color\',\\n            action=\\"store_true\\",\\n            default=None,\\n            help=\'Force colored output (by default, the output will include ANSI\'\\n            \'color codes if run in a color-capable terminal). See also --monochrome\'\\n        )\\n        self.parser.add_argument(\\n            \'-m\', \'--monochrome\',\\n            dest=\'color\',\\n            action=\'store_false\',\\n            help=\'Force monochrome output. See also --color\'\\n        )\\n        self.parser.add_argument(\\n            \'-x\', \'--xyz\',\\n            dest=\'xyz\',\\n            action=\'store_true\',\\n            help=\'Open pinout.xyz in the default web browser\'\\n        )\\n\\n    def __call__(self, args=None):\\n        if args is None:\\n            args = sys.argv[1:]\\n        try:\\n            return self.main(self.parser.parse_args(args)) or 0\\n        except argparse.ArgumentError as e:\\n            # argparse errors are already nicely formatted, print to stderr and\\n            # exit with code 2\\n            raise e\\n        except Exception as e:\\n            raise\\n            # Output anything else nicely formatted on stderr and exit code 1\\n            self.parser.exit(1, \'{prog}: error: {message}\\\\n\'.format(\\n                prog=self.parser.prog, message=e))\\n\\n    def main(self, args):\\n        warnings.simplefilter(\'ignore\')\\n        if args.xyz:\\n            webbrowser.open(\'https://pinout.xyz\')\\n        else:\\n            if args.revision == \'\':\\n                try:\\n                    pi_info().pprint(color=args.color)\\n                except ImportError:\\n                    formatter = self.parser._get_formatter()\\n                    formatter.add_text(\\n                        \\"Unable to initialize GPIO Zero. This usually means \\"\\n                        \\"that you are not running %(prog)s on a Raspberry Pi. \\"\\n                        \\"If you still wish to run %(prog)s, set the \\"\\n                        \\"GPIOZERO_PIN_FACTORY environment variable to \'mock\' \\"\\n                        \\"and retry, or refer to the Remote GPIO section of \\"\\n                        \\"the manual* to configure your environment to \\"\\n                        \\"remotely access your Pi.\\"\\n                    )\\n                    formatter.add_text(\\n                        \\"* https://gpiozero.readthedocs.io/en/stable/\\"\\n                        \\"remote_gpio.html\\"\\n                    )\\n                    sys.stderr.write(formatter.format_help())\\n                except IOError:\\n                    raise IOError(\'This device is not a Raspberry Pi\')\\n            else:\\n                pi_info(args.revision).pprint(color=args.color)\\n            formatter = self.parser._get_formatter()\\n            formatter.add_text(\\n                \\"For further information, please refer to \\"\\n                \\"https://pinout.xyz/\\"\\n            )\\n            sys.stdout.write(\'\\\\n\')\\n            sys.stdout.write(formatter.format_help())\\n\\n\\nmain = PinoutTool()\\n"}\n'
line: b'{"repo_name":"lordmos/blink","ref":"refs/heads/master","path":"Tools/TestResultServer/model/jsonresults_unittest.py","content":"# Copyright (C) 2010 Google Inc. All rights reserved.\\n#\\n# Redistribution and use in source and binary forms, with or without\\n# modification, are permitted provided that the following conditions are\\n# met:\\n#\\n#     * Redistributions of source code must retain the above copyright\\n# notice, this list of conditions and the following disclaimer.\\n#     * Redistributions in binary form must reproduce the above\\n# copyright notice, this list of conditions and the following disclaimer\\n# in the documentation and/or other materials provided with the\\n# distribution.\\n#     * Neither the name of Google Inc. nor the names of its\\n# contributors may be used to endorse or promote products derived from\\n# this software without specific prior written permission.\\n#\\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\\n# \\"AS IS\\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n\\ntry:\\n    import jsonresults\\n    from jsonresults import *\\nexcept ImportError:\\n    print \\"ERROR: Add the TestResultServer, google_appengine and yaml/lib directories to your PYTHONPATH\\"\\n    raise\\n\\nimport json\\nimport logging\\nimport unittest\\n\\nFULL_RESULT_EXAMPLE = \\"\\"\\"ADD_RESULTS({\\n    \\"seconds_since_epoch\\": 1368146629,\\n    \\"tests\\": {\\n        \\"media\\": {\\n            \\"encrypted-media\\": {\\n                \\"encrypted-media-v2-events.html\\": {\\n                    \\"bugs\\": [\\"crbug.com/1234\\"],\\n                    \\"expected\\": \\"TIMEOUT\\",\\n                    \\"actual\\": \\"TIMEOUT\\",\\n                    \\"time\\": 6.0\\n                },\\n                \\"encrypted-media-v2-syntax.html\\": {\\n                    \\"expected\\": \\"TIMEOUT\\",\\n                    \\"actual\\": \\"TIMEOUT\\"\\n                }\\n            },\\n            \\"progress-events-generated-correctly.html\\": {\\n                \\"expected\\": \\"PASS FAIL IMAGE TIMEOUT CRASH MISSING\\",\\n                \\"actual\\": \\"TIMEOUT\\",\\n                \\"time\\": 6.0\\n            },\\n            \\"W3C\\": {\\n                \\"audio\\": {\\n                    \\"src\\": {\\n                        \\"src_removal_does_not_trigger_loadstart.html\\": {\\n                            \\"expected\\": \\"PASS\\",\\n                            \\"actual\\": \\"PASS\\",\\n                            \\"time\\": 3.5\\n                        }\\n                    }\\n                },\\n                \\"video\\": {\\n                    \\"src\\": {\\n                        \\"src_removal_does_not_trigger_loadstart.html\\": {\\n                            \\"expected\\": \\"PASS\\",\\n                            \\"actual\\": \\"PASS\\",\\n                            \\"time\\": 1.1\\n                        },\\n                        \\"notrun.html\\": {\\n                            \\"expected\\": \\"NOTRUN\\",\\n                            \\"actual\\": \\"SKIP\\",\\n                            \\"time\\": 1.1\\n                        }\\n                    }\\n                }\\n            },\\n            \\"unexpected-skip.html\\": {\\n                \\"expected\\": \\"PASS\\",\\n                \\"actual\\": \\"SKIP\\"\\n            },\\n            \\"unexpected-fail.html\\": {\\n                \\"expected\\": \\"PASS\\",\\n                \\"actual\\": \\"FAIL\\"\\n            },\\n            \\"flaky-failed.html\\": {\\n                \\"expected\\": \\"PASS FAIL\\",\\n                \\"actual\\": \\"FAIL\\"\\n            },\\n            \\"media-document-audio-repaint.html\\": {\\n                \\"expected\\": \\"IMAGE\\",\\n                \\"actual\\": \\"IMAGE\\",\\n                \\"time\\": 0.1\\n            }\\n        }\\n    },\\n    \\"skipped\\": 2,\\n    \\"num_regressions\\": 0,\\n    \\"build_number\\": \\"3\\",\\n    \\"interrupted\\": false,\\n    \\"layout_tests_dir\\": \\"\\\\/tmp\\\\/cr\\\\/src\\\\/third_party\\\\/WebKit\\\\/LayoutTests\\",\\n    \\"version\\": 3,\\n    \\"builder_name\\": \\"Webkit\\",\\n    \\"num_passes\\": 10,\\n    \\"pixel_tests_enabled\\": true,\\n    \\"blink_revision\\": \\"1234\\",\\n    \\"has_pretty_patch\\": true,\\n    \\"fixable\\": 25,\\n    \\"num_flaky\\": 0,\\n    \\"num_failures_by_type\\": {\\n        \\"CRASH\\": 3,\\n        \\"MISSING\\": 0,\\n        \\"TEXT\\": 3,\\n        \\"IMAGE\\": 1,\\n        \\"PASS\\": 10,\\n        \\"SKIP\\": 2,\\n        \\"TIMEOUT\\": 16,\\n        \\"IMAGE+TEXT\\": 0,\\n        \\"FAIL\\": 2,\\n        \\"AUDIO\\": 0\\n    },\\n    \\"has_wdiff\\": true,\\n    \\"chromium_revision\\": \\"5678\\"\\n});\\"\\"\\"\\n\\nJSON_RESULTS_OLD_TEMPLATE = (\\n    \'{\\"[BUILDER_NAME]\\":{\'\\n    \'\\"allFixableCount\\":[[TESTDATA_COUNT]],\'\\n    \'\\"blinkRevision\\":[[TESTDATA_WEBKITREVISION]],\'\\n    \'\\"buildNumbers\\":[[TESTDATA_BUILDNUMBERS]],\'\\n    \'\\"chromeRevision\\":[[TESTDATA_CHROMEREVISION]],\'\\n    \'\\"failure_map\\": %s,\'\\n    \'\\"fixableCount\\":[[TESTDATA_COUNT]],\'\\n    \'\\"fixableCounts\\":[[TESTDATA_COUNTS]],\'\\n    \'\\"secondsSinceEpoch\\":[[TESTDATA_TIMES]],\'\\n    \'\\"tests\\":{[TESTDATA_TESTS]}\'\\n    \'},\'\\n    \'\\"version\\":[VERSION]\'\\n    \'}\') % json.dumps(CHAR_TO_FAILURE)\\n\\nJSON_RESULTS_COUNTS = \'{\\"\' + \'\\":[[TESTDATA_COUNT]],\\"\'.join([char for char in CHAR_TO_FAILURE.values()]) + \'\\":[[TESTDATA_COUNT]]}\'\\n\\nJSON_RESULTS_TEMPLATE = (\\n    \'{\\"[BUILDER_NAME]\\":{\'\\n    \'\\"blinkRevision\\":[[TESTDATA_WEBKITREVISION]],\'\\n    \'\\"buildNumbers\\":[[TESTDATA_BUILDNUMBERS]],\'\\n    \'\\"chromeRevision\\":[[TESTDATA_CHROMEREVISION]],\'\\n    \'\\"failure_map\\": %s,\'\\n    \'\\"num_failures_by_type\\":%s,\'\\n    \'\\"secondsSinceEpoch\\":[[TESTDATA_TIMES]],\'\\n    \'\\"tests\\":{[TESTDATA_TESTS]}\'\\n    \'},\'\\n    \'\\"version\\":[VERSION]\'\\n    \'}\') % (json.dumps(CHAR_TO_FAILURE), JSON_RESULTS_COUNTS)\\n\\nJSON_RESULTS_COUNTS_TEMPLATE = \'{\\"\' + \'\\":[TESTDATA],\\"\'.join([char for char in CHAR_TO_FAILURE]) + \'\\":[TESTDATA]}\'\\n\\nJSON_RESULTS_TEST_LIST_TEMPLATE = \'{\\"Webkit\\":{\\"tests\\":{[TESTDATA_TESTS]}}}\'\\n\\n\\nclass MockFile(object):\\n    def __init__(self, name=\'results.json\', data=\'\'):\\n        self.master = \'MockMasterName\'\\n        self.builder = \'MockBuilderName\'\\n        self.test_type = \'MockTestType\'\\n        self.name = name\\n        self.data = data\\n\\n    def save(self, data):\\n        self.data = data\\n        return True\\n\\n\\nclass JsonResultsTest(unittest.TestCase):\\n    def setUp(self):\\n        self._builder = \\"Webkit\\"\\n        self.old_log_level = logging.root.level\\n        logging.root.setLevel(logging.ERROR)\\n\\n    def tearDown(self):\\n        logging.root.setLevel(self.old_log_level)\\n\\n    # Use this to get better error messages than just string compare gives.\\n    def assert_json_equal(self, a, b):\\n        self.maxDiff = None\\n        a = json.loads(a) if isinstance(a, str) else a\\n        b = json.loads(b) if isinstance(b, str) else b\\n        self.assertEqual(a, b)\\n\\n    def test_strip_prefix_suffix(self):\\n        json = \\"[\'contents\']\\"\\n        self.assertEqual(JsonResults._strip_prefix_suffix(\\"ADD_RESULTS(\\" + json + \\");\\"), json)\\n        self.assertEqual(JsonResults._strip_prefix_suffix(json), json)\\n\\n    def _make_test_json(self, test_data, json_string=JSON_RESULTS_TEMPLATE, builder_name=\\"Webkit\\"):\\n        if not test_data:\\n            return \\"\\"\\n\\n        builds = test_data[\\"builds\\"]\\n        tests = test_data[\\"tests\\"]\\n        if not builds or not tests:\\n            return \\"\\"\\n\\n        counts = []\\n        build_numbers = []\\n        webkit_revision = []\\n        chrome_revision = []\\n        times = []\\n        for build in builds:\\n            counts.append(JSON_RESULTS_COUNTS_TEMPLATE.replace(\\"[TESTDATA]\\", build))\\n            build_numbers.append(\\"1000%s\\" % build)\\n            webkit_revision.append(\\"2000%s\\" % build)\\n            chrome_revision.append(\\"3000%s\\" % build)\\n            times.append(\\"100000%s000\\" % build)\\n\\n        json_string = json_string.replace(\\"[BUILDER_NAME]\\", builder_name)\\n        json_string = json_string.replace(\\"[TESTDATA_COUNTS]\\", \\",\\".join(counts))\\n        json_string = json_string.replace(\\"[TESTDATA_COUNT]\\", \\",\\".join(builds))\\n        json_string = json_string.replace(\\"[TESTDATA_BUILDNUMBERS]\\", \\",\\".join(build_numbers))\\n        json_string = json_string.replace(\\"[TESTDATA_WEBKITREVISION]\\", \\",\\".join(webkit_revision))\\n        json_string = json_string.replace(\\"[TESTDATA_CHROMEREVISION]\\", \\",\\".join(chrome_revision))\\n        json_string = json_string.replace(\\"[TESTDATA_TIMES]\\", \\",\\".join(times))\\n\\n        version = str(test_data[\\"version\\"]) if \\"version\\" in test_data else \\"4\\"\\n        json_string = json_string.replace(\\"[VERSION]\\", version)\\n        json_string = json_string.replace(\\"{[TESTDATA_TESTS]}\\", json.dumps(tests, separators=(\',\', \':\'), sort_keys=True))\\n        return json_string\\n\\n    def _test_merge(self, aggregated_data, incremental_data, expected_data, max_builds=jsonresults.JSON_RESULTS_MAX_BUILDS):\\n        aggregated_results = self._make_test_json(aggregated_data, builder_name=self._builder)\\n        incremental_json, _ = JsonResults._get_incremental_json(self._builder, self._make_test_json(incremental_data, builder_name=self._builder), is_full_results_format=False)\\n        merged_results, status_code = JsonResults.merge(self._builder, aggregated_results, incremental_json, num_runs=max_builds, sort_keys=True)\\n\\n        if expected_data:\\n            expected_results = self._make_test_json(expected_data, builder_name=self._builder)\\n            self.assert_json_equal(merged_results, expected_results)\\n            self.assertEqual(status_code, 200)\\n        else:\\n            self.assertTrue(status_code != 200)\\n\\n    def _test_get_test_list(self, input_data, expected_data):\\n        input_results = self._make_test_json(input_data)\\n        expected_results = JSON_RESULTS_TEST_LIST_TEMPLATE.replace(\\"{[TESTDATA_TESTS]}\\", json.dumps(expected_data, separators=(\',\', \':\')))\\n        actual_results = JsonResults.get_test_list(self._builder, input_results)\\n        self.assert_json_equal(actual_results, expected_results)\\n\\n    def test_update_files_empty_aggregate_data(self):\\n        small_file = MockFile(name=\'results-small.json\')\\n        large_file = MockFile(name=\'results.json\')\\n\\n        incremental_data = {\\n            \\"builds\\": [\\"2\\", \\"1\\"],\\n            \\"tests\\": {\\n                \\"001.html\\": {\\n                    \\"results\\": [[200, TEXT]],\\n                    \\"times\\": [[200, 0]],\\n                }\\n            }\\n        }\\n        incremental_string = self._make_test_json(incremental_data, builder_name=small_file.builder)\\n\\n        self.assertTrue(JsonResults.update_files(small_file.builder, incremental_string, small_file, large_file, is_full_results_format=False))\\n        self.assert_json_equal(small_file.data, incremental_string)\\n        self.assert_json_equal(large_file.data, incremental_string)\\n\\n    def test_update_files_null_incremental_data(self):\\n        small_file = MockFile(name=\'results-small.json\')\\n        large_file = MockFile(name=\'results.json\')\\n\\n        aggregated_data = {\\n            \\"builds\\": [\\"2\\", \\"1\\"],\\n            \\"tests\\": {\\n                \\"001.html\\": {\\n                    \\"results\\": [[200, TEXT]],\\n                    \\"times\\": [[200, 0]],\\n                }\\n            }\\n        }\\n        aggregated_string = self._make_test_json(aggregated_data, builder_name=small_file.builder)\\n\\n        small_file.data = large_file.data = aggregated_string\\n\\n        incremental_string = \\"\\"\\n\\n        self.assertEqual(JsonResults.update_files(small_file.builder, incremental_string, small_file, large_file, is_full_results_format=False),\\n            (\'No incremental JSON data to merge.\', 403))\\n        self.assert_json_equal(small_file.data, aggregated_string)\\n        self.assert_json_equal(large_file.data, aggregated_string)\\n\\n    def test_update_files_empty_incremental_data(self):\\n        small_file = MockFile(name=\'results-small.json\')\\n        large_file = MockFile(name=\'results.json\')\\n\\n        aggregated_data = {\\n            \\"builds\\": [\\"2\\", \\"1\\"],\\n            \\"tests\\": {\\n                \\"001.html\\": {\\n                    \\"results\\": [[200, TEXT]],\\n                    \\"times\\": [[200, 0]],\\n                }\\n            }\\n        }\\n        aggregated_string = self._make_test_json(aggregated_data, builder_name=small_file.builder)\\n\\n        small_file.data = large_file.data = aggregated_string\\n\\n        incremental_data = {\\n            \\"builds\\": [],\\n            \\"tests\\": {}\\n        }\\n        incremental_string = self._make_test_json(incremental_data, builder_name=small_file.builder)\\n\\n        self.assertEqual(JsonResults.update_files(small_file.builder, incremental_string, small_file, large_file, is_full_results_format=False),\\n            (\'No incremental JSON data to merge.\', 403))\\n        self.assert_json_equal(small_file.data, aggregated_string)\\n        self.assert_json_equal(large_file.data, aggregated_string)\\n\\n    def test_merge_with_empty_aggregated_results(self):\\n        incremental_data = {\\n            \\"builds\\": [\\"2\\", \\"1\\"],\\n            \\"tests\\": {\\n                \\"001.html\\": {\\n                    \\"results\\": [[200, TEXT]],\\n                    \\"times\\": [[200, 0]],\\n                }\\n            }\\n        }\\n        incremental_results, _ = JsonResults._get_incremental_json(self._builder, self._make_test_json(incremental_data), is_full_results_format=False)\\n        aggregated_results = \\"\\"\\n        merged_results, _ = JsonResults.merge(self._builder, aggregated_results, incremental_results, num_runs=jsonresults.JSON_RESULTS_MAX_BUILDS, sort_keys=True)\\n        self.assert_json_equal(merged_results, incremental_results)\\n\\n    def test_failures_by_type_added(self):\\n        aggregated_results = self._make_test_json({\\n            \\"builds\\": [\\"2\\", \\"1\\"],\\n            \\"tests\\": {\\n                \\"001.html\\": {\\n                    \\"results\\": [[100, TEXT], [100, FAIL]],\\n                    \\"times\\": [[200, 0]],\\n                }\\n            }\\n        }, json_string=JSON_RESULTS_OLD_TEMPLATE)\\n        incremental_results = self._make_test_json({\\n            \\"builds\\": [\\"3\\"],\\n            \\"tests\\": {\\n                \\"001.html\\": {\\n                    \\"results\\": [[1, TEXT]],\\n                    \\"times\\": [[1, 0]],\\n                }\\n            }\\n        }, json_string=JSON_RESULTS_OLD_TEMPLATE)\\n        incremental_json, _ = JsonResults._get_incremental_json(self._builder, incremental_results, is_full_results_format=False)\\n        merged_results, _ = JsonResults.merge(self._builder, aggregated_results, incremental_json, num_runs=201, sort_keys=True)\\n        self.assert_json_equal(merged_results, self._make_test_json({\\n            \\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n            \\"tests\\": {\\n                \\"001.html\\": {\\n                    \\"results\\": [[101, TEXT], [100, FAIL]],\\n                    \\"times\\": [[201, 0]],\\n                }\\n            }\\n        }))\\n\\n    def test_merge_full_results_format(self):\\n        expected_incremental_results = {\\n            \\"Webkit\\": {\\n                \\"blinkRevision\\": [\\"1234\\"],\\n                \\"buildNumbers\\": [\\"3\\"],\\n                \\"chromeRevision\\": [\\"5678\\"],\\n                \\"failure_map\\": CHAR_TO_FAILURE,\\n                \\"num_failures_by_type\\": {\\"AUDIO\\": [0], \\"CRASH\\": [3], \\"FAIL\\": [2], \\"IMAGE\\": [1], \\"IMAGE+TEXT\\": [0], \\"MISSING\\": [0], \\"PASS\\": [10], \\"SKIP\\": [2], \\"TEXT\\": [3], \\"TIMEOUT\\": [16]},\\n                \\"secondsSinceEpoch\\": [1368146629],\\n                \\"tests\\": {\\n                    \\"media\\": {\\n                        \\"W3C\\": {\\n                            \\"audio\\": {\\n                                \\"src\\": {\\n                                    \\"src_removal_does_not_trigger_loadstart.html\\": {\\n                                        \\"results\\": [[1, PASS]],\\n                                        \\"times\\": [[1, 4]],\\n                                    }\\n                                }\\n                            }\\n                        },\\n                        \\"encrypted-media\\": {\\n                            \\"encrypted-media-v2-events.html\\": {\\n                                \\"bugs\\": [\\"crbug.com/1234\\"],\\n                                \\"expected\\": \\"TIMEOUT\\",\\n                                \\"results\\": [[1, TIMEOUT]],\\n                                \\"times\\": [[1, 6]],\\n                            },\\n                            \\"encrypted-media-v2-syntax.html\\": {\\n                                \\"expected\\": \\"TIMEOUT\\",\\n                                \\"results\\": [[1, TIMEOUT]],\\n                                \\"times\\": [[1, 0]],\\n                            }\\n                        },\\n                        \\"media-document-audio-repaint.html\\": {\\n                            \\"expected\\": \\"IMAGE\\",\\n                            \\"results\\": [[1, IMAGE]],\\n                            \\"times\\": [[1, 0]],\\n                        },\\n                        \\"progress-events-generated-correctly.html\\": {\\n                            \\"expected\\": \\"PASS FAIL IMAGE TIMEOUT CRASH MISSING\\",\\n                            \\"results\\": [[1, TIMEOUT]],\\n                            \\"times\\": [[1, 6]],\\n                        },\\n                        \\"flaky-failed.html\\": {\\n                            \\"expected\\": \\"PASS FAIL\\",\\n                            \\"results\\": [[1, FAIL]],\\n                            \\"times\\": [[1, 0]],\\n                        },\\n                        \\"unexpected-fail.html\\": {\\n                            \\"results\\": [[1, FAIL]],\\n                            \\"times\\": [[1, 0]],\\n                        },\\n                    }\\n                }\\n            },\\n            \\"version\\": 4\\n        }\\n\\n        aggregated_results = \\"\\"\\n        incremental_json, _ = JsonResults._get_incremental_json(self._builder, FULL_RESULT_EXAMPLE, is_full_results_format=True)\\n        merged_results, _ = JsonResults.merge(\\"Webkit\\", aggregated_results, incremental_json, num_runs=jsonresults.JSON_RESULTS_MAX_BUILDS, sort_keys=True)\\n        self.assert_json_equal(merged_results, expected_incremental_results)\\n\\n    def test_merge_empty_aggregated_results(self):\\n        # No existing aggregated results.\\n        # Merged results == new incremental results.\\n        self._test_merge(\\n            # Aggregated results\\n            None,\\n            # Incremental results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT]],\\n                           \\"times\\": [[200, 0]]}}},\\n            # Expected result\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT]],\\n                           \\"times\\": [[200, 0]]}}})\\n\\n    def test_merge_duplicate_build_number(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[100, TEXT]],\\n                           \\"times\\": [[100, 0]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"2\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TEXT]],\\n                           \\"times\\": [[1, 0]]}}},\\n            # Expected results\\n            None)\\n\\n    def test_merge_incremental_single_test_single_run_same_result(self):\\n        # Incremental results has the latest build and same test results for\\n        # that run.\\n        # Insert the incremental results at the first place and sum number\\n        # of runs for TEXT (200 + 1) to get merged results.\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT]],\\n                           \\"times\\": [[200, 0]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TEXT]],\\n                           \\"times\\": [[1, 0]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[201, TEXT]],\\n                           \\"times\\": [[201, 0]]}}})\\n\\n    def test_merge_single_test_single_run_different_result(self):\\n        # Incremental results has the latest build but different test results\\n        # for that run.\\n        # Insert the incremental results at the first place.\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT]],\\n                           \\"times\\": [[200, 0]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, IMAGE]],\\n                           \\"times\\": [[1, 1]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, IMAGE], [200, TEXT]],\\n                           \\"times\\": [[1, 1], [200, 0]]}}})\\n\\n    def test_merge_single_test_single_run_result_changed(self):\\n        # Incremental results has the latest build but results which differ from\\n        # the latest result (but are the same as an older result).\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT], [10, IMAGE]],\\n                           \\"times\\": [[200, 0], [10, 1]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, IMAGE]],\\n                           \\"times\\": [[1, 1]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, IMAGE], [200, TEXT], [10, IMAGE]],\\n                           \\"times\\": [[1, 1], [200, 0], [10, 1]]}}})\\n\\n    def test_merge_multiple_tests_single_run(self):\\n        # All tests have incremental updates.\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT]],\\n                           \\"times\\": [[200, 0]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[100, IMAGE]],\\n                           \\"times\\": [[100, 1]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TEXT]],\\n                           \\"times\\": [[1, 0]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[1, IMAGE]],\\n                           \\"times\\": [[1, 1]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[201, TEXT]],\\n                           \\"times\\": [[201, 0]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[101, IMAGE]],\\n                           \\"times\\": [[101, 1]]}}})\\n\\n    def test_merge_multiple_tests_single_run_one_no_result(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT]],\\n                           \\"times\\": [[200, 0]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[100, IMAGE]],\\n                           \\"times\\": [[100, 1]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"002.html\\": {\\n                           \\"results\\": [[1, IMAGE]],\\n                           \\"times\\": [[1, 1]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, NO_DATA], [200, TEXT]],\\n                           \\"times\\": [[201, 0]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[101, IMAGE]],\\n                           \\"times\\": [[101, 1]]}}})\\n\\n    def test_merge_single_test_multiple_runs(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT]],\\n                           \\"times\\": [[200, 0]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"4\\", \\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[2, IMAGE], [1, FAIL]],\\n                           \\"times\\": [[3, 2]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"4\\", \\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, FAIL], [2, IMAGE], [200, TEXT]],\\n                           \\"times\\": [[3, 2], [200, 0]]}}})\\n\\n    def test_merge_multiple_tests_multiple_runs(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT]],\\n                           \\"times\\": [[200, 0]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[10, IMAGE_PLUS_TEXT]],\\n                           \\"times\\": [[10, 0]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"4\\", \\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[2, IMAGE]],\\n                           \\"times\\": [[2, 2]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[1, CRASH]],\\n                           \\"times\\": [[1, 1]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"4\\", \\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[2, IMAGE], [200, TEXT]],\\n                           \\"times\\": [[2, 2], [200, 0]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[1, CRASH], [10, IMAGE_PLUS_TEXT]],\\n                           \\"times\\": [[1, 1], [10, 0]]}}})\\n\\n    def test_merge_incremental_result_older_build(self):\\n        # Test the build in incremental results is older than the most recent\\n        # build in aggregated results.\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"3\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[5, TEXT]],\\n                           \\"times\\": [[5, 0]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"2\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TEXT]],\\n                           \\"times\\": [[1, 0]]}}},\\n            # Expected no merge happens.\\n            {\\"builds\\": [\\"2\\", \\"3\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[6, TEXT]],\\n                           \\"times\\": [[6, 0]]}}})\\n\\n    def test_merge_incremental_result_same_build(self):\\n        # Test the build in incremental results is same as the build in\\n        # aggregated results.\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[5, TEXT]],\\n                           \\"times\\": [[5, 0]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\", \\"2\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[2, TEXT]],\\n                           \\"times\\": [[2, 0]]}}},\\n            # Expected no merge happens.\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[7, TEXT]],\\n                           \\"times\\": [[7, 0]]}}})\\n\\n    def test_merge_remove_new_test(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[199, TEXT]],\\n                           \\"times\\": [[199, 0]]},\\n                       }},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TEXT]],\\n                           \\"times\\": [[1, 0]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[1, PASS]],\\n                           \\"times\\": [[1, 0]]},\\n                       \\"notrun.html\\": {\\n                           \\"results\\": [[1, NOTRUN]],\\n                           \\"times\\": [[1, 0]]},\\n                       \\"003.html\\": {\\n                           \\"results\\": [[1, NO_DATA]],\\n                           \\"times\\": [[1, 0]]},\\n                        }},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT]],\\n                           \\"times\\": [[200, 0]]},\\n                       }},\\n            max_builds=200)\\n\\n    def test_merge_remove_test(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\n                \\"builds\\": [\\"2\\", \\"1\\"],\\n                \\"tests\\": {\\n                    \\"directory\\": {\\n                        \\"directory\\": {\\n                            \\"001.html\\": {\\n                                \\"results\\": [[200, PASS]],\\n                                \\"times\\": [[200, 0]]\\n                            }\\n                        }\\n                    },\\n                    \\"002.html\\": {\\n                        \\"results\\": [[10, TEXT]],\\n                        \\"times\\": [[10, 0]]\\n                    },\\n                    \\"003.html\\": {\\n                        \\"results\\": [[190, PASS], [9, NO_DATA], [1, TEXT]],\\n                        \\"times\\": [[200, 0]]\\n                    },\\n                }\\n            },\\n            # Incremental results\\n            {\\n                \\"builds\\": [\\"3\\"],\\n                \\"tests\\": {\\n                    \\"directory\\": {\\n                        \\"directory\\": {\\n                            \\"001.html\\": {\\n                                \\"results\\": [[1, PASS]],\\n                                \\"times\\": [[1, 0]]\\n                            }\\n                        }\\n                    },\\n                    \\"002.html\\": {\\n                        \\"results\\": [[1, PASS]],\\n                        \\"times\\": [[1, 0]]\\n                    },\\n                    \\"003.html\\": {\\n                        \\"results\\": [[1, PASS]],\\n                        \\"times\\": [[1, 0]]\\n                    },\\n                }\\n            },\\n            # Expected results\\n            {\\n                \\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n                \\"tests\\": {\\n                    \\"002.html\\": {\\n                        \\"results\\": [[1, PASS], [10, TEXT]],\\n                        \\"times\\": [[11, 0]]\\n                    }\\n                }\\n            },\\n            max_builds=200)\\n\\n    def test_merge_updates_expected(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\n                \\"builds\\": [\\"2\\", \\"1\\"],\\n                \\"tests\\": {\\n                    \\"directory\\": {\\n                        \\"directory\\": {\\n                            \\"001.html\\": {\\n                                \\"expected\\": \\"FAIL\\",\\n                                \\"results\\": [[200, PASS]],\\n                                \\"times\\": [[200, 0]]\\n                            }\\n                        }\\n                    },\\n                    \\"002.html\\": {\\n                        \\"bugs\\": [\\"crbug.com/1234\\"],\\n                        \\"expected\\": \\"FAIL\\",\\n                        \\"results\\": [[10, TEXT]],\\n                        \\"times\\": [[10, 0]]\\n                    },\\n                    \\"003.html\\": {\\n                        \\"expected\\": \\"FAIL\\",\\n                        \\"results\\": [[190, PASS], [9, NO_DATA], [1, TEXT]],\\n                        \\"times\\": [[200, 0]]\\n                    },\\n                    \\"004.html\\": {\\n                        \\"results\\": [[199, PASS], [1, TEXT]],\\n                        \\"times\\": [[200, 0]]\\n                    },\\n                }\\n            },\\n            # Incremental results\\n            {\\n                \\"builds\\": [\\"3\\"],\\n                \\"tests\\": {\\n                    \\"002.html\\": {\\n                        \\"expected\\": \\"PASS\\",\\n                        \\"results\\": [[1, PASS]],\\n                        \\"times\\": [[1, 0]]\\n                    },\\n                    \\"003.html\\": {\\n                        \\"expected\\": \\"TIMEOUT\\",\\n                        \\"results\\": [[1, PASS]],\\n                        \\"times\\": [[1, 0]]\\n                    },\\n                    \\"004.html\\": {\\n                        \\"bugs\\": [\\"crbug.com/1234\\"],\\n                        \\"results\\": [[1, PASS]],\\n                        \\"times\\": [[1, 0]]\\n                    },\\n                }\\n            },\\n            # Expected results\\n            {\\n                \\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n                \\"tests\\": {\\n                    \\"002.html\\": {\\n                        \\"results\\": [[1, PASS], [10, TEXT]],\\n                        \\"times\\": [[11, 0]]\\n                    },\\n                    \\"003.html\\": {\\n                        \\"expected\\": \\"TIMEOUT\\",\\n                        \\"results\\": [[191, PASS], [9, NO_DATA]],\\n                        \\"times\\": [[200, 0]]\\n                    },\\n                    \\"004.html\\": {\\n                        \\"bugs\\": [\\"crbug.com/1234\\"],\\n                        \\"results\\": [[200, PASS]],\\n                        \\"times\\": [[200, 0]]\\n                    },\\n                }\\n            },\\n            max_builds=200)\\n\\n\\n    def test_merge_keep_test_with_all_pass_but_slow_time(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, PASS]],\\n                           \\"times\\": [[200, jsonresults.JSON_RESULTS_MIN_TIME]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[10, TEXT]],\\n                           \\"times\\": [[10, 0]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, PASS]],\\n                           \\"times\\": [[1, 1]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[1, PASS]],\\n                           \\"times\\": [[1, 0]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[201, PASS]],\\n                           \\"times\\": [[1, 1], [200, jsonresults.JSON_RESULTS_MIN_TIME]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[1, PASS], [10, TEXT]],\\n                           \\"times\\": [[11, 0]]}}})\\n\\n    def test_merge_pruning_slow_tests_for_debug_builders(self):\\n        self._builder = \\"MockBuilder(dbg)\\"\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, PASS]],\\n                           \\"times\\": [[200, 3 * jsonresults.JSON_RESULTS_MIN_TIME]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[10, TEXT]],\\n                           \\"times\\": [[10, 0]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, PASS]],\\n                           \\"times\\": [[1, 1]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[1, PASS]],\\n                           \\"times\\": [[1, 0]]},\\n                       \\"003.html\\": {\\n                           \\"results\\": [[1, PASS]],\\n                           \\"times\\": [[1, jsonresults.JSON_RESULTS_MIN_TIME]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[201, PASS]],\\n                           \\"times\\": [[1, 1], [200, 3 * jsonresults.JSON_RESULTS_MIN_TIME]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[1, PASS], [10, TEXT]],\\n                           \\"times\\": [[11, 0]]}}})\\n\\n    def test_merge_prune_extra_results(self):\\n        # Remove items from test results and times that exceed the max number\\n        # of builds to track.\\n        max_builds = jsonresults.JSON_RESULTS_MAX_BUILDS\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[max_builds, TEXT], [1, IMAGE]],\\n                           \\"times\\": [[max_builds, 0], [1, 1]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TIMEOUT]],\\n                           \\"times\\": [[1, 1]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TIMEOUT], [max_builds, TEXT]],\\n                           \\"times\\": [[1, 1], [max_builds, 0]]}}})\\n\\n    def test_merge_prune_extra_results_small(self):\\n        # Remove items from test results and times that exceed the max number\\n        # of builds to track, using smaller threshold.\\n        max_builds = jsonresults.JSON_RESULTS_MAX_BUILDS_SMALL\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[max_builds, TEXT], [1, IMAGE]],\\n                           \\"times\\": [[max_builds, 0], [1, 1]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TIMEOUT]],\\n                           \\"times\\": [[1, 1]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TIMEOUT], [max_builds, TEXT]],\\n                           \\"times\\": [[1, 1], [max_builds, 0]]}}},\\n            int(max_builds))\\n\\n    def test_merge_prune_extra_results_with_new_result_of_same_type(self):\\n        # Test that merging in a new result of the same type as the last result\\n        # causes old results to fall off.\\n        max_builds = jsonresults.JSON_RESULTS_MAX_BUILDS_SMALL\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[max_builds, TEXT], [1, NO_DATA]],\\n                           \\"times\\": [[max_builds, 0], [1, 1]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TEXT]],\\n                           \\"times\\": [[1, 0]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[max_builds, TEXT]],\\n                           \\"times\\": [[max_builds, 0]]}}},\\n            int(max_builds))\\n\\n    def test_merge_build_directory_hierarchy(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"bar\\": {\\"baz\\": {\\n                           \\"003.html\\": {\\n                                \\"results\\": [[25, TEXT]],\\n                                \\"times\\": [[25, 0]]}}},\\n                       \\"foo\\": {\\n                           \\"001.html\\": {\\n                                \\"results\\": [[50, TEXT]],\\n                                \\"times\\": [[50, 0]]},\\n                           \\"002.html\\": {\\n                                \\"results\\": [[100, IMAGE]],\\n                                \\"times\\": [[100, 0]]}}},\\n              \\"version\\": 4},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"baz\\": {\\n                           \\"004.html\\": {\\n                               \\"results\\": [[1, IMAGE]],\\n                               \\"times\\": [[1, 0]]}},\\n                       \\"foo\\": {\\n                           \\"001.html\\": {\\n                               \\"results\\": [[1, TEXT]],\\n                               \\"times\\": [[1, 0]]},\\n                           \\"002.html\\": {\\n                               \\"results\\": [[1, IMAGE]],\\n                               \\"times\\": [[1, 0]]}}},\\n             \\"version\\": 4},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"bar\\": {\\"baz\\": {\\n                           \\"003.html\\": {\\n                               \\"results\\": [[1, NO_DATA], [25, TEXT]],\\n                               \\"times\\": [[26, 0]]}}},\\n                       \\"baz\\": {\\n                           \\"004.html\\": {\\n                               \\"results\\": [[1, IMAGE]],\\n                               \\"times\\": [[1, 0]]}},\\n                       \\"foo\\": {\\n                           \\"001.html\\": {\\n                               \\"results\\": [[51, TEXT]],\\n                               \\"times\\": [[51, 0]]},\\n                           \\"002.html\\": {\\n                               \\"results\\": [[101, IMAGE]],\\n                               \\"times\\": [[101, 0]]}}},\\n             \\"version\\": 4})\\n\\n    # FIXME(aboxhall): Add some tests for xhtml/svg test results.\\n\\n    def test_get_test_name_list(self):\\n        # Get test name list only. Don\'t include non-test-list data and\\n        # of test result details.\\n        # FIXME: This also tests a temporary bug in the data where directory-level\\n        # results have a results and times values. Once that bug is fixed,\\n        # remove this test-case and assert we don\'t ever hit it.\\n        self._test_get_test_list(\\n            # Input results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"foo\\": {\\n                           \\"001.html\\": {\\n                               \\"results\\": [[200, PASS]],\\n                               \\"times\\": [[200, 0]]},\\n                           \\"results\\": [[1, NO_DATA]],\\n                           \\"times\\": [[1, 0]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[10, TEXT]],\\n                           \\"times\\": [[10, 0]]}}},\\n            # Expected results\\n            {\\"foo\\": {\\"001.html\\": {}}, \\"002.html\\": {}})\\n\\n    def test_gtest(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"foo.bar\\": {\\n                           \\"results\\": [[50, TEXT]],\\n                           \\"times\\": [[50, 0]]},\\n                       \\"foo.bar2\\": {\\n                           \\"results\\": [[100, IMAGE]],\\n                           \\"times\\": [[100, 0]]},\\n                       \\"test.failed\\": {\\n                           \\"results\\": [[5, FAIL]],\\n                           \\"times\\": [[5, 0]]},\\n                       },\\n             \\"version\\": 3},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"foo.bar2\\": {\\n                           \\"results\\": [[1, IMAGE]],\\n                           \\"times\\": [[1, 0]]},\\n                       \\"foo.bar3\\": {\\n                           \\"results\\": [[1, TEXT]],\\n                           \\"times\\": [[1, 0]]},\\n                       \\"test.failed\\": {\\n                           \\"results\\": [[5, FAIL]],\\n                           \\"times\\": [[5, 0]]},\\n                       },\\n             \\"version\\": 4},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"foo.bar\\": {\\n                           \\"results\\": [[1, NO_DATA], [50, TEXT]],\\n                           \\"times\\": [[51, 0]]},\\n                       \\"foo.bar2\\": {\\n                           \\"results\\": [[101, IMAGE]],\\n                           \\"times\\": [[101, 0]]},\\n                       \\"foo.bar3\\": {\\n                           \\"results\\": [[1, TEXT]],\\n                           \\"times\\": [[1, 0]]},\\n                       \\"test.failed\\": {\\n                           \\"results\\": [[10, FAIL]],\\n                           \\"times\\": [[10, 0]]},\\n                       },\\n             \\"version\\": 4})\\n\\nif __name__ == \'__main__\':\\n    unittest.main()\\n"}\n'
line: b'{"repo_name":"fabian4/trove","ref":"refs/heads/master","path":"trove/guestagent/datastore/experimental/redis/manager.py","content":"# Copyright (c) 2013 Rackspace\\n# All Rights Reserved.\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\nfrom oslo_log import log as logging\\nfrom oslo_service import periodic_task\\n\\nfrom trove.common import cfg\\nfrom trove.common import exception\\nfrom trove.common.i18n import _\\nfrom trove.common import instance as rd_instance\\nfrom trove.common import utils\\nfrom trove.guestagent import backup\\nfrom trove.guestagent.common import operating_system\\nfrom trove.guestagent.datastore.experimental.redis import service\\nfrom trove.guestagent import dbaas\\nfrom trove.guestagent.strategies.replication import get_replication_strategy\\nfrom trove.guestagent import volume\\n\\n\\nLOG = logging.getLogger(__name__)\\nCONF = cfg.CONF\\nMANAGER = CONF.datastore_manager or \'redis\'\\nREPLICATION_STRATEGY = CONF.get(MANAGER).replication_strategy\\nREPLICATION_NAMESPACE = CONF.get(MANAGER).replication_namespace\\nREPLICATION_STRATEGY_CLASS = get_replication_strategy(REPLICATION_STRATEGY,\\n                                                      REPLICATION_NAMESPACE)\\n\\n\\nclass Manager(periodic_task.PeriodicTasks):\\n    \\"\\"\\"\\n    This is the Redis manager class. It is dynamically loaded\\n    based off of the service_type of the trove instance\\n    \\"\\"\\"\\n\\n    def __init__(self):\\n        super(Manager, self).__init__(CONF)\\n        self._app = service.RedisApp()\\n\\n    @periodic_task.periodic_task\\n    def update_status(self, context):\\n        \\"\\"\\"\\n        Updates the redis trove instance. It is decorated with\\n        perodic task so it is automatically called every 3 ticks.\\n        \\"\\"\\"\\n        LOG.debug(\\"Update status called.\\")\\n        self._app.status.update()\\n\\n    def rpc_ping(self, context):\\n        LOG.debug(\\"Responding to RPC ping.\\")\\n        return True\\n\\n    def change_passwords(self, context, users):\\n        \\"\\"\\"\\n        Changes the redis instance password,\\n        it is currently not not implemented.\\n        \\"\\"\\"\\n        LOG.debug(\\"Change passwords called.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'change_passwords\', datastore=MANAGER)\\n\\n    def reset_configuration(self, context, configuration):\\n        \\"\\"\\"\\n        Resets to the default configuration,\\n        currently this does nothing.\\n        \\"\\"\\"\\n        LOG.debug(\\"Reset configuration called.\\")\\n        self._app.reset_configuration(configuration)\\n\\n    def _perform_restore(self, backup_info, context, restore_location, app):\\n        \\"\\"\\"Perform a restore on this instance.\\"\\"\\"\\n        LOG.info(_(\\"Restoring database from backup %s.\\") % backup_info[\'id\'])\\n        try:\\n            backup.restore(context, backup_info, restore_location)\\n        except Exception:\\n            LOG.exception(_(\\"Error performing restore from backup %s.\\") %\\n                          backup_info[\'id\'])\\n            app.status.set_status(rd_instance.ServiceStatuses.FAILED)\\n            raise\\n        LOG.info(_(\\"Restored database successfully.\\"))\\n\\n    def prepare(self, context, packages, databases, memory_mb, users,\\n                device_path=None, mount_point=None, backup_info=None,\\n                config_contents=None, root_password=None, overrides=None,\\n                cluster_config=None, snapshot=None):\\n        \\"\\"\\"\\n        This is called when the trove instance first comes online.\\n        It is the first rpc message passed from the task manager.\\n        prepare handles all the base configuration of the redis instance.\\n        \\"\\"\\"\\n        try:\\n            self._app.status.begin_install()\\n            if device_path:\\n                device = volume.VolumeDevice(device_path)\\n                # unmount if device is already mounted\\n                device.unmount_device(device_path)\\n                device.format()\\n                device.mount(mount_point)\\n                operating_system.chown(mount_point, \'redis\', \'redis\',\\n                                       as_root=True)\\n                LOG.debug(\'Mounted the volume.\')\\n            self._app.install_if_needed(packages)\\n            LOG.info(_(\'Writing redis configuration.\'))\\n            if cluster_config:\\n                config_contents = (config_contents + \\"\\\\n\\"\\n                                   + \\"cluster-enabled yes\\\\n\\"\\n                                   + \\"cluster-config-file cluster.conf\\\\n\\")\\n            self._app.configuration_manager.save_configuration(config_contents)\\n            self._app.apply_initial_guestagent_configuration()\\n            if backup_info:\\n                persistence_dir = self._app.get_working_dir()\\n                self._perform_restore(backup_info, context, persistence_dir,\\n                                      self._app)\\n            if snapshot:\\n                self.attach_replica(context, snapshot, snapshot[\'config\'])\\n            self._app.restart()\\n            if cluster_config:\\n                self._app.status.set_status(\\n                    rd_instance.ServiceStatuses.BUILD_PENDING)\\n            else:\\n                self._app.complete_install_or_restart()\\n            LOG.info(_(\'Redis instance has been setup and configured.\'))\\n        except Exception:\\n            LOG.exception(_(\\"Error setting up Redis instance.\\"))\\n            self._app.status.set_status(rd_instance.ServiceStatuses.FAILED)\\n            raise\\n\\n    def restart(self, context):\\n        \\"\\"\\"\\n        Restart this redis instance.\\n        This method is called when the guest agent\\n        gets a restart message from the taskmanager.\\n        \\"\\"\\"\\n        LOG.debug(\\"Restart called.\\")\\n        self._app.restart()\\n\\n    def start_db_with_conf_changes(self, context, config_contents):\\n        \\"\\"\\"\\n        Start this redis instance with new conf changes.\\n        \\"\\"\\"\\n        LOG.debug(\\"Start DB with conf changes called.\\")\\n        self._app.start_db_with_conf_changes(config_contents)\\n\\n    def stop_db(self, context, do_not_start_on_reboot=False):\\n        \\"\\"\\"\\n        Stop this redis instance.\\n        This method is called when the guest agent\\n        gets a stop message from the taskmanager.\\n        \\"\\"\\"\\n        LOG.debug(\\"Stop DB called.\\")\\n        self._app.stop_db(do_not_start_on_reboot=do_not_start_on_reboot)\\n\\n    def get_filesystem_stats(self, context, fs_path):\\n        \\"\\"\\"Gets the filesystem stats for the path given.\\"\\"\\"\\n        LOG.debug(\\"Get Filesystem Stats.\\")\\n        mount_point = CONF.get(\\n            \'mysql\' if not MANAGER else MANAGER).mount_point\\n        return dbaas.get_filesystem_volume_stats(mount_point)\\n\\n    def create_backup(self, context, backup_info):\\n        \\"\\"\\"Create a backup of the database.\\"\\"\\"\\n        LOG.debug(\\"Creating backup.\\")\\n        backup.backup(context, backup_info)\\n\\n    def mount_volume(self, context, device_path=None, mount_point=None):\\n        device = volume.VolumeDevice(device_path)\\n        device.mount(mount_point, write_to_fstab=False)\\n        LOG.debug(\\"Mounted the device %s at the mount point %s.\\" %\\n                  (device_path, mount_point))\\n\\n    def unmount_volume(self, context, device_path=None, mount_point=None):\\n        device = volume.VolumeDevice(device_path)\\n        device.unmount(mount_point)\\n        LOG.debug(\\"Unmounted the device %s from the mount point %s.\\" %\\n                  (device_path, mount_point))\\n\\n    def resize_fs(self, context, device_path=None, mount_point=None):\\n        device = volume.VolumeDevice(device_path)\\n        device.resize_fs(mount_point)\\n        LOG.debug(\\"Resized the filesystem at %s.\\" % mount_point)\\n\\n    def update_overrides(self, context, overrides, remove=False):\\n        LOG.debug(\\"Updating overrides.\\")\\n        if remove:\\n            self._app.remove_overrides()\\n        else:\\n            self._app.update_overrides(context, overrides, remove)\\n\\n    def apply_overrides(self, context, overrides):\\n        LOG.debug(\\"Applying overrides.\\")\\n        self._app.apply_overrides(self._app.admin, overrides)\\n\\n    def update_attributes(self, context, username, hostname, user_attrs):\\n        LOG.debug(\\"Updating attributes.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'update_attributes\', datastore=MANAGER)\\n\\n    def create_database(self, context, databases):\\n        LOG.debug(\\"Creating database.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'create_database\', datastore=MANAGER)\\n\\n    def create_user(self, context, users):\\n        LOG.debug(\\"Creating user.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'create_user\', datastore=MANAGER)\\n\\n    def delete_database(self, context, database):\\n        LOG.debug(\\"Deleting database.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'delete_database\', datastore=MANAGER)\\n\\n    def delete_user(self, context, user):\\n        LOG.debug(\\"Deleting user.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'delete_user\', datastore=MANAGER)\\n\\n    def get_user(self, context, username, hostname):\\n        LOG.debug(\\"Getting user.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'get_user\', datastore=MANAGER)\\n\\n    def grant_access(self, context, username, hostname, databases):\\n        LOG.debug(\\"Granting access.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'grant_access\', datastore=MANAGER)\\n\\n    def revoke_access(self, context, username, hostname, database):\\n        LOG.debug(\\"Revoking access.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'revoke_access\', datastore=MANAGER)\\n\\n    def list_access(self, context, username, hostname):\\n        LOG.debug(\\"Listing access.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'list_access\', datastore=MANAGER)\\n\\n    def list_databases(self, context, limit=None, marker=None,\\n                       include_marker=False):\\n        LOG.debug(\\"Listing databases.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'list_databases\', datastore=MANAGER)\\n\\n    def list_users(self, context, limit=None, marker=None,\\n                   include_marker=False):\\n        LOG.debug(\\"Listing users.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'list_users\', datastore=MANAGER)\\n\\n    def enable_root(self, context):\\n        LOG.debug(\\"Enabling root.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'enable_root\', datastore=MANAGER)\\n\\n    def enable_root_with_password(self, context, root_password=None):\\n        LOG.debug(\\"Enabling root with password.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'enable_root_with_password\', datastore=MANAGER)\\n\\n    def is_root_enabled(self, context):\\n        LOG.debug(\\"Checking if root is enabled.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'is_root_enabled\', datastore=MANAGER)\\n\\n    def backup_required_for_replication(self, context):\\n        replication = REPLICATION_STRATEGY_CLASS(context)\\n        return replication.backup_required_for_replication()\\n\\n    def get_replication_snapshot(self, context, snapshot_info,\\n                                 replica_source_config=None):\\n        LOG.debug(\\"Getting replication snapshot.\\")\\n        replication = REPLICATION_STRATEGY_CLASS(context)\\n        replication.enable_as_master(self._app, replica_source_config)\\n\\n        snapshot_id, log_position = (\\n            replication.snapshot_for_replication(context, self._app, None,\\n                                                 snapshot_info))\\n\\n        mount_point = CONF.get(MANAGER).mount_point\\n        volume_stats = dbaas.get_filesystem_volume_stats(mount_point)\\n\\n        replication_snapshot = {\\n            \'dataset\': {\\n                \'datastore_manager\': MANAGER,\\n                \'dataset_size\': volume_stats.get(\'used\', 0.0),\\n                \'volume_size\': volume_stats.get(\'total\', 0.0),\\n                \'snapshot_id\': snapshot_id\\n            },\\n            \'replication_strategy\': REPLICATION_STRATEGY,\\n            \'master\': replication.get_master_ref(self._app, snapshot_info),\\n            \'log_position\': log_position\\n        }\\n\\n        return replication_snapshot\\n\\n    def enable_as_master(self, context, replica_source_config):\\n        LOG.debug(\\"Calling enable_as_master.\\")\\n        replication = REPLICATION_STRATEGY_CLASS(context)\\n        replication.enable_as_master(self._app, replica_source_config)\\n\\n    def detach_replica(self, context, for_failover=False):\\n        LOG.debug(\\"Detaching replica.\\")\\n        replication = REPLICATION_STRATEGY_CLASS(context)\\n        replica_info = replication.detach_slave(self._app, for_failover)\\n        return replica_info\\n\\n    def get_replica_context(self, context):\\n        LOG.debug(\\"Getting replica context.\\")\\n        replication = REPLICATION_STRATEGY_CLASS(context)\\n        replica_info = replication.get_replica_context(self._app)\\n        return replica_info\\n\\n    def _validate_slave_for_replication(self, context, replica_info):\\n        if (replica_info[\'replication_strategy\'] != REPLICATION_STRATEGY):\\n            raise exception.IncompatibleReplicationStrategy(\\n                replica_info.update({\\n                    \'guest_strategy\': REPLICATION_STRATEGY\\n                }))\\n\\n    def attach_replica(self, context, replica_info, slave_config):\\n        LOG.debug(\\"Attaching replica.\\")\\n        try:\\n            if \'replication_strategy\' in replica_info:\\n                self._validate_slave_for_replication(context, replica_info)\\n            replication = REPLICATION_STRATEGY_CLASS(context)\\n            replication.enable_as_slave(self._app, replica_info,\\n                                        slave_config)\\n        except Exception:\\n            LOG.exception(\\"Error enabling replication.\\")\\n            self._app.status.set_status(rd_instance.ServiceStatuses.FAILED)\\n            raise\\n\\n    def make_read_only(self, context, read_only):\\n        LOG.debug(\\"Executing make_read_only(%s)\\" % read_only)\\n        self._app.make_read_only(read_only)\\n\\n    def _get_repl_info(self):\\n        return self._app.admin.get_info(\'replication\')\\n\\n    def _get_master_host(self):\\n        slave_info = self._get_repl_info()\\n        return slave_info and slave_info[\'master_host\'] or None\\n\\n    def _get_repl_offset(self):\\n        repl_info = self._get_repl_info()\\n        LOG.debug(\\"Got repl info: %s\\" % repl_info)\\n        offset_key = \'%s_repl_offset\' % repl_info[\'role\']\\n        offset = repl_info[offset_key]\\n        LOG.debug(\\"Found offset %s for key %s.\\" % (offset, offset_key))\\n        return int(offset)\\n\\n    def get_last_txn(self, context):\\n        master_host = self._get_master_host()\\n        repl_offset = self._get_repl_offset()\\n        return master_host, repl_offset\\n\\n    def get_latest_txn_id(self, context):\\n        LOG.info(_(\\"Retrieving latest repl offset.\\"))\\n        return self._get_repl_offset()\\n\\n    def wait_for_txn(self, context, txn):\\n        LOG.info(_(\\"Waiting on repl offset \'%s\'.\\") % txn)\\n\\n        def _wait_for_txn():\\n            current_offset = self._get_repl_offset()\\n            LOG.debug(\\"Current offset: %s.\\" % current_offset)\\n            return current_offset \\u003e= txn\\n\\n        try:\\n            utils.poll_until(_wait_for_txn, time_out=120)\\n        except exception.PollTimeOut:\\n            raise RuntimeError(_(\\"Timeout occurred waiting for Redis repl \\"\\n                                 \\"offset to change to \'%s\'.\\") % txn)\\n\\n    def cleanup_source_on_replica_detach(self, context, replica_info):\\n        LOG.debug(\\"Cleaning up the source on the detach of a replica.\\")\\n        replication = REPLICATION_STRATEGY_CLASS(context)\\n        replication.cleanup_source_on_replica_detach(self._app, replica_info)\\n\\n    def demote_replication_master(self, context):\\n        LOG.debug(\\"Demoting replica source.\\")\\n        replication = REPLICATION_STRATEGY_CLASS(context)\\n        replication.demote_master(self._app)\\n\\n    def cluster_meet(self, context, ip, port):\\n        LOG.debug(\\"Executing cluster_meet to join node to cluster.\\")\\n        self._app.cluster_meet(ip, port)\\n\\n    def get_node_ip(self, context):\\n        LOG.debug(\\"Retrieving cluster node ip address.\\")\\n        return self._app.get_node_ip()\\n\\n    def get_node_id_for_removal(self, context):\\n        LOG.debug(\\"Validating removal of node from cluster.\\")\\n        return self._app.get_node_id_for_removal()\\n\\n    def remove_nodes(self, context, node_ids):\\n        LOG.debug(\\"Removing nodes from cluster.\\")\\n        self._app.remove_nodes(node_ids)\\n\\n    def cluster_addslots(self, context, first_slot, last_slot):\\n        LOG.debug(\\"Executing cluster_addslots to assign hash slots %s-%s.\\",\\n                  first_slot, last_slot)\\n        self._app.cluster_addslots(first_slot, last_slot)\\n\\n    def cluster_complete(self, context):\\n        LOG.debug(\\"Cluster creation complete, starting status checks.\\")\\n        self._app.complete_install_or_restart()\\n"}\n'
line: b'{"repo_name":"Hernanarce/pelisalacarta","ref":"refs/heads/master","path":"python/version-mediaserver/platformcode/platformtools.py","content":"# -*- coding: utf-8 -*-\\r\\n# ------------------------------------------------------------\\r\\n# pelisalacarta 4\\r\\n# Copyright 2015 tvalacarta@gmail.com\\r\\n# http://blog.tvalacarta.info/plugin-xbmc/pelisalacarta/\\r\\n#\\r\\n# Distributed under the terms of GNU General Public License v3 (GPLv3)\\r\\n# http://www.gnu.org/licenses/gpl-3.0.html\\r\\n# ------------------------------------------------------------\\r\\n# This file is part of pelisalacarta 4.\\r\\n#\\r\\n# pelisalacarta 4 is free software: you can redistribute it and/or modify\\r\\n# it under the terms of the GNU General Public License as published by\\r\\n# the Free Software Foundation, either version 3 of the License, or\\r\\n# (at your option) any later version.\\r\\n#\\r\\n# pelisalacarta 4 is distributed in the hope that it will be useful,\\r\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\r\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\r\\n# GNU General Public License for more details.\\r\\n#\\r\\n# You should have received a copy of the GNU General Public License\\r\\n# along with pelisalacarta 4.  If not, see \\u003chttp://www.gnu.org/licenses/\\u003e.\\r\\n# ------------------------------------------------------------\\r\\n# platformtools\\r\\n# ------------------------------------------------------------\\r\\n# Herramientas responsables de adaptar los diferentes \\r\\n# cuadros de dialogo a una plataforma en concreto,\\r\\n# en este caso Mediserver.\\r\\n# version 1.3\\r\\n# ------------------------------------------------------------\\r\\nimport os\\r\\nimport sys\\r\\nfrom core import config\\r\\nfrom core import logger\\r\\nimport threading\\r\\ncontrollers = {}\\r\\n\\r\\n\\r\\ndef dialog_ok(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].dialog_ok(*args, **kwargs)\\r\\n    \\r\\ndef dialog_notification(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].dialog_notification(*args, **kwargs)\\r\\n\\r\\ndef dialog_yesno(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].dialog_yesno(*args, **kwargs)\\r\\n    \\r\\ndef dialog_select(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].dialog_select(*args, **kwargs)\\r\\n\\r\\ndef dialog_progress(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].dialog_progress(*args, **kwargs)\\r\\n \\r\\ndef dialog_progress_bg(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].dialog_progress_bg(*args, **kwargs)\\r\\n\\r\\ndef dialog_input(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].dialog_input(*args, **kwargs)\\r\\n    \\r\\ndef dialog_numeric(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].dialog_numeric(*args, **kwargs)\\r\\n\\r\\ndef itemlist_refresh(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].itemlist_refresh(*args, **kwargs)\\r\\n\\r\\ndef itemlist_update(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].itemlist_update(*args, **kwargs)\\r\\n\\r\\ndef render_items(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].render_items(*args,**kwargs)\\r\\n\\r\\ndef is_playing(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].is_playing(*args, **kwargs)\\r\\n\\r\\ndef play_video(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].play_video(*args, **kwargs)\\r\\n\\r\\ndef open_settings(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].open_settings(*args, **kwargs)\\r\\n\\r\\ndef show_channel_settings(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].show_channel_settings(*args, **kwargs)\\r\\n\\r\\ndef show_video_info(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].show_video_info(*args, **kwargs)\\r\\n\\r\\ndef show_recaptcha(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].show_recaptcha(*args, **kwargs)"}\n'
line: b'{"repo_name":"hobson/pyexiv2","ref":"refs/heads/master","path":"test/usercomment.py","content":"# -*- coding: utf-8 -*-\\n\\n# ******************************************************************************\\n#\\n# Copyright (C) 2010 Olivier Tilloy \\u003colivier@tilloy.net\\u003e\\n#\\n# This file is part of the pyexiv2 distribution.\\n#\\n# pyexiv2 is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU General Public License\\n# as published by the Free Software Foundation; either version 2\\n# of the License, or (at your option) any later version.\\n#\\n# pyexiv2 is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU General Public License for more details.\\n#\\n# You should have received a copy of the GNU General Public License\\n# along with pyexiv2; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin Street, 5th Floor, Boston, MA 02110-1301 USA.\\n#\\n# Author: Olivier Tilloy \\u003colivier@tilloy.net\\u003e\\n#\\n# ******************************************************************************\\n\\nfrom pyexiv2.metadata import ImageMetadata\\n\\nimport unittest\\nimport testutils\\nimport os\\nimport tempfile\\nfrom testutils import EMPTY_JPG_DATA\\n\\n\\nclass TestUserCommentReadWrite(unittest.TestCase):\\n\\n    checksums = {\\n        \'usercomment-ascii.jpg\': \'ad29ac65fb6f63c8361aaed6cb02f8c7\',\\n        \'usercomment-unicode-ii.jpg\': \'13b7cc09129a8677f2cf18634f5abd3c\',\\n        \'usercomment-unicode-mm.jpg\': \'7addfed7823c556ba489cd4ab2037200\',\\n        }\\n\\n    def _read_image(self, filename):\\n        filepath = testutils.get_absolute_file_path(os.path.join(\'data\', filename))\\n        self.assert_(testutils.CheckFileSum(filepath, self.checksums[filename]))\\n        m = ImageMetadata(filepath)\\n        m.read()\\n        return m\\n\\n    def _expected_raw_value(self, endianness, value):\\n        from pyexiv2 import __exiv2_version__\\n        if __exiv2_version__ \\u003e= \'0.20\':\\n            return value\\n        else:\\n            encodings = {\'ii\': \'utf-16le\', \'mm\': \'utf-16be\'}\\n            return value.decode(\'utf-8\').encode(encodings[endianness])\\n\\n    def test_read_ascii(self):\\n        m = self._read_image(\'usercomment-ascii.jpg\')\\n        tag = m[\'Exif.Photo.UserComment\']\\n        self.assertEqual(tag.type, \'Comment\')\\n        self.assertEqual(tag.raw_value, \'charset=\\"Ascii\\" deja vu\')\\n        self.assertEqual(tag.value, u\'deja vu\')\\n\\n    def test_read_unicode_little_endian(self):\\n        m = self._read_image(\'usercomment-unicode-ii.jpg\')\\n        tag = m[\'Exif.Photo.UserComment\']\\n        self.assertEqual(tag.type, \'Comment\')\\n        self.assertEqual(tag.raw_value, \'charset=\\"Unicode\\" %s\' % self._expected_raw_value(\'ii\', \'d\xc3\xa9j\xc3\xa0 vu\'))\\n        self.assertEqual(tag.value, u\'d\xc3\xa9j\xc3\xa0 vu\')\\n\\n    def test_read_unicode_big_endian(self):\\n        m = self._read_image(\'usercomment-unicode-mm.jpg\')\\n        tag = m[\'Exif.Photo.UserComment\']\\n        self.assertEqual(tag.type, \'Comment\')\\n        self.assertEqual(tag.raw_value, \'charset=\\"Unicode\\" %s\' % self._expected_raw_value(\'mm\', \'d\xc3\xa9j\xc3\xa0 vu\'))\\n        self.assertEqual(tag.value, u\'d\xc3\xa9j\xc3\xa0 vu\')\\n\\n    def test_write_ascii(self):\\n        m = self._read_image(\'usercomment-ascii.jpg\')\\n        tag = m[\'Exif.Photo.UserComment\']\\n        self.assertEqual(tag.type, \'Comment\')\\n        tag.value = \'foo bar\'\\n        self.assertEqual(tag.raw_value, \'charset=\\"Ascii\\" foo bar\')\\n        self.assertEqual(tag.value, u\'foo bar\')\\n\\n    def test_write_unicode_over_ascii(self):\\n        m = self._read_image(\'usercomment-ascii.jpg\')\\n        tag = m[\'Exif.Photo.UserComment\']\\n        self.assertEqual(tag.type, \'Comment\')\\n        tag.value = u\'d\xc3\xa9j\xc3\xa0 vu\'\\n        self.assertEqual(tag.raw_value, \'d\xc3\xa9j\xc3\xa0 vu\')\\n        self.assertEqual(tag.value, u\'d\xc3\xa9j\xc3\xa0 vu\')\\n\\n    def test_write_unicode_little_endian(self):\\n        m = self._read_image(\'usercomment-unicode-ii.jpg\')\\n        tag = m[\'Exif.Photo.UserComment\']\\n        self.assertEqual(tag.type, \'Comment\')\\n        tag.value = u\'D\xc3\x89J\xc3\x80 VU\'\\n        self.assertEqual(tag.raw_value, \'charset=\\"Unicode\\" %s\' % self._expected_raw_value(\'ii\', \'D\xc3\x89J\xc3\x80 VU\'))\\n        self.assertEqual(tag.value, u\'D\xc3\x89J\xc3\x80 VU\')\\n\\n    def test_write_unicode_big_endian(self):\\n        m = self._read_image(\'usercomment-unicode-mm.jpg\')\\n        tag = m[\'Exif.Photo.UserComment\']\\n        self.assertEqual(tag.type, \'Comment\')\\n        tag.value = u\'D\xc3\x89J\xc3\x80 VU\'\\n        self.assertEqual(tag.raw_value, \'charset=\\"Unicode\\" %s\' % self._expected_raw_value(\'mm\', \'D\xc3\x89J\xc3\x80 VU\'))\\n        self.assertEqual(tag.value, u\'D\xc3\x89J\xc3\x80 VU\')\\n\\n\\nclass TestUserCommentAdd(unittest.TestCase):\\n\\n    def setUp(self):\\n        # Create an empty image file\\n        fd, self.pathname = tempfile.mkstemp(suffix=\'.jpg\')\\n        os.write(fd, EMPTY_JPG_DATA)\\n        os.close(fd)\\n\\n    def tearDown(self):\\n        os.remove(self.pathname)\\n\\n    def _test_add_comment(self, value):\\n        metadata = ImageMetadata(self.pathname)\\n        metadata.read()\\n        key = \'Exif.Photo.UserComment\'\\n        metadata[key] = value\\n        metadata.write()\\n\\n        metadata = ImageMetadata(self.pathname)\\n        metadata.read()\\n        self.assert_(key in metadata.exif_keys)\\n        tag = metadata[key]\\n        self.assertEqual(tag.type, \'Comment\')\\n        self.assertEqual(tag.value, value)\\n\\n    def test_add_comment_ascii(self):\\n        self._test_add_comment(\'deja vu\')\\n\\n    def test_add_comment_unicode(self):\\n        self._test_add_comment(u\'d\xc3\xa9j\xc3\xa0 vu\')\\n\\n"}\n'
line: b'{"repo_name":"andyzsf/edx","ref":"refs/heads/master","path":"common/djangoapps/student/migrations/0020_add_test_center_user.py","content":"# -*- coding: utf-8 -*-\\nimport datetime\\nfrom south.db import db\\nfrom south.v2 import SchemaMigration\\nfrom django.db import models\\n\\n\\nclass Migration(SchemaMigration):\\n\\n    def forwards(self, orm):\\n        # Adding model \'TestCenterUser\'\\n        db.create_table(\'student_testcenteruser\', (\\n            (\'id\', self.gf(\'django.db.models.fields.AutoField\')(primary_key=True)),\\n            (\'user\', self.gf(\'django.db.models.fields.related.ForeignKey\')(default=None, to=orm[\'auth.User\'], unique=True)),\\n            (\'created_at\', self.gf(\'django.db.models.fields.DateTimeField\')(auto_now_add=True, db_index=True, blank=True)),\\n            (\'updated_at\', self.gf(\'django.db.models.fields.DateTimeField\')(auto_now=True, db_index=True, blank=True)),\\n            (\'user_updated_at\', self.gf(\'django.db.models.fields.DateTimeField\')(db_index=True)),\\n            (\'candidate_id\', self.gf(\'django.db.models.fields.IntegerField\')(null=True, db_index=True)),\\n            (\'client_candidate_id\', self.gf(\'django.db.models.fields.CharField\')(max_length=50, db_index=True)),\\n            (\'first_name\', self.gf(\'django.db.models.fields.CharField\')(max_length=30, db_index=True)),\\n            (\'last_name\', self.gf(\'django.db.models.fields.CharField\')(max_length=50, db_index=True)),\\n            (\'middle_name\', self.gf(\'django.db.models.fields.CharField\')(max_length=30, blank=True)),\\n            (\'suffix\', self.gf(\'django.db.models.fields.CharField\')(max_length=255, blank=True)),\\n            (\'salutation\', self.gf(\'django.db.models.fields.CharField\')(max_length=50, blank=True)),\\n            (\'address_1\', self.gf(\'django.db.models.fields.CharField\')(max_length=40)),\\n            (\'address_2\', self.gf(\'django.db.models.fields.CharField\')(max_length=40, blank=True)),\\n            (\'address_3\', self.gf(\'django.db.models.fields.CharField\')(max_length=40, blank=True)),\\n            (\'city\', self.gf(\'django.db.models.fields.CharField\')(max_length=32, db_index=True)),\\n            (\'state\', self.gf(\'django.db.models.fields.CharField\')(db_index=True, max_length=20, blank=True)),\\n            (\'postal_code\', self.gf(\'django.db.models.fields.CharField\')(db_index=True, max_length=16, blank=True)),\\n            (\'country\', self.gf(\'django.db.models.fields.CharField\')(max_length=3, db_index=True)),\\n            (\'phone\', self.gf(\'django.db.models.fields.CharField\')(max_length=35)),\\n            (\'extension\', self.gf(\'django.db.models.fields.CharField\')(db_index=True, max_length=8, blank=True)),\\n            (\'phone_country_code\', self.gf(\'django.db.models.fields.CharField\')(max_length=3, db_index=True)),\\n            (\'fax\', self.gf(\'django.db.models.fields.CharField\')(max_length=35, blank=True)),\\n            (\'fax_country_code\', self.gf(\'django.db.models.fields.CharField\')(max_length=3, blank=True)),\\n            (\'company_name\', self.gf(\'django.db.models.fields.CharField\')(max_length=50, blank=True)),\\n        ))\\n        db.send_create_signal(\'student\', [\'TestCenterUser\'])\\n\\n\\n    def backwards(self, orm):\\n        # Deleting model \'TestCenterUser\'\\n        db.delete_table(\'student_testcenteruser\')\\n\\n\\n    models = {\\n        \'auth.group\': {\\n            \'Meta\': {\'object_name\': \'Group\'},\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'unique\': \'True\', \'max_length\': \'80\'}),\\n            \'permissions\': (\'django.db.models.fields.related.ManyToManyField\', [], {\'to\': \\"orm[\'auth.Permission\']\\", \'symmetrical\': \'False\', \'blank\': \'True\'})\\n        },\\n        \'auth.permission\': {\\n            \'Meta\': {\'ordering\': \\"(\'content_type__app_label\', \'content_type__model\', \'codename\')\\", \'unique_together\': \\"((\'content_type\', \'codename\'),)\\", \'object_name\': \'Permission\'},\\n            \'codename\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'content_type\': (\'django.db.models.fields.related.ForeignKey\', [], {\'to\': \\"orm[\'contenttypes.ContentType\']\\"}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\'})\\n        },\\n        \'auth.user\': {\\n            \'Meta\': {\'object_name\': \'User\'},\\n            \'about\': (\'django.db.models.fields.TextField\', [], {\'blank\': \'True\'}),\\n            \'avatar_type\': (\'django.db.models.fields.CharField\', [], {\'default\': \\"\'n\'\\", \'max_length\': \'1\'}),\\n            \'bronze\': (\'django.db.models.fields.SmallIntegerField\', [], {\'default\': \'0\'}),\\n            \'consecutive_days_visit_count\': (\'django.db.models.fields.IntegerField\', [], {\'default\': \'0\'}),\\n            \'country\': (\'django_countries.fields.CountryField\', [], {\'max_length\': \'2\', \'blank\': \'True\'}),\\n            \'date_joined\': (\'django.db.models.fields.DateTimeField\', [], {\'default\': \'datetime.datetime.now\'}),\\n            \'date_of_birth\': (\'django.db.models.fields.DateField\', [], {\'null\': \'True\', \'blank\': \'True\'}),\\n            \'display_tag_filter_strategy\': (\'django.db.models.fields.SmallIntegerField\', [], {\'default\': \'0\'}),\\n            \'email\': (\'django.db.models.fields.EmailField\', [], {\'max_length\': \'75\', \'blank\': \'True\'}),\\n            \'email_isvalid\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'email_key\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'32\', \'null\': \'True\'}),\\n            \'email_tag_filter_strategy\': (\'django.db.models.fields.SmallIntegerField\', [], {\'default\': \'1\'}),\\n            \'first_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'30\', \'blank\': \'True\'}),\\n            \'gold\': (\'django.db.models.fields.SmallIntegerField\', [], {\'default\': \'0\'}),\\n            \'gravatar\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'32\'}),\\n            \'groups\': (\'django.db.models.fields.related.ManyToManyField\', [], {\'to\': \\"orm[\'auth.Group\']\\", \'symmetrical\': \'False\', \'blank\': \'True\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'ignored_tags\': (\'django.db.models.fields.TextField\', [], {\'blank\': \'True\'}),\\n            \'interesting_tags\': (\'django.db.models.fields.TextField\', [], {\'blank\': \'True\'}),\\n            \'is_active\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'is_staff\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'is_superuser\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'last_login\': (\'django.db.models.fields.DateTimeField\', [], {\'default\': \'datetime.datetime.now\'}),\\n            \'last_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'30\', \'blank\': \'True\'}),\\n            \'last_seen\': (\'django.db.models.fields.DateTimeField\', [], {\'default\': \'datetime.datetime.now\'}),\\n            \'location\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\', \'blank\': \'True\'}),\\n            \'new_response_count\': (\'django.db.models.fields.IntegerField\', [], {\'default\': \'0\'}),\\n            \'password\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'128\'}),\\n            \'questions_per_page\': (\'django.db.models.fields.SmallIntegerField\', [], {\'default\': \'10\'}),\\n            \'real_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\', \'blank\': \'True\'}),\\n            \'reputation\': (\'django.db.models.fields.PositiveIntegerField\', [], {\'default\': \'1\'}),\\n            \'seen_response_count\': (\'django.db.models.fields.IntegerField\', [], {\'default\': \'0\'}),\\n            \'show_country\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'silver\': (\'django.db.models.fields.SmallIntegerField\', [], {\'default\': \'0\'}),\\n            \'status\': (\'django.db.models.fields.CharField\', [], {\'default\': \\"\'w\'\\", \'max_length\': \'2\'}),\\n            \'user_permissions\': (\'django.db.models.fields.related.ManyToManyField\', [], {\'to\': \\"orm[\'auth.Permission\']\\", \'symmetrical\': \'False\', \'blank\': \'True\'}),\\n            \'username\': (\'django.db.models.fields.CharField\', [], {\'unique\': \'True\', \'max_length\': \'30\'}),\\n            \'website\': (\'django.db.models.fields.URLField\', [], {\'max_length\': \'200\', \'blank\': \'True\'})\\n        },\\n        \'contenttypes.contenttype\': {\\n            \'Meta\': {\'ordering\': \\"(\'name\',)\\", \'unique_together\': \\"((\'app_label\', \'model\'),)\\", \'object_name\': \'ContentType\', \'db_table\': \\"\'django_content_type\'\\"},\\n            \'app_label\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'model\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'})\\n        },\\n        \'student.courseenrollment\': {\\n            \'Meta\': {\'unique_together\': \\"((\'user\', \'course_id\'),)\\", \'object_name\': \'CourseEnrollment\'},\\n            \'course_id\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'255\', \'db_index\': \'True\'}),\\n            \'created\': (\'django.db.models.fields.DateTimeField\', [], {\'auto_now_add\': \'True\', \'null\': \'True\', \'db_index\': \'True\', \'blank\': \'True\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'user\': (\'django.db.models.fields.related.ForeignKey\', [], {\'to\': \\"orm[\'auth.User\']\\"})\\n        },\\n        \'student.pendingemailchange\': {\\n            \'Meta\': {\'object_name\': \'PendingEmailChange\'},\\n            \'activation_key\': (\'django.db.models.fields.CharField\', [], {\'unique\': \'True\', \'max_length\': \'32\', \'db_index\': \'True\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'new_email\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'255\', \'blank\': \'True\'}),\\n            \'user\': (\'django.db.models.fields.related.OneToOneField\', [], {\'to\': \\"orm[\'auth.User\']\\", \'unique\': \'True\'})\\n        },\\n        \'student.pendingnamechange\': {\\n            \'Meta\': {\'object_name\': \'PendingNameChange\'},\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'new_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'255\', \'blank\': \'True\'}),\\n            \'rationale\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'1024\', \'blank\': \'True\'}),\\n            \'user\': (\'django.db.models.fields.related.OneToOneField\', [], {\'to\': \\"orm[\'auth.User\']\\", \'unique\': \'True\'})\\n        },\\n        \'student.registration\': {\\n            \'Meta\': {\'object_name\': \'Registration\', \'db_table\': \\"\'auth_registration\'\\"},\\n            \'activation_key\': (\'django.db.models.fields.CharField\', [], {\'unique\': \'True\', \'max_length\': \'32\', \'db_index\': \'True\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'user\': (\'django.db.models.fields.related.ForeignKey\', [], {\'to\': \\"orm[\'auth.User\']\\", \'unique\': \'True\'})\\n        },\\n        \'student.testcenteruser\': {\\n            \'Meta\': {\'object_name\': \'TestCenterUser\'},\\n            \'address_1\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'40\'}),\\n            \'address_2\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'40\', \'blank\': \'True\'}),\\n            \'address_3\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'40\', \'blank\': \'True\'}),\\n            \'candidate_id\': (\'django.db.models.fields.IntegerField\', [], {\'null\': \'True\', \'db_index\': \'True\'}),\\n            \'city\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'32\', \'db_index\': \'True\'}),\\n            \'client_candidate_id\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\', \'db_index\': \'True\'}),\\n            \'company_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\', \'blank\': \'True\'}),\\n            \'country\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'3\', \'db_index\': \'True\'}),\\n            \'created_at\': (\'django.db.models.fields.DateTimeField\', [], {\'auto_now_add\': \'True\', \'db_index\': \'True\', \'blank\': \'True\'}),\\n            \'extension\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'8\', \'blank\': \'True\'}),\\n            \'fax\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'35\', \'blank\': \'True\'}),\\n            \'fax_country_code\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'3\', \'blank\': \'True\'}),\\n            \'first_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'30\', \'db_index\': \'True\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'last_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\', \'db_index\': \'True\'}),\\n            \'middle_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'30\', \'blank\': \'True\'}),\\n            \'phone\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'35\'}),\\n            \'phone_country_code\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'3\', \'db_index\': \'True\'}),\\n            \'postal_code\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'16\', \'blank\': \'True\'}),\\n            \'salutation\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\', \'blank\': \'True\'}),\\n            \'state\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'20\', \'blank\': \'True\'}),\\n            \'suffix\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'255\', \'blank\': \'True\'}),\\n            \'updated_at\': (\'django.db.models.fields.DateTimeField\', [], {\'auto_now\': \'True\', \'db_index\': \'True\', \'blank\': \'True\'}),\\n            \'user\': (\'django.db.models.fields.related.ForeignKey\', [], {\'default\': \'None\', \'to\': \\"orm[\'auth.User\']\\", \'unique\': \'True\'}),\\n            \'user_updated_at\': (\'django.db.models.fields.DateTimeField\', [], {\'db_index\': \'True\'})\\n        },\\n        \'student.userprofile\': {\\n            \'Meta\': {\'object_name\': \'UserProfile\', \'db_table\': \\"\'auth_userprofile\'\\"},\\n            \'courseware\': (\'django.db.models.fields.CharField\', [], {\'default\': \\"\'course.xml\'\\", \'max_length\': \'255\', \'blank\': \'True\'}),\\n            \'gender\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'6\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'goals\': (\'django.db.models.fields.TextField\', [], {\'null\': \'True\', \'blank\': \'True\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'language\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'255\', \'blank\': \'True\'}),\\n            \'level_of_education\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'6\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'location\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'255\', \'blank\': \'True\'}),\\n            \'mailing_address\': (\'django.db.models.fields.TextField\', [], {\'null\': \'True\', \'blank\': \'True\'}),\\n            \'meta\': (\'django.db.models.fields.TextField\', [], {\'blank\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'255\', \'blank\': \'True\'}),\\n            \'user\': (\'django.db.models.fields.related.OneToOneField\', [], {\'related_name\': \\"\'profile\'\\", \'unique\': \'True\', \'to\': \\"orm[\'auth.User\']\\"}),\\n            \'year_of_birth\': (\'django.db.models.fields.IntegerField\', [], {\'db_index\': \'True\', \'null\': \'True\', \'blank\': \'True\'})\\n        },\\n        \'student.usertestgroup\': {\\n            \'Meta\': {\'object_name\': \'UserTestGroup\'},\\n            \'description\': (\'django.db.models.fields.TextField\', [], {\'blank\': \'True\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'32\', \'db_index\': \'True\'}),\\n            \'users\': (\'django.db.models.fields.related.ManyToManyField\', [], {\'to\': \\"orm[\'auth.User\']\\", \'db_index\': \'True\', \'symmetrical\': \'False\'})\\n        }\\n    }\\n\\n    complete_apps = [\'student\']\\n"}\n'
line: b'{"repo_name":"Syrcon/servo","ref":"refs/heads/master","path":"tests/wpt/web-platform-tests/tools/serve/serve.py","content":"# -*- coding: utf-8 -*-\\nimport argparse\\nimport json\\nimport os\\nimport signal\\nimport socket\\nimport sys\\nimport threading\\nimport time\\nimport traceback\\nimport urllib2\\nimport uuid\\nfrom collections import defaultdict, OrderedDict\\nfrom multiprocessing import Process, Event\\n\\nfrom .. import localpaths\\n\\nimport sslutils\\nfrom wptserve import server as wptserve, handlers\\nfrom wptserve import stash\\nfrom wptserve.logger import set_logger\\nfrom mod_pywebsocket import standalone as pywebsocket\\n\\nrepo_root = localpaths.repo_root\\n\\nclass WorkersHandler(object):\\n    def __init__(self):\\n        self.handler = handlers.handler(self.handle_request)\\n\\n    def __call__(self, request, response):\\n        return self.handler(request, response)\\n\\n    def handle_request(self, request, response):\\n        worker_path = request.url_parts.path.replace(\\".worker\\", \\".worker.js\\")\\n        return \\"\\"\\"\\u003c!doctype html\\u003e\\n\\u003cmeta charset=utf-8\\u003e\\n\\u003cscript src=\\"/resources/testharness.js\\"\\u003e\\u003c/script\\u003e\\n\\u003cscript src=\\"/resources/testharnessreport.js\\"\\u003e\\u003c/script\\u003e\\n\\u003cdiv id=log\\u003e\\u003c/div\\u003e\\n\\u003cscript\\u003e\\nfetch_tests_from_worker(new Worker(\\"%s\\"));\\n\\u003c/script\\u003e\\n\\"\\"\\" % (worker_path,)\\n\\nrewrites = [(\\"GET\\", \\"/resources/WebIDLParser.js\\", \\"/resources/webidl2/lib/webidl2.js\\")]\\n\\nsubdomains = [u\\"www\\",\\n              u\\"www1\\",\\n              u\\"www2\\",\\n              u\\"\xe5\xa4\xa9\xe6\xb0\x97\xe3\x81\xae\xe8\x89\xaf\xe3\x81\x84\xe6\x97\xa5\\",\\n              u\\"\xc3\xa9l\xc3\xa8ve\\"]\\n\\nclass RoutesBuilder(object):\\n    def __init__(self):\\n        self.forbidden_override = [(\\"GET\\", \\"/tools/runner/*\\", handlers.file_handler),\\n                                   (\\"POST\\", \\"/tools/runner/update_manifest.py\\",\\n                                    handlers.python_script_handler)]\\n\\n        self.forbidden = [(\\"*\\", \\"/_certs/*\\", handlers.ErrorHandler(404)),\\n                          (\\"*\\", \\"/tools/*\\", handlers.ErrorHandler(404)),\\n                          (\\"*\\", \\"{spec}/tools/*\\", handlers.ErrorHandler(404)),\\n                          (\\"*\\", \\"/serve.py\\", handlers.ErrorHandler(404))]\\n\\n        self.static = [(\\"GET\\", \\"*.worker\\", WorkersHandler())]\\n\\n        self.mountpoint_routes = OrderedDict()\\n\\n        self.add_mount_point(\\"/\\", None)\\n\\n    def get_routes(self):\\n        routes = self.forbidden_override + self.forbidden + self.static\\n        # Using reversed here means that mount points that are added later\\n        # get higher priority. This makes sense since / is typically added\\n        # first.\\n        for item in reversed(self.mountpoint_routes.values()):\\n            routes.extend(item)\\n        return routes\\n\\n    def add_static(self, path, format_args, content_type, route):\\n        handler = handlers.StaticHandler(path, format_args, content_type)\\n        self.static.append((b\\"GET\\", str(route), handler))\\n\\n    def add_mount_point(self, url_base, path):\\n        url_base = \\"/%s/\\" % url_base.strip(\\"/\\") if url_base != \\"/\\" else \\"/\\"\\n\\n        self.mountpoint_routes[url_base] = []\\n\\n        routes = [(\\"GET\\", \\"*.asis\\", handlers.AsIsHandler),\\n                  (\\"*\\", \\"*.py\\", handlers.PythonScriptHandler),\\n                  (\\"GET\\", \\"*\\", handlers.FileHandler)]\\n\\n        for (method, suffix, handler_cls) in routes:\\n            self.mountpoint_routes[url_base].append(\\n                (method,\\n                 b\\"%s%s\\" % (str(url_base) if url_base != \\"/\\" else \\"\\", str(suffix)),\\n                 handler_cls(base_path=path, url_base=url_base)))\\n\\n\\ndef default_routes():\\n    return RoutesBuilder().get_routes()\\n\\n\\ndef setup_logger(level):\\n    import logging\\n    global logger\\n    logger = logging.getLogger(\\"web-platform-tests\\")\\n    logging.basicConfig(level=getattr(logging, level.upper()))\\n    set_logger(logger)\\n\\n\\ndef open_socket(port):\\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n    if port != 0:\\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\\n    sock.bind((\'127.0.0.1\', port))\\n    sock.listen(5)\\n    return sock\\n\\n\\ndef get_port():\\n    free_socket = open_socket(0)\\n    port = free_socket.getsockname()[1]\\n    logger.debug(\\"Going to use port %s\\" % port)\\n    free_socket.close()\\n    return port\\n\\n\\nclass ServerProc(object):\\n    def __init__(self):\\n        self.proc = None\\n        self.daemon = None\\n        self.stop = Event()\\n\\n    def start(self, init_func, host, port, paths, routes, bind_hostname, external_config,\\n              ssl_config, **kwargs):\\n        self.proc = Process(target=self.create_daemon,\\n                            args=(init_func, host, port, paths, routes, bind_hostname,\\n                                  external_config, ssl_config))\\n        self.proc.daemon = True\\n        self.proc.start()\\n\\n    def create_daemon(self, init_func, host, port, paths, routes, bind_hostname,\\n                      external_config, ssl_config, **kwargs):\\n        try:\\n            self.daemon = init_func(host, port, paths, routes, bind_hostname, external_config,\\n                                    ssl_config, **kwargs)\\n        except socket.error:\\n            print \\u003e\\u003e sys.stderr, \\"Socket error on port %s\\" % port\\n            raise\\n        except:\\n            print \\u003e\\u003e sys.stderr, traceback.format_exc()\\n            raise\\n\\n        if self.daemon:\\n            try:\\n                self.daemon.start(block=False)\\n                try:\\n                    self.stop.wait()\\n                except KeyboardInterrupt:\\n                    pass\\n            except:\\n                print \\u003e\\u003e sys.stderr, traceback.format_exc()\\n                raise\\n\\n    def wait(self):\\n        self.stop.set()\\n        self.proc.join()\\n\\n    def kill(self):\\n        self.stop.set()\\n        self.proc.terminate()\\n        self.proc.join()\\n\\n    def is_alive(self):\\n        return self.proc.is_alive()\\n\\n\\ndef check_subdomains(host, paths, bind_hostname, ssl_config):\\n    port = get_port()\\n    subdomains = get_subdomains(host)\\n\\n    wrapper = ServerProc()\\n    wrapper.start(start_http_server, host, port, paths, default_routes(), bind_hostname,\\n                  None, ssl_config)\\n\\n    connected = False\\n    for i in range(10):\\n        try:\\n            urllib2.urlopen(\\"http://%s:%d/\\" % (host, port))\\n            connected = True\\n            break\\n        except urllib2.URLError:\\n            time.sleep(1)\\n\\n    if not connected:\\n        logger.critical(\\"Failed to connect to test server on http://%s:%s You may need to edit /etc/hosts or similar\\" % (host, port))\\n        sys.exit(1)\\n\\n    for subdomain, (punycode, host) in subdomains.iteritems():\\n        domain = \\"%s.%s\\" % (punycode, host)\\n        try:\\n            urllib2.urlopen(\\"http://%s:%d/\\" % (domain, port))\\n        except Exception as e:\\n            logger.critical(\\"Failed probing domain %s. You may need to edit /etc/hosts or similar.\\" % domain)\\n            sys.exit(1)\\n\\n    wrapper.wait()\\n\\n\\ndef get_subdomains(host):\\n    #This assumes that the tld is ascii-only or already in punycode\\n    return {subdomain: (subdomain.encode(\\"idna\\"), host)\\n            for subdomain in subdomains}\\n\\n\\ndef start_servers(host, ports, paths, routes, bind_hostname, external_config, ssl_config,\\n                  **kwargs):\\n    servers = defaultdict(list)\\n    for scheme, ports in ports.iteritems():\\n        assert len(ports) == {\\"http\\":2}.get(scheme, 1)\\n\\n        for port in ports:\\n            if port is None:\\n                continue\\n            init_func = {\\"http\\":start_http_server,\\n                         \\"https\\":start_https_server,\\n                         \\"ws\\":start_ws_server,\\n                         \\"wss\\":start_wss_server}[scheme]\\n\\n            server_proc = ServerProc()\\n            server_proc.start(init_func, host, port, paths, routes, bind_hostname,\\n                              external_config, ssl_config, **kwargs)\\n            servers[scheme].append((port, server_proc))\\n\\n    return servers\\n\\n\\ndef start_http_server(host, port, paths, routes, bind_hostname, external_config, ssl_config,\\n                      **kwargs):\\n    return wptserve.WebTestHttpd(host=host,\\n                                 port=port,\\n                                 doc_root=paths[\\"doc_root\\"],\\n                                 routes=routes,\\n                                 rewrites=rewrites,\\n                                 bind_hostname=bind_hostname,\\n                                 config=external_config,\\n                                 use_ssl=False,\\n                                 key_file=None,\\n                                 certificate=None,\\n                                 latency=kwargs.get(\\"latency\\"))\\n\\n\\ndef start_https_server(host, port, paths, routes, bind_hostname, external_config, ssl_config,\\n                       **kwargs):\\n    return wptserve.WebTestHttpd(host=host,\\n                                 port=port,\\n                                 doc_root=paths[\\"doc_root\\"],\\n                                 routes=routes,\\n                                 rewrites=rewrites,\\n                                 bind_hostname=bind_hostname,\\n                                 config=external_config,\\n                                 use_ssl=True,\\n                                 key_file=ssl_config[\\"key_path\\"],\\n                                 certificate=ssl_config[\\"cert_path\\"],\\n                                 encrypt_after_connect=ssl_config[\\"encrypt_after_connect\\"],\\n                                 latency=kwargs.get(\\"latency\\"))\\n\\n\\nclass WebSocketDaemon(object):\\n    def __init__(self, host, port, doc_root, handlers_root, log_level, bind_hostname,\\n                 ssl_config):\\n        self.host = host\\n        cmd_args = [\\"-p\\", port,\\n                    \\"-d\\", doc_root,\\n                    \\"-w\\", handlers_root,\\n                    \\"--log-level\\", log_level]\\n\\n        if ssl_config is not None:\\n            # This is usually done through pywebsocket.main, however we\'re\\n            # working around that to get the server instance and manually\\n            # setup the wss server.\\n            if pywebsocket._import_ssl():\\n                tls_module = pywebsocket._TLS_BY_STANDARD_MODULE\\n            elif pywebsocket._import_pyopenssl():\\n                tls_module = pywebsocket._TLS_BY_PYOPENSSL\\n            else:\\n                print \\"No SSL module available\\"\\n                sys.exit(1)\\n\\n            cmd_args += [\\"--tls\\",\\n                         \\"--private-key\\", ssl_config[\\"key_path\\"],\\n                         \\"--certificate\\", ssl_config[\\"cert_path\\"],\\n                         \\"--tls-module\\", tls_module]\\n\\n        if (bind_hostname):\\n            cmd_args = [\\"-H\\", host] + cmd_args\\n        opts, args = pywebsocket._parse_args_and_config(cmd_args)\\n        opts.cgi_directories = []\\n        opts.is_executable_method = None\\n        self.server = pywebsocket.WebSocketServer(opts)\\n        ports = [item[0].getsockname()[1] for item in self.server._sockets]\\n        assert all(item == ports[0] for item in ports)\\n        self.port = ports[0]\\n        self.started = False\\n        self.server_thread = None\\n\\n    def start(self, block=False):\\n        self.started = True\\n        if block:\\n            self.server.serve_forever()\\n        else:\\n            self.server_thread = threading.Thread(target=self.server.serve_forever)\\n            self.server_thread.setDaemon(True)  # don\'t hang on exit\\n            self.server_thread.start()\\n\\n    def stop(self):\\n        \\"\\"\\"\\n        Stops the server.\\n\\n        If the server is not running, this method has no effect.\\n        \\"\\"\\"\\n        if self.started:\\n            try:\\n                self.server.shutdown()\\n                self.server.server_close()\\n                self.server_thread.join()\\n                self.server_thread = None\\n            except AttributeError:\\n                pass\\n            self.started = False\\n        self.server = None\\n\\n\\ndef start_ws_server(host, port, paths, routes, bind_hostname, external_config, ssl_config,\\n                    **kwargs):\\n    return WebSocketDaemon(host,\\n                           str(port),\\n                           repo_root,\\n                           paths[\\"ws_doc_root\\"],\\n                           \\"debug\\",\\n                           bind_hostname,\\n                           ssl_config = None)\\n\\n\\ndef start_wss_server(host, port, paths, routes, bind_hostname, external_config, ssl_config,\\n                     **kwargs):\\n    return WebSocketDaemon(host,\\n                           str(port),\\n                           repo_root,\\n                           paths[\\"ws_doc_root\\"],\\n                           \\"debug\\",\\n                           bind_hostname,\\n                           ssl_config)\\n\\n\\ndef get_ports(config, ssl_environment):\\n    rv = defaultdict(list)\\n    for scheme, ports in config[\\"ports\\"].iteritems():\\n        for i, port in enumerate(ports):\\n            if scheme in [\\"wss\\", \\"https\\"] and not ssl_environment.ssl_enabled:\\n                port = None\\n            if port == \\"auto\\":\\n                port = get_port()\\n            else:\\n                port = port\\n            rv[scheme].append(port)\\n    return rv\\n\\n\\n\\ndef normalise_config(config, ports):\\n    host = config[\\"external_host\\"] if config[\\"external_host\\"] else config[\\"host\\"]\\n    domains = get_subdomains(host)\\n    ports_ = {}\\n    for scheme, ports_used in ports.iteritems():\\n        ports_[scheme] = ports_used\\n\\n    for key, value in domains.iteritems():\\n        domains[key] = \\".\\".join(value)\\n\\n    domains[\\"\\"] = host\\n\\n    ports_ = {}\\n    for scheme, ports_used in ports.iteritems():\\n        ports_[scheme] = ports_used\\n\\n    return {\\"host\\": host,\\n            \\"domains\\": domains,\\n            \\"ports\\": ports_}\\n\\n\\ndef get_ssl_config(config, external_domains, ssl_environment):\\n    key_path, cert_path = ssl_environment.host_cert_path(external_domains)\\n    return {\\"key_path\\": key_path,\\n            \\"cert_path\\": cert_path,\\n            \\"encrypt_after_connect\\": config[\\"ssl\\"][\\"encrypt_after_connect\\"]}\\n\\ndef start(config, ssl_environment, routes, **kwargs):\\n    host = config[\\"host\\"]\\n    domains = get_subdomains(host)\\n    ports = get_ports(config, ssl_environment)\\n    bind_hostname = config[\\"bind_hostname\\"]\\n\\n    paths = {\\"doc_root\\": config[\\"doc_root\\"],\\n             \\"ws_doc_root\\": config[\\"ws_doc_root\\"]}\\n\\n    external_config = normalise_config(config, ports)\\n\\n    ssl_config = get_ssl_config(config, external_config[\\"domains\\"].values(), ssl_environment)\\n\\n    if config[\\"check_subdomains\\"]:\\n        check_subdomains(host, paths, bind_hostname, ssl_config)\\n\\n    servers = start_servers(host, ports, paths, routes, bind_hostname, external_config,\\n                            ssl_config, **kwargs)\\n\\n    return external_config, servers\\n\\n\\ndef iter_procs(servers):\\n    for servers in servers.values():\\n        for port, server in servers:\\n            yield server.proc\\n\\n\\ndef value_set(config, key):\\n    return key in config and config[key] is not None\\n\\n\\ndef get_value_or_default(config, key, default=None):\\n    return config[key] if value_set(config, key) else default\\n\\n\\ndef set_computed_defaults(config):\\n    if not value_set(config, \\"doc_root\\"):\\n        config[\\"doc_root\\"] = repo_root\\n\\n    if not value_set(config, \\"ws_doc_root\\"):\\n        root = get_value_or_default(config, \\"doc_root\\", default=repo_root)\\n        config[\\"ws_doc_root\\"] = os.path.join(root, \\"websockets\\", \\"handlers\\")\\n\\n\\ndef merge_json(base_obj, override_obj):\\n    rv = {}\\n    for key, value in base_obj.iteritems():\\n        if key not in override_obj:\\n            rv[key] = value\\n        else:\\n            if isinstance(value, dict):\\n                rv[key] = merge_json(value, override_obj[key])\\n            else:\\n                rv[key] = override_obj[key]\\n    return rv\\n\\n\\ndef get_ssl_environment(config):\\n    implementation_type = config[\\"ssl\\"][\\"type\\"]\\n    cls = sslutils.environments[implementation_type]\\n    try:\\n        kwargs = config[\\"ssl\\"][implementation_type].copy()\\n    except KeyError:\\n        raise ValueError(\\"%s is not a vaid ssl type.\\" % implementation_type)\\n    return cls(logger, **kwargs)\\n\\n\\ndef load_config(default_path, override_path=None, **kwargs):\\n    if os.path.exists(default_path):\\n        with open(default_path) as f:\\n            base_obj = json.load(f)\\n    else:\\n        raise ValueError(\\"Config path %s does not exist\\" % default_path)\\n\\n    if os.path.exists(override_path):\\n        with open(override_path) as f:\\n            override_obj = json.load(f)\\n    else:\\n        override_obj = {}\\n    rv = merge_json(base_obj, override_obj)\\n\\n    if kwargs.get(\\"config_path\\"):\\n        other_path = os.path.abspath(os.path.expanduser(kwargs.get(\\"config_path\\")))\\n        if os.path.exists(other_path):\\n            base_obj = rv\\n            with open(other_path) as f:\\n                override_obj = json.load(f)\\n            rv = merge_json(base_obj, override_obj)\\n        else:\\n            raise ValueError(\\"Config path %s does not exist\\" % other_path)\\n\\n    overriding_path_args = [(\\"doc_root\\", \\"Document root\\"),\\n                            (\\"ws_doc_root\\", \\"WebSockets document root\\")]\\n    for key, title in overriding_path_args:\\n        value = kwargs.get(key)\\n        if value is None:\\n            continue\\n        value = os.path.abspath(os.path.expanduser(value))\\n        if not os.path.exists(value):\\n            raise ValueError(\\"%s path %s does not exist\\" % (title, value))\\n        rv[key] = value\\n\\n    set_computed_defaults(rv)\\n    return rv\\n\\n\\ndef get_parser():\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\\"--latency\\", type=int,\\n                        help=\\"Artificial latency to add before sending http responses, in ms\\")\\n    parser.add_argument(\\"--config\\", action=\\"store\\", dest=\\"config_path\\",\\n                        help=\\"Path to external config file\\")\\n    parser.add_argument(\\"--doc_root\\", action=\\"store\\", dest=\\"doc_root\\",\\n                        help=\\"Path to document root. Overrides config.\\")\\n    parser.add_argument(\\"--ws_doc_root\\", action=\\"store\\", dest=\\"ws_doc_root\\",\\n                        help=\\"Path to WebSockets document root. Overrides config.\\")\\n    return parser\\n\\n\\ndef main():\\n    kwargs = vars(get_parser().parse_args())\\n    config = load_config(\\"config.default.json\\",\\n                         \\"config.json\\",\\n                         **kwargs)\\n\\n    setup_logger(config[\\"log_level\\"])\\n\\n    with stash.StashServer((config[\\"host\\"], get_port()), authkey=str(uuid.uuid4())):\\n        with get_ssl_environment(config) as ssl_env:\\n            config_, servers = start(config, ssl_env, default_routes(), **kwargs)\\n\\n            try:\\n                while any(item.is_alive() for item in iter_procs(servers)):\\n                    for item in iter_procs(servers):\\n                        item.join(1)\\n            except KeyboardInterrupt:\\n                logger.info(\\"Shutting down\\")\\n"}\n'
line: b'{"repo_name":"xadahiya/django","ref":"refs/heads/master","path":"django/contrib/admin/actions.py","content":"\\"\\"\\"\\nBuilt-in, globally-available admin actions.\\n\\"\\"\\"\\n\\nfrom django.contrib import messages\\nfrom django.contrib.admin import helpers\\nfrom django.contrib.admin.utils import get_deleted_objects, model_ngettext\\nfrom django.core.exceptions import PermissionDenied\\nfrom django.db import router\\nfrom django.template.response import TemplateResponse\\nfrom django.utils.encoding import force_text\\nfrom django.utils.translation import ugettext as _, ugettext_lazy\\n\\n\\ndef delete_selected(modeladmin, request, queryset):\\n    \\"\\"\\"\\n    Default action which deletes the selected objects.\\n\\n    This action first displays a confirmation page whichs shows all the\\n    deleteable objects, or, if the user has no permission one of the related\\n    childs (foreignkeys), a \\"permission denied\\" message.\\n\\n    Next, it deletes all selected objects and redirects back to the change list.\\n    \\"\\"\\"\\n    opts = modeladmin.model._meta\\n    app_label = opts.app_label\\n\\n    # Check that the user has delete permission for the actual model\\n    if not modeladmin.has_delete_permission(request):\\n        raise PermissionDenied\\n\\n    using = router.db_for_write(modeladmin.model)\\n\\n    # Populate deletable_objects, a data structure of all related objects that\\n    # will also be deleted.\\n    deletable_objects, model_count, perms_needed, protected = get_deleted_objects(\\n        queryset, opts, request.user, modeladmin.admin_site, using)\\n\\n    # The user has already confirmed the deletion.\\n    # Do the deletion and return a None to display the change list view again.\\n    if request.POST.get(\'post\'):\\n        if perms_needed:\\n            raise PermissionDenied\\n        n = queryset.count()\\n        if n:\\n            for obj in queryset:\\n                obj_display = force_text(obj)\\n                modeladmin.log_deletion(request, obj, obj_display)\\n            queryset.delete()\\n            modeladmin.message_user(request, _(\\"Successfully deleted %(count)d %(items)s.\\") % {\\n                \\"count\\": n, \\"items\\": model_ngettext(modeladmin.opts, n)\\n            }, messages.SUCCESS)\\n        # Return None to display the change list page again.\\n        return None\\n\\n    if len(queryset) == 1:\\n        objects_name = force_text(opts.verbose_name)\\n    else:\\n        objects_name = force_text(opts.verbose_name_plural)\\n\\n    if perms_needed or protected:\\n        title = _(\\"Cannot delete %(name)s\\") % {\\"name\\": objects_name}\\n    else:\\n        title = _(\\"Are you sure?\\")\\n\\n    context = dict(\\n        modeladmin.admin_site.each_context(request),\\n        title=title,\\n        objects_name=objects_name,\\n        deletable_objects=[deletable_objects],\\n        model_count=dict(model_count).items(),\\n        queryset=queryset,\\n        perms_lacking=perms_needed,\\n        protected=protected,\\n        opts=opts,\\n        action_checkbox_name=helpers.ACTION_CHECKBOX_NAME,\\n    )\\n\\n    request.current_app = modeladmin.admin_site.name\\n\\n    # Display the confirmation page\\n    return TemplateResponse(request, modeladmin.delete_selected_confirmation_template or [\\n        \\"admin/%s/%s/delete_selected_confirmation.html\\" % (app_label, opts.model_name),\\n        \\"admin/%s/delete_selected_confirmation.html\\" % app_label,\\n        \\"admin/delete_selected_confirmation.html\\"\\n    ], context)\\n\\ndelete_selected.short_description = ugettext_lazy(\\"Delete selected %(verbose_name_plural)s\\")\\n"}\n'
line: b'{"repo_name":"wsmith323/django","ref":"refs/heads/master","path":"tests/save_delete_hooks/tests.py","content":"from __future__ import unicode_literals\\n\\nfrom django.test import TestCase\\nfrom django.utils import six\\n\\nfrom .models import Person\\n\\n\\nclass SaveDeleteHookTests(TestCase):\\n    def test_basic(self):\\n        p = Person(first_name=\\"John\\", last_name=\\"Smith\\")\\n        self.assertEqual(p.data, [])\\n        p.save()\\n        self.assertEqual(p.data, [\\n            \\"Before save\\",\\n            \\"After save\\",\\n        ])\\n\\n        self.assertQuerysetEqual(\\n            Person.objects.all(), [\\n                \\"John Smith\\",\\n            ],\\n            six.text_type\\n        )\\n\\n        p.delete()\\n        self.assertEqual(p.data, [\\n            \\"Before save\\",\\n            \\"After save\\",\\n            \\"Before deletion\\",\\n            \\"After deletion\\",\\n        ])\\n        self.assertQuerysetEqual(Person.objects.all(), [])\\n"}\n'
line: b'{"repo_name":"sparkslabs/kamaelia","ref":"refs/heads/master","path":"Sketches/CL/Topology/src/RelationTopology/Util/RelationAttributeParsing.py","content":"#!/usr/bin/env python\\n# -*- coding: utf-8 -*-\\n\\n# Copyright 2010 British Broadcasting Corporation and Kamaelia Contributors(1)\\n#\\n# (1) Kamaelia Contributors are listed in the AUTHORS file and at\\n#     http://www.kamaelia.org/AUTHORS - please extend this file,\\n#     not this notice.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\"\\"\\"\\\\\\n===============================================================\\nParse entities, attributes and relations definition received\\n===============================================================\\n\\nParse entities and relations definition received, one line one time.\\n\\n1. Definition format\\n1.) Empty line (including any number of white spaces)\\n2.) Line starting with # to comment\\n3.) Entity definition\\nExample:\\n--------\\nperson mum\\nperson dad gender=male,shape=rect,width=80,height=80\\nperson son gender=\\"male\\",photo=\\"../Files/son.gif,width=60,height=60\\"\\nperson daughter radius=100\\n4.) Relation definition\\nExample: \\n--------\\nchildof(mum, son)\\n\\n2. NOTE:\\n1.) Any number of spaces can exist before, after and between the above line\\nExample:\\n--------\\n  person    mum  \\n     childof  (  mum  , son  )  \\n2.) Parse one line one time and then send out\\n3.) Entity definition needs to come before relation definition \\nif the relations definition uses the entity\\n4.) When encountering repeated entity, it will update its attributes rather than\\ncreate a new one.       \\n\\"\\"\\"\\n\\ndef parseEntity(entityLine):\\n    \\"\\"\\" parse entity line \\"\\"\\"\\n    result = entityLine.split()\\n    #entity_ID = result[0]+\'_\'+result[1]\\n    entity_name = result[1]\\n    #particle = \'-\'\\n    particle = \'GenericParticle\'\\n    if len(result) == 3:\\n        attributes = result[2]\\n        #attributes = attributes.lower()\\n        attributes = attributes.replace(\'gender\',\'color\')\\n        attributes = attributes.replace(\'female\',\'pink\')\\n        attributes = attributes.replace(\'male\',\'blue\')\\n        attributes = attributes.replace(\'photo\',\'pic\')\\n        attributes = attributes + \',type=\' + result[0]\\n    else:\\n        attributes = \'type=\' + result[0]               \\n    return \\"ADD NODE %s %s auto %s %s\\" % (entity_name,entity_name,particle,attributes)\\n\\ndef parseUpdatedEntity(entityLine):\\n    \\"\\"\\" parse entity line \\"\\"\\"\\n    result = entityLine.split()\\n    #entity_ID = result[0]+\'_\'+result[1]\\n    entity_name = result[1]\\n    #particle = \'-\'\\n    #particle = \'GenericParticle\'\\n    if len(result) == 3:\\n        attributes = result[2]\\n        #attributes = attributes.lower()\\n        attributes = attributes.replace(\'gender\',\'color\')\\n        attributes = attributes.replace(\'female\',\'pink\')\\n        attributes = attributes.replace(\'male\',\'blue\')\\n        attributes = attributes.replace(\'photo\',\'pic\')\\n        attributes = attributes.replace(\'name\',\'label\')\\n    else:\\n        attributes = \'label=\' + entity_name              \\n    return \\"UPDATE NODE %s %s\\" % (entity_name,attributes)\\n\\ndef parseRelation(relationLine):\\n    \\"\\"\\" parse relation line \\"\\"\\"\\n    result = relationLine.split(\'(\')\\n    relation = result[0].strip()\\n    entities_str = result[1].rstrip(\')\')\\n    entities_list = entities_str.split(\',\')\\n    src = entities_list[0].strip()\\n    dst = entities_list[1].strip()\\n    return \\"ADD LINK %s %s %s\\" % (src,dst,relation)\\n\\n\\n        \\nimport re\\n\\nimport Axon\\nfrom Axon.Ipc import producerFinished, shutdownMicroprocess\\n\\nclass RelationAttributeParser(Axon.Component.component):\\n    \\"\\"\\"\\\\\\n======================================================================\\nA component to parse entities, attributes and relations definition\\n======================================================================\\n\\"\\"\\"\\n    def shutdown(self):\\n        \\"\\"\\" shutdown method: define when to shun down\\"\\"\\"\\n        while self.dataReady(\\"control\\"):\\n            data = self.recv(\\"control\\")\\n            if isinstance(data, producerFinished) or isinstance(data, shutdownMicroprocess):\\n                self.shutdown_mess = data\\n                return True\\n        return False\\n      \\n    def main(self):\\n        \\"\\"\\" main method: do stuff \\"\\"\\"\\n        \\n        previousNodes = []  \\n        \\n        # Put all codes within the loop, so that others can be run even it doesn\'t shut down\\n        while not self.shutdown():\\n            X = []\\n            links = []\\n            nodes = []\\n            updatedNodes = []\\n            while not self.anyReady():\\n                self.pause()\\n                yield 1\\n    \\n            while self.dataReady(\\"inbox\\"):\\n                L = self.recv(\\"inbox\\")\\n                if L.strip() == \\"\\": continue # empty line\\n                if L.lstrip()[0] == \\"#\\": continue # comment\\n                X.append(L.strip())\\n            #yield 1\\n\\n            for item in X:            \\n                if re.match(\'(.+)\\\\((.+),(.+)\\\\)\',item): # relation\\n                    command = parseRelation(item)\\n                    links.append(command)\\n                else:\\n                    isRepeated = False\\n                    for node in previousNodes:\\n                        if item.split()[1] == node.split()[2]:\\n                            isRepeated = True\\n                    if not isRepeated: # new entity\\n                        command = parseEntity(item)\\n                        nodes.append(command)        \\n                        previousNodes.append(command)\\n                    else: # old entity\\n                        command = parseUpdatedEntity(item)\\n                        updatedNodes.append(command)\\n            #yield 1\\n            for node in nodes:\\n                self.send(node, \\"outbox\\")\\n            for updatedNode in updatedNodes:\\n                self.send(updatedNode, \\"outbox\\")\\n            for link in links:\\n                self.send(link, \\"outbox\\")\\n            yield 1\\n            \\n        \\n        self.send(self.shutdown_mess,\\"signal\\")\\n        \\nif __name__ == \\"__main__\\":\\n    from Kamaelia.Util.DataSource import DataSource\\n    from Kamaelia.Visualisation.PhysicsGraph.lines_to_tokenlists import lines_to_tokenlists\\n    from Kamaelia.Util.Console import ConsoleReader,ConsoleEchoer\\n    from GenericTopologyViewer import GenericTopologyViewer\\n    from Kamaelia.Chassis.Graphline import Graphline\\n    \\n    # Data can be from both DataSource and console inputs\\n    Graphline(\\n        CONSOLEREADER = ConsoleReader(),\\n        DATASOURCE = DataSource([\\"  person  mum   gender=female,photo=../Files/mum.jpg,width=80,height=80 \\", \'  \', \\"\\"\\"   \\n                    \\"\\"\\", \'person dad gender=male,shape=rect,width=80,height=80\', \\n                    \'  person  son   gender=male,photo=../Files/son.gif,width=60,height=60\',\\n                    \'person son photo=../Files/son1.gif\',\\n                     \'person daughter radius=20\', \'person daughter radius=100\',\\n                     \' childof  (  mum  , son  ) \', \'childof(mum, daughter)\',\\n                     \'childof(dad, son)\', \'childof(dad, daughter)\']),\\n        PARSER = RelationAttributeParser(),\\n        TOKENS = lines_to_tokenlists(),\\n        VIEWER = GenericTopologyViewer(),\\n        CONSOLEECHOER = ConsoleEchoer(),\\n    linkages = {\\n        (\\"CONSOLEREADER\\",\\"outbox\\") : (\\"PARSER\\",\\"inbox\\"),\\n        (\\"DATASOURCE\\",\\"outbox\\") : (\\"PARSER\\",\\"inbox\\"),\\n        (\\"PARSER\\",\\"outbox\\") : (\\"TOKENS\\",\\"inbox\\"),\\n        (\\"TOKENS\\",\\"outbox\\")   : (\\"VIEWER\\",\\"inbox\\"),\\n        (\\"VIEWER\\",\\"outbox\\")  : (\\"CONSOLEECHOER\\",\\"inbox\\"),\\n        \\n    }\\n).run()"}\n'
line: b'{"repo_name":"dssg/wikienergy","ref":"refs/heads/master","path":"disaggregator/build/pandas/pandas/io/tests/__init__.py","content":"\\ndef setUp():\\n    import socket\\n    socket.setdefaulttimeout(5)\\n"}\n'
line: b'{"repo_name":"soulxu/libvirt-xuhj","ref":"refs/heads/master","path":"src/esx/esx_vi_generator.py","content":"#!/usr/bin/env python\\n\\n#\\n# esx_vi_generator.py: generates most of the SOAP type mapping code\\n#\\n# Copyright (C) 2010-2011 Matthias Bolte \\u003cmatthias.bolte@googlemail.com\\u003e\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n#\\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n#\\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307  USA\\n#\\n\\nimport sys\\nimport os\\nimport os.path\\n\\n\\n\\nOCCURRENCE__REQUIRED_ITEM = \\"r\\"\\nOCCURRENCE__REQUIRED_LIST = \\"rl\\"\\nOCCURRENCE__OPTIONAL_ITEM = \\"o\\"\\nOCCURRENCE__OPTIONAL_LIST = \\"ol\\"\\nOCCURRENCE__IGNORED = \\"i\\"\\n\\nvalid_occurrences = [OCCURRENCE__REQUIRED_ITEM,\\n                     OCCURRENCE__REQUIRED_LIST,\\n                     OCCURRENCE__OPTIONAL_ITEM,\\n                     OCCURRENCE__OPTIONAL_LIST,\\n                     OCCURRENCE__IGNORED]\\n\\nautobind_names = set()\\n\\nseparator = \\"/* \\" + (\\"* \\" * 37) + \\"*\\\\n\\"\\n\\n\\n\\ndef aligned(left, right, length=59):\\n    while len(left) \\u003c length:\\n        left += \\" \\"\\n\\n    return left + right\\n\\n\\n\\nclass Member:\\n    def __init__(self, type, occurrence):\\n        self.type = type\\n        self.occurrence = occurrence\\n\\n\\n    def is_enum(self):\\n        return self.type in predefined_enums or self.type in enums_by_name\\n\\n\\n    def is_object(self):\\n        return self.type in predefined_objects or self.type in objects_by_name\\n\\n\\n    def is_type_generated(self):\\n        return self.type in enums_by_name or self.type in objects_by_name\\n\\n\\n    def get_occurrence_comment(self):\\n        if self.occurrence == OCCURRENCE__REQUIRED_ITEM:\\n            return \\"/* required */\\"\\n        elif self.occurrence == OCCURRENCE__REQUIRED_LIST:\\n            return \\"/* required, list */\\"\\n        elif self.occurrence == OCCURRENCE__OPTIONAL_ITEM:\\n            return \\"/* optional */\\"\\n        elif self.occurrence == OCCURRENCE__OPTIONAL_LIST:\\n            return \\"/* optional, list */\\"\\n\\n        raise ValueError(\\"unknown occurrence value \'%s\'\\" % self.occurrence)\\n\\n\\n\\nclass Parameter(Member):\\n    def __init__(self, type, name, occurrence):\\n        Member.__init__(self, type, occurrence)\\n\\n        if \':\' in name and name.startswith(\\"_this\\"):\\n            self.name, self.autobind_name = name.split(\\":\\")\\n        else:\\n            self.name = name\\n            self.autobind_name = None\\n\\n\\n    def generate_parameter(self, is_last=False, is_header=True, offset=0):\\n        if self.occurrence == OCCURRENCE__IGNORED:\\n            raise ValueError(\\"invalid function parameter occurrence value \'%s\'\\"\\n                             % self.occurrence)\\n        elif self.autobind_name is not None:\\n            return \\"\\"\\n        else:\\n            string = \\"       \\"\\n            string += \\" \\" * offset\\n            string += \\"%s%s\\" % (self.get_type_string(), self.name)\\n\\n            if is_last:\\n                if is_header:\\n                    string += \\"); \\"\\n                else:\\n                    string += \\"), \\"\\n            else:\\n                string += \\", \\"\\n\\n            return aligned(string, self.get_occurrence_comment() + \\"\\\\n\\")\\n\\n\\n    def generate_return(self, offset = 0, end_of_line = \\";\\"):\\n        if self.occurrence == OCCURRENCE__IGNORED:\\n            raise ValueError(\\"invalid function parameter occurrence value \'%s\'\\"\\n                             % self.occurrence)\\n        else:\\n            string = \\"       \\"\\n            string += \\" \\" * offset\\n            string += \\"%s%s)%s\\" \\\\\\n                      % (self.get_type_string(True), self.name, end_of_line)\\n\\n            return aligned(string, self.get_occurrence_comment() + \\"\\\\n\\")\\n\\n\\n    def generate_require_code(self):\\n        if self.occurrence in [OCCURRENCE__REQUIRED_ITEM,\\n                               OCCURRENCE__REQUIRED_LIST]:\\n            return \\"    ESX_VI__METHOD__PARAMETER__REQUIRE(%s)\\\\n\\" % self.name\\n        else:\\n            return \\"\\"\\n\\n\\n    def generate_serialize_code(self):\\n        if self.occurrence in [OCCURRENCE__REQUIRED_LIST,\\n                               OCCURRENCE__OPTIONAL_LIST]:\\n            return \\"    ESX_VI__METHOD__PARAMETER__SERIALIZE_LIST(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n        elif self.type == \\"String\\":\\n            return \\"    ESX_VI__METHOD__PARAMETER__SERIALIZE_VALUE(String, %s)\\\\n\\" \\\\\\n                   % self.name\\n        else:\\n            return \\"    ESX_VI__METHOD__PARAMETER__SERIALIZE(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n\\n\\n    def get_type_string(self, as_return_value=False):\\n        string = \\"\\"\\n\\n        if self.type == \\"String\\" and \\\\\\n           self.occurrence not in [OCCURRENCE__REQUIRED_LIST,\\n                                   OCCURRENCE__OPTIONAL_LIST]:\\n            if as_return_value:\\n                string += \\"char *\\"\\n            else:\\n                string += \\"const char *\\"\\n        elif self.is_enum():\\n            string += \\"esxVI_%s \\" % self.type\\n        else:\\n            string += \\"esxVI_%s *\\" % self.type\\n\\n        if as_return_value:\\n            string += \\"*\\"\\n\\n        return string\\n\\n\\n    def get_occurrence_short_enum(self):\\n        if self.occurrence == OCCURRENCE__REQUIRED_ITEM:\\n            return \\"RequiredItem\\"\\n        elif self.occurrence == OCCURRENCE__REQUIRED_LIST:\\n            return \\"RequiredList\\"\\n        elif self.occurrence == OCCURRENCE__OPTIONAL_ITEM:\\n            return \\"OptionalItem\\"\\n        elif self.occurrence == OCCURRENCE__OPTIONAL_LIST:\\n            return \\"OptionalList\\"\\n\\n        raise ValueError(\\"unknown occurrence value \'%s\'\\" % self.occurrence)\\n\\n\\n\\nclass Method:\\n    def __init__(self, name, parameters, returns):\\n        self.name = name\\n        self.parameters = []\\n        self.autobind_parameter = None\\n        self.returns = returns\\n\\n        for parameter in parameters:\\n            if parameter.autobind_name is None:\\n                self.parameters.append(parameter)\\n            else:\\n                self.autobind_parameter = parameter\\n\\n\\n    def generate_header(self):\\n        header = \\"int esxVI_%s\\\\n\\" % self.name\\n        header += \\"      (esxVI_Context *ctx\\"\\n\\n        if len(self.parameters) \\u003e 0 or self.returns is not None:\\n            header += \\",\\\\n\\"\\n\\n            for parameter in self.parameters[:-1]:\\n                header += parameter.generate_parameter()\\n\\n            if self.returns is None:\\n                header += self.parameters[-1].generate_parameter(is_last=True)\\n            else:\\n                header += self.parameters[-1].generate_parameter()\\n                header += self.returns.generate_return()\\n        else:\\n            header += \\");\\\\n\\"\\n\\n        header += \\"\\\\n\\"\\n\\n        return header\\n\\n\\n    def generate_source(self):\\n        source = \\"/* esxVI_%s */\\\\n\\" % self.name\\n        source += \\"ESX_VI__METHOD(%s,\\" % self.name\\n\\n        if self.autobind_parameter is not None:\\n            autobind_names.add(self.autobind_parameter.autobind_name)\\n            source += \\" %s,\\\\n\\" % self.autobind_parameter.autobind_name\\n        else:\\n            source += \\" /* explicit _this */,\\\\n\\"\\n\\n        source += \\"               (esxVI_Context *ctx\\"\\n\\n        if len(self.parameters) \\u003e 0 or self.returns is not None:\\n            source += \\",\\\\n\\"\\n\\n            for parameter in self.parameters[:-1]:\\n                source += parameter.generate_parameter(is_header=False,\\n                                                       offset=9)\\n\\n            if self.returns is None:\\n                source += self.parameters[-1].generate_parameter(is_last=True,\\n                                                                 is_header=False,\\n                                                                 offset=9)\\n            else:\\n                source += self.parameters[-1].generate_parameter(is_header=False,\\n                                                                 offset=9)\\n                source += self.returns.generate_return(offset=9,\\n                                                       end_of_line=\\",\\")\\n        else:\\n            source += \\"),\\\\n\\"\\n\\n        if self.returns is None:\\n            source += \\"               void, /* nothing */, None,\\\\n\\"\\n        elif self.returns.type == \\"String\\":\\n            source += \\"               String, Value, %s,\\\\n\\" \\\\\\n                      % self.returns.get_occurrence_short_enum()\\n        else:\\n            source += \\"               %s, /* nothing */, %s,\\\\n\\" \\\\\\n                      % (self.returns.type,\\n                         self.returns.get_occurrence_short_enum())\\n\\n        source += \\"{\\\\n\\"\\n\\n        if self.autobind_parameter is not None:\\n            source += self.autobind_parameter.generate_require_code()\\n\\n        for parameter in self.parameters:\\n            source += parameter.generate_require_code()\\n\\n        source += \\"},\\\\n\\"\\n        source += \\"{\\\\n\\"\\n\\n        if self.autobind_parameter is not None:\\n            source += self.autobind_parameter.generate_serialize_code()\\n\\n        for parameter in self.parameters:\\n            source += parameter.generate_serialize_code()\\n\\n        source += \\"})\\\\n\\\\n\\\\n\\\\n\\"\\n\\n        return source\\n\\n\\n\\nclass Property(Member):\\n    def __init__(self, type, name, occurrence):\\n        Member.__init__(self, type, occurrence)\\n\\n        self.name = name\\n\\n\\n    def generate_struct_member(self):\\n        if self.occurrence == OCCURRENCE__IGNORED:\\n            return \\"    /* FIXME: %s is currently ignored */\\\\n\\" % self.name\\n        else:\\n            string = \\"    %s%s; \\" % (self.get_type_string(), self.name)\\n\\n            return aligned(string, self.get_occurrence_comment() + \\"\\\\n\\")\\n\\n\\n    def generate_free_code(self):\\n        if self.type == \\"String\\" and \\\\\\n           self.occurrence not in [OCCURRENCE__REQUIRED_LIST,\\n                                   OCCURRENCE__OPTIONAL_LIST,\\n                                   OCCURRENCE__IGNORED]:\\n            return \\"    VIR_FREE(item-\\u003e%s);\\\\n\\" % self.name\\n        elif self.is_enum():\\n            return \\"\\"\\n        else:\\n            if self.occurrence == OCCURRENCE__IGNORED:\\n                return \\"    /* FIXME: %s is currently ignored */\\\\n\\" % self.name\\n            else:\\n                return \\"    esxVI_%s_Free(\\u0026item-\\u003e%s);\\\\n\\" % (self.type, self.name)\\n\\n\\n    def generate_validate_code(self, managed=False):\\n        if managed:\\n            macro = \\"ESX_VI__TEMPLATE__PROPERTY__MANAGED_REQUIRE\\"\\n        else:\\n            macro = \\"ESX_VI__TEMPLATE__PROPERTY__REQUIRE\\"\\n\\n        if self.occurrence in [OCCURRENCE__REQUIRED_ITEM,\\n                               OCCURRENCE__REQUIRED_LIST]:\\n            return \\"    %s(%s)\\\\n\\" % (macro, self.name)\\n        elif self.occurrence == OCCURRENCE__IGNORED:\\n            return \\"    /* FIXME: %s is currently ignored */\\\\n\\" % self.name\\n        else:\\n            return \\"\\"\\n\\n\\n    def generate_deep_copy_code(self):\\n        if self.occurrence == OCCURRENCE__IGNORED:\\n            return \\"    /* FIXME: %s is currently ignored */\\\\n\\" % self.name\\n        elif self.occurrence in [OCCURRENCE__REQUIRED_LIST,\\n                                 OCCURRENCE__OPTIONAL_LIST]:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__DEEP_COPY_LIST(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n        elif self.type == \\"String\\":\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__DEEP_COPY_VALUE(String, %s)\\\\n\\" \\\\\\n                   % self.name\\n        elif self.is_enum():\\n            return \\"    (*dest)-\\u003e%s = src-\\u003e%s;\\\\n\\" % (self.name, self.name)\\n        else:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__DEEP_COPY(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n\\n\\n    def generate_serialize_code(self):\\n        if self.occurrence == OCCURRENCE__IGNORED:\\n            return \\"    /* FIXME: %s is currently ignored */\\\\n\\" % self.name\\n        elif self.occurrence in [OCCURRENCE__REQUIRED_LIST,\\n                                 OCCURRENCE__OPTIONAL_LIST]:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__SERIALIZE_LIST(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n        elif self.type == \\"String\\":\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__SERIALIZE_VALUE(String, %s)\\\\n\\" \\\\\\n                   % self.name\\n        else:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__SERIALIZE(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n\\n\\n    def generate_deserialize_code(self):\\n        if self.occurrence == OCCURRENCE__IGNORED:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__DESERIALIZE_IGNORE(%s) /* FIXME */\\\\n\\" \\\\\\n                   % self.name\\n        elif self.occurrence in [OCCURRENCE__REQUIRED_LIST,\\n                                 OCCURRENCE__OPTIONAL_LIST]:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__DESERIALIZE_LIST(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n        elif self.type == \\"String\\":\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__DESERIALIZE_VALUE(String, %s)\\\\n\\" \\\\\\n                   % self.name\\n        else:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__DESERIALIZE(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n\\n\\n    def generate_lookup_code(self):\\n        if self.occurrence == OCCURRENCE__IGNORED:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__CAST_FROM_ANY_TYPE_IGNORE(%s) /* FIXME */\\\\n\\" \\\\\\n                   % self.name\\n        elif self.occurrence in [OCCURRENCE__REQUIRED_LIST,\\n                                 OCCURRENCE__OPTIONAL_LIST]:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__CAST_LIST_FROM_ANY_TYPE(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n        elif self.type == \\"String\\":\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__CAST_VALUE_FROM_ANY_TYPE(String, %s)\\\\n\\" \\\\\\n                   % self.name\\n        else:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__CAST_FROM_ANY_TYPE(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n\\n\\n    def get_type_string(self):\\n        if self.type == \\"String\\" and \\\\\\n           self.occurrence not in [OCCURRENCE__REQUIRED_LIST,\\n                                   OCCURRENCE__OPTIONAL_LIST]:\\n            return \\"char *\\"\\n        elif self.is_enum():\\n            return \\"esxVI_%s \\" % self.type\\n        else:\\n            return \\"esxVI_%s *\\" % self.type\\n\\n\\n\\nclass Type:\\n    def __init__(self, kind, name):\\n        self.kind = kind\\n        self.name = name\\n\\n\\n    def generate_typedef(self):\\n        return \\"typedef %s _esxVI_%s esxVI_%s;\\\\n\\" \\\\\\n               % (self.kind, self.name, self.name)\\n\\n\\n    def generate_typeenum(self):\\n        return \\"    esxVI_Type_%s,\\\\n\\" % self.name\\n\\n\\n    def generate_typetostring(self):\\n        string = \\"          case esxVI_Type_%s:\\\\n\\" % self.name\\n        string += \\"            return \\\\\\"%s\\\\\\";\\\\n\\\\n\\" % self.name\\n\\n        return string\\n\\n\\n    def generate_typefromstring(self):\\n        string =  \\"           else if (STREQ(type, \\\\\\"%s\\\\\\")) {\\\\n\\" % self.name\\n        string += \\"               return esxVI_Type_%s;\\\\n\\" % self.name\\n        string += \\"           }\\\\n\\"\\n\\n        return string\\n\\n\\n\\nclass Object(Type):\\n    FEATURE__DYNAMIC_CAST = (1 \\u003c\\u003c 1)\\n    FEATURE__LIST         = (1 \\u003c\\u003c 2)\\n    FEATURE__DEEP_COPY    = (1 \\u003c\\u003c 3)\\n    FEATURE__ANY_TYPE     = (1 \\u003c\\u003c 4)\\n    FEATURE__SERIALIZE    = (1 \\u003c\\u003c 5)\\n    FEATURE__DESERIALIZE  = (1 \\u003c\\u003c 6)\\n\\n\\n    def __init__(self, name, extends, properties, features=0, extended_by=None):\\n        Type.__init__(self, \\"struct\\", name)\\n        self.extends = extends\\n        self.features = features\\n        self.properties = properties\\n        self.extended_by = extended_by\\n        self.candidate_for_dynamic_cast = False\\n\\n        if self.extended_by is not None:\\n            self.extended_by.sort()\\n\\n\\n    def generate_struct_members(self, add_banner=False, struct_gap=False):\\n        members = \\"\\"\\n\\n        if struct_gap:\\n            members += \\"\\\\n\\"\\n\\n        if self.extends is not None:\\n            members += objects_by_name[self.extends] \\\\\\n                       .generate_struct_members(add_banner=True,\\n                                                struct_gap=False) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            members += \\"    /* %s */\\\\n\\" % self.name\\n\\n        for property in self.properties:\\n            members += property.generate_struct_member()\\n\\n        if len(self.properties) \\u003c 1:\\n            members += \\"    /* no properties */\\\\n\\"\\n\\n        return members\\n\\n\\n    def generate_free_code(self, add_banner=False):\\n        source = \\"\\"\\n\\n        if self.extends is not None:\\n            source += objects_by_name[self.extends] \\\\\\n                      .generate_free_code(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n        if len(self.properties) \\u003c 1:\\n            source += \\"    /* no properties */\\\\n\\"\\n        else:\\n            string = \\"\\"\\n\\n            for property in self.properties:\\n                string += property.generate_free_code()\\n\\n            if len(string) \\u003c 1:\\n                source += \\"    /* no properties to be freed */\\\\n\\"\\n            else:\\n                source += string\\n\\n        return source\\n\\n\\n    def generate_validate_code(self, add_banner=False):\\n        source = \\"\\"\\n\\n        if self.extends is not None:\\n            source += objects_by_name[self.extends] \\\\\\n                      .generate_validate_code(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n        if len(self.properties) \\u003c 1:\\n            source += \\"    /* no properties */\\\\n\\"\\n        else:\\n            string = \\"\\"\\n\\n            for property in self.properties:\\n                string += property.generate_validate_code()\\n\\n            if len(string) \\u003c 1:\\n                source += \\"    /* no required properties */\\\\n\\"\\n            else:\\n                source += string\\n\\n        return source\\n\\n\\n    def generate_dynamic_cast_code(self, is_first=True):\\n        source = \\"\\"\\n\\n        if self.extended_by is not None:\\n            if not is_first:\\n                source += \\"\\\\n\\"\\n\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n            for extended_by in self.extended_by:\\n                source += \\"    ESX_VI__TEMPLATE__DYNAMIC_CAST__ACCEPT(%s)\\\\n\\" \\\\\\n                          % extended_by\\n\\n            for extended_by in self.extended_by:\\n                source += objects_by_name[extended_by] \\\\\\n                          .generate_dynamic_cast_code(False)\\n\\n        return source\\n\\n\\n    def generate_deep_copy_code(self, add_banner = False):\\n        source = \\"\\"\\n\\n        if self.extends is not None:\\n            source += objects_by_name[self.extends] \\\\\\n                      .generate_deep_copy_code(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n        if len(self.properties) \\u003c 1:\\n            source += \\"    /* no properties */\\\\n\\"\\n        else:\\n            string = \\"\\"\\n\\n            for property in self.properties:\\n                string += property.generate_deep_copy_code()\\n\\n            if len(string) \\u003c 1:\\n                source += \\"    /* no properties to be deep copied */\\\\n\\"\\n            else:\\n                source += string\\n\\n        return source\\n\\n\\n    def generate_serialize_code(self, add_banner=False):\\n        source = \\"\\"\\n\\n        if self.extends is not None:\\n            source += objects_by_name[self.extends] \\\\\\n                      .generate_serialize_code(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n        if len(self.properties) \\u003c 1:\\n            source += \\"    /* no properties */\\\\n\\"\\n        else:\\n            for property in self.properties:\\n                source += property.generate_serialize_code()\\n\\n        return source\\n\\n\\n    def generate_deserialize_code(self, add_banner=False):\\n        source = \\"\\"\\n\\n        if self.extends is not None:\\n            source += objects_by_name[self.extends] \\\\\\n                      .generate_deserialize_code(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n        if len(self.properties) \\u003c 1:\\n            source += \\"    /* no properties */\\\\n\\"\\n        else:\\n            for property in self.properties:\\n                source += property.generate_deserialize_code()\\n\\n        return source\\n\\n\\n    def generate_header(self):\\n        header = separator\\n        header += \\" * VI Object: %s\\\\n\\" % self.name\\n\\n        if self.extends is not None:\\n            header += \\" *            extends %s\\\\n\\" % self.extends\\n\\n        first = True\\n\\n        if self.extended_by is not None:\\n            for extended_by in self.extended_by:\\n                if first:\\n                    header += \\" *            extended by %s\\\\n\\" % extended_by\\n                    first = False\\n                else:\\n                    header += \\" *                        %s\\\\n\\" % extended_by\\n\\n        header += \\" */\\\\n\\\\n\\"\\n\\n        # struct\\n        header += \\"struct _esxVI_%s {\\\\n\\" % self.name\\n\\n        if self.features \\u0026 Object.FEATURE__LIST:\\n            header += aligned(\\"    esxVI_%s *_next; \\" % self.name,\\n                              \\"/* optional */\\\\n\\")\\n        else:\\n            header += aligned(\\"    esxVI_%s *_unused; \\" % self.name,\\n                              \\"/* optional */\\\\n\\")\\n\\n        header += aligned(\\"    esxVI_Type _type; \\", \\"/* required */\\\\n\\")\\n        header += self.generate_struct_members(struct_gap=True)\\n        header += \\"};\\\\n\\\\n\\"\\n\\n        # functions\\n        header += \\"int esxVI_%s_Alloc(esxVI_%s **item);\\\\n\\" \\\\\\n                  % (self.name, self.name)\\n        header += \\"void esxVI_%s_Free(esxVI_%s **item);\\\\n\\" \\\\\\n                  % (self.name, self.name)\\n        header += \\"int esxVI_%s_Validate(esxVI_%s *item);\\\\n\\" \\\\\\n                  % (self.name, self.name)\\n\\n        if self.features \\u0026 Object.FEATURE__DYNAMIC_CAST:\\n            if self.extended_by is not None or self.extends is not None:\\n                header += \\"esxVI_%s *esxVI_%s_DynamicCast(void *item);\\\\n\\" \\\\\\n                          % (self.name, self.name)\\n            else:\\n                report_error(\\"cannot add dynamic cast support for an untyped object\\")\\n\\n        if self.features \\u0026 Object.FEATURE__LIST:\\n            header += \\"int esxVI_%s_AppendToList(esxVI_%s **list, esxVI_%s *item);\\\\n\\" \\\\\\n                      % (self.name, self.name, self.name)\\n\\n        if self.features \\u0026 Object.FEATURE__DEEP_COPY:\\n            header += \\"int esxVI_%s_DeepCopy(esxVI_%s **dst, esxVI_%s *src);\\\\n\\" \\\\\\n                      % (self.name, self.name, self.name)\\n\\n            if self.features \\u0026 Object.FEATURE__LIST:\\n                header += (\\"int esxVI_%s_DeepCopyList(esxVI_%s **dstList, \\"\\n                                                     \\"esxVI_%s *srcList);\\\\n\\") \\\\\\n                          % (self.name, self.name, self.name)\\n\\n        if self.features \\u0026 Object.FEATURE__ANY_TYPE:\\n            header += (\\"int esxVI_%s_CastFromAnyType(esxVI_AnyType *anyType, \\"\\n                                                    \\"esxVI_%s **item);\\\\n\\") \\\\\\n                      % (self.name, self.name)\\n\\n            if self.features \\u0026 Object.FEATURE__LIST:\\n                header += (\\"int esxVI_%s_CastListFromAnyType(esxVI_AnyType *anyType, \\"\\n                                                            \\"esxVI_%s **list);\\\\n\\") \\\\\\n                          % (self.name, self.name)\\n\\n        if self.features \\u0026 Object.FEATURE__SERIALIZE:\\n            header += (\\"int esxVI_%s_Serialize(esxVI_%s *item, \\"\\n                                              \\"const char *element, \\"\\n                                              \\"virBufferPtr output);\\\\n\\") \\\\\\n                      % (self.name, self.name)\\n\\n            if self.features \\u0026 Object.FEATURE__LIST:\\n                header += (\\"int esxVI_%s_SerializeList(esxVI_%s *list, \\"\\n                                                      \\"const char *element, \\"\\n                                                      \\"virBufferPtr output);\\\\n\\") \\\\\\n                          % (self.name, self.name)\\n\\n        if self.features \\u0026 Object.FEATURE__DESERIALIZE:\\n            header += \\"int esxVI_%s_Deserialize(xmlNodePtr node, esxVI_%s **item);\\\\n\\" \\\\\\n                      % (self.name, self.name)\\n\\n            if self.features \\u0026 Object.FEATURE__LIST:\\n                header += (\\"int esxVI_%s_DeserializeList(xmlNodePtr node, \\"\\n                                                        \\"esxVI_%s **list);\\\\n\\") \\\\\\n                          % (self.name, self.name)\\n\\n        header += \\"\\\\n\\\\n\\\\n\\"\\n\\n        return header\\n\\n\\n    def generate_source(self):\\n        source = separator\\n        source += \\" * VI Object: %s\\\\n\\" % self.name\\n\\n        if self.extends is not None:\\n            source += \\" *            extends %s\\\\n\\" % self.extends\\n\\n        first = True\\n\\n        if self.extended_by is not None:\\n            for extended_by in self.extended_by:\\n                if first:\\n                    source += \\" *            extended by %s\\\\n\\" % extended_by\\n                    first = False\\n                else:\\n                    source += \\" *                        %s\\\\n\\" % extended_by\\n\\n        source += \\" */\\\\n\\\\n\\"\\n\\n        # functions\\n        source += \\"/* esxVI_%s_Alloc */\\\\n\\" % self.name\\n        source += \\"ESX_VI__TEMPLATE__ALLOC(%s)\\\\n\\\\n\\" % self.name\\n\\n        # free\\n        if self.extended_by is None:\\n            source += \\"/* esxVI_%s_Free */\\\\n\\" % self.name\\n            source += \\"ESX_VI__TEMPLATE__FREE(%s,\\\\n\\" % self.name\\n            source += \\"{\\\\n\\"\\n\\n            if self.features \\u0026 Object.FEATURE__LIST:\\n                if self.extends is not None:\\n                    # avoid \\"dereferencing type-punned pointer will break\\n                    # strict-aliasing rules\\" warnings\\n                    source += \\"    esxVI_%s *next = (esxVI_%s *)item-\\u003e_next;\\\\n\\\\n\\" \\\\\\n                              % (self.extends, self.extends)\\n                    source += \\"    esxVI_%s_Free(\\u0026next);\\\\n\\" % self.extends\\n                    source += \\"    item-\\u003e_next = (esxVI_%s *)next;\\\\n\\\\n\\" % self.name\\n                else:\\n                    source += \\"    esxVI_%s_Free(\\u0026item-\\u003e_next);\\\\n\\\\n\\" % self.name\\n\\n            source += self.generate_free_code()\\n\\n            source += \\"})\\\\n\\\\n\\"\\n        else:\\n            source += \\"/* esxVI_%s_Free */\\\\n\\" % self.name\\n            source += \\"ESX_VI__TEMPLATE__DYNAMIC_FREE(%s,\\\\n\\" % self.name\\n            source += \\"{\\\\n\\"\\n\\n            for extended_by in self.extended_by:\\n                source += \\"    ESX_VI__TEMPLATE__DISPATCH__FREE(%s)\\\\n\\" \\\\\\n                          % extended_by\\n\\n            source += \\"},\\\\n\\"\\n            source += \\"{\\\\n\\"\\n\\n            if self.features \\u0026 Object.FEATURE__LIST:\\n                if self.extends is not None:\\n                    # avoid \\"dereferencing type-punned pointer will brea\\n                    # strict-aliasing rules\\" warnings\\n                    source += \\"    esxVI_%s *next = (esxVI_%s *)item-\\u003e_next;\\\\n\\\\n\\" \\\\\\n                              % (self.extends, self.extends)\\n                    source += \\"    esxVI_%s_Free(\\u0026next);\\\\n\\" % self.extends\\n                    source += \\"    item-\\u003e_next = (esxVI_%s *)next;\\\\n\\\\n\\" % self.name\\n                else:\\n                    source += \\"    esxVI_%s_Free(\\u0026item-\\u003e_next);\\\\n\\\\n\\" % self.name\\n\\n            source += self.generate_free_code()\\n\\n            source += \\"})\\\\n\\\\n\\"\\n\\n        # validate\\n        source += \\"/* esxVI_%s_Validate */\\\\n\\" % self.name\\n        source += \\"ESX_VI__TEMPLATE__VALIDATE(%s,\\\\n\\" % self.name\\n        source += \\"{\\\\n\\"\\n\\n        source += self.generate_validate_code()\\n\\n        source += \\"})\\\\n\\\\n\\"\\n\\n        # dynamic cast\\n        if self.features \\u0026 Object.FEATURE__DYNAMIC_CAST:\\n            if self.extended_by is not None or self.extends is not None:\\n                source += \\"/* esxVI_%s_DynamicCast */\\\\n\\" % self.name\\n                source += \\"ESX_VI__TEMPLATE__DYNAMIC_CAST(%s,\\\\n\\" % self.name\\n                source += \\"{\\\\n\\"\\n\\n                source += self.generate_dynamic_cast_code()\\n\\n                source += \\"})\\\\n\\\\n\\"\\n            else:\\n                report_error(\\"cannot add dynamic cast support for an untyped object\\")\\n\\n        # append to list\\n        if self.features \\u0026 Object.FEATURE__LIST:\\n            source += \\"/* esxVI_%s_AppendToList */\\\\n\\" % self.name\\n            source += \\"ESX_VI__TEMPLATE__LIST__APPEND(%s)\\\\n\\\\n\\" % self.name\\n\\n        # deep copy\\n        if self.extended_by is None:\\n            if self.features \\u0026 Object.FEATURE__DEEP_COPY:\\n                source += \\"/* esxVI_%s_DeepCopy */\\\\n\\" % self.name\\n                source += \\"ESX_VI__TEMPLATE__DEEP_COPY(%s,\\\\n\\" % self.name\\n                source += \\"{\\\\n\\"\\n\\n                source += self.generate_deep_copy_code()\\n\\n                source += \\"})\\\\n\\\\n\\"\\n\\n                if self.features \\u0026 Object.FEATURE__LIST:\\n                    source += \\"/* esxVI_%s_DeepCopyList */\\\\n\\" % self.name\\n                    source += \\"ESX_VI__TEMPLATE__LIST__DEEP_COPY(%s)\\\\n\\\\n\\" \\\\\\n                              % self.name\\n        else:\\n            if self.features \\u0026 Object.FEATURE__DEEP_COPY:\\n                source += \\"/* esxVI_%s_DeepCopy */\\\\n\\" % self.name\\n                source += \\"ESX_VI__TEMPLATE__DYNAMIC_DEEP_COPY(%s)\\\\n\\" % self.name\\n                source += \\"{\\\\n\\"\\n\\n                for extended_by in self.extended_by:\\n                    source += \\"    ESX_VI__TEMPLATE__DISPATCH__DEEP_COPY(%s)\\\\n\\" \\\\\\n                              % extended_by\\n\\n                source += \\"},\\\\n\\"\\n                source += \\"{\\\\n\\"\\n\\n                source += self.generate_deep_copy_code()\\n\\n                source += \\"})\\\\n\\\\n\\"\\n\\n                if self.features \\u0026 Object.FEATURE__LIST:\\n                    source += \\"/* esxVI_%s_DeepCopyList */\\\\n\\" % self.name\\n                    source += \\"ESX_VI__TEMPLATE__LIST__DEEP_COPY(%s)\\\\n\\\\n\\" \\\\\\n                              % self.name\\n\\n        # cast from any type\\n        if self.features \\u0026 Object.FEATURE__ANY_TYPE:\\n            source += \\"/* esxVI_%s_CastFromAnyType */\\\\n\\" % self.name\\n\\n            if self.extended_by is None:\\n                source += \\"ESX_VI__TEMPLATE__CAST_FROM_ANY_TYPE(%s)\\\\n\\\\n\\" \\\\\\n                          % self.name\\n            else:\\n                source += \\"ESX_VI__TEMPLATE__DYNAMIC_CAST_FROM_ANY_TYPE(%s,\\\\n\\" \\\\\\n                          % self.name\\n                source += \\"{\\\\n\\"\\n\\n                for extended_by in self.extended_by:\\n                    source += \\"    ESX_VI__TEMPLATE__DISPATCH__CAST_FROM_ANY_TYPE(%s)\\\\n\\" \\\\\\n                              % extended_by\\n\\n                source += \\"})\\\\n\\\\n\\"\\n\\n            if self.features \\u0026 Object.FEATURE__LIST:\\n                source += \\"/* esxVI_%s_CastListFromAnyType */\\\\n\\" % self.name\\n                source += \\"ESX_VI__TEMPLATE__LIST__CAST_FROM_ANY_TYPE(%s)\\\\n\\\\n\\" \\\\\\n                          % self.name\\n\\n        # serialize\\n        if self.extended_by is None:\\n            if self.features \\u0026 Object.FEATURE__SERIALIZE:\\n                source += \\"/* esxVI_%s_Serialize */\\\\n\\" % self.name\\n                source += \\"ESX_VI__TEMPLATE__SERIALIZE(%s,\\\\n\\" % self.name\\n                source += \\"{\\\\n\\"\\n\\n                source += self.generate_serialize_code()\\n\\n                source += \\"})\\\\n\\\\n\\"\\n\\n                if self.features \\u0026 Object.FEATURE__LIST:\\n                    source += \\"/* esxVI_%s_SerializeList */\\\\n\\" % self.name\\n                    source += \\"ESX_VI__TEMPLATE__LIST__SERIALIZE(%s)\\\\n\\\\n\\" \\\\\\n                              % self.name\\n        else:\\n            if self.features \\u0026 Object.FEATURE__SERIALIZE:\\n                source += \\"/* esxVI_%s_Serialize */\\\\n\\" % self.name\\n                source += \\"ESX_VI__TEMPLATE__DYNAMIC_SERIALIZE(%s,\\\\n\\" % self.name\\n                source += \\"{\\\\n\\"\\n\\n                for extended_by in self.extended_by:\\n                    source += \\"    ESX_VI__TEMPLATE__DISPATCH__SERIALIZE(%s)\\\\n\\" \\\\\\n                              % extended_by\\n\\n                source += \\"},\\\\n\\"\\n                source += \\"{\\\\n\\"\\n\\n                source += self.generate_serialize_code()\\n\\n                source += \\"})\\\\n\\\\n\\"\\n\\n                if self.features \\u0026 Object.FEATURE__LIST:\\n                    source += \\"/* esxVI_%s_SerializeList */\\\\n\\" % self.name\\n                    source += \\"ESX_VI__TEMPLATE__LIST__SERIALIZE(%s)\\\\n\\\\n\\" \\\\\\n                              % self.name\\n\\n        # deserialize\\n        if self.extended_by is None:\\n            if self.features \\u0026 Object.FEATURE__DESERIALIZE:\\n                source += \\"/* esxVI_%s_Deserialize */\\\\n\\" % self.name\\n                source += \\"ESX_VI__TEMPLATE__DESERIALIZE(%s,\\\\n\\" % self.name\\n                source += \\"{\\\\n\\"\\n\\n                source += self.generate_deserialize_code()\\n\\n                source += \\"})\\\\n\\\\n\\"\\n\\n                if self.features \\u0026 Object.FEATURE__LIST:\\n                    source += \\"/* esxVI_%s_DeserializeList */\\\\n\\" % self.name\\n                    source += \\"ESX_VI__TEMPLATE__LIST__DESERIALIZE(%s)\\\\n\\\\n\\" \\\\\\n                              % self.name\\n        else:\\n            if self.features \\u0026 Object.FEATURE__DESERIALIZE:\\n                source += \\"/* esxVI_%s_Deserialize */\\\\n\\" % self.name\\n                source += \\"ESX_VI__TEMPLATE__DYNAMIC_DESERIALIZE(%s,\\\\n\\" \\\\\\n                          % self.name\\n                source += \\"{\\\\n\\"\\n\\n                for extended_by in self.extended_by:\\n                    source += \\"    ESX_VI__TEMPLATE__DISPATCH__DESERIALIZE(%s)\\\\n\\" \\\\\\n                              % extended_by\\n\\n                source += \\"},\\\\n\\"\\n                source += \\"{\\\\n\\"\\n\\n                source += self.generate_deserialize_code()\\n\\n                source += \\"})\\\\n\\\\n\\"\\n\\n                if self.features \\u0026 Object.FEATURE__LIST:\\n                    source += \\"/* esxVI_%s_DeserializeList */\\\\n\\" % self.name\\n                    source += \\"ESX_VI__TEMPLATE__LIST__DESERIALIZE(%s)\\\\n\\\\n\\" \\\\\\n                              % self.name\\n\\n        source += \\"\\\\n\\\\n\\"\\n\\n        return source\\n\\n\\n\\nclass ManagedObject(Type):\\n    FEATURE__LIST = (1 \\u003c\\u003c 2)\\n\\n\\n    def __init__(self, name, extends, properties, features=0, extended_by=None):\\n        Type.__init__(self, \\"struct\\", name)\\n        self.extends = extends\\n        self.features = features\\n        self.properties = properties\\n        self.extended_by = extended_by\\n\\n        if self.extended_by is not None:\\n            self.extended_by.sort()\\n\\n\\n    def generate_struct_members(self, add_banner=False, struct_gap=False):\\n        members = \\"\\"\\n\\n        if struct_gap:\\n            members += \\"\\\\n\\"\\n\\n        if self.extends is not None:\\n            members += managed_objects_by_name[self.extends] \\\\\\n                       .generate_struct_members(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            members += \\"    /* %s */\\\\n\\" % self.name\\n\\n        for property in self.properties:\\n            members += property.generate_struct_member()\\n\\n        if len(self.properties) \\u003c 1:\\n            members += \\"    /* no properties */\\\\n\\"\\n\\n        return members\\n\\n\\n    def generate_free_code(self, add_banner=False):\\n        source = \\"\\"\\n\\n        if self.extends is not None:\\n            source += managed_objects_by_name[self.extends] \\\\\\n                      .generate_free_code(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n        if len(self.properties) \\u003c 1:\\n            source += \\"    /* no properties */\\\\n\\"\\n        else:\\n            string = \\"\\"\\n\\n            for property in self.properties:\\n                string += property.generate_free_code()\\n\\n            if len(string) \\u003c 1:\\n                source += \\"    /* no properties to be freed */\\\\n\\"\\n            else:\\n                source += string\\n\\n        return source\\n\\n\\n    def generate_validate_code(self, add_banner=False):\\n        source = \\"\\"\\n\\n        if self.extends is not None:\\n            source += managed_objects_by_name[self.extends] \\\\\\n                      .generate_validate_code(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n        if len(self.properties) \\u003c 1:\\n            source += \\"    /* no properties */\\\\n\\"\\n        else:\\n            string = \\"\\"\\n\\n            for property in self.properties:\\n                string += property.generate_validate_code(managed=True)\\n\\n            if len(string) \\u003c 1:\\n                source += \\"    /* no required properties */\\\\n\\"\\n            else:\\n                source += string\\n\\n        return source\\n\\n\\n    def generate_lookup_code1(self, add_banner=False):\\n        source = \\"\\"\\n\\n        if self.extends is not None:\\n            source += managed_objects_by_name[self.extends] \\\\\\n                      .generate_lookup_code1(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n        if len(self.properties) \\u003c 1:\\n            source += \\"    /* no properties */\\\\n\\"\\n        else:\\n            string = \\"\\"\\n\\n            for property in self.properties:\\n                string += \\"    \\\\\\"%s\\\\\\\\0\\\\\\"\\\\n\\" % property.name\\n\\n            if len(string) \\u003c 1:\\n                source += \\"    /* no properties */\\\\n\\"\\n            else:\\n                source += string\\n\\n        return source\\n\\n\\n    def generate_lookup_code2(self, add_banner=False):\\n        source = \\"\\"\\n\\n        if self.extends is not None:\\n            source += managed_objects_by_name[self.extends] \\\\\\n                      .generate_lookup_code2(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n        if len(self.properties) \\u003c 1:\\n            source += \\"    /* no properties */\\\\n\\"\\n        else:\\n            string = \\"\\"\\n\\n            for property in self.properties:\\n                string += property.generate_lookup_code()\\n\\n            if len(string) \\u003c 1:\\n                source += \\"    /* no properties */\\\\n\\"\\n            else:\\n                source += string\\n\\n        return source\\n\\n\\n    def generate_comment(self):\\n        comment = separator\\n        comment += \\" * VI Managed Object: %s\\\\n\\" % self.name\\n\\n        if self.extends is not None:\\n            comment += \\" *                    extends %s\\\\n\\" % self.extends\\n\\n        first = True\\n\\n        if self.extended_by is not None:\\n            for extended_by in self.extended_by:\\n                if first:\\n                    comment += \\" *                    extended by %s\\\\n\\" \\\\\\n                               % extended_by\\n                    first = False\\n                else:\\n                    comment += \\" *                                %s\\\\n\\" \\\\\\n                               % extended_by\\n\\n        comment += \\" */\\\\n\\\\n\\"\\n\\n        return comment\\n\\n\\n    def generate_header(self):\\n        header = self.generate_comment()\\n\\n        # struct\\n        header += \\"struct _esxVI_%s {\\\\n\\" % self.name\\n\\n        if self.features \\u0026 Object.FEATURE__LIST:\\n            header += aligned(\\"    esxVI_%s *_next; \\" % self.name,\\n                              \\"/* optional */\\\\n\\")\\n        else:\\n            header += aligned(\\"    esxVI_%s *_unused; \\" % self.name,\\n                              \\"/* optional */\\\\n\\")\\n\\n        header += aligned(\\"    esxVI_Type _type; \\", \\"/* required */\\\\n\\")\\n        header += aligned(\\"    esxVI_ManagedObjectReference *_reference; \\",\\n                          \\"/* required */\\\\n\\")\\n        header += \\"\\\\n\\"\\n        header += self.generate_struct_members()\\n\\n        header += \\"};\\\\n\\\\n\\"\\n\\n        # functions\\n        header += \\"int esxVI_%s_Alloc(esxVI_%s **item);\\\\n\\" % (self.name, self.name)\\n        header += \\"void esxVI_%s_Free(esxVI_%s **item);\\\\n\\" % (self.name, self.name)\\n        header += (\\"int esxVI_%s_Validate(esxVI_%s *item, \\"\\n                                         \\"esxVI_String *selectedPropertyNameList);\\\\n\\") \\\\\\n                  % (self.name, self.name)\\n\\n        if self.features \\u0026 Object.FEATURE__LIST:\\n            header += \\"int esxVI_%s_AppendToList(esxVI_%s **list, esxVI_%s *item);\\\\n\\" \\\\\\n                      % (self.name, self.name, self.name)\\n\\n        header += \\"\\\\n\\\\n\\\\n\\"\\n\\n        return header\\n\\n\\n    def generate_helper_header(self):\\n        header = \\"\\"\\n\\n        # functions\\n        header += (\\"int esxVI_Lookup%s(esxVI_Context *ctx, \\"\\n                                      \\"const char *name, \\"\\n                                      \\"esxVI_ManagedObjectReference *root, \\"\\n                                      \\"esxVI_String *selectedPropertyNameList, \\"\\n                                      \\"esxVI_%s **item, \\"\\n                                      \\"esxVI_Occurrence occurrence);\\\\n\\") \\\\\\n                  % (self.name, self.name)\\n\\n        header += \\"\\\\n\\"\\n\\n        return header\\n\\n\\n    def generate_source(self):\\n        source = self.generate_comment()\\n\\n        # functions\\n        source += \\"/* esxVI_%s_Alloc */\\\\n\\" % self.name\\n        source += \\"ESX_VI__TEMPLATE__ALLOC(%s)\\\\n\\\\n\\" % self.name\\n\\n        # free\\n        if self.extended_by is None:\\n            source += \\"/* esxVI_%s_Free */\\\\n\\" % self.name\\n            source += \\"ESX_VI__TEMPLATE__FREE(%s,\\\\n\\" % self.name\\n            source += \\"{\\\\n\\"\\n\\n            if self.features \\u0026 ManagedObject.FEATURE__LIST:\\n                if self.extends is not None:\\n                    # avoid \\"dereferencing type-punned pointer will break\\n                    # strict-aliasing rules\\" warnings\\n                    source += \\"    esxVI_%s *next = (esxVI_%s *)item-\\u003e_next;\\\\n\\\\n\\" \\\\\\n                              % (self.extends, self.extends)\\n                    source += \\"    esxVI_%s_Free(\\u0026next);\\\\n\\" % self.extends\\n                    source += \\"    item-\\u003e_next = (esxVI_%s *)next;\\\\n\\\\n\\" % self.name\\n                else:\\n                    source += \\"    esxVI_%s_Free(\\u0026item-\\u003e_next);\\\\n\\" % self.name\\n\\n            source += \\"    esxVI_ManagedObjectReference_Free(\\u0026item-\\u003e_reference);\\\\n\\\\n\\"\\n\\n            source += self.generate_free_code()\\n\\n            source += \\"})\\\\n\\\\n\\"\\n        else:\\n            source += \\"/* esxVI_%s_Free */\\\\n\\" % self.name\\n            source += \\"ESX_VI__TEMPLATE__DYNAMIC_FREE(%s,\\\\n\\" % self.name\\n            source += \\"{\\\\n\\"\\n\\n            for extended_by in self.extended_by:\\n                source += \\"    ESX_VI__TEMPLATE__DISPATCH__FREE(%s)\\\\n\\" % extended_by\\n\\n            source += \\"},\\\\n\\"\\n            source += \\"{\\\\n\\"\\n\\n            if self.features \\u0026 Object.FEATURE__LIST:\\n                if self.extends is not None:\\n                    # avoid \\"dereferencing type-punned pointer will break\\n                    # strict-aliasing rules\\" warnings\\n                    source += \\"    esxVI_%s *next = (esxVI_%s *)item-\\u003e_next;\\\\n\\\\n\\" \\\\\\n                              % (self.extends, self.extends)\\n                    source += \\"    esxVI_%s_Free(\\u0026next);\\\\n\\" % self.extends\\n                    source += \\"    item-\\u003e_next = (esxVI_%s *)next;\\\\n\\\\n\\" % self.name\\n                else:\\n                    source += \\"    esxVI_%s_Free(\\u0026item-\\u003e_next);\\\\n\\" % self.name\\n\\n            source += \\"    esxVI_ManagedObjectReference_Free(\\u0026item-\\u003e_reference);\\\\n\\\\n\\"\\n\\n            source += self.generate_free_code()\\n\\n            source += \\"})\\\\n\\\\n\\"\\n\\n        # validate\\n        source += \\"/* esxVI_%s_Validate */\\\\n\\" % self.name\\n        source += \\"ESX_VI__TEMPLATE__MANAGED_VALIDATE(%s,\\\\n\\" % self.name\\n        source += \\"{\\\\n\\"\\n\\n        source += self.generate_validate_code()\\n\\n        source += \\"})\\\\n\\\\n\\"\\n\\n        # append to list\\n        if self.features \\u0026 ManagedObject.FEATURE__LIST:\\n            source += \\"/* esxVI_%s_AppendToList */\\\\n\\" % self.name\\n            source += \\"ESX_VI__TEMPLATE__LIST__APPEND(%s)\\\\n\\\\n\\" % self.name\\n\\n        source += \\"\\\\n\\\\n\\"\\n\\n        return source\\n\\n\\n    def generate_helper_source(self):\\n        source = \\"\\"\\n\\n        # lookup\\n        source += \\"/* esxVI_Lookup%s */\\\\n\\" % self.name\\n        source += \\"ESX_VI__TEMPLATE__LOOKUP(%s,\\\\n\\" % self.name\\n        source += \\"{\\\\n\\"\\n\\n        source += self.generate_lookup_code1()\\n\\n        source += \\"},\\\\n\\"\\n        source += \\"{\\\\n\\"\\n\\n        source += self.generate_lookup_code2()\\n\\n        source += \\"})\\\\n\\\\n\\"\\n\\n        source += \\"\\\\n\\\\n\\"\\n\\n        return source\\n\\n\\n\\nclass Enum(Type):\\n    FEATURE__ANY_TYPE = (1 \\u003c\\u003c 1)\\n    FEATURE__SERIALIZE = (1 \\u003c\\u003c 2)\\n    FEATURE__DESERIALIZE = (1 \\u003c\\u003c 3)\\n\\n\\n    def __init__(self, name, values, features=0):\\n        Type.__init__(self, \\"enum\\", name)\\n        self.values = values\\n        self.features = features\\n\\n\\n    def generate_header(self):\\n        header = separator\\n        header += \\" * VI Enum: %s\\\\n\\" % self.name\\n        header += \\" */\\\\n\\\\n\\"\\n\\n        # enum\\n        header += \\"enum _esxVI_%s {\\\\n\\" % self.name\\n        header += \\"    esxVI_%s_Undefined = 0,\\\\n\\" % self.name\\n\\n        for value in self.values:\\n            header += \\"    esxVI_%s_%s,\\\\n\\" % (self.name, capitalize_first(value))\\n\\n        header += \\"};\\\\n\\\\n\\"\\n\\n        # functions\\n        if self.features \\u0026 Enum.FEATURE__ANY_TYPE:\\n            header += (\\"int esxVI_%s_CastFromAnyType(esxVI_AnyType *anyType, \\"\\n                                                    \\"esxVI_%s *item);\\\\n\\") \\\\\\n                      % (self.name, self.name)\\n\\n        if self.features \\u0026 Enum.FEATURE__SERIALIZE:\\n            header += (\\"int esxVI_%s_Serialize(esxVI_%s item, const char *element, \\"\\n                                              \\"virBufferPtr output);\\\\n\\") \\\\\\n                      % (self.name, self.name)\\n\\n        if self.features \\u0026 Enum.FEATURE__DESERIALIZE:\\n            header += (\\"int esxVI_%s_Deserialize(xmlNodePtr node, \\"\\n                                                \\"esxVI_%s *item);\\\\n\\") \\\\\\n                      % (self.name, self.name)\\n\\n        header += \\"\\\\n\\\\n\\\\n\\"\\n\\n        return header\\n\\n\\n    def generate_source(self):\\n        source = separator\\n        source += \\" * VI Enum: %s\\\\n\\" % self.name\\n        source += \\" */\\\\n\\\\n\\"\\n\\n        source += \\"static const esxVI_Enumeration _esxVI_%s_Enumeration = {\\\\n\\" \\\\\\n                  % self.name\\n        source += \\"    esxVI_Type_%s, {\\\\n\\" % self.name\\n\\n        for value in self.values:\\n            source += \\"        { \\\\\\"%s\\\\\\", esxVI_%s_%s },\\\\n\\" \\\\\\n                      % (value, self.name, capitalize_first(value))\\n\\n        source += \\"        { NULL, -1 },\\\\n\\"\\n        source += \\"    },\\\\n\\"\\n        source += \\"};\\\\n\\\\n\\"\\n\\n        # functions\\n        if self.features \\u0026 Enum.FEATURE__ANY_TYPE:\\n            source += \\"/* esxVI_%s_CastFromAnyType */\\\\n\\" % self.name\\n            source += \\"ESX_VI__TEMPLATE__ENUMERATION__CAST_FROM_ANY_TYPE(%s)\\\\n\\\\n\\" \\\\\\n                      % self.name\\n\\n        if self.features \\u0026 Enum.FEATURE__SERIALIZE:\\n            source += \\"/* esxVI_%s_Serialize */\\\\n\\" % self.name\\n            source += \\"ESX_VI__TEMPLATE__ENUMERATION__SERIALIZE(%s)\\\\n\\\\n\\" \\\\\\n                      % self.name\\n\\n        if self.features \\u0026 Enum.FEATURE__DESERIALIZE:\\n            source += \\"/* esxVI_%s_Deserialize */\\\\n\\" % self.name\\n            source += \\"ESX_VI__TEMPLATE__ENUMERATION__DESERIALIZE(%s)\\\\n\\\\n\\" \\\\\\n                      % self.name\\n\\n        source += \\"\\\\n\\\\n\\"\\n\\n        return source\\n\\n\\n\\ndef report_error(message):\\n    print \\"error: \\" + message\\n    sys.exit(1)\\n\\n\\n\\ndef capitalize_first(string):\\n    return string[:1].upper() + string[1:]\\n\\n\\n\\ndef parse_object(block):\\n    # expected format: [managed] object \\u003cname\\u003e [extends \\u003cname\\u003e]\\n    header_items = block[0][1].split()\\n    managed = False\\n\\n    if header_items[0] == \\"managed\\":\\n        managed = True\\n        del header_items[0]\\n\\n    if len(header_items) \\u003c 2:\\n        report_error(\\"line %d: invalid block header\\" % (number))\\n\\n    assert header_items[0] == \\"object\\"\\n\\n    name = header_items[1]\\n    extends = None\\n\\n    if len(header_items) \\u003e 2:\\n        if header_items[2] != \\"extends\\":\\n            report_error(\\"line %d: invalid block header\\" % (number))\\n        else:\\n            extends = header_items[3]\\n\\n    properties = []\\n\\n    for line in block[1:]:\\n        # expected format: \\u003ctype\\u003e \\u003cname\\u003e \\u003coccurrence\\u003e\\n        items = line[1].split()\\n\\n        if len(items) != 3:\\n            report_error(\\"line %d: invalid property\\" % line[0])\\n\\n        if items[2] not in valid_occurrences:\\n            report_error(\\"line %d: invalid occurrence\\" % line[0])\\n\\n        properties.append(Property(type=items[0], name=items[1],\\n                                   occurrence=items[2]))\\n\\n    if managed:\\n        return ManagedObject(name=name, extends=extends, properties=properties)\\n    else:\\n        return Object(name=name, extends=extends, properties=properties)\\n\\n\\n\\ndef parse_enum(block):\\n    # expected format: enum \\u003cname\\u003e\\n    header_items = block[0][1].split()\\n\\n    if len(header_items) \\u003c 2:\\n        report_error(\\"line %d: invalid block header\\" % (number))\\n\\n    assert header_items[0] == \\"enum\\"\\n\\n    name = header_items[1]\\n\\n    values = []\\n\\n    for line in block[1:]:\\n        # expected format: \\u003cvalue\\u003e\\n        values.append(line[1])\\n\\n    return Enum(name=name, values=values)\\n\\n\\n\\ndef parse_method(block):\\n    # expected format: method \\u003cname\\u003e [returns \\u003ctype\\u003e \\u003coccurrence\\u003e]\\n    header_items = block[0][1].split()\\n\\n    if len(header_items) \\u003c 2:\\n        report_error(\\"line %d: invalid block header\\" % (number))\\n\\n    assert header_items[0] == \\"method\\"\\n\\n    name = header_items[1]\\n    returns = None\\n\\n    if len(header_items) \\u003e 2:\\n        if header_items[2] != \\"returns\\":\\n            report_error(\\"line %d: invalid block header\\" % (number))\\n        else:\\n            returns = Parameter(type=header_items[3], name=\\"output\\",\\n                                occurrence=header_items[4])\\n\\n    parameters = []\\n\\n    for line in block[1:]:\\n        # expected format: \\u003ctype\\u003e \\u003cname\\u003e \\u003coccurrence\\u003e\\n        items = line[1].split()\\n\\n        if len(items) != 3:\\n            report_error(\\"line %d: invalid property\\" % line[0])\\n\\n        if items[2] not in valid_occurrences:\\n            report_error(\\"line %d: invalid occurrence\\" % line[0])\\n\\n        parameters.append(Parameter(type=items[0], name=items[1],\\n                                    occurrence=items[2]))\\n\\n    return Method(name=name, parameters=parameters, returns=returns)\\n\\n\\n\\ndef is_known_type(type):\\n    return type in predefined_objects or \\\\\\n           type in predefined_enums or \\\\\\n           type in objects_by_name or \\\\\\n           type in managed_objects_by_name or \\\\\\n           type in enums_by_name\\n\\n\\n\\ndef open_and_print(filename):\\n    if filename.startswith(\\"./\\"):\\n        print \\"  GEN    \\" + filename[2:]\\n    else:\\n        print \\"  GEN    \\" + filename\\n\\n    return open(filename, \\"wb\\")\\n\\n\\n\\npredefined_enums = [\\"Boolean\\"]\\n\\npredefined_objects = [\\"AnyType\\",\\n                      \\"Int\\",\\n                      \\"Long\\",\\n                      \\"String\\",\\n                      \\"DateTime\\",\\n                      \\"MethodFault\\",\\n                      \\"ManagedObjectReference\\"]\\n\\nadditional_enum_features = { \\"ManagedEntityStatus\\"      : Enum.FEATURE__ANY_TYPE,\\n                             \\"TaskInfoState\\"            : Enum.FEATURE__ANY_TYPE,\\n                             \\"VirtualMachinePowerState\\" : Enum.FEATURE__ANY_TYPE }\\n\\nadditional_object_features = { \\"AutoStartDefaults\\"          : Object.FEATURE__ANY_TYPE,\\n                               \\"AutoStartPowerInfo\\"         : Object.FEATURE__ANY_TYPE,\\n                               \\"DatastoreHostMount\\"         : Object.FEATURE__DEEP_COPY |\\n                                                              Object.FEATURE__LIST |\\n                                                              Object.FEATURE__ANY_TYPE,\\n                               \\"DatastoreInfo\\"              : Object.FEATURE__ANY_TYPE |\\n                                                              Object.FEATURE__DYNAMIC_CAST,\\n                               \\"HostConfigManager\\"          : Object.FEATURE__ANY_TYPE,\\n                               \\"HostCpuIdInfo\\"              : Object.FEATURE__LIST |\\n                                                              Object.FEATURE__ANY_TYPE,\\n                               \\"HostDatastoreBrowserSearchResults\\" : Object.FEATURE__LIST |\\n                                                              Object.FEATURE__ANY_TYPE,\\n                               \\"ManagedObjectReference\\"     : Object.FEATURE__ANY_TYPE,\\n                               \\"ObjectContent\\"              : Object.FEATURE__DEEP_COPY,\\n                               \\"ResourcePoolResourceUsage\\"  : Object.FEATURE__ANY_TYPE,\\n                               \\"ServiceContent\\"             : Object.FEATURE__DESERIALIZE,\\n                               \\"SharesInfo\\"                 : Object.FEATURE__ANY_TYPE,\\n                               \\"TaskInfo\\"                   : Object.FEATURE__LIST |\\n                                                              Object.FEATURE__ANY_TYPE,\\n                               \\"UserSession\\"                : Object.FEATURE__ANY_TYPE,\\n                               \\"VirtualMachineQuestionInfo\\" : Object.FEATURE__ANY_TYPE,\\n                               \\"VirtualMachineSnapshotTree\\" : Object.FEATURE__DEEP_COPY |\\n                                                              Object.FEATURE__ANY_TYPE,\\n                               \\"VmEventArgument\\"            : Object.FEATURE__DESERIALIZE }\\n\\nremoved_object_features = {}\\n\\n\\n\\nif \\"srcdir\\" in os.environ:\\n    input_filename = os.path.join(os.environ[\\"srcdir\\"], \\"esx/esx_vi_generator.input\\")\\n    output_dirname = os.path.join(os.environ[\\"srcdir\\"], \\"esx\\")\\nelse:\\n    input_filename = os.path.join(os.getcwd(), \\"esx_vi_generator.input\\")\\n    output_dirname = os.getcwd()\\n\\n\\n\\ntypes_typedef = open_and_print(os.path.join(output_dirname, \\"esx_vi_types.generated.typedef\\"))\\ntypes_typeenum = open_and_print(os.path.join(output_dirname, \\"esx_vi_types.generated.typeenum\\"))\\ntypes_typetostring = open_and_print(os.path.join(output_dirname, \\"esx_vi_types.generated.typetostring\\"))\\ntypes_typefromstring = open_and_print(os.path.join(output_dirname, \\"esx_vi_types.generated.typefromstring\\"))\\ntypes_header = open_and_print(os.path.join(output_dirname, \\"esx_vi_types.generated.h\\"))\\ntypes_source = open_and_print(os.path.join(output_dirname, \\"esx_vi_types.generated.c\\"))\\nmethods_header = open_and_print(os.path.join(output_dirname, \\"esx_vi_methods.generated.h\\"))\\nmethods_source = open_and_print(os.path.join(output_dirname, \\"esx_vi_methods.generated.c\\"))\\nmethods_macro = open_and_print(os.path.join(output_dirname, \\"esx_vi_methods.generated.macro\\"))\\nhelpers_header = open_and_print(os.path.join(output_dirname, \\"esx_vi.generated.h\\"))\\nhelpers_source = open_and_print(os.path.join(output_dirname, \\"esx_vi.generated.c\\"))\\n\\n\\n\\nnumber = 0\\nobjects_by_name = {}\\nmanaged_objects_by_name = {}\\nenums_by_name = {}\\nmethods_by_name = {}\\nblock = None\\n\\n\\n\\n# parse input file\\nfor line in file(input_filename, \\"rb\\").readlines():\\n    number += 1\\n\\n    if \\"#\\" in line:\\n        line = line[:line.index(\\"#\\")]\\n\\n    line = line.lstrip().rstrip()\\n\\n    if len(line) \\u003c 1:\\n        continue\\n\\n    if line.startswith(\\"object\\") or line.startswith(\\"managed object\\") or \\\\\\n       line.startswith(\\"enum\\") or line.startswith(\\"method\\"):\\n        if block is not None:\\n            report_error(\\"line %d: nested block found\\" % (number))\\n        else:\\n            block = []\\n\\n    if block is not None:\\n        if line == \\"end\\":\\n            if block[0][1].startswith(\\"object\\"):\\n                obj = parse_object(block)\\n                objects_by_name[obj.name] = obj\\n            elif block[0][1].startswith(\\"managed object\\"):\\n                obj = parse_object(block)\\n                managed_objects_by_name[obj.name] = obj\\n            elif block[0][1].startswith(\\"enum\\"):\\n                enum = parse_enum(block)\\n                enums_by_name[enum.name] = enum\\n            else:\\n                method = parse_method(block)\\n                methods_by_name[method.name] = method\\n\\n            block = None\\n        else:\\n            block.append((number, line))\\n\\n\\n\\nfor method in methods_by_name.values():\\n    # method parameter types must be serializable\\n    for parameter in method.parameters:\\n        if not parameter.is_type_generated():\\n            continue\\n\\n        if parameter.is_enum():\\n            enums_by_name[parameter.type].features |= Enum.FEATURE__SERIALIZE\\n        else:\\n            objects_by_name[parameter.type].features |= Object.FEATURE__SERIALIZE\\n            objects_by_name[parameter.type].candidate_for_dynamic_cast = True\\n\\n        # detect list usage\\n        if parameter.occurrence == OCCURRENCE__REQUIRED_LIST or \\\\\\n           parameter.occurrence == OCCURRENCE__OPTIONAL_LIST:\\n            if parameter.is_enum():\\n                report_error(\\"unsupported usage of enum \'%s\' as list in \'%s\'\\"\\n                             % (parameter.type, method.name))\\n            else:\\n                objects_by_name[parameter.type].features |= Object.FEATURE__LIST\\n\\n    # method return types must be deserializable\\n    if method.returns and method.returns.is_type_generated():\\n        if method.returns.is_enum():\\n            enums_by_name[method.returns.type].features |= Enum.FEATURE__DESERIALIZE\\n        else:\\n            objects_by_name[method.returns.type].features |= Object.FEATURE__DESERIALIZE\\n            objects_by_name[method.returns.type].candidate_for_dynamic_cast = True\\n\\n        # detect list usage\\n        if method.returns.occurrence == OCCURRENCE__REQUIRED_LIST or \\\\\\n           method.returns.occurrence == OCCURRENCE__OPTIONAL_LIST:\\n            if method.returns.is_enum():\\n                report_error(\\"unsupported usage of enum \'%s\' as list in \'%s\'\\"\\n                             % (method.returns.type, method.name))\\n            else:\\n                objects_by_name[method.returns.type].features |= Object.FEATURE__LIST\\n\\n\\n\\nfor enum in enums_by_name.values():\\n    # apply additional features\\n    if enum.name in additional_enum_features:\\n        enum.features |= additional_enum_features[enum.name]\\n\\n        if additional_enum_features[enum.name] \\u0026 Enum.FEATURE__ANY_TYPE:\\n            enum.features |= Enum.FEATURE__DESERIALIZE\\n\\n\\n\\nfor obj in objects_by_name.values():\\n    for property in obj.properties:\\n        if property.occurrence != OCCURRENCE__IGNORED and \\\\\\n           not is_known_type(property.type):\\n            report_error(\\"object \'%s\' contains unknown property type \'%s\'\\"\\n                         % (obj.name, property.type))\\n\\n    if obj.extends is not None:\\n        if not is_known_type(obj.extends):\\n            report_error(\\"object \'%s\' extends unknown object \'%s\'\\"\\n                         % (obj.name, obj.extends))\\n\\n    for property in obj.properties:\\n        if not property.is_type_generated():\\n            continue\\n\\n        if property.is_enum():\\n            enums_by_name[property.type].candidate_for_dynamic_cast = True\\n        else:\\n            objects_by_name[property.type].candidate_for_dynamic_cast = True\\n\\n        # detect list usage\\n        if property.occurrence == OCCURRENCE__REQUIRED_LIST or \\\\\\n           property.occurrence == OCCURRENCE__OPTIONAL_LIST:\\n            if property.is_enum():\\n                report_error(\\"unsupported usage of enum \'%s\' as list in \'%s\'\\"\\n                             % (property.type, obj.type))\\n            else:\\n                objects_by_name[property.type].features |= Object.FEATURE__LIST\\n\\n    # apply/remove additional features\\n    if obj.name in additional_object_features:\\n        obj.features |= additional_object_features[obj.name]\\n\\n        if additional_object_features[obj.name] \\u0026 Object.FEATURE__ANY_TYPE:\\n            obj.features |= Object.FEATURE__DESERIALIZE\\n\\n    if obj.name in removed_object_features:\\n        obj.features \\u0026= ~removed_object_features[obj.name]\\n\\n    # detect extended_by relation\\n    if obj.extends is not None:\\n        extended_obj = objects_by_name[obj.extends]\\n\\n        if extended_obj.extended_by is None:\\n            extended_obj.extended_by = [obj.name]\\n        else:\\n            extended_obj.extended_by.append(obj.name)\\n            extended_obj.extended_by.sort()\\n\\n\\n\\nfor obj in objects_by_name.values():\\n    # if an object is a candidate (it is used directly as parameter or return\\n    # type or is a member of another object) and it is extended by another\\n    # object then this type needs the dynamic cast feature\\n    if obj.candidate_for_dynamic_cast and obj.extended_by:\\n        obj.features |= Object.FEATURE__DYNAMIC_CAST\\n\\n\\n\\ndef propagate_feature(obj, feature):\\n    global features_have_changed\\n\\n    if not (obj.features \\u0026 feature):\\n        return\\n\\n    for property in obj.properties:\\n        if property.occurrence == OCCURRENCE__IGNORED or \\\\\\n           not property.is_type_generated():\\n            continue\\n\\n        if property.is_enum():\\n            if feature == Object.FEATURE__SERIALIZE and \\\\\\n               not (enums_by_name[property.type].features \\u0026 Enum.FEATURE__SERIALIZE):\\n                enums_by_name[property.type].features |= Enum.FEATURE__SERIALIZE\\n                features_have_changed = True\\n            elif feature == Object.FEATURE__DESERIALIZE and \\\\\\n               not (enums_by_name[property.type].features \\u0026 Enum.FEATURE__DESERIALIZE):\\n                enums_by_name[property.type].features |= Enum.FEATURE__DESERIALIZE\\n                features_have_changed = True\\n        elif property.is_object():\\n            if not (objects_by_name[property.type].features \\u0026 feature):\\n                objects_by_name[property.type].features |= feature\\n                features_have_changed = True\\n\\n            if obj.name != property.type:\\n                propagate_feature(objects_by_name[property.type], feature)\\n\\n\\n\\ndef inherit_features(obj):\\n    global features_have_changed\\n\\n    if obj.extended_by is not None:\\n        for extended_by in obj.extended_by:\\n            previous = objects_by_name[extended_by].features\\n            objects_by_name[extended_by].features |= obj.features\\n\\n            if objects_by_name[extended_by].features != previous:\\n                features_have_changed = True\\n\\n    if obj.extends is not None:\\n        previous = objects_by_name[obj.extends].features\\n        objects_by_name[obj.extends].features |= obj.features\\n\\n        if objects_by_name[obj.extends].features != previous:\\n            features_have_changed = True\\n\\n    if obj.extended_by is not None:\\n        for extended_by in obj.extended_by:\\n            inherit_features(objects_by_name[extended_by])\\n\\n\\n\\n# there are two directions to spread features:\\n# 1) up and down the inheritance chain\\n# 2) from object types to their member property types\\n# spreading needs to be done alternating on both directions because they can\\n# affect each other\\nfeatures_have_changed = True\\n\\nwhile features_have_changed:\\n    features_have_changed = False\\n\\n    for obj in objects_by_name.values():\\n        propagate_feature(obj, Object.FEATURE__DEEP_COPY)\\n        propagate_feature(obj, Object.FEATURE__SERIALIZE)\\n        propagate_feature(obj, Object.FEATURE__DESERIALIZE)\\n\\n    for obj in objects_by_name.values():\\n        inherit_features(obj)\\n\\n\\n\\nfor obj in managed_objects_by_name.values():\\n    for property in obj.properties:\\n        if property.occurrence != OCCURRENCE__IGNORED and \\\\\\n           not is_known_type(property.type):\\n            report_error(\\"object \'%s\' contains unknown property type \'%s\'\\"\\n                         % (obj.name, property.type))\\n\\n    if obj.extends is not None:\\n        if not is_known_type(obj.extends):\\n            report_error(\\"object \'%s\' extends unknown object \'%s\'\\"\\n                         % (obj.name, obj.extends))\\n\\n    # detect extended_by relation\\n    if obj.extends is not None:\\n        extended_obj = managed_objects_by_name[obj.extends]\\n\\n        if extended_obj.extended_by is None:\\n            extended_obj.extended_by = [obj.name]\\n        else:\\n            extended_obj.extended_by.append(obj.name)\\n            extended_obj.extended_by.sort()\\n\\n\\n\\nnotice = \\"/* Generated by esx_vi_generator.py */\\\\n\\\\n\\\\n\\\\n\\"\\n\\ntypes_typedef.write(notice)\\ntypes_typeenum.write(notice)\\ntypes_typetostring.write(notice)\\ntypes_typefromstring.write(notice)\\ntypes_header.write(notice)\\ntypes_source.write(notice)\\nmethods_header.write(notice)\\nmethods_source.write(notice)\\nmethods_macro.write(notice)\\nhelpers_header.write(notice)\\nhelpers_source.write(notice)\\n\\n\\n\\n# output enums\\ntypes_typedef.write(separator +\\n                    \\" * VI Enums\\\\n\\" +\\n                    \\" */\\\\n\\\\n\\")\\n\\nnames = enums_by_name.keys()\\nnames.sort()\\n\\nfor name in names:\\n    types_typedef.write(enums_by_name[name].generate_typedef())\\n    types_typeenum.write(enums_by_name[name].generate_typeenum())\\n    types_typetostring.write(enums_by_name[name].generate_typetostring())\\n    types_typefromstring.write(enums_by_name[name].generate_typefromstring())\\n    types_header.write(enums_by_name[name].generate_header())\\n    types_source.write(enums_by_name[name].generate_source())\\n\\n\\n\\n# output objects\\ntypes_typedef.write(\\"\\\\n\\\\n\\\\n\\" +\\n                    separator +\\n                    \\" * VI Objects\\\\n\\" +\\n                    \\" */\\\\n\\\\n\\")\\ntypes_typeenum.write(\\"\\\\n\\")\\ntypes_typetostring.write(\\"\\\\n\\")\\ntypes_typefromstring.write(\\"\\\\n\\")\\n\\nnames = objects_by_name.keys()\\nnames.sort()\\n\\nfor name in names:\\n    types_typedef.write(objects_by_name[name].generate_typedef())\\n    types_typeenum.write(objects_by_name[name].generate_typeenum())\\n    types_typetostring.write(objects_by_name[name].generate_typetostring())\\n    types_typefromstring.write(objects_by_name[name].generate_typefromstring())\\n    types_header.write(objects_by_name[name].generate_header())\\n    types_source.write(objects_by_name[name].generate_source())\\n\\n\\n\\n# output managed objects\\ntypes_typedef.write(\\"\\\\n\\\\n\\\\n\\" +\\n                    separator +\\n                    \\" * VI Managed Objects\\\\n\\" +\\n                    \\" */\\\\n\\\\n\\")\\ntypes_typeenum.write(\\"\\\\n\\")\\ntypes_typetostring.write(\\"\\\\n\\")\\ntypes_typefromstring.write(\\"\\\\n\\")\\n\\nnames = managed_objects_by_name.keys()\\nnames.sort()\\n\\nfor name in names:\\n    types_typedef.write(managed_objects_by_name[name].generate_typedef())\\n    types_typeenum.write(managed_objects_by_name[name].generate_typeenum())\\n    types_typetostring.write(managed_objects_by_name[name].generate_typetostring())\\n    types_typefromstring.write(managed_objects_by_name[name].generate_typefromstring())\\n    types_header.write(managed_objects_by_name[name].generate_header())\\n    types_source.write(managed_objects_by_name[name].generate_source())\\n\\n\\n\\n# output methods\\nnames = methods_by_name.keys()\\nnames.sort()\\n\\nfor name in names:\\n    methods_header.write(methods_by_name[name].generate_header())\\n    methods_source.write(methods_by_name[name].generate_source())\\n\\nnames = list(autobind_names)\\nnames.sort()\\n\\nfor name in names:\\n    string = aligned(\\"#define ESX_VI__METHOD__PARAMETER__THIS__%s \\" % name, \\"\\\\\\\\\\\\n\\", 78)\\n    string += \\"    ESX_VI__METHOD__PARAMETER__THIS_FROM_SERVICE(ManagedObjectReference,      \\\\\\\\\\\\n\\"\\n    string += aligned(\\"\\", \\"%s)\\\\n\\\\n\\\\n\\\\n\\" % name, 49)\\n\\n    methods_macro.write(string)\\n\\n\\n\\n# output helpers\\nnames = managed_objects_by_name.keys()\\nnames.sort()\\n\\nfor name in names:\\n    helpers_header.write(managed_objects_by_name[name].generate_helper_header())\\n    helpers_source.write(managed_objects_by_name[name].generate_helper_source())\\n"}\n'
line: b'{"repo_name":"patrickstocklin/chattR","ref":"refs/heads/master","path":"lib/python2.7/site-packages/pip/index.py","content":"\\"\\"\\"Routines related to PyPI, indexes\\"\\"\\"\\nfrom __future__ import absolute_import\\n\\nimport logging\\nimport cgi\\nfrom collections import namedtuple\\nimport itertools\\nimport sys\\nimport os\\nimport re\\nimport mimetypes\\nimport posixpath\\nimport warnings\\n\\nfrom pip._vendor.six.moves.urllib import parse as urllib_parse\\nfrom pip._vendor.six.moves.urllib import request as urllib_request\\n\\nfrom pip.compat import ipaddress\\nfrom pip.utils import (\\n    Inf, cached_property, normalize_name, splitext, normalize_path,\\n    ARCHIVE_EXTENSIONS, SUPPORTED_EXTENSIONS)\\nfrom pip.utils.deprecation import RemovedInPip8Warning\\nfrom pip.utils.logging import indent_log\\nfrom pip.exceptions import (\\n    DistributionNotFound, BestVersionAlreadyInstalled, InvalidWheelFilename,\\n    UnsupportedWheel,\\n)\\nfrom pip.download import HAS_TLS, url_to_path, path_to_url\\nfrom pip.models import PyPI\\nfrom pip.wheel import Wheel, wheel_ext\\nfrom pip.pep425tags import supported_tags, supported_tags_noarch, get_platform\\nfrom pip._vendor import html5lib, requests, pkg_resources, six\\nfrom pip._vendor.packaging.version import parse as parse_version\\nfrom pip._vendor.requests.exceptions import SSLError\\n\\n\\n__all__ = [\'FormatControl\', \'fmt_ctl_handle_mutual_exclude\', \'PackageFinder\']\\n\\n\\n# Taken from Chrome\'s list of secure origins (See: http://bit.ly/1qrySKC)\\nSECURE_ORIGINS = [\\n    # protocol, hostname, port\\n    (\\"https\\", \\"*\\", \\"*\\"),\\n    (\\"*\\", \\"localhost\\", \\"*\\"),\\n    (\\"*\\", \\"127.0.0.0/8\\", \\"*\\"),\\n    (\\"*\\", \\"::1/128\\", \\"*\\"),\\n    (\\"file\\", \\"*\\", None),\\n]\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass InstallationCandidate(object):\\n\\n    def __init__(self, project, version, location):\\n        self.project = project\\n        self.version = parse_version(version)\\n        self.location = location\\n        self._key = (self.project, self.version, self.location)\\n\\n    def __repr__(self):\\n        return \\"\\u003cInstallationCandidate({0!r}, {1!r}, {2!r})\\u003e\\".format(\\n            self.project, self.version, self.location,\\n        )\\n\\n    def __hash__(self):\\n        return hash(self._key)\\n\\n    def __lt__(self, other):\\n        return self._compare(other, lambda s, o: s \\u003c o)\\n\\n    def __le__(self, other):\\n        return self._compare(other, lambda s, o: s \\u003c= o)\\n\\n    def __eq__(self, other):\\n        return self._compare(other, lambda s, o: s == o)\\n\\n    def __ge__(self, other):\\n        return self._compare(other, lambda s, o: s \\u003e= o)\\n\\n    def __gt__(self, other):\\n        return self._compare(other, lambda s, o: s \\u003e o)\\n\\n    def __ne__(self, other):\\n        return self._compare(other, lambda s, o: s != o)\\n\\n    def _compare(self, other, method):\\n        if not isinstance(other, InstallationCandidate):\\n            return NotImplemented\\n\\n        return method(self._key, other._key)\\n\\n\\nclass PackageFinder(object):\\n    \\"\\"\\"This finds packages.\\n\\n    This is meant to match easy_install\'s technique for looking for\\n    packages, by reading pages and looking for appropriate links.\\n    \\"\\"\\"\\n\\n    def __init__(self, find_links, index_urls,\\n                 allow_external=(), allow_unverified=(),\\n                 allow_all_external=False, allow_all_prereleases=False,\\n                 trusted_hosts=None, process_dependency_links=False,\\n                 session=None, format_control=None):\\n        \\"\\"\\"Create a PackageFinder.\\n\\n        :param format_control: A FormatControl object or None. Used to control\\n            the selection of source packages / binary packages when consulting\\n            the index and links.\\n        \\"\\"\\"\\n        if session is None:\\n            raise TypeError(\\n                \\"PackageFinder() missing 1 required keyword argument: \\"\\n                \\"\'session\'\\"\\n            )\\n\\n        # Build find_links. If an argument starts with ~, it may be\\n        # a local file relative to a home directory. So try normalizing\\n        # it and if it exists, use the normalized version.\\n        # This is deliberately conservative - it might be fine just to\\n        # blindly normalize anything starting with a ~...\\n        self.find_links = []\\n        for link in find_links:\\n            if link.startswith(\'~\'):\\n                new_link = normalize_path(link)\\n                if os.path.exists(new_link):\\n                    link = new_link\\n            self.find_links.append(link)\\n\\n        self.index_urls = index_urls\\n        self.dependency_links = []\\n\\n        # These are boring links that have already been logged somehow:\\n        self.logged_links = set()\\n\\n        self.format_control = format_control or FormatControl(set(), set())\\n\\n        # Do we allow (safe and verifiable) externally hosted files?\\n        self.allow_external = set(normalize_name(n) for n in allow_external)\\n\\n        # Which names are allowed to install insecure and unverifiable files?\\n        self.allow_unverified = set(\\n            normalize_name(n) for n in allow_unverified\\n        )\\n\\n        # Anything that is allowed unverified is also allowed external\\n        self.allow_external |= self.allow_unverified\\n\\n        # Do we allow all (safe and verifiable) externally hosted files?\\n        self.allow_all_external = allow_all_external\\n\\n        # Domains that we won\'t emit warnings for when not using HTTPS\\n        self.secure_origins = [\\n            (\\"*\\", host, \\"*\\")\\n            for host in (trusted_hosts if trusted_hosts else [])\\n        ]\\n\\n        # Stores if we ignored any external links so that we can instruct\\n        #   end users how to install them if no distributions are available\\n        self.need_warn_external = False\\n\\n        # Stores if we ignored any unsafe links so that we can instruct\\n        #   end users how to install them if no distributions are available\\n        self.need_warn_unverified = False\\n\\n        # Do we want to allow _all_ pre-releases?\\n        self.allow_all_prereleases = allow_all_prereleases\\n\\n        # Do we process dependency links?\\n        self.process_dependency_links = process_dependency_links\\n\\n        # The Session we\'ll use to make requests\\n        self.session = session\\n\\n        # If we don\'t have TLS enabled, then WARN if anyplace we\'re looking\\n        # relies on TLS.\\n        if not HAS_TLS:\\n            for link in itertools.chain(self.index_urls, self.find_links):\\n                parsed = urllib_parse.urlparse(link)\\n                if parsed.scheme == \\"https\\":\\n                    logger.warning(\\n                        \\"pip is configured with locations that require \\"\\n                        \\"TLS/SSL, however the ssl module in Python is not \\"\\n                        \\"available.\\"\\n                    )\\n                    break\\n\\n    def add_dependency_links(self, links):\\n        # # FIXME: this shouldn\'t be global list this, it should only\\n        # # apply to requirements of the package that specifies the\\n        # # dependency_links value\\n        # # FIXME: also, we should track comes_from (i.e., use Link)\\n        if self.process_dependency_links:\\n            warnings.warn(\\n                \\"Dependency Links processing has been deprecated and will be \\"\\n                \\"removed in a future release.\\",\\n                RemovedInPip8Warning,\\n            )\\n            self.dependency_links.extend(links)\\n\\n    @staticmethod\\n    def _sort_locations(locations, expand_dir=False):\\n        \\"\\"\\"\\n        Sort locations into \\"files\\" (archives) and \\"urls\\", and return\\n        a pair of lists (files,urls)\\n        \\"\\"\\"\\n        files = []\\n        urls = []\\n\\n        # puts the url for the given file path into the appropriate list\\n        def sort_path(path):\\n            url = path_to_url(path)\\n            if mimetypes.guess_type(url, strict=False)[0] == \'text/html\':\\n                urls.append(url)\\n            else:\\n                files.append(url)\\n\\n        for url in locations:\\n\\n            is_local_path = os.path.exists(url)\\n            is_file_url = url.startswith(\'file:\')\\n\\n            if is_local_path or is_file_url:\\n                if is_local_path:\\n                    path = url\\n                else:\\n                    path = url_to_path(url)\\n                if os.path.isdir(path):\\n                    if expand_dir:\\n                        path = os.path.realpath(path)\\n                        for item in os.listdir(path):\\n                            sort_path(os.path.join(path, item))\\n                    elif is_file_url:\\n                        urls.append(url)\\n                elif os.path.isfile(path):\\n                    sort_path(path)\\n            else:\\n                urls.append(url)\\n\\n        return files, urls\\n\\n    def _candidate_sort_key(self, candidate):\\n        \\"\\"\\"\\n        Function used to generate link sort key for link tuples.\\n        The greater the return value, the more preferred it is.\\n        If not finding wheels, then sorted by version only.\\n        If finding wheels, then the sort order is by version, then:\\n          1. existing installs\\n          2. wheels ordered via Wheel.support_index_min()\\n          3. source archives\\n        Note: it was considered to embed this logic into the Link\\n              comparison operators, but then different sdist links\\n              with the same version, would have to be considered equal\\n        \\"\\"\\"\\n        support_num = len(supported_tags)\\n        if candidate.location == INSTALLED_VERSION:\\n            pri = 1\\n        elif candidate.location.is_wheel:\\n            # can raise InvalidWheelFilename\\n            wheel = Wheel(candidate.location.filename)\\n            if not wheel.supported():\\n                raise UnsupportedWheel(\\n                    \\"%s is not a supported wheel for this platform. It \\"\\n                    \\"can\'t be sorted.\\" % wheel.filename\\n                )\\n            pri = -(wheel.support_index_min())\\n        else:  # sdist\\n            pri = -(support_num)\\n        return (candidate.version, pri)\\n\\n    def _sort_versions(self, applicable_versions):\\n        \\"\\"\\"\\n        Bring the latest version (and wheels) to the front, but maintain the\\n        existing ordering as secondary. See the docstring for `_link_sort_key`\\n        for details. This function is isolated for easier unit testing.\\n        \\"\\"\\"\\n        return sorted(\\n            applicable_versions,\\n            key=self._candidate_sort_key,\\n            reverse=True\\n        )\\n\\n    def _validate_secure_origin(self, logger, location):\\n        # Determine if this url used a secure transport mechanism\\n        parsed = urllib_parse.urlparse(str(location))\\n        origin = (parsed.scheme, parsed.hostname, parsed.port)\\n\\n        # Determine if our origin is a secure origin by looking through our\\n        # hardcoded list of secure origins, as well as any additional ones\\n        # configured on this PackageFinder instance.\\n        for secure_origin in (SECURE_ORIGINS + self.secure_origins):\\n            # Check to see if the protocol matches\\n            if origin[0] != secure_origin[0] and secure_origin[0] != \\"*\\":\\n                continue\\n\\n            try:\\n                # We need to do this decode dance to ensure that we have a\\n                # unicode object, even on Python 2.x.\\n                addr = ipaddress.ip_address(\\n                    origin[1]\\n                    if (\\n                        isinstance(origin[1], six.text_type) or\\n                        origin[1] is None\\n                    )\\n                    else origin[1].decode(\\"utf8\\")\\n                )\\n                network = ipaddress.ip_network(\\n                    secure_origin[1]\\n                    if isinstance(secure_origin[1], six.text_type)\\n                    else secure_origin[1].decode(\\"utf8\\")\\n                )\\n            except ValueError:\\n                # We don\'t have both a valid address or a valid network, so\\n                # we\'ll check this origin against hostnames.\\n                if origin[1] != secure_origin[1] and secure_origin[1] != \\"*\\":\\n                    continue\\n            else:\\n                # We have a valid address and network, so see if the address\\n                # is contained within the network.\\n                if addr not in network:\\n                    continue\\n\\n            # Check to see if the port patches\\n            if (origin[2] != secure_origin[2] and\\n                    secure_origin[2] != \\"*\\" and\\n                    secure_origin[2] is not None):\\n                continue\\n\\n            # If we\'ve gotten here, then this origin matches the current\\n            # secure origin and we should return True\\n            return True\\n\\n        # If we\'ve gotten to this point, then the origin isn\'t secure and we\\n        # will not accept it as a valid location to search. We will however\\n        # log a warning that we are ignoring it.\\n        logger.warning(\\n            \\"The repository located at %s is not a trusted or secure host and \\"\\n            \\"is being ignored. If this repository is available via HTTPS it \\"\\n            \\"is recommended to use HTTPS instead, otherwise you may silence \\"\\n            \\"this warning and allow it anyways with \'--trusted-host %s\'.\\",\\n            parsed.hostname,\\n            parsed.hostname,\\n        )\\n\\n        return False\\n\\n    def _get_index_urls_locations(self, project_name):\\n        \\"\\"\\"Returns the locations found via self.index_urls\\n\\n        Checks the url_name on the main (first in the list) index and\\n        use this url_name to produce all locations\\n        \\"\\"\\"\\n\\n        def mkurl_pypi_url(url):\\n            loc = posixpath.join(url, project_url_name)\\n            # For maximum compatibility with easy_install, ensure the path\\n            # ends in a trailing slash.  Although this isn\'t in the spec\\n            # (and PyPI can handle it without the slash) some other index\\n            # implementations might break if they relied on easy_install\'s\\n            # behavior.\\n            if not loc.endswith(\'/\'):\\n                loc = loc + \'/\'\\n            return loc\\n\\n        project_url_name = urllib_parse.quote(project_name.lower())\\n\\n        if self.index_urls:\\n            # Check that we have the url_name correctly spelled:\\n\\n            # Only check main index if index URL is given\\n            main_index_url = Link(\\n                mkurl_pypi_url(self.index_urls[0]),\\n                trusted=True,\\n            )\\n\\n            page = self._get_page(main_index_url)\\n            if page is None and PyPI.netloc not in str(main_index_url):\\n                warnings.warn(\\n                    \\"Failed to find %r at %s. It is suggested to upgrade \\"\\n                    \\"your index to support normalized names as the name in \\"\\n                    \\"/simple/{name}.\\" % (project_name, main_index_url),\\n                    RemovedInPip8Warning,\\n                )\\n\\n                project_url_name = self._find_url_name(\\n                    Link(self.index_urls[0], trusted=True),\\n                    project_url_name,\\n                ) or project_url_name\\n\\n        if project_url_name is not None:\\n            return [mkurl_pypi_url(url) for url in self.index_urls]\\n        return []\\n\\n    def _find_all_versions(self, project_name):\\n        \\"\\"\\"Find all available versions for project_name\\n\\n        This checks index_urls, find_links and dependency_links\\n        All versions found are returned\\n\\n        See _link_package_versions for details on which files are accepted\\n        \\"\\"\\"\\n        index_locations = self._get_index_urls_locations(project_name)\\n        index_file_loc, index_url_loc = self._sort_locations(index_locations)\\n        fl_file_loc, fl_url_loc = self._sort_locations(\\n            self.find_links, expand_dir=True)\\n        dep_file_loc, dep_url_loc = self._sort_locations(self.dependency_links)\\n\\n        file_locations = (\\n            Link(url) for url in itertools.chain(\\n                index_file_loc, fl_file_loc, dep_file_loc)\\n        )\\n\\n        # We trust every url that the user has given us whether it was given\\n        #   via --index-url or --find-links\\n        # We explicitly do not trust links that came from dependency_links\\n        # We want to filter out any thing which does not have a secure origin.\\n        url_locations = [\\n            link for link in itertools.chain(\\n                (Link(url, trusted=True) for url in index_url_loc),\\n                (Link(url, trusted=True) for url in fl_url_loc),\\n                (Link(url) for url in dep_url_loc),\\n            )\\n            if self._validate_secure_origin(logger, link)\\n        ]\\n\\n        logger.debug(\'%d location(s) to search for versions of %s:\',\\n                     len(url_locations), project_name)\\n\\n        for location in url_locations:\\n            logger.debug(\'* %s\', location)\\n\\n        canonical_name = pkg_resources.safe_name(project_name).lower()\\n        formats = fmt_ctl_formats(self.format_control, canonical_name)\\n        search = Search(project_name.lower(), canonical_name, formats)\\n        find_links_versions = self._package_versions(\\n            # We trust every directly linked archive in find_links\\n            (Link(url, \'-f\', trusted=True) for url in self.find_links),\\n            search\\n        )\\n\\n        page_versions = []\\n        for page in self._get_pages(url_locations, project_name):\\n            logger.debug(\'Analyzing links from page %s\', page.url)\\n            with indent_log():\\n                page_versions.extend(\\n                    self._package_versions(page.links, search)\\n                )\\n\\n        dependency_versions = self._package_versions(\\n            (Link(url) for url in self.dependency_links), search\\n        )\\n        if dependency_versions:\\n            logger.debug(\\n                \'dependency_links found: %s\',\\n                \', \'.join([\\n                    version.location.url for version in dependency_versions\\n                ])\\n            )\\n\\n        file_versions = self._package_versions(file_locations, search)\\n        if file_versions:\\n            file_versions.sort(reverse=True)\\n            logger.debug(\\n                \'Local files found: %s\',\\n                \', \'.join([\\n                    url_to_path(candidate.location.url)\\n                    for candidate in file_versions\\n                ])\\n            )\\n\\n        # This is an intentional priority ordering\\n        return (\\n            file_versions + find_links_versions + page_versions +\\n            dependency_versions\\n        )\\n\\n    def find_requirement(self, req, upgrade):\\n        \\"\\"\\"Try to find an InstallationCandidate for req\\n\\n        Expects req, an InstallRequirement and upgrade, a boolean\\n        Returns an InstallationCandidate or None\\n        May raise DistributionNotFound or BestVersionAlreadyInstalled\\n        \\"\\"\\"\\n        all_versions = self._find_all_versions(req.name)\\n\\n        # Filter out anything which doesn\'t match our specifier\\n        _versions = set(\\n            req.specifier.filter(\\n                # We turn the version object into a str here because otherwise\\n                # when we\'re debundled but setuptools isn\'t, Python will see\\n                # packaging.version.Version and\\n                # pkg_resources._vendor.packaging.version.Version as different\\n                # types. This way we\'ll use a str as a common data interchange\\n                # format. If we stop using the pkg_resources provided specifier\\n                # and start using our own, we can drop the cast to str().\\n                [str(x.version) for x in all_versions],\\n                prereleases=(\\n                    self.allow_all_prereleases\\n                    if self.allow_all_prereleases else None\\n                ),\\n            )\\n        )\\n        applicable_versions = [\\n            # Again, converting to str to deal with debundling.\\n            x for x in all_versions if str(x.version) in _versions\\n        ]\\n\\n        if req.satisfied_by is not None:\\n            # Finally add our existing versions to the front of our versions.\\n            applicable_versions.insert(\\n                0,\\n                InstallationCandidate(\\n                    req.name,\\n                    req.satisfied_by.version,\\n                    INSTALLED_VERSION,\\n                )\\n            )\\n            existing_applicable = True\\n        else:\\n            existing_applicable = False\\n\\n        applicable_versions = self._sort_versions(applicable_versions)\\n\\n        if not upgrade and existing_applicable:\\n            if applicable_versions[0].location is INSTALLED_VERSION:\\n                logger.debug(\\n                    \'Existing installed version (%s) is most up-to-date and \'\\n                    \'satisfies requirement\',\\n                    req.satisfied_by.version,\\n                )\\n            else:\\n                logger.debug(\\n                    \'Existing installed version (%s) satisfies requirement \'\\n                    \'(most up-to-date version is %s)\',\\n                    req.satisfied_by.version,\\n                    applicable_versions[0][2],\\n                )\\n            return None\\n\\n        if not applicable_versions:\\n            logger.critical(\\n                \'Could not find a version that satisfies the requirement %s \'\\n                \'(from versions: %s)\',\\n                req,\\n                \', \'.join(\\n                    sorted(\\n                        set(str(i.version) for i in all_versions),\\n                        key=parse_version,\\n                    )\\n                )\\n            )\\n\\n            if self.need_warn_external:\\n                logger.warning(\\n                    \\"Some externally hosted files were ignored as access to \\"\\n                    \\"them may be unreliable (use --allow-external %s to \\"\\n                    \\"allow).\\",\\n                    req.name,\\n                )\\n\\n            if self.need_warn_unverified:\\n                logger.warning(\\n                    \\"Some insecure and unverifiable files were ignored\\"\\n                    \\" (use --allow-unverified %s to allow).\\",\\n                    req.name,\\n                )\\n\\n            raise DistributionNotFound(\\n                \'No matching distribution found for %s\' % req\\n            )\\n\\n        if applicable_versions[0].location is INSTALLED_VERSION:\\n            # We have an existing version, and its the best version\\n            logger.debug(\\n                \'Installed version (%s) is most up-to-date (past versions: \'\\n                \'%s)\',\\n                req.satisfied_by.version,\\n                \', \'.join(str(i.version) for i in applicable_versions[1:]) or\\n                \\"none\\",\\n            )\\n            raise BestVersionAlreadyInstalled\\n\\n        if len(applicable_versions) \\u003e 1:\\n            logger.debug(\\n                \'Using version %s (newest of versions: %s)\',\\n                applicable_versions[0].version,\\n                \', \'.join(str(i.version) for i in applicable_versions)\\n            )\\n\\n        selected_version = applicable_versions[0].location\\n\\n        if (selected_version.verifiable is not None and not\\n                selected_version.verifiable):\\n            logger.warning(\\n                \\"%s is potentially insecure and unverifiable.\\", req.name,\\n            )\\n\\n        return selected_version\\n\\n    def _find_url_name(self, index_url, url_name):\\n        \\"\\"\\"\\n        Finds the true URL name of a package, when the given name isn\'t quite\\n        correct.\\n        This is usually used to implement case-insensitivity.\\n        \\"\\"\\"\\n        if not index_url.url.endswith(\'/\'):\\n            # Vaguely part of the PyPI API... weird but true.\\n            # FIXME: bad to modify this?\\n            index_url.url += \'/\'\\n        page = self._get_page(index_url)\\n        if page is None:\\n            logger.critical(\'Cannot fetch index base URL %s\', index_url)\\n            return\\n        norm_name = normalize_name(url_name)\\n        for link in page.links:\\n            base = posixpath.basename(link.path.rstrip(\'/\'))\\n            if norm_name == normalize_name(base):\\n                logger.debug(\\n                    \'Real name of requirement %s is %s\', url_name, base,\\n                )\\n                return base\\n        return None\\n\\n    def _get_pages(self, locations, project_name):\\n        \\"\\"\\"\\n        Yields (page, page_url) from the given locations, skipping\\n        locations that have errors, and adding download/homepage links\\n        \\"\\"\\"\\n        all_locations = list(locations)\\n        seen = set()\\n        normalized = normalize_name(project_name)\\n\\n        while all_locations:\\n            location = all_locations.pop(0)\\n            if location in seen:\\n                continue\\n            seen.add(location)\\n\\n            page = self._get_page(location)\\n            if page is None:\\n                continue\\n\\n            yield page\\n\\n            for link in page.rel_links():\\n\\n                if (normalized not in self.allow_external and not\\n                        self.allow_all_external):\\n                    self.need_warn_external = True\\n                    logger.debug(\\n                        \\"Not searching %s for files because external \\"\\n                        \\"urls are disallowed.\\",\\n                        link,\\n                    )\\n                    continue\\n\\n                if (link.trusted is not None and not\\n                        link.trusted and\\n                        normalized not in self.allow_unverified):\\n                    logger.debug(\\n                        \\"Not searching %s for urls, it is an \\"\\n                        \\"untrusted link and cannot produce safe or \\"\\n                        \\"verifiable files.\\",\\n                        link,\\n                    )\\n                    self.need_warn_unverified = True\\n                    continue\\n\\n                all_locations.append(link)\\n\\n    _py_version_re = re.compile(r\'-py([123]\\\\.?[0-9]?)$\')\\n\\n    def _sort_links(self, links):\\n        \\"\\"\\"\\n        Returns elements of links in order, non-egg links first, egg links\\n        second, while eliminating duplicates\\n        \\"\\"\\"\\n        eggs, no_eggs = [], []\\n        seen = set()\\n        for link in links:\\n            if link not in seen:\\n                seen.add(link)\\n                if link.egg_fragment:\\n                    eggs.append(link)\\n                else:\\n                    no_eggs.append(link)\\n        return no_eggs + eggs\\n\\n    def _package_versions(self, links, search):\\n        result = []\\n        for link in self._sort_links(links):\\n            v = self._link_package_versions(link, search)\\n            if v is not None:\\n                result.append(v)\\n        return result\\n\\n    def _log_skipped_link(self, link, reason):\\n        if link not in self.logged_links:\\n            logger.debug(\'Skipping link %s; %s\', link, reason)\\n            self.logged_links.add(link)\\n\\n    def _link_package_versions(self, link, search):\\n        \\"\\"\\"Return an InstallationCandidate or None\\"\\"\\"\\n        platform = get_platform()\\n\\n        version = None\\n        if link.egg_fragment:\\n            egg_info = link.egg_fragment\\n            ext = link.ext\\n        else:\\n            egg_info, ext = link.splitext()\\n            if not ext:\\n                self._log_skipped_link(link, \'not a file\')\\n                return\\n            if ext not in SUPPORTED_EXTENSIONS:\\n                self._log_skipped_link(\\n                    link, \'unsupported archive format: %s\' % ext)\\n                return\\n            if \\"binary\\" not in search.formats and ext == wheel_ext:\\n                self._log_skipped_link(\\n                    link, \'No binaries permitted for %s\' % search.supplied)\\n                return\\n            if \\"macosx10\\" in link.path and ext == \'.zip\':\\n                self._log_skipped_link(link, \'macosx10 one\')\\n                return\\n            if ext == wheel_ext:\\n                try:\\n                    wheel = Wheel(link.filename)\\n                except InvalidWheelFilename:\\n                    self._log_skipped_link(link, \'invalid wheel filename\')\\n                    return\\n                if (pkg_resources.safe_name(wheel.name).lower() !=\\n                        search.canonical):\\n                    self._log_skipped_link(\\n                        link, \'wrong project name (not %s)\' % search.supplied)\\n                    return\\n                if not wheel.supported():\\n                    self._log_skipped_link(\\n                        link, \'it is not compatible with this Python\')\\n                    return\\n                # This is a dirty hack to prevent installing Binary Wheels from\\n                # PyPI unless it is a Windows or Mac Binary Wheel. This is\\n                # paired with a change to PyPI disabling uploads for the\\n                # same. Once we have a mechanism for enabling support for\\n                # binary wheels on linux that deals with the inherent problems\\n                # of binary distribution this can be removed.\\n                comes_from = getattr(link, \\"comes_from\\", None)\\n                if (\\n                        (\\n                            not platform.startswith(\'win\') and not\\n                            platform.startswith(\'macosx\') and not\\n                            platform == \'cli\'\\n                        ) and\\n                        comes_from is not None and\\n                        urllib_parse.urlparse(\\n                            comes_from.url\\n                        ).netloc.endswith(PyPI.netloc)):\\n                    if not wheel.supported(tags=supported_tags_noarch):\\n                        self._log_skipped_link(\\n                            link,\\n                            \\"it is a pypi-hosted binary \\"\\n                            \\"Wheel on an unsupported platform\\",\\n                        )\\n                        return\\n                version = wheel.version\\n\\n        # This should be up by the search.ok_binary check, but see issue 2700.\\n        if \\"source\\" not in search.formats and ext != wheel_ext:\\n            self._log_skipped_link(\\n                link, \'No sources permitted for %s\' % search.supplied)\\n            return\\n\\n        if not version:\\n            version = egg_info_matches(egg_info, search.supplied, link)\\n        if version is None:\\n            self._log_skipped_link(\\n                link, \'wrong project name (not %s)\' % search.supplied)\\n            return\\n\\n        if (link.internal is not None and not\\n                link.internal and not\\n                normalize_name(search.supplied).lower()\\n                in self.allow_external and not\\n                self.allow_all_external):\\n            # We have a link that we are sure is external, so we should skip\\n            #   it unless we are allowing externals\\n            self._log_skipped_link(link, \'it is externally hosted\')\\n            self.need_warn_external = True\\n            return\\n\\n        if (link.verifiable is not None and not\\n                link.verifiable and not\\n                (normalize_name(search.supplied).lower()\\n                    in self.allow_unverified)):\\n            # We have a link that we are sure we cannot verify its integrity,\\n            #   so we should skip it unless we are allowing unsafe installs\\n            #   for this requirement.\\n            self._log_skipped_link(\\n                link, \'it is an insecure and unverifiable file\')\\n            self.need_warn_unverified = True\\n            return\\n\\n        match = self._py_version_re.search(version)\\n        if match:\\n            version = version[:match.start()]\\n            py_version = match.group(1)\\n            if py_version != sys.version[:3]:\\n                self._log_skipped_link(\\n                    link, \'Python version is incorrect\')\\n                return\\n        logger.debug(\'Found link %s, version: %s\', link, version)\\n\\n        return InstallationCandidate(search.supplied, version, link)\\n\\n    def _get_page(self, link):\\n        return HTMLPage.get_page(link, session=self.session)\\n\\n\\ndef egg_info_matches(\\n        egg_info, search_name, link,\\n        _egg_info_re=re.compile(r\'([a-z0-9_.]+)-([a-z0-9_.!+-]+)\', re.I)):\\n    \\"\\"\\"Pull the version part out of a string.\\n\\n    :param egg_info: The string to parse. E.g. foo-2.1\\n    :param search_name: The name of the package this belongs to. None to\\n        infer the name. Note that this cannot unambiguously parse strings\\n        like foo-2-2 which might be foo, 2-2 or foo-2, 2.\\n    :param link: The link the string came from, for logging on failure.\\n    \\"\\"\\"\\n    match = _egg_info_re.search(egg_info)\\n    if not match:\\n        logger.debug(\'Could not parse version from link: %s\', link)\\n        return None\\n    if search_name is None:\\n        full_match = match.group(0)\\n        return full_match[full_match.index(\'-\'):]\\n    name = match.group(0).lower()\\n    # To match the \\"safe\\" name that pkg_resources creates:\\n    name = name.replace(\'_\', \'-\')\\n    # project name and version must be separated by a dash\\n    look_for = search_name.lower() + \\"-\\"\\n    if name.startswith(look_for):\\n        return match.group(0)[len(look_for):]\\n    else:\\n        return None\\n\\n\\nclass HTMLPage(object):\\n    \\"\\"\\"Represents one page, along with its URL\\"\\"\\"\\n\\n    def __init__(self, content, url, headers=None, trusted=None):\\n        # Determine if we have any encoding information in our headers\\n        encoding = None\\n        if headers and \\"Content-Type\\" in headers:\\n            content_type, params = cgi.parse_header(headers[\\"Content-Type\\"])\\n\\n            if \\"charset\\" in params:\\n                encoding = params[\'charset\']\\n\\n        self.content = content\\n        self.parsed = html5lib.parse(\\n            self.content,\\n            encoding=encoding,\\n            namespaceHTMLElements=False,\\n        )\\n        self.url = url\\n        self.headers = headers\\n        self.trusted = trusted\\n\\n    def __str__(self):\\n        return self.url\\n\\n    @classmethod\\n    def get_page(cls, link, skip_archives=True, session=None):\\n        if session is None:\\n            raise TypeError(\\n                \\"get_page() missing 1 required keyword argument: \'session\'\\"\\n            )\\n\\n        url = link.url\\n        url = url.split(\'#\', 1)[0]\\n\\n        # Check for VCS schemes that do not support lookup as web pages.\\n        from pip.vcs import VcsSupport\\n        for scheme in VcsSupport.schemes:\\n            if url.lower().startswith(scheme) and url[len(scheme)] in \'+:\':\\n                logger.debug(\'Cannot look at %s URL %s\', scheme, link)\\n                return None\\n\\n        try:\\n            if skip_archives:\\n                filename = link.filename\\n                for bad_ext in ARCHIVE_EXTENSIONS:\\n                    if filename.endswith(bad_ext):\\n                        content_type = cls._get_content_type(\\n                            url, session=session,\\n                        )\\n                        if content_type.lower().startswith(\'text/html\'):\\n                            break\\n                        else:\\n                            logger.debug(\\n                                \'Skipping page %s because of Content-Type: %s\',\\n                                link,\\n                                content_type,\\n                            )\\n                            return\\n\\n            logger.debug(\'Getting page %s\', url)\\n\\n            # Tack index.html onto file:// URLs that point to directories\\n            (scheme, netloc, path, params, query, fragment) = \\\\\\n                urllib_parse.urlparse(url)\\n            if (scheme == \'file\' and\\n                    os.path.isdir(urllib_request.url2pathname(path))):\\n                # add trailing slash if not present so urljoin doesn\'t trim\\n                # final segment\\n                if not url.endswith(\'/\'):\\n                    url += \'/\'\\n                url = urllib_parse.urljoin(url, \'index.html\')\\n                logger.debug(\' file: URL is directory, getting %s\', url)\\n\\n            resp = session.get(\\n                url,\\n                headers={\\n                    \\"Accept\\": \\"text/html\\",\\n                    \\"Cache-Control\\": \\"max-age=600\\",\\n                },\\n            )\\n            resp.raise_for_status()\\n\\n            # The check for archives above only works if the url ends with\\n            #   something that looks like an archive. However that is not a\\n            #   requirement of an url. Unless we issue a HEAD request on every\\n            #   url we cannot know ahead of time for sure if something is HTML\\n            #   or not. However we can check after we\'ve downloaded it.\\n            content_type = resp.headers.get(\'Content-Type\', \'unknown\')\\n            if not content_type.lower().startswith(\\"text/html\\"):\\n                logger.debug(\\n                    \'Skipping page %s because of Content-Type: %s\',\\n                    link,\\n                    content_type,\\n                )\\n                return\\n\\n            inst = cls(\\n                resp.content, resp.url, resp.headers,\\n                trusted=link.trusted,\\n            )\\n        except requests.HTTPError as exc:\\n            level = 2 if exc.response.status_code == 404 else 1\\n            cls._handle_fail(link, exc, url, level=level)\\n        except requests.ConnectionError as exc:\\n            cls._handle_fail(link, \\"connection error: %s\\" % exc, url)\\n        except requests.Timeout:\\n            cls._handle_fail(link, \\"timed out\\", url)\\n        except SSLError as exc:\\n            reason = (\\"There was a problem confirming the ssl certificate: \\"\\n                      \\"%s\\" % exc)\\n            cls._handle_fail(link, reason, url, level=2, meth=logger.info)\\n        else:\\n            return inst\\n\\n    @staticmethod\\n    def _handle_fail(link, reason, url, level=1, meth=None):\\n        if meth is None:\\n            meth = logger.debug\\n\\n        meth(\\"Could not fetch URL %s: %s - skipping\\", link, reason)\\n\\n    @staticmethod\\n    def _get_content_type(url, session):\\n        \\"\\"\\"Get the Content-Type of the given url, using a HEAD request\\"\\"\\"\\n        scheme, netloc, path, query, fragment = urllib_parse.urlsplit(url)\\n        if scheme not in (\'http\', \'https\'):\\n            # FIXME: some warning or something?\\n            # assertion error?\\n            return \'\'\\n\\n        resp = session.head(url, allow_redirects=True)\\n        resp.raise_for_status()\\n\\n        return resp.headers.get(\\"Content-Type\\", \\"\\")\\n\\n    @cached_property\\n    def api_version(self):\\n        metas = [\\n            x for x in self.parsed.findall(\\".//meta\\")\\n            if x.get(\\"name\\", \\"\\").lower() == \\"api-version\\"\\n        ]\\n        if metas:\\n            try:\\n                return int(metas[0].get(\\"value\\", None))\\n            except (TypeError, ValueError):\\n                pass\\n\\n        return None\\n\\n    @cached_property\\n    def base_url(self):\\n        bases = [\\n            x for x in self.parsed.findall(\\".//base\\")\\n            if x.get(\\"href\\") is not None\\n        ]\\n        if bases and bases[0].get(\\"href\\"):\\n            return bases[0].get(\\"href\\")\\n        else:\\n            return self.url\\n\\n    @property\\n    def links(self):\\n        \\"\\"\\"Yields all links in the page\\"\\"\\"\\n        for anchor in self.parsed.findall(\\".//a\\"):\\n            if anchor.get(\\"href\\"):\\n                href = anchor.get(\\"href\\")\\n                url = self.clean_link(\\n                    urllib_parse.urljoin(self.base_url, href)\\n                )\\n\\n                # Determine if this link is internal. If that distinction\\n                #   doesn\'t make sense in this context, then we don\'t make\\n                #   any distinction.\\n                internal = None\\n                if self.api_version and self.api_version \\u003e= 2:\\n                    # Only api_versions \\u003e= 2 have a distinction between\\n                    #   external and internal links\\n                    internal = bool(\\n                        anchor.get(\\"rel\\") and\\n                        \\"internal\\" in anchor.get(\\"rel\\").split()\\n                    )\\n\\n                yield Link(url, self, internal=internal)\\n\\n    def rel_links(self, rels=(\'homepage\', \'download\')):\\n        \\"\\"\\"Yields all links with the given relations\\"\\"\\"\\n        rels = set(rels)\\n\\n        for anchor in self.parsed.findall(\\".//a\\"):\\n            if anchor.get(\\"rel\\") and anchor.get(\\"href\\"):\\n                found_rels = set(anchor.get(\\"rel\\").split())\\n                # Determine the intersection between what rels were found and\\n                #   what rels were being looked for\\n                if found_rels \\u0026 rels:\\n                    href = anchor.get(\\"href\\")\\n                    url = self.clean_link(\\n                        urllib_parse.urljoin(self.base_url, href)\\n                    )\\n                    yield Link(url, self, trusted=False)\\n\\n    _clean_re = re.compile(r\'[^a-z0-9$\\u0026+,/:;=?@.#%_\\\\\\\\|-]\', re.I)\\n\\n    def clean_link(self, url):\\n        \\"\\"\\"Makes sure a link is fully encoded.  That is, if a \' \' shows up in\\n        the link, it will be rewritten to %20 (while not over-quoting\\n        % or other characters).\\"\\"\\"\\n        return self._clean_re.sub(\\n            lambda match: \'%%%2x\' % ord(match.group(0)), url)\\n\\n\\nclass Link(object):\\n\\n    def __init__(self, url, comes_from=None, internal=None, trusted=None):\\n\\n        # url can be a UNC windows share\\n        if url != Inf and url.startswith(\'\\\\\\\\\\\\\\\\\'):\\n            url = path_to_url(url)\\n\\n        self.url = url\\n        self.comes_from = comes_from\\n        self.internal = internal\\n        self.trusted = trusted\\n\\n    def __str__(self):\\n        if self.comes_from:\\n            return \'%s (from %s)\' % (self.url, self.comes_from)\\n        else:\\n            return str(self.url)\\n\\n    def __repr__(self):\\n        return \'\\u003cLink %s\\u003e\' % self\\n\\n    def __eq__(self, other):\\n        if not isinstance(other, Link):\\n            return NotImplemented\\n        return self.url == other.url\\n\\n    def __ne__(self, other):\\n        if not isinstance(other, Link):\\n            return NotImplemented\\n        return self.url != other.url\\n\\n    def __lt__(self, other):\\n        if not isinstance(other, Link):\\n            return NotImplemented\\n        return self.url \\u003c other.url\\n\\n    def __le__(self, other):\\n        if not isinstance(other, Link):\\n            return NotImplemented\\n        return self.url \\u003c= other.url\\n\\n    def __gt__(self, other):\\n        if not isinstance(other, Link):\\n            return NotImplemented\\n        return self.url \\u003e other.url\\n\\n    def __ge__(self, other):\\n        if not isinstance(other, Link):\\n            return NotImplemented\\n        return self.url \\u003e= other.url\\n\\n    def __hash__(self):\\n        return hash(self.url)\\n\\n    @property\\n    def filename(self):\\n        _, netloc, path, _, _ = urllib_parse.urlsplit(self.url)\\n        name = posixpath.basename(path.rstrip(\'/\')) or netloc\\n        name = urllib_parse.unquote(name)\\n        assert name, (\'URL %r produced no filename\' % self.url)\\n        return name\\n\\n    @property\\n    def scheme(self):\\n        return urllib_parse.urlsplit(self.url)[0]\\n\\n    @property\\n    def netloc(self):\\n        return urllib_parse.urlsplit(self.url)[1]\\n\\n    @property\\n    def path(self):\\n        return urllib_parse.unquote(urllib_parse.urlsplit(self.url)[2])\\n\\n    def splitext(self):\\n        return splitext(posixpath.basename(self.path.rstrip(\'/\')))\\n\\n    @property\\n    def ext(self):\\n        return self.splitext()[1]\\n\\n    @property\\n    def url_without_fragment(self):\\n        scheme, netloc, path, query, fragment = urllib_parse.urlsplit(self.url)\\n        return urllib_parse.urlunsplit((scheme, netloc, path, query, None))\\n\\n    _egg_fragment_re = re.compile(r\'#egg=([^\\u0026]*)\')\\n\\n    @property\\n    def egg_fragment(self):\\n        match = self._egg_fragment_re.search(self.url)\\n        if not match:\\n            return None\\n        return match.group(1)\\n\\n    _hash_re = re.compile(\\n        r\'(sha1|sha224|sha384|sha256|sha512|md5)=([a-f0-9]+)\'\\n    )\\n\\n    @property\\n    def hash(self):\\n        match = self._hash_re.search(self.url)\\n        if match:\\n            return match.group(2)\\n        return None\\n\\n    @property\\n    def hash_name(self):\\n        match = self._hash_re.search(self.url)\\n        if match:\\n            return match.group(1)\\n        return None\\n\\n    @property\\n    def show_url(self):\\n        return posixpath.basename(self.url.split(\'#\', 1)[0].split(\'?\', 1)[0])\\n\\n    @property\\n    def verifiable(self):\\n        \\"\\"\\"\\n        Returns True if this link can be verified after download, False if it\\n        cannot, and None if we cannot determine.\\n        \\"\\"\\"\\n        trusted = self.trusted or getattr(self.comes_from, \\"trusted\\", None)\\n        if trusted is not None and trusted:\\n            # This link came from a trusted source. It *may* be verifiable but\\n            #   first we need to see if this page is operating under the new\\n            #   API version.\\n            try:\\n                api_version = getattr(self.comes_from, \\"api_version\\", None)\\n                api_version = int(api_version)\\n            except (ValueError, TypeError):\\n                api_version = None\\n\\n            if api_version is None or api_version \\u003c= 1:\\n                # This link is either trusted, or it came from a trusted,\\n                #   however it is not operating under the API version 2 so\\n                #   we can\'t make any claims about if it\'s safe or not\\n                return\\n\\n            if self.hash:\\n                # This link came from a trusted source and it has a hash, so we\\n                #   can consider it safe.\\n                return True\\n            else:\\n                # This link came from a trusted source, using the new API\\n                #   version, and it does not have a hash. It is NOT verifiable\\n                return False\\n        elif trusted is not None:\\n            # This link came from an untrusted source and we cannot trust it\\n            return False\\n\\n    @property\\n    def is_wheel(self):\\n        return self.ext == wheel_ext\\n\\n    @property\\n    def is_artifact(self):\\n        \\"\\"\\"\\n        Determines if this points to an actual artifact (e.g. a tarball) or if\\n        it points to an \\"abstract\\" thing like a path or a VCS location.\\n        \\"\\"\\"\\n        from pip.vcs import vcs\\n\\n        if self.scheme in vcs.all_schemes:\\n            return False\\n\\n        return True\\n\\n\\n# An object to represent the \\"link\\" for the installed version of a requirement.\\n# Using Inf as the url makes it sort higher.\\nINSTALLED_VERSION = Link(Inf)\\n\\n\\nFormatControl = namedtuple(\'FormatControl\', \'no_binary only_binary\')\\n\\"\\"\\"This object has two fields, no_binary and only_binary.\\n\\nIf a field is falsy, it isn\'t set. If it is {\':all:\'}, it should match all\\npackages except those listed in the other field. Only one field can be set\\nto {\':all:\'} at a time. The rest of the time exact package name matches\\nare listed, with any given package only showing up in one field at a time.\\n\\"\\"\\"\\n\\n\\ndef fmt_ctl_handle_mutual_exclude(value, target, other):\\n    new = value.split(\',\')\\n    while \':all:\' in new:\\n        other.clear()\\n        target.clear()\\n        target.add(\':all:\')\\n        del new[:new.index(\':all:\') + 1]\\n        if \':none:\' not in new:\\n            # Without a none, we want to discard everything as :all: covers it\\n            return\\n    for name in new:\\n        if name == \':none:\':\\n            target.clear()\\n            continue\\n        name = pkg_resources.safe_name(name).lower()\\n        other.discard(name)\\n        target.add(name)\\n\\n\\ndef fmt_ctl_formats(fmt_ctl, canonical_name):\\n    result = set([\\"binary\\", \\"source\\"])\\n    if canonical_name in fmt_ctl.only_binary:\\n        result.discard(\'source\')\\n    elif canonical_name in fmt_ctl.no_binary:\\n        result.discard(\'binary\')\\n    elif \':all:\' in fmt_ctl.only_binary:\\n        result.discard(\'source\')\\n    elif \':all:\' in fmt_ctl.no_binary:\\n        result.discard(\'binary\')\\n    return frozenset(result)\\n\\n\\ndef fmt_ctl_no_binary(fmt_ctl):\\n    fmt_ctl_handle_mutual_exclude(\\n        \':all:\', fmt_ctl.no_binary, fmt_ctl.only_binary)\\n\\n\\ndef fmt_ctl_no_use_wheel(fmt_ctl):\\n    fmt_ctl_no_binary(fmt_ctl)\\n    warnings.warn(\\n        \'--no-use-wheel is deprecated and will be removed in the future. \'\\n        \' Please use --no-binary :all: instead.\', DeprecationWarning,\\n        stacklevel=2)\\n\\n\\nSearch = namedtuple(\'Search\', \'supplied canonical formats\')\\n\\"\\"\\"Capture key aspects of a search.\\n\\n:attribute supplied: The user supplied package.\\n:attribute canonical: The canonical package name.\\n:attribute formats: The formats allowed for this package. Should be a set\\n    with \'binary\' or \'source\' or both in it.\\n\\"\\"\\"\\n"}\n'
train bpe ...
regroup and select data for training bpe in /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/train.with_comments.tok.NoneGB ...
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.4.tok
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.2.tok
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.7.tok
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.6.tok
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.5.tok
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.3.tok
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.1.tok
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.0.tok
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.4.tok
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.2.tok
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.7.tok
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.6.tok
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.5.tok
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.3.tok
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.1.tok
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.0.tok
training bpe on /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/train.with_comments.tok.NoneGB...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.4.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.2.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.7.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.6.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.5.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.3.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.1.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.0.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.4.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.2.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.7.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.6.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.5.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.3.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.1.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.0.tok ...
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.4.tok ...\nRead 25658 words (1957 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.4.tok ...\nModified 25658 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.2.tok ...\nRead 25509 words (2198 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.2.tok ...\nModified 25509 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.7.tok ...\nRead 37704 words (2164 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.7.tok ...\nModified 37704 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.6.tok ...\nRead 16071 words (1374 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.6.tok ...\nModified 16071 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.5.tok ...\nRead 37828 words (2369 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.5.tok ...\nModified 37828 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.3.tok ...\nRead 33001 words (2059 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.3.tok ...\nModified 33001 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.1.tok ...\nRead 29589 words (1349 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.1.tok ...\nModified 29589 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.0.tok ...\nRead 18838 words (1521 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.0.tok ...\nModified 18838 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.4.tok ...\nRead 10542 words (1151 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.4.tok ...\nModified 10542 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.2.tok ...\nRead 7862 words (1066 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.2.tok ...\nModified 7862 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.7.tok ...\nRead 14132 words (1370 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.7.tok ...\nModified 14132 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.6.tok ...\nRead 12125 words (1275 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.6.tok ...\nModified 12125 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.5.tok ...\nRead 15283 words (1514 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.5.tok ...\nModified 15283 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.3.tok ...\nRead 8419 words (1019 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.3.tok ...\nModified 8419 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.1.tok ...\nRead 17882 words (1496 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.1.tok ...\nModified 17882 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.0.tok ...\nRead 12083 words (1222 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.0.tok ...\nModified 12083 words from text file.\n'
stdout: b''
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/test.with_comments.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/test.with_comments.tok ...
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/test.with_comments.tok ...\nRead 35920 words (2108 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/test.with_comments.tok ...\nModified 35920 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/test.with_comments.tok ...\nRead 4276 words (656 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/test.with_comments.tok ...\nModified 4276 words from text file.\n'
stdout: b''
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/valid.with_comments.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/valid.with_comments.tok ...
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/valid.with_comments.tok ...\nRead 42489 words (2795 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/valid.with_comments.tok ...\nModified 42489 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/valid.with_comments.tok ...\nRead 6317 words (820 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/valid.with_comments.tok ...\nModified 6317 words from text file.\n'
stdout: b''
get vocab ...
regroup and select data in /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/train.with_comments.bpe.NoneGB to get vocab ...
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.2.bpe
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.1.bpe
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.7.bpe
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.3.bpe
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.5.bpe
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.6.bpe
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.4.bpe
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.0.bpe
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.2.bpe
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.4.bpe
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.7.bpe
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.6.bpe
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.0.bpe
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.1.bpe
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.3.bpe
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.5.bpe
computing vocab on /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/train.with_comments.bpe.NoneGB...
binarize train.with_comments.[0123456789].bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.2.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.1.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.7.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.3.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.5.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.6.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.4.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.0.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.2.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.4.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.7.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.6.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.0.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.1.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.3.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.5.bpe ...
stderr: b'INFO - 04/17/21 23:21:53 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:21:53 - 0:00:00 - 26576 words (14272 unique) in 9 sentences.\n'
stderr: b'INFO - 04/17/21 23:21:53 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:21:54 - 0:00:00 - 30074 words (14272 unique) in 9 sentences.\n'
stderr: b'INFO - 04/17/21 23:21:54 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:21:54 - 0:00:00 - 38367 words (14272 unique) in 9 sentences.\n'
stderr: b'INFO - 04/17/21 23:21:55 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:21:55 - 0:00:00 - 33606 words (14272 unique) in 9 sentences.\n'
stderr: b'INFO - 04/17/21 23:21:56 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:21:56 - 0:00:00 - 38678 words (14272 unique) in 9 sentences.\n'
stderr: b'INFO - 04/17/21 23:21:56 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:21:56 - 0:00:00 - 16468 words (14272 unique) in 9 sentences.\n'
stderr: b'INFO - 04/17/21 23:21:57 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:21:57 - 0:00:00 - 26318 words (14272 unique) in 9 sentences.\n'
stderr: b'INFO - 04/17/21 23:21:58 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:21:58 - 0:00:00 - 19303 words (14272 unique) in 9 sentences.\n'
stderr: b'INFO - 04/17/21 23:21:58 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:21:58 - 0:00:00 - 8127 words (14272 unique) in 20 sentences.\n'
stderr: b'INFO - 04/17/21 23:21:59 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:21:59 - 0:00:00 - 10758 words (14272 unique) in 20 sentences.\n'
stderr: b'INFO - 04/17/21 23:22:00 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:22:00 - 0:00:00 - 14535 words (14272 unique) in 20 sentences.\n'
stderr: b'INFO - 04/17/21 23:22:00 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:22:00 - 0:00:00 - 12424 words (14272 unique) in 20 sentences.\n'
stderr: b'INFO - 04/17/21 23:22:01 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:22:01 - 0:00:00 - 12391 words (14272 unique) in 20 sentences.\n'
stderr: b'INFO - 04/17/21 23:22:02 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:22:02 - 0:00:00 - 18204 words (14272 unique) in 20 sentences.\n'
stderr: b'INFO - 04/17/21 23:22:02 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:22:02 - 0:00:00 - 8685 words (14272 unique) in 20 sentences.\n'
stderr: b'INFO - 04/17/21 23:22:03 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:22:03 - 0:00:00 - 15657 words (14272 unique) in 20 sentences.\n'
binarize test.with_comments.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.test.with_comments.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.test.with_comments.bpe ...
stderr: b'INFO - 04/17/21 23:22:04 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:22:04 - 0:00:00 - 41998 words (14272 unique) in 10 sentences.\nINFO - 04/17/21 23:22:04 - 0:00:00 - 3296 unknown words (383 unique), covering 7.85% of the data.\nINFO - 04/17/21 23:22:04 - 0:00:00 - _@@: 183\nINFO - 04/17/21 23:22:04 - 0:00:00 - in@@: 84\nINFO - 04/17/21 23:22:04 - 0:00:00 - y@@: 67\nINFO - 04/17/21 23:22:04 - 0:00:00 - v@@: 64\nINFO - 04/17/21 23:22:04 - 0:00:00 - pro@@: 63\nINFO - 04/17/21 23:22:04 - 0:00:00 - dest@@: 62\nINFO - 04/17/21 23:22:04 - 0:00:00 - cache_@@: 61\nINFO - 04/17/21 23:22:04 - 0:00:00 - f@@: 56\nINFO - 04/17/21 23:22:04 - 0:00:00 - c@@: 54\nINFO - 04/17/21 23:22:04 - 0:00:00 - g_@@: 49\nINFO - 04/17/21 23:22:04 - 0:00:00 - n@@: 49\nINFO - 04/17/21 23:22:04 - 0:00:00 - g@@: 49\nINFO - 04/17/21 23:22:04 - 0:00:00 - t_@@: 48\nINFO - 04/17/21 23:22:04 - 0:00:00 - ache@@: 41\nINFO - 04/17/21 23:22:04 - 0:00:00 - C@@: 38\nINFO - 04/17/21 23:22:04 - 0:00:00 - re@@: 37\nINFO - 04/17/21 23:22:04 - 0:00:00 - i@@: 37\nINFO - 04/17/21 23:22:04 - 0:00:00 - ur@@: 34\nINFO - 04/17/21 23:22:04 - 0:00:00 - s@@: 31\nINFO - 04/17/21 23:22:04 - 0:00:00 - d@@: 29\nINFO - 04/17/21 23:22:04 - 0:00:00 - u@@: 28\nINFO - 04/17/21 23:22:04 - 0:00:00 - s_@@: 28\nINFO - 04/17/21 23:22:04 - 0:00:00 - old_@@: 26\nINFO - 04/17/21 23:22:04 - 0:00:00 - a@@: 26\nINFO - 04/17/21 23:22:04 - 0:00:00 - R@@: 26\nINFO - 04/17/21 23:22:04 - 0:00:00 - par@@: 26\nINFO - 04/17/21 23:22:04 - 0:00:00 - el@@: 26\nINFO - 04/17/21 23:22:04 - 0:00:00 - h@@: 25\nINFO - 04/17/21 23:22:04 - 0:00:00 - e@@: 24\nINFO - 04/17/21 23:22:04 - 0:00:00 - ter@@: 24\n'
stderr: b'INFO - 04/17/21 23:22:05 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:22:05 - 0:00:00 - 5125 words (14272 unique) in 10 sentences.\nINFO - 04/17/21 23:22:05 - 0:00:00 - 518 unknown words (137 unique), covering 10.11% of the data.\nINFO - 04/17/21 23:22:05 - 0:00:00 - t@@: 42\nINFO - 04/17/21 23:22:05 - 0:00:00 - c@@: 38\nINFO - 04/17/21 23:22:05 - 0:00:00 - y@@: 22\nINFO - 04/17/21 23:22:05 - 0:00:00 - z@@: 21\nINFO - 04/17/21 23:22:05 - 0:00:00 - in@@: 14\nINFO - 04/17/21 23:22:05 - 0:00:00 - r@@: 13\nINFO - 04/17/21 23:22:05 - 0:00:00 - e@@: 13\nINFO - 04/17/21 23:22:05 - 0:00:00 - on@@: 12\nINFO - 04/17/21 23:22:05 - 0:00:00 - am@@: 12\nINFO - 04/17/21 23:22:05 - 0:00:00 - di@@: 12\nINFO - 04/17/21 23:22:05 - 0:00:00 - k@@: 12\nINFO - 04/17/21 23:22:05 - 0:00:00 - t_@@: 11\nINFO - 04/17/21 23:22:05 - 0:00:00 - ful@@: 10\nINFO - 04/17/21 23:22:05 - 0:00:00 - j@@: 10\nINFO - 04/17/21 23:22:05 - 0:00:00 - s@@: 8\nINFO - 04/17/21 23:22:05 - 0:00:00 - i@@: 8\nINFO - 04/17/21 23:22:05 - 0:00:00 - od@@: 8\nINFO - 04/17/21 23:22:05 - 0:00:00 - ta@@: 7\nINFO - 04/17/21 23:22:05 - 0:00:00 - num@@: 6\nINFO - 04/17/21 23:22:05 - 0:00:00 - g@@: 6\nINFO - 04/17/21 23:22:05 - 0:00:00 - ical@@: 6\nINFO - 04/17/21 23:22:05 - 0:00:00 - categor@@: 6\nINFO - 04/17/21 23:22:05 - 0:00:00 - zen@@: 6\nINFO - 04/17/21 23:22:05 - 0:00:00 - sche@@: 6\nINFO - 04/17/21 23:22:05 - 0:00:00 - path@@: 5\nINFO - 04/17/21 23:22:05 - 0:00:00 - _@@: 5\nINFO - 04/17/21 23:22:05 - 0:00:00 - D@@: 5\nINFO - 04/17/21 23:22:05 - 0:00:00 - ry@@: 5\nINFO - 04/17/21 23:22:05 - 0:00:00 - el@@: 4\nINFO - 04/17/21 23:22:05 - 0:00:00 - 8@@: 4\n'
binarize valid.with_comments.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.valid.with_comments.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.valid.with_comments.bpe ...
stderr: b'INFO - 04/17/21 23:22:05 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:22:05 - 0:00:00 - 50409 words (14272 unique) in 10 sentences.\nINFO - 04/17/21 23:22:05 - 0:00:00 - 4422 unknown words (494 unique), covering 8.77% of the data.\nINFO - 04/17/21 23:22:05 - 0:00:00 - _@@: 284\nINFO - 04/17/21 23:22:05 - 0:00:00 - k@@: 135\nINFO - 04/17/21 23:22:05 - 0:00:00 - g@@: 118\nINFO - 04/17/21 23:22:05 - 0:00:00 - w@@: 97\nINFO - 04/17/21 23:22:05 - 0:00:00 - s@@: 83\nINFO - 04/17/21 23:22:05 - 0:00:00 - me@@: 72\nINFO - 04/17/21 23:22:05 - 0:00:00 - t_@@: 65\nINFO - 04/17/21 23:22:05 - 0:00:00 - ri@@: 62\nINFO - 04/17/21 23:22:05 - 0:00:00 - tr@@: 53\nINFO - 04/17/21 23:22:05 - 0:00:00 - R@@: 53\nINFO - 04/17/21 23:22:05 - 0:00:00 - l@@: 51\nINFO - 04/17/21 23:22:05 - 0:00:00 - O@@: 50\nINFO - 04/17/21 23:22:05 - 0:00:00 - c@@: 50\nINFO - 04/17/21 23:22:05 - 0:00:00 - r@@: 50\nINFO - 04/17/21 23:22:05 - 0:00:00 - d_@@: 47\nINFO - 04/17/21 23:22:05 - 0:00:00 - mp@@: 45\nINFO - 04/17/21 23:22:05 - 0:00:00 - t@@: 45\nINFO - 04/17/21 23:22:05 - 0:00:00 - IN@@: 43\nINFO - 04/17/21 23:22:05 - 0:00:00 - f@@: 42\nINFO - 04/17/21 23:22:05 - 0:00:00 - E@@: 42\nINFO - 04/17/21 23:22:05 - 0:00:00 - st@@: 41\nINFO - 04/17/21 23:22:05 - 0:00:00 - SG@@: 40\nINFO - 04/17/21 23:22:05 - 0:00:00 - o@@: 40\nINFO - 04/17/21 23:22:05 - 0:00:00 - al@@: 38\nINFO - 04/17/21 23:22:05 - 0:00:00 - Line@@: 36\nINFO - 04/17/21 23:22:05 - 0:00:00 - se@@: 35\nINFO - 04/17/21 23:22:05 - 0:00:00 - IP@@: 34\nINFO - 04/17/21 23:22:05 - 0:00:00 - x@@: 33\nINFO - 04/17/21 23:22:05 - 0:00:00 - ar@@: 32\nINFO - 04/17/21 23:22:05 - 0:00:00 - V@@: 31\n'
stderr: b'INFO - 04/17/21 23:22:06 - 0:00:00 - Read 14272 words from the vocabulary file.\n\nINFO - 04/17/21 23:22:06 - 0:00:00 - 7238 words (14272 unique) in 10 sentences.\nINFO - 04/17/21 23:22:06 - 0:00:00 - 505 unknown words (137 unique), covering 6.98% of the data.\nINFO - 04/17/21 23:22:06 - 0:00:00 - ti@@: 18\nINFO - 04/17/21 23:22:06 - 0:00:00 - S@@: 18\nINFO - 04/17/21 23:22:06 - 0:00:00 - n@@: 17\nINFO - 04/17/21 23:22:06 - 0:00:00 - v@@: 14\nINFO - 04/17/21 23:22:06 - 0:00:00 - re@@: 14\nINFO - 04/17/21 23:22:06 - 0:00:00 - T@@: 14\nINFO - 04/17/21 23:22:06 - 0:00:00 - D@@: 11\nINFO - 04/17/21 23:22:06 - 0:00:00 - TE@@: 11\nINFO - 04/17/21 23:22:06 - 0:00:00 - s@@: 10\nINFO - 04/17/21 23:22:06 - 0:00:00 - r@@: 10\nINFO - 04/17/21 23:22:06 - 0:00:00 - or_@@: 10\nINFO - 04/17/21 23:22:06 - 0:00:00 - um@@: 10\nINFO - 04/17/21 23:22:06 - 0:00:00 - C@@: 10\nINFO - 04/17/21 23:22:06 - 0:00:00 - id_@@: 9\nINFO - 04/17/21 23:22:06 - 0:00:00 - an@@: 9\nINFO - 04/17/21 23:22:06 - 0:00:00 - ab@@: 9\nINFO - 04/17/21 23:22:06 - 0:00:00 - c@@: 8\nINFO - 04/17/21 23:22:06 - 0:00:00 - y@@: 8\nINFO - 04/17/21 23:22:06 - 0:00:00 - Bar@@: 8\nINFO - 04/17/21 23:22:06 - 0:00:00 - amp@@: 8\nINFO - 04/17/21 23:22:06 - 0:00:00 - le@@: 8\nINFO - 04/17/21 23:22:06 - 0:00:00 - s_@@: 7\nINFO - 04/17/21 23:22:06 - 0:00:00 - ths: 7\nINFO - 04/17/21 23:22:06 - 0:00:00 - mo@@: 7\nINFO - 04/17/21 23:22:06 - 0:00:00 - d@@: 7\nINFO - 04/17/21 23:22:06 - 0:00:00 - u@@: 7\nINFO - 04/17/21 23:22:06 - 0:00:00 - W@@: 6\nINFO - 04/17/21 23:22:06 - 0:00:00 - a@@: 6\nINFO - 04/17/21 23:22:06 - 0:00:00 - ul@@: 6\nINFO - 04/17/21 23:22:06 - 0:00:00 - B@@: 6\n'r: split train, test and valid ... 
r: train for is 185 lines and 0.00042357295751571655 Go. 
extacting functions for /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.4.tok
python: split train, test and valid ... 
python: train for is 99 lines and 0.0014626001939177513 Go. 
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.4.tok
func: function ( ) { movie_data = read . csv ( file = movies . csv , header = TRUE , sep = ' , ' ) print ( ' Content  of  the  . csv  file : ' ' ) print ( movie_data ) }
func: function ( vector ) { result < - 0 for ( i in vector ) { result = result + i }
func: function ( vector ) { result < - 1 for ( i in vector ) { result = result * i }
func: function ( num ) { if ( num > 0 ) { print ( ' Positive  number ' ) }
func: function  for  writing  the  link  properties  in  interactive  Glimma  plots ENDCOM # '  @ param  from  the  index  of  the  plot  from  which  the  event  is  dispatched . ENDCOM # '  @ param  to  the  index  of  the  plot  which  receives  the  event  and  performs  an  action . ENDCOM # '  @ param  src  the  action  that  is  performed  in  the  " from "  plot . ENDCOM # '  @ param  dest  the  action  that  is  performed  in  the  " to "  plot . ENDCOM # '  @ param  flag  indicates  special  links  for  particular  chart  types . ENDCOM # '  @ param  both  creates  symmetric  links  whereby  the  " dest "  action  in  " to "  also  triggers  the  " src "  action  in  " from " . ENDCOM # '  @ param  info  additional  info  for  creating  the  link . ENDCOM # '  @ return  a  link  object  containing  the  plot  linking  information . ENDCOM # '  @ examples ENDCOM # '  data ( iris ) ENDCOM # '  data  < -  data . frame ( Name = paste ( " Flower " ,  1 : nrow ( iris ) ,  sep = " - " ) ,  iris ) ENDCOM # '  \donttest { ENDCOM # '  plot1  < -  glScatter ( data ,  xval = " Sepal . Length " ,  yval = " Sepal . Width " ,  colval = " Species " ) ENDCOM # '  plot2  < -  glScatter ( data ,  xval = " Species " ,  yval = " Petal . Length " ,  colval = " Species " ) ENDCOM # '  link1  < -  gllink ( 1 ,  2 ,  src = " hover " ,  dest = " hover " ,  both = TRUE ) ENDCOM # '  glimma ( plot1 ,  plot2 ,  link1 ,  layout = c ( 1,2 ) ) ENDCOM gllink < - function ( from , to , src = " none " , dest = " none " , flag = " none " , both = FALSE , info = " none " ) { out < - list ( ) if ( src != " none " & & dest == " none " ) { stop ( " src  cannot  be  defined  while  dest  is  ' none ' " ) }
func: function ( ) { jobs = system ( " pgrep  BayesTraits " , intern = T ) for ( n in 1 : length ( jobs ) ) { system ( paste ( " kill " , jobs [ n ] ) ) }
func: function ( data , layers = NULL , title = " " , maxpixels = 500000 , reduction = 1 , ... ) { if ( ! requireNamespace ( ' ggplot2' ) ) stop ( " ggplot2  package  required  for  plotting " ) if ( ! is . null ( layers ) ) data < - subset ( data , layers ) #  Theres  a  glitch  in  sampleRegular :  we  can ' t  just  simply  ask  for  sR ( data ,  number  of  cells ,  xy = TRUE )  as  it  doesn ' t  return  the  same  values ENDCOM #  for  xy  as  if  we  do  it  step  by  step :  x  < -  sR ( data , # cells , xy = FALSE , raster = TRUE ) ;  xyFromCell ( x , 1 : # cells ) .  I  don ' t  know  why  this  is ,  but  it ENDCOM #  causes  artifacts  in  the  final  plot  so  it  is  necessary  to  do  the  work  around  shown  here . . .  It  just  means  we  have  to  extract  the  data ENDCOM #  twice  so  it ' s  a  bit  slower . ENDCOM colNames < - names ( data ) data < - raster : : sampleRegular ( data , size = min ( raster : : ncell ( data ) / ( reduction * reduction ) , maxpixels ) , asRaster = TRUE ) #  Return  a  raster  so  we  can  extract  the  coords  afterwards . ENDCOM data < - data . frame ( raster : : xyFromCell ( data , 1 : raster : : ncell ( data ) ) , raster : : getValues ( data ) ) names ( data ) < - c ( ' x ' , ' y ' , colNames ) dat < - reshape ( data = data , direction = ' long ' , idvar = 1 : 2 , varying = 3 : dim ( data ) [ 2 ] , v . names = ' value ' , timevar = ' type ' , times = names ( data ) [ 3 : dim ( data ) [ 2 ] ] ) #  Massage  into  long  form ;  over  a  GB  for  a  full  tile ! ENDCOM #  Create  a  base  plot  object  with  some  useful  aesthetics  and  that  holds  the  data ;  then  if  this  is  not  the  only  plot  the  user  wants ,  they  should  be  able  to  output  more  without  needing  to  resample  the  data ENDCOM return ( ggplot2 : : ggplot ( ggplot2 : : aes ( x = ' x ' , y = ' y ' ) , data = ' dat ' , ... ) + ggplot2 : : theme ( axis . text . y = ggplot2 : : element_text ( angle = 90 , hjust = 0.5 ) ) + ggplot2 : : coord_equal ( ) + ggplot2 : : labs ( title = title , x = ' Latitude  ( UTM ) ' , y = ' Longitude  ( UTM ) ' , fill = ' Value ' ) ) }
func: function ( gp , layers , discrete , colours , labels = NULL , ... ) { #  Note :  ggplot  saves  the  data  in  the  object  which  both  allows  this ,  but  also  makes  for  a  large  object . . .  like  over  a  GB  for  a  full  tile  - -  I  recommend  against ENDCOM #  saving  the  R  database  on  exit  without  being  sure  you  have  time  for  this  file  to  save / load . ENDCOM if ( ! requireNamespace ( ' ggplot2' ) ) stop ( " ggplot2  package  required  for  plotting " ) if ( discrete ) { if ( is . null ( colours ) ) stop ( " plotTile :  if  discrete  is  specified ,  so  must  be  colours " ) }
func: function ENDCOM plotTiles < - function ( path , base , extension , models , type = ' pdf ' , ... ) { for ( fType in models ) { fName < - paste0 ( path , base , fType , extension ) Tile < - raster : : brick ( paste0 ( fName , ' . tif ' ) ) if ( raster : : nlayers ( Tile ) > 1 ) { names ( Tile ) < - c ( ' Class ' , ' Prob ' , paste0 ( ' Prob . ' , 1 : ( raster : : nlayers ( Tile ) - 2 ) ) ) }
func: function ( pack , update = FALSE ) { invisible ( sapply ( pack , function ( package ) { if ( ! require ( package , quietly = T , character . only = T ) ) { if ( length ( find ( " biocLite " ) ) < 1 ) { source ( " http : / / bioconductor . org / biocLite . R " ) }
func: function  I ' m  going  to  make  up  called ENDCOM #  increment ( ) .  Most  of  the  time  I  want  to  use  this  function  to  increase  the ENDCOM #  value  of  a  number  by  one .  This  function  will  take  two  arguments :  " number "  and ENDCOM #  " by "  where  " number "  is  the  digit  I  want  to  increment  and  " by "  is  the  amount  I ENDCOM #  want  to  increment  " number "  by .  I ' ve  written  the  function  below .  ENDCOM #  increment  < -  function ( number ,  by  =  1 ) { ENDCOM #  number  +  by ENDCOM #  If  you  take  a  look  in  between  the  parentheses  you  can  see  that  I ' ve  set ENDCOM #  " by "  equal  to  1 .  This  means  that  the  " by "  argument  will  have  the  default ENDCOM #  value  of  1 . ENDCOM #  I  can  now  use  the  increment  function  without  providing  a  value  for  " by " :  ENDCOM #  increment ( 5 )  will  evaluate  to  6 .  ENDCOM #  However  if  I  want  to  provide  a  value  for  the  " by "  argument  I  still  can !  The ENDCOM #  expression :  increment ( 5 ,  2 )  will  evaluate  to  7 .  ENDCOM #  You ' re  going  to  write  a  function  called  " remainder . "  remainder ( )  will  take ENDCOM #  two  arguments :  " num "  and  " divisor "  where  " num "  is  divided  by  " divisor "  and ENDCOM #  the  remainder  is  returned .  Imagine  that  you  usually  want  to  know  the  remainder ENDCOM #  when  you  divide  by  2 ,  so  set  the  default  value  of  " divisor "  to  2 .  Please  be ENDCOM #  sure  that  " num "  is  the  first  argument  and  " divisor "  is  the  second  argument . ENDCOM #  Hint  # 1 :  You  can  use  the  modulus  operator  % %  to  find  the  remainder . ENDCOM #  Ex :  7  % %  4  evaluates  to  3 .  ENDCOM #  Remember  to  set  appropriate  default  values !  Be  sure  to  save  this  ENDCOM #  script  and  type  submit ( )  in  the  console  after  you  write  the  function . ENDCOM remainder < - function ( num , divisor ) { #  Write  your  code  here ! ENDCOM #  Remember :  the  last  expression  evaluated  will  be  returned !  ENDCOM }
func: function ( xyz , zcol = 3 , xcol = 1 , ycol = 2 , tolerance = 10 * . Machine $ double . eps ) NEW_LINE { if ( ncol ( xyz ) < 3 ) stop ( " xyz  object  should  have  at  least  three  columns " ) z = xyz [ , zcol ] x = xyz [ , xcol ] y = xyz [ , ycol ] xx = sort ( unique ( x ) ) yy = sort ( unique ( y ) ) nx = length ( xx ) ny = length ( yy ) nmax = max ( nx , ny ) difx = diff ( xx ) if ( diff ( range ( unique ( difx ) ) ) > tolerance ) stop ( " x  intervals  are  not  constant " ) dify = diff ( yy ) if ( diff ( range ( unique ( dify ) ) ) > tolerance ) stop ( " y  intervals  are  not  constant " ) dx = mean ( difx ) dy = mean ( dify ) xmin = min ( xx ) xmax = max ( xx ) xrange = xmax - xmin ymin = min ( yy ) ymax = max ( yy ) yrange = ymax - ymin row = round ( ( x - xmin ) / dx ) + 1 col = round ( ( y - ymin ) / dy ) + 1 zz = rep ( as . numeric ( NA ) , nx * ny ) zz [ row + nx * ( col - 1 ) ] = z zz = matrix ( zz , nrow = nx , ncol = ny ) list ( x = seq ( xmin , xmax , dx ) , y = seq ( ymin , ymax , dy ) , z = zz ) }
func: function  to  see  the  entire  list  of  TCGA  CDEs  that  may  be  queried  via  this  method .  For  more  information  on  how  clinical  data  are  processed ,  see  our  < a  href = " https : / / confluence . broadinstitute . org / display / GDAC / Documentation # Documentation - ClinicalPipeline " > pipeline  documentation < / a > . ENDCOM # '  @ param  format  Format  of  result .  Default  value  is  json .  While  json , tsv , csv  are  available .  ENDCOM # '  @ param  cohort  Narrow  search  to  one  or  more  TCGA  disease  cohorts  from  the  scrollable  list .  Multiple  values  are  allowed  ACC , BLCA , BRCA , CESC , CHOL , COAD , COADREAD , DLBC , ESCA , FPPP , GBM , GBMLGG , HNSC , KICH , KIPAN , KIRC , KIRP , LAML , LGG , LIHC , LUAD , LUSC , MESO , OV , PAAD , PCPG , PRAD , READ , SARC , SKCM , STAD , STES , TGCT , THCA , THYM , UCEC , UCS , UVM . ENDCOM # '  @ param  tcga _ participant _ barcode  Comma  separated  list  of  TCGA  participant  barcodes  ( e . g .  TCGA - GF - A4EO ) .  Multiple  values  are  allowed  . ENDCOM # '  @ param  cde _ name  Retrieve  results  only  for  specified  CDEs ,  per  the  Metadata / ClinicalNames  function  Multiple  values  are  allowed  . ENDCOM # '  @ param  page  Which  page  ( slice )  of  entire  results  set  should  be  returned .  Multiple  values  are  allowed  .  Default  value  is  1 .  ENDCOM # '  @ param  page _ size  Number  of  records  per  page  of  results .  Max  is  2000 .  Multiple  values  are  allowed  .  Default  value  is  150 .  ENDCOM # '  @ param  sort _ by  Which  column  in  the  results  should  be  used  for  sorting  paginated  results ?  Default  value  is  cohort .  While  tcga _ participant _ barcode , cohort , cde _ name  are  available .  ENDCOM # '  @ export ENDCOM Samples . Clinical = function ( format = " json " , cohort = " " , tcga_participant_barcode = " " , cde_name = " " , page = "1" , page_size = "150" , sort_by = " cohort " ) { parameters = list ( format = format , cohort = cohort , tcga_participant_barcode = tcga_participant_barcode , cde_name = cde_name , page = page , page_size = page_size , sort_by = sort_by ) to . Validate = c ( " cohort " , " tcga _ participant _ barcode " , " cde _ name " ) validate . Parameters ( params = parameters , to . Validate = to . Validate ) url = build . Query ( parameters = parameters , invoker = " Samples " , method = " Clinical " ) ret = download . Data ( url , format , page ) return ( ret ) }
func: function ( X , Y , FUN = " * " , ... ) NEW_LINE { if ( is . array ( X ) ) { dX < - dim ( X ) nx < - dimnames ( X ) no . nx < - is . null ( nx ) }
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.2.tok
func: function ( url ) { path = curlRequestUrlToList ( url ) relations = list ( ) if ( length ( path ) > 0 ) { for ( i in 1 : length ( path ) ) { start = curlRequestUrlToList ( path [ [ i ] ] $ start ) end = curlRequestUrlToList ( path [ [ i ] ] $ end ) type = path [ [ i ] ] $ type dataSource = path [ [ i ] ] $ data $ source startGID = start $ data $ GID startName = start $ data $ name startXref = paste0 ( start $ data $ xref , collapse = " | | " ) startLabel = start $ metadata $ labels [ [ 1 ] ] endGID = end $ data $ GID endName = end $ data $ name endXref = paste0 ( end $ data $ xref , collapse = " | | " ) endLabel = end $ metadata $ labels [ [ 1 ] ] # #  Set  the  name  for  the  class ENDCOM relation = data . frame ( startGID = startGID , startName = startName , startXref = startXref , startLabel = startLabel , endGID = endGID , endName = endName , endXref = endXref , endLabel = endLabel , type = type , dataSource = dataSource ) relations = rbind ( relations , relation ) }
func: function ( ) { print ( nld ) }
func: function ( ) { nums = c ( 10 , 20 , 30 , 40 , 50 , 60 ) print ( ' Original  vector : ' ) print ( nums ) print ( paste ( ' Maximum  value  of  the  said  vector : ' , max ( nums ) ) ) print ( paste ( ' Minimum  value  of  the  said  vector : ' , min ( nums ) ) ) }
func: function ( path ) { path < - check_file ( path ) ext < - tolower ( tools : : file_ext ( path ) ) switch ( excel_format ( path ) , xls = xls_sheets ( path ) , xlsx = xlsx_sheets ( path ) ) }
func: function  is  used  for  the  side  effect  of  ENDCOM # '  starting  the  ShinyStan  app ,  but  it  also  returns  a  shinystan  object ,  an ENDCOM # '  instance  of  S4  class  \code { " shinystan " }
func: function ( object , rstudio = getOption ( " shinystan . rstudio " ) , ... ) { if ( is . shinystan ( object ) ) { sso_check ( object ) }
func: function ( demo_name = " eight _ schools " , rstudio = getOption ( " shinystan . rstudio " ) , ... ) { demo_name < - match . arg ( demo_name ) demo_object < - get ( demo_name ) invisible ( launch ( demo_object , rstudio = rstudio , ... ) ) }
func: function  ENDCOM #  @ param  sso  shinystan  object ENDCOM #  @ param  rstudio  launch  in  rstudio  viewer  instead  of  web  browser ?  ENDCOM #  @ param  . . .  passed  to  shiny : : runApp ENDCOM launch < - function ( sso , rstudio = FALSE , ... ) { launch . browser < - if ( ! rstudio ) TRUE else getOption ( " shiny . launch . browser " , interactive ( ) ) . sso_env $ . SHINYSTAN_OBJECT < - sso #  see  zzz . R  for  . sso _ env ENDCOM on . exit ( . sso_env $ . SHINYSTAN_OBJECT < - NULL , add = TRUE ) shiny : : runApp ( system . file ( " ShinyStan " , package = " shinystan " ) , launch . browser = launch . browser , ... ) }
func: function ( ) { tryCatch ( swirl : : : swirl_courses_dir ( ) , error = function ( c ) { file . path ( find . package ( " swirl " ) , " Courses " ) }
func: function ( fileName ) { mypath < - file . path ( . get_course_path ( ) , " Exploratory _ Data _ Analysis " , " CaseStudy " , fileName ) }
func: function ( fileName ) { mypath < - pathtofile ( fileName ) file . copy ( mypath , fileName ) }
func: function ( iname ) { par ( mfrow = c ( 1 , 1 ) ) par ( mar = c ( 8 , 10 , 8 , 10 ) ) image ( t ( iname ) [ , nrow ( iname ) : 1 ] ) }
func: function ( fname ) { # fxfer ( fname ) ENDCOM # file . edit ( fname ) ENDCOM mypath < - pathtofile ( fname ) file . edit ( mypath ) }
func: function ( x , y , cx , cy ) { distTmp < - matrix ( NA , nrow = 3 , ncol = 12 ) distTmp [ 1 , ] < - ( x - cx [ 1 ] ) ^ 2 + ( y - cy [ 1 ] ) ^ 2 distTmp [ 2 , ] < - ( x - cx [ 2 ] ) ^ 2 + ( y - cy [ 2 ] ) ^ 2 distTmp [ 3 , ] < - ( x - cx [ 3 ] ) ^ 2 + ( y - cy [ 3 ] ) ^ 2 return ( distTmp ) }
func: function ( cv ) { myarg < - deparse ( substitute ( cv ) ) z < - outer ( 1 : 20 , 1 : 20 , " + " ) obj < - list ( x = 1 : 20 , y = 1 : 20 , z = z ) image ( obj , col = cv , main = myarg ) }
func: function ( input , output ) { #  Expression  that  generates  a  histogram .  The  expression  is ENDCOM #  wrapped  in  a  call  to  renderPlot  to  indicate  that : ENDCOM #  1 )  It  is  " reactive "  and  therefore  should  be  automatically ENDCOM #  re - executed  when  inputs  change ENDCOM #  2 )  Its  output  type  is  a  plot ENDCOM output $ distPlot < - renderPlot ( { x < - faithful [ , 2 ] #  Old  Faithful  Geyser  data ENDCOM bins < - seq ( min ( x ) , max ( x ) , length . out = input $ bins + 1 ) #  draw  the  histogram  with  the  specified  number  of  bins ENDCOM hist ( x , breaks = bins , col = ' darkgray ' , border = ' white ' ) }
func: function ( n ) { if ( n >= 2 ) { x = seq ( 2 , n ) prime_nums = c ( ) for ( i in seq ( 2 , n ) ) { if ( any ( x == i ) ) { prime_nums = c ( prime_nums , i ) x = c ( x [ ( x % % i ) != 0 ] , i ) }
func: function ENDCOM convert_to_binary < - function ( n ) { if ( n > 1 ) { convert_to_binary ( as . integer ( n / 2 ) ) }
func: function ( X , frequency = 7 ) { if ( is . vector ( X ) & ! is . data . frame ( X ) ) X = data . frame ( y = X ) ma . ts = do . call ( ' rbind ' , apply ( X , 2 , function ( j ) { j . matrix = matrix ( j , nrow = frequency ) means = apply ( j . matrix [ , 1 : ( ncol ( j . matrix ) - 1 ) ] , 1 , mean ) sds = apply ( j . matrix [ , 1 : ( ncol ( j . matrix ) - 1 ) ] , 1 , sd ) upperbounds = means + 1.6 * sds lowerbounds = means - 1.6 * sds anomalous = t ( apply ( cbind ( upperbounds , lowerbounds , j . matrix ) , 1 , function ( i ) { i [ - ( 1 : 2 ) ] > i [ 1 ] | i [ - ( 1 : 2 ) ] < i [ 2 ] }
func: function ( anomalyDetection ) { ggplot ( anomalyDetection , aes ( x = time , y = X ) ) + geom_line ( size = 1 ) + geom_point ( data = subset ( anomalyDetection , anomaly == T ) , color = ' red ' , size = 6 ) + facet_wrap ( ~ name , scale = ' free ' ) }
func: function ( x ) { pat < - " ^ ( [ ^ \\ ( [ : space : ] ] + ) [ [ : space : ] ] * \\ ( ( [ ^ \\ ) ] + ) \\ ) . * " x1 < - sub ( pat , " \\1" , x ) x2 < - sub ( pat , " \\2" , x ) if ( x2 != x1 ) { pat < - " [ [ : space : ] ] * ( [ [ < > = ! ] + ) [ [ : space : ] ] + ( . * ) " version < - sub ( pat , " \\2" , x2 ) if ( ! grepl ( " ^ r " , version ) ) version < - package_version ( version ) list ( name = x1 , op = sub ( pat , " \\1" , x2 ) , version = version ) }
func: function ( x ) { if ( ! length ( x ) ) return ( list ( ) ) x < - unlist ( strsplit ( x , " , " ) ) x < - sub ( " [ [ : space : ] ] + $ " , " " , x ) x < - unique ( sub ( " ^ [ [ : space : ] ] * ( . * ) " , " \\1" , x ) ) names ( x ) < - sub ( " ^ ( [ [ : alnum : ] . ] + ) . * $ " , " \\1" , x ) lapply ( x , split_op_version ) }
func: function ( val ) { if ( is . na ( val ) ) return ( character ( 0 ) ) val < - names ( split_dependencies ( val ) ) if ( is . null ( val ) ) return ( character ( 0 ) ) val < - val [ ! val % in % " R " ] if ( length ( val ) ) return ( val ) return ( character ( 0 ) ) }
func: function ( sex ) { idb1 ( " US " , yrs , sex = sex ) % > % mutate ( sex_label = sex ) }
func: function ( ) {  return  Math . abs ( this . value )  /  1000000  +  ' M ' ;  }
func: function  ( )  {  return  ' < b > '  +  this . series . name  +  ' ,  age  '  +  this . point . category  +  ' < / b > < br / > '  +  ' Population :  '  +  Highcharts . numberFormat ( Math . abs ( this . point . y ) ,  0 ) ; }
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.7.tok
func: function ( x ) { n < - nrow ( x ) key < - " sp500 _ R " if ( redisExists ( key ) ) redisDelete ( key ) M < - cbind ( as . numeric ( index ( x ) ) , coredata ( x ) ) for ( i in 1 : n ) { redisRPush ( key , M [ i , , drop = TRUE ] ) }
func: function ( x ) { n < - nrow ( x ) key < - " sp500 _ C " if ( redisExists ( key ) ) redisDelete ( key ) M < - cbind ( as . numeric ( index ( x ) ) , coredata ( x ) ) for ( i in 1 : n ) { redis $ listRPush ( key , M [ i , ] ) }
func: function ( x , v , maxit , reorthog = 0 , upper . bound . prob = NULL ) { stopifnot ( is . numeric ( x ) | inherits ( x , " CsparseMatrix " ) ) stopifnot ( is . numeric ( v ) ) reorthog < - as . integer ( reorthog ) if ( inherits ( x , " CsparseMatrix " ) ) { . Call ( " GKLBidiagSparse " , A = x , v = v , k = maxit , reorthog = reorthog , PACKAGE = " rfunctions " ) }
func: function ( num ) { msg1 < - ' Invalid  measurement ' msg2 < - ' Area  of  the  Square  is :  ' if ( num <= 0 ) { print ( msg1 ) }
func: function ( P = NULL , X = NULL ) { # #  2 - column - matrix  of  home  objects  ( ' obj ' )  and  their  owners  ( ' ind ' ) ENDCOM Y < - data . frame ( ind = NULL , obj = NULL ) for ( z in 1 : length ( unique ( X $ obj ) ) ) { # #  1 .  Find  cycle ENDCOM Cycle < - findCycle ( P = P , X = X ) # #  2 .  Add  objects  in  this  cycle  to  ' home  territory ' ENDCOM Y < - rbind ( Y , Cycle ) # #  3 .  Remove  objects  in  this  cycle  from  tradable  objects ENDCOM X < - X [ - which ( X $ obj % in % Cycle $ obj ) , ] for ( i in 1 : length ( P ) ) { P [ [ i ] ] < - P [ [ i ] ] [ ! P [ [ i ] ] % in % Y $ ind ] }
func: function ( P = NULL , X = NULL ) { Cycle < - data . frame ( ind = NA , obj = NA ) thisind < - X $ ind [ 1 ] #  start  with  first  individual  in  line ENDCOM for ( j in 1 : length ( unique ( X $ ind ) ) ) { Cycle [ j , ] < - c ( thisind , P [ [ thisind ] ] [ 1 ] ) #  id  and  top - ranked  object  of  the  individual  in  line ENDCOM thisind < - X [ X $ obj == P [ [ thisind ] ] [ 1 ] , " ind " ] #  individual  whose  object  is  requested ENDCOM if ( Cycle [ j , 1 ] == Cycle [ j , 2 ] ) { #  if  individual  points  to  own  object ENDCOM return ( Cycle [ j , ] ) break }
func: function ,  please  cite  OncodriveCLUST  article . ENDCOM # '  @ references  Tamborero  D ,  Gonzalez - Perez  A  and  Lopez - Bigas  N .  OncodriveCLUST :  exploiting  the  positional  clustering  of  somatic  mutations  to  identify  cancer  genes .  Bioinformatics .  2013 ;  doi :  10.1093 / bioinformatics / btt395s ENDCOM # '  @ param  maf  an  \code { \link { MAF }
func: function ( maf , AACol = NULL , minMut = 5 , pvalMethod = ' zscore ' , nBgGenes = 100 , bgEstimate = TRUE , ignoreGenes = NULL ) { # Proetin  Length  source ENDCOM gl = system . file ( ' extdata ' , ' prot _ len . txt . gz ' , package = ' maftools ' ) if ( Sys . info ( ) [ [ ' sysname ' ] ] == ' Windows ' ) { gl . gz = gzfile ( description = gl , open = ' r ' ) gl < - suppressWarnings ( data . table ( read . csv ( file = gl . gz , header = TRUE , sep = ' \t ' , stringsAsFactors = FALSE ) ) ) close ( gl . gz ) }
func: function ( x ) { poisson . test ( as . numeric ( x [ observed_mut_colIndex ] ) , as . numeric ( x [ expected_mut_colIndex ] ) ) $ p . value }
func: function  of  number  of  clusters  and  protein  length .  Calculate  expected  number  of  events  based  on  poisson  distribution . ENDCOM nonsyn . scores = merge ( getGeneSummary ( maf ) , nonsyn . scores , by = ' Hugo _ Symbol ' ) nonsyn . scores [ , fract_muts_in_clusters : = muts_in_clusters / total ] counts . glm = glm ( formula = total ~ protLen + clusters , family = poisson ( link = identity ) , data = nonsyn . scores ) # Poisson  model ENDCOM nonsyn . scores $ Expected = counts . glm $ fitted . values # Get  expected  number  of  events  ( mutations )  from  the  model ENDCOM observed_mut_colIndex = which ( colnames ( nonsyn . scores ) == ' total ' ) expected_mut_colIndex = which ( colnames ( nonsyn . scores ) == ' Expected ' ) # Poisson  test  to  caluclate  difference  ( p - value ) ENDCOM nonsyn . scores $ pval = apply ( nonsyn . scores , 1 , function ( x ) { poisson . test ( as . numeric ( x [ observed_mut_colIndex ] ) , as . numeric ( x [ expected_mut_colIndex ] ) ) $ p . value }
func: function ( scaffold . stats , #  scaff  table  from  gbt  object ENDCOM marker . list , #  markTab  table  from  gbt  object ENDCOM taxon , #  Taxonomic  level  to  do  the  coloring ENDCOM consensus , #  Logical - if  taxon  assgs  conflict ,  take  consens ? ENDCOM weightCutoff #  Cutoff  quantile  for  assigning  colors  ( between  0  and  1 ) ENDCOM ) { #  This  took  a  very  long  time  to  get  it  right ENDCOM # #  Generates  colors  for  marker  gene  phylotypes  in  plot ENDCOM # #  Merge  tables  to  have  points  to  plot  for  the  markers  # # # # # ENDCOM marker . stats < - mergeScaffMarker ( scaffold . stats , marker . list , taxon , consensus ) # #  Calculate  total  weight  for  each  taxon  # # # # # ENDCOM taxon . agg < - aggregate ( marker . stats $ Length * marker . stats $ Avg_fold , by = list ( marker . stats $ taxon ) , FUN = sum ) total . weight < - sum ( taxon . agg $ x ) taxon . agg . order < - taxon . agg [ order ( taxon . agg $ x , decreasing = TRUE ) , ] #  Sort  descending ENDCOM # #  Make  list  of  top  taxa  # # # # # ENDCOM if ( weightCutoff > 1 | | weightCutoff < 0 ) { #  Catch  errors  for  weightCutoff ENDCOM cat ( " gbtools  ERROR :  weightCutoff  parameter  must  be  between  0  and  1 \n " ) }
func: function ( n ) { rev = 0 num = n while ( n > 0 ) { r = n % % 10 rev = rev * 10 + r n = n % / % 10 }
func: function ( ) { return ( bunyan_globals $ errorssincemark ) }
func: function ( a ) { b = a * a ; return ( b ) ; }
func: function ( X , npc . max = ncol ( X ) ) { error1 < - matrix ( NaN , nrow = dim ( X ) [ 1 ] , ncol = min ( dim ( X ) [ 2 ] , npc . max ) ) error2 < - matrix ( NaN , nrow = dim ( X ) [ 1 ] , ncol = min ( dim ( X ) [ 2 ] , npc . max ) ) error3 < - matrix ( NaN , nrow = dim ( X ) [ 1 ] , ncol = min ( dim ( X ) [ 2 ] , npc . max ) ) for ( n in 1 : dim ( X ) [ 1 ] ) { Xtrain = X [ - n , ] Xtrain = scale ( Xtrain , center = TRUE , scale = FALSE ) V = svd ( Xtrain ) $ v Xtest = X [ n , , drop = FALSE ] Xtest = scale ( Xtest , center = attr ( Xtrain , " scaled : center " ) , scale = FALSE ) for ( j in 1 : min ( dim ( V ) [ 2 ] , npc . max ) ) { P = V [ , 1 : j ] % * % t ( V [ , 1 : j ] ) err1 < - Xtest % * % ( diag ( length ( diag ( P ) ) ) - P ) err2 < - Xtest % * % ( diag ( length ( diag ( P ) ) ) - P + diag ( diag ( P ) ) ) err3 < - array ( NaN , dim = dim ( Xtest ) ) for ( k in 1 : dim ( Xtest ) [ 2 ] ) { proj = Xtest [ , - k ] % * % t ( expmat ( V [ - k , 1 : j ] ) ) % * % t ( V [ , 1 : j ] ) err3 [ k ] = Xtest [ k ] - proj [ k ] }
func: function ( this . fold ) { #  prepare  data  for  training ENDCOM test . data < - PREPARE . MODEL . DATA ( train . raw [ this . fold , ] ) test . data $ ID < - train . raw [ this . fold , ID ] train . data < - PREPARE . MODEL . DATA ( train . raw [ - this . fold , ] ) if ( FRACTION . TRAIN . DATA != 1 ) { #  extract  subset  for  inital  training ENDCOM set . seed ( 29 ) idx < - createDataPartition ( train . data $ response , p = FRACTION . TRAIN . DATA , list = FALSE ) train . data $ predictors < - train . data $ predictors [ idx , ] train . data $ response < - train . data $ response [ idx ] }
func: function ENDCOM #  put  response  as  first  column  in  data  set ENDCOM write . table ( cbind ( response = train . data $ response , train . data $ predictors ) , file = paste0 ( WORK . DIR , " / py _ train . tsv " ) , row . names = FALSE , sep = " \t " ) #  invoke  Python  training  model ENDCOM python . train . command < - paste ( PYTHON_COMMAND , paste0 ( WORK . DIR , " / train _ model . py " ) , WORK . DIR ) Sys . time ( ) time . data < - system . time ( system ( python . train . command ) ) time . data #  stopCluster ( cl ) ENDCOM #  prepare  data  for  training ENDCOM write . table ( test . data $ predictors , file = paste0 ( WORK . DIR , " / py _ test . tsv " ) , row . names = FALSE , sep = " \t " ) #  execute  Python  prediction  code ENDCOM python . test . command < - paste ( PYTHON_COMMAND , paste0 ( WORK . DIR , " / make _ prediction . py " ) , WORK . DIR , " possible _ model " , " py _ test . tsv " , " py _ test _ predictions . tsv " ) system ( python . test . command ) #  get  predictions  from  Python  model ENDCOM pred . probs < - fread ( paste0 ( WORK . DIR , " / py _ test _ predictions . tsv " ) , sep = " \t " ) score < - logLossEval ( pred . probs [ , Class_1 ] , test . data $ response ) score #  clean  up  files  no  longer  needed ENDCOM file . remove ( c ( paste0 ( WORK . DIR , " / py _ train . tsv " ) , paste0 ( WORK . DIR , " / py _ test . tsv " ) , paste0 ( WORK . DIR , " / possible _ model " ) , paste0 ( WORK . DIR , " / py _ test _ predictions . tsv " ) ) ) ans < - list ( score = score , level1 . features = data . frame ( ID = test . data $ ID , pred . probs , response = test . data $ response ) ) return ( ans ) }
func: function ( x ) { x $ score }
func: function ( x ) { x $ level1 . features }
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.6.tok
func: function  for  a  complete  description  of  sample  types . ENDCOM # '  @ param  format  Format  of  result .  Default  value  is  json .  While  json , tsv , csv  are  available .  ENDCOM # '  @ param  date  Select  one  or  more  date  stamps .  Multiple  values  are  allowed  2016_01_28,2015_11_01,2015_08_21,2015_06_01,2015_04_02,2015_02_04,2014_12_06,2014_10_17,2014_09_02,2014_07_15,2014_05_18,2014_04_16,2014_03_16 .  Default  value  is  2016_01_28 .  ENDCOM # '  @ param  cohort  Narrow  search  to  one  or  more  TCGA  disease  cohorts  from  the  scrollable  list .  Multiple  values  are  allowed  ACC , BLCA , BRCA , CESC , CHOL , COAD , COADREAD , DLBC , ESCA , FPPP , GBM , GBMLGG , HNSC , KICH , KIPAN , KIRC , KIRP , LAML , LGG , LIHC , LUAD , LUSC , MESO , OV , PAAD , PCPG , PRAD , READ , SARC , SKCM , STAD , STES , TGCT , THCA , THYM , UCEC , UCS , UVM . ENDCOM # '  @ param  sample _ type  Narrow  search  to  one  or  more  TCGA  sample  types  from  the  scrollable  list .  Multiple  values  are  allowed  FFPE , NB , NBC , NBM , NT , TAM , TAP , TB , TM , TP , TR , Tumor . ENDCOM # '  @ param  data _ type  Narrow  search  to  one  or  more  TCGA  data  types  from  the  scrollable  list .  Multiple  values  are  allowed  bcr , clinical , cn , lowp , methylation , mrna , mrnaseq , mir , mirseq , rppa , maf , rawmaf . ENDCOM # '  @ param  totals  Output  an  entry  providing  the  totals  for  each  data  type .  Default  value  is  TRUE .  While  are  available .  ENDCOM # '  @ param  sort _ by  Which  column  in  the  results  should  be  used  for  sorting  paginated  results ?  Default  value  is  cohort .  While  cohort  are  available .  ENDCOM # '  @ export ENDCOM Metadata . Counts = function ( format = " json " , date = "2016_01_28" , cohort = " " , sample_type = " " , data_type = " " , totals = " TRUE " , sort_by = " cohort " ) { parameters = list ( format = format , date = date , cohort = cohort , sample_type = sample_type , data_type = data_type , totals = totals , sort_by = sort_by ) validate . Parameters ( params = parameters ) url = build . Query ( parameters = parameters , invoker = " Metadata " , method = " Counts " ) ret = download . Data ( url , format ) return ( ret ) }
func: function ( n ) { num = n if ( ( num % % 2 ) == 0 ) { print ( paste ( num , ' is  Even ' ) ) }
func: function ( cl , zlmfit , R = 99 ) NEW_LINE { sca < - zlmfit @ sca N < - ncol ( sca ) LMlike < - zlmfit @ LMlike parallel : : clusterEvalQ ( cl , require ( MAST ) ) # #  clusterEvalQ ( cl , require ( abind ) ) ENDCOM parallel : : clusterExport ( cl , " N " , envir = environment ( ) ) parallel : : clusterExport ( cl , " LMlike " , envir = environment ( ) ) parallel : : clusterExport ( cl , " sca " , envir = environment ( ) ) manyvc < - parallel : : parSapply ( cl , 1 : R , function ( i , ... ) { s < - sample ( N , replace = TRUE ) newsca < - sca [ , s ] LMlike < - update ( LMlike , design = colData ( newsca ) ) zlm . SingleCellAssay ( sca = newsca , LMlike = LMlike , onlyCoef = TRUE ) }
func: function ( zlmfit , R = 99 ) { sca < - zlmfit @ sca N < - ncol ( sca ) LMlike < - zlmfit @ LMlike manyvc < - raply ( R , { s < - sample ( N , replace = TRUE ) newsca < - sca [ , s ] LMlike < - update ( LMlike , design = colData ( newsca ) ) zlm . SingleCellAssay ( sca = newsca , LMlike = LMlike , onlyCoef = TRUE ) }
func: function ( design , test ) { ' Update the latent trait ( theta ) values using information from the design and test objects ' responses2 < - responses responses2 [ design @ items_not_scored ] < - NA if ( score ) { method < - design @ method if ( last_item ( items_answered ) % in % design @ items_not_scored ) method < - ' fixed ' if ( method == ' ML ' ) { if ( length ( unique ( na . omit ( responses2 ) ) ) < 2 L ) method < - ' MAP ' }
func: function ( x , thetas ) FI ( extract . item ( test @ mo , x ) , Theta = thetas ) , thetas = thetas ) tmp < - matrix ( 0 , nrow ( infos [ [ 1 L ] ] ) , ncol ( infos [ [ 1 L ] ] ) ) for ( i in 1 L : length ( infos ) ) tmp < - tmp + infos [ [ i ] ] if ( design @ criteria % in % c ( ' DPrule ' , ' TPrule ' , ' EPrule ' , ' WPrule ' , ' APrule ' ) ) tmp < - tmp + solve ( test @ gp $ gcov ) info_thetas << - tmp }
func: function ( ) { tryCatch ( swirl : : : swirl_courses_dir ( ) , error = function ( c ) { file . path ( find . package ( " swirl " ) , " Courses " ) }
func: function ( ... ) { e < - get ( " e " , parent . frame ( ) ) any ( sapply ( c ( ... ) , function ( expr ) omnitest ( expr ) ) ) }
func: function ( correctExpr ) { e < - get ( " e " , parent . frame ( ) ) #  Do  what  the  user  should  have  done ENDCOM eSw < - cleanEnv ( e $ snapshot ) mdlSw < - eval ( parse ( text = correctExpr ) , eSw ) #  Recreate  what  the  user  has  done ENDCOM eUsr < - cleanEnv ( e $ snapshot ) mdlUsr < - eval ( e $ expr , eUsr ) #  If  the  correct  model  is  named : ENDCOM if ( length ( ls ( eSw ) ) > 0 ) { #  Check  whether  the  model ' s  name  is  correct ENDCOM nameGood < - sum ( ls ( eUsr ) % in % ls ( eSw ) ) & sum ( ls ( eSw ) % in % ls ( eUsr ) ) #  If  not ,  highlight  the  misspelling ENDCOM if ( ! nameGood ) { swirl_out ( paste0 ( " You  seem  to  have  misspelled  the  model ' s  name .  I  was  expecting  " , ls ( eSw ) , "  but  you  apparently  typed  " , ls ( eUsr ) , " . " ) ) return ( FALSE ) }
func: function ( ) { #  Whenever  swirl  is  running ,  its  callback  is  at  the  top  of  its  call  stack . ENDCOM #  Swirl ' s  state ,  named  e ,  is  stored  in  the  environment  of  the  callback . ENDCOM environment ( sys . function ( 1 ) ) $ e }
func: function ( ) { getState ( ) $ val }
func: function ( ) { getState ( ) $ expr }
func: function ( ) { selection < - getState ( ) $ val if ( selection == " Yes " ) { email < - readline ( " What  is  your  email  address ?  " ) token < - readline ( " What  is  your  assignment  token ?  " ) payload < - sprintf ( ' { " assignmentKey " : " iGMF3K8wEeWVdAqQVb1YyQ " , " submitterEmail " : " % s " , " secret " : " % s " , " parts " : { " Dxjqj " : { " output " : " correct " }
func: function ( url ) { out < - tryCatch ( { path = curlRequestUrlToList ( url ) start = curlRequestUrlToList ( path $ start ) end = curlRequestUrlToList ( path $ end ) type = path $ type dataSource = path $ data $ source startGID = start $ data $ GID startName = start $ data $ name startXref = paste0 ( start $ data $ xref , collapse = " | | " ) startLabel = start $ metadata $ labels [ [ 1 ] ] endGID = end $ data $ GID endName = end $ data $ name endXref = paste0 ( end $ data $ xref , collapse = " | | " ) endLabel = end $ metadata $ labels [ [ 1 ] ] # #  Set  the  name  for  the  class ENDCOM relation = list ( startGID = startGID , startName = startName , startXref = startXref , startLabel = startLabel , endGID = endGID , endName = endName , endXref = endXref , endLabel = endLabel , type = type , dataSource = dataSource ) }
func: function ( e ) { message ( e ) cat ( " \n . . RETURN  empty  list  of  relations " ) out = list ( ) #  Choose  a  return  value  in  case  of  error ENDCOM }
func: function ( graph ) { out < - tryCatch ( { # #  Set  the  name  for  the  class ENDCOM data . frame ( t ( sapply ( graph $ graph $ relationships , function ( x ) list ( source = x $ startNode , target = x $ endNode , relname = x $ type , relsource = x $ properties [ " source " ] ) ) ) ) # relationInfo  =  list ( source = graph $ graph $ relationships [ [1 ] ] $ startNode ,  target = graph $ graph $ relationships [ [1 ] ] $ endNode ,  relname = graph $ graph $ relationships [ [1 ] ] $ type ,  relsource = graph $ graph $ relationships [ [1 ] ] $ properties [ " source " ] ) ENDCOM }
func: function ( e ) { message ( e ) cat ( " \n . . RETURN  empty  list  of  relations " ) out = data . frame ( ) #  Choose  a  return  value  in  case  of  error ENDCOM }
func: function ( node ) { out < - tryCatch ( { data . frame ( t ( sapply ( node , function ( x ) list ( id = x $ id , gid = x $ properties $ GID , nodename = x $ properties $ name , xref = paste0 ( x $ properties $ xref , collapse = " | | " ) , nodetype = x $ labels ) ) ) ) # nodeInfo  =  list ( id = node $ id ,  gid = node $ properties $ GID ,  nodename = node $ properties $ name ,  xref = paste0 ( node $ properties $ xref , collapse  =  " | | " ) ,  nodetype = node $ labels ) ENDCOM }
func: function ( e ) { message ( e ) cat ( " \n . . RETURN  empty  list  of  relations " ) out = data . frame ( ) #  Choose  a  return  value  in  case  of  error ENDCOM }
func: function ( list_of_matrices ) { listOfVectors < - lapply ( list_of_matrices , as . vector ) # print ( listOfVectors ) ENDCOM #  unlist  takes  list  to  vector . ENDCOM unlist ( listOfVectors ) }
func: function ( list_of_matrices , f ) { flattened < - . Flatten ( list_of_matrices ) #  NOTE :  UpdateJointConditional  does  outer  product  of  dimensions ! ENDCOM #  3  letter  strings  are  null  terminated ENDCOM writeBin ( ' ne  ' , con = f ) num_entries < - length ( list_of_matrices ) writeBin ( num_entries , con = f ) Log ( ' Wrote  num _ entries  =  % d ' , num_entries ) #  For  2x3 ,  this  is  6 ENDCOM writeBin ( ' es  ' , con = f ) entry_size < - as . integer ( prod ( dim ( list_of_matrices [ [ 1 ] ] ) ) ) writeBin ( entry_size , con = f ) Log ( ' Wrote  entry _ size  =  % d ' , entry_size ) #  now  write  the  data ENDCOM writeBin ( ' dat ' , con = f ) writeBin ( flattened , con = f ) }
func: function ( f , tag ) { #  Read  a  single  NUL - terminated  character  string . ENDCOM actual < - readBin ( con = f , what = " char " , n = 1 ) #  Assert  that  we  got  what  was  expected . ENDCOM if ( length ( actual ) != 1 ) { stop ( sprintf ( " Failed  to  read  a  tag  ' % s ' " , tag ) ) }
func: function ( f , entry_size , matrix_dims ) { . ExpectTag ( f , " emi " ) #  NOTE :  assuming  R  integers  are  4  bytes  ( uint32 _ t ) ENDCOM num_em_iters < - readBin ( con = f , what = " int " , n = 1 ) . ExpectTag ( f , " pij " ) pij < - readBin ( con = f , what = " double " , n = entry_size ) #  Adjust  dimensions ENDCOM dim ( pij ) < - matrix_dims Log ( " Number  of  EM  iterations :  % d " , num_em_iters ) Log ( " PIJ  read  from  external  implementation : " ) print ( pij ) #  est ,  sd ,  var _ cov ,  hist ENDCOM list ( est = pij , num_em_iters = num_em_iters ) }
func: function ( joint_conditional ) { #  Display  some  stats  before  sending  it  over  to  C + + . ENDCOM inf_counts < - lapply ( joint_conditional , function ( m ) { sum ( m == Inf ) }
func: function ( m ) { sum ( is . nan ( m ) ) }
func: function ( m ) { sum ( m == 0.0 ) }
func: function ( em_executable , tmp_dir ) { return ( function ( joint_conditional , max_em_iters = 1000 , epsilon = 10 ^ - 6 , verbose = FALSE , estimate_var = FALSE ) { matrix_dims < - dim ( joint_conditional [ [ 1 ] ] ) #  Check  that  number  of  dimensions  is  2 . ENDCOM if ( length ( matrix_dims ) != 2 ) { Log ( ' FATAL :  Expected  2  dimensions ,  got  % d ' , length ( matrix_dims ) ) stop ( ) }
func: function ( input , output ) { # #  Optimiser  ( Single ) ENDCOM optimise_single < - reactive ( { odds_all < - c ( ) if ( input $ odds1 > 1 ) odds_all < - c ( odds_all , input $ odds1 ) if ( input $ odds2 > 1 ) odds_all < - c ( odds_all , input $ odds2 ) if ( input $ odds3 > 1 ) odds_all < - c ( odds_all , input $ odds3 ) if ( input $ odds4 > 1 ) odds_all < - c ( odds_all , input $ odds4 ) if ( input $ odds5 > 1 ) odds_all < - c ( odds_all , input $ odds5 ) if ( input $ odds6 > 1 ) odds_all < - c ( odds_all , input $ odds6 ) if ( input $ odds7 > 1 ) odds_all < - c ( odds_all , input $ odds7 ) if ( input $ odds8 > 1 ) odds_all < - c ( odds_all , input $ odds8 ) if ( input $ odds9 > 1 ) odds_all < - c ( odds_all , input $ odds9 ) if ( input $ odds10 > 1 ) odds_all < - c ( odds_all , input $ odds10 ) as . matrix ( odds_all ) # #  Evaluate  Function ENDCOM eval_odds < - function ( inp_p ) { total_p < - sum ( inp_p ) rtn < - odds_all * as . matrix ( inp_p ) - total_p # return ( min ( rtn ) ) ENDCOM return ( 1 - ( max ( rtn ) - min ( rtn ) ) / max ( rtn ) ) }
func: function ( inp_p ) { total_p < - sum ( inp_p ) rtn < - odds_all * as . matrix ( inp_p ) - total_p # return ( min ( rtn ) ) ENDCOM return ( 1 - ( max ( rtn ) - min ( rtn ) ) / max ( rtn ) ) }
func: function ( dataset , colors , width = NULL , height = NULL ) { #  forward  options  using  x ENDCOM x = list ( dataset = dataset , colors = colors ) #  create  widget ENDCOM htmlwidgets : : createWidget ( name = ' C3LineBarChart ' , x , width = width , height = height , package = ' C3' ) }
func: function ( outputId , width = '100 % ' , height = '400px ' ) { htmlwidgets : : shinyWidgetOutput ( outputId , ' C3LineBarChart ' , width , height , package = ' C3' ) }
func: function ( expr , env = parent . frame ( ) , quoted = FALSE ) { if ( ! quoted ) { expr < - substitute ( expr ) }
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.5.tok
func: function ( input , output , session ) { react < - reactiveValues ( tab = { }
func: function ( d , # '  @ param  d  Object  of  class  gbt  or  gbtbin ,  in  plot ENDCOM slice = 1 , # '  @ param  slice  Which  sample  data  was  plotted ?  ( Default :  1 ) ENDCOM ... # '  @ param  . . .  Further  arguments  passed  to  identify . default ( ) ENDCOM ) { #  Catch  invalid  " slice "  parameters ENDCOM if ( is . na ( slice ) | | ! is . numeric ( slice ) ) { cat ( " gbtools  ERROR :  Please  specify  valid  value  for  slice  parameter \n " ) }
func: function ( x , #  Object  of  class  gbt ENDCOM slice , #  Which  slices  used  for  the  plot  from  which  points  to  be  chosen ? ENDCOM taxon = " Class " , #  Deprecated  -  user  don ' t  change  this ENDCOM binpolygon = NA , #  The  polygon ENDCOM save = FALSE , #  Save  list  of  contigs  in  bin  to  external  file ? ENDCOM file = " interactive _ bin . list " #  Name  of  file  to  save  list  of  contigs  in  bin ENDCOM ) { require ( sp ) # #  Wrapper  for  picking  bin  interactively  from  GC - cov  or  diff - cov  plot ENDCOM if ( ! is . numeric ( slice ) | | length ( slice ) > 2 ) { cat ( " gbtools  ERROR :  Please  specify  the  library ( - ies )  used  to  make  the  plot  in  focus \n " ) }
func: function  call ENDCOM return ( result ) }
func: function ( sims = 10000 , x . mean = TRUE , postmean , poststd , X , toLatex ) { # #  source :  http : / / researchrepository . ucd . ie / handle / 10197/3404 ENDCOM # #  method :  average  of  individual  marginal  effects  at  each  observation ENDCOM # #  interpretation :  http : / / www . indiana . edu / ~ statmath / stat / all / cdvm / cdvm . pdf  page  8 ENDCOM set . seed ( 1984 ) if ( x . mean == TRUE ) { # #  marginal  effects  are  calculated  at  the  means  of  independent  variables ENDCOM pdf < - dnorm ( mean ( X % * % postmean ) ) pdfsd < - dnorm ( sd ( X % * % postmean ) ) }
func: function ( postmean , poststd , nrowX , toLatex ) { # #  Reference :  Sorensen  ( 2007 ,  p .  2748 ) ENDCOM mx < - dnorm ( 0 ) * postmean / sqrt ( 2 ) s . e . < - dnorm ( 0 ) * poststd / sqrt ( 2 ) t . stat < - mx / s . e . p . val < - pt ( - abs ( t . stat ) , df = nrowX - length ( postmean ) ) stars < - ifelse ( p . val < 0.001 , " * * * " , ifelse ( p . val < 0.01 , " * * " , ifelse ( p . val < 0.05 , " * " , ifelse ( p . val < 0.10 , " . " , " " ) ) ) ) if ( toLatex == FALSE ) { res < - data . frame ( round ( cbind ( mx , s . e . , t . stat , p . val ) , 3 ) , stars ) }
func: function ( p , method = p . adjust . methods , n = length ( p ) ) NEW_LINE { # #  Methods  ' Hommel ' ,  ' BH ' ,  ' BY '  and  speed  improvements ENDCOM # #  contributed  by  Gordon  Smyth ENDCOM method < - match . arg ( method ) if ( method == " fdr " ) method < - " BH " #  back  compatibility ENDCOM nm < - names ( p ) p < - as . numeric ( p ) p0 < - setNames ( p , nm ) if ( all ( nna < - ! is . na ( p ) ) ) nna < - TRUE p < - p [ nna ] lp < - length ( p ) stopifnot ( n >= lp ) if ( n <= 1 ) return ( p0 ) if ( n == 2 & & method == " hommel " ) method < - " hochberg " p0 [ nna ] < - switch ( method , bonferroni = pmin ( 1 , n * p ) , holm = { i < - seq_len ( lp ) o < - order ( p ) ro < - order ( o ) pmin ( 1 , cummax ( ( n - i + 1 L ) * p [ o ] ) ) [ ro ] }
func: function ( src_name , species_name , lat_dec_deg = NULL , lon_dec_deg = NULL , x_coord_km = NULL , y_coord_km = NULL , UTM_zone = NULL , UTM_hemisphere = NULL , effective_height , base_elev , init_sigma_z , emission_rate , emission_units ) { #  Add  require  statements ENDCOM require ( rgdal ) require ( raster ) require ( stringr ) require ( plyr ) #  Get  expected  filename  for  area  sources ENDCOM area_sources_filename < - paste0 ( unlist ( str_split ( getwd ( ) , pattern = " / " ) ) [ length ( unlist ( str_split ( getwd ( ) , pattern = " / " ) ) ) ] , " - - area _ sources . txt " ) #  Create  area  sources  text  file  with  header  if  it  doesn ' t  exist ENDCOM if ( file . exists ( area_sources_filename ) == FALSE ) { #  Create  empty  file  in  working  folder ENDCOM file . create ( area_sources_filename ) #  Add  header  row  to  new  area  sources  file ENDCOM cat ( paste0 ( " src _ name " , " , " , " species _ name " , " , " , " lat _ dec _ deg _ 1" , " , " , " lon _ dec _ deg _ 1" , " , " , " lat _ dec _ deg _ 2" , " , " , " lon _ dec _ deg _ 2" , " , " , " lat _ dec _ deg _ 3" , " , " , " lon _ dec _ deg _ 3" , " , " , " lat _ dec _ deg _ 4" , " , " , " lon _ dec _ deg _ 4" , " , " , " x _ coord _ km _ 1" , " , " , " y _ coord _ km _ 1" , " , " , " x _ coord _ km _ 2" , " , " , " y _ coord _ km _ 2" , " , " , " x _ coord _ km _ 3" , " , " , " y _ coord _ km _ 3" , " , " , " x _ coord _ km _ 4" , " , " , " y _ coord _ km _ 4" , " , " , " UTM _ zone " , " , " , " UTM _ hemisphere " , " , " , " effective _ height " , " , " , " base _ elev " , " , " , " init _ sigma _ z " , " , " , " emission _ rate " , " , " , " emission _ units " ) , sep = " \n " , file = area_sources_filename , append = TRUE ) }
func: function  directely  but  rather  to  use  the  nni ( )  wrapper ENDCOM # # '  function .  The  methods  provides  two  ways  for  missing  value ENDCOM # # '  estimation ,  selected  by  the  \code { allVariables }
func: function ( Matrix , k = 10 , center = FALSE , completeObs = TRUE , correlation = " pearson " , allVariables = FALSE , maxSteps = 100 , xval = NULL , verbose = FALSE , ... ) { threshold < - 0.001 correlation < - match . arg ( correlation , c ( " pearson " , " kendall " , " spearman " ) ) # #  If  the  data  is  a  data  frame ,  convert  it  into  a  matrix ENDCOM Matrix < - as . matrix ( Matrix , rownames . force = TRUE ) # #  And  now  check  if  everything  is  right . . . ENDCOM if ( ! checkData ( Matrix , verbose = interactive ( ) ) ) { stop ( " Invalid  data  format !  Use  checkData ( Matrix ,  verbose  =  TRUE )  for  details . \n " ) }
func: function ( x , y , cat ) { }
func: function ( d , colors = c ( " green4" , " red3" , " brown " ) , sz = 17 ) { nms < - names ( d ) names ( d ) < - c ( " xvar " , " yvar " , " fvar " ) df < - d % > % dplyr : : mutate ( abs_diff = abs ( xvar - yvar ) , pos_neg = sign ( xvar - yvar ) , midpoint = abs_diff / 2 , clr = ifelse ( pos_neg > 0 , colors [ 1 ] , colors [ 2 ] ) ) % > % dplyr : : rowwise ( ) % > % dplyr : : mutate ( xmin = min ( xvar , yvar ) , xmax = max ( xvar , yvar ) ) % > % ungroup df_tall < - df % > % tidyr : : gather ( keys , values , - c ( fvar , xmin , xmax , midpoint , pos_neg , abs_diff , clr ) ) % > % mutate ( keys = ifelse ( keys == " xvar " , nms [ 1 ] , nms [ 2 ] ) ) l < - as . character ( df [ [ " fvar " ] ] [ as . integer ( df [ [ " fvar " ] ] ) == max ( as . integer ( df [ [ " fvar " ] ] ) ) ] ) ll < - if ( d [ 1 , 1 ] < d [ 1 , 2 ] ) nms [ 1 : 2 ] else nms [ 2 : 1 ] ggplot ( df_tall , aes ( y = fvar , x = values ) ) + geom_segment ( data = df , aes ( x = xmin , xend = xmax , y = fvar , yend = fvar , alpha = abs_diff ) , color = df $ clr , size = 3 ) + geom_point ( color = colors [ 3 ] , size = 5.5 ) + geom_point ( shape = 21 , color = colors [ 3 ] , size = 4.8 , aes ( fill = keys ) ) + scale_fill_manual ( values = c ( " white " , colors [ 3 ] ) , guide = FALSE ) + theme_bw ( ) + theme ( text = element_text ( size = sz ) , legend . position = " none " , panel . grid = element_blank ( ) ) + annotate ( " text " , x = c ( df [ df $ " fvar " == l , ] $ xmin , df [ df $ " fvar " == l , ] $ xmax ) , y = c ( df [ df $ " fvar " == l , ] $ fvar , df [ df $ " fvar " == l , ] $ fvar ) , label = ll , color = colors [ 3 ] , size = 4.8 , vjust = 2.5 ) + labs ( list ( x = " Values " , y = nms [ 3 ] ) ) + annotate ( " text " , x = 8.75 , y = 5 , label = " Green / Red  signifies \n higher / lower  performance . \n \n  Color  intensity  signifies \n imporance  of  difference . " , color = colors [ 3 ] , size = 7 , vjust = .75 ) }
func: function ( x , userTab , userSource = NA ) { # #  Check  that  userTab  is  data . frame  with  col  " scaffold "  # # # # # ENDCOM if ( ! is . data . frame ( userTab ) | | length ( which ( names ( userTab ) == " scaffold " ) ) == 0 | | is . na ( userSource ) ) { cat ( " gbtools  ERROR :  Please  check  inputs .  See  help ( userAdd )  \n " ) }
func: function  call ENDCOM return ( x ) #  Return  result ENDCOM }
func: function  returns  a  table  of  all  of  the  technology  platforms  used  to  sequence  or  characterize  samples  in  TCGA - - both  their  short  platform  codes  and  full  names .  A  subset  of  this  table  may  be  obtained  by  explicitly  specifying  one  or  more  platform  codes . ENDCOM # '  @ param  format  Format  of  result .  Default  value  is  json .  While  json , tsv , csv  are  available .  ENDCOM # '  @ param  platform  Narrow  search  to  one  or  more  TCGA  data  generation  platforms  from  the  scrollable  list .  Multiple  values  are  allowed  454 , ABI , AgilentG4502A _ 07 , AgilentG4502A _ 07_1 , AgilentG4502A _ 07_2 , AgilentG4502A _ 07_3 , bio , biotab , CGH - 1x1M _ G4447A , diagnostic _ images , fh _ analyses , fh _ reports , fh _ stddata , Genome _ Wide _ SNP _ 6 , GenomeWideSNP _ 5 , H - miRNA _ 8x15K , H - miRNA _ 8x15Kv2 , H - miRNA _ EarlyAccess , H - miRNA _ G4470A , HG - CGH - 244A , HG - CGH - 415K _ G4124A , HG - U133 _ Plus _ 2 , HG - U133A _ 2 , HT _ HG - U133A , HuEx - 1_0 - st - v2 , Human1MDuo , HumanHap550 , HumanMethylation27 , HumanMethylation450 , IlluminaDNAMethylation _ OMA002 _ CPI , IlluminaDNAMethylation _ OMA003 _ CPI , IlluminaGA _ DNASeq , IlluminaGA _ DNASeq _ automated , IlluminaGA _ DNASeq _ Cont , IlluminaGA _ DNASeq _ Cont _ automated , IlluminaGA _ DNASeq _ Cont _ curated , IlluminaGA _ DNASeq _ curated , IlluminaGA _ miRNASeq , IlluminaGA _ mRNA _ DGE , IlluminaGA _ RNASeq , IlluminaGA _ RNASeqV2 , IlluminaGG , IlluminaHiSeq _ DNASeq , IlluminaHiSeq _ DNASeq _ automated , IlluminaHiSeq _ DNASeq _ Cont , IlluminaHiSeq _ DNASeq _ Cont _ automated , IlluminaHiSeq _ DNASeq _ Cont _ curated , IlluminaHiSeq _ DNASeq _ curated , IlluminaHiSeq _ DNASeqC , IlluminaHiSeq _ miRNASeq , IlluminaHiSeq _ mRNA _ DGE , IlluminaHiSeq _ RNASeq , IlluminaHiSeq _ RNASeqV2 , IlluminaHiSeq _ TotalRNASeqV2 , IlluminaHiSeq _ WGBS , Mapping250K _ Nsp , Mapping250K _ Sty , MDA _ RPPA _ Core , microsat _ i , minbio , minbiotab , Mixed _ DNASeq , Mixed _ DNASeq _ automated , Mixed _ DNASeq _ Cont , Mixed _ DNASeq _ Cont _ automated , Mixed _ DNASeq _ Cont _ curated , Mixed _ DNASeq _ curated , pathology _ reports , SOLiD _ DNASeq , SOLiD _ DNASeq _ automated , SOLiD _ DNASeq _ Cont , SOLiD _ DNASeq _ Cont _ automated , SOLiD _ DNASeq _ Cont _ curated , SOLiD _ DNASeq _ curated , tissue _ images , WHG - 1x44K _ G4112A , WHG - 4x44K _ G4112F , WHG - CGH _ 4x44B . ENDCOM # '  @ export ENDCOM Metadata . Platforms = function ( format = " json " , platform = " " ) { parameters = list ( format = format , platform = platform ) validate . Parameters ( params = parameters ) url = build . Query ( parameters = parameters , invoker = " Metadata " , method = " Platforms " ) ret = download . Data ( url , format ) return ( ret ) }
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.3.tok
func: function ( ) { tryCatch ( swirl : : : swirl_courses_dir ( ) , error = function ( c ) { file . path ( find . package ( " swirl " ) , " Courses " ) }
func: function ( a , b ) { sum ( ( a - b ) ^ 2 ) / length ( a ) }
func: function ( coef ) NEW_LINE { actual < - ( input [ , 1 ] * coef [ 1 ] ) ^ 2 + ( input [ , 1 ] * coef [ 2 ] ) + coef [ 3 ] actual < - as . table ( matrix ( actual , ncol = 1 , byrow = TRUE ) ) rownames ( actual ) = 1 : nrow ( actual ) colnames ( actual ) < - c ( " y " ) actual }
func: function ( coef ) { actual < - calcPolynomial ( coef ) mse ( actual , ideal ) }
func: function ( x1 , x2 ) { # #  Take  difference  between  two  bins  -  non  commutative !  i . e .  left  exclusive  join ENDCOM shortlist < - x1 $ scaff $ ID [ which ( ! x1 $ scaff $ ID % in % x2 $ scaff $ ID ) ] result < - setOperation . gbtbin ( x1 = x1 , x2 = x2 , shortlist = shortlist ) result $ call [ [ length ( result $ call ) + 1 ] ] < - match . call ( ) #  Record  function  call  ENDCOM return ( result ) }
func: function ( . x_y_z , a ) NEW_LINE { a < - F #  not  indented  on  purpose ENDCOM if ( missing ( . x_y_z ) ) { print ( ' Please  set  a  list  . x _ y _ z . ' ) }
func: function ( ) NEW_LINE { select < - function ( ) { NULL }
func: function ( ) { NULL }
func: function  of  one  or  two  existing ENDCOM # '  parameters . ENDCOM # '  @ export ENDCOM # '  @ template  args - sso ENDCOM # '  @ param  fun  Function  to  call ,  i . e .  \code { function ( param1 ) }
func: function ( param1 , param2 ) }
func: function ( x )  x ^ 2 ,  ENDCOM # '  param1  =  " tau " ,  new _ name  =  " tau _ sq " ) ENDCOM # '  sso  < -  generate _ quantity ( sso ,  fun  =  " - " ,  ENDCOM # '  param1  =  " theta [ 1 ] " ,  param2  =  " theta [ 2 ] " ,  ENDCOM # '  new _ name  =  " theta1minus2 " ) ENDCOM generate_quantity < - function ( sso , param1 , param2 , fun , new_name ) { sso_check ( sso ) if ( isTRUE ( new_name % in % slot ( sso , " param _ names " ) ) ) stop ( paste ( " There  is  already  a  parameter  named " , new_name ) ) message ( " \n This  might  take  a  moment  for  large  shinystan  objects . . . " ) two_params < - ! missing ( param2 ) posterior < - slot ( sso , " posterior _ sample " ) dims < - dim ( posterior ) ndim < - length ( dims ) if ( ndim == 3 ) { #  i . e .  multiple  chains ENDCOM x_samp < - posterior [ , , param1 ] if ( two_params ) y_samp < - posterior [ , , param2 ] }
func: function ( X , FUN , ... ) NEW_LINE { FUN < - match . fun ( FUN ) # #  internal  code  handles  all  vector  types ,  including  expressions ENDCOM # #  However ,  it  would  be  OK  to  have  attributes  which  is . vector ENDCOM # #  disallows . ENDCOM if ( ! is . vector ( X ) | | is . object ( X ) ) X < - as . list ( X ) # #  Note  . . .  is  not  passed  down .  Rather  the  internal  code ENDCOM # #  evaluates  FUN ( X [ i ] ,  . . . )  in  the  frame  of  this  function ENDCOM . Internal ( lapply ( X , FUN ) ) }
func: function ( object , f , classes = " ANY " , deflt = NULL , how = c ( " unlist " , " replace " , " list " ) , ... ) NEW_LINE DEDENT { if ( typeof ( object ) != " list " ) stop ( " ' object '  must  be  a  list " ) how < - match . arg ( how ) res < - . Internal ( rapply ( object , f , classes , deflt , how ) ) if ( how == " unlist " ) unlist ( res , recursive = TRUE ) else res }
func: function ( vector ) { count < - 0 for ( i in vector ) { count < - count + 1 }
func: function ( src , rodzajEgzaminu , rok , punktuj = TRUE , idSkali = NULL , skroc = TRUE ) { stopifnot ( is . src ( src ) , is . vector ( rodzajEgzaminu ) , is . character ( rodzajEgzaminu ) , length ( rodzajEgzaminu ) == 1 , is . vector ( rok ) , is . numeric ( rok ) , length ( rok ) == 1 , is . vector ( punktuj ) , is . logical ( punktuj ) , length ( punktuj ) == 1 , punktuj % in % c ( T , F ) , is . null ( idSkali ) | is . vector ( idSkali ) & is . numeric ( idSkali ) & length ( idSkali ) == 1 , is . vector ( skroc ) , is . logical ( skroc ) , length ( skroc ) == 1 , skroc % in % c ( T , F ) ) regExp = e ( paste0 ( ' ^ zrwnywanie ; ' , rodzajEgzaminu , ' ; ' , rok , ' ; . * $ ' ) ) tests = pobierz_testy ( src ) % > % collect ( ) % > % filter_ ( ~ grepl ( regExp , opis_testu ) ) if ( nrow ( tests ) == 0 ) { stop ( e ( ' w  bazie  nie  ma  takiego  zrownywania ' ) ) }
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.1.tok
func: function ( n , X ) { d < - Diagonal ( n ) last < - rep ( - 1 , n ) rbind2 ( rbind2 ( d , last ) , - X ) }
func: function ( n , Y , stds ) { #  set  the  floor  at  0.01  to  avoid  degenerate  cases ENDCOM YY < - apply ( Y + 3 * stds , #  in  each  bin  don ' t  overshoot  by  more  than  3  stds ENDCOM 1 : 2 , function ( x ) min ( 1 , max ( 0.01 , x ) ) ) #  clamp  the  bound  to  [ 0.01,1 ] ENDCOM c ( rep ( 0 , n ) , #  non - negativity  condition ENDCOM - 1 , #  coefficients  sum  up  to  no  more  than  1 ENDCOM - as . vector ( t ( YY ) ) #  t  is  important ! ENDCOM ) }
func: function ( X , Y , stds ) { m < - dim ( X ) [ 1 ] n < - dim ( X ) [ 2 ] #  no  slack  variables  for  now ENDCOM #  slack  < -  Matrix ( FALSE ,  nrow  =  m ,  ncol  =  m ,  sparse  =  TRUE ) ENDCOM #  colnames ( slack )  < -  1 : m ENDCOM #  diag ( slack )  < -  TRUE ENDCOM #  G  < -  MakeG ( n  +  m ) ENDCOM #  H  < -  MakeH ( n  +  m ) ENDCOM #  G [ n + m + 1 , n : ( n + m ) ]  < -  - 0.1 ENDCOM #  A  =  cbind2 ( X ,  slack ) ENDCOM w < - as . vector ( t ( 1 / stds ) ) w_median < - median ( w [ ! is . infinite ( w ) ] ) if ( is . na ( w_median ) ) #  all  w  are  infinite ENDCOM w_median < - 1 w [ w > w_median * 2 ] < - w_median * 2 w < - w / mean ( w ) list ( #  coerce  sparse  Boolean  matrix  X  to  sparse  numeric  matrix ENDCOM A = Diagonal ( x = w ) % * % ( X + 0 ) , B = as . vector ( t ( Y ) ) * w , #  transform  to  vector  in  the  row - first  order ENDCOM G = MakeG ( n , X ) , H = MakeH ( n , Y , stds ) , type = 2 ) #  Since  there  are  no  equality  constraints ,  lsei  defaults  to ENDCOM #  solve . QP  anyway ,  but  outputs  a  warning  unless  type  = =  2 . ENDCOM }
func: function ( X , Y ) { model < - MakeLseiModel ( X , Y $ estimates , Y $ stds ) coefs < - do . call ( lsei , model ) $ X names ( coefs ) < - colnames ( X ) coefs }
func: function ( ... ) { e < - get ( " e " , parent . frame ( ) ) any ( sapply ( c ( ... ) , function ( expr ) omnitest ( expr ) ) ) }
func: function ( correctExpr ) { e < - get ( " e " , parent . frame ( ) ) #  Do  what  the  user  should  have  done ENDCOM eSw < - cleanEnv ( e $ snapshot ) mdlSw < - eval ( parse ( text = correctExpr ) , eSw ) #  Recreate  what  the  user  has  done ENDCOM eUsr < - cleanEnv ( e $ snapshot ) mdlUsr < - eval ( e $ expr , eUsr ) #  If  the  correct  model  is  named : ENDCOM if ( length ( ls ( eSw ) ) > 0 ) { #  Check  whether  the  model ' s  name  is  correct ENDCOM nameGood < - sum ( ls ( eUsr ) % in % ls ( eSw ) ) & sum ( ls ( eSw ) % in % ls ( eUsr ) ) #  If  not ,  highlight  the  misspelling ENDCOM if ( ! nameGood ) { swirl_out ( paste0 ( " You  seem  to  have  misspelled  the  model ' s  name .  I  was  expecting  " , ls ( eSw ) , "  but  you  apparently  typed  " , ls ( eUsr ) , " . " ) ) return ( FALSE ) }
func: function ( expr ) { e < - get ( " e " , parent . frame ( ) ) #  Calculate  what  the  user  should  have  done . ENDCOM eSnap < - cleanEnv ( e $ snapshot ) val < - eval ( parse ( text = expr ) , eSnap ) isTRUE ( all . equal ( val , e $ val ) ) }
func: function ( ... ) { e < - get ( " e " , parent . frame ( ) ) any ( sapply ( c ( ... ) , function ( expr ) calculates_same_value ( expr ) ) ) }
func: function ( ) { #  Whenever  swirl  is  running ,  its  callback  is  at  the  top  of  its  call  stack . ENDCOM #  Swirl ' s  state ,  named  e ,  is  stored  in  the  environment  of  the  callback . ENDCOM environment ( sys . function ( 1 ) ) $ e }
func: function ( ) { getState ( ) $ val }
func: function ( ) { getState ( ) $ expr }
func: function ( ) { selection < - getState ( ) $ val if ( selection == " Yes " ) { email < - readline ( " What  is  your  email  address ?  " ) token < - readline ( " What  is  your  assignment  token ?  " ) payload < - sprintf ( ' { " assignmentKey " : " hHsdF68wEeWxaw7Jay15BQ " , " submitterEmail " : " % s " , " secret " : " % s " , " parts " : { "3wpmw " : { " output " : " correct " }
func: function ( ... ) { e < - get ( " e " , parent . frame ( ) ) any ( sapply ( c ( ... ) , function ( expr ) omnitest ( expr ) ) ) }
func: function ( correctVal ) { e < - get ( " e " , parent . frame ( ) ) # print ( paste ( " User  val  is  " , e $ val , " Correct  ans  is  " , correctVal ) ) ENDCOM isTRUE ( all . equal ( correctVal , e $ val ) ) }
func: function ( ) { #  Whenever  swirl  is  running ,  its  callback  is  at  the  top  of  its  call  stack . ENDCOM #  Swirl ' s  state ,  named  e ,  is  stored  in  the  environment  of  the  callback . ENDCOM environment ( sys . function ( 1 ) ) $ e }
func: function ( ) { getState ( ) $ val }
func: function ( ) { getState ( ) $ expr }
func: function ( ) { selection < - getState ( ) $ val if ( selection == " Yes " ) { email < - readline ( " What  is  your  email  address ?  " ) token < - readline ( " What  is  your  assignment  token ?  " ) payload < - sprintf ( ' { " assignmentKey " : " BOLw8680EeWRRQpQejjiSw " , " submitterEmail " : " % s " , " secret " : " % s " , " parts " : { " jwsCv " : { " output " : " correct " }
func: function ( pkg , availPkgs = cranJuly2014 , repos = MRAN ( ) , type = " source " , suggests = TRUE , enhances = FALSE , includeBasePkgs = FALSE ) { if ( ! require ( igraph , quietly = TRUE ) ) { skip ( " package  igraph  not  installed " ) }
func: function ( pkg , ... ) { packages . to . exclude < - c ( " igraph " ) inSearchPath < - any ( grepl ( sprintf ( " package : % s $ " , paste ( packages . to . exclude , collapse = " | " ) ) , search ( ) ) ) if ( inSearchPath ) stop ( " Required  package  already  in  search  path " ) package < - as . character ( substitute ( pkg ) ) if ( package % in % packages . to . exclude ) FALSE else base : : requireNamespace ( package , character . only = TRUE , ... ) }
func: function ( pkg , ... ) { packages . to . exclude < - c ( " igraph " ) inSearchPath < - any ( grepl ( sprintf ( " package : % s $ " , paste ( packages . to . exclude , collapse = " | " ) ) , search ( ) ) ) if ( inSearchPath ) stop ( " Required  package  already  in  search  path " ) package < - as . character ( substitute ( pkg ) ) if ( package % in % packages . to . exclude ) FALSE else base : : requireNamespace ( package , character . only = TRUE , ... ) }
func: function ( num ) { if ( num % % 10 == 0 ) { print ( ' True ' ) }
func: function ( input , output , session ) { dat < - reactiveValues ( tab = NULL ) counter < - reactiveValues ( count = - 1 ) observe ( { if ( counter $ count != input $ goButton ) { counter $ count < - input $ goButton withProgress ( message = " Simulating  1000  experiments " , value = 0 , { n < - 1000 m1 < - replicate ( n , woc ( n = input $ n , val = input $ val , error = input $ error , soc = 0 ) ) m2 < - replicate ( n , woc ( n = input $ n , val = input $ val , error = input $ error , soc = input $ soc ) ) dat $ tab < - data . frame ( SOC = as . factor ( rep ( c ( " Control    " , " Experimental    " ) , each = n * 2 ) ) , TYPE = rep ( c ( " mean " , " sd " , " mean " , " sd " ) , each = n ) , VAL = c ( apply ( m1 , 2 , mean ) , apply ( m1 , 2 , sd ) , apply ( m2 , 2 , mean ) , apply ( m2 , 2 , sd ) ) ) }
func: function ( data , str_input , type = c ( ' stop ' , ' set _ na ' ) , top_percent , bottom_percent ) NEW_LINE { if ( ! ( type % in % c ( ' stop ' , ' set _ na ' , ' sigmoid ' ) ) ) stop ( " Parameter  ' type '  must  be  one  ' stop '  or  ' set _ na ' " ) if ( missing ( str_input ) ) str_input = give_me_num_vars ( data ) #  # # #  Sigmoid  procesing ENDCOM #  if ( type  = =  ' sigmoid ' ) ENDCOM #  TABSYMBOL for ( i  in  1 : length ( str _ input ) ) ENDCOM #  TABSYMBOL data [ ,  str _ input [ i ] ] = sigmoid ( as . numeric ( scale ( data [ ,  str _ input [ i ] ] ) ) ) ENDCOM #  TABSYMBOL return ( data ) ENDCOM # # #  Stopping  and  Setting  NA  processing ENDCOM # #  If  not  sigmoid ,  then  it ' s  stop  or  set _ na ,  thus  it  has  to  have  top  or  bottom  param . ENDCOM if ( missing ( top_percent ) & missing ( bottom_percent ) ) stop ( " Parameters  ' top _ percent '  and  ' bottom _ percent '  cannot  be  missing  at  the  same  time " ) # #  Logic  for  top  value ENDCOM if ( ! missing ( top_percent ) ) { for ( i in 1 : length ( str_input ) ) { top_value = round ( quantile ( data [ , str_input [ i ] ] , probs = ( 1 - top_percent ) , names = F , na . rm = T ) ) data [ , str_input [ i ] ] [ data [ , str_input [ i ] ] > top_value ] = ifelse ( type == ' stop ' , top_value , NA ) }
func: function ( key_x , key_y ) NEW_LINE { #  key _ x = v1 ; key _ y = v2 ENDCOM df_x = data . frame ( key_x = key_x , flag_x = 1 ) df_y = data . frame ( key_y = key_y , flag_y = 1 ) df_x $ key_x = as . character ( df_x $ key_x ) df_y $ key_y = as . character ( df_y $ key_y ) merge_all = merge ( df_x , df_y , by . x = ' key _ x ' , by . y = ' key _ y ' , all = T ) names ( merge_all ) [ 1 ] = " key " merge_all_nona = merge_all [ ! is . na ( merge_all $ flag_x ) & ! is . na ( merge_all $ flag_y ) , ] not_in_x = merge_all [ is . na ( merge_all $ flag_x ) , ] not_in_y = merge_all [ is . na ( merge_all $ flag_y ) , ] print ( sprintf ( " Coincident  in  both :  % s " , nrow ( merge_all_nona ) ) ) print ( sprintf ( " Rows  not  present  in  X :  % s " , nrow ( not_in_x ) ) ) print ( sprintf ( " Rows  not  present  in  Y :  % s " , nrow ( not_in_y ) ) ) list_diff = list ( ) res = list ( present_in_both = merge_all_nona $ key , rows_not_in_X = not_in_x $ key , rows_not_in_Y = not_in_y $ key ) return ( res ) }
func: function ( data , str_target ) NEW_LINE { data [ , str_target ] = as . numeric ( data [ , str_target ] ) data = data [ , c ( give_me_num_vars ( data , str_target ) , str_target ) ] df_cor = as . data . frame ( round ( cor ( data , use = " complete . obs " ) , 2 ) ) df_cor $ Variable = rownames ( df_cor ) df_cor = df_cor [ , names ( df_cor ) % in % c ( str_target , " Variable " ) ] df_cor = df_cor [ interp ( ~ order ( df_cor , - v ) , v = as . name ( str_target ) ) , ] row . names ( df_cor ) = NULL df_cor = df_cor [ , c ( 2 , 1 ) ] df_cor [ order ( - df_cor [ , 2 ] ) , ] return ( df_cor ) }
func: function ENDCOM # '  @ description  Sigmoid  function ,  also  known  as  logistic  or  s - shaped ENDCOM # '  @ param  x  numeric  input  vector ENDCOM # '  @ param  a  constant  to  multiply  ' x ' ,  default = 1 ENDCOM # '  @ examples ENDCOM # '  sigmoid ( ) ENDCOM # '  @ return  vector  transformed  with  sigmoid ENDCOM # '  @ export ENDCOM sigmoid < - function ( x , a = 1 ) NEW_LINE { if ( missing ( a ) ) a = 1 y = 1 / ( 1 + exp ( - a * x ) ) return ( y ) }
func: function ( x ) NEW_LINE { return ( ( x - min ( x , na . rm = T ) ) / ( max ( x , na . rm = T ) - min ( x , na . rm = T ) ) ) }
func: function ( values , legendPosition = " bottom " , width = NULL , height = NULL ) { #  forward  options  using  x ENDCOM x = list ( values = values , legendPosition = legendPosition ) #  create  widget ENDCOM htmlwidgets : : createWidget ( name = ' C3Pie ' , x , width = width , height = height , package = ' C3' ) }
func: function ( outputId , width = '100 % ' , height = '400px ' ) { htmlwidgets : : shinyWidgetOutput ( outputId , ' C3Pie ' , width , height , package = ' C3' ) }
func: function ( expr , env = parent . frame ( ) , quoted = FALSE ) { if ( ! quoted ) { expr < - substitute ( expr ) }
func: function ( x , ... ) { UseMethod ( " glMDSPlot " ) }
func: function ( x , top = 500 , labels = 1 : ncol ( x ) , groups = rep ( 1 , ncol ( x ) ) , gene . selection = " pairwise " , main = " MDS  Plot " , path = getwd ( ) , folder = " glimma - plots " , html = " MDS - Plot " , launch = TRUE , ... ) { #  Multi - dimensional  scaling  with  top - distance ENDCOM #  Di  Wu  and  Gordon  Smyth ENDCOM #  19  March  2009 .  Last  modified  14  Jan  2015 ENDCOM #  Modified  by  Shian  Su  on  25  Jan  2016 ENDCOM #  Check  Inputs ENDCOM x < - as . matrix ( x ) nsamples < - ncol ( x ) ndim < - nsamples - 1 if ( nsamples < 3 ) { stop ( paste ( " Only " , nsamples , " columns  of  data :  need  at  least  3" ) ) }
func: function ( x , top = 500 , labels = 1 : ncol ( x ) , groups = rep ( 1 , ncol ( x ) ) , gene . selection = " pairwise " , main = " MDS  Plot " , path = getwd ( ) , folder = " glimma - plots " , html = " MDS - Plot " , launch = TRUE , ... ) { x < - edgeR : : cpm ( x , log = TRUE ) glMDSPlot . default ( x , top = 500 , labels = labels , groups = groups , gene . selection = " pairwise " , main = main , path = path , folder = folder , html = html , launch = launch , ... ) }
func: function ( ) { suppressMessages ( library ( RcppRedis ) ) #  we  start  the  Redis  server  for  this  test  as  a  slave  so  we  don ' t  clobber  the  main  running  version  of  redis  for  the  rest  of  the  tests ENDCOM writeLines ( " requirepass  badPassword " , " / tmp / redis . conf " ) system ( " redis - server  / tmp / redis . conf  - - port  7777  - - slaveof  localhost  6379" , wait = FALSE ) #  Wait  for  server  to  come  up ENDCOM Sys . sleep ( 5 ) }
func: function ( ) { redis << - new ( RcppRedis : : Redis , " localhost " , 7777 , auth = " " , 10 ) #  we  expect  an  exception  because  we  haven ' t  send  the  password ENDCOM checkException ( redis $ ping ( ) ) }
func: function ( ) { redis << - new ( RcppRedis : : Redis , " localhost " , 7777 , auth = " badPassword " , 10 ) checkEquals ( redis $ ping ( ) , " PONG " ) }
func: function ( ) { #  confirm  server  is  up ENDCOM checkEquals ( redis $ ping ( ) , " PONG " ) #  kill  server ENDCOM checkException ( redis $ exec ( " SHUTDOWN " ) ) }
func: function ( ) { NULL }
func: function ( ... ) { e < - get ( " e " , parent . frame ( ) ) any ( sapply ( c ( ... ) , function ( expr ) omnitest ( expr ) ) ) }
func: function ( correctVal ) { e < - get ( " e " , parent . frame ( ) ) # print ( paste ( " User  val  is  " , e $ val , " Correct  ans  is  " , correctVal ) ) ENDCOM isTRUE ( all . equal ( correctVal , e $ val ) ) }
func: function ( ) { #  Whenever  swirl  is  running ,  its  callback  is  at  the  top  of  its  call  stack . ENDCOM #  Swirl ' s  state ,  named  e ,  is  stored  in  the  environment  of  the  callback . ENDCOM environment ( sys . function ( 1 ) ) $ e }
func: function ( ) { getState ( ) $ val }
func: function ( ) { getState ( ) $ expr }
func: function ( ) { selection < - getState ( ) $ val if ( selection == " Yes " ) { email < - readline ( " What  is  your  email  address ?  " ) token < - readline ( " What  is  your  assignment  token ?  " ) payload < - sprintf ( ' { " assignmentKey " : " jbRkna8dEeWxIhKVGQB0WQ " , " submitterEmail " : " % s " , " secret " : " % s " , " parts " : { " Lxfoi " : { " output " : " correct " }
func: function ( sso ) { sso_check ( sso ) sso_name < - deparse ( substitute ( sso ) ) has_notes < - sso @ user_model_info != " Use  this  space  to  store  notes  about  your  model " has_code < - sso @ model_code != " Use  this  space  to  store  your  model  code " cat ( sso_name , " - - - - - - - - - - - - - - - - - - - - - " , paste ( " Model  name : " , sso @ model_name ) , paste ( " Parameters : " , length ( sso @ param_names ) ) , paste ( " Parameter  groups : " , length ( names ( sso @ param_dims ) ) ) , paste ( " Chains : " , sso @ n_chain ) , paste ( " Iterations : " , sso @ n_iter ) , paste ( " Warmup : " , sso @ n_warmup ) , paste ( " Has  model  code : " , has_code ) , paste ( " Has  user  notes : " , has_notes ) , sep = " \n " ) }
func: function ( sso , code = NULL ) { sso_check ( sso ) validate_model_code ( code ) if ( is . null ( code ) ) return ( slot ( sso , " model _ code " ) ) slot ( sso , " model _ code " ) < - code message ( paste0 ( " Successfully  added  code . " , " \n You  can  view  the  code  in  the " , " ShinyStan  GUI  on  the  ' Model  Code '  page . " ) ) sso }
func: function ( code ) { if ( is . null ( code ) | | is . character ( code ) ) { invisible ( TRUE ) }
func: function ( sso , note = NULL , replace = FALSE ) { sso_check ( sso ) if ( is . null ( note ) ) return ( slot ( sso , " user _ model _ info " ) ) if ( ! is . character ( note ) | | ! isTRUE ( length ( note ) == 1 ) ) stop ( " ' note '  should  be  a  single  string " ) slot ( sso , " user _ model _ info " ) < - if ( replace ) note else c ( slot ( sso , " user _ model _ info " ) , paste0 ( " \n \n " , note ) ) message ( paste ( " Successfully  added  note . " , " \n You  can  view  the  notes  in  the " , " ShinyStan  GUI  on  the  ' Notepad '  page . " ) ) sso }
func: function ( sso , name = NULL ) { sso_check ( sso ) if ( is . null ( name ) ) return ( slot ( sso , " model _ name " ) ) if ( ! is . character ( name ) | | ! isTRUE ( length ( name ) == 1 ) ) stop ( " ' name '  should  be  a  single  string " ) slot ( sso , " model _ name " ) < - name message ( paste ( " Successfully  changed  model  name  to " , name ) ) sso }
func: function  is  deprecated  and  will  be  removed  in  a  future  release .  Please  ENDCOM # '  use  the  \code { \link { model _ name }
func: function  instead . ENDCOM # '  @ export ENDCOM # '  @ keywords  internal ENDCOM # '  @ param  sso , new _ model _ name  Use  the  \code { \link { model _ name }
func: function  instead . ENDCOM rename_model < - function ( sso , new_model_name ) { . Deprecated ( " model _ name ( ) " ) model_name ( sso , new_model_name ) }
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.0.tok
func: function ( format = " json " ) { parameters = list ( format = format ) validate . Parameters ( params = parameters ) url = build . Query ( parameters = parameters , invoker = " Metadata " , method = " SampleTypes " ) ret = download . Data ( url , format ) return ( ret ) }
func: function ( ) { try ( { func < - get ( ' boring _ function ' , globalenv ( ) ) t1 < - identical ( func ( 9 ) , 9 ) t2 < - identical ( func ( 4 ) , 4 ) t3 < - identical ( func ( 0 ) , 0 ) ok < - all ( t1 , t2 , t3 ) }
func: function ( ) { try ( { func < - get ( ' my _ mean ' , globalenv ( ) ) t1 < - identical ( func ( 9 ) , mean ( 9 ) ) t2 < - identical ( func ( 1 : 10 ) , mean ( 1 : 10 ) ) t3 < - identical ( func ( c ( - 5 , - 2 , 4 , 10 ) ) , mean ( c ( - 5 , - 2 , 4 , 10 ) ) ) ok < - all ( t1 , t2 , t3 ) }
func: function ( ) { try ( { func < - get ( ' remainder ' , globalenv ( ) ) t1 < - identical ( func ( 9 , 4 ) , 9 % % 4 ) t2 < - identical ( func ( divisor = 5 , num = 2 ) , 2 % % 5 ) t3 < - identical ( func ( 5 ) , 5 % % 2 ) ok < - all ( t1 , t2 , t3 ) }
func: function ( ) { try ( { func < - get ( ' evaluate ' , globalenv ( ) ) t1 < - identical ( func ( sum , c ( 2 , 4 , 7 ) ) , 13 ) t2 < - identical ( func ( median , c ( 9 , 200 , 100 ) ) , 100 ) t3 < - identical ( func ( floor , 12.1 ) , 12 ) ok < - all ( t1 , t2 , t3 ) }
func: function ( ) { try ( { func < - get ( ' telegram ' , globalenv ( ) ) t1 < - identical ( func ( " Good " , " morning " ) , " START  Good  morning  STOP " ) t2 < - identical ( func ( " hello " , " there " , " sir " ) , " START  hello  there  sir  STOP " ) t3 < - identical ( func ( ) , " START  STOP " ) ok < - all ( t1 , t2 , t3 ) }
func: function ( ) { try ( { func < - get ( ' mad _ libs ' , globalenv ( ) ) t1 < - identical ( func ( place = " Baltimore " , adjective = " smelly " , noun = " Roger  Peng  statue " ) , " News  from  Baltimore  today  where  smelly  students  took  to  the  streets  in  protest  of  the  new  Roger  Peng  statue  being  installed  on  campus . " ) t2 < - identical ( func ( place = " Washington " , adjective = " angry " , noun = " Shake  Shack " ) , " News  from  Washington  today  where  angry  students  took  to  the  streets  in  protest  of  the  new  Shake  Shack  being  installed  on  campus . " ) ok < - all ( t1 , t2 ) }
func: function ( ) { try ( { func < - get ( ' % p % ' , globalenv ( ) ) t1 < - identical ( func ( " Good " , " job ! " ) , " Good  job ! " ) t2 < - identical ( func ( " one " , func ( " two " , " three " ) ) , " one  two  three " ) ok < - all ( t1 , t2 ) }
func: function ( ) { try ( { e < - get ( " e " , parent . frame ( ) ) expr < - e $ expr t1 < - identical ( expr [ [ 3 ] ] , 6 ) expr [ [ 3 ] ] < - 7 t2 < - identical ( eval ( expr ) , 8 ) ok < - all ( t1 , t2 ) }
func: function ( ) { try ( { e < - get ( " e " , parent . frame ( ) ) expr < - e $ expr t1 < - identical ( expr [ [ 3 ] ] , quote ( c ( 8 , 4 , 0 ) ) ) t2 < - identical ( expr [ [ 1 ] ] , quote ( evaluate ) ) expr [ [ 3 ] ] < - c ( 5 , 6 ) t3 < - identical ( eval ( expr ) , 5 ) ok < - all ( t1 , t2 , t3 ) }
func: function ( ) { try ( { e < - get ( " e " , parent . frame ( ) ) expr < - e $ expr t1 < - identical ( expr [ [ 3 ] ] , quote ( c ( 8 , 4 , 0 ) ) ) t2 < - identical ( expr [ [ 1 ] ] , quote ( evaluate ) ) expr [ [ 3 ] ] < - c ( 5 , 6 ) t3 < - identical ( eval ( expr ) , 6 ) ok < - all ( t1 , t2 , t3 ) }
func: function ( n ) { factorial = 1 #  check  is  the  number  is  negative ,  positive  or  zero ENDCOM if ( num < 0 ) { print ( ' Sorry  factorial  does  not  exist  for  negative  numbers ' ) }
func: function  simply  preprocesses  the  time  series  and  calls  RcppRPCP .  ENDCOM # '  @ return  ENDCOM # '  \itemize { ENDCOM # '  \item  X _ transform .  The  transformation  applied  to  the  time  series , ENDCOM # '  can  be  the  identity  or  could  be  differencing ENDCOM # '  \item  L _ transform .  The  low  rank  component  in  the  transformed  space ENDCOM # '  \item  S _ transform .  The  sparse  outliers  in  the  transformed  space ENDCOM # '  \item  E _ transform .  The  noise  in  the  transformed  space ENDCOM # '  \item  X _ original .  The  original  time  series ENDCOM # '  \item  time .  The  time  index ENDCOM # '  \item  name .  The  name  of  the  time  series  if  X  was  a  named  data  frame ENDCOM # '  @ references ENDCOM # '  The  following  are  recommended  educational  material : ENDCOM # '  \itemize { ENDCOM # '  \item  Candes '  paper  on  RPCP  \url { http : / / statweb . stanford . edu / ~ candes / papers / RobustPCA . pdf }
func: function ( X , frequency = 7 , dates = NULL , autodiff = T , forcediff = F , scale = T , L . penalty = 1 , s . penalty = 1.4 / sqrt ( max ( frequency , ifelse ( is . data . frame ( X ) , nrow ( X ) , length ( X ) ) / frequency ) ) , verbose = F ) { if ( is . vector ( X ) & ! is . data . frame ( X ) ) X = data . frame ( y = X ) time = if ( is . null ( dates ) ) 1 : nrow ( X ) else dates # look  through  columns  which  are  separate  time  series ENDCOM # transform  each  column  vector  into  a  matrix  with  nrow  =  observations  per  period ENDCOM # the  number  of  columns  will  be  equal  to  the  number  of  periods ENDCOM rpca . ts = apply ( X , 2 , function ( j ) { j . init = j [ 1 ] useddiff = F if ( forcediff ) { useddiff = T j = c ( 0 , diff ( j ) ) }
func: function ( i ) { if ( i $ diff ) { X . orig = c ( i $ j . init + cumsum ( ( as . vector ( i $ rpca $ X ) ) * i $ sd + i $ mean ) ) X . transform = ( as . vector ( i $ rpca $ X ) ) * i $ sd + i $ mean L . transform = ( as . vector ( i $ rpca $ L ) ) * i $ sd + i $ mean S . transform = ( as . vector ( i $ rpca $ S ) ) * i $ sd E . transform = ( as . vector ( i $ rpca $ E ) ) * i $ sd L . orig = cumsum ( L . transform ) + i $ j . init X . rough = X . orig - L . orig # S . orig  =  cumsum ( S . transform ) ENDCOM # E . orig  =  X . orig  -  L . orig  -  S . orig ENDCOM # X . rough . rpca  =  RcppRPCA ( matrix ( X . rough ,  nrow ( i $ rpca $ X ) ,  ncol ( i $ rpca $ X ) ) , ENDCOM #  Lpenalty  =  10 , ENDCOM #  Spenalty  =  2  /  sqrt ( 10 ) ) ENDCOM # S . orig  =  as . numeric ( X . rough . rpca $ S ) ENDCOM # E . orig  =  X . orig  -  L . orig  -  S . orig ENDCOM S . orig = softThreshold ( X . rough , 3 * ( 1 / sqrt ( 2 ) ) * sd ( E . transform ) ) E . orig = X . orig - ( L . orig ) - S . orig data . frame ( X . transform = X . transform , L . transform = L . transform , S . transform = S . transform , E . transform = E . transform , X . orig = X . orig , time = time ) [ - 1 , ] }
func: function ( df , name ) { rep ( name , nrow ( df ) ) }
func: function  which  shows  the  low  rank  signal  in  blue ,  the  random  noise  in  green , ENDCOM # '  and  any  outliers  in  red .  If  a  transformation  was  applied ,  these  signals  will  be  plotted ENDCOM # '  in  the  transformed  space ,  along  with  the  original  time  series ENDCOM # '  @ param  anomalyDetection  output  from  AnomalyDetection . rpca ENDCOM # '  @ import  ggplot2 ENDCOM # '  @ export ENDCOM # '  @ examples ENDCOM # '  frequency  =  7 ENDCOM # '  numPeriods  =  10 ENDCOM # '  ts . sinusoidal  =  sin ( (2  *  pi  /  frequency  )  *  1 : ( numPeriods  *  frequency ) ) ENDCOM # '  ts  =  ts . sinusoidal ENDCOM # '  ts  =  sin ( (2  *  pi  /  frequency  )  *  1 : ( numPeriods  *  frequency ) ) ENDCOM # '  ts [ 58:60 ]  =  100 ENDCOM # '  ggplot _ AnomalyDetection . rpca ( AnomalyDetection . rpca ( ts ) )  +  ggplot2 : : theme _ grey ( base _ size  =  25 ) ENDCOM ggplot_AnomalyDetection . rpca = function ( anomalyDetection ) { ggplot2 : : ggplot ( anomalyDetection , ggplot2 : : aes ( time , X_original ) ) + ggplot2 : : geom_line ( size = 1 ) + ggplot2 : : geom_line ( ggplot2 : : aes ( y = X_transform ) , size = 1 , color = " black " , linetype = ' dashed ' ) + ggplot2 : : geom_line ( ggplot2 : : aes ( y = L_transform ) , size = .5 , color = " blue " ) + ggplot2 : : geom_line ( ggplot2 : : aes ( y = E_transform ) , size = .5 , color = " green " ) + ggplot2 : : geom_point ( data = subset ( anomalyDetection , abs ( S_transform ) > 0 ) , color = " red " , ggplot2 : : aes ( size = abs ( S_transform ) ) ) + ggplot2 : : scale_size_continuous ( range = c ( 4 , 6 ) ) + ggplot2 : : facet_wrap ( ~ name , scale = " free " ) }
func: function ( x , penalty ) { sign ( x ) * pmax ( abs ( x ) - penalty , 0 ) }
func: function  initiates  a  download  of  the  NCDC  surface  station  history  file . ENDCOM # '  @ param  replace . file  selecting  ' yes '  will  overwrite  history  file  if  it  exists  in  the  working  directory . ENDCOM # '  @ export  calmet _ get _ ncdc _ history ENDCOM # '  @ examples ENDCOM # '  \dontrun { ENDCOM # '  #  Obtain  the  NCDC  history  file ENDCOM # '  calmet _ get _ ncdc _ history ( ) ENDCOM calmet_get_ncdc_history < - function ( replace . file = FALSE ) { #  Get  hourly  surface  data  history  CSV  from  NOAA / NCDC  FTP ENDCOM file < - " ftp : / / ftp . ncdc . noaa . gov / pub / data / noaa / isd - history . csv " if ( replace . file == TRUE ) { repeat { try ( download . file ( file , " ish - history . csv " , quiet = TRUE ) ) if ( file . info ( " ish - history . csv " ) $ size > 0 ) { break }
func: function ( x , y = NULL , correct = TRUE ) NEW_LINE { if ( is . matrix ( x ) ) { r < - nrow ( x ) if ( ( r < 2 ) | | ( ncol ( x ) != r ) ) stop ( " ' x '  must  be  square  with  at  least  two  rows  and  columns " ) if ( any ( x < 0 ) | | anyNA ( x ) ) stop ( " all  entries  of  ' x '  must  be  nonnegative  and  finite " ) DNAME < - deparse ( substitute ( x ) ) }
func: function ( expr , from = NULL , to = NULL , n = 101 , add = FALSE , type = " l " , xname = " x " , xlab = xname , ylab = NULL , log = NULL , xlim = NULL , ... ) NEW_LINE { sexpr < - substitute ( expr ) if ( is . name ( sexpr ) ) { # #  beter  than  parse ( )  ! ENDCOM expr < - call ( as . character ( sexpr ) , as . name ( xname ) ) }
func: function ,  or  a  call  or  an  expression  containing  ' % s ' " , xname ) , domain = NA ) expr < - sexpr }
func: function ( where ) { setClass ( " ClassUnionRepresentation " , " classRepresentation " , validity = function ( object ) { if ( identical ( object @ virtual , TRUE ) & & length ( object @ slots ) == 0 & & is . null ( object @ prototype ) ) TRUE else " Class  must  be  an  empty  virtual  class  with  NULL  prototype " }
func: function " , " NULL " ) , where ) setClassUnion ( " PossibleMethod " , c ( " function " , " MethodDefinition " ) , where ) clList < - c ( " ClassUnionRepresentation " , " OptionalFunction " , " PossibleMethod " ) assign ( " . SealedClasses " , c ( get ( " . SealedClasses " , where ) , clList ) , where ) }
func: function ( name , members = character ( ) , where = topenv ( parent . frame ( ) ) ) { if ( length ( members ) > 0 ) { membersDefined < - sapply ( members , isClass , where = as . environment ( where ) ) if ( ! all ( membersDefined ) ) stop ( gettextf ( " the  member  classes  must  be  defined :  not  true  of  % s " , paste ( . dQ ( as ( members [ ! membersDefined ] , " character " ) ) , collapse = " ,  " ) ) , domain = NA ) }
func: function ( Class ) { # #  test  the  class  DEFINITION  for  representing  a  union ENDCOM if ( is . character ( Class ) ) Class < - getClass ( Class , TRUE ) #  the  real  def .  or  a  dummy ENDCOM extends ( class ( Class ) , " ClassUnionRepresentation " ) }
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/test.with_comments.tok
func: function ( src ) { stopifnot ( is . src ( src ) ) query = " SELECT ' k _ ' | | id_kryterium AS kryterium , dystraktor , kolejnosc AS kolejnosc_dystr FROM pytania JOIN kryteria_oceny USING ( id_pytania ) JOIN sl_schematy_odp_dystr USING ( schemat_odp ) ORDER BY 1 , 3 " data = tbl ( src , sql ( e ( query ) ) ) return ( data ) }
func: function ,  this  sumes  the  euclidean  distance  over  the  entire  path ENDCOM distance < - function ( path ) { path2 < - embed ( path , 2 ) sqrt ( sum ( ( city . x [ path2 [ , 1 ] ] - city . x [ path2 [ , 2 ] ] ) ^ 2 + ( city . y [ path2 [ , 1 ] ] - city . y [ path2 [ , 2 ] ] ) ^ 2 ) ) }
func: function ( path ) { idx < - seq ( 2 , length ( path ) - 1 ) changepoints < - sample ( idx , size = 2 , replace = FALSE ) tmp < - path [ changepoints [ 1 ] ] path [ changepoints [ 1 ] ] < - path [ changepoints [ 2 ] ] path [ changepoints [ 2 ] ] < - tmp path }
func: function ( n ) { print ( n + 1 ) }
func: function ( df ) { n < - nrow ( df ) within ( df , { h < - h + rnorm ( n , sd = pi / 6 ) x < - x + s * cos ( h ) y < - y + s * sin ( h ) h [ x < - 1 ] < - 0 h [ x > 1 ] < - pi h [ y < - 1 ] < - pi / 2 h [ y > 1 ] < - - pi / 2 }
func: function ( x , a = 0 , k = 1 , b = 0.1 , m = 100 , v = 1 , q = 1 ) { a + ( k - a ) / ( ( 1 + q * exp ( - b * ( x - m ) ) ) ^ ( 1 / v ) ) }
func: function ( A , B ) { an = apply ( A , 1 , function ( rvec ) crossprod ( rvec , rvec ) ) bn = apply ( B , 1 , function ( rvec ) crossprod ( rvec , rvec ) ) m = nrow ( A ) n = nrow ( B ) tmp = matrix ( rep ( an , n ) , nrow = m ) tmp = tmp + matrix ( rep ( bn , m ) , nrow = m , byrow = TRUE ) sqrt ( tmp - 2 * tcrossprod ( A , B ) ) }
func: function ( df , affinSame = 0 , affinOther = 0 ) { dist < - pdist ( as . matrix ( df [ , 1 : 2 ] ) , as . matrix ( df [ , 1 : 2 ] ) ) neighbor < - dist < 0.125 b_neighbor < - apply ( df $ col == " # 107AB6" & neighbor , 2 , sum ) - ( df $ col == " # 107AB6" ) r_neighbor < - apply ( df $ col == " # D86810" & neighbor , 2 , sum ) - ( df $ col == " # D86810" ) nb < - ( df $ col == " # 107AB6" ) * affinSame * b_neighbor + ( df $ col == " # D86810" ) * affinSame * r_neighbor + ( df $ col == " # 107AB6" ) * affinOther * r_neighbor + ( df $ col == " # D86810" ) * affinOther * b_neighbor df $ s < - sigmoid ( nb , k = 0.1 , m = 4 , b = - 1 ) df }
func: function ( tbl , nuniques = 10 ) { selections < - purrr : : map_lgl ( tbl , function ( x ) { is . character ( x ) | | is . factor ( x ) | | is . logical ( x ) | | ( length ( unique ( x ) ) <= nuniques ) #  this  is  when  you  have  numeric  variables  with  few  uniques  ( dummies ) ENDCOM }
func: function ( tbl ) { selections < - purrr : : map_lgl ( tbl , function ( x ) is . numeric ( x ) ) tbl [ , selections ] }
func: function ( tbl ) { grp_cols < - names ( attr ( tbl , " labels " ) ) tbl % > % purrr : : map_if ( is . factor , as . character ) % > % #  avoid  warning ENDCOM as_data_frame ( ) % > % #  this  ungroup  the  tbl ENDCOM group_by_ ( . dots = lapply ( grp_cols , as . symbol ) ) % > % #  http : / / stackoverflow . com / questions / 21208801 / ENDCOM do ( { ezsum = tidyr : : gather ( . , key , value ) % > % #  you  can  use  tidyr : : gather ( . ,  variable ,  category ) ENDCOM ungroup ( ) % > % count ( key , value ) % > % mutate ( p = n / sum ( n ) ) }
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/valid.with_comments.tok
func: ['def dist_raw ( v1 , v2 ) : NEW_LINE INDENT delta = v1 - v2 NEW_LINE return sp . linalg . norm ( delta . toarray ( ) ) NEW_LINE DEDENT', 'def dist_norm ( v1 , v2 ) : NEW_LINE INDENT v1_normalized = v1 / sp . linalg . norm ( v1 . toarray ( ) ) NEW_LINE v2_normalized = v2 / sp . linalg . norm ( v2 . toarray ( ) ) NEW_LINE delta = v1_normalized - v2_normalized NEW_LINE return sp . linalg . norm ( delta . toarray ( ) ) NEW_LINE DEDENT']
func: function ( x ) { x . tab < - table ( x ) uniq < - length ( which ( x . tab == 1 ) ) return ( uniq ) }
func: function ( word ) { result < - toupper ( word ) print ( result ) }
func: function ( word ) { result < - tolower ( word ) print ( result ) }
func: function ( maf , useSyn = FALSE , plot = TRUE , file = NULL ) NEW_LINE { # Synonymous  variants ENDCOM maf . silent = maf @ maf . silent # Main  data ENDCOM maf = maf @ data # in  case  user  read  maf  without  removing  silent  variants ,  remove  theme  here . ENDCOM silent = c ( "3 ' UTR " , "5 ' UTR " , "3 ' Flank " , " Targeted _ Region " , " Silent " , " Intron " , " RNA " , " IGR " , " Splice _ Region " , "5 ' Flank " , " lincRNA " ) maf = maf [ ! Variant_Classification % in % silent ] # Remove  silent  variants  from  main  table ENDCOM if ( useSyn ) { maf = rbind ( maf , maf . silent , fill = TRUE ) }
func: function ( url , type = " source " , filters = NULL , head = 5 , cols = c ( " Package " , " Version " ) ) { contribUrl < - contrib . url ( url , type = type ) p < - available . packages ( contribUrl , type = type , filters = filters ) p [ 1 : head , cols ] }
func: function ( line , cursor = nchar ( line ) ) NEW_LINE { str ( utils : : : . win32consoleCompletion ( line , cursor ) ) }

extract functions ... 
