r: process ...
r: tokenizing 2 json files ...
input_path: /home/gcloud/TransCoder/data/test_dataset/r/r.003.json.gz
language: r
output_path: /home/gcloud/TransCoder/data/test_dataset/r/r.003.with_comments.tok
line: b'{ "repo_name": "SurajGupta/r-source", "ref": "refs/heads/master", "path": "src/library/methods/R/ClassUnion.R", "content": "#  File src/library/methods/R/ClassUnion.R\\n#  Part of the R package, https://www.R-project.org\\n#\\n#  Copyright (C) 1995-2012 The R Core Team\\n#\\n#  This program is free software; you can redistribute it and/or modify\\n#  it under the terms of the GNU General Public License as published by\\n#  the Free Software Foundation; either version 2 of the License, or\\n#  (at your option) any later version.\\n#\\n#  This program is distributed in the hope that it will be useful,\\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#  GNU General Public License for more details.\\n#\\n#  A copy of the GNU General Public License is available at\\n#  https://www.R-project.org/Licenses/\\n\\n.InitClassUnion <- function(where) {\\n    setClass(\\"ClassUnionRepresentation\\",  \\"classRepresentation\\",\\n             validity =function(object) {\\n                 if(identical(object@virtual, TRUE) && length(object@slots)==0 &&\\n                    is.null(object@prototype))\\n                     TRUE\\n                 else\\n                     \\"Class must be an empty virtual class with NULL prototype\\"\\n             }, where = where)\\n    ## some classes in methods package are unions--now they can be officially\\n    setClassUnion(\\"OptionalFunction\\", c(\\"function\\", \\"NULL\\"), where)\\n    setClassUnion(\\"PossibleMethod\\", c(\\"function\\", \\"MethodDefinition\\"), where)\\n    clList <- c(\\"ClassUnionRepresentation\\", \\"OptionalFunction\\",\\n                \\"PossibleMethod\\")\\n    assign(\\".SealedClasses\\", c(get(\\".SealedClasses\\", where), clList), where)\\n}\\n\\nsetClassUnion <- function(name, members = character(), where = topenv(parent.frame())) {\\n    if(length(members)>0) {\\n        membersDefined <- sapply(members, isClass, where = as.environment(where))\\n        if(!all(membersDefined))\\n            stop(gettextf(\\"the member classes must be defined: not true of %s\\",\\n                          paste(.dQ(as(members[!membersDefined], \\"character\\")), collapse=\\", \\")), domain = NA)\\n    }\\n    def <- new(\\"ClassUnionRepresentation\\",\\n               makeClassRepresentation(name, package = getPackageName(where), where = where))\\n    prev <- getClassDef(name, where = where)\\n    value <- setClass(name, def, where = where)\\n    failed <- character()\\n    ## the prototype of the union will be from the first non-virtual\\n    ## subclass, except that we prefer NULL if \\"NULL\\" is a subclass\\n    hasNull <- match(\\"NULL\\", members, 0)\\n    if(hasNull)\\n        members <- c(\\"NULL\\", members[-hasNull])\\n    for(what in members) {\\n        if(is(try(setIs(what, name, where = where)), \\"try-error\\")) {\\n            if(!is.character(what))\\n                what <- getClass(what, TRUE, where)@className\\n            failed <- c(failed, what)\\n        }\\n    }\\n    if(length(failed)>0) {\\n        if(is.null(prev))\\n            try(removeClass(name, where = where))\\n        else\\n            try(setClass(name, prev, where = where))\\n        stop(gettextf(\\"unable to create union class:  could not set members %s\\",\\n                      paste(.dQ(failed), collapse=\\", \\")), domain = NA)\\n    }\\n    invisible(value)\\n}\\n\\nisClassUnion <- function(Class) {\\n    ## test the class DEFINITION for representing a union\\n    if(is.character(Class))\\n        Class <- getClass(Class, TRUE) # the real def. or a dummy\\n    extends(class(Class), \\"ClassUnionRepresentation\\")\\n}\\n" }\n'
line: b'{ "repo_name": "SurajGupta/r-source", "ref": "refs/heads/master", "path": "src/library/utils/tests/completion.R", "content": "\\n## test some typical completion attempts\\n\\ntestLine <- function(line, cursor = nchar(line))\\n{\\n    str(utils:::.win32consoleCompletion(line, cursor))\\n}\\n\\ntestLine(\\"\\")\\n\\ntestLine(\\"lib\\")\\ntestLine(\\"data(\\")\\ntestLine(\\"data(US\\")\\ntestLine(\\"data(US\\", 3)\\n\\ntestLine(\\"?INS\\")\\n\\ntestLine(\\"utils::data\\")\\ntestLine(\\"utils:::.show_help_on_topic_\\")\\ntestLine(\\"utils::.show_help_on_topic_\\")\\n\\ntestLine(\\"update(\\")\\n\\ntestLine(\\"version$m\\")\\ntestLine(\\"nchar(version[\\")\\n\\n\\n\\ntestLine(\\"method?coe\\")\\ntestLine(\\"?coe\\")\\ntestLine(\\"?\\\\\\"coerce,AN\\")\\ntestLine(\\"method?\\\\\\"coerce,AN\\")\\n\\n\\n## testLine(\\"\\")\\n## testLine(\\"\\")\\n## testLine(\\"\\")\\n" }\n'
line: b'{ "repo_name": "SurajGupta/r-source", "ref": "refs/heads/master", "path": "src/library/graphics/R/curve.R", "content": "#  File src/library/graphics/R/curve.R\\n#  Part of the R package, https://www.R-project.org\\n#\\n#  Copyright (C) 1995-2012 The R Core Team\\n#\\n#  This program is free software; you can redistribute it and/or modify\\n#  it under the terms of the GNU General Public License as published by\\n#  the Free Software Foundation; either version 2 of the License, or\\n#  (at your option) any later version.\\n#\\n#  This program is distributed in the hope that it will be useful,\\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#  GNU General Public License for more details.\\n#\\n#  A copy of the GNU General Public License is available at\\n#  https://www.R-project.org/Licenses/\\n\\ncurve <- function(expr, from = NULL, to = NULL, n = 101, add = FALSE,\\n                  type = \\"l\\", xname = \\"x\\", xlab = xname,\\n                  ylab = NULL, log = NULL, xlim = NULL, ...)\\n{\\n    sexpr <- substitute(expr)\\n    if (is.name(sexpr)) {\\n        ## beter than parse() !\\n        expr <- call(as.character(sexpr), as.name(xname))\\n    } else {\\n\\tif ( !( (is.call(sexpr) || is.expression(sexpr)) &&\\n              xname %in% all.vars(sexpr) ))\\n\\t    stop(gettextf(\\"\'expr\' must be a function, or a call or an expression containing \'%s\'\\", xname), domain = NA)\\n\\texpr <- sexpr\\n    }\\n    if (dev.cur() == 1L && !identical(add, FALSE)) {\\n        warning(\\"\'add\' will be ignored as there is no existing plot\\")\\n        add <- FALSE\\n    }\\n    addF <- identical(add, FALSE)\\n    if (is.null(ylab)) ylab <- deparse(expr)\\n    if (is.null(from) || is.null(to)) {\\n        xl <- if (!is.null(xlim)) xlim\\n        else if (!addF) {\\n            ## determine xlim of current plot.\\n            pu <- par(\\"usr\\")[1L:2L]\\n            if (par(\\"xaxs\\") == \\"r\\") pu <- extendrange(pu, f = -1/27)\\n            if (par(\\"xlog\\")) 10^pu else pu\\n       } else c(0, 1) # was c(1/27, 26/27) in R < 2.14.0\\n        if (is.null(from)) from <- xl[1L]\\n        if (is.null(to)) to <- xl[2L]\\n    }\\n    lg <- if (length(log)) log else if (!addF && par(\\"xlog\\")) \\"x\\" else \\"\\"\\n    if (length(lg) == 0) lg <- \\"\\"\\n    if (grepl(\\"x\\", lg, fixed = TRUE)) {\\n        if (from <= 0 || to <= 0)\\n            stop(\\"\'from\' and \'to\' must be > 0 with log=\\\\\\"x\\\\\\"\\")\\n        x <- exp(seq.int(log(from), log(to), length.out = n))\\n    } else x <- seq.int(from, to, length.out = n)\\n    ll <- list(x = x); names(ll) <- xname\\n    y <- eval(expr, envir = ll, enclos = parent.frame())\\n    if (length(y) != length(x))\\n        stop(\\"\'expr\' did not evaluate to an object of length \'n\'\\")\\n    if (isTRUE(add))\\n\\tlines(x = x, y = y, type = type, ...)\\n    else\\n        plot(x = x, y = y, type = type, xlab = xlab, ylab = ylab,\\n             xlim = xlim, log = lg, ...)\\n    invisible(list(x = x, y = y))\\n}\\n" }\n'
line: b'{ "repo_name": "SurajGupta/r-source", "ref": "refs/heads/master", "path": "tests/reg-plot-latin1.R", "content": "pdf(file = \\"reg-plot-latin1.pdf\\", encoding = \\"ISOLatin1\\",\\n    width = 7, height = 7, paper = \\"a4r\\", compress = FALSE)\\nlibrary(graphics) # to be sure\\nexample(text)     # has examples that need to he plotted in latin-1\\nq(\\"no\\")\\n" }\n'
line: b'{ "repo_name": "SurajGupta/r-source", "ref": "refs/heads/master", "path": "src/library/base/R/lapply.R", "content": "#  File src/library/base/R/lapply.R\\n#  Part of the R package, https://www.R-project.org\\n#\\n#  Copyright (C) 1995-2012 The R Core Team\\n#\\n#  This program is free software; you can redistribute it and/or modify\\n#  it under the terms of the GNU General Public License as published by\\n#  the Free Software Foundation; either version 2 of the License, or\\n#  (at your option) any later version.\\n#\\n#  This program is distributed in the hope that it will be useful,\\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#  GNU General Public License for more details.\\n#\\n#  A copy of the GNU General Public License is available at\\n#  https://www.R-project.org/Licenses/\\n\\nlapply <- function (X, FUN, ...)\\n{\\n    FUN <- match.fun(FUN)\\n    ## internal code handles all vector types, including expressions\\n    ## However, it would be OK to have attributes which is.vector\\n    ## disallows.\\n    if(!is.vector(X) || is.object(X)) X <- as.list(X)\\n    ## Note ... is not passed down.  Rather the internal code\\n    ## evaluates FUN(X[i], ...) in the frame of this function\\n    .Internal(lapply(X, FUN))\\n}\\n\\nrapply <-\\n    function(object, f, classes = \\"ANY\\", deflt = NULL,\\n             how = c(\\"unlist\\", \\"replace\\", \\"list\\"), ...)\\n{\\n    if(typeof(object) != \\"list\\")\\n        stop(\\"\'object\' must be a list\\")\\n    how <- match.arg(how)\\n    res <- .Internal(rapply(object, f, classes, deflt, how))\\n    if(how == \\"unlist\\") unlist(res, recursive = TRUE) else res\\n}\\n" }\n'
line: b'{ "repo_name": "SurajGupta/r-source", "ref": "refs/heads/master", "path": "src/library/stats/R/mcnemar.test.R", "content": "#  File src/library/stats/R/mcnemar.test.R\\n#  Part of the R package, https://www.R-project.org\\n#\\n#  Copyright (C) 1995-2013 The R Core Team\\n#\\n#  This program is free software; you can redistribute it and/or modify\\n#  it under the terms of the GNU General Public License as published by\\n#  the Free Software Foundation; either version 2 of the License, or\\n#  (at your option) any later version.\\n#\\n#  This program is distributed in the hope that it will be useful,\\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#  GNU General Public License for more details.\\n#\\n#  A copy of the GNU General Public License is available at\\n#  https://www.R-project.org/Licenses/\\n\\nmcnemar.test <- function(x, y = NULL, correct = TRUE)\\n{\\n    if (is.matrix(x)) {\\n        r <- nrow(x)\\n        if ((r < 2) || (ncol (x) != r))\\n            stop(\\"\'x\' must be square with at least two rows and columns\\")\\n        if (any(x < 0) || anyNA(x))\\n            stop(\\"all entries of \'x\' must be nonnegative and finite\\")\\n        DNAME <- deparse(substitute(x))\\n    }\\n    else {\\n        if (is.null(y))\\n            stop(\\"if \'x\' is not a matrix, \'y\' must be given\\")\\n        if (length(x) != length(y))\\n            stop(\\"\'x\' and \'y\' must have the same length\\")\\n        DNAME <- paste(deparse(substitute(x)), \\"and\\",\\n                       deparse(substitute(y)))\\n        OK <- complete.cases(x, y)\\n        x <- as.factor(x[OK])\\n        y <- as.factor(y[OK])\\n        r <- nlevels(x)\\n        if ((r < 2) || (nlevels(y) != r))\\n            stop(\\"\'x\' and \'y\' must have the same number of levels (minimum 2)\\")\\n        x <- table(x, y)\\n    }\\n\\n    PARAMETER <- r * (r-1) / 2\\n    METHOD <- \\"McNemar\'s Chi-squared test\\"\\n\\n    if (correct && (r == 2) && any(x - t(x) != 0)) {\\n        y <- (abs(x - t(x)) - 1)\\n        METHOD <- paste(METHOD, \\"with continuity correction\\")\\n    }\\n    else\\n        y <- x - t(x)\\n    x <- x + t(x)\\n\\n    STATISTIC <- sum(y[upper.tri(x)]^2 / x[upper.tri(x)])\\n    PVAL <- pchisq(STATISTIC, PARAMETER, lower.tail = FALSE)\\n    names(STATISTIC) <- \\"McNemar\'s chi-squared\\"\\n    names(PARAMETER) <- \\"df\\"\\n\\n    RVAL <- list(statistic = STATISTIC,\\n                 parameter = PARAMETER,\\n                 p.value = PVAL,\\n                 method = METHOD,\\n                 data.name = DNAME)\\n    class(RVAL) <- \\"htest\\"\\n    return(RVAL)\\n}\\n" }\n'
line: b'{ "repo_name": "SurajGupta/r-source", "ref": "refs/heads/master", "path": "src/library/base/R/outer.R", "content": "#  File src/library/base/R/outer.R\\n#  Part of the R package, https://www.R-project.org\\n#\\n#  Copyright (C) 1995-2013 The R Core Team\\n#\\n#  This program is free software; you can redistribute it and/or modify\\n#  it under the terms of the GNU General Public License as published by\\n#  the Free Software Foundation; either version 2 of the License, or\\n#  (at your option) any later version.\\n#\\n#  This program is distributed in the hope that it will be useful,\\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#  GNU General Public License for more details.\\n#\\n#  A copy of the GNU General Public License is available at\\n#  https://www.R-project.org/Licenses/\\n\\nouter <- function (X, Y, FUN = \\"*\\", ...)\\n{\\n    if(is.array(X)) {\\n        dX <- dim(X)\\n        nx <- dimnames(X)\\n        no.nx <- is.null(nx)\\n    } else { # a vector\\n        dX <- length(X)  # cannot be long, as form a matrix below\\n        no.nx <- is.null(names(X))\\n        if(!no.nx) nx <- list(names(X))\\n    }\\n    if(is.array(Y)) {\\n        dY <- dim(Y)\\n        ny <- dimnames(Y)\\n        no.ny <- is.null(ny)\\n    } else { # a vector\\n        dY <- length(Y)\\n        no.ny <- is.null(names(Y))\\n        if(!no.ny) ny <- list(names(Y))\\n    }\\n    if (is.character(FUN) && FUN==\\"*\\") {\\n        if(!missing(...)) stop(\'using ... with FUN = \\"*\\" is an error\')\\n        # this is for numeric vectors, so dropping attributes is OK\\n        robj <- as.vector(X) %*% t(as.vector(Y))\\n        dim(robj) <- c(dX, dY)\\n    } else {\\n        FUN <- match.fun(FUN)\\n        ## Y may have a class, so don\'t use rep.int\\n        Y <- rep(Y, rep.int(length(X), length(Y)))\\n        ##  length.out is not an argument of the generic rep()\\n        ##  X <- rep(X, length.out = length(Y))\\n        if(length(X))\\n            X <- rep(X, times = ceiling(length(Y)/length(X)))\\n        robj <- FUN(X, Y, ...)\\n        dim(robj) <- c(dX, dY) # careful not to lose class here\\n    }\\n    ## no dimnames if both don\'t have ..\\n    if(!(no.nx && no.ny)) {\\n\\tif(no.nx) nx <- vector(\\"list\\", length(dX)) else\\n\\tif(no.ny) ny <- vector(\\"list\\", length(dY))\\n\\tdimnames(robj) <- c(nx, ny)\\n    }\\n    robj\\n}\\n\\n## Binary operator, hence don\'t simply do \\"%o%\\" <- outer.\\n`%o%` <- function(X, Y) outer(X, Y)\\n" }\n'
line: b'{ "repo_name": "SurajGupta/r-source", "ref": "refs/heads/master", "path": "src/library/stats/R/p.adjust.R", "content": "#  File src/library/stats/R/p.adjust.R\\n#  Part of the R package, https://www.R-project.org\\n#\\n#  Copyright (C) 1995-2012 The R Core Team\\n#\\n#  This program is free software; you can redistribute it and/or modify\\n#  it under the terms of the GNU General Public License as published by\\n#  the Free Software Foundation; either version 2 of the License, or\\n#  (at your option) any later version.\\n#\\n#  This program is distributed in the hope that it will be useful,\\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#  GNU General Public License for more details.\\n#\\n#  A copy of the GNU General Public License is available at\\n#  https://www.R-project.org/Licenses/\\n\\np.adjust.methods <-\\n    c(\\"holm\\", \\"hochberg\\", \\"hommel\\", \\"bonferroni\\", \\"BH\\", \\"BY\\", \\"fdr\\", \\"none\\")\\n\\np.adjust <- function(p, method = p.adjust.methods, n = length(p))\\n{\\n    ## Methods \'Hommel\', \'BH\', \'BY\' and speed improvements\\n    ## contributed by Gordon Smyth\\n    method <- match.arg(method)\\n    if(method == \\"fdr\\") method <- \\"BH\\"\\t# back compatibility\\n    nm <- names(p)\\n    p <- as.numeric(p)\\n    p0 <- setNames(p, nm)\\n    if(all(nna <- !is.na(p))) nna <- TRUE\\n    p <- p[nna]\\n    lp <- length(p)\\n    stopifnot(n >= lp)\\n    if (n <= 1) return(p0)\\n    if (n == 2 && method == \\"hommel\\") method <- \\"hochberg\\"\\n\\n    p0[nna] <-\\n\\tswitch(method,\\n\\t       bonferroni = pmin(1, n * p),\\n\\t       holm = {\\n\\t\\t   i <- seq_len(lp)\\n\\t\\t   o <- order(p)\\n\\t\\t   ro <- order(o)\\n\\t\\t   pmin(1, cummax( (n - i + 1L) * p[o] ))[ro]\\n\\t       },\\n\\t       hommel = { ## needs n-1 >= 2 in for() below\\n\\t\\t   if(n > lp) p <- c(p, rep.int(1, n-lp))\\n\\t\\t   i <- seq_len(n)\\n\\t\\t   o <- order(p)\\n\\t\\t   p <- p[o]\\n\\t\\t   ro <- order(o)\\n\\t\\t   q <- pa <- rep.int( min(n*p/i), n)\\n\\t\\t   for (j in (n-1):2) {\\n\\t\\t       ij <- seq_len(n-j+1)\\n\\t\\t       i2 <- (n-j+2):n\\n\\t\\t       q1 <- min(j*p[i2]/(2:j))\\n\\t\\t       q[ij] <- pmin(j*p[ij], q1)\\n\\t\\t       q[i2] <- q[n-j+1]\\n\\t\\t       pa <- pmax(pa,q)\\n\\t\\t   }\\n\\t\\t   pmax(pa,p)[if(lp < n) ro[1:lp] else ro]\\n\\t       },\\n\\t       hochberg = {\\n\\t\\t   i <- lp:1L\\n\\t\\t   o <- order(p, decreasing = TRUE)\\n\\t\\t   ro <- order(o)\\n\\t\\t   pmin(1, cummin( (n - i + 1L) * p[o] ))[ro]\\n\\t       },\\n\\t       BH = {\\n\\t\\t   i <- lp:1L\\n\\t\\t   o <- order(p, decreasing = TRUE)\\n\\t\\t   ro <- order(o)\\n\\t\\t   pmin(1, cummin( n / i * p[o] ))[ro]\\n\\t       },\\n\\t       BY = {\\n\\t\\t   i <- lp:1L\\n\\t\\t   o <- order(p, decreasing = TRUE)\\n\\t\\t   ro <- order(o)\\n\\t\\t   q <- sum(1L/(1L:n))\\n\\t\\t   pmin(1, cummin(q * n / i * p[o]))[ro]\\n\\t       },\\n\\t       none = p)\\n    p0\\n}\\n" }\n'
line: b'{ "repo_name": "SurajGupta/r-source", "ref": "refs/heads/master", "path": "src/library/base/R/paste.R", "content": "#  File src/library/base/R/paste.R\\n#  Part of the R package, https://www.R-project.org\\n#\\n#  Copyright (C) 1995-2012 The R Core Team\\n#\\n#  This program is free software; you can redistribute it and/or modify\\n#  it under the terms of the GNU General Public License as published by\\n#  the Free Software Foundation; either version 2 of the License, or\\n#  (at your option) any later version.\\n#\\n#  This program is distributed in the hope that it will be useful,\\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#  GNU General Public License for more details.\\n#\\n#  A copy of the GNU General Public License is available at\\n#  https://www.R-project.org/Licenses/\\n\\npaste <- function (..., sep = \\" \\", collapse = NULL)\\n    .Internal(paste(list(...), sep, collapse))\\npaste0 <- function(..., collapse = NULL)\\n    .Internal(paste0(list(...), collapse))\\n\\n##=== Could we extend  paste(.) to (optionally) accept a\\n##    2-vector for collapse ?\\t With the following functionality\\n\\n##- paste.extra <- function(r, collapse=c(\\", \\",\\" and \\")) {\\n##-\\t    n <- length(r)\\n##-\\t    if(n <= 1) paste(r)\\n##-\\t    else\\n##-\\t      paste(paste(r[-n],collapse=collapse[1L]),\\n##-\\t\\t    r[n], sep=collapse[min(2,length(collapse))])\\n##- }\\n" }\n'
line: b'{ "repo_name": "jimthompson5802/kaggle-BNP-Paribas", "ref": "refs/heads/master", "path": "src/L0_xgb21/train_model.R", "content": "###\\n# training skeleton\\n###\\n\\nlibrary(data.table)\\nlibrary(caret)\\n# add any model specific package library commands\\nlibrary(xgboost)\\n\\n# set working directory\\nWORK.DIR <- \\"./src/L0_xgb21\\"  # modify to specify directory to contain model artififacts\\n\\n# Common Functions and Global variables\\nsource(\\"./src/CommonFunctions.R\\")\\n\\n# import model configuration parameters\\nsource(paste0(WORK.DIR,\\"/model_parameters.R\\"))\\n\\nMODEL.COMMENT <- \\"Build Model\\"\\n\\n\\n# model specific training parameter\\nCARET.TRAIN.CTRL <- trainControl(method=\\"none\\",\\n                                 number=5,\\n                                 repeats=1,\\n                                 verboseIter=FALSE,\\n                                 classProbs=TRUE,\\n                                 summaryFunction=caretLogLossSummary)\\n\\nCARET.TRAIN.OTHER.PARMS <- list(trControl=CARET.TRAIN.CTRL,\\n                            maximize=FALSE,\\n                           tuneGrid=CARET.TUNE.GRID,\\n                           tuneLength=5,\\n                           metric=\\"LogLoss\\")\\n\\n\\n\\n# amount of data to train\\nFRACTION.TRAIN.DATA <- 1.0\\n\\n# force recording model flag\\nFORCE_RECORDING_MODEL <- TRUE\\n\\n# get training data\\ntrain.df <- fread(paste0(DATA.DIR,\\"/train.csv\\"))\\nsetkey(train.raw,ID)\\n\\nif (FRACTION.TRAIN.DATA != 1.0) {\\n    # extract subset for inital training\\n    set.seed(29)\\n    idx <- createDataPartition(train.df$target,p=FRACTION.TRAIN.DATA,list=FALSE)\\n    train.df <- train.df[idx,]\\n}\\n\\n# prepare data for training\\ntrain.data <- PREPARE.MODEL.DATA(train.df)\\n\\nlibrary(doMC)\\nregisterDoMC(cores = 6)\\n\\n# library(doSNOW)\\n# cl <- makeCluster(5,type=\\"SOCK\\")\\n# registerDoSNOW(cl)\\n# clusterExport(cl,list(\\"logLossEval\\"))\\n\\n# train the model\\nSys.time()\\nset.seed(825)\\n\\ntime.data <- system.time(mdl.fit <- do.call(train,c(list(x=train.data$predictors,\\n                                                         y=train.data$response),\\n                                                    CARET.TRAIN.PARMS,\\n                                                    MODEL.SPECIFIC.PARMS,\\n                                                    CARET.TRAIN.OTHER.PARMS)))\\n\\ntime.data\\nmdl.fit\\n# stopCluster(cl)\\n\\ncat(\\"saving...\\\\n\\")\\ndate.time <- as.character(Sys.time())\\nfile.name <- paste0(\\"model_\\",CARET.TRAIN.PARMS$method,\\"_\\",date.time[last.idx],\\".RData\\")\\nfile.name <- gsub(\\" \\",\\"_\\",file.name)\\nfile.name <- gsub(\\":\\",\\"_\\",file.name)\\n\\nsave(mdl.fit,PREPARE.MODEL.DATA,file=paste0(WORK.DIR,\\"/\\",file.name))\\n\\n# estalish pointer to current model\\nwriteLines(file.name,paste0(WORK.DIR,\\"/this_model\\"))\\n\\n\\n\\n\\n" } \n'
line: b'{ "repo_name": "jimthompson5802/kaggle-BNP-Paribas", "ref": "refs/heads/master", "path": "src/L0_xtc31/train_model.R", "content": "###\\n# training skeleton\\n###\\n\\nlibrary(data.table)\\nlibrary(caret)\\n# add any model specific package library commands\\n\\n\\n# set working directory\\nWORK.DIR <- \\"./src/L0_xtc31\\"  # modify to specify directory to contain model artififacts\\n\\n# Common Functions and Global variables\\nsource(\\"./src/CommonFunctions.R\\")\\n\\n# import model configuration parameters\\nsource(paste0(WORK.DIR,\\"/model_parameters.R\\"))\\n\\n# import model configuration parameters\\nsource(paste0(WORK.DIR,\\"/model_parameters.R\\"))\\n\\nMODEL.COMMENT <- \\"K-Fold, Build Model\\"\\n\\n\\n# amount of data to train\\nFRACTION.TRAIN.DATA <- 1.0\\n\\n# force recording model flag\\nFORCE_RECORDING_MODEL <- FALSE\\n\\n# get training data\\ntrain.df <- fread(paste0(DATA.DIR,\\"/train.csv\\"))\\nsetkey(train.df,ID)\\n\\nif (FRACTION.TRAIN.DATA != 1.0) {\\n    # extract subset for inital training\\n    set.seed(29)\\n    idx <- createDataPartition(train.df$target,p=FRACTION.TRAIN.DATA,list=FALSE)\\n    train.df <- train.df[idx,]\\n}\\n\\n# prepare data for training\\ntrain.data <- PREPARE.MODEL.DATA(train.df)\\n\\n# save prepared training data for Python function\\n# put response as first column in data set\\nwrite.table(cbind(response=train.data$response,train.data$predictors),\\n            file=paste0(WORK.DIR,\\"/py_train.tsv\\"),row.names = FALSE,\\n            sep=\\"\\\\t\\")\\n\\n\\n# invoke Python training model\\npython.train.command <- paste(PYTHON_COMMAND,paste0(WORK.DIR,\\"/train_model.py\\"),WORK.DIR)\\n\\nSys.time()\\n\\n\\ntime.data <- system.time(system(python.train.command))\\n\\ntime.data\\n\\n\\ncat(\\"saving...\\\\n\\")\\ndate.time <- as.character(Sys.time())\\nfile.name <- paste0(\\"model_\\",MODEL.NAME,\\"_\\",date.time,\\".RData\\")\\nfile.name <- gsub(\\" \\",\\"_\\",file.name)\\nfile.name <- gsub(\\":\\",\\"_\\",file.name)\\nsave(PREPARE.MODEL.DATA,file=paste0(WORK.DIR,\\"/\\",file.name))\\n\\n# save Python model data\\npy.file.name <- paste0(\\"model_\\",MODEL.NAME,\\"_\\",date.time,\\".PyData\\")\\npy.file.name <- gsub(\\" \\",\\"_\\",py.file.name)\\npy.file.name <- gsub(\\":\\",\\"_\\",py.file.name)\\nfile.rename(paste0(WORK.DIR,\\"/possible_model\\"),paste0(WORK.DIR,\\"/\\",py.file.name))\\n\\n# estalish pointer to current model\\nwriteLines(c(file.name,py.file.name),paste0(WORK.DIR,\\"/this_model\\"))\\n\\n# clean up files no longer needed\\nfile.remove(paste0(WORK.DIR,\\"/py_train.tsv\\"))\\n              #paste0(WORK.DIR,\\"/py_test.tsv\\"),\\n              #paste0(WORK.DIR,\\"/py_test_predictions.tsv\\")\\n              #))\\n\\n\\n" } \n'
line: b'{ "repo_name": "jimthompson5802/kaggle-BNP-Paribas", "ref": "refs/heads/master", "path": "src/L2_nnet1/create_submission.R", "content": "###\\n#  create ensemble model combining selected models\\n###\\n\\nlibrary(data.table)\\nlibrary(caret)\\n\\n\\n# import global variabels and common functions\\nsource(\\"./src/CommonFunctions.R\\")\\nWORK.DIR <- \\"./src/L2_nnet1\\"\\n\\n# retrive generated blending weights data structure\\nmodel.file.name <- readLines(paste0(WORK.DIR,\\"/this_model\\"))\\nload(paste0(WORK.DIR,\\"/\\",model.file.name))\\n\\n# retrieve Level 1 submissions\\n# L1_gbm2\\ngbm2.pred.probs <- read.csv(\\"./src/L1_gbm2/submission.csv\\")\\n\\n#L1_nnet1\\nnnet1.pred.probs <- read.csv(\\"./src/L1_nnet1/submission.csv\\")\\n\\n#create data for predictions\\nsubmission <- list()\\nsubmission$predictors <- cbind(gbm2=gbm2.pred.probs[,\\"PredictedProb\\"],\\n                               nnet1=nnet1.pred.probs[,\\"PredictedProb\\"])\\n\\n# make prediction\\npred.probs <- predict(mdl.fit,newdata = submission$predictors,type = \\"prob\\")\\n\\n#create kaggle submission file\\nwrite.csv(data.frame(ID=gbm2.pred.probs[,\\"ID\\"],PredictedProb=pred.probs[,\\"Class_1\\"]),file=paste0(WORK.DIR,\\"/submission.csv\\"),\\n          row.names=FALSE)\\n\\n" }\n'
line: b'{ "repo_name": "jimthompson5802/kaggle-BNP-Paribas", "ref": "refs/heads/master", "path": "src/L0_xtc31/create_level1_features.R", "content": "###\\n# Model training\\n###\\n\\nlibrary(data.table)\\nlibrary(plyr)\\nlibrary(caret)\\n# add any model specific package library commands\\n\\n\\n# set working directory\\nWORK.DIR <- \\"./src/L0_xtc31\\"  # modify to specify directory to contain model artififacts\\n\\n# Common Functions and Global variables\\nsource(\\"./src/CommonFunctions.R\\")\\n\\n# import model configuration parameters\\nsource(paste0(WORK.DIR,\\"/model_parameters.R\\"))\\n\\n\\n\\nMODEL.COMMENT <- \\"prepL0FeatureSet3, 5-fold training\\"\\n\\n# amount of data to train\\nFRACTION.TRAIN.DATA <- 1.0\\n\\n# force recording model flag\\nFORCE_RECORDING_MODEL <- FALSE\\n\\n# get training data\\ntrain.raw <- fread(paste0(DATA.DIR,\\"/train.csv\\"))\\nsetkey(train.raw,ID)\\n\\n# get data fold specification\\nload(paste0(DATA.DIR,\\"/fold_specification.RData\\"))\\n\\n\\ntrainFolds <- function(this.fold) {\\n    # prepare data for training\\n    test.data <- PREPARE.MODEL.DATA(train.raw[this.fold,])\\n    test.data$ID <- train.raw[this.fold,ID]\\n    \\n    train.data <- PREPARE.MODEL.DATA(train.raw[-this.fold,])\\n    \\n    \\n    if (FRACTION.TRAIN.DATA != 1 ) {\\n        # extract subset for inital training\\n        set.seed(29)\\n        idx <- createDataPartition(train.data$response,p=FRACTION.TRAIN.DATA,list=FALSE)\\n        train.data$predictors <- train.data$predictors[idx,]\\n        train.data$response <- train.data$response[idx]\\n    }\\n    \\n    # save prepared training data for Python function\\n    # put response as first column in data set\\n    write.table(cbind(response=train.data$response,train.data$predictors),\\n                file=paste0(WORK.DIR,\\"/py_train.tsv\\"),row.names = FALSE,\\n                sep=\\"\\\\t\\")\\n    \\n    \\n    # invoke Python training model\\n    python.train.command <- paste(PYTHON_COMMAND,paste0(WORK.DIR,\\"/train_model.py\\"),WORK.DIR)\\n    \\n    Sys.time()\\n    \\n    \\n    time.data <- system.time(system(python.train.command))\\n    \\n    time.data\\n    # stopCluster(cl)\\n    \\n    # prepare data for training\\n    write.table(test.data$predictors,file=paste0(WORK.DIR,\\"/py_test.tsv\\"),row.names = FALSE,\\n                sep=\\"\\\\t\\")\\n    \\n    # execute Python prediction code\\n    python.test.command <- paste(PYTHON_COMMAND,paste0(WORK.DIR,\\"/make_prediction.py\\"),\\n                                 WORK.DIR,\\n                                 \\"possible_model\\",\\n                                 \\"py_test.tsv\\",\\n                                 \\"py_test_predictions.tsv\\")\\n    system(python.test.command)\\n    \\n    # get predictions from Python model\\n    pred.probs <- fread(paste0(WORK.DIR,\\"/py_test_predictions.tsv\\"), sep=\\"\\\\t\\")\\n    \\n    score <- logLossEval(pred.probs[,Class_1],test.data$response)\\n    score\\n    \\n    # clean up files no longer needed\\n    file.remove(c(paste0(WORK.DIR,\\"/py_train.tsv\\"),paste0(WORK.DIR,\\"/py_test.tsv\\"),\\n                  paste0(WORK.DIR,\\"/possible_model\\"),\\n                  paste0(WORK.DIR,\\"/py_test_predictions.tsv\\")))\\n    \\n    ans <- list(score=score,\\n                level1.features=data.frame(ID=test.data$ID,pred.probs,response=test.data$response))\\n    \\n    return(ans)\\n    \\n}\\n# train the model\\nSys.time()\\n\\ntime.data <- system.time(ll <- llply(data.folds,trainFolds,.parallel = FALSE))\\n\\ntime.data\\n\\nfold.scores <- unlist(lapply(ll,function(x){x$score}))\\nlevel1.features <- do.call(rbind,lapply(ll,function(x){x$level1.features}))\\n\\nmean(fold.scores)\\n\\n# record Model performance\\nmodelPerf.df <- read.delim(paste0(WORK.DIR,\\"/model_performance.tsv\\"),\\n                         stringsAsFactors=FALSE)\\n# determine if score improved\\nimproved <- ifelse(mean(fold.scores) < min(modelPerf.df$score),\\"Yes\\",\\"No\\")\\n\\nrecordModelPerf(paste0(WORK.DIR,\\"/model_performance.tsv\\"),\\n                                MODEL.NAME,\\n                              time.data,\\n                              data.frame(),\\n                              mean(fold.scores),\\n                              improved=improved,\\n                              bestTune=NA,\\n                              tune.grid=NA,\\n                              model.parms=NA,\\n                              comment=MODEL.COMMENT)\\n\\nmodelPerf.df <- read.delim(paste0(WORK.DIR,\\"/model_performance.tsv\\"),\\n                         stringsAsFactors=FALSE)\\n\\n\\n#display model performance record for this run\\ntail(modelPerf.df[,1:10],1)\\n\\n# if last score recorded is better than previous ones save model object\\nlast.idx <- length(modelPerf.df$score)\\nif (last.idx == 1 || improved == \\"Yes\\"  || FORCE_RECORDING_MODEL) {\\n    cat(\\"found improved model, saving...\\\\n\\")\\n    flush.console()\\n    #yes we have improvement or first score, save generated model\\n    file.name <- paste0(\\"level1_features_\\",MODEL.NAME,\\"_\\",modelPerf.df$date.time[last.idx],\\".RData\\")\\n    file.name <- gsub(\\" \\",\\"_\\",file.name)\\n    file.name <- gsub(\\":\\",\\"_\\",file.name)\\n    \\n    save(level1.features,PREPARE.MODEL.DATA,file=paste0(WORK.DIR,\\"/\\",file.name))\\n    \\n    # estalish pointer to current model\\n    writeLines(file.name,paste0(WORK.DIR,\\"/this_level1_features\\"))\\n} else {\\n    cat(\\"no improvement!!!\\\\n\\")\\n    flush.console()\\n}\\n\\n\\n\\n" }\n'
line: b'{ "repo_name": "jimthompson5802/kaggle-BNP-Paribas", "ref": "refs/heads/master", "path": "src/L1_nnet11/train_model.R", "content": "###\\n# neural network model\\n###\\n\\nlibrary(data.table)\\nlibrary(caret)\\n# add any model specific package library commands\\nlibrary(nnet)\\n\\n# set working directory\\nWORK.DIR <- \\"./src/L1_nnet11\\"  # modify to specify directory to contain model artififacts\\n\\n# Common Functions and Global variables\\nsource(\\"./src/CommonFunctions.R\\")\\n\\n# set caret training parameters\\nCARET.TRAIN.PARMS <- list(method=\\"nnet\\")   # Replace MODEL.METHOD with appropriate caret model\\n\\nCARET.TUNE.GRID <-  NULL  # NULL provides model specific default tuning parameters\\n\\n# user specified tuning parameters\\n#CARET.TUNE.GRID <- expand.grid(nIter=c(100))\\n\\n# model specific training parameter\\nCARET.TRAIN.CTRL <- trainControl(method=\\"repeatedcv\\",\\n                                 number=5,\\n                                 repeats=1,\\n                                 verboseIter=FALSE,\\n                                 classProbs=TRUE,\\n                                 summaryFunction=caretLogLossSummary)\\n\\nCARET.TRAIN.OTHER.PARMS <- list(trControl=CARET.TRAIN.CTRL,\\n                            maximize=FALSE,\\n                           tuneGrid=CARET.TUNE.GRID,\\n                           tuneLength=7,\\n                           metric=\\"LogLoss\\")\\n\\nMODEL.SPECIFIC.PARMS <- list(verbose=FALSE) #NULL # Other model specific parameters\\n\\nPREPARE.MODEL.DATA <- prepL1FeatureSet1\\n\\nMODEL.COMMENT <- \\"Only Class_1 probabilites as features\\"\\n\\n\\nLEVEL0.MODELS <- c(\\"L0_gbm21\\",\\n                   \\"L0_gbm41\\",\\n                   \\"L0_xtc11\\",\\n                   \\"L0_xtc21\\",\\n                   \\"L0_xtc31\\",\\n                   #\\"L0_xtc4\\",  did not improve score\\n                   \\"L0_xtc51\\",\\n                   #\\"L0_nnet1\\",\\n                   \\"L0_xgb21\\",\\n                   \\"L0_xgb31\\")\\n\\n\\n# amount of data to train\\nFRACTION.TRAIN.DATA <- 1.0\\n\\n# force recording model flag\\nFORCE_RECORDING_MODEL <- FALSE\\n\\n\\n# get training data\\ntrain.data <- prepL1FeatureSet3(LEVEL0.MODELS)\\n\\n\\n# # create the partitions\\n# set.seed(13)\\n# data.folds <- createFolds(raw$target, k=5)\\n\\nlibrary(doMC)\\nregisterDoMC(cores = 7)\\n\\n\\n# train the model\\nSys.time()\\nset.seed(825)\\n\\ntime.data <- system.time(mdl.fit <- do.call(train,c(list(x=train.data$predictors,\\n                                                         y=train.data$response),\\n                                                    CARET.TRAIN.PARMS,\\n                                                    MODEL.SPECIFIC.PARMS,\\n                                                    CARET.TRAIN.OTHER.PARMS)))\\n\\ntime.data\\nmdl.fit\\n# stopCluster(cl)\\n\\nscore <- mean(mdl.fit$resample$LogLoss)\\nscore\\n\\n# record Model performance\\nmodelPerf.df <- read.delim(paste0(WORK.DIR,\\"/model_performance.tsv\\"),\\n                         stringsAsFactors=FALSE)\\n# determine if score improved\\nimproved <- ifelse(score < min(modelPerf.df$score),\\"Yes\\",\\"No\\")\\n\\nrecordModelPerf(paste0(WORK.DIR,\\"/model_performance.tsv\\"),\\n                              mdl.fit$method,\\n                              time.data,\\n                              train.data$predictors,\\n                              score,\\n                              improved=improved,\\n                              bestTune=flattenDF(mdl.fit$bestTune),\\n                              tune.grid=flattenDF(CARET.TUNE.GRID),\\n                              model.parms=paste(names(MODEL.SPECIFIC.PARMS),\\n                                                as.character(MODEL.SPECIFIC.PARMS),\\n                                                sep=\\"=\\",collapse=\\",\\"),\\n                              comment=paste0(MODEL.COMMENT,\\":\\",paste0(LEVEL0.MODELS,collapse=\\", \\")))\\n\\nmodelPerf.df <- read.delim(paste0(WORK.DIR,\\"/model_performance.tsv\\"),\\n                         stringsAsFactors=FALSE)\\n\\n\\n#display model performance record for this run\\ntail(modelPerf.df[,1:10],1)\\n\\n# if last score recorded is better than previous ones save model object\\nlast.idx <- length(modelPerf.df$score)\\nif (last.idx == 1 || improved == \\"Yes\\" || FORCE_RECORDING_MODEL) {\\n    cat(\\"found improved model, saving...\\\\n\\")\\n    flush.console()\\n    #yes we have improvement or first score, save generated model\\n    # file.name <- paste0(\\"model_\\",mdl.fit$method,\\"_\\",modelPerf.df$date.time[last.idx],\\".RData\\")\\n    file.name <- paste0(\\"model_\\",mdl.fit$method,\\"_\\",as.character(Sys.time()),\\".RData\\")\\n    file.name <- gsub(\\" \\",\\"_\\",file.name)\\n    file.name <- gsub(\\":\\",\\"_\\",file.name)\\n\\n    save(LEVEL0.MODELS,PREPARE.MODEL.DATA,mdl.fit,file=paste0(WORK.DIR,\\"/\\",file.name))\\n\\n    # estalish pointer to current model\\n    writeLines(file.name,paste0(WORK.DIR,\\"/this_model\\"))\\n} else {\\n    cat(\\"no improvement!!!\\\\n\\")\\n    flush.console()\\n}\\n\\n\\n\\n" }\n'
line: b'{ "repo_name": "jeffheaton/aifh", "ref": "refs/heads/master", "path": "vol1/r-examples/ch5/kmeans.R", "content": "## Artificial Intelligence for Humans\\n## Volume 1: Fundamental Algorithms\\n## R Version\\n## http://www.aifh.org\\n## http://www.jeffheaton.com\\n##\\n## Code repository:\\n## https://github.com/jeffheaton/aifh\\n##\\n## Copyright 2013 by Jeff Heaton\\n##\\n## Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n## you may not use this file except in compliance with the License.\\n## You may obtain a copy of the License at\\n##\\n##     http://www.apache.org/licenses/LICENSE-2.0\\n##\\n## Unless required by applicable law or agreed to in writing, software\\n## distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n## See the License for the specific language governing permissions and\\n## limitations under the License.\\n##\\n## For more information on Heaton Research copyrights, licenses\\n## and trademarks visit:\\n## http://www.heatonresearch.com/copyright\\n\\n## Chapter 5 Example: Random numbers\\n\\n## first load the iris data set\\nirisdata <- read.csv(file=\\"iris.csv\\",head=TRUE,sep=\\",\\")\\n\\n## perform the kmeans into 3 clusters, max iterations of 1000\\niris.kmeans <- kmeans(irisdata[, -5], 3, iter.max = 1000)\\n\\n## display the kmeans cluster as a table\\ntable(irisdata[, 5], iris.kmeans$cluster)\\n\\n# The iris data is 4-dimension.  To display that in a chart we must scale the\\n# dimensions down to just 2.\\niris.dist <- dist(iris[, -5])\\niris.mds <- cmdscale(iris.dist)\\n\\n# Ideal species assignments will be characters (from the iris data)\\nideal.chars <- c(\\"*\\", \\"o\\", \\"+\\")[as.integer(iris$Species)]\\n\\n# Actual cluster assignments will be colors (from the kmeans cluster)\\nactual.colors <- rainbow(3)[iris.kmeans$cluster]\\n\\n# You now plot\\nplot(iris.mds, col = actual.colors, pch = ideal.chars, xlab = \\"X\\", ylab = \\"Y\\")\\n\\n# You will notice items on the graph.  Their char type shows their real (ideal) species.\\n# The colors shows what cluster kMeans put them into.  You will notice that most errors\\n# occur right at the border between the two clusters on the right of the graph.  That is\\n# because there is no clearly defined border between them for kmeans to figure out on its \\n# own.\\n" }\n'
line: b'{ "repo_name": "jeffheaton/aifh", "ref": "refs/heads/master", "path": "vol1/r-examples/ch7/learnPoly.R", "content": "## Artificial Intelligence for Humans\\n## Volume 1: Fundamental Algorithms\\n## R Version\\n## http://www.aifh.org\\n## http://www.jeffheaton.com\\n##\\n## Code repository:\\n## https://github.com/jeffheaton/aifh\\n##\\n## Copyright 2013 by Jeff Heaton\\n##\\n## Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n## you may not use this file except in compliance with the License.\\n## You may obtain a copy of the License at\\n##\\n##     http://www.apache.org/licenses/LICENSE-2.0\\n##\\n## Unless required by applicable law or agreed to in writing, software\\n## distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n## See the License for the specific language governing permissions and\\n## limitations under the License.\\n##\\n## For more information on Heaton Research copyrights, licenses\\n## and trademarks visit:\\n## http://www.heatonresearch.com/copyright\\n\\n## Chapter 7 Example: Towards Machine Learning, Learn Polynomial w/ Greedy Random\\n## http://en.wikipedia.org/wiki/Polynomial\\n\\n# Calculate the error with Mean Square Error\\n# http://www.heatonresearch.com/wiki/Mean_Square_Error\\nmse <- function(a,b) {\\n  sum(( a - b )^2) /length(a)  \\n}\\n\\n##################################################################################\\n## Generate the input data for training.  We use the x integer values of \\n## -50 to 50 to train the polynomial over.\\n##################################################################################\\n\\n# Generate input data range\\ninputData = -50:50\\n\\n# Create a 2d matrix from the 1d vector\\ninput <- matrix(inputData,ncol=1,byrow=TRUE)\\n\\n# Convert into a data table\\ninput <- as.table(input)\\n\\n# Name the columns\\ncolnames(input) <- c(\\"x\\")\\n\\n# Name the rows, simply provide a index number for each.  Not really used.\\nrownames(input) = 1:nrow(input)\\n\\n##################################################################################\\n## Generate the ideal data for training.  We calculate the y value of the polynomial\\n## for each of the input values from the previous step.\\n##################################################################################\\n\\n# Generate input data range (2x^2 + 4x + 6)\\nidealData = (2*inputData)^2 + (4*inputData) + 6\\n\\n# Create a 2d matrix from the 1d vector\\nideal <- matrix(idealData,ncol=1,byrow=TRUE)\\n\\n# Convert into a data table\\nideal <- as.table(ideal)\\n\\n# Name the columns\\ncolnames(ideal) <- c(\\"y\\")\\n\\n# Name the rows, simply provide a index number for each.  Not really used.\\nrownames(ideal) = 1:nrow(ideal)\\n\\n# Calculate the polynomial from the coefficient\\ncalcPolynomial <- function(coef)\\n{\\n  actual <- (input[,1]*coef[1])^2 + (input[,1]*coef[2]) + coef[3]\\n  actual <- as.table(matrix(actual,ncol=1,byrow=TRUE))\\n  rownames(actual) = 1:nrow(actual)\\n  colnames(actual) <- c(\\"y\\")\\n  actual\\n}\\n\\n# Score the polynomial\\nscore <- function(coef) {\\n  actual <-calcPolynomial(coef)\\n  mse(actual,ideal)\\n}\\n\\n# Begin the iterations\\niteration <- 0\\nbestScore <- .Machine$double.xmax\\nupdate <- -1\\n\\nrepeat {\\n  newCoef <- runif(3,-10,10)\\n  s <- score(newCoef)\\n  \\n  # Greedy, only improve\\n  if( s<bestScore )\\n  {\\n    bestScore <- s\\n    coef <- newCoef\\n  }\\n  \\n  update <- update + 1\\n  \\n  if( update>=1000 ) \\n  {\\n    update<-0\\n    cat(\\"Iteration: \\", iteration, \\", Score: \\" , bestScore, \\"\\\\n\\")\\n    if( iteration>100000 ) {\\n      break\\n    }\\n  }\\n  \\n  iteration <- iteration + 1\\n}\\n\\ncoef\\ncat( coef[1],\\"x^2 + \\", coef[2], \\"x + \\", coef[3] ) \\n\\n" }\n'
line: b'{ "repo_name": "jeffheaton/aifh", "ref": "refs/heads/master", "path": "vol1/r-examples/ch9/tsp.R", "content": "## Artificial Intelligence for Humans\\n## Volume 1: Fundamental Algorithms\\n## R Version\\n## http://www.aifh.org\\n## http://www.jeffheaton.com\\n##\\n## Code repository:\\n## https://github.com/jeffheaton/aifh\\n##\\n## Copyright 2013 by Jeff Heaton\\n##\\n## Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n## you may not use this file except in compliance with the License.\\n## You may obtain a copy of the License at\\n##\\n##     http://www.apache.org/licenses/LICENSE-2.0\\n##\\n## Unless required by applicable law or agreed to in writing, software\\n## distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n## See the License for the specific language governing permissions and\\n## limitations under the License.\\n##\\n## For more information on Heaton Research copyrights, licenses\\n## and trademarks visit:\\n## http://www.heatonresearch.com/copyright\\n\\n## Chapter 9 Example: Discrete Optimization, Traveling Salesman\\n## \\n## http://xkcd.com/399/\\n## http://en.wikipedia.org/wiki/Travelling_salesman_problem\\n\\n# how many cities\\ncityCount <- 30\\n\\n# define cities to be in a circle\\nratio <- (2 * pi) / cityCount\\ncity.x <- (cos(1:cityCount*ratio) * 10) + 10\\ncity.y <- (sin(1:cityCount*ratio) * 10) + 10\\n\\n# define an initial, random, path\\npath <- sample(1:cityCount, cityCount, replace=F)\\n\\n# display a plot of the initial path\\nplot(city.x, city.y, xlim=c(0,20), ylim=c(0,20),asp = 1,xlab = \\"\\", ylab = \\"\\", axes = TRUE, main = \\"Traveling Salesman (before)\\")\\narrows(city.x[path[1:(cityCount-1)]],\\n  city.y[path[1:(cityCount-1)]],\\n  city.x[path[2:cityCount]],\\n  city.y[path[2:cityCount]],\\n  angle = 10, col = \\"blue\\")\\n\\n# Define a score function, this sumes the euclidean distance over the entire path\\ndistance <- function(path) {  \\n  path2 <- embed(path,2)\\n  sqrt(sum(  (city.x[path2[,1]]-city.x[path2[,2]])^2 +  (city.y[path2[,1]]-city.y[path2[,2]])^2  ))\\n}\\n\\n# move to neighbor solution, swap two cities on the path\\nmoveNeighbor <- function(path) {  \\n  idx <- seq(2, length(path)-1)\\n  changepoints <- sample(idx, size = 2, replace = FALSE)\\n  tmp <- path[changepoints[1]]\\n  path[changepoints[1]] <- path[changepoints[2]]\\n  path[changepoints[2]] <- tmp\\n  path\\n}\\n\\n# perform the simulated annealing\\nresult <- optim(path, distance, moveNeighbor, method = \\"SANN\\",\\n             control = list(maxit = 200000, temp = 10, tmax=20, trace = TRUE,\\n                            REPORT = 500) )\\n\\n# get the result path\\nresultPath <- result$par\\n\\n# plot the result path\\nplot(city.x, city.y, xlim=c(0,20), ylim=c(0,20),asp = 1,xlab = \\"\\", ylab = \\"\\", axes = TRUE, main = \\"Traveling Salesman (After)\\")\\narrows(city.x[resultPath[1:(cityCount-1)]],\\n       city.y[resultPath[1:(cityCount-1)]],\\n       city.x[resultPath[2:cityCount]],\\n       city.y[resultPath[2:cityCount]],\\n       angle = 10, length=0.15, col = \\"blue\\")\\n\\n\\n" }\n'
line: b'{ "repo_name": "tonyfischetti/sake", "ref": "refs/heads/master", "path": "functests/test1/dui-correlates.R", "content": "#!/usr/bin/rscript --vanilla\\n\\nrm(list=ls())\\n\\n\\n\\ndui.frame <- read.table(\\"duistats\xe2\x98\x8e.tsv\\", stringsAsFactors=FALSE, \\n                        sep=\'\\\\t\', header=TRUE)\\n\\nteen.frame <- read.csv(\\"teenstats.csv\\", stringsAsFactors=FALSE)\\n\\n\\n# dui frame to upper\\ndui.frame$State <- toupper(dui.frame$State)\\ndui.frame[9,1] <- \\"D.C.\\"\\n\\n# control for population size (the higher, the worse)\\ndui.frame$dui.score <- dui.frame$DUI.Arrests..2012. / dui.frame$Population.Size\\n\\n# rank states by fewest dui arrests by population\\ndui.frame <- dui.frame[ order(dui.frame$dui.score), ]\\ndui.frame$rank <- 1:nrow(dui.frame)\\n\\nnames(teen.frame)[1] <- \\"State\\"\\nnames(teen.frame)[2] <- \\"Percent.HS.Grad\\"\\ntest <- merge(dui.frame, teen.frame[,c(1,2)])\\n\\nplot(test$dui.score ~ test$Percent.HS.Grad)\\ndev.copy(png,\'correlation.png\')\\ndev.off()\\n\\nres <- lm(test$dui.score ~ test$Percent.HS.Grad)\\n\\nwrite(res$coefficients, \\"lmcoeffs.txt\\")\\n\\n" }\n'
line: b'{ "repo_name": "marchtaylor/sinkr", "ref": "refs/heads/master", "path": "R/pca_loocv.R", "content": "#\' Principal component analysis \\"leave-one-out\\" cross-validation\\n#\'\\n#\' @param X Matrix to be subjected to svd\\n#\' @param npc.max The maximum number of principal components to test. Default=ncol(X)\\n#\'\\n#\' @return Matrix of square error values for each element in X  \\n#\' \\n#\' @references \\\\url{http://stats.stackexchange.com/a/115477/10675}\\n#\' @export\\n#\'\\n#\' @examples\\n#\' \\n#\' X <- as.matrix(iris[,1:4])\\n#\' res <- pca_loocv(X)\\n#\' res2 <- lapply(res, colSums)\\n#\' res2\\n#\' \\n#\' COL <- 2:4\\n#\' LTY <- 1:3\\n#\' op <- par(mar=c(4,4,2,1), tcl=-0.25, mgp=c(2.5,0.5,0))\\n#\' for(i in seq(res)){\\n#\'   if(i==1) {\\n#\'     plot(res2[[i]], t=\\"n\\", ylim=range(unlist(res2)), \\n#\'      main=\\"iris\\", xlab=\\"n PCs\\", ylab=\\"PRESS\\")\\n#\'     grid()\\n#\'   } \\n#\'   lines(res2[[i]], t=\\"b\\", bg=c(NaN,COL[i])[(res2[[i]]==min(res2[[i]])) + 1],\\n#\'    col=COL[i], lty=LTY[i], pch=21)\\n#\' }\\n#\' legend(\\"topright\\", legend=c(\\"naive\\", \\"approximate\\", \\"pseudoinverse\\"),\\n#\'  col=COL, lty=LTY, pch=21, bty=\\"n\\")\\n#\' par(op)\\n#\' \\n#\' \\npca_loocv <- function(X, npc.max=ncol(X)){\\n  error1 <- matrix(NaN, nrow=dim(X)[1], ncol=min(dim(X)[2],npc.max))\\n  error2 <- matrix(NaN, nrow=dim(X)[1], ncol=min(dim(X)[2],npc.max))\\n  error3 <- matrix(NaN, nrow=dim(X)[1], ncol=min(dim(X)[2],npc.max))\\n  for(n in 1:dim(X)[1]){\\n    Xtrain = X[-n,]\\n    Xtrain = scale(Xtrain, center=TRUE, scale=FALSE)\\n    V = svd(Xtrain)$v\\n    Xtest = X[n,,drop = FALSE]\\n    Xtest = scale(Xtest, center=attr(Xtrain, \\"scaled:center\\"), scale=FALSE)\\n    for(j in 1:min(dim(V)[2],npc.max)){\\n        P = V[,1:j] %*% t(V[,1:j])\\n        err1 <- Xtest %*% (diag(length(diag(P))) - P)\\n        err2 <- Xtest %*% (diag(length(diag(P))) - P + diag(diag(P)))\\n        err3 <- array(NaN, dim=dim(Xtest))\\n        for(k in 1:dim(Xtest)[2]){\\n          proj = Xtest[,-k] %*% t(expmat(V[-k,1:j])) %*% t(V[,1:j])\\n          err3[k] = Xtest[k] - proj[k]\\n        }\\n        error1[n,j] <- sum(sqrt(err1^2))\\n        error2[n,j] <- sum(sqrt(err2^2))\\n        error3[n,j] <- sum(sqrt(err3^2))\\n    }\\n  }\\n  res <- list(\\n    error1=error1,\\n    error2=error2,\\n    error3=error3\\n  )\\n  return(res)\\n}\\n" }\n'
line: b'{ "repo_name": "pablo14/funModeling", "ref": "refs/heads/master", "path": "R/numbers.R", "content": "#\' @title Outliers Data Preparation\\n#\' @description\\n#\' Deal with outliers by setting an \'NA value\' or by \'stopping\' them at a certain. The parameters: \'top_percent\'/\'bottom_percent\' are used to consider a value as outlier.\\n#\' Setting NA is recommended when doing statistical analysis, parameter: type=\'set_na\'.\\n#\' Stopping is recommended when creating a predictive model without biasing the result due to outliers, parameter: type=\'stop\'.\\n#\' @param data data frame\\n#\' @param str_input string input variable (if empty, it runs for all numeric variable).\\n#\' @param top_percent value from 0 to 1, represents the highest X percentage of values to treat\\n#\' @param bottom_percent value from 0 to 1, represents the lowest X percentage of values to treat\\n#\' @param type can be \'stop\' or \'set_na\', in the first case the original variable is stopped at the desiered percentile, \'set_na\'  sets NA to the same values.\\n#\' @examples\\n#\' # Creating data frame with outliers\\n#\' set.seed(10)\\n#\' df=data.frame(var1=rchisq(1000,df = 1), var2=rnorm(1000))\\n#\' df=rbind(df, 1135, 2432) # forcing outliers\\n#\' df$id=as.character(seq(1:1002))\\n#\'\\n#\' # for var1: mean is ~ 4.56, and max 2432\\n#\' summary(df)\\n#\'\\n#\' ########################################################\\n#\' ### PREPARING OUTLIERS FOR DESCRIPTIVE STATISTICS\\n#\' ########################################################\\n#\'\\n#\' #### EXAMPLE 1: Removing top 1% for a single variable\\n#\' # checking the value for the top 1% of highest values (percentile 0.99), which is ~ 7.05\\n#\' quantile(df$var1, 0.99)\\n#\'\\n#\' # Setting type=\'set_na\' sets NA to the highest value)\\n#\' var1_treated=prep_outliers(data = df,  str_input = \'var1\', type=\'set_na\', top_percent  = 0.01)\\n#\'\\n#\' # now the mean (~ 0.94) is more accurate, and note that: 1st, median and 3rd quartiles remaining very similar to the original variable.\\n#\' summary(var1_treated)\\n#\'\\n#\' #### EXAMPLE 2: if \'str_input\' is missing, then it runs for all numeric variables (which have 3 or more distinct values).\\n#\' df_treated2=prep_outliers(data = df, type=\'set_na\', top_percent  = 0.01)\\n#\' summary(df_treated2)\\n#\'\\n#\' #### EXAMPLE 3: Removing top 1% (and bottom 1%) for \'N\' specific variables.\\n#\' vars_to_process=c(\'var1\', \'var2\')\\n#\' df_treated3=prep_outliers(data = df, str_input = vars_to_process, type=\'set_na\', bottom_percent = 0.01, top_percent  = 0.01)\\n#\' summary(df_treated3)\\n#\'\\n#\' ########################################################\\n#\' ### PREPARING OUTLIERS FOR PREDICTIVE MODELING\\n#\' ########################################################\\n#\'\\n#\' #### EXAMPLE 4: Stopping outliers at the top 1% value for all variables. For example if the top 1% has a value of 7, then all values above will be set to 7. Useful when modeling because outlier cases can be used.\\n#\' df_treated4=prep_outliers(data = df, top_percent  = 0.01, type=\'stop\')\\n\\n#\' @return A vector or data frame with the desired outlier transformation\\n#\' @export\\nprep_outliers <- function(data, str_input, type=c(\'stop\', \'set_na\'), top_percent, bottom_percent)\\n{\\n\\tif(!(type %in% c(\'stop\', \'set_na\', \'sigmoid\')))\\n\\t\\tstop(\\"Parameter \'type\' must be one \'stop\' or \'set_na\'\\")\\n\\n\\n\\tif(missing(str_input))\\n\\t\\tstr_input=give_me_num_vars(data)\\n\\n\\n\\t# #########################################################\\n\\t# ### Sigmoid procesing\\n\\t# #########################################################\\n\\t# if(type == \'sigmoid\')\\n\\t# {\\n\\t# \\tfor(i in 1:length(str_input))\\n\\t#   {\\n\\t#    \\tdata[, str_input[i]]=sigmoid(as.numeric(scale(data[, str_input[i]])))\\n\\t# \\t}\\n\\t# \\treturn(data)\\n\\t# }\\n\\n\\t#########################################################\\n\\t### Stopping and Setting NA processing\\n\\t#########################################################\\n\\t## If not sigmoid, then it\'s stop or set_na, thus it has to have top or bottom param.\\n\\tif(missing(top_percent) & missing(bottom_percent))\\n\\t\\tstop(\\"Parameters \'top_percent\' and \'bottom_percent\' cannot be missing at the same time\\")\\n\\n\\t## Logic for top value\\n\\tif(!missing(top_percent))\\n\\t{\\n\\t  for(i in 1:length(str_input))\\n\\t  {\\n\\t   \\ttop_value=round(quantile(data[,str_input[i]], probs=(1-top_percent), names=F, na.rm=T))\\n\\t   \\tdata[, str_input[i]][data[, str_input[i]]>top_value]=ifelse(type==\'stop\', top_value, NA)\\n\\t  }\\n\\t}\\n\\n\\t## Logic for bottom value\\n\\tif(!missing(bottom_percent))\\n\\t{\\n\\t  for(i in 1:length(str_input))\\n\\t  {\\n\\t   \\tbottom_value=round(quantile(data[,str_input[i]], probs=bottom_percent, names=F, na.rm=T))\\n\\t   \\tdata[, str_input[i]][data[, str_input[i]]<bottom_value]=ifelse(type==\'stop\', bottom_value, NA)\\n\\t  }\\n\\t}\\n\\n\\t## Return the input vector if only 1 var was desired, otherwise it returns all the data frame transformed\\n\\tif(length(str_input)==1) {\\n\\t\\treturn(data[, str_input[i]])\\n\\t} else {\\n \\t\\treturn(data)\\n\\t}\\n\\n}\\n\\n#\' @title Compare two vectors of keys\\n#\' @description Obtain correlation table of all variables that belongs to data against target variable\\n#\' @param data data frame\\n#\' @param str_target string variable to predict\\n#\' @examples\\n#\' v1=c(1,2,4)\\n#\' v2=c(1,2,5,6)\\n#\' res=compare_df(key_x=v1, key_y=v2)\\n#\' # Print the keys that didn\'t match\\n#\' res\\n#\' # Accessing the keys not present in\\n#\' @return Correlation index for all data input variable\\n#\' @export\\ncompare_df <- function(key_x, key_y)\\n{\\n\\t# key_x=v1;key_y=v2\\n  df_x=data.frame(key_x=key_x, flag_x=1)\\n  df_y=data.frame(key_y=key_y, flag_y=1)\\n\\n  df_x$key_x=as.character(df_x$key_x)\\n\\tdf_y$key_y=as.character(df_y$key_y)\\n\\n  merge_all=merge(df_x, df_y, by.x=\'key_x\', by.y=\'key_y\', all=T)\\n\\n  names(merge_all)[1]=\\"key\\"\\n\\n  merge_all_nona=merge_all[!is.na(merge_all$flag_x) & !is.na(merge_all$flag_y),]\\n\\n  not_in_x=merge_all[is.na(merge_all$flag_x),]\\n  not_in_y=merge_all[is.na(merge_all$flag_y),]\\n\\n  print(sprintf(\\"Coincident in both: %s\\", nrow(merge_all_nona)))\\n  print(sprintf(\\"Rows not present in X: %s\\", nrow(not_in_x)))\\n  print(sprintf(\\"Rows not present in Y: %s\\", nrow(not_in_y)))\\n\\n\\n  list_diff=list()\\n\\n  res=list(\\n    present_in_both=merge_all_nona$key,\\n    rows_not_in_X=not_in_x$key,\\n    rows_not_in_Y=not_in_y$key\\n  \\t)\\n\\n  return(res)\\n}\\n\\n#\' @title Correlation analyisis against target variable\\n#\' @description Obtain correlation table of all variables that belongs to data against target variable. Only numeric variables are analyzed.\\n#\' @param data data frame\\n#\' @param str_target string variable to predict\\n#\' @examples\\n#\' correlation_table(data=heart_disease, str_target=\\"has_heart_disease\\")\\n#\' @return Correlation index for all data input variable\\n#\' @export\\ncorrelation_table <- function(data, str_target)\\n{\\n\\tdata[, str_target]=as.numeric(data[, str_target])\\n\\n\\tdata=data[, c(give_me_num_vars(data, str_target), str_target)]\\n\\n  df_cor=as.data.frame(round(cor(data, use=\\"complete.obs\\"\\t),2))\\n  df_cor$Variable = rownames(df_cor)\\n  df_cor=df_cor[, names(df_cor) %in% c(str_target, \\"Variable\\")]\\n\\n  df_cor=df_cor[interp(~order(df_cor, -v) , v=as.name(str_target)),  ]\\n\\n  row.names(df_cor) = NULL\\n  df_cor=df_cor[, c(2,1)]\\n\\n  df_cor[order(-df_cor[,2]) , ]\\n\\n  return(df_cor)\\n}\\n\\n#\' @title Sigmoid function\\n#\' @description Sigmoid function, also known as logistic or s-shaped\\n#\' @param x numeric input vector\\n#\' @param a constant to multiply \'x\', default=1\\n#\' @examples\\n#\' sigmoid()\\n#\' @return vector transformed with sigmoid\\n#\' @export\\nsigmoid<-function(x, a=1)\\n{\\n\\tif(missing(a))\\n\\t\\ta=1\\n\\n\\ty = 1/(1 + exp(-a*x))\\n\\n\\treturn(y)\\n}\\n\\n#\' @title Transform a variable into the 0 to 1 range\\n#\' @description Range a variable into [0-1], assigning 0 to the min and 1 to the max of the input variable.\\n#\' @param x numeric input vector\\n#\' @examples\\n#\' range01(mtcars$cyl)\\n#\' @return vector ranged into 0-1\\n#\' @export\\nrange01 <- function(x)\\n{\\n\\treturn((x-min(x, na.rm=T))/(max(x, na.rm=T)-min(x, na.rm=T)))\\n}\\n" }\n'
line: b'{ "repo_name": "zozlak/ZPD", "ref": "refs/heads/master", "path": "R/pobierz_schematy_odp.R", "content": "#\' @title Pobiera dystraktory kryteri\xc3\xb3w oceny\\r\\n#\' @description\\r\\n#\' W wypadku pobierania z bazy wynik\xc3\xb3w w postaci niewypunktowanej wybrana przez \\r\\n#\' ucznia odpowied\xc5\xba zakodowana jest liczbowo. Dane pobierane funkcj\xc4\x85 \\r\\n#\' pobierz_schematy_odp() pozwalaj\xc4\x85 przekodowa\xc4\x87 je na faktyczne oznaczenia u\xc5\xbcyte\\r\\n#\' w te\xc5\x9bcie.\\r\\n#\' \\r\\n#\' Innym zastosowaniem mo\xc5\xbce by\xc4\x87 sprawdzanie, czy zbi\xc3\xb3r danych z wynikami testu \\r\\n#\' nie zawiera warto\xc5\x9bci spoza mo\xc5\xbcliwych do wyboru dla danego zadania odpowiedzi.\\r\\n#\' @param src uchwyt \xc5\xbar\xc3\xb3d\xc5\x82a danych dplyr-a \\r\\n#\' @import dplyr\\r\\n#\' @export\\r\\npobierz_schematy_odp = function(\\r\\n  src\\r\\n){\\r\\n  stopifnot(is.src(src))\\r\\n  \\r\\n  query = \\"\\r\\n    SELECT \'k_\' || id_kryterium AS kryterium, dystraktor, kolejnosc AS kolejnosc_dystr\\r\\n    FROM \\r\\n      pytania \\r\\n      JOIN kryteria_oceny USING (id_pytania) \\r\\n      JOIN sl_schematy_odp_dystr USING (schemat_odp)\\r\\n    ORDER BY 1, 3\\r\\n  \\"\\r\\n  data = tbl(src, sql(e(query)))\\r\\n  return(data)\\r\\n}\\r\\nattr(pobierz_schematy_odp, \'grupa\') = \'kryteriaOceny\'\\r\\n" }\n'
line: b'{ "repo_name": "zozlak/ZPD", "ref": "refs/heads/master", "path": "tests/testthat/test-normalizuj.R", "content": "context(\'normalizuj\')\\n\\nsrc = polacz()\\n\\ntest_that(\'normalizuj dzia\xc5\x82a\', {\\n  dane = data.frame(wynik = rep(1:40, 50))\\n  \\n  norm = normalizuj(dane)\\n  \\n  expect_equal(norm$wynik, dane$wynik)\\n  expect_less_than(abs(mean(norm$wynik_norm) - 100), 1)\\n  expect_less_than(abs(median(norm$wynik_norm) - 100), 1)\\n  expect_less_than(abs(sd(norm$wynik_norm) - 15), 1)\\n  \\n  skale = pobierz_skale(src) %>% \\n    filter(posiada_normy == T, rodzaj_egzaminu == \'sprawdzian\', rok == 2010, rodzaj_skali == \'zr\xc3\xb3wnywanie\') %>% \\n    select(id_skali, skalowanie, grupa) %>%\\n    distinct() %>%\\n    collect()\\n  norm = normalizuj(dane, src, idSkali = skale$id_skali[1], skalowanie = skale$skalowanie[1], grupa = skale$grupa[1])\\n  \\n  expect_equal(norm$wynik, dane$wynik)\\n  expect_more_than(min(norm$wynik_norm), 0)\\n  expect_less_than(max(norm$wynik_norm), 40.001)\\n  expect_less_than(sd(norm$wynik_norm), 15)\\n})" }\n'
line: b'{ "repo_name": "zozlak/ZPD", "ref": "refs/heads/master", "path": "R/pobierz_wyniki_zrownywania.R", "content": "#\' @title Pobiera ramke danych z wynikami egzaminacyjnymi testow zrownujacych\\r\\n#\' @param src uchwyt \xc5\xbar\xc3\xb3d\xc5\x82a danych dplyr-a\\r\\n#\' @param rodzajEgzaminu rodzaj egzaminu, ktorego wyniki maja zostac pobrane\\r\\n#\' @param punktuj wybor, czy dane maja byc pobrane w postaci dystraktorow, czy punktow\\r\\n#\' @param rok rok, z ktorego dane maja zostac pobrane\\r\\n#\' @param idSkali identyfikator skali, ktora ma zostac zastosowana do danych\\r\\n#\' @param skroc czy do danych zastosowac skrocenia skal opisane w skali\\r\\n#\' @import dplyr\\r\\n#\' @export\\r\\npobierz_wyniki_zrownywania = function(\\r\\n  src,\\r\\n\\trodzajEgzaminu, \\r\\n\\trok, \\r\\n\\tpunktuj = TRUE, \\r\\n\\tidSkali = NULL,\\r\\n\\tskroc   = TRUE\\r\\n){\\r\\n  stopifnot(\\r\\n    is.src(src),\\r\\n    is.vector(rodzajEgzaminu), is.character(rodzajEgzaminu), length(rodzajEgzaminu) == 1,\\r\\n    is.vector(rok), is.numeric(rok), length(rok) == 1, \\r\\n    is.vector(punktuj), is.logical(punktuj), length(punktuj) == 1, punktuj %in% c(T, F),\\r\\n    is.null(idSkali) | is.vector(idSkali) & is.numeric(idSkali) & length(idSkali) == 1,\\r\\n    is.vector(skroc), is.logical(skroc), length(skroc) == 1, skroc %in% c(T, F)\\r\\n  )\\r\\n  \\r\\n  regExp = e(paste0(\'^zr\xc3\xb3wnywanie;\', rodzajEgzaminu, \';\', rok, \';.*$\'))\\r\\n\\ttests = pobierz_testy(src) %>% \\r\\n    collect() %>%\\r\\n    filter_(~grepl(regExp, opis_testu))\\r\\n\\tif(nrow(tests) == 0){\\r\\n\\t\\tstop(e(\'w bazie nie ma takiego zrownywania\'))\\r\\n\\t}\\r\\n\\r\\n\\ttmpName = sub(\'[.]\', \'_\', paste0(\'t\', as.numeric(Sys.time(), runif(1))))\\r\\n\\tDBI::dbGetQuery(src$con, e(paste0(\\"CREATE TEMPORARY VIEW \\", tmpName, \\" AS SELECT 1\\")))\\r\\n\\tquery = sprintf(\\r\\n    \\"SELECT zbuduj_widok_zrownywania(%s, %s, %d, %s, %s, %s, true)\\",\\r\\n    escape(tmpName),\\r\\n    escape(rodzajEgzaminu),\\r\\n    rok,\\r\\n    ifelse(punktuj, \'true\', \'false\'),\\r\\n    ifelse(is.null(idSkali), \'null\', as.numeric(idSkali)),\\r\\n    ifelse(skroc, \'true\', \'false\')\\r\\n  )\\r\\n\\tDBI::dbGetQuery(src$con, e(query))\\r\\n\\tdata = tbl(src, sql(e(paste0(\\"SELECT * FROM \\", tmpName))))\\r\\n\\r\\n\\tattr(data, \'idSkali\') = idSkali\\r\\n  \\r\\n\\treturn(data)\\r\\n}\\r\\nattr(pobierz_wyniki_zrownywania, \'grupa\') = \'wyniki\'\\r\\nattr(pobierz_wyniki_zrownywania, \'testArgs\') = list(\\r\\n  \'rodzajEgzaminu\' = \'sprawdzian\', \'rok\' = 2013, \'idSkali\' = 41\\r\\n)\\r\\n" }\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Decimal-to-binary.R","content":"# Program to convert decimal number into binary number using recursive function\\nconvert_to_binary <- function(n) {\\n  if(n > 1) {\\n    convert_to_binary(as.integer(n/2))\\n  }\\n  cat(n %% 2)\\n}\\n\\nconvert_to_binary(52)"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Factorial.R","content":"# take input from the user\\nfactorial <- function(n) {\\nfactorial = 1\\n# check is the number is negative, positive or zero\\nif(num < 0) {\\n  print(\'Sorry factorial does not exist for negative numbers\')\\n} else if(num == 0) {\\n  print(\'The factorial of 0 is 1\')\\n} else {\\n  for(i in 1:num) {\\n    factorial = factorial * i\\n  }\\n  print(paste(\'The factorial of\', num ,\'is\',factorial))\\n}\\n}\\n\\nfactorial(4)"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/First-10-Fibonacci.R","content":"fibonacci <- function(n) {\\nFibonacci <- numeric(n)\\nFibonacci[1] <- Fibonacci[2] <- 1\\nfor (i in 3:10) Fibonacci[i] <- Fibonacci[i - 2] + Fibonacci[i - 1]\\nprint(\'First 10 Fibonacci numbers:\'\')\\nprint(Fibonacci)\\n}\\n\\nfibonacci(10)"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Max-Min-of-Vector.R","content":"max_min_vector <- function() {\\nnums = c(10, 20, 30, 40, 50, 60)\\nprint(\'Original vector:\')\\nprint(nums)   \\nprint(paste(\'Maximum value of the said vector:\',max(nums)))\\nprint(paste(\'Minimum value of the said vector:\',min(nums)))\\n}\\n\\nmax_min_vector()"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Odd-or-Even.R","content":"# Program to check if the input number is odd or even.\\n# A number is even if division by 2 give a remainder of 0.\\n# If remainder is 1, it is odd.\\nodd_or_even <- function(n){\\nnum = n\\nif((num %% 2) == 0) {\\n  print(paste(num,\'is Even\'))\\n} else {\\n  print(paste(num,\'is Odd\'))\\n}\\n}\\n\\nodd_or_even(4)"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Prime-Numbers.R","content":"prime_numbers <- function(n) {\\n  if (n >= 2) {\\n    x = seq(2, n)\\n    prime_nums = c()\\n    for (i in seq(2, n)) {\\n      if (any(x == i)) {\\n        prime_nums = c(prime_nums, i)\\n        x = c(x[(x %% i) != 0], i)\\n      }\\n    }\\n    return(prime_nums)\\n  }\\n  else \\n  {\\n    stop(\'Input number should be at least 2.\'\')\\n  }\\n} \\nprime_numbers(12)"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Read-csv-file.R","content":"read_csv_file <- function() {\\nmovie_data = read.csv(file=movies.csv, header=TRUE, sep=\',\')\\nprint(\'Content of the .csv file:\'\')\\nprint(movie_data)\\n}\\n\\nread_csv_file()"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/add1.r","content":"add1 <- function(n) {\\n   print( n+1)\\n}\\n\\nadd1(90)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/add2numbers.r","content":"add2nums <- function(num1, num2) {\\n   print( num1+num2)\\n}\\n\\nadd2nums(34, 6)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/areaofsquare.r","content":"areaSquare <- function(num) {\\n   msg1 <- \'Invalid measurement\'\\n   msg2 <- \'Area of the Square is: \'\\n   if  (num <= 0){\\n        print(msg1)\\n   }else {\\n       print (msg2)\\n       print(num*num)\\n   }\\n  \\n}\\n\\nareaSquare(6)"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/divisibleby10.r","content":"divisibleby10 <- function(num) {\\nif(num %% 10 == 0){\\n    print(\'True\')\\n}else{\\n    print(\'False\')\\n}\\n\\n}\\n\\ndivisibleby10(60)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/integertype.r","content":"integerType <- function(num){\\nif(num > 0) {\\nprint(\'Positive number\')\\n} else {\\nif(num == 0) {\\nprint(\'Zero\')\\n} else {\\nprint(\'Negative number\')\\n}\\n}\\n}\\n\\nintegerType(-90)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/lengthoflist.r","content":"\\nlengthofVector <- function(vector){\\ncount <- 0\\nfor (i in vector){\\n    count <- count + 1\\n}\\nprint(count)\\n}\\n\\narray <- c(3,4,5,1,6)\\nlengthofVector(array)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/palindrome.r","content":"\\npalindrome <- function(n){\\nrev = 0\\n    num = n\\n\\n    while (n > 0) {\\n      r = n %% 10\\n      rev = rev * 10 + r\\n      n = n %/% 10\\n    }\\n\\n    if (rev == num)\\n    {\\n      print(paste(\'Number is palindrome :\', rev))\\n    }\\n    else{\\n      print(paste(\'Number is not palindrome :\', rev))\\n    }\\n}\\npalindrome(121)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/sort.r","content":"integerType <- function(num){\\nif(num > 0) {\\nsortvector <- function(vector){\\nprint(sort(vector))\\n}\\n\\narray <- c(23,12,11,34,21)\\nsortvector(array)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/spmv.r","content":"\\nsum <- function(vector){\\nresult <- 0\\nfor(i in vector){\\n    result = result + i\\n}\\nprint(result)\\n}\\n\\nproduct <- function(vector){\\n    result <- 1\\n    for(i in vector){\\n        result = result*i\\n    }\\n    print(result)\\n}\\n\\narray <- c(1,2,3,4)\\nsum(array)\\nproduct(array)\\nmean(array, na.rm=TRUE)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/upperlower.r","content":"\\nupperCase <- function(word){\\nresult <- toupper(word)\\nprint(result)\\n}\\n\\nlowerCase <- function(word){\\nresult <- tolower(word)\\nprint(result)\\n}\\n\\nupperCase(\'Function\')\\nlowerCase(\'FUNCTION\')\\n"}\n'
line: b'{ "repo_name": "mathii/europe_selection", "ref": "refs/heads/master", "path": "analysis/genome_wide_scan_imputed.R", "content": "#This is the genome-wide scan but using the read level information to try and\\n#get some idea about diploid calls. For reasons of speed, we break this one\\n#by chromosome. Genomic correction has to be done in another step for this reason.\\n\\n#Test whether the modern population frequencies can be modelled as a mixture of the\\n#Three ancestral populations.\\n#Using genotype probabilities imputed with Beagle.\\n\\nsource(\\"~/selection/code/lib/3pop_lib.R\\")\\n\\n#Modern GBR, CEU, IBS, TSI\\n#Ancient WHG, ENeo, Yamnaya\\n\\n########################################################################\\n## Details\\nchr <- 1                                #set manually, or from --args\\nversion <- \\"vx\\" #v6, v7 etc...\\nresults.tag <- \\"\\"\\nwhich.impute <- \\"within\\"\\n\\ncA <- commandArgs(TRUE)\\nif(length(cA)){\\n  chr <- cA[1]\\n  version <- cA[2]\\n  if(length(cA)>2){\\n    results.tag <- cA[3]\\n  }\\n  if(length(cA)>3){\\n    which.impute <- cA[4]\\n  }\\n}\\n\\nverbose=TRUE\\n## Supposed to check if running on cluster, but YMMV\\nif( Sys.info()[\\"login\\"]!=Sys.info()[\\"user\\"]){\\n    verbose=FALSE\\n}\\n\\n########################################################################\\n## Details\\nroot <- paste0(\\"~/selection/counts/\\", version, \\"/all\\")\\nout <- paste0(\\"~/selection/analysis/\\", version, \\"/gscan/\\")\\nindfile <- paste0(\\"~/data/\\", version, \\"/use/\\", version,\\"1kg_europe2names.ind\\")\\nimpute.file <- paste0(\\"~/selection/imputation/\\", version, \\"/imputed.\\", which.impute ,\\".chr\\", chr, \\".vcf.gz\\")\\nerror.prob <- 0.001\\n\\npops <- c(\\"WHG\\", \\"EN\\", \\"Yamnaya\\", \\"CEU\\", \\"GBR\\", \\"IBS\\", \\"TSI\\")\\n#Check if the SNP is monomorphic in these populations. \\nmonocheck <- c(\\"CEU\\", \\"GBR\\", \\"IBS\\", \\"TSI\\", \\"HungaryGamba_HG\\", \\"Loschbour\\", \\"Stuttgart\\",\\n               \\"LBK_EN\\", \\"HungaryGamba_EN\\", \\"Spain_EN\\", \\"Starcevo_EN\\", \\"LBKT_EN\\", \\"Yamnaya\\")\\nA <- matrix(c(0.164, 0.366, 0.470, 0.213, 0.337, 0.450, 0, 0.773, 0.227, 0, 0.712, 0.288),3, 4) \\n\\n########################################################################\\n\\nif(version==\\"v6\\" | version==\\"v7\\"){\\n\\n  pops <- c(\\"WHG\\", \\"EN\\", \\"Yamnaya\\", \\"CEU\\", \\"GBR\\", \\"IBS\\", \\"TSI\\")\\n#Check if the SNP is monomorphic in these populations. \\n  monocheck <- c(\\"CEU\\", \\"GBR\\", \\"IBS\\", \\"TSI\\", \\"HungaryGamba_HG\\", \\"Loschbour\\", \\"Stuttgart\\",\\n               \\"LBK_EN\\", \\"HungaryGamba_EN\\", \\"Spain_EN\\", \\"Starcevo_EN\\", \\"LBKT_EN\\", \\"Yamnaya\\")\\n  A <- matrix(c(0.164, 0.366, 0.470, 0.213, 0.337, 0.450, 0, 0.773, 0.227, 0, 0.712, 0.288),3, 4) \\n\\n  include.counts <- list(                 #Include these populations as hard calls. \\n    \\"WHG\\"=\\"Loschbour\\",\\n    \\"EN\\"=\\"Stuttgart\\",\\n    \\"CEU\\"=\\"CEU\\", \\"GBR\\"=\\"GBR\\", \\"IBS\\"=\\"IBS\\", \\"TSI\\"=\\"TSI\\" )\\n  \\ninclude.probs <- list(                  #Include these populations as reads\\n    ## \\"WHG\\"=c(\\"LaBrana1\\", \\"HungaryGamba_HG\\"), #Replace LaBrana1 with SpanishMesolithic for the high coverage LaBrana I0585\\n    \\"WHG\\"=c(\\"SpanishMesolithic\\", \\"HungaryGamba_HG\\"), #Replace LaBrana1 with SpanishMesolithic for the high coverage LaBrana I0585\\n    \\"EN\\"=c(\\"LBK_EN\\", \\"HungaryGamba_EN\\", \\"Spain_EN\\", \\"Starcevo_EN\\", \\"LBKT_EN\\"), \\n    \\"Yamnaya\\"=\\"Yamnaya\\")\\n}\\nif(version==\\"v7\\"){\\n  include.probs[[\\"WHG\\"]] <- gsub(\\"SpanishMesolithic\\", \\"Iberian_Mesolithic\\", include.probs[[\\"WHG\\"]], fixed=TRUE)\\n}\\nif(version==\\"v8\\"){\\n  mix.dir <- \\"~/selection/code/files/v8/mixtures/\\"\\n  \\n  if(results.tag==\\"\\"){stop(\\"Must specify results tag - group from 1-6 - for v8 analysis\\")}\\n  include.counts <- list( \\"CEU\\"=\\"CEU\\", \\"GBR\\"=\\"GBR\\", \\"IBS\\"=\\"IBS\\", \\"TSI\\"=\\"TSI\\" )\\n  always.counts <- c(\\"Loschbour\\", \\"Stuttgart\\")\\n  group <- results.tag\\n  choice <- read.table(paste0(mix.dir, \\"Choice\\", results.tag), as.is=TRUE, header=FALSE)\\n  include.probs <- list(c(), c(), c())\\n  names(include.probs) <- unique(choice[,2])\\n  for(i in 1:NROW(choice)){\\n    if(choice[i,1] %in% c(\\"Loschbour\\", \\"Stuttgart\\")){\\n      include.counts[[choice[i,2]]] <- choice[i,1]\\n    } else{\\n      include.probs[[choice[i,2]]] <- c(include.probs[[choice[i,2]]], choice[i,1])\\n    }\\n  }\\n  mix.mat <- read.table(paste0(mix.dir, \\"Proportion\\", results.tag), as.is=TRUE, header=TRUE)\\n  rownames(mix.mat) <- mix.mat[,1]\\n  mix.mat <- mix.mat[,2:NCOL(mix.mat)]\\n  \\n  anc.pops <- names(include.probs)\\n  mod.pops <- c(\\"CEU\\", \\"GBR\\", \\"IBS\\", \\"TSI\\")\\n  pops <- c(anc.pops, mod.pops)\\n  A <- t(mix.mat)[anc.pops,mod.pops]\\n\\n  monocheck <- c(unlist(include.probs), unlist(include.counts))\\n  names(monocheck) <- NULL\\n}\\n\\n##################################################################################################\\n\\n## Setup the count data. \\ncounts <- read.table(paste0(root, \\".count\\"), header=TRUE, as.is=TRUE)\\ntotals <- read.table(paste0(root, \\".total\\"), header=TRUE, as.is=TRUE)\\ndata <- counts[,1:5]\\ninclude <- data$CHR==chr\\ndata <- data[include,]\\n\\ncounts <- data.matrix(counts[,6:NCOL(counts)])\\ntotals <- data.matrix(totals[,6:NCOL(totals)])\\ncounts <- counts[include,]\\ntotals <- totals[include,]\\n\\n#Load imputed likelihoods\\nimpute <- read.table(impute.file, comment.char=\\"\\", as.is=TRUE, header=FALSE, sep=\\"\\\\t\\", fill=TRUE)\\ncomment.lines <- sum(grepl(\\"^##\\", impute[,1]))\\nimpute <- read.table(impute.file, comment.char=\\"\\", as.is=TRUE, header=TRUE, skip=comment.lines, sep=\\"\\\\t\\", fill=TRUE)\\nrownames(impute) <- impute$ID\\nimpute <- impute[impute$ID %in% data[,1],]\\nimpute.info <- impute[,8]\\nimpute <- impute[,10:NCOL(impute)]\\n\\n## get list of samples in each population of reads\\ninclude.prob.samples <- read.samples(indfile, include.probs)\\n\\n## set up results\\nresults <- matrix(0, nrow=NROW(data), ncol=3)\\nrownames(results) <- data$ID\\n\\n## Data structure\\nempty.data <- make.empty.data(pops)\\n\\nfor(i in 1:NROW(data)){\\n    this.snp <- data[i,1]\\n    if(verbose){cat(paste0(\\"\\\\r\\", i, \\" \\", this.snp))}\\n    this.prob <- impute[i,]\\n\\n    freq.data <- make.prob.freq.data(pops, include.probs, include.prob.samples, include.counts,\\n                                this.prob, counts[i,], totals[i,], empty.data)\\n    monomorphic <- all(counts[i,monocheck]==0)|all(counts[i,monocheck]==totals[i,monocheck])\\n    if(monomorphic){\\n        results[i,] <- NA\\n    }else{\\n        AR2 <- as.numeric(strsplit(strsplit(impute.info[i], \\";\\", fixed=TRUE)[[1]][1], \\"=\\", fixed=TRUE)[[1]][2])\\n        results[i,] <- c(test.3pop.reads(freq.data, A, error.prob=error.prob), AR2)\\n    }\\n}\\n\\nresults <- results[!is.na(results[,2]),]\\n\\nresults <- cbind(rownames(results), results)\\ncolnames(results) <- c(\\"ID\\", \\"ChiSq\\", \\"uncorrected.p\\", \\"AR2\\")\\nresults <- data.frame(results)\\nout.file <-  paste0(\\"~/selection/analysis/\\",version,\\"/gscan/scan_results_imputed\\", results.tag, \\".\\", which.impute, \\".chr\\", chr, \\".txt\\")\\nprint(out.file)\\nwrite.table(results,out.file, row.names=FALSE, col.names=TRUE, quote=FALSE, sep=\\"\\\\t\\")\\n\\n" }\n'
line: b'{ "repo_name": "jonhoo/volley", "ref": "refs/heads/master", "path": "benchmark/plot.R", "content": "#!/usr/bin/env Rint\\nlibrary(grDevices)\\nlibrary(utils)\\nX11(width=12, height=10)\\n\\nlibrary(ggplot2)\\nargs <- commandArgs(trailingOnly = TRUE)\\nargs <- if (length(args) == 0) Sys.getenv(\\"ARGS\\") else args\\nargs <- if (args[1] == \\"\\") \\"plot.dat\\" else args\\n\\nd <- data.frame(read.table(\\n\\t\\t\\t   text=gsub(\'us \', \' \', readLines(file(args[1]))),\\n\\t\\t\\t   col.names=c(\\"server\\", \\"clients\\", \\"cores\\", \\"time\\", \\"stddev\\", \\"n\\")\\n\\t\\t\\t   ))\\n\\nd$ci = 2.58 * d$stddev / sqrt(d$n)\\nd$ops = d$clients/(d$time/1000.0/1000.0)\\nd$min = d$clients/((d$time-d$ci)/1000.0/1000.0)\\nd$max = d$clients/((d$time+d$ci)/1000.0/1000.0)\\n\\n#d = d[d[, \\"clients\\"] == 80,]\\n#d = d[grep(\\"^go\\", d[, \\"server\\"]),]\\nprint(d)\\np <- ggplot(data=d, aes(x = cores, y = ops, ymin = min, ymax = max, color = server))\\np <- p + geom_line()\\np <- p + ylim(0, 2000000)\\np <- p + geom_errorbar()\\np <- p + facet_wrap(~ clients)\\np <- p + xlab(\\"CPU cores\\")\\np <- p + ylab(\\"Mean ops/s\\")\\n\\np\\nggsave(\\"plot.png\\", plot = p, width = 8, height = 6)\\n" }\n'
line: b'{ "repo_name": "jaredhuling/rfunctions", "ref": "refs/heads/master", "path": "R/lanczos.R", "content": "\\n\\n#\' Compute Largest Singular Value of Matrix x\\n#\'\\n#\' @param x matrix input\\n#\' @param v numeric vector. Initialize GKL bidiagonalization with random vector on unit sphere\\n#\' @param maxit integer. Maximum number of Lanczos iterations\\n#\' @param reorthog Takes values in 0:2. 0 for no reorthogonalization, 1 for reorthogonalization of \\n#\' V vectors (slower, more accurate), 2 for reorthogonalization of V and U vectors (slowest and most \\n#\' memory, most accurate. Not implemented yet)\\n#\' @param upper.bound.prob upper bound probability for the largest singular value\\n#\' @return list \\n#\' @references Hochstenbach (2013) Probabilistic upper bounds for the matrix two-norm, http://link.springer.com/article/10.1007/s10915-013-9716-x \\n#\' Journal of Scientific Computing, Vol. 57(3), Dec. 2013\\n#\' @export\\n#\' @examples\\n#\'n.obs <- 1e5\\n#\'n.vars <- 150\\n#\'\\n#\'x <- matrix(rnorm(n.obs * n.vars), n.obs, n.vars)\\n#\'\\n#\'## compute largest singular value of x\\n#\'lanczos <- gklBidiag(x, runif(ncol(x)), 10L, reorth = 0, upper.bound.prob = 0.99)\\n#\'str(lanczos)\\nsetGeneric(\\"gklBidiag\\", function(x, v, maxit, reorthog = 0, upper.bound.prob = NULL) {\\n  stopifnot(is.numeric(x) | inherits(x, \\"CsparseMatrix\\"))\\n  stopifnot(is.numeric(v))\\n  reorthog <- as.integer(reorthog)\\n  if (inherits(x, \\"CsparseMatrix\\")) {\\n    .Call(\\"GKLBidiagSparse\\", A = x, v = v, k = maxit, reorthog = reorthog, PACKAGE = \\"rfunctions\\")\\n  } else {\\n    .Call(\\"GKLBidiag\\", A = x, v = v, k = maxit, reorthog = reorthog, PACKAGE = \\"rfunctions\\")\\n  }\\n})\\n\\nsetMethod(\\"gklBidiag\\", signature(x = \\"matrix\\", v = \\"numeric\\", maxit = \\"numeric\\", \\n                                 reorthog = \\"numeric\\", upper.bound.prob = \\"numeric\\"),\\n            function(x, v, maxit = 50L, reorthog = 0, upper.bound.prob = NULL) {\\n              maxit <- as.integer(maxit)\\n              reorthog <- as.integer(reorthog)\\n              delt <- computeDelta(eps = 1 - upper.bound.prob, p = ncol(x))\\n              res <- .Call(\\"GKLBidiag\\", A = x, v = v, k = maxit, reorthog = reorthog, PACKAGE = \\"rfunctions\\")\\n              upper <- computeUpperBound(res, delt)\\n              res$upper.bound <- upper\\n              res$upper.bound.prob <- upper.bound.prob\\n              res\\n            })\\n\\nsetMethod(\\"gklBidiag\\", signature(x = \\"dgeMatrix\\", v = \\"numeric\\", maxit = \\"numeric\\", \\n                                 reorthog = \\"numeric\\", upper.bound.prob = \\"numeric\\"),\\n          function(x, v, maxit = 50L, reorthog = 0, upper.bound.prob = NULL) {\\n            maxit <- as.integer(maxit)\\n            reorthog <- as.integer(reorthog)\\n            delt <- computeDelta(eps = 1 - upper.bound.prob, p = ncol(x))\\n            res <- .Call(\\"GKLBidiag\\", A = x, v = v, k = maxit, reorthog = reorthog, PACKAGE = \\"rfunctions\\")\\n            upper <- computeUpperBound(res, delt)\\n            res$upper.bound <- upper\\n            res$upper.bound.prob <- upper.bound.prob\\n            res\\n          })\\n\\n\\nsetMethod(\\"gklBidiag\\", signature(x = \\"CsparseMatrix\\", v = \\"numeric\\", maxit = \\"numeric\\", \\n                                 reorthog = \\"numeric\\", upper.bound.prob = \\"numeric\\"),\\n          function(x, v, maxit = 50L, reorthog = 0, upper.bound.prob = NULL) {\\n            maxit <- as.integer(maxit)\\n            reorthog = as.integer(reorthog)\\n            delt <- computeDelta(eps = 1 - upper.bound.prob, p = ncol(x))\\n            res <- .Call(\\"GKLBidiagSparse\\", A = x, v = v, k = maxit, reorthog = reorthog, PACKAGE = \\"rfunctions\\")\\n            upper <- computeUpperBound(res, delt)\\n            res$upper.bound <- upper\\n            res$upper.bound.prob <- upper.bound.prob\\n            res\\n          })\\n\\n\\nsetMethod(\\"gklBidiag\\", signature(x = \\"matrix\\", v = \\"numeric\\", maxit = \\"numeric\\", \\n                                 reorthog = \\"numeric\\", upper.bound.prob = \\"missing\\"),\\n          function(x, v, maxit = 50L, reorthog = 0, upper.bound.prob = NULL) {\\n            maxit <- as.integer(maxit)\\n            reorthog <- as.integer(reorthog)\\n            res <- .Call(\\"GKLBidiag\\", A = x, v = v, k = maxit, reorthog = reorthog, PACKAGE = \\"rfunctions\\")\\n            res$upper.bound <- NULL\\n            res$upper.bound.prob <- NULL\\n            res\\n          })\\n\\nsetMethod(\\"gklBidiag\\", signature(x = \\"dgeMatrix\\", v = \\"numeric\\", maxit = \\"numeric\\", \\n                                 reorthog = \\"numeric\\", upper.bound.prob = \\"missing\\"),\\n          function(x, v, maxit = 50L, reorthog = 0, upper.bound.prob = NULL) {\\n            maxit <- as.integer(maxit)\\n            reorthog <- as.integer(reorthog)\\n            res <- .Call(\\"GKLBidiag\\", A = x, v = v, k = maxit, reorthog = reorthog, PACKAGE = \\"rfunctions\\")\\n            res$upper.bound <- NULL\\n            res$upper.bound.prob <- NULL\\n            res\\n          })\\n\\n\\nsetMethod(\\"gklBidiag\\", signature(x = \\"CsparseMatrix\\", v = \\"numeric\\", maxit = \\"numeric\\", \\n                                 reorthog = \\"numeric\\", upper.bound.prob = \\"missing\\"),\\n          function(x, v, maxit = 50L, reorthog = 0, upper.bound.prob = NULL) {\\n            maxit <- as.integer(maxit)\\n            reorthog <- as.integer(reorthog)\\n            res <- .Call(\\"GKLBidiagSparse\\", A = x, v = v, k = maxit, reorthog = reorthog, PACKAGE = \\"rfunctions\\")\\n            res$upper.bound <- NULL\\n            res$upper.bound.prob <- NULL\\n            res\\n          })\\n\\n" }\n'
line: b'{ "repo_name": "edzer/gstat", "ref": "refs/heads/master", "path": "tests/sim.R", "content": "options(digits=6)\\nlibrary(sp)\\ndata(meuse)\\nset.seed(158229572)\\nnew.locs <- data.frame(x = c(181170, 180310, 180205, 178673, 178770, 178270),\\n\\ty = c(333250, 332189, 331707, 330066, 330675, 331075))\\nlibrary(gstat)\\nkrige(zinc ~ 1, ~ x + y, meuse, newdata = new.locs, \\n\\t\\tmodel = vgm(1.34e5, \\"Sph\\", 800, nug = 2.42e4), \\n\\t\\tblock = c(40,40), nmax = 40, nsim = 10)\\n" }\n'
line: b'{ "repo_name": "edzer/gstat", "ref": "refs/heads/master", "path": "tests/na.action.R", "content": "library(sp)\\n\\ndata(meuse)\\ndata(meuse.grid)\\n\\nset.seed(13131) # reproduce results\\n\\n# select 10 random rows;\\n# create two missing values in the coordinates:\\nm = meuse.grid[sample(nrow(meuse.grid), 10), ]\\nm[c(2,8), \\"x\\"] = NA\\n\\nlibrary(gstat)\\n## this is not allowed anymore:\\ntry(krige(log(zinc)~1,~x+y,meuse,m, na.action = na.pass))\\ntry(krige(log(zinc)~1,~x+y,meuse,m, na.action = na.omit))\\ntry(krige(log(zinc)~1,~x+y,meuse,m, na.action = na.exclude))\\ntry(krige(log(zinc)~1,~x+y,meuse,m, na.action = na.fail))\\n\\n# select 10 random rows;\\n# create two missing values in the regressor variable:\\nm = meuse.grid[sample(nrow(meuse.grid), 10), ]\\nm[c(3,7), \\"dist\\"] = NA\\nkrige(log(zinc)~dist,~x+y,meuse,m, na.action = na.pass)\\nkrige(log(zinc)~dist,~x+y,meuse,m, na.action = na.omit)\\nkrige(log(zinc)~dist,~x+y,meuse,m, na.action = na.exclude)\\ntry(krige(log(zinc)~dist,~x+y,meuse,m, na.action = na.fail))\\n" }\n'
line: b'{ "repo_name": "edzer/gstat", "ref": "refs/heads/master", "path": "demo/grass.R", "content": "# $Id: grass.R,v 1.4 2006-02-10 19:05:02 edzer Exp $\\n# this demo assumes quite a lot:\\n#  a. it assumes GRASS gis is running\\n#  b. it assumes that the meuse data zinc variable is available as a site list\\n#  c. it assumes that mask_map is present, and contains the mask map values\\n#     (i.e., the study area)\\n\\nlibrary(sp)\\nlibrary(GRASS)           # load R GRASS interface\\n\\nG = gmeta()              # retrieves active data base locations and topology\\nd = sites.get(G, \\"zinc\\") # retrieve zinc observations \\nplot(d$east, d$north, asp=1)\\nnames(d)[4] = \\"zinc\\"     # rename attribute\\nmask = rast.get\\n\\nhist(d$zinc)\\nhist(log(d$zinc))\\n\\nmask = rast.get(G, \\"mask_map\\")\\nplot(G, mask$mask.map)\\npoints(d$east,d$north, pch=\\"+\\")\\n\\nlibrary(gstat)           # load gstat library\\n\\nbubble(d, zcol = \\"zinc\\", col=c(4,5), maxsize=2)\\n\\n# explain S formulae: ~\\nv = variogram(log(zinc)~1, ~east+north, d)\\nplot(v)\\n\\nv.mod = vgm(.6, \\"Sph\\", 900, .1)\\nplot(v, model = v.mod)\\n\\nv.fit = fit.variogram(v, v.mod)\\nplot(v, model = v.fit)\\n\\nzinc.g = gstat(NULL, \\"lzinc\\", log(zinc)~1, ~east+north, d, model = v.fit)\\nnew.data = data.frame(east = east(G), north = north(G))\\nnew.data[is.na(mask$mask.map), ] = c(NA,NA)\\n\\nzinc.kr = predict(zinc.g, new.data)\\nimage(zinc.kr)\\n\\nlibrary(lattice)\\n\\nlevelplot(lzinc.pred~east+north, zinc.kr, asp=1.34, col.regions=bpy.colors(100))\\n\\n# push prediction and variances grids back into GRASS data base:\\nrast.put(G, \\"lzinc.pred\\", zinc.kr$lzinc.pred)\\nrast.put(G, \\"lzinc.var\\",  zinc.kr$lzinc.var)\\n\\n# push cross validation residuals back to GRASS data base:\\nxv = krige.cv(log(zinc)~1, ~east+north, d, v.fit, nmax = 40, verb=F)\\nsites.put2(G, data = xv, dims = c(\\"east\\", \\"north\\", \\"residual\\", \\"zscore\\"), \\n\\tlname = \\"lzinc.xv\\")\\n" }\n'
line: b'{ "repo_name": "edzer/gstat", "ref": "refs/heads/master", "path": "R/xyz2img.R", "content": "# $Id: xyz2img.q,v 1.4 2006-02-10 19:01:07 edzer Exp $\\n\\n\\"xyz2img\\" <-\\nfunction (xyz, zcol = 3, xcol = 1, ycol = 2, tolerance = 10 * .Machine$double.eps) \\n{\\n    if (ncol(xyz) < 3) \\n        stop(\\"xyz object should have at least three columns\\")\\n    z = xyz[, zcol]\\n    x = xyz[, xcol]\\n    y = xyz[, ycol]\\n    xx = sort(unique(x))\\n    yy = sort(unique(y))\\n    nx = length(xx)\\n    ny = length(yy)\\n    nmax = max(nx, ny)\\n    difx = diff(xx)\\n    if (diff(range(unique(difx))) > tolerance) \\n        stop(\\"x intervals are not constant\\")\\n    dify = diff(yy)\\n    if (diff(range(unique(dify))) > tolerance) \\n        stop(\\"y intervals are not constant\\")\\n    dx = mean(difx)\\n    dy = mean(dify)\\n    xmin = min(xx)\\n    xmax = max(xx)\\n    xrange = xmax - xmin\\n    ymin = min(yy)\\n    ymax = max(yy)\\n    yrange = ymax - ymin\\n    row = round((x - xmin)/dx) + 1\\n    col = round((y - ymin)/dy) + 1\\n\\tzz = rep(as.numeric(NA), nx * ny)\\n\\tzz[row + nx * (col - 1)] = z\\n\\tzz = matrix(zz, nrow = nx, ncol = ny)\\n    list(x = seq(xmin, xmax, dx), y = seq(ymin, ymax, dy), z = zz)\\n}\\n" }\n'
line: b'{ "repo_name": "edzer/gstat", "ref": "refs/heads/master", "path": "tests/vdist.R", "content": "library(sp)\\nlibrary(gstat)\\n\\ndata(meuse)\\ncoordinates(meuse) = ~x+y\\ndata(meuse.grid)\\ngridded(meuse.grid) = ~x+y\\n\\nmg = meuse.grid\\ngridded(mg) = FALSE\\nmg= mg[1500,]\\nkrige(log(zinc)~1,meuse,mg,vgm(1, \\"Exp\\", 300, anis=c(0,0.01)),\\n\\tvdist=FALSE, maxdist=1000,nmax=10)\\nkrige(log(zinc)~1,meuse,mg,vgm(1, \\"Exp\\", 300, anis=c(0,0.01)),\\n\\tvdist=TRUE, maxdist=1000,nmax=10)\\n" }\n'
line: b'{ "repo_name": "edzer/gstat", "ref": "refs/heads/master", "path": "demo/ikr.R", "content": "library(sp)\\nlibrary(gstat)\\ndata(meuse)\\ndata(meuse.grid)\\ncoordinates(meuse)=~x+y\\ngridded(meuse.grid)=~x+y\\nv = variogram(I(zinc < 500)~1,meuse)\\nplot(v)\\nvm = fit.variogram(v, vgm(1, \\"Sph\\", 300, 1))\\nplot(v,vm)\\nvm\\n# possibly adjust sum of sill to be max. 0.25?\\nik = krige(I(zinc>500)~1, meuse, meuse.grid, vm)\\nspplot(ik[1],col.regions=bpy.colors())\\nsummary(ik[[1]])\\n# adjust values outside [0,1] to nearest limit:\\nik[[1]][ik[[1]]<0] = 0\\nik[[1]][ik[[1]]>1] = 1\\nsummary(ik[[1]])\\nspplot(ik[1],col.regions=bpy.colors())\\n" }\n'
line: b'{ "repo_name": "google/rappor", "ref": "refs/heads/master", "path": "analysis/R/alternative.R", "content": "# Copyright 2014 Google Inc. All rights reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\nlibrary(limSolve)\\nlibrary(Matrix)\\n\\n# The next two functions create a matrix (G) and a vector (H) encoding\\n# linear inequality constraints that a solution vector (x) must satisfy:\\n#                       G * x >= H\\n\\n# Currently represent three sets of constraints on the solution vector:\\n#  - all solution coefficients are nonnegative\\n#  - the sum total of all solution coefficients is no more than 1\\n#  - in each of the coordinates of the target vector (estimated Bloom filter)\\n#    we don\'t overshoot by more than three standard deviations.\\nMakeG <- function(n, X) {\\n  d <- Diagonal(n)\\n  last <- rep(-1, n)\\n  rbind2(rbind2(d, last), -X)\\n}\\n\\nMakeH <- function(n, Y, stds) {\\n  # set the floor at 0.01 to avoid degenerate cases\\n  YY <- apply(Y + 3 * stds,  # in each bin don\'t overshoot by more than 3 stds\\n              1:2,\\n              function(x) min(1, max(0.01, x)))  # clamp the bound to [0.01,1]\\n\\n  c(rep(0, n),  # non-negativity condition\\n    -1,         # coefficients sum up to no more than 1\\n    -as.vector(t(YY))   # t is important!\\n    )\\n}\\n\\nMakeLseiModel <- function(X, Y, stds) {\\n  m <- dim(X)[1]\\n  n <- dim(X)[2]\\n\\n# no slack variables for now\\n#   slack <- Matrix(FALSE, nrow = m, ncol = m, sparse = TRUE)\\n#   colnames(slack) <- 1:m\\n#   diag(slack) <- TRUE\\n#\\n#   G <- MakeG(n + m)\\n#   H <- MakeH(n + m)\\n#\\n#   G[n+m+1,n:(n+m)] <- -0.1\\n#  A = cbind2(X, slack)\\n\\n  w <- as.vector(t(1 / stds))\\n  w_median <- median(w[!is.infinite(w)])\\n  if(is.na(w_median))  # all w are infinite\\n    w_median <- 1\\n  w[w > w_median * 2] <- w_median * 2\\n  w <- w / mean(w)\\n\\n  list(# coerce sparse Boolean matrix X to sparse numeric matrix\\n       A = Diagonal(x = w) %*% (X + 0),\\n       B = as.vector(t(Y)) * w,  # transform to vector in the row-first order\\n       G = MakeG(n, X),\\n       H = MakeH(n, Y, stds),\\n       type = 2)  # Since there are no equality constraints, lsei defaults to\\n                  # solve.QP anyway, but outputs a warning unless type == 2.\\n}\\n\\n# CustomLM(X, Y)\\nConstrainedLinModel <- function(X,Y) {\\n  model <- MakeLseiModel(X, Y$estimates, Y$stds)\\n  coefs <- do.call(lsei, model)$X\\n  names(coefs) <- colnames(X)\\n\\n  coefs\\n}" }\n'
line: b'{ "repo_name": "google/rappor", "ref": "refs/heads/master", "path": "analysis/R/fast_em.R", "content": "# fast_em.R: Wrapper around analysis/cpp/fast_em.cc.\\n#\\n# This serializes the input, shells out, and deserializes the output.\\n\\n.Flatten <- function(list_of_matrices) {\\n  listOfVectors <- lapply(list_of_matrices, as.vector)\\n  #print(listOfVectors)\\n\\n  # unlist takes list to vector.\\n  unlist(listOfVectors)\\n}\\n\\n.WriteListOfMatrices <- function(list_of_matrices, f) {\\n  flattened <- .Flatten(list_of_matrices)\\n\\n  # NOTE: UpdateJointConditional does outer product of dimensions!\\n\\n  # 3 letter strings are null terminated\\n  writeBin(\'ne \', con = f)\\n  num_entries <- length(list_of_matrices)\\n  writeBin(num_entries, con = f)\\n\\n  Log(\'Wrote num_entries = %d\', num_entries)\\n\\n  # For 2x3, this is 6\\n  writeBin(\'es \', con = f)\\n\\n  entry_size <- as.integer(prod(dim(list_of_matrices[[1]])))\\n  writeBin(entry_size, con = f)\\n\\n  Log(\'Wrote entry_size = %d\', entry_size)\\n\\n  # now write the data\\n  writeBin(\'dat\', con = f)\\n  writeBin(flattened, con = f)\\n}\\n\\n.ExpectTag <- function(f, tag) {\\n  # Read a single NUL-terminated character string.\\n  actual <- readBin(con = f, what = \\"char\\", n = 1)\\n\\n  # Assert that we got what was expected.\\n  if (length(actual) != 1) {\\n    stop(sprintf(\\"Failed to read a tag \'%s\'\\", tag))\\n  }\\n  if (actual != tag) {\\n    stop(sprintf(\\"Expected \'%s\', got \'%s\'\\", tag, actual))\\n  }\\n}\\n\\n.ReadResult <- function (f, entry_size, matrix_dims) {\\n  .ExpectTag(f, \\"emi\\")\\n  # NOTE: assuming R integers are 4 bytes (uint32_t)\\n  num_em_iters <- readBin(con = f, what = \\"int\\", n = 1)\\n\\n  .ExpectTag(f, \\"pij\\")\\n  pij <- readBin(con = f, what = \\"double\\", n = entry_size)\\n\\n  # Adjust dimensions\\n  dim(pij) <- matrix_dims\\n\\n  Log(\\"Number of EM iterations: %d\\", num_em_iters)\\n  Log(\\"PIJ read from external implementation:\\")\\n  print(pij)\\n   \\n  # est, sd, var_cov, hist\\n  list(est = pij, num_em_iters = num_em_iters)\\n}\\n\\n.SanityChecks <- function(joint_conditional) {\\n  # Display some stats before sending it over to C++.\\n\\n  inf_counts <- lapply(joint_conditional, function(m) {\\n    sum(m == Inf)\\n  })\\n  total_inf <- sum(as.numeric(inf_counts))\\n\\n  nan_counts <- lapply(joint_conditional, function(m) {\\n    sum(is.nan(m))\\n  })\\n  total_nan <- sum(as.numeric(nan_counts))\\n\\n  zero_counts <- lapply(joint_conditional, function(m) {\\n    sum(m == 0.0)\\n  })\\n  total_zero <- sum(as.numeric(zero_counts))\\n\\n  #sum(joint_conditional[joint_conditional == Inf, ])\\n  Log(\'total inf: %s\', total_inf)\\n  Log(\'total nan: %s\', total_nan)\\n  Log(\'total zero: %s\', total_zero)\\n}\\n\\nConstructFastEM <- function(em_executable, tmp_dir) {\\n\\n  return(function(joint_conditional, max_em_iters = 1000,\\n                  epsilon = 10 ^ -6, verbose = FALSE,\\n                  estimate_var = FALSE) {\\n    matrix_dims <- dim(joint_conditional[[1]])\\n    # Check that number of dimensions is 2.\\n    if (length(matrix_dims) != 2) {\\n      Log(\'FATAL: Expected 2 dimensions, got %d\', length(matrix_dims))\\n      stop()\\n    }\\n\\n    entry_size <- prod(matrix_dims)\\n    Log(\'entry size: %d\', entry_size)\\n\\n    .SanityChecks(joint_conditional)\\n\\n    input_path <- file.path(tmp_dir, \'list_of_matrices.bin\')\\n    Log(\\"Writing flattened list of matrices to %s\\", input_path)\\n    f <- file(input_path, \'wb\')  # binary file\\n    .WriteListOfMatrices(joint_conditional, f)\\n    close(f)\\n    Log(\\"Done writing %s\\", input_path)\\n     \\n    output_path <- file.path(tmp_dir, \'pij.bin\')\\n\\n    cmd <- sprintf(\\"%s %s %s %s\\", em_executable, input_path, output_path,\\n                   max_em_iters)\\n\\n    Log(\\"Shell command: %s\\", cmd)\\n    exit_code <- system(cmd)\\n\\n    Log(\\"Done running shell command\\")\\n    if (exit_code != 0) {\\n      stop(sprintf(\\"Command failed with code %d\\", exit_code))\\n    }\\n\\n    f <- file(output_path, \'rb\')\\n    result <- .ReadResult(f, entry_size, matrix_dims)\\n    close(f)\\n\\n    result\\n  })\\n}\\n" }\n'
line: b'{ "repo_name": "kwanjeeraw/grinn", "ref": "refs/heads/master", "path": "R/getGrinnDb.R", "content": "#\'Get Grinn database location\\n#\'@description get Grinn database location of the current working envornment. \\n#\'By default Grinn database lacation is http://grinn.genomecenter.ucdavis.edu:7474/db/data/cypher \\n#\'which contains only human database.\\n#\'@usage getGrinnDb()\\n#\'@return url of Grinn database location.\\n#\'@author Kwanjeera W \\\\email{kwanich@@ucdavis.edu}\\n#\'@export\\ngetGrinnDb <- function(){\\n  print(nld)\\n}" }\n'
line: b'{ "repo_name": "kwanjeeraw/grinn", "ref": "refs/heads/master", "path": "R/fetchtRelation.R", "content": "#\' \\\\code{fetchRelation} get path information\\n#\'@description get path information as the output for further uses by \\\\code{formatNetworkOutput}.\\n#\'@seealso \\\\code{formatNetworkOutput}\\n#\'#result <- fetchRelation(\\"http://localhost:7474/db/data/relationship/53\\")\\n#\'#return start-relation-end\\n\\nfetchRelation <- function(url){\\n  out <- tryCatch(\\n  {\\n    path = curlRequestUrlToList(url)\\n    start = curlRequestUrlToList(path$start)\\n    end = curlRequestUrlToList(path$end)\\n    type = path$type\\n    dataSource = path$data$source\\n    startGID = start$data$GID\\n    startName = start$data$name\\n    startXref = paste0(start$data$xref,collapse = \\"||\\")\\n    startLabel = start$metadata$labels[[1]]\\n    endGID = end$data$GID\\n    endName = end$data$name\\n    endXref = paste0(end$data$xref,collapse = \\"||\\")\\n    endLabel = end$metadata$labels[[1]]\\n    ## Set the name for the class\\n    relation = list(startGID=startGID, startName=startName, startXref=startXref, startLabel=startLabel, \\n                    endGID=endGID, endName=endName, endXref=endXref, endLabel=endLabel, type=type, dataSource=dataSource)\\n  },\\n  error=function(e) {\\n    message(e)\\n    cat(\\"\\\\n..RETURN empty list of relations\\")\\n    out = list() # Choose a return value in case of error\\n  })    \\n  return(out)\\n}\\n\\nfetchRelation.TRANSACTION <- function(graph){\\n  out <- tryCatch(\\n    {\\n      ## Set the name for the class\\n      data.frame(t(sapply(graph$graph$relationships, \\n                                 function(x) list(source=x$startNode, target=x$endNode, relname=x$type, relsource=x$properties[\\"source\\"]))))\\n      \\n      #relationInfo = list(source=graph$graph$relationships[[1]]$startNode, target=graph$graph$relationships[[1]]$endNode, relname=graph$graph$relationships[[1]]$type, relsource=graph$graph$relationships[[1]]$properties[\\"source\\"])\\n    },\\n    error=function(e) {\\n      message(e)\\n      cat(\\"\\\\n..RETURN empty list of relations\\")\\n      out = data.frame() # Choose a return value in case of error\\n    })    \\n  return(out)\\n}\\n\\nfetchNode.TRANSACTION <- function(node){\\n  out <- tryCatch(\\n    {\\n      data.frame(t(sapply(node, \\n                          function(x) list(id=x$id, gid=x$properties$GID, nodename=x$properties$name, xref=paste0(x$properties$xref,collapse = \\"||\\"), nodetype=x$labels))))\\n      #nodeInfo = list(id=node$id, gid=node$properties$GID, nodename=node$properties$name, xref=paste0(node$properties$xref,collapse = \\"||\\"), nodetype=node$labels)\\n    },\\n    error=function(e) {\\n      message(e)\\n      cat(\\"\\\\n..RETURN empty list of relations\\")\\n      out = data.frame() # Choose a return value in case of error\\n    })    \\n  return(out)\\n}" }\n'
line: b'{ "repo_name": "kwanjeeraw/grinn", "ref": "refs/heads/master", "path": "inst/shiny/layout/setdb.R", "content": "mainPanel(width=12,\\n          fluidRow(column(12,\\n                          mainPanel(width=12,\\n                                    h3(\\"setGrinnDb\\"),\\n                                    p(\\"Set the graph database location for the currently working environment, see \\",\\n                                      a(href=\'http://kwanjeeraw.github.io/grinn/setdb.html\',target=\'_blank\',\'here\'),\' for argument details.\'\\n                                    )\\n                          )#end mainPanel\\n          )),\\n          wellPanel(\\n            h4(\\"Current database location:\\"),\\n            fluidRow(\\n              column(12, verbatimTextOutput(\\"currentdb\\"))\\n            ),\\n            h4(\\"Input arguments:\\"), helpText(\\"* required field\\"),\\n            fluidRow(\\n              column(6, textInput(inputId=\'dburl\', label=\'url *\', value=\\"\\"))\\n            ),\\n            hr(),\\n            actionButton(\\"submit\\",\\"Submit\\")\\n          )\\n)" }\n'
line: b'{ "repo_name": "kwanjeeraw/grinn", "ref": "refs/heads/master", "path": "R/fetchNodeRelation.R", "content": "#\' \\\\code{fetchNodeRelation} format get node relationships\\n#\'@description get node relationships as the output for further uses by \\\\code{formatNodeOutput}.\\n#\'@seealso \\\\code{formatNodeOutput}\\n#\'#result <- fetchNodeRelation(\\"http://localhost:7474/db/data/node/53/relationships/in\\")\\n#\'#return start-relation-end\\n\\nfetchNodeRelation <- function(url){\\n  path = curlRequestUrlToList(url)\\n  relations = list()\\n  if(length(path)>0){\\n    for(i in 1:length(path)){\\n      start = curlRequestUrlToList(path[[i]]$start)\\n      end = curlRequestUrlToList(path[[i]]$end)\\n      type = path[[i]]$type\\n      dataSource = path[[i]]$data$source\\n      startGID = start$data$GID\\n      startName = start$data$name\\n      startXref = paste0(start$data$xref,collapse = \\"||\\")\\n      startLabel = start$metadata$labels[[1]]\\n      endGID = end$data$GID\\n      endName = end$data$name\\n      endXref = paste0(end$data$xref,collapse = \\"||\\")\\n      endLabel = end$metadata$labels[[1]]\\n      ## Set the name for the class\\n      relation = data.frame(startGID=startGID, startName=startName, startXref=startXref, startLabel=startLabel, \\n                      endGID=endGID, endName=endName, endXref=endXref, endLabel=endLabel, type=type, dataSource=dataSource)\\n      relations = rbind(relations,relation)\\n    }\\n    relations <- unique(relations)\\n  }else{\\n    #cat(\\"\\\\n..RETURN empty list\\")\\n    relations = list() # Choose a return value in case of error\\n  } \\n  return(relations)\\n}" }\n'
line: b'{ "repo_name": "sestelo/shiny_npregfast", "ref": "refs/heads/master", "path": "ui.R", "content": "#detach(\\"package:npregfast\\")\\nlibrary(shiny)\\n#library(shinyjs)\\nlibrary(miniUI)\\nlibrary(wesanderson)\\nlibrary(npregfast)\\n\\n\\nshinyUI(fluidPage(\\n  title = \\"Demo of npregfast\\",\\n  tags$head(includeCSS(file.path(\'www\', \'style.css\'))),   \\n  shinyjs::useShinyjs(),\\n  \\n  fluidRow(id = \\"title-row\\",\\n           column(12,\\n                  h1(\\"Demo of\\",em(a(\\"npregfast\\", href = \\"https://github.com/sestelo/npregfast\\"))),\\n                  h4(\\"Example with\\", a(\\"barnacle\\", href = \\"https://github.com/sestelo/npregfast/blob/master/man/barnacle.Rd\\"),\\" data set\\"),\\n                  div(\\"Created by\\", a(\\"Marta Sestelo\\", href = \\"http://sestelo.github.io\\"),\\n                      \\"and\\", a(\\"Nora M. Villanueva\\",href = \\"http://noramvillanueva.github.io\\"), HTML(\\"&bull;\\"),\\n                      \\"Code on\\", a(\\"GitHub\\", href = \\"https://github.com/sestelo/shiny_npregfast/\\")\\n                  )\\n           )\\n  ),\\n  \\n  \\n  \\n  div(id = \\"loading-content\\", h2(\\"Loading...\\")),\\n  \\n  fluidRow(id = \\"app-content\\",\\n           column(2, wellPanel(\\n             class = \\"settings\\",\\n             h4(class = \\"settings-title\\", \\"Estimation\\"),\\n             \\n             selectInput(inputId = \\"type\\", \\n                         label = \\"Factor-by-curve interaction?\\",\\n                         choices = c(\\"Without\\" = \\"without\\", \\"With\\" = \\"with\\")),\\n             \\n             selectInput(inputId = \\"kernel\\",\\n                         label = \\"Choose a kernel:\\",\\n                         choices = c(\\"Epanechnikov\\" = \\"epanech\\", \\n                                     \\"Gaussian\\" = \\"gaussian\\",\\n                                     \\"Triangular\\" = \\"triang\\")),\\n             \\n             selectInput(inputId = \\"poly\\",\\n                         label = \\"Polynomial degree:\\",\\n                         choices = c(1, \\n                                     2,\\n                                     3),\\n                         selected = 3),\\n             \\n             radioButtons(inputId = \\"selband\\",\\n                          label = \\"Bandwidth selection:\\",\\n                          choices = c(\\"Cross-validation\\" = \\"cv\\", \\n                                      \\"Manual\\" = \\"man\\"),\\n                          selected = \\"cv\\"),\\n             \\n             conditionalPanel(\\n               condition = \\"input.selband == \'man\'\\",\\n               sliderInput(inputId = \\"band\\",\\n                           label = \\"Bandwidth selection:\\",\\n                           min = 0,\\n                           max = 1,\\n                           value = 0.5,\\n                           step = 0.1, \\n                           ticks = TRUE,\\n                           animate = TRUE))\\n             \\n           )),\\n           \\n           \\n           \\n           \\n           \\n           column(2, wellPanel(\\n             class = \\"settings\\",\\n             h4(class = \\"settings-title\\", \\"Graphical\\"),\\n             \\n             conditionalPanel(\\n               condition = \\"input.poly == 1\\",\\n               checkboxGroupInput(inputId = \\"der1\\",\\n                                  label = \\"Output:\\",\\n                                  choices = c(\\"Conditional mean\\" = \'0\'),\\n                                  selected = \'0\')),\\n             \\n             conditionalPanel(\\n               condition = \\"input.poly == 2\\",\\n               checkboxGroupInput(inputId = \\"der2\\",\\n                                  label = \\"Output:\\",\\n                                  choices = c(\\"Conditional mean\\" = \'0\', \\n                                              \\"First derivative\\" = \'1\'),\\n                                  selected = \'0\')),\\n             \\n             conditionalPanel(\\n               condition = \\"input.poly == 3\\",\\n               checkboxGroupInput(inputId = \\"der3\\",\\n                                  label = \\"Output:\\",\\n                                  choices = c(\\"Conditional mean\\" = \'0\', \\n                                              \\"First derivative\\" = \'1\',\\n                                              \\"Second derivative\\" = \'2\'),\\n                                  selected = \'0\')),\\n             \\n             \\n             \\n             \\n             div(id = \\"marginal-settings\\",\\n                 shinyjs::colourInput(\\"colmu\\", \\"Line color\\", \\"#D67236\\", \\n                                      showColour = \\"background\\",\\n                                      palette = \\"limited\\",\\n                                      allowedCols = unlist(wes_palettes),\\n                                      allowTransparent = FALSE),\\n                 \\n                 \\n                 shinyjs::colourInput(\\"colci\\", \\"CI color\\", \\"#5B1A18\\", \\n                                      showColour = \\"background\\",\\n                                      palette = \\"limited\\",\\n                                      allowedCols = unlist(wes_palettes),\\n                                      allowTransparent = FALSE)\\n             ),\\n             \\n             conditionalPanel(\\n               condition =\\"input.poly == 1 & input.der1[0] == \'0\'||input.poly == 2 & input.der2[0] == \'0\'||input.poly == 3 & input.der3[0] == \'0\'\\",\\n               checkboxInput(\\"show_points\\", \\"Show data points\\", TRUE),\\n               conditionalPanel(\\n                 condition =\\"input.show_points == true\\",\\n                 shinyjs::colourInput(\\"pcol\\", \\"Points color\\", \\"#899DA4\\", \\n                                      showColour = \\"background\\",\\n                                      palette = \\"limited\\",\\n                                      allowedCols = unlist(wes_palettes),\\n                                      allowTransparent = FALSE)\\n               )\\n             )\\n           )),\\n           \\n           \\n           \\n           column(8,\\n                  plotOutput(\\"distPlot\\", \\n                             height = \\"500px\\", \\n                             width = \\"100%\\",\\n                             click = \\"plot1_click\\",\\n                             brush = brushOpts(id = \\"plot1_brush\\"),\\n                  ),\\n                  \\n                  miniButtonBlock(\\n                   \\n                    actionButton(\\"exclude_toggle\\", \\"Toggle points\\",\\n                                 icon = icon(\\"fa fa-codiepie\\", class = \\"fa-1x\\")),\\n                    \\n                    actionButton(\\"exclude_reset\\", \\"Reset\\", \\n                                 icon = icon(\\"fa fa-refresh\\", class = \\"fa-1x\\")),\\n                    actionButton(inputId =\\"info_btn\\", \\n                                 label = \\"Info\\", \\n                                 icon = icon(\\"fa fa-info-circle\\", class = \\"fa-1x\\"))\\n                    \\n                  )\\n                  \\n                  \\n                 # includeMarkdown(\\"plot_shiny.md\\")\\n                  \\n                  \\n                  \\n           )\\n           \\n  )\\n))\\n\\n\\n\\n\\n\\n\\n\\n\\n" }\n'
line: b'{ "repo_name": "FrissAnalytics/shinyJsTutorials", "ref": "refs/heads/master", "path": "tutorials/materials2/C3/R/C3Pie.R", "content": "#\' <Add Title>\\n#\'\\n#\' <Add Description>\\n#\'\\n#\' @import htmlwidgets\\n#\'\\n#\' @export\\nC3Pie <- function(values, legendPosition = \\"bottom\\", width = NULL, height = NULL) {\\n\\n  # forward options using x\\n  x = list(\\n    values = values,\\n    legendPosition = legendPosition\\n  )\\n\\n  # create widget\\n  htmlwidgets::createWidget(\\n    name = \'C3Pie\',\\n    x,\\n    width = width,\\n    height = height,\\n    package = \'C3\'\\n  )\\n}\\n\\n#\' Shiny bindings for C3Pie\\n#\'\\n#\' Output and render functions for using C3Pie within Shiny\\n#\' applications and interactive Rmd documents.\\n#\'\\n#\' @param outputId output variable to read from\\n#\' @param width,height Must be a valid CSS unit (like \\\\code{\'100\\\\%\'},\\n#\'   \\\\code{\'400px\'}, \\\\code{\'auto\'}) or a number, which will be coerced to a\\n#\'   string and have \\\\code{\'px\'} appended.\\n#\' @param expr An expression that generates a C3Pie\\n#\' @param env The environment in which to evaluate \\\\code{expr}.\\n#\' @param quoted Is \\\\code{expr} a quoted expression (with \\\\code{quote()})? This\\n#\'   is useful if you want to save an expression in a variable.\\n#\'\\n#\' @name C3Pie-shiny\\n#\'\\n#\' @export\\nC3PieOutput <- function(outputId, width = \'100%\', height = \'400px\'){\\n  htmlwidgets::shinyWidgetOutput(outputId, \'C3Pie\', width, height, package = \'C3\')\\n}\\n\\n#\' @rdname C3Pie-shiny\\n#\' @export\\nrenderC3Pie <- function(expr, env = parent.frame(), quoted = FALSE) {\\n  if (!quoted) { expr <- substitute(expr) } # force quoted\\n  htmlwidgets::shinyRenderWidget(expr, C3PieOutput, env, quoted = TRUE)\\n}\\n" }\n'
line: b'{ "repo_name": "FrissAnalytics/shinyJsTutorials", "ref": "refs/heads/master", "path": "tutorials/materials2/C3/R/C3LineBarChart.R", "content": "#\' <Add Title>\\n#\'\\n#\' <Add Description>\\n#\'\\n#\' @import htmlwidgets\\n#\'\\n#\' @export\\nC3LineBarChart <- function(dataset, colors, width = NULL, height = NULL) {\\n\\n  # forward options using x\\n  x = list(\\n    dataset  = dataset,\\n    colors   = colors\\n  )\\n\\n  # create widget\\n  htmlwidgets::createWidget(\\n    name = \'C3LineBarChart\',\\n    x,\\n    width = width,\\n    height = height,\\n    package = \'C3\'\\n  )\\n}\\n\\n#\' Shiny bindings for C3LineBarChart\\n#\'\\n#\' Output and render functions for using C3LineBarChart within Shiny\\n#\' applications and interactive Rmd documents.\\n#\'\\n#\' @param outputId output variable to read from\\n#\' @param width,height Must be a valid CSS unit (like \\\\code{\'100\\\\%\'},\\n#\'   \\\\code{\'400px\'}, \\\\code{\'auto\'}) or a number, which will be coerced to a\\n#\'   string and have \\\\code{\'px\'} appended.\\n#\' @param expr An expression that generates a C3LineBarChart\\n#\' @param env The environment in which to evaluate \\\\code{expr}.\\n#\' @param quoted Is \\\\code{expr} a quoted expression (with \\\\code{quote()})? This\\n#\'   is useful if you want to save an expression in a variable.\\n#\'\\n#\' @name C3LineBarChart-shiny\\n#\'\\n#\' @export\\nC3LineBarChartOutput <- function(outputId, width = \'100%\', height = \'400px\'){\\n  htmlwidgets::shinyWidgetOutput(outputId, \'C3LineBarChart\', width, height, package = \'C3\')\\n}\\n\\n#\' @rdname C3LineBarChart-shiny\\n#\' @export\\nrenderC3LineBarChart <- function(expr, env = parent.frame(), quoted = FALSE) {\\n  if (!quoted) { expr <- substitute(expr) } # force quoted\\n  htmlwidgets::shinyRenderWidget(expr, C3LineBarChartOutput, env, quoted = TRUE)\\n}\\n" }\n'
line: b'{ "repo_name": "swarm-lab/Shiny", "ref": "refs/heads/master", "path": "aggregation_segregation/global.R", "content": "rw <- function(df) {\\n  n <- nrow(df)\\n  \\n  within(df, {\\n    h <- h + rnorm(n, sd = pi / 6)\\n    x <- x + s * cos(h)\\n    y <- y + s * sin(h)\\n    \\n    h[x < -1] <- 0\\n    h[x > 1] <- pi\\n    h[y < -1] <- pi / 2\\n    h[y > 1] <- -pi / 2\\n  })\\n}\\n\\nsigmoid <- function(x, a = 0, k = 1, b = 0.1, m = 100, v = 1, q = 1) {\\n  a + (k - a) / ((1 + q * exp(-b * (x - m))) ^ (1 / v))\\n}\\n\\npdist <- function(A, B) {\\n  an = apply(A, 1, function(rvec) crossprod(rvec,rvec))\\n  bn = apply(B, 1, function(rvec) crossprod(rvec,rvec))\\n  \\n  m = nrow(A)\\n  n = nrow(B)\\n  \\n  tmp = matrix(rep(an, n), nrow = m) \\n  tmp = tmp +  matrix(rep(bn, m), nrow = m, byrow = TRUE)\\n  sqrt( tmp - 2 * tcrossprod(A,B) )\\n}\\n\\nsp <- function(df, affinSame = 0, affinOther = 0) {\\n  dist <- pdist(as.matrix(df[, 1:2]), as.matrix(df[, 1:2]))\\n  neighbor <- dist < 0.125\\n  b_neighbor <- apply(df$col == \\"#107AB6\\" & neighbor, 2, sum) - (df$col == \\"#107AB6\\")\\n  r_neighbor <- apply(df$col == \\"#D86810\\" & neighbor, 2, sum) - (df$col == \\"#D86810\\")\\n  \\n  nb <- (df$col == \\"#107AB6\\") * affinSame * b_neighbor + \\n    (df$col == \\"#D86810\\") * affinSame * r_neighbor + \\n    (df$col == \\"#107AB6\\") * affinOther * r_neighbor + \\n    (df$col == \\"#D86810\\") * affinOther * b_neighbor\\n  \\n  df$s <- sigmoid(nb, k = 0.1, m = 4, b = -1)\\n  df\\n}\\n\\npop <- NULL\\nrun <- FALSE\\n" }\n'
line: b'{ "repo_name": "swarm-lab/Shiny", "ref": "refs/heads/master", "path": "wisdom_of_crowds/model/old/server_old.R", "content": "library(shiny)\\nlibrary(dplyr)\\nlibrary(ggplot2)\\nsource(\\"ibm.R\\")\\n\\nshinyServer(function(input, output, session) {  \\n  \\n  dat <- reactiveValues(tab = NULL)\\n  counter <- reactiveValues(count = -1)\\n  \\n  observe({\\n    if (counter$count != input$goButton) {\\n      counter$count <- input$goButton\\n      \\n      withProgress(message = \\"Simulating 1000 experiments\\", value = 0, {\\n        n <- 1000\\n        \\n        m1 <- replicate(n, woc(n = input$n, \\n                               val = input$val,\\n                               error = input$error,\\n                               soc = 0))\\n        \\n        m2 <- replicate(n, woc(n = input$n, \\n                               val = input$val,\\n                               error = input$error,\\n                               soc = input$soc))\\n        \\n        dat$tab <- data.frame(SOC = as.factor(rep(c(\\"Control   \\", \\"Experimental   \\"), each = n * 2)),\\n                              TYPE = rep(c(\\"mean\\", \\"sd\\", \\"mean\\", \\"sd\\"), each = n),\\n                              VAL = c(apply(m1, 2, mean),\\n                                      apply(m1, 2, sd),\\n                                      apply(m2, 2, mean),\\n                                      apply(m2, 2, sd)))\\n      }) \\n    }\\n  })\\n  \\n  output$IBM.plot1 <- renderPlot({\\n    g <- ggplot(filter(dat$tab, TYPE == \\"mean\\"),\\n                aes(x = VAL, color = SOC)) + \\n      geom_density(size = 1) +\\n      theme_minimal(base_size = 18) + \\n      theme(legend.position = \\"top\\", legend.title = element_blank()) +\\n      xlab(\\"Group average\\") + ylab(\\"Density\\") +\\n      scale_color_manual(values = c(\\"tomato3\\", \\"dodgerblue3\\"))\\n    \\n    print(g)\\n  })\\n  \\n  output$IBM.plot2 <- renderPlot({\\n    g <- ggplot(filter(dat$tab, TYPE == \\"sd\\"),\\n                aes(x = VAL, color = SOC)) + \\n      geom_density(size = 1) +\\n      theme_minimal(base_size = 18) + \\n      theme(legend.position = \\"top\\", legend.title = element_blank()) +\\n      xlab(\\"Group standard deviation\\") + ylab(\\"Density\\") +\\n      scale_color_manual(values = c(\\"tomato3\\", \\"dodgerblue3\\"))\\n    \\n    print(g)\\n  })\\n})\\n" }\n'
line: b'{ "repo_name": "swarm-lab/Shiny", "ref": "refs/heads/master", "path": "wisdom_of_crowds/model/server.R", "content": "shinyServer(function(input, output, session) {\\n  \\n  react <- reactiveValues(tab = {}, count = -1)\\n  \\n  observe({\\n    if (react$count != input$goButton) {\\n      react$count <- input$goButton\\n      \\n      withProgress(message = \\"Simulating 1000 experiments\\", {\\n        m1 <- replicate(N, woc(n = input$n, \\n                               val = 200,\\n                               error = input$error / 100,\\n                               soc = 0))\\n        tmp1 <- abs(m1 - 200) \\n        tmp2 <- abs(matrix(apply(m1, 2, mean), nrow = input$n, ncol = 1000, byrow = TRUE) - 200)\\n        r1 <- apply(tmp2 < tmp1, 2, sum)\\n        \\n        m2 <- replicate(N, woc(n = input$n, \\n                               val = 200,\\n                               error = input$error / 100,\\n                               soc = input$soc))\\n        tmp1 <- abs(m2 - 200) \\n        tmp2 <- abs(matrix(apply(m2, 2, mean), nrow = input$n, ncol = 1000, byrow = TRUE) - 200)\\n        r2 <- apply(tmp2 < tmp1, 2, sum)\\n        \\n        react$tab <- data.frame(\\n          SOC = as.factor(rep(c(\\"Control   \\", \\"Experimental   \\"), each = N * 2)),\\n          TYPE = rep(c(\\"mean\\", \\"sd\\", \\"mean\\", \\"sd\\"), each = N),\\n          VAL = c(100 * (r1 / input$n),\\n                  apply(m1, 2, sd),\\n                  100 * (r2 / input$n),\\n                  apply(m2, 2, sd)))\\n      }) \\n    }\\n  })\\n  \\n  output$IBM.plot1 <- renderPlot({\\n    g <- ggplot(filter(react$tab, TYPE == \\"mean\\"),\\n                aes(x = VAL, color = SOC, fill = SOC)) + \\n      geom_histogram(position = \\"identity\\", bins = 40) +\\n      geom_vline(xintercept = 50, linetype = 2) +\\n      xlim(0, 100) +\\n      theme_minimal(base_size = 16) + \\n      theme(legend.position = \\"top\\", legend.title = element_blank()) +\\n      xlab(\\"Average > x% of group members\\") + ylab(\\"Density\\") +\\n      scale_color_manual(values = c(\\"tomato3\\", \\"dodgerblue3\\")) + \\n      scale_fill_manual(values = alpha(c(\\"tomato3\\", \\"dodgerblue3\\"), 0.25))\\n    \\n    print(g)\\n  })\\n  \\n  output$IBM.plot2 <- renderPlot({\\n    g <- ggplot(filter(react$tab, TYPE == \\"sd\\"),\\n                aes(x = VAL, color = SOC, fill = SOC)) + \\n      geom_histogram(position = \\"identity\\", bins = 40) +\\n      theme_minimal(base_size = 16) + \\n      theme(legend.position = \\"top\\", legend.title = element_blank()) +\\n      xlab(\\"Group standard deviation\\") + ylab(\\"Density\\") +\\n      scale_color_manual(values = c(\\"tomato3\\", \\"dodgerblue3\\")) + \\n      scale_fill_manual(values = alpha(c(\\"tomato3\\", \\"dodgerblue3\\"), 0.25))\\n    \\n    print(g)\\n  })\\n})\\n" }\n'
line: b'{ "repo_name": "swarm-lab/Shiny", "ref": "refs/heads/master", "path": "opinion_dynamic/global.R", "content": "grid_sys <- function(time, init, parms) {\\n  o1 <- init\\n  o1[o1 < 0] <- 0\\n  \\n  o2 <- init\\n  o2[o2 > 0] <- 0\\n  \\n  p1 <- eightneighbors(o1) / 8\\n  p2 <- p1 - eightneighbors(o2) / 8\\n  \\n  r <- runif(nrow(init) * ncol(init))\\n  \\n  idx1 <- r <= p1\\n  idx2 <- r <= p2 & r > p1\\n  \\n  init[idx1] <- 1 * parms$w1\\n  init[idx2] <- -1 * parms$w2\\n  \\n  init\\n}\\n\\n\\n\\n\\n\\n\\n" }\n'
line: b'{ "repo_name": "swarm-lab/Shiny", "ref": "refs/heads/master", "path": "wisdom_of_crowds/experiment/panels/panel3.R", "content": "panel3 <- bsCollapsePanelNoHead(\\n  title = \\"NULL\\", id = \\"col3\\", value = \\"col3\\",\\n\\n  div(class = \\"button\\",\\n      uiOutput(\\"plot_title\\"),\\n      \\n      plotOutput(\\"dots\\", width = \\"400px\\", inline = TRUE),\\n      \\n      h4(textOutput(\\"time_left\\"))\\n  )\\n)\\n" }\n'
line: b'{ "repo_name": "bryantrobbins/baseball", "ref": "refs/heads/master", "path": "worker/build/staging/generic.R", "content": "library(jsonlite)\\nlibrary(plyr)\\nquery <- fromJSON(\'config.json\')\\n\\ntableName <- query$table\\nload(paste(tableName,\'Rdata\', sep = \'.\'))\\nmy.table <- eval(parse(text = tableName))\\n\\nfilterv <- rep(TRUE, nrow(my.table))\\nif (\'filter\' %in% names(query$metadata)) {\\n\\tfilters <- query$metadata$filter\\n\\tfor (i in 1:length(filters)){\\n\\t\\tif(!is.na(filters[i,1])) {\\n\\t\\t\\tfilterv <- filterv & grepl(filters[i,1], my.table[[query$metadata$colName[i]]]) \\n\\t\\t}\\n\\t\\tif (!is.na(filters[i,2])) {\\n\\t\\t\\tfilterv <- filterv & my.table[[query$metadata$colName[i]]] > filters[i,2]\\n\\t\\t}\\n\\t\\tif (!is.na(filters[i,3])) {\\n\\t\\t\\tfilterv <- filterv & my.table[[query$metadata$colName[i]]] < filters[i,3]\\t\\n\\t\\t}\\n\\t}\\n}\\n\\nout.tab <- my.table[query$metadata$colName][which(filterv),]\\n\\nif (\'fields\' %in% names(query$export)) {\\n\\tdecr <- query$export$fields[[1]]$value[2] == \'desc\' \\n\\tout.tab <- out.tab[order(out.tab[[query$export$fields[[1]]$value[1]]], decreasing = decr),]\\n}\\n\\n\\n#fix POS variable\\nif(\'POS\' %in% colnames(out.tab)) {\\n\\tref.v <- c(\'P\' = 1, \'C\' = 2, \'1B\' = 3, \'2B\' = 4, \'3B\' = 5, \'SS\' = 6, \'LF\' = 7, \'CF\' = 8, \'RF\' = 9, \'OF\' = 10, \'DH\' = 11)\\n\\tout.tab$POS <- names(ref.v)[out.tab$POS]\\n}\\nwrite.csv(out.tab, file = \'output.csv\')\\n" }\n'
line: b'{ "repo_name": "broadinstitute/gatk", "ref": "refs/heads/master", "path": "src/main/resources/org/broadinstitute/hellbender/tools/picard/analysis/qualityScoreDistribution.R", "content": "# Script to generate a chart of quality score distribution in a file\\n# @author Tim Fennell\\n\\n# Parse the arguments\\nargs <- commandArgs(trailing=T)\\nmetricsFile  <- args[1]\\noutputFile   <- args[2]\\nbamFile  <- args[3]\\nsubtitle <- ifelse(length(args) < 4, \\"\\", args[4])\\n\\n# Figure out where the metrics and the histogram are in the file and parse them out\\nstartFinder <- scan(metricsFile, what=\\"character\\", sep=\\"\\\\n\\", quiet=TRUE, blank.lines.skip=FALSE)\\n\\nfirstBlankLine=0\\n\\nfor (i in 1:length(startFinder))\\n{\\n        if (startFinder[i] == \\"\\") {\\n                if (firstBlankLine==0) {\\n                        firstBlankLine=i+1\\n                } else {\\n                        secondBlankLine=i+1\\n                        break\\n                }\\n        }\\n}\\n\\nmetrics <- read.table(metricsFile, header=T, nrows=1, sep=\\"\\\\t\\", skip=firstBlankLine)\\nhistogram <- read.table(metricsFile, header=T, sep=\\"\\\\t\\", skip=secondBlankLine)\\n\\n# Then plot the histogram as a PDF\\npdf(outputFile)\\n\\nplot(histogram$QUALITY,\\n     histogram$COUNT_OF_Q,\\n     type=\\"n\\",\\n     main=paste(\\"Quality Score Distribution\\\\nin file \\",bamFile,\\" \\",ifelse(subtitle == \\"\\",\\"\\",paste(\\"(\\",subtitle,\\")\\",sep=\\"\\")),sep=\\"\\"),\\n     xlab=\\"Quality Score\\",\\n     ylab=\\"Observations\\")\\n\\nqColor  <- \\"blue\\"\\noqColor <- \\"lightcyan2\\"\\nwidth <- 5\\n\\n# Plot OQ first so that it\'s \\"behind\\" the regular qualities\\nif (!is.null(histogram$COUNT_OF_OQ)) {\\n    lines(histogram$QUALITY+0.25, histogram$COUNT_OF_OQ, type=\\"h\\", col=oqColor, lty=1, lwd=width, lend=\\"square\\");\\n}\\n\\n# Then plot the regular qualities\\nlines(histogram$QUALITY, histogram$COUNT_OF_Q, type=\\"h\\", col=qColor, lty=1, lwd=width, lend=\\"square\\");\\n\\n# And add a legend\\nlegend(\\"topleft\\", pch=c(15,15), legend=c(\\"Quality Scores\\", \\"Original Quality Scores\\"), col=c(qColor, oqColor))\\n\\ndev.off()\\n\\n" }\n'
line: b'{ "repo_name": "RevolutionAnalytics/miniCRAN", "ref": "refs/heads/master", "path": "inst/doc/miniCRAN-non-CRAN-repos.R", "content": "## ----setup---------------------------------------------------------------\\n# Wrapper around available.packages ---------------------------------------\\n \\nindex <- function(url, type=\\"source\\", filters=NULL, head=5, cols=c(\\"Package\\", \\"Version\\")){\\n  contribUrl <- contrib.url(url, type=type)\\n  p <- available.packages(contribUrl, type=type, filters=filters)\\n  p[1:head, cols]\\n}\\n \\n\\n## ----CRAN, eval=FALSE----------------------------------------------------\\n#  CRAN <- \\"http://cran.r-project.org\\"\\n#  index(CRAN)\\n\\n## ----revo, eval=FALSE----------------------------------------------------\\n#  revoStable <- \\"http://packages.revolutionanalytics.com/cran/3.1/stable\\"\\n#  index(revoStable)\\n#  \\n#  revoMirror <- \\"http://cran.revolutionanalytics.com\\"\\n#  index(revoMirror)\\n\\n## ----rforge, eval=FALSE--------------------------------------------------\\n#  rforge <- \\"http://r-forge.r-project.org\\"\\n#  index(rforge)\\n\\n## ----bioc, eval=FALSE----------------------------------------------------\\n#  bioc <- local({\\n#    env <- new.env()\\n#    on.exit(rm(env))\\n#    evalq(source(\\"http://bioconductor.org/biocLite.R\\", local=TRUE), env)\\n#    biocinstallRepos()\\n#  })\\n#  \\n#  bioc\\n#  bioc[grep(\\"BioC\\", names(bioc))]\\n#  \\n#  \\n#  index(bioc[\\"BioCsoft\\"])\\n\\n" }\n'
line: b'{ "repo_name": "RevolutionAnalytics/miniCRAN", "ref": "refs/heads/master", "path": "inst/examples/example_pkgDep.R", "content": "\\n\\\\dontrun{\\npkgDep(pkg = c(\\"ggplot2\\", \\"plyr\\", \\"reshape2\\"), \\n       repos = c(CRAN = \\"http://mran.microsoft.com\\")\\n)\\n}\\n\\npdb <- cranJuly2014\\n\\\\dontrun{\\npdb <- pkgAvail(repos = c(CRAN = \\"http://mran.microsoft.com\\"))\\n}\\n\\npkgDep(pkg=c(\\"ggplot2\\", \\"plyr\\", \\"reshape2\\"), pdb)\\n\\n" }\n'
line: b'{ "repo_name": "RevolutionAnalytics/miniCRAN", "ref": "refs/heads/master", "path": "inst/examples/example_plot.pkgDepGraph.R", "content": "tags <- \\"chron\\"\\n\\n# Plot using defaults\\npdb <- cranJuly2014\\n\\n\\\\dontrun{\\n  pdb <- pkgAvail(\\n    repos = c(CRAN = \\"http://mran.microsoft.com\\"),\\n    type=\\"source\\"\\n  )\\n}\\n\\ndg <- makeDepGraph(tags, availPkgs = pdb  , includeBasePkgs=FALSE, suggests=TRUE, enhances=TRUE)\\n\\nset.seed(42); \\nplot(dg)\\n\\n# Move edge legend to top left\\nset.seed(42); \\nplot(dg, legendPosition=c(-1, 1))\\n\\n# Change font size and shape size\\nset.seed(42); \\nplot(dg, legendPosition=c(-1, 1), vertex.size=20,  cex=0.5)\\n\\n\\n# Move vertex legend to top right\\nset.seed(42); \\nplot(dg, legendPosition=c(1, 1), vertex.size=20,  cex=0.5)\\n\\n" }\n'
line: b'{ "repo_name": "RevolutionAnalytics/miniCRAN", "ref": "refs/heads/master", "path": "R/pkgDepTools.R", "content": "# Code copied from the pkgDepTools project\\n# Copyright (C) Seth Falcon\\n# http://www.bioconductor.org/packages/release/bioc/html/pkgDepTools.html\\n# \\n# This program is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU General Public License version 2\\n# as published by the Free Software Foundation\\n\\n\\n\\n# Code copied from the pkgDepTools project\\n# Copyright (C) Seth Falcon\\n# http://www.bioconductor.org/packages/release/bioc/html/pkgDepTools.html\\n\\n\\n# Copy of tools:::split_op_version.\\n\\n# @rdname pkgDepTools\\n# @keywords internal\\nsplit_op_version <- function (x) {\\n  pat <- \\"^([^\\\\\\\\([:space:]]+)[[:space:]]*\\\\\\\\(([^\\\\\\\\)]+)\\\\\\\\).*\\"\\n  x1 <- sub(pat, \\"\\\\\\\\1\\", x)\\n  x2 <- sub(pat, \\"\\\\\\\\2\\", x)\\n  if (x2 != x1) {\\n    pat <- \\"[[:space:]]*([[<>=!]+)[[:space:]]+(.*)\\"\\n    version <- sub(pat, \\"\\\\\\\\2\\", x2)\\n    if (!grepl(\\"^r\\", version)) \\n      version <- package_version(version)\\n    list(name = x1, op = sub(pat, \\"\\\\\\\\1\\", x2), version = version)\\n  }\\n  else list(name = x1)\\n}\\n\\n\\n# Copy of tools:::.split_dependencies.\\n\\n# @rdname pkgDepTools\\n# @keywords internal\\nsplit_dependencies <- function (x) {\\n  if (!length(x)) \\n    return(list())\\n  x <- unlist(strsplit(x, \\",\\"))\\n  x <- sub(\\"[[:space:]]+$\\", \\"\\", x)\\n  x <- unique(sub(\\"^[[:space:]]*(.*)\\", \\"\\\\\\\\1\\", x))\\n  names(x) <- sub(\\"^([[:alnum:].]+).*$\\", \\"\\\\\\\\1\\", x)\\n  lapply(x, split_op_version)\\n}\\n\\n\\n# Clean package fields.\\n# \\n# Given the value from a field like \'Depends\' in a package\'s DESCRIPTION file, return a character vector of package names with the version restrictions stripped and \\\\R~removed.\\n# @param val Value from a field like \'Depends\' in a package\'s DESCRIPTION file\\n# @rdname pkgDepTools\\n# @keywords internal\\ncleanPkgField <- function(val) {\\n  if (is.na(val))\\n    return(character(0))\\n  val <- names(split_dependencies(val))\\n  if (is.null(val))\\n    return(character(0))\\n  val <- val[! val %in% \\"R\\"]\\n  if (length(val))\\n    return(val)\\n  return(character(0))\\n}\\n" }\n'
line: b'{ "repo_name": "RevolutionAnalytics/miniCRAN", "ref": "refs/heads/master", "path": "tests/testthat/test-3-makeRepo.R", "content": "if (interactive()) {library(testthat); Sys.setenv(NOT_CRAN = \\"true\\")}\\n\\ncontext(\\"makeRepo\\")\\n\\nrevolution <- MRAN(\\"2014-10-15\\")\\nif(!miniCRAN:::is.online(revolution, tryHttp = FALSE)) {\\n  # Use http:// for older versions of R\\n  revolution <- sub(\\"^https://\\", \\"http://\\", revolution)\\n}\\nrvers = \\"3.2\\"\\npkgs <- c(\\"Bmix\\")\\nrepo_root <- file.path(tempdir(), \\"miniCRAN\\", Sys.Date())\\nif (file.exists(repo_root)) unlink(repo_root, recursive = TRUE)\\n\\n# list.files(repo_root, recursive = TRUE)\\n\\n\\ntypes <- c(\\"source\\", \\"win.binary\\", \\"mac.binary\\", \\"mac.binary.mavericks\\")\\nnames(types) <- c(\\"source\\", \\"win.binary\\", \\"mac.binary\\", \\"mac.binary\\")\\n\\nfor (pkg_type in names(types)) {\\n  test_that(sprintf(\\"makeRepo downloads %s files and builds PACKAGES file\\", pkg_type), {\\n    skip_on_cran()\\n    skip_if_offline()\\n\\n    pdb <- pkgAvail(repos = revolution, type = pkg_type, Rversion = rvers)\\n    pkgList <- pkgDep(pkgs, availPkgs = pdb, repos = revolution, type = pkg_type,\\n                      suggests = FALSE, Rversion = rvers)\\n    prefix <- miniCRAN:::repoPrefix(pkg_type, Rversion = rvers)\\n    dir.create(repo_root, recursive = TRUE, showWarnings = FALSE)\\n\\n    ret <- makeRepo(pkgList, path = repo_root, repos = revolution, \\n             type = pkg_type, quiet = TRUE, Rversion = rvers)\\n    \\n    expect_is(ret, \\"character\\")\\n    expect_equal(length(ret), length(pkgList))\\n\\n    expect_true(\\n      miniCRAN:::.checkForRepoFiles(repo_root, pkgList, prefix)\\n    )\\n    expect_true(\\n      file.exists(file.path(repo_root, prefix, \\"PACKAGES.gz\\"))\\n    )\\n    expect_true(\\n      all(\\n        pkgList %in% pkgAvail(repos = repo_root, type = pkg_type, Rversion = rvers)[, \\"Package\\"]\\n      )\\n    )\\n  })\\n}\\n\\nunlink(repo_root, recursive = TRUE)\\n" }\n'
line: b'{ "repo_name": "RevolutionAnalytics/miniCRAN", "ref": "refs/heads/master", "path": "tests/testthat/test-2-makeDepGraph.R", "content": "checkPkgDepFunctions <- function(pkg, availPkgs = cranJuly2014, \\n                                 repos = MRAN(), \\n                                 type=\\"source\\", \\n                                 suggests=TRUE, \\n                                 enhances=FALSE, \\n                                 includeBasePkgs=FALSE){\\n  \\n  if(!require(igraph, quietly = TRUE)){\\n    skip(\\"package igraph not installed\\")\\n  }\\n  p1 <- pkgDep(pkg, availPkgs=availPkgs, \\n               repos=repos, type=type, \\n               suggests=suggests, enhances=enhances, \\n               includeBasePkgs=includeBasePkgs)\\n  p2 <- makeDepGraph(pkg, availPkgs=availPkgs, \\n                     repos=repos, type=type, \\n                     suggests=suggests, enhances=enhances, \\n                     includeBasePkgs=includeBasePkgs)\\n\\n  vnames <- V(p2)$name\\n  diff1 <- setdiff(vnames, p1)\\n  diff2 <- setdiff(p1, vnames)\\n  result <- length(diff1) == 0 & length(diff2) == 0\\n  if(!result) {\\n    msg <- paste0(\\"\\\\nmakeDepGraph() results not in pkgDep(): \\\\n - \\", paste(diff1, collapse=\\", \\"),\\n                  \\"\\\\npkgDep() results not in makeDepGraph(): \\\\n - \\", paste(diff2, collapse=\\", \\"))\\n    \\n    warning(msg)\\n  }\\n  result\\n}\\n\\n\\ncontext(\\"makeDepGraph \\")\\n\\nmock_require <- function(pkg, ...){\\n  packages.to.exclude <- c(\\"igraph\\")\\n  inSearchPath <- any(\\n    grepl(sprintf(\\"package:%s$\\", paste(packages.to.exclude, collapse = \\"|\\")), search())\\n  )\\n  if(inSearchPath) stop(\\"Required package already in search path\\")\\n  \\n  package <- as.character(substitute(pkg))\\n  if(package %in% packages.to.exclude)\\n    FALSE \\n  else \\n    base::requireNamespace(package, character.only = TRUE, ...)\\n}\\n\\n\\ntest_that(\\"throws error if igraph not available\\", {\\n  skip_if_offline()\\n  with_mock(\\n    `base::requireNamespace` = function(pkg, ...){\\n      packages.to.exclude <- c(\\"igraph\\")\\n      inSearchPath <- any(\\n        grepl(sprintf(\\"package:%s$\\", paste(packages.to.exclude, collapse = \\"|\\")), search())\\n      )\\n      if(inSearchPath) stop(\\"Required package already in search path\\")\\n      \\n      package <- as.character(substitute(pkg))\\n      if(package %in% packages.to.exclude)\\n        FALSE \\n      else \\n        base::requireNamespace(package, character.only = TRUE, ...)\\n    }, \\n{\\n  expect_false(requireNamespace(\\"igraph\\"))\\n  \\n  tag <- \\"MASS\\"\\n  \\n  expect_error(\\n    makeDepGraph(tag, availPkgs=cranJuly2014)\\n  )\\n  \\n})\\n\\n})\\n\\ntest_that(\\"makeDepGraph and pgkDep gives similar results for MASS\\", {\\n  skip_if_offline()\\n\\n  tag <- \\"MASS\\"\\n  \\n  expect_true(\\n    checkPkgDepFunctions(tag)\\n  )\\n  \\n  skip_on_cran()\\n  \\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE)\\n  )\\n  \\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, suggests=FALSE)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, enhances=TRUE)\\n  )\\n  \\n})\\n\\n\\ntest_that(\\"makeDepGraph and pgkDep gives similar results for chron\\", {\\n  \\n  skip_on_cran()\\n  \\n  tag <- \\"chron\\"\\n  \\n  expect_true(\\n    checkPkgDepFunctions(tag)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, suggests=FALSE)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, enhances=TRUE)\\n  )\\n  \\n})\\n\\n\\ntest_that(\\"makeDepGraph and pgkDep gives similar results for data.table\\", {\\n  \\n  skip_on_cran()\\n  \\n  tag <- \\"data.table\\"\\n  \\n  expect_true(\\n    checkPkgDepFunctions(tag)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, suggests=FALSE)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, enhances=TRUE)\\n  )\\n  \\n})\\n\\ntest_that(\\"makeDepGraph and pgkDep gives similar results for ggplot2\\", {\\n  \\n  skip_on_cran()\\n  \\n  tag <- \\"ggplot2\\"\\n  \\n  expect_true(\\n    checkPkgDepFunctions(tag)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, suggests=FALSE)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, enhances=TRUE)\\n  )\\n  \\n})\\n\\n\\ntest_that(\\"makeDepGraph and pgkDep gives similar results for complex query\\", {\\n  \\n  skip_on_cran()\\n  \\n  tag <- c(\\"ggplot2\\", \\"data.table\\", \\"plyr\\", \\"knitr\\", \\"shiny\\", \\"xts\\", \\"lattice\\")\\n  \\n  expect_true(\\n    checkPkgDepFunctions(tag)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, suggests=FALSE)\\n  )\\n  expect_true(\\n    checkPkgDepFunctions(tag, includeBasePkgs = TRUE, enhances=TRUE)\\n  )\\n  \\n})\\n\\n" }\n'
line: b'{ "repo_name": "WinVector/CampaignPlanner", "ref": "refs/heads/master", "path": "server.R", "content": "\\n# This is the server logic for a Shiny web application.\\n# You can find out more about building applications with Shiny here:\\n#\\n# http://shiny.rstudio.com\\n#\\n\\nlibrary(\'shiny\')\\nlibrary(\'ggplot2\')\\nsource(\\"functions.R\\")\\n\\n\\nmakeTypicalTable = function(planTable, sizes, dummy) {\\n  dummy  # cause dummy promise to be evaluated (and trigger recalc)\\n  ptab = typicalTable(planTable, sizes)\\n  if(sum(sizes) > 0) {\\n    ptab$expectedSuccessRate = with(ptab, (Successes+0.5)/(Actions+1)) # posteriors from Jeffrey prior\\n    ptab$expectedValuePerAction = ptab$expectedSuccessRate*planTable$ValueSuccess\\n  }\\n  ptab\\n}\\n\\nlabeledPlan = function(sizes,rates,values,countGoal) {\\n  data.frame(Label=paste(\\"Campaign_\\", seq_len(length(sizes)), sep=\'\'),\\n             ActionsToMeetErrorGoals=sizes,\\n             ActionsToMeetCountGoals=ceiling(countGoal/rates),\\n             MatchingRates=c(rates[2]*values[2]/values[1],rates[1]*values[1]/values[2]))\\n}\\n\\ndisplayGraph = function(pgraph, doplot) {\\n  if(doplot) {\\n    plotSample(pgraph)\\n  } else NULL\\n}\\n\\n# ---------------------------------\\n\\nassembleResultTable = function(actions, successes, values, wishPrice) {\\n  ptab = data.frame(Label=paste(\\"Campaign_\\", seq_len(length(successes)), sep=\'\'),\\n             Actions=actions,\\n             Successes=successes,\\n             ValueSuccess=values)\\n  ptab$observedSuccessRate = (successes+0.5)/(actions+1) #  posterior Jeffreys prior 0.5,0.5 smoothing\\n  ptab$observedValuePerAction = ptab$observedSuccessRate*values\\n  ptab$pAboveWishPrice = pbeta(wishPrice/values,\\n                               shape1=0.5+successes,\\n                               shape2=0.5+actions-successes,\\n                               lower.tail=FALSE)\\n  ptab\\n}\\n\\nshinyServer(function(input, output) {\\n   #\\n   # for planning campaign\\n   #\\n   cprobabilities = reactive(c(input$conv1a, input$conv2a))\\n   values = reactive(c(input$value1a, input$value2a))\\n   sizes = reactive(c(input$sizes1a, input$sizes2a))\\n   proposedsizes = reactive(heuristicPowerPlan(data.frame(Probability=cprobabilities(), \\n                                                          ValueSuccess=values()), \\n                                       errorProbability=input$errorProb,relativeError=input$relErr)) \\n   countGoalV <- reactive(input$countGoal)\\n   \\n  docalc = reactive(sum(sizes()) != 0)\\n\\n  planTable = reactive(data.frame(Label=c(\'Campaign1\',\'Campaign2\'),\\n                                  Probability=cprobabilities(), \\n                                  ValueSuccess=values()))\\n  typicalTable = reactive(makeTypicalTable(planTable(), sizes(), input$reseed))\\n  pgraph2T = reactive(posteriorGraph(typicalTable()))\\n  output$planGraph2T = renderPlot(plotPosterior(pgraph2T()))\\n  output$probTable2T = renderPrint(computeProbsGEP(typicalTable(),pgraph2T()$graph))\\n  \\n  pgraph = reactive(sampleGraph(planTable(),sizes()))\\n  bgraph = reactive(computeProbsGES(planTable(),pgraph()))\\n\\n  output$plan = renderTable(labeledPlan(proposedsizes(),cprobabilities(),values(),countGoalV()),digits=4)\\n  output$typicalTable = renderPrint(typicalTable()) # I\'ll render it verbatim, rather than as a table.\\n                                               # Saves me from having to worry about sig figs\\n  output$planGraph = renderPlot(displayGraph(pgraph(), docalc()))\\n  output$probTable = renderPrint(bgraph())\\n\\n\\n  #\\n  # for evaluating campaign\\n  #\\n  actions = reactive(c(input$actions1b, input$actions2b))\\n  successes = reactive(c(input$success1b, input$success2b))\\n  svalues = reactive(c(input$value1b, input$value2b))\\n\\n  resTable = reactive(assembleResultTable(round(input$rescale*actions()), \\n                                          round(input$rescale*successes()), \\n                                          svalues(),\\n                                          input$wishPrice))\\n  pgraph2 = reactive(posteriorGraph(resTable()))\\n\\n  output$resTable = renderPrint(resTable())\\n  output$planGraph2 = renderPlot(plotPosterior(pgraph2(),input$wishPrice))\\n  output$probTable2 = renderPrint(computeProbsGEP(resTable(),pgraph2()$graph))\\n\\n})\\n" }\n'
line: b'{ "repo_name": "Reproducible-Science-Curriculum/rr-organization1", "ref": "refs/heads/master", "path": "files/file-org/forensic-science/rev1_final_analysis.R", "content": "data <- read.csv(file = \\"data1_full.csv\\", header = T)\\n\\nsub_data <- data[data$country == \\"Canada\\", ]\\n\\nwrite.csv(Canada, file = \\"/Users/csoderbe/rr-organization1/files/Canada.csv\\", row.names = FALSE)\\n\\nlibrary(ggplot2)\\nggplot(data = sub_data, aes(x = year, y = lifeExp)) +\\n  geom_point() +\\n  geom_line() \\nggsave(\\"graph.png\\")\\n\\n\\n\\n" }\n'
line: b'{ "repo_name": "Shians/Glimma", "ref": "refs/heads/master", "path": "R/gllink.R", "content": "#\' Plot linkages\\n#\' \\n#\' Helper function for writing the link properties in interactive Glimma plots\\n#\' \\n#\' @param from the index of the plot from which the event is dispatched.\\n#\' @param to the index of the plot which receives the event and performs an action.\\n#\' @param src the action that is performed in the \\"from\\" plot.\\n#\' @param dest the action that is performed in the \\"to\\" plot.\\n#\' @param flag indicates special links for particular chart types.\\n#\' @param both creates symmetric links whereby the \\"dest\\" action in \\"to\\" also triggers the \\"src\\" action in \\"from\\".\\n#\' @param info additional info for creating the link.\\n#\' \\n#\' @return a link object containing the plot linking information.\\n#\' \\n#\' @examples\\n#\' data(iris)\\n#\' data <- data.frame(Name=paste(\\"Flower\\", 1:nrow(iris), sep=\\"-\\"), iris)\\n#\' \\\\donttest{\\n#\' plot1 <- glScatter(data, xval=\\"Sepal.Length\\", yval=\\"Sepal.Width\\", colval=\\"Species\\")\\n#\' plot2 <- glScatter(data, xval=\\"Species\\", yval=\\"Petal.Length\\", colval=\\"Species\\")\\n#\' link1 <- gllink(1, 2, src=\\"hover\\", dest=\\"hover\\", both=TRUE)\\n#\' glimma(plot1, plot2, link1, layout=c(1,2))\\n#\' }\\n\\ngllink <- function(from, to, src=\\"none\\", dest=\\"none\\", flag=\\"none\\", both=FALSE, info=\\"none\\") {\\n    out <- list()\\n\\n    if (src != \\"none\\" && dest == \\"none\\") {\\n        stop(\\"src cannot be defined while dest is \'none\'\\")\\n    }\\n\\n    if (src == \\"none\\" && dest != \\"none\\") {\\n        stop(\\"dest cannot be defined while src is \'none\'\\")\\n    }   \\n\\n    if (src == \\"none\\" && dest == \\"none\\" && flag == \\"none\\") {\\n        stop(\\"\'src\', \'dest\' and \'flag\' cannot simultaneously be \'none\'\\")\\n    }\\n\\n    out$link <- data.frame(from=from, to=to, src=src, dest=dest, flag=flag, info=info)\\n    if (both) {\\n        out$link <- rbind(out$link, data.frame(from=to, to=from, src=dest, dest=src, flag=flag, info=info))\\n    }\\n\\n    out$type <- \\"link\\"\\n\\n    class(out) <- \\"jslink\\"\\n    return(out)\\n}\\n" }\n'
line: b'{ "repo_name": "Shians/Glimma", "ref": "refs/heads/master", "path": "R/glMDSPlot.R", "content": "#\' Glimma MDS Plot\\n#\'\\n#\' Draw an interactive MDS plot gene expression matrix with distances calculated from most variable genes.\\n#\'\\n#\' @author Shian Su, Gordon Smyth\\n#\'\\n#\' @param x the data.frame containing data to plot.\\n#\' @param ... additional arguments affecting the plots produced. See specific methods for detailed arguments.\\n#\'\\n#\' @return Draws a two-panel interactive MDS plot in an html page. The left panel contains the plot between two MDS dimensions, with annotations displayed on hover. The right panel contains a bar plot of the eigenvalues of each dimension, clicking on any of the bars will plot the corresponding dimension against the next dimension.\\n#\'\\n#\' @seealso \\\\code{\\\\link{glMDSPlot.default}}, \\\\code{\\\\link{glMDSPlot.DGEList}}\\n#\'\\n#\' @examples\\n#\' data(lymphomaRNAseq)\\n#\' genotype <- relevel(lymphomaRNAseq$samples$group, \\"Smchd1-null\\")\\n#\' \\\\donttest{\\n#\' glMDSPlot(lymphomaRNAseq, labels=1:7, groups=genotype)\\n#\' }\\n#\'\\n#\' @export\\n\\nglMDSPlot <- function(x, ...) {\\n    UseMethod(\\"glMDSPlot\\")\\n}\\n\\n#\' Glimma MDS Plot\\n#\'\\n#\' Draw an interactive MDS plot from a gene expression matrix with distances calculated from most variable genes.\\n#\'\\n#\' @author Shian Su, Gordon Smyth\\n#\'\\n#\' @param x the matrix containing the gene expressions.\\n#\' @param top the number of top most variable genes to use.\\n#\' @param labels the labels for each sample.\\n#\' @param groups the experimental group to which samples belong.\\n#\' @param gene.selection \\"pairwise\\" if most variable genes are to be chosen for each pair of samples or \\"common\\" to select the same genes for all comparisons.\\n#\' @param main the title of the plot.\\n#\' @param path the path in which the folder will be created.\\n#\' @param folder the name of the fold to save html file to.\\n#\' @param html the name of the html file to save plots to.\\n#\' @param launch TRUE to launch plot after call.\\n#\' @param ... additional arguments.\\n#\'\\n#\' @return Draws a two-panel interactive MDS plot in an html page. The left panel contains the plot between two MDS dimensions, with annotations displayed on hover. The right panel contains a bar plot of the eigenvalues of each dimension, clicking on any of the bars will plot the corresponding dimension against the next dimension.\\n#\'\\n#\' @method glMDSPlot default\\n#\'\\n#\' @importFrom stats cmdscale as.dist\\n#\'\\n#\' @export\\n\\n# Code taken from plotMDS of limma bioConductor package with alterations\\nglMDSPlot.default <- function(x, top=500, labels=1:ncol(x),\\n                            groups=rep(1, ncol(x)), gene.selection=\\"pairwise\\",\\n                            main=\\"MDS Plot\\", path=getwd(),\\n                            folder=\\"glimma-plots\\", html=\\"MDS-Plot\\",\\n                            launch=TRUE, ...) {\\n    #   Multi-dimensional scaling with top-distance\\n    #   Di Wu and Gordon Smyth\\n    #   19 March 2009.  Last modified 14 Jan 2015\\n    #   Modified by Shian Su on 25 Jan 2016\\n\\n    ##\\n    # Check Inputs\\n\\n    x <- as.matrix(x)\\n    nsamples <- ncol(x)\\n    ndim <- nsamples - 1\\n\\n    if (nsamples < 3) {\\n        stop(paste(\\"Only\\", nsamples, \\"columns of data: need at least 3\\"))\\n    }\\n\\n    cn <- colnames(x)\\n    bad <- rowSums(is.finite(x)) < nsamples\\n\\n    if (any(bad)) {\\n        x <- x[!bad, drop=FALSE]\\n    }\\n\\n    nprobes <- nrow(x)\\n    top <- min(top, nprobes)\\n\\n    #\\n    ##\\n\\n    plot.title <- quotify(main)\\n\\n    gene.selection <- match.arg(gene.selection, c(\\"pairwise\\", \\"common\\"))\\n\\n    # Distance matrix from pairwise leading fold changes\\n    dd <- matrix(0, nrow=nsamples, ncol=nsamples, dimnames=list(cn, cn))\\n    if (gene.selection == \\"pairwise\\") {\\n    # Distance measure is mean of top squared deviations for each pair of arrays\\n        topindex <- nprobes - top + 1L\\n        for (i in 2L:(nsamples)) {\\n            for (j in 1L:(i - 1L)) {\\n                dist <- sort.int((x[, i] - x[, j])^2, partial=topindex)\\n                topdist <- dist[topindex:nprobes]\\n                dd[i, j] <- sqrt(mean(topdist))\\n            }\\n        }\\n    } else {\\n    # Same genes used for all comparisons\\n        if (nprobes > top) {\\n            s <- rowMeans((x-rowMeans(x))^2)\\n            o <- order(s, decreasing=TRUE)\\n            x <- x[o[1L:top],, drop=FALSE]\\n        }\\n        for (i in 2L:(nsamples))\\n            dist <- sqrt(colMeans( (x[, i]-x[, 1:(i-1), drop=FALSE])^2 ))\\n            dd[i, 1L:(i-1L)] <- dist\\n        axislabel <- \\"Principal Component\\"\\n    }\\n\\n    # Multi-dimensional scaling\\n    a1 <- suppressWarnings(cmdscale(as.dist(dd), k=min(ndim, 8), eig=TRUE))\\n\\n    # Method for MDS objects\\n    points <- a1$points\\n\\n    if (!is.data.frame(groups)) {\\n    # Rename for the column name in dataframe\\n        group <- groups\\n        groups <- data.frame(group)\\n    }\\n\\n    first.col.name <- colnames(groups)[1]\\n\\n    points <- data.frame(points)\\n    names(points) <- paste0(\\"dim\\", 1:ncol(points))\\n    points <- data.frame(points, label=labels, groups)\\n\\n    eigen <- data.frame(name = 1:min(ndim, 8),\\n                        eigen = round(a1$eig[1:min(ndim, 8)]/sum(a1$eig), 2))\\n\\n    plot1 <- glScatter(points, xval=\\"dim1\\", yval=\\"dim2\\", point.size=4,\\n                        xlab=\\"Dimension 1\\", ylab=\\"Dimension 2\\",\\n                        annot=c(\\"label\\", first.col.name, \\"dim1\\", \\"dim2\\"),\\n                        colval=first.col.name, main=main,\\n                        info=list(groupsNames=colnames(groups)))\\n\\n    plot2 <- glBar(eigen, names.arg=\\"name\\", yval=\\"eigen\\",\\n                    main=\\"Variance Explained\\",\\n                    xlab=\\"Dimension\\", ylab=\\"Proportion\\",\\n                    height=300, width=300, info=list(dims=ndim))\\n\\n    link1 <- gllink(2, 1, flag=\\"mds\\")\\n\\n    glimma(plot1, plot2, link1, layout=c(1, 2), overwrite=TRUE,\\n            path=path, folder=folder, html=html, launch=launch)\\n}\\n\\n#\' Glimma MDS Plot\\n#\'\\n#\' Draw an interactive MD plot from a DGEList object with distances calculated from most variable genes.\\n#\'\\n#\' @author Shian Su, Gordon Smyth\\n#\'\\n#\' @param x the DGEList containing the gene expressions.\\n#\' @param top the number of top most variable genes to use.\\n#\' @param labels the labels for each sample.\\n#\' @param groups the experimental group to which samples belong.\\n#\' @param gene.selection \\"pairwise\\" if most variable genes are to be chosen for each pair of samples or \\"common\\" to select the same genes for all comparisons.\\n#\' @param main the title of the plot.\\n#\' @param path the path in which the folder will be created.\\n#\' @param folder the name of the fold to save html file to.\\n#\' @param html the name of the html file to save plots to.\\n#\' @param launch TRUE to launch plot after call.\\n#\' @param ... additional arguments.\\n#\'\\n#\' @return Draws a two-panel interactive MDS plot in an html page. The left panel contains the plot between two MDS dimensions, with annotations displayed on hover. The right panel contains a bar plot of the eigenvalues of each dimension, clicking on any of the bars will plot the corresponding dimension against the next dimension.\\n#\'\\n#\' @method glMDSPlot DGEList\\n#\'\\n#\' @export\\nglMDSPlot.DGEList <- function (x, top=500, labels=1:ncol(x),\\n                            groups=rep(1, ncol(x)), gene.selection=\\"pairwise\\",\\n                            main=\\"MDS Plot\\", path=getwd(),\\n                            folder=\\"glimma-plots\\", html=\\"MDS-Plot\\",\\n                            launch=TRUE, ...) {\\n    x <- edgeR::cpm(x, log=TRUE)\\n    glMDSPlot.default(x, top=500, labels=labels, groups=groups,\\n                    gene.selection=\\"pairwise\\", main=main, path=path,\\n                    folder=folder, html=html, launch=launch, ...)\\n}\\n" }\n'
line: b'{ "repo_name": "percyfal/biomake", "ref": "refs/heads/master", "path": "scripts/plotThetas.R", "content": "#! /usr/bin/Rscript --vanilla\\n# File: plotThetas.R\\n# Created: Tue Oct 29 17:12:00 2013\\n# $Id: $\\n#\\n# Copyright (C) 2013 by Per Unneberg\\n#\\n# Author: Per Unneberg\\n#\\n# Description:\\n#\\n\\nlibrary(utils)\\nlibrary(lattice)\\nlibrary(RColorBrewer)\\ncpal <- colorRampPalette(brewer.pal(9,\\"Paired\\"))(1000)\\n\\nargs <- commandArgs(TRUE)\\nif (length(args) != 2) {\\n    message(\\"Usage: plotThetas.R infile outfile!\\")\\n    quit(\\"yes\\")\\n}\\n\\ninfile <- args[1]\\n\\ndir <- basename(dirname(normalizePath(infile)))\\nif (grep(\\"w[0-9]+_[0-9]+\\", dir)) {\\n    message(\\"We have a window match\\")\\n    window <- gsub(\\"w([0-9]+)_.*\\", \\"\\\\\\\\1\\", dir)\\n    step <- gsub(\\".*_([0-9]+)\\", \\"\\\\\\\\1\\", dir)\\n} else {\\n    window <- 50000\\n    step <- 50000\\n}\\npop = gsub(\\"(^[A-Z]+)_.*\\", \\"\\\\\\\\1\\", infile)\\n\\nd <- read.table(infile)\\nnames(d) <- c(\\"range\\", \\"chr\\", \\"pos\\", \\"tW\\", \\"tP\\", \\"tF\\", \\"tH\\", \\"tL\\", \\"tajD\\", \\"fulif\\", \\"fuliD\\", \\"fayH\\", \\"zengsE\\", \\"numSites\\")\\nd <- cbind(d, d[,c(\\"tW\\", \\"tP\\", \\"tF\\", \\"tH\\", \\"tL\\")] / d$numSites)\\nnames(d) <- c(\\"range\\", \\"chr\\", \\"pos\\", \\"tW\\", \\"tP\\", \\"tF\\", \\"tH\\", \\"tL\\", \\"tajD\\", \\"fulif\\", \\"fuliD\\", \\"fayH\\", \\"zengsE\\", \\"numSites\\", \\"tW.norm\\", \\"tP.norm\\", \\"tF.norm\\", \\"tH.norm\\", \\"tL.norm\\")\\nd$pop = pop\\nd.stack <- cbind(stack(d[,c(\\"numSites\\", \\"tajD\\", \\"fuliD\\", \\"fulif\\", \\"fayH\\", \\"zengsE\\", \\"tW\\", \\"tP\\", \\"tW.norm\\", \\"tP.norm\\")]), pop=d$pop, pos=d$pos, chr=d$chr)\\noutfile <- args[2]\\n\\n# Redefine factor levels\\nd.stack$ind <- factor(d.stack$ind, levels = c(\\"numSites\\",  \\"tajD\\", \\"tW\\", \\"fuliD\\", \\"tW.norm\\", \\"fulif\\", \\"tP\\", \\"fayH\\", \\"tP.norm\\",   \\"zengsE\\"))\\n\\npdf(outfile)\\nprint(xyplot(values ~ pos/1e6 | ind, data=d.stack, type=\\"l\\", layout=c(2,5), xlab=\\"pos (Mb)\\", scales=list(y=list(rot=45, relation=\\"free\\")), strip=FALSE, strip.left=TRUE, main=paste(basename(infile), \\", w:\\", window, \\", s:\\", step, sep=\\"\\"), par.settings=simpleTheme()))\\n\\n\\nd.stack$ind <- factor(d.stack$ind, levels = c(\\"numSites\\", \\"tW\\",  \\"tW.norm\\",  \\"tP\\", \\"tP.norm\\", \\"tajD\\", \\"fuliD\\", \\"fulif\\", \\"fayH\\", \\"zengsE\\"))\\nprint(xyplot(values ~ pos/1e6 | ind, data=d.stack, type=\\"l\\", layout=c(1,10), xlab=\\"pos (Mb)\\", scales=list(y=list(rot=45, relation=\\"free\\")), strip=FALSE, strip.left=TRUE, main=paste(basename(infile), \\", w:\\", window, \\", s:\\", step, sep=\\"\\"), par.settings=simpleTheme()))\\n\\ndev.off()\\n" }\n'
line: b'{ "repo_name": "kbseah/genome-bin-tools", "ref": "refs/heads/master", "path": "gbtools/R/setOperation.R", "content": "#\' Generic operation for merging and subsetting two gbtbin objects\\n#\' \\n#\' @param x1 Object of class gbtbin\\n#\' @param x2 Object of class gbtbin\\n#\' @param shortlist Vector of contig IDs to make new bin\\n#\' @return Object of class gbtbin\\n#\' @keywords internal\\n\\nsetOperation <- function(x1, x2, shortlist) UseMethod(\\"setOperation\\")\\n" }\n'
line: b'{ "repo_name": "kbseah/genome-bin-tools", "ref": "refs/heads/master", "path": "gbtools/R/choosebinPolygon.gbt.R", "content": "#\' Choose bin from gbt object by defining polygon in coverage-GC or differential coverage plot\\n#\'\\n#\' Choose genome bin by specifying coordinates of a polygon from\\n#\' GC-coverage or differential coverage plot of a gbt object\\n#\'\\n#\' @param x Object of class gbt\\n#\' @param slice Slice parameter for drawing the polygon\\n#\' @param polygon Polygon that would define the bin\\n#\'\\n#\' @importFrom sp point.in.polygon\\n#\' @return Object of class gbtbin\\n#\' @seealso \\\\code{\\\\link{plot.gbt}}\\n#\' @export\\n#\'\\nchoosebinPolygon.gbt <- function(x,  # Object of class gbt\\n                          slice,  # Which slices used for the plot from which points to be chosen?\\n                          taxon=\\"Class\\",  # Deprecated - user don\'t change this\\n                          binpolygon=NA, # The polygon\\n                          save=FALSE,  # Save list of contigs in bin to external file?\\n                          file=\\"interactive_bin.list\\"  # Name of file to save list of contigs in bin\\n                          ) {\\n    require(sp)\\n## Wrapper for picking bin interactively from GC-cov or diff-cov plot\\n    if (!is.numeric(slice) || length(slice) > 2) {\\n        cat (\\"gbtools ERROR: Please specify the library(-ies) used to make the plot in focus\\\\n\\")\\n    } else {\\n        if (length(slice)==1) {  # Pick bin from GC-coverage plot\\n            X <- merge(data.frame(ID=x$scaff$ID,\\n                                  Ref_GC=x$scaff$Ref_GC),\\n                       data.frame(ID=x$covs$ID,\\n                                  Avg_fold=x$covs[slice[1]+1]),\\n                       by=\\"ID\\")\\n            names(X) <- c(\\"ID\\",\\"Ref_GC\\",\\"Avg_fold\\")\\n            inpolygon <- sp::point.in.polygon(X$Ref_GC,\\n                                              X$Avg_fold,\\n                                              binpolygon$x,\\n                                              binpolygon$y)\\n        }\\n        else if (length(slice)==2) {  # Pick bin from differential coverage plot\\n            X <- merge(data.frame(ID=x$scaff$ID,\\n                                  Ref_GC=x$scaff$Ref_GC),\\n                       data.frame(ID=x$covs$ID,\\n                                  Avg_fold_1=x$covs[slice[1]+1],\\n                                  Avg_fold_2=x$covs[slice[2]+1]),\\n                       by=\\"ID\\")\\n            names(X) <- c(\\"ID\\",\\"Ref_GC\\",\\"Avg_fold_1\\",\\"Avg_fold_2\\")\\n            inpolygon <- sp::point.in.polygon(X$Avg_fold_1,\\n                                              X$Avg_fold_2,\\n                                              binpolygon$x,\\n                                              binpolygon$y)\\n        }\\n        X.subset <- X[which(inpolygon==1),]\\n        X.shortlist <- as.character(X.subset$ID)\\n        result <- gbtbin(shortlist=X.shortlist,\\n                         x=x,\\n                         slice=slice,\\n                         taxon=taxon,\\n                         points=binpolygon,\\n                         save=save,\\n                         file=file)\\n        result$call[[length(result$call)+1]] <- match.call()  # Record choosebin() function call\\n        return(result)\\n    }\\n}\\n" }\n'
line: b'{ "repo_name": "kbseah/genome-bin-tools", "ref": "refs/heads/master", "path": "gbtools/R/countSingleFromTable.R", "content": "#\' Tabulate objects and count how many singletons\\n#\'\\n#\' @param x Object of class data.frame or vector\\n#\' @return Numeric vector of length 1\\n#\' @keywords internal\\ncountSingleFromTable <- function(x) {\\n    x.tab <- table(x)\\n    uniq <- length(which(x.tab==1))\\n    return(uniq)\\n}\\n" }\n'
line: b'{ "repo_name": "kbseah/genome-bin-tools", "ref": "refs/heads/master", "path": "gbtools/R/lej.gbtbin.R", "content": "#\' Take difference between two gbtbin objects\\n#\'\\n#\' Takes the reverse complement of two gbtbin objects. Equivalent to setdiff\\n#\' in R, or left-exclusive-join in SQL. Non commutative!\\n#\'\\n#\' Self explanatory...\\n#\'\\n#\' @inheritParams add\\n#\'\\n#\' @seealso \\\\code{\\\\link{add}}\\n#\'\\n#\' @export\\nlej.gbtbin <- function(x1,x2) {\\n## Take difference between two bins - non commutative! i.e. left exclusive join\\n    shortlist <- x1$scaff$ID[which(!x1$scaff$ID %in% x2$scaff$ID)]\\n    result <- setOperation.gbtbin(x1=x1,\\n                                  x2=x2,\\n                                  shortlist=shortlist)\\n    result$call[[length(result$call)+1]] <- match.call()  # Record function call \\n    return(result)\\n}\\n" }\n'
line: b'{ "repo_name": "kbseah/genome-bin-tools", "ref": "refs/heads/master", "path": "gbtools/R/generatePlotColors2.R", "content": "#\' Generates colors for marker gene phylotypes in plot by cumulative weight\\n#\'\\n#\' For each taxon, calculates the total contigLength*coverage, and assigns\\n#\' colors for the top taxa which in total account for more than a specified\\n#\' minimum weight.\\n#\'\\n#\' @param scaffold.stats Scaff table from gbt object\\n#\' @param marker.list markTab table from gbt object\\n#\' @param taxon Taxonomic level to do coloring\\n#\' @param consensus Logical - if taxon assignments conflict, take consensus?\\n#\' @param weightCutoff Cutoff quantile for contig length*coverage weight (between 0 and 1)\\n#\' @return data.frame with color assignments for each scaffold\\n#\' @keywords internal\\n#\'\\ngeneratePlotColors2 <- function(scaffold.stats,  # scaff table from gbt object\\n                               marker.list,  # markTab table from gbt object\\n                               taxon,  # Taxonomic level to do the coloring\\n                               consensus,  # Logical-if taxon assgs conflict, take consens?\\n                               weightCutoff # Cutoff quantile for assigning colors (between 0 and 1)\\n                               ) {           # This took a very long time to get it right\\n## Generates colors for marker gene phylotypes in plot\\n    ## Merge tables to have points to plot for the markers #########################\\n    marker.stats <- mergeScaffMarker(scaffold.stats,\\n                                     marker.list,\\n                                     taxon,\\n                                     consensus)\\n    ## Calculate total weight for each taxon #######################################\\n    taxon.agg <- aggregate(marker.stats$Length*marker.stats$Avg_fold,\\n                           by=list(marker.stats$taxon),\\n                           FUN=sum\\n                           )\\n    total.weight <- sum(taxon.agg$x)\\n    taxon.agg.order <- taxon.agg[order(taxon.agg$x,decreasing=TRUE),] # Sort descending\\n    ## Make list of top taxa #######################################################\\n    if (weightCutoff > 1 || weightCutoff < 0) { # Catch errors for weightCutoff\\n        cat (\\"gbtools ERROR: weightCutoff parameter must be between 0 and 1\\\\n\\")\\n    } \\n    else {\\n        # Count taxa which have the highest weight, until weightCutoff\\n        numAboveCutoff <- length(\\n                                 which(\\n                                       cumsum(taxon.agg.order$x) <= weightCutoff*total.weight\\n                                       )\\n                                 )\\n        numBelowCutoff <- length(\\n                                 which(\\n                                       cumsum(taxon.agg.order$x) > weightCutoff*total.weight\\n                                       )\\n                                 )\\n        # Generate colors from red to violet for taxa above cutoff\\n        thecolors <- rainbow (numAboveCutoff,\\n                              start=0,\\n                              end=3/4)\\n        # Everything else is colored grey\\n        repgrey <- rep (\\"grey50\\", numBelowCutoff)\\n        # Combine the two vectors\\n        thecolors <- c(thecolors, repgrey)\\n        colorframe <- data.frame(taxon=taxon.agg.order[,1],\\n                                 colors=thecolors)\\n        # Merge into marker.stats df\\n        marker.stats <- merge(marker.stats,\\n                              colorframe,\\n                              by=\\"taxon\\")\\n        # Return table\\n        return(marker.stats)\\n    }\\n\\n}" }\n'
line: b'{ "repo_name": "kbseah/genome-bin-tools", "ref": "refs/heads/master", "path": "gbtools/R/identify.gbt.R", "content": "#\' Identify points in gbtools plot\\n#\'\\n#\' Click on GC-coverage plot or differential coverage plot to identify contigs.\\n#\' In cluttered plots it may not be very accurate! Will write the contig ID\\n#\' as a label overlay on the plot.\\n#\'\\n#\' @return Object of class gbtbin containing identified contigs\\n#\' @export\\n#\' @seealso \\\\code{\\\\link{plot.gbt}}\\n#\' @seealso \\\\code{\\\\link{choosebin}}\\n\\nidentify.gbt <- function(d,\\n#\' @param d Object of class gbt or gbtbin, in plot\\n                         slice=1,\\n#\' @param slice Which sample data was plotted? (Default: 1)\\n                         ...\\n#\' @param ... Further arguments passed to identify.default()\\n                         ) {\\n    # Catch invalid \\"slice\\" parameters\\n    if (is.na(slice) || !is.numeric(slice)) {\\n        cat (\\"gbtools ERROR: Please specify valid value for slice parameter\\\\n\\")\\n    } else {\\n        # Data frame for GC-coverage plots\\n        if (length (slice) == 1) {\\n            X <- merge(data.frame(ID=d$scaff$ID,\\n                                  Ref_GC=d$scaff$Ref_GC,\\n                                  Length=d$scaff$Length,\\n                                  Avg_fold=d$scaff$Avg_fold,\\n                                  xVals=d$scaff$Ref_GC),\\n                       data.frame(ID=d$covs$ID,\\n                                  yVals=d$covs[[slice[1]+1]]\\n                                  ),\\n                       by=\\"ID\\")\\n            names(X) <- c(\\"ID\\",\\"Ref_GC\\",\\"Length\\",\\"Avg_fold\\",\\"xVals\\",\\"yVals\\")\\n        }\\n        # Data frame for differential coverage plots\\n        else if (length(slice) == 2) {\\n            X <- merge (data.frame (ID=d$scaff$ID,\\n                                    Ref_GC=d$scaff$Ref_GC,\\n                                    Length=d$scaff$Length,\\n                                    Avg_fold=d$scaff$Avg_fold),\\n                        data.frame (ID=d$covs$ID,\\n                                    xVals=d$covs[[slice[1]+1]],\\n                                    yVals=d$covs[[slice[2]+1]]),\\n                        by=\\"ID\\")\\n            names(X) <- c(\\"ID\\",\\"Ref_GC\\",\\"Length\\",\\"Avg_fold\\",\\"xVals\\",\\"yVals\\")\\n        }\\n        # Implement the identify parameter...\\n        shortlist <- identify(x=X$xVals,\\n                              y=X$yVals,\\n                              labels=X$ID,\\n                              ...\\n                              )\\n        shortlist.contigs <- X$ID[shortlist]\\n        return(gbtbin(as.character(shortlist.contigs),d,slice=slice))\\n        #return(shortlist)\\n    }\\n}" }\n'
line: b'{ "repo_name": "kbseah/genome-bin-tools", "ref": "refs/heads/master", "path": "gbtools/R/userAdd.gbt.R", "content": "#\' Add custom user annotations to gbt object\\n#\'\\n#\' Custom user annotations for each scaffold can be added to existing gbt\\n#\' objects. The annotations should be in a data.frame, with at least column\\n#\' \\"scaffold\\" that matches scaffold IDs in the gbt object. Pass the name of the\\n#\' data.frame to the userTab parameter. Give a unique name for this annotation\\n#\' to the userSource parameter.\\n#\'\\n#\' @param x Object of class gbt\\n#\' @param userTab data.frame with user annotations, see Details\\n#\' @param userSource Name for this annotation table\\n#\' @return Object of class gbt\\n#\' @seealso \\\\code{\\\\link{gbt}} \\\\code{\\\\link{plot.gbt}}\\n#\' @export\\nuserAdd.gbt <- function(x,\\n                        userTab,\\n                        userSource=NA\\n                        ) {\\n    ## Check that userTab is data.frame with col \\"scaffold\\" ###################\\n    if (!is.data.frame(userTab) ||\\n        length(which(names(userTab)==\\"scaffold\\"))==0 ||\\n        is.na(userSource) ) {\\n        cat(\\"gbtools ERROR: Please check inputs. See help(userAdd) \\\\n\\")\\n    } else {\\n        ## Check that userTab scaffold IDs match x scaffold IDs ###############\\n        if (length(which(userTab$scaffold %in% x$scaff$ID))==0) {\\n            cat (\\"gbtools ERROR: Scaffold IDs in userTab don\'t match gbt object\\\\n\\")\\n        } else {\\n            x$userTab[[length(x$userTab)+1]] <- userTab  # Append userTab\\n            x$userSource[length(x$userTab)] <- userSource # Append userSource\\n            # NB: Using c() will create discrepancy between userTab and userSource\\n            # because c() on an empty vector will create first element \\"\\"\\n            x$call[[length(x$call)+1]] <- match.call()  # Record function call\\n            return(x)  # Return result\\n        }\\n    }\\n}\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/binary/matrix/UaggOuterChainEquals.R", "content": "#-------------------------------------------------------------\\r\\n#\\r\\n# (C) Copyright IBM Corp. 2010, 2015\\r\\n#\\r\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\r\\n# you may not use this file except in compliance with the License.\\r\\n# You may obtain a copy of the License at\\r\\n#\\r\\n#     http://www.apache.org/licenses/LICENSE-2.0\\r\\n#\\r\\n# Unless required by applicable law or agreed to in writing, software\\r\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\r\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\r\\n# See the License for the specific language governing permissions and\\r\\n# limitations under the License.\\r\\n#\\r\\n#-------------------------------------------------------------\\r\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nA <- as.matrix(readMM(paste(args[1], \\"A.mtx\\", sep=\\"\\")))\\r\\nB <- as.matrix(readMM(paste(args[1], \\"B.mtx\\", sep=\\"\\")))\\r\\n\\r\\nC = rowSums(outer(A,B,\\"==\\"));\\r\\n\\r\\nwriteMM(as(C, \\"CsparseMatrix\\"), paste(args[2], \\"C\\", sep=\\"\\")); \\r\\n\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/aggregate/RowMaxs.R", "content": "#-------------------------------------------------------------\\r\\n#\\r\\n# (C) Copyright IBM Corp. 2010, 2015\\r\\n#\\r\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\r\\n# you may not use this file except in compliance with the License.\\r\\n# You may obtain a copy of the License at\\r\\n#\\r\\n#     http://www.apache.org/licenses/LICENSE-2.0\\r\\n#\\r\\n# Unless required by applicable law or agreed to in writing, software\\r\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\r\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\r\\n# See the License for the specific language governing permissions and\\r\\n# limitations under the License.\\r\\n#\\r\\n#-------------------------------------------------------------\\r\\n\\r\\nargs <- commandArgs(TRUE)\\r\\n\\r\\nif(!(\\"matrixStats\\" %in% rownames(installed.packages()))){\\r\\n   install.packages(\\"matrixStats\\")\\r\\n}\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\nlibrary(\\"matrixStats\\") \\r\\n\\r\\nA <- as.matrix(readMM(paste(args[1], \\"A.mtx\\", sep=\\"\\")))\\r\\nB <- rowMaxs(A);\\r\\n\\r\\nwriteMM(as(B, \\"CsparseMatrix\\"), paste(args[2], \\"B\\", sep=\\"\\")); " }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/misc/ScalarFunctionTest2.R", "content": "#-------------------------------------------------------------\\r\\n#\\r\\n# (C) Copyright IBM Corp. 2010, 2015\\r\\n#\\r\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\r\\n# you may not use this file except in compliance with the License.\\r\\n# You may obtain a copy of the License at\\r\\n#\\r\\n#     http://www.apache.org/licenses/LICENSE-2.0\\r\\n#\\r\\n# Unless required by applicable law or agreed to in writing, software\\r\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\r\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\r\\n# See the License for the specific language governing permissions and\\r\\n# limitations under the License.\\r\\n#\\r\\n#-------------------------------------------------------------\\r\\n\\r\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nsquare <- function(a) {\\r\\n   b = a*a;   \\r\\n   return(b);\\r\\n}\\r\\n\\r\\nx = 1.9/2.9;\\r\\ny = square(x);\\r\\nR = as.matrix(y);\\r\\n\\r\\nwriteMM(as(R, \\"CsparseMatrix\\"), paste(args[1], \\"R\\", sep=\\"\\")); \\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/binary/matrix/UaggOuterChain.R", "content": "#-------------------------------------------------------------\\r\\n#\\r\\n# (C) Copyright IBM Corp. 2010, 2015\\r\\n#\\r\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\r\\n# you may not use this file except in compliance with the License.\\r\\n# You may obtain a copy of the License at\\r\\n#\\r\\n#     http://www.apache.org/licenses/LICENSE-2.0\\r\\n#\\r\\n# Unless required by applicable law or agreed to in writing, software\\r\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\r\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\r\\n# See the License for the specific language governing permissions and\\r\\n# limitations under the License.\\r\\n#\\r\\n#-------------------------------------------------------------\\r\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nA <- as.matrix(readMM(paste(args[1], \\"A.mtx\\", sep=\\"\\")))\\r\\nB <- as.matrix(readMM(paste(args[1], \\"B.mtx\\", sep=\\"\\")))\\r\\n\\r\\nC = rowSums(outer(A,B,\\"<\\"));\\r\\n\\r\\nwriteMM(as(C, \\"CsparseMatrix\\"), paste(args[2], \\"C\\", sep=\\"\\")); \\r\\n\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/vect/VectorizeLixRowPos.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nA = as.matrix(readMM(paste(args[1], \\"A.mtx\\", sep=\\"\\")))\\r\\n\\r\\nR = A;\\r\\nR[3,7] = as.matrix(3);\\r\\nR[3,8] = as.matrix(4);\\r\\n\\r\\n\\r\\nwriteMM(as(R, \\"CsparseMatrix\\"), paste(args[2], \\"R\\", sep=\\"\\")); \\r\\n\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/parfor/parfor_optimizer3.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nV1 <- readMM(paste(args[1], \\"V.mtx\\", sep=\\"\\"))\\r\\nV <- as.matrix(V1);\\r\\nn <- ncol(V); \\r\\nn2 <- n/2;\\r\\n\\r\\nR <- array(0,dim=c(1,n2))\\r\\n\\r\\nfor( i in 1:n2 )\\r\\n{\\r\\n   X <- V[,i];                 \\r\\n   Y <- V[,n-i+1];                \\r\\n   R[1,i] <- sum(X)+sum(Y);\\r\\n}   \\r\\n\\r\\nwriteMM(as(R, \\"CsparseMatrix\\"), paste(args[2], \\"Rout\\", sep=\\"\\")); \\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/unary/matrix/replace_maxmin.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nA <- as.matrix(readMM(paste(args[1], \\"A.mtx\\", sep=\\"\\")))\\r\\n\\r\\nC <- replace(A, A==as.numeric(min(A)), max(A));\\r\\n\\r\\nwriteMM(as(C, \\"CsparseMatrix\\"), paste(args[3], \\"C\\", sep=\\"\\")); \\r\\n\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/binary/matrix_full_other/IntegerDivision_mod.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nA <- as.matrix(readMM(paste(args[1], \\"A.mtx\\", sep=\\"\\")))\\r\\nB <- as.matrix(readMM(paste(args[1], \\"B.mtx\\", sep=\\"\\")))\\r\\nif( nrow(A)==1 ){ #support for scalars        \\r\\n   A <- as.numeric(A);\\r\\n}\\r\\nif( nrow(B)==1 ){ #support for scalars\\r\\n   B <- as.numeric(B);\\r\\n}\\r\\nC <- A%%B;\\r\\n\\r\\n#note: writeMM replaces NaN and Inf\\r\\nwriteMM(as(C, \\"CsparseMatrix\\"), paste(args[2], \\"C\\", sep=\\"\\")); \\r\\n\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/parfor/parfor_optimizer2.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\n\\r\\nD1 <- readMM(paste(args[1], \\"D.mtx\\", sep=\\"\\"))\\r\\nS11 <- readMM(paste(args[1], \\"S1.mtx\\", sep=\\"\\"))\\r\\nS21 <- readMM(paste(args[1], \\"S2.mtx\\", sep=\\"\\"))\\r\\nK11 <- readMM(paste(args[1], \\"K1.mtx\\", sep=\\"\\"))\\r\\nK21 <- readMM(paste(args[1], \\"K2.mtx\\", sep=\\"\\"))\\r\\nD <- as.matrix(D1);\\r\\nS1 <- as.matrix(S11);\\r\\nS2 <- as.matrix(S21);\\r\\nK1 <- as.matrix(K11);\\r\\nK2 <- as.matrix(K21);\\r\\n\\r\\nnumPairs <- ncol(S1) * ncol(S2); # number of attribute pairs (|S1|*|S2|)\\r\\nmaxC <- args[2]; # max number of categories in any categorical attribute\\r\\n\\r\\ns1size <- ncol(S1);\\r\\ns2size <- ncol(S2);\\r\\n\\r\\n# R, chisq, cramers, spearman, eta, anovaf\\r\\nnumstats <- 8;\\r\\nbasestats <- array(0,dim=c(numstats,numPairs)); \\r\\ncat_counts <- array(0,dim=c(maxC,numPairs)); \\r\\ncat_means <- array(0,dim=c(maxC,numPairs));\\r\\ncat_vars <- array(0,dim=c(maxC,numPairs));\\r\\n\\r\\n\\r\\nfor( i in 1:s1size ) { \\r\\n    a1 <- S1[,i];\\r\\n    k1 <- K1[1,i];\\r\\n    A1 <- as.matrix(D[,a1]);\\r\\n\\r\\n    for( j in 1:s2size ) {\\r\\n        pairID <-(i-1)*s2size+j;\\r\\n        a2 <- S2[,j];\\r\\n        k2 <- K2[1,j];\\r\\n        A2 <- as.matrix(D[,a2]);\\r\\n    \\r\\n        if (k1 == k2) {\\r\\n            if (k1 == 1) {   \\r\\n                # scale-scale\\r\\n                print(\\"scale-scale\\");\\r\\n                basestats[1,pairID] <- cor(D[,a1], D[,a2]);\\r\\n                #basestats[1,pairID] <- cor(A1, A2);\\r\\n                \\r\\n                print(basestats[1,pairID]);\\r\\n            } else {\\r\\n                # nominal-nominal or ordinal-ordinal\\r\\n                print(\\"categorical-categorical\\");\\r\\n                F <- table(A1,A2);\\r\\n                cst <- chisq.test(F);\\r\\n                chi_squared <- as.numeric(cst[1]);\\r\\n                degFreedom <- (nrow(F)-1)*(ncol(F)-1);\\r\\n                pValue <- as.numeric(cst[3]);\\r\\n                q <- min(dim(F));\\r\\n                W <- sum(F);\\r\\n                cramers_v <- sqrt(chi_squared/(W*(q-1)));\\r\\n\\r\\n                basestats[2,pairID] <- chi_squared;\\r\\n                basestats[3,pairID] <- degFreedom;\\r\\n                basestats[4,pairID] <- pValue;\\r\\n                basestats[5,pairID] <- cramers_v;\\r\\n\\r\\n                if ( k1 == 3 ) {\\r\\n                    # ordinal-ordinal   \\r\\n                    print(\\"ordinal-ordinal\\");\\r\\n                    basestats[6,pairID] <- cor(A1,A2, method=\\"spearman\\");\\r\\n                }\\r\\n            }\\r\\n        } \\r\\n        else {       \\r\\n            if (k1 == 1 || k2 == 1) {    \\r\\n                # Scale-nominal/ordinal\\r\\n                print(\\"scale-categorical\\");\\r\\n                if ( k1 == 1 ) {\\r\\n                    Av <- as.matrix(A2); \\r\\n                    Yv <- as.matrix(A1); \\r\\n                }\\r\\n                else {\\r\\n                    Av <- as.matrix(A1); \\r\\n                    Yv <- as.matrix(A2); \\r\\n                }\\r\\n                \\r\\n                W <- nrow(Av);\\r\\n                my <- mean(Yv); \\r\\n                varY <- var(Yv);\\r\\n                \\r\\n                CFreqs <- as.matrix(table(Av)); \\r\\n                CMeans <- as.matrix(aggregate(Yv, by=list(Av), \\"mean\\")$V1);\\r\\n                CVars <- as.matrix(aggregate(Yv, by=list(Av), \\"var\\")$V1);\\r\\n                R <- nrow(CFreqs);\\r\\n              \\r\\n                Eta <- sqrt(1 - ( sum((CFreqs-1)*CVars) / ((W-1)*varY) ));\\r\\n                anova_num <- sum( (CFreqs*(CMeans-my)^2) )/(R-1);\\r\\n                anova_den <- sum( (CFreqs-1)*CVars )/(W-R);\\r\\n                ANOVAF <- anova_num/anova_den;\\r\\n\\r\\n                basestats[7,pairID] <- Eta;\\r\\n                basestats[8,pairID] <- ANOVAF;\\r\\n\\r\\n                cat_counts[ 1:length(CFreqs),pairID] <- CFreqs;\\r\\n                cat_means[ 1:length(CMeans),pairID] <- CMeans;\\r\\n                cat_vars[ 1:length(CVars),pairID] <- CVars;\\r\\n            }\\r\\n            else {\\r\\n                # nominal-ordinal or ordinal-nominal    \\r\\n                print(\\"nomial-ordinal\\"); #TODO should not be same code            \\r\\n                F <- table(A1,A2);\\r\\n                cst <- chisq.test(F);\\r\\n                chi_squared <- as.numeric(cst[1]);\\r\\n                degFreedom <- (nrow(F)-1)*(ncol(F)-1);\\r\\n                pValue <- as.numeric(cst[3]);\\r\\n                q <- min(dim(F));\\r\\n                W <- sum(F);\\r\\n                cramers_v <- sqrt(chi_squared/(W*(q-1)));\\r\\n                \\r\\n                basestats[2,pairID] <- chi_squared;\\r\\n                basestats[3,pairID] <- degFreedom;\\r\\n                basestats[4,pairID] <- pValue;\\r\\n                basestats[5,pairID] <- cramers_v;\\r\\n            }\\r\\n        }\\r\\n    }\\r\\n}\\r\\n\\r\\nwriteMM(as(basestats, \\"CsparseMatrix\\"), paste(args[3], \\"bivar.stats\\", sep=\\"\\"));\\r\\nwriteMM(as(cat_counts, \\"CsparseMatrix\\"), paste(args[3], \\"category.counts\\", sep=\\"\\"));\\r\\nwriteMM(as(cat_means, \\"CsparseMatrix\\"), paste(args[3], \\"category.means\\", sep=\\"\\"));\\r\\nwriteMM(as(cat_vars, \\"CsparseMatrix\\"), paste(args[3], \\"category.variances\\", sep=\\"\\"));\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/unary/matrix/Floor.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nA <- as.matrix(readMM(paste(args[1], \\"math.mtx\\", sep=\\"\\")))\\r\\n\\r\\nR = floor(A);\\r\\n\\r\\nwriteMM(as(R, \\"CsparseMatrix\\"), paste(args[2], \\"R\\", sep=\\"\\")); \\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/binary/matrix/UltraSparseMatrixMultiplication.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nA <- as.matrix(readMM(paste(args[1], \\"A.mtx\\", sep=\\"\\")))\\r\\nB <- as.matrix(readMM(paste(args[1], \\"B.mtx\\", sep=\\"\\")))\\r\\n\\r\\nP <- diag( as.vector(B==2) )\\r\\nPx <- P[rowSums((P==0) | is.na(P)) != ncol(P),];\\r\\n\\r\\nC <- Px %*% A;\\r\\n\\r\\nwriteMM(as(C, \\"CsparseMatrix\\"), paste(args[2], \\"C\\", sep=\\"\\")); \\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/applications/descriptivestats/ScaleCategoricalWithWeightsTest.R", "content": "#-------------------------------------------------------------\\r\\n#\\r\\n# (C) Copyright IBM Corp. 2010, 2015\\r\\n#\\r\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\r\\n# you may not use this file except in compliance with the License.\\r\\n# You may obtain a copy of the License at\\r\\n#\\r\\n#     http://www.apache.org/licenses/LICENSE-2.0\\r\\n#\\r\\n# Unless required by applicable law or agreed to in writing, software\\r\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\r\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\r\\n# See the License for the specific language governing permissions and\\r\\n# limitations under the License.\\r\\n#\\r\\n#-------------------------------------------------------------\\r\\n\\r\\n# JUnit test class: dml.test.integration.descriptivestats.BivariateScaleCategoricalTest.java\\r\\n# command line invocation assuming $SC_HOME is set to the home of the R script\\r\\n# Rscript $SC_HOME/ScaleCategorical.R $SC_HOME/in/ $SC_HOME/expected/\\r\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n# Usage: R --vanilla -args Xfile X < ScaleCategoricalTest.R\\r\\n\\r\\n#parseCommandArgs()\\r\\n######################\\r\\nAtemp = readMM(paste(args[1], \\"A.mtx\\", sep=\\"\\"));\\r\\nYtemp = readMM(paste(args[1], \\"Y.mtx\\", sep=\\"\\"));\\r\\nWM = readMM(paste(args[1], \\"WM.mtx\\", sep=\\"\\"));\\r\\n\\r\\nYv=rep(Ytemp[,1],WM[,1])\\r\\nAv=rep(Atemp[,1],WM[,1])\\r\\n\\r\\nW = sum(WM);\\r\\nmy = sum(Yv)/W;\\r\\nvarY = var(Yv);\\r\\n\\r\\nCFreqs = as.matrix(table(Av)); \\r\\nCMeans = as.matrix(aggregate(Yv, by=list(Av), \\"mean\\")$x);\\r\\nCVars = as.matrix(aggregate(Yv, by=list(Av), \\"var\\")$x);\\r\\n\\r\\n# number of categories\\r\\nR = nrow(CFreqs);\\r\\n\\r\\nEta = sqrt(1 - ( sum((CFreqs-1)*CVars) / ((W-1)*varY) ));\\r\\n\\r\\nanova_num = sum( (CFreqs*(CMeans-my)^2) )/(R-1);\\r\\nanova_den = sum( (CFreqs-1)*CVars )/(W-R);\\r\\nANOVAF = anova_num/anova_den;\\r\\n\\r\\nprint(W, digits=15);\\r\\nprint(R, digits=15);\\r\\nprint(anova_num, digits=15);\\r\\nprint(anova_den, digits=15);\\r\\n\\r\\n#######################\\r\\n\\r\\nwrite(Eta, paste(args[2], \\"Eta\\", sep=\\"\\"));\\r\\n\\r\\nwrite(ANOVAF, paste(args[2], \\"AnovaF\\", sep=\\"\\"));\\r\\n\\r\\nwrite(varY, paste(args[2], \\"VarY\\", sep=\\"\\"));\\r\\n\\r\\nwrite(my, paste(args[2], \\"MeanY\\", sep=\\"\\"));\\r\\n\\r\\nwriteMM(as(CVars,\\"CsparseMatrix\\"), paste(args[2], \\"CVars\\", sep=\\"\\"), format=\\"text\\");\\r\\nwriteMM(as(CFreqs,\\"CsparseMatrix\\"), paste(args[2], \\"CFreqs\\", sep=\\"\\"), format=\\"text\\");\\r\\nwriteMM(as(CMeans,\\"CsparseMatrix\\"), paste(args[2], \\"CMeans\\", sep=\\"\\"), format=\\"text\\");\\r\\n\\r\\n\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/quaternary/WeightedSigmoidP4.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nX = as.matrix(readMM(paste(args[1], \\"X.mtx\\", sep=\\"\\")))\\r\\nU = as.matrix(readMM(paste(args[1], \\"U.mtx\\", sep=\\"\\")))\\r\\nV = as.matrix(readMM(paste(args[1], \\"V.mtx\\", sep=\\"\\")))\\r\\n\\r\\nUV = -(U%*%t(V));\\r\\nR = X * log(1/(1 + exp(-UV)));\\r\\n\\r\\nwriteMM(as(R, \\"CsparseMatrix\\"), paste(args[2], \\"R\\", sep=\\"\\")); \\r\\n\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/binary/matrix/UaggOuterChainNotEqualsRowIndexMin.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\nargs <- commandArgs(TRUE)\\noptions(digits=22)\\n\\nlibrary(\\"Matrix\\")\\n\\nA <- as.vector(readMM(paste(args[1], \\"A.mtx\\", sep=\\"\\")))\\nB <- as.vector(readMM(paste(args[1], \\"B.mtx\\", sep=\\"\\")))\\n\\nI <- as.matrix(outer(A,B,\\"!=\\"));\\nC <- max.col(-I,ties.method=\\"first\\");\\n\\nwriteMM(as(C, \\"CsparseMatrix\\"), paste(args[2], \\"C\\", sep=\\"\\"));" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/unary/scalar/DFTest_T.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\n\\r\\nargs <- commandArgs(TRUE)\\r\\nlibrary(Matrix)\\r\\n\\r\\nqtle = qt(as.numeric(args[1]), df=as.numeric(args[2]));\\r\\np = pt(qtle, df=as.numeric(args[2]));\\r\\npl = pt(qtle, df=as.numeric(args[2]), lower.tail=F);\\r\\n\\r\\nout = matrix(0,nrow=3, ncol=1);\\r\\nout[1,1] = qtle;\\r\\nout[2,1] = p;\\r\\nout[3,1] = pl;\\r\\n\\r\\nwriteMM(as(out, \\"CsparseMatrix\\"), args[3]); \\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/external/DynProject.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nX <- as.matrix(readMM(paste(args[1], \\"X.mtx\\", sep=\\"\\")));\\r\\nc <- as.matrix(readMM(paste(args[1], \\"c.mtx\\", sep=\\"\\")));\\r\\n\\r\\nif( ncol(X)==1 )\\r\\n{\\r\\n   Y <- X[c];\\r\\n} else {\\r\\n   Y <- X[c,c];\\r\\n}\\r\\n\\r\\nwriteMM(as(Y, \\"CsparseMatrix\\"), paste(args[2], \\"Y.mtx\\", sep=\\"\\")); " }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/applications/parfor/parfor_naive-bayes.R", "content": "#-------------------------------------------------------------\\r\\n#\\r\\n# (C) Copyright IBM Corp. 2010, 2015\\r\\n#\\r\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\r\\n# you may not use this file except in compliance with the License.\\r\\n# You may obtain a copy of the License at\\r\\n#\\r\\n#     http://www.apache.org/licenses/LICENSE-2.0\\r\\n#\\r\\n# Unless required by applicable law or agreed to in writing, software\\r\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\r\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\r\\n# See the License for the specific language governing permissions and\\r\\n# limitations under the License.\\r\\n#\\r\\n#-------------------------------------------------------------\\r\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nD = as.matrix(readMM(paste(args[1], \\"D.mtx\\", sep=\\"\\")))\\r\\nC = as.matrix(readMM(paste(args[1], \\"C.mtx\\", sep=\\"\\")))\\r\\n\\r\\n# reading input args\\r\\nnumClasses = as.integer(args[2]);\\r\\nlaplace_correction = 1\\r\\n\\r\\nnumRows = nrow(D)\\r\\nnumFeatures = ncol(D)\\r\\n\\r\\n# Compute conditionals\\r\\n\\r\\n# Compute the feature counts for each class\\r\\nclassFeatureCounts = matrix(0, numClasses, numFeatures)\\r\\nfor (i in 1:numFeatures) {\\r\\n  Col = D[,i]\\r\\n  classFeatureCounts[,i] = aggregate(as.vector(Col), by=list(as.vector(C)), FUN=sum)[,2];\\r\\n}\\r\\n\\r\\n# Compute the total feature count for each class \\r\\n# and add the number of features to this sum\\r\\n# for subsequent regularization (Laplace\'s rule)\\r\\nclassSums = rowSums(classFeatureCounts) + numFeatures*laplace_correction\\r\\n\\r\\n# Compute class conditional probabilities\\r\\nrepClassSums = classSums %*% matrix(1,1,numFeatures);\\r\\nclass_conditionals = (classFeatureCounts + laplace_correction) / repClassSums;\\r\\n\\r\\n# Compute class priors\\r\\nclass_counts = aggregate(as.vector(C), by=list(as.vector(C)), FUN=length)[,2]\\r\\nclass_prior = class_counts / numRows;\\r\\n\\r\\n# write out the model\\r\\nwriteMM(as(class_prior, \\"CsparseMatrix\\"), paste(args[3], \\"class_prior\\", sep=\\"\\"));\\r\\nwriteMM(as(class_conditionals, \\"CsparseMatrix\\"), paste(args[3], \\"class_conditionals\\", sep=\\"\\"));\\r\\n" }\n'
line: b'{ "repo_name": "deroneriksson/systemml", "ref": "refs/heads/master", "path": "system-ml/src/test/scripts/functions/recompile/remove_empty_recompile.R", "content": "#-------------------------------------------------------------\\n#\\n# (C) Copyright IBM Corp. 2010, 2015\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#-------------------------------------------------------------\\n\\n\\r\\nargs <- commandArgs(TRUE)\\r\\noptions(digits=22)\\r\\n\\r\\nlibrary(\\"Matrix\\")\\r\\n\\r\\nX <- readMM(paste(args[1], \\"X.mtx\\", sep=\\"\\"))\\r\\n\\r\\ntype = as.integer(args[2]);\\r\\n\\r\\nR = X;\\r\\n\\r\\nif( type==0 ){\\r\\n  R = as.matrix( sum(X) );\\r\\n}\\r\\nif( type==1 ){\\r\\n  R = round(X);\\r\\n}\\r\\nif( type==2 ){\\r\\n  R = t(X); \\r\\n}\\r\\nif( type==3 ){\\r\\n  R = X*(X-1);\\r\\n}\\r\\nif( type==4 ){\\r\\n  R = (X-1)*X;\\r\\n}\\r\\nif( type==5 ){\\r\\n  R = X+(X-1);\\r\\n}\\r\\nif( type==6 ){\\r\\n  R = (X-1)+X;\\r\\n}\\r\\nif( type==7 ){\\r\\n  R = X-(X+2);\\r\\n}\\r\\nif( type==8 ){\\r\\n  R = (X+2)-X;\\r\\n}\\r\\nif( type==9 ){\\r\\n  R = X%*%(X-1);\\r\\n}\\r\\nif( type==10 ){\\r\\n  R = (X-1)%*%X;\\r\\n}\\r\\nif( type==11 ){\\r\\n  R = X[1:(nrow(X)-1), 1:(ncol(X)-1)];\\r\\n}\\r\\nif( type==12 ){\\r\\n  X[1,] = X[2,];\\r\\n  R = X;\\r\\n}\\r\\n\\r\\nwriteMM(as(R, \\"CsparseMatrix\\"), paste(args[3], \\"R\\", sep=\\"\\")); " }'
input_path: /home/gcloud/TransCoder/data/test_dataset/r/r.002.json.gz
language: r
output_path: /home/gcloud/TransCoder/data/test_dataset/r/r.002.with_comments.tok
line: b'{ "repo_name": "hadley/readxl", "ref": "refs/heads/master", "path": "R/excel-sheets.R", "content": "#\' List all sheets in an excel spreadsheet.\\n#\'\\n#\' @inheritParams read_excel\\n#\' @export\\n#\' @examples\\n#\' excel_sheets(system.file(\\"extdata/datasets.xlsx\\", package = \\"readxl\\"))\\n#\' excel_sheets(system.file(\\"extdata/datasets.xls\\", package = \\"readxl\\"))\\n#\'\\n#\' # To load all sheets in a workbook, use lapply\\n#\' path <- system.file(\\"extdata/datasets.xls\\", package = \\"readxl\\")\\n#\' lapply(excel_sheets(path), read_excel, path = path)\\nexcel_sheets <- function(path) {\\n  path <- check_file(path)\\n  ext <- tolower(tools::file_ext(path))\\n\\n  switch(excel_format(path),\\n    xls =  xls_sheets(path),\\n    xlsx = xlsx_sheets(path)\\n  )\\n}\\n" }\n'
line: b'{ "repo_name": "eddelbuettel/rcppredis", "ref": "refs/heads/master", "path": "inst/tests/runit.serverIssues.R", "content": "test_01_setup <- function() {\\n    suppressMessages(library(RcppRedis))\\n    # we start the Redis server for this test as a slave so we don\'t clobber the main running version of redis for the rest of the tests\\n    writeLines(\\"requirepass badPassword\\",\\"/tmp/redis.conf\\")\\n    system(\\"redis-server /tmp/redis.conf --port 7777 --slaveof localhost 6379\\", wait=FALSE)\\n    # Wait for server to come up\\n    Sys.sleep(5)\\n}\\n\\ntest_02_testUnauth <- function () {\\n  redis <<- new(RcppRedis::Redis, \\"localhost\\", 7777, auth = \\"\\", 10)\\n  # we expect an exception because we haven\'t send the password\\n  checkException(redis$ping())\\n}\\n\\ntest_03_testAuth <- function () {\\n  redis <<- new(RcppRedis::Redis, \\"localhost\\", 7777, auth = \\"badPassword\\", 10)\\n  checkEquals(redis$ping(), \\"PONG\\")\\n}\\n\\ntest_04_killServer <- function() {\\n    # confirm server is up\\n    checkEquals(redis$ping(),\\"PONG\\")\\n    # kill server\\n    checkException(redis$exec(\\"SHUTDOWN\\"))\\n}\\n\\ntest_05_cleanup <- function() {\\n    NULL\\n}\\n" }\n'
line: b'{ "repo_name": "eddelbuettel/rcppredis", "ref": "refs/heads/master", "path": "demo/spDemo.R", "content": "\\n\\nlibrary(rredis)\\nsuppressMessages(library(RcppRedis))\\nsuppressMessages(library(xts))\\nlibrary(rbenchmark)\\n\\nredisConnect()                          # default localhost\\nredis <- new(Redis)\\n\\nif (!redisExists(\\"sp500\\")) {\\n    suppressMessages(library(quantmod))\\n    options(\\"getSymbols.warning4.0\\"=FALSE)   ## suppress some line noise\\n    sp <- getSymbols(\\"^GSPC\\", auto.assign=FALSE, from=\\"1950-01-01\\", to=Sys.Date())\\n    redisSet(\\"sp500\\", sp)\\n    cat(\\"Downloaded SP500 and stored in redis\\\\n\\")\\n} else { \\n    cat(\\"Retrieving SP500 from redis\\\\n\\")\\n    sp <- redisGet(\\"sp500\\")\\n}\\n#print(summary(sp))\\n\\nrInsert <- function(x) {\\n    n <- nrow(x)\\n    key <- \\"sp500_R\\"\\n    if (redisExists(key)) redisDelete(key)\\n    M <- cbind(as.numeric(index(x)), coredata(x))\\n    for (i in 1:n) {\\n        redisRPush(key, M[i,,drop=TRUE])\\n    }\\n    invisible(NULL)\\n}\\n\\n## This is atrociously slow:\\n##\\n## R> system.time(rInsert(sp))\\n##    user  system elapsed \\n##  16.392   0.292 645.643 \\n## R> \\n\\nsystem.time(m1 <- do.call(rbind, redisLRange(\\"sp500_R\\", 0, -1)))\\nsystem.time(m2 <- do.call(rbind, redis$lrange(\\"sp500_R\\", 0, -1)))\\nidentical(m1,m2)\\nsystem.time(m3 <- redis$listToMatrix(redis$lrange(\\"sp500_R\\", 0, -1)))\\nm4 <- m1\\ndimnames(m4) <- list()\\nidentical(m4,m3)\\n\\n## approx factor 20\\nres <- benchmark(do.call(rbind, redisLRange(\\"sp500_R\\", 0, -1)),\\n                 do.call(rbind, redis$lrange(\\"sp500_R\\", 0, -1)),\\n                 redis$listToMatrix(redis$lrange(\\"sp500_R\\", 0, -1)),\\n                 order=\\"relative\\", replications=25)[,1:4]\\nprint(res)\\n\\n\\ncInsert <- function(x) {\\n    n <- nrow(x)\\n    key <- \\"sp500_C\\"\\n    if (redisExists(key)) redisDelete(key)\\n    M <- cbind(as.numeric(index(x)), coredata(x))\\n    for (i in 1:n) {\\n        redis$listRPush(key, M[i,])\\n    }\\n    invisible(NULL)\\n}\\n\\n\\n## approx factor 20\\nres <- benchmark(do.call(rbind, redisLRange(\\"sp500_R\\", 0, -1)),\\n                 do.call(rbind, redis$lrange(\\"sp500_R\\", 0, -1)),\\n                 redis$listToMatrix(redis$lrange(\\"sp500_R\\", 0, -1)),\\n                 redis$listToMatrix(redis$listRange(\\"sp500_C\\", 0, -1)),\\n                 order=\\"relative\\", replications=25)[,1:4]\\nprint(res)\\n\\n\\n## R> print(res)\\n##                                                    test replications elapsed relative\\n## 4 redis$listToMatrix(redis$listRange(\\"sp500_C\\", 0, -1))           25   0.296    1.000\\n## 3    redis$listToMatrix(redis$lrange(\\"sp500_R\\", 0, -1))           25   2.300    7.770\\n## 2        do.call(rbind, redis$lrange(\\"sp500_R\\", 0, -1))           25   2.629    8.882\\n## 1         do.call(rbind, redisLRange(\\"sp500_R\\", 0, -1))           25  48.028  162.257\\n## R> \\n\\n## redo after Bryan\'s socket/nagle update to rredis:\\n##                                                    test replications elapsed relative\\n## 4 redis$listToMatrix(redis$listRange(\\"sp500_C\\", 0, -1))           25   0.407    1.000\\n## 3    redis$listToMatrix(redis$lrange(\\"sp500_R\\", 0, -1))           25   2.112    5.189\\n## 2        do.call(rbind, redis$lrange(\\"sp500_R\\", 0, -1))           25   2.458    6.039\\n## 1         do.call(rbind, redisLRange(\\"sp500_R\\", 0, -1))           25  48.367  118.838\\n## edd@max:~/git/rhiredis/demo$ \\n" }\n'
line: b'{ "repo_name": "NCBI-Hackathons/SRA2R", "ref": "refs/heads/master", "path": "tests/runTests.R", "content": "BiocGenerics:::testPackage(\\"SRA2R\\")\\n" }\n'
line: b'{ "repo_name": "NCBI-Hackathons/SRA2R", "ref": "refs/heads/master", "path": "tests/testthat/test_getReads.R", "content": "library(SRA2R)\\ncontext(\\"getReads\\")\\n\\ntest_that(\\"getReads returns reads from a run\\", {\\n  expect_equal(getFastqCount(\'SRR000123\'),4583)\\n  expect_equal(getFastqCount(\'SRR1607152\'),78377869)\\n})\\n" }\n'
line: b'{ "repo_name": "thiloklein/matchingMarkets", "ref": "refs/heads/master", "path": "R/ttc.R", "content": "# ----------------------------------------------------------------------------\\n# R-code (www.r-project.org/) for the Top-Trading-Cycles Algorithm\\n#\\n# Copyright (c) 2013 Thilo Klein\\n#\\n# This library is distributed under the terms of the GNU Public License (GPL)\\n# for full details see the file LICENSE\\n#\\n# ----------------------------------------------------------------------------\\n\\n#\' @title Top-Trading-Cycles Algorithm for the house allocation problem\\n#\'\\n#\' @description Finds the stable matching in the \\\\href{http://en.wikipedia.org/wiki/Herbert_Scarf#8._The_Housing_Market}{house allocation problem} with existing tenants.\\n#\' Uses the Top-Trading-Cycles Algorithm proposed in Abdulkadiroglu and Sonmez (1999).\\n#\'\\n#\' @param P list of individuals\' preference rankings over objects.\\n#\' @param X 2-column-matrix of objects (\\\\code{obj}) and their owners (\\\\code{ind}).\\n#\' @return \\\\code{ttc} returns a 2-column matrix of the stable matching solution for the housing market problem based on the Top-Trading-Cycles algorithm.\\n#\' @author Thilo Klein \\n#\' @keywords algorithms\\n#\' @references Abdulkadiroglu, A. and Sonmez, T. (1999). House Allocation with Existing Tenants. \\\\emph{Journal of Economic Theory}, 88(2):233--260.\\n#\' @examples\\n#\' ## generate list of individuals\' preference rankings over objects\\n#\' P <- list()\\n#\' P[[1]] <- c(2,5,1,4,3)    # individual 1\\n#\' P[[2]] <- c(1,5,4,3,2)    # individual 2\\n#\' P[[3]] <- c(2,1,4,3,5)    # individual 3\\n#\' P[[4]] <- c(2,4,3,1,5)    # individual 4\\n#\' P[[5]] <- c(4,3,1,2,5); P # individual 5\\n#\' \\n#\' ## generate 2-column-matrix of objects (\'obj\') and their owners (\'ind\')\\n#\' X <- data.frame(ind=1:5, obj=1:5); X\\n#\' \\n#\' ## find assignment based on TTC algorithm\\n#\' ttc(P=P,X=X)\\n#\' @export\\nttc <- function(P=NULL,X=NULL){\\n  \\n  ## 2-column-matrix of home objects (\'obj\') and their owners (\'ind\')\\n  Y <- data.frame(ind=NULL, obj=NULL)\\n  \\n  for(z in 1:length(unique(X$obj))){\\n\\n    ## 1. Find cycle\\n    Cycle <- findCycle(P=P,X=X)\\n    \\n    ## 2. Add objects in this cycle to \'home territory\'\\n    Y <- rbind(Y,Cycle)\\n\\n    ## 3. Remove objects in this cycle from tradable objects\\n    X <- X[-which(X$obj %in% Cycle$obj),]\\n    for(i in 1:length(P)){\\n      P[[i]] <- P[[i]][!P[[i]] %in% Y$ind]\\n    }\\n\\n    ## 4. Process ends if no tradable objects remain\\n    if(nrow(X)==0){\\n      Y <- rbind(Y,X)\\n      return(Y)\\n      break\\n    }\\n  }\\n}\\nfindCycle <- function(P=NULL,X=NULL){\\n  Cycle   <- data.frame(ind=NA, obj=NA)\\n  thisind <- X$ind[1] # start with first individual in line\\n  for(j in 1:length(unique(X$ind))){\\n    Cycle[j,] <- c(thisind,P[[thisind]][1]) # id and top-ranked object of the individual in line\\n    thisind   <- X[X$obj == P[[thisind]][1],\\"ind\\"] # individual whose object is requested\\n    if(Cycle[j,1] == Cycle[j,2]){ # if individual points to own object\\n      return(Cycle[j,])\\n      break\\n    }\\n    if(thisind %in% Cycle$ind){ # if this individual completes a cycle\\n      return(Cycle)\\n      break\\n    } \\n  }\\n}" }\n'
line: b'{ "repo_name": "thiloklein/matchingMarkets", "ref": "refs/heads/master", "path": "R/mfx.R", "content": "# ----------------------------------------------------------------------------\\n# R-code (www.r-project.org/) to obtain marginal effects for probit and matching models\\n#\\n# Copyright (c) 2013 Thilo Klein\\n#\\n# This library is distributed under the terms of the GNU Public License (GPL)\\n# for full details see the file LICENSE\\n#\\n# ----------------------------------------------------------------------------\\n\\n#\' @title Marginal effects for probit and matching models\\n#\'\\n#\' @description Marginal effects from regression coefficients for probit \\n#\' and matching models. \\n#\'\\n#\' @param m an object returned from functions \\\\code{stabit} or \\\\code{stabit2}.\\n#\' @param toLatex logical: if \\\\code{TRUE} the result tables are printed in Latex format. The default setting is \\\\code{FALSE}.\\n#\' \\n#\' @export\\n#\' \\n#\' @import stats\\n#\' \\n#\' @author Thilo Klein \\n#\' \\n#\' @keywords summary\\n#\' \\n#\' @references Klein, T. (2015a). \\\\href{https://ideas.repec.org/p/cam/camdae/1521.html}{Does Anti-Diversification Pay? A One-Sided Matching Model of Microcredit}.\\n#\' \\\\emph{Cambridge Working Papers in Economics}, #1521.\\n#\' \\n#\' @examples\\n#\' ## 1. load results from Klein (2015a)\\n#\'  data(klein15a)\\n#\' \\n#\' ## 2. apply mfx function and print results\\n#\'  mfx(m=klein15a)\\nmfx <- function(m,toLatex=FALSE){\\n  \\n  if(!is.null(m$coefs$alpha)){ ## Selectiom and Outcome Eqns\\n\\n    ## model matrix\\n    X <- do.call(rbind.data.frame, m$model.list$X)\\n    eta <- c(m$coefs$eta, rep(0, length(m$model.list$X)-length(m$model.list$W)))\\n    X <- as.matrix(data.frame(X=X,eta=eta))\\n    \\n    ## valuation equation\\n    nrowX <- dim(do.call(rbind.data.frame, m$model.list$W))[1]\\n    sel <- mfxVal(postmean=m$coefs$alpha[,1], poststd=m$coefs$alpha[,2],\\n           nrowX=nrowX, toLatex=toLatex)\\n    \\n    ## structral model outcome\\n    out <- mfxOut(sims=10000, postmean=unlist(c(m$coefs$beta[,1], data.frame(delta=m$coefs$delta[1]))),\\n           poststd=c(m$coefs$beta[,2], m$coefs$delta[2]), X=X, toLatex=toLatex)\\n\\n    return(list(mfx.selection=sel, mfx.outcome=out))\\n\\n  } else{ ## Outcome Eqn only\\n    \\n    ## model matrix\\n    X <- do.call(rbind.data.frame, m$model.list$X)\\n    X <- as.matrix(X)\\n    \\n    ## model outcome    \\n    out <- mfxOut(sims=10000, postmean=m$coefs$beta[,1],\\n           poststd=m$coefs$beta[,2], X=X, toLatex=toLatex)\\n    \\n    return(list(mfx.outcome=out))\\n  }\\n}\\n\\n\\nmfxOut <- function(sims=10000,x.mean=TRUE,postmean,poststd,X,toLatex){\\n  ## source: http://researchrepository.ucd.ie/handle/10197/3404\\n  ## method: average of individual marginal effects at each observation\\n  ## interpretation: http://www.indiana.edu/~statmath/stat/all/cdvm/cdvm.pdf page 8\\n  set.seed(1984)\\n  if(x.mean==TRUE){\\n    ## marginal effects are calculated at the means of independent variables\\n    pdf <- dnorm(mean(X%*%postmean))\\n    pdfsd <- dnorm(sd(X%*%postmean))\\n  } else{\\n    ## marginal effects are calculated for each observation and then averaged\\n    pdf <- mean(dnorm(X%*%postmean))\\n    pdfsd <- sd(dnorm(X%*%postmean))\\n  }  \\n  mx <- pdf*postmean\\n\\n  sim <- matrix(rep(NA,sims*length(postmean)), nrow=sims)\\n  for(i in 1:length(postmean)){\\n    sim[,i] <- rnorm(sims,postmean[i],poststd[i])\\n  }\\n  pdfsim <- rnorm(sims,pdf,pdfsd)\\n  sim.se <- pdfsim*sim\\n  s.e. <- apply(sim.se,2,sd)\\n\\n  t.stat <- mx/s.e.\\n  p.val <- pt(-abs(t.stat),df=dim(X)[1]-length(postmean)+1)\\n  stars <- ifelse(p.val<0.001,\\"***\\",ifelse(p.val<0.01,\\"**\\",ifelse(p.val<0.05,\\"*\\",ifelse(p.val<0.10,\\".\\",\\"\\"))))\\n  if(toLatex==FALSE){\\n    res <- data.frame(round(cbind(mx, s.e., t.stat, p.val),3), stars)\\n  } else{\\n      sign <- ifelse(mx>0,\\"~\\",\\"\\")\\n      res <- data.frame(\\"&\\", sign, round(mx,3), se=paste(paste(\\"(\\",round(s.e.,3),sep=\\"\\"),\\")\\",sep=\\"\\"), stars, \\"\\\\\\\\\\")\\n  }\\n  return(res)\\n}\\n\\n\\nmfxVal <- function(postmean,poststd,nrowX,toLatex){\\n\\n  ## Reference: Sorensen (2007, p. 2748)\\n\\n  mx <- dnorm(0)*postmean/sqrt(2)\\n  s.e. <- dnorm(0)*poststd/sqrt(2)\\n  t.stat <- mx/s.e.\\n  p.val <- pt(-abs(t.stat),df=nrowX-length(postmean))\\n  stars <- ifelse(p.val<0.001,\\"***\\",ifelse(p.val<0.01,\\"**\\",ifelse(p.val<0.05,\\"*\\",ifelse(p.val<0.10,\\".\\",\\"\\"))))\\n  if(toLatex==FALSE){\\n    res <- data.frame( round(cbind(mx, s.e., t.stat, p.val),3), stars)\\n  } else{\\n    sign <- ifelse(mx>0,\\"~\\",\\"\\")\\n    res <- data.frame( \\"&\\", sign, round(mx,3), se=paste(paste(\\"(\\",round(s.e.,3),sep=\\"\\"),\\")\\",sep=\\"\\"), stars, \\"\\\\\\\\\\")\\n  }\\n  return(res)\\n}\\n" } \n'
line: b'{ "repo_name": "jbkunst/r-posts", "ref": "refs/heads/master", "path": "015-www-working-with-weather/translation_data.R", "content": "# Data from \'https://centroccbb.cl/clima/indexData.php\'\\nrm(list = ls())\\n\\nlibrary(\\"rio\\")\\nlibrary(\\"dplyr\\")\\nlibrary(\\"rvest\\")\\n\\n\\ndata <- import(\\"Informe parcial.xls\\")\\n\\ndata <- data %>% tbl_df()\\n\\nnames(data)\\n\\n\\n# names(data) <- c(\\"date\\", \\"time\\", \\"temperature\\", \\"humidity\\", \\"pressure\\", \\"precipitation\\",\\n#                  \\"solar_radiation\\", \\"wind_speed\\", \\"wind_direction\\",\\n#                  \\"chill\\", \\"Temperature_flushing\\", \\"htw_index\\", \\"thws_index\\", \\"dew_point\\",\\n#                  \\"rainfall_rate\\", \\"air_density\\")\\n\\nnames(data) <- c(\\"date\\", \\"time\\", \\"temperature\\", \\"humidity\\", \\"pressure\\", \\"precipitation\\",\\n                 \\"solar_radiation\\", \\"wind_speed\\", \\"wind_direction\\",\\n                 \\"thermal_sensation\\", \\"rainfall_rate\\")\\n\\nnames(data)\\n\\ndata\\n\\nstr(data)\\n\\nsave(data, file = \\"wheather_data.RData\\")\\n" }\n'
line: b'{ "repo_name": "jbkunst/r-posts", "ref": "refs/heads/master", "path": "052-motion-piramid/readme.R", "content": "#\' ---\\r\\n#\' title: \\"Motion Piramid\\"\\r\\n#\' author: \\"Joshua Kunst\\"\\r\\n#\' output:\\r\\n#\'  html_document:\\r\\n#\'    toc: true\\r\\n#\'    keep_md: yes\\r\\n#\' ---\\r\\n\\r\\n#+ echo=FALSE, message=FALSE, warning=FALSE\\r\\nrm(list = ls())\\r\\nknitr::opts_chunk$set(message = FALSE, warning = FALSE,\\r\\n                      fig.showtext = TRUE, dev = \\"CairoPNG\\")\\r\\n\\r\\n#\'\\r\\nlibrary(idbr)\\r\\nlibrary(dplyr)\\r\\nlibrary(purrr)\\r\\nlibrary(highcharter)\\r\\nrm(list = ls())\\r\\n\\r\\nidb_api_key(\\"35f116582d5a89d11a47c7ffbfc2ba309133f09d\\")\\r\\n\\r\\nyrs <-  seq(1980, 2030, by = 1)\\r\\n\\r\\ndf <- map_df(c(\\"male\\", \\"female\\"), function(sex){\\r\\n  idb1(\\"US\\", yrs, sex = sex) %>%\\r\\n    mutate(sex_label = sex)\\r\\n})\\r\\n\\r\\nnames(df) <- tolower(names(df))\\r\\n\\r\\nstr(df)\\r\\nhead(df) \\r\\n\\r\\ndf <- df %>%\\r\\n  mutate(population = pop*ifelse(sex_label == \\"male\\", -1, 1))\\r\\n\\r\\nseries <- df %>% \\r\\n  group_by(sex_label, age) %>% \\r\\n  do(data = list(sequence = .$population)) %>% \\r\\n  ungroup() %>% \\r\\n  group_by(sex_label) %>% \\r\\n  do(data = .$data) %>%\\r\\n  mutate(name = sex_label) %>% \\r\\n  list.parse3()\\r\\n\\r\\nmaxpop <- max(abs(df$population))\\r\\n\\r\\nxaxis <- list(categories = sort(unique(df$age)),\\r\\n              reversed = FALSE, tickInterval = 5,\\r\\n              labels = list(step = 5))\\r\\n\\r\\nhighchart() %>%\\r\\n  hc_chart(type = \\"bar\\") %>%\\r\\n  hc_motion(enabled = TRUE, labels = yrs, series = c(0,1), autoplay = TRUE, updateInterval = 1) %>% \\r\\n  hc_add_series_list(series) %>% \\r\\n  hc_plotOptions(\\r\\n    series = list(stacking = \\"normal\\"),\\r\\n    bar = list(groupPadding = 0, pointPadding =  0, borderWidth = 0)\\r\\n  ) %>% \\r\\n  hc_tooltip(shared = TRUE) %>% \\r\\n  hc_yAxis(\\r\\n    labels = list(\\r\\n      formatter = JS(\\"function(){ return Math.abs(this.value) / 1000000 + \'M\'; }\\") \\r\\n    ),\\r\\n    tickInterval = 0.5e6,\\r\\n    min = -maxpop,\\r\\n    max = maxpop) %>% \\r\\n  hc_xAxis(\\r\\n    xaxis,\\r\\n    rlist::list.merge(xaxis, list(opposite = TRUE, linkedTo = 0))\\r\\n  ) %>% \\r\\n  hc_tooltip(shared = FALSE,\\r\\n             formatter = JS(\\"function () { return \'<b>\' + this.series.name + \', age \' + this.point.category + \'</b><br/>\' + \'Population: \' + Highcharts.numberFormat(Math.abs(this.point.y), 0);}\\")\\r\\n  ) %>% \\r\\n  hc_add_theme(hc_theme_smpl())\\r\\n\\r\\n  \\r\\n" }\n'
line: b'{ "repo_name": "jbkunst/r-posts", "ref": "refs/heads/master", "path": "001-GTFS-Transantiago-ggplot2/script/script_02.R", "content": "# Source\\n# https://developers.google.com/transit/gtfs/reference\\nrm(list=ls())\\noptions(stringsAsFactors=FALSE)\\n\\nlibrary(devtools)\\nlibrary(ggplot2)\\nlibrary(plyr)\\nlibrary(dplyr)\\n\\nsource_url(\\"https://raw.githubusercontent.com/jbkunst/reuse/master/R/gg_themes.R\\")\\n\\nfile_temp <- tempfile(fileext = \\".zip\\")\\n\\n# http://www.mbta.com/rider_tools/developers/default.asp?id=21895\\ndownload.file(\\"http://www.mbta.com/uploadedfiles/MBTA_GTFS.zip\\", file_temp)\\n\\ndata <- read.table()\\n\\nshapes <- read.csv(unz(file_temp, \\"shapes.txt\\"))\\nroutes <- read.csv(unz(file_temp, \\"routes.txt\\"))\\ntrips <- read.csv(unz(file_temp, \\"trips.txt\\"))\\nstops <- read.csv(unz(file_temp, \\"stops.txt\\"))\\n\\nhead(routes)\\ntail(routes)\\nhead(shapes)\\n\\nunlink(file_temp)\\n\\n\\np <- ggplot(shapes) +\\n  geom_path(aes(shape_pt_lon, shape_pt_lat, group=shape_id), size=.2, alpha=.1) +\\n  xlim(-71.2, -71) + ylim(42.0,42.6) +\\n  coord_equal()\\n  \\np\\n\\n\\nstops_metro <- stops %>% filter(!grepl(\\"\\\\\\\\d\\", stop_id))\\nshapes_colors <- left_join(left_join(shapes %>% select(shape_id) %>% unique(),\\n                                     trips %>% select(shape_id, route_id) %>% unique()),\\n                           routes %>% select(route_id, route_color) %>% unique())\\nshapes_colors <- shapes_colors  %>% mutate(route_color = paste0(\\"#\\", route_color))\\nroutes_metro <- routes %>% filter(grepl(\\"^L\\\\\\\\d\\",route_id))\\nshapes_metro <- shapes %>% filter(shape_id %in% trips$shape_id[trips$route_id %in% routes_metro$route_id])\\nshapes_colors_metro <- shapes_colors %>% filter(shape_id %in% trips$shape_id[trips$route_id %in% routes_metro$route_id])\\n\\np2 <- ggplot() +\\n  geom_path(data=shapes, aes(shape_pt_lon, shape_pt_lat, group=shape_id), color=\\"white\\", size=.2, alpha=.1) +\\n  geom_path(data=shapes_metro, aes(shape_pt_lon, shape_pt_lat, group=shape_id, colour=shape_id), size = 2, alpha=.7) +\\n  scale_color_manual(values=shapes_colors_metro$route_color) +\\n  geom_point(data=stops_metro, aes(stop_lon, stop_lat), shape=21, colour=\\"white\\", alpha =.8) +\\n  coord_equal() +\\n  theme_null() +\\n  theme(plot.background = element_rect(fill = \\"black\\", colour = \\"black\\"),\\n        title = element_text(hjust=1, colour=\\"white\\")) +\\n  ggtitle(\\"TRANSANTIAGO\\\\nSantiago\'s public transport system\\")\\np2\\n" }\n'
line: b'{ "repo_name": "jbkunst/r-posts", "ref": "refs/heads/master", "path": "025-stackoverflow/get-github-language-colors.R", "content": "library(\\"httr\\")\\r\\ndfcols <- \\"https://raw.githubusercontent.com/doda/github-language-colors/master/colors.json\\" %>% \\r\\n  GET() %>%\\r\\n  content() %>% \\r\\n  jsonlite::fromJSON() %>% \\r\\n  { data_frame(language = tolower(names(.)),\\r\\n               color = unlist(.)) }\\r\\n\\r\\ndfcols\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "jbkunst/r-posts", "ref": "refs/heads/master", "path": "018-riv-woe-package/readme.R", "content": "#\' ---\\n#\' title: \\"RIV WOE Package\\"\\n#\' author: \\"Joshua Kunst\\"\\n#\' output: \\n#\'  html_document: \\n#\'    keep_md: yes\\n#\' ---\\n\\n#+ fig.width=10, fig.height=5\\n#+ echo=FALSE\\n# library(\\"printr\\")\\nlibrary(\\"knitr\\")\\noptions(digits = 3, knitr.table.format = \\"markdown\\")\\nknitr::opts_chunk$set(collapse = TRUE, comment = \\">\\", warning = FALSE,\\n                      fig.width = 10, fig.height = 6,\\n                      fig.align = \\"center\\", dpi = 72)\\n\\n#\' # Introducction\\n#\' \\n#\' - woe is data frame oriented, the functions have always a data frame argument.\\n#\' - riskr is variable oriented, the functions have always a variable (non dataframe) argument.\\n\\n#\' # Load packages and data\\n# devtools::install_github(\\"tomasgreif/woe\\")\\nlibrary(\\"woe\\")\\nlibrary(\\"riskr\\")\\n\\ndata(\\"german_data\\")\\n\\n#\' Required for riskr\\ngerman_data$gb2 <- ifelse(german_data$gb == \\"good\\", 1, 0)\\n\\n#\' # Bivariate analysis:\\n#\' \\n#\' ## 1 variable case\\n\\nlvls <- names(sort(table(german_data$purpose)))\\ngerman_data$purpose <- factor(as.character(german_data$purpose), levels = lvls)\\n\\n#\' ### woe \\niv.str(german_data,\\"purpose\\",\\"gb\\", verbose = FALSE)\\niv.plot.woe(iv.mult(german_data,\\"gb\\",vars = c(\\"purpose\\"), summary = FALSE))\\n\\n#\' ### riskr\\nbt(german_data$purpose, german_data$gb2)\\n\\nplot_ba(german_data$purpose, german_data$gb2) +\\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0))\\n  \\n\\n#\' ## data frame case\\n#\' \\n#\' ### woe\\niv.mult(german_data, \\"gb\\", vars =  c(\\"purpose\\", \\"ca_status\\", \\"credit_history\\", \\"status_sex\\"),\\n        verbose = FALSE)\\n\\n#\' ### riskr\\nlibrary(\\"tidyr\\")\\nlibrary(\\"dplyr\\")\\n\\ngerman_data %>% \\n  tbl_df() %>% \\n  select(gb2, purpose, ca_status, credit_history, status_sex) %>% \\n  gather(variable, value, -gb2) %>% \\n  group_by(variable) %>% \\n  do({ bt(.$value, .$gb2)  })\\n\\n\\n# iv.num(german_data,\\"duration\\",\\"gb\\")\\n# iv.mult(german_data,\\"gb\\",TRUE, verbose = TRUE)\\n# iv.plot.summary(iv.mult(german_data,\\"gb\\",TRUE))\\n# head(german_data$credit_amount)\\n# iv.binning.simple(german_data,\\"credit_amount\\")\\n" }\n'
line: b'{ "repo_name": "jbkunst/r-posts", "ref": "refs/heads/master", "path": "036-ezsummary-tests/readme.R", "content": "#\' ---\\r\\n#\' title: \\"\\"\\r\\n#\' author: \\"Joshua Kunst\\"\\r\\n#\' output:\\r\\n#\'  html_document:\\r\\n#\'    toc: true\\r\\n#\'    keep_md: yes\\r\\n#\' ---\\r\\n\\r\\n#+echo=FALSE\\r\\nknitr::opts_chunk$set(warning=FALSE)\\r\\n\\r\\n#+echo=TRUE\\r\\nlibrary(\\"dplyr\\")\\r\\nlibrary(\\"ezsummary\\")\\r\\n\\r\\ndata(diamonds, package = \\"ggplot2\\")\\r\\ntbl <- diamonds\\r\\n\\r\\nselect_categorical <- function(tbl, nuniques = 10){\\r\\n  \\r\\n  selections <- purrr::map_lgl(tbl, function(x) {\\r\\n    is.character(x) ||\\r\\n      is.factor(x) ||\\r\\n      is.logical(x) ||\\r\\n      (length(unique(x)) <= nuniques) # this is when you have numeric variables with few uniques (dummies)\\r\\n  })\\r\\n  tbl[, selections]\\r\\n  \\r\\n}\\r\\n\\r\\nselect_quantitative <- function(tbl){\\r\\n  \\r\\n  selections <- purrr::map_lgl(tbl, function(x) is.numeric(x))\\r\\n  tbl[, selections]\\r\\n  \\r\\n}\\r\\n\\r\\ndiamonds %>% \\r\\n  select_categorical() \\r\\n\\r\\ndiamonds %>% \\r\\n  select_quantitative()\\r\\n\\r\\n\\r\\n#### ####\\r\\n\\r\\n# tbl <- diamonds %>% select_categorical() %>%  group_by(cut)\\r\\n# tbl <- diamonds %>% select_categorical() %>%  group_by(cut, color)\\r\\n\\r\\nezsummary_categorical2 <- function(tbl){\\r\\n  \\r\\n  grp_cols <- names(attr(tbl, \\"labels\\"))\\r\\n  \\r\\n  tbl %>%\\r\\n    purrr::map_if(is.factor, as.character) %>% # avoid warning\\r\\n    as_data_frame() %>% # this ungroup the tbl\\r\\n    group_by_(.dots = lapply(grp_cols, as.symbol)) %>%  # http://stackoverflow.com/questions/21208801/\\r\\n    do({ezsum = \\r\\n      tidyr::gather(., key, value) %>% # you can use tidyr::gather(., variable, category)\\r\\n      ungroup() %>%\\r\\n      count(key, value) %>% \\r\\n      mutate(p = n/sum(n))\\r\\n    }) %>% \\r\\n    # ungroup() %>%\\r\\n    filter(!key %in% grp_cols) # not sure whiy appear as key a group col\\r\\n \\r\\n}\\r\\n\\r\\ndiamonds %>%\\r\\n  select_categorical() %>% \\r\\n  ezsummary_categorical() \\r\\n\\r\\ndiamonds %>%\\r\\n  select_categorical() %>% \\r\\n  ezsummary_categorical2() \\r\\n\\r\\nlibrary(rbenchmark)\\r\\n\\r\\ntblcat <- diamonds %>% select_categorical()\\r\\ntblcat <- diamonds %>% select_categorical() %>%  group_by(cut, color)\\r\\n\\r\\nbenchmark(\\r\\n  ezsummary_categorical(tblcat),\\r\\n  ezsummary_categorical2(tblcat),\\r\\n  replications = 100\\r\\n)\\r\\n# Mmm not so fast\\r\\n\\r\\nt1 <- diamonds %>%\\r\\n  select_categorical() %>% \\r\\n  ezsummary_categorical2() \\r\\n\\r\\nt1\\r\\n\\r\\n# check how much groups are \\r\\nsum(t1$p) == nrow(distinct(t1, key))\\r\\n\\r\\nt2 <- diamonds %>%\\r\\n  select_categorical() %>% \\r\\n  group_by(cut, color) %>% \\r\\n  ezsummary_categorical2()\\r\\n\\r\\nt2\\r\\n\\r\\n# check how much groups are \\r\\nsum(t2$p) == nrow(distinct(t2, cut, color, key))\\r\\n\\r\\nmtcars %>% \\r\\n  select_categorical(nuniques = 2) %>% \\r\\n  ezsummary_categorical2()\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n" }\n'
line: b'{ "repo_name": "greenelab/TDM", "ref": "refs/heads/master", "path": "R/package_loader.R", "content": "#\' Loads a character vector of packages, installing them from Bioconductor if necessary.\\n#\' This will also load most CRAN packages.\\n#\'  \\n#\' @title load_it\\n#\' @name load_it\\n#\' @param pack -- A vector of character strings containing package names.\\n#\' @return nothing\\n#\' @export\\n#\'\\nload_it = function(pack, update=FALSE) {\\n\\tinvisible(sapply(pack, function(package) {\\n\\t\\tif(!require(package, quietly=T, character.only=T)){\\n\\t\\t\\t\\tif(length(find(\\"biocLite\\")) < 1) {\\n\\t\\t\\t\\t\\t\\tsource(\\"http://bioconductor.org/biocLite.R\\")\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tif(update) {\\n\\t\\t\\t\\t\\t\\tbiocLite(package)\\t\\n\\t\\t\\t\\t} else {\\n\\t\\t\\t\\t\\t\\tbiocLite(package, suppressUpdates=TRUE)\\n\\t\\t\\t\\t}\\n\\n\\t\\t\\t\\tlibrary(package, quietly=T, character.only=T)\\n\\t\\t\\t}\\n\\t\\t}))\\n} #end load_it" }\n'
line: b'{ "repo_name": "PoisonAlien/maftools", "ref": "refs/heads/master", "path": "R/oncodrive.R", "content": "#\' Detect cancer driver genes based on positional clustering of variants.\\n#\'\\n#\' @description Clusters variants based on their position to detect disease causing genes.\\n#\' @details This is the re-implimentation of algorithm defined in OncodriveCLUST article. Concept is based on the fact that most of the variants in cancer causing genes are enriched at few specific loci (aka hotspots).\\n#\' This method takes advantage of such positions to identify cancer genes. Cluster score of 1 means, a single hotspot hosts all observed variants. If you use this function, please cite OncodriveCLUST article.\\n#\' @references Tamborero D, Gonzalez-Perez A and Lopez-Bigas N. OncodriveCLUST: exploiting the positional clustering of somatic mutations to identify cancer genes. Bioinformatics. 2013; doi: 10.1093/bioinformatics/btt395s\\n#\' @param maf an \\\\code{\\\\link{MAF}} object generated by \\\\code{\\\\link{read.maf}}\\n#\' @param AACol manually specify column name for amino acid changes. Default looks for field \'AAChange\'\\n#\' @param pvalMethod either zscore (default method for oncodriveCLUST), poisson or combined (uses lowest of the two pvalues).\\n#\' @param nBgGenes minimum number of genes required to estimate background score. Default 100. Do not change this unless its necessary.\\n#\' @param minMut minimum number of mutations required for a gene to be included in analysis. Default 5.\\n#\' @param bgEstimate If FALSE skips background estimation from synonymous variants and uses predifined values estimated from COSMIC synonymous variants.\\n#\' @param ignoreGenes Ignore these genes from analysis. Default NULL. Helpful in case data contains large number of variants belonging to polymorphic genes such as mucins and TTN.\\n#\' @return data table of genes ordered according to p-values.\\n#\' @seealso \\\\code{\\\\link{plotOncodrive}}\\n#\' @examples\\n#\'\\n#\' laml.maf <- system.file(\\"extdata\\", \\"tcga_laml.maf.gz\\", package = \\"maftools\\")\\n#\' laml <- read.maf(maf = laml.maf, removeSilent = TRUE, useAll = FALSE)\\n#\' laml.sig <- oncodrive(maf = laml, AACol = \'Protein_Change\', minMut = 5)\\n#\'\\n#\'\\n#\' @importFrom dplyr filter\\n#\' @export\\n\\n\\noncodrive = function(maf, AACol = NULL, minMut = 5, pvalMethod = \'zscore\', nBgGenes = 100, bgEstimate = TRUE, ignoreGenes = NULL){\\n\\n  #Proetin Length source\\n  gl = system.file(\'extdata\', \'prot_len.txt.gz\', package = \'maftools\')\\n\\n  if(Sys.info()[[\'sysname\']] == \'Windows\'){\\n    gl.gz = gzfile(description = gl, open = \'r\')\\n    gl <- suppressWarnings( data.table(read.csv( file = gl.gz, header = TRUE, sep = \'\\\\t\', stringsAsFactors = FALSE)) )\\n    close(gl.gz)\\n  } else{\\n    gl = fread(input = paste(\'zcat <\', gl), sep = \'\\\\t\', stringsAsFactors = FALSE)\\n  }\\n\\n  pval.options = c(\'zscore\', \'poisson\', \'combined\')\\n\\n  if(!pvalMethod %in% pval.options){\\n    stop(\'pvalMethod can only be either zscore, poisson or combined\')\\n  }\\n\\n  if(length(pvalMethod) > 1){\\n    stop(\'pvalMethod can only be either zscore, poisson or combined\')\\n  }\\n\\n\\n  #syn variants for background\\n  syn.maf = maf@maf.silent\\n  #number of samples in maf\\n  numSamples = as.numeric(maf@summary[3,summary])\\n  #Perform clustering and calculate background scores.\\n  if(bgEstimate){\\n    if(nrow(syn.maf) == 0){\\n      message(\'No syn mutations found! Skipping background estimation. Using predefined values. (Mean = 0.279; SD = 0.13)\')\\n      bg.mean = 0.279\\n      bg.sd = 0.13\\n    }else{\\n      message(\'Estimating background scores from synonymous variants..\')\\n      syn.bg.scores = parse_prot(dat = syn.maf, AACol = AACol, gl, m = minMut, calBg = TRUE, nBg = nBgGenes)\\n\\n      #If number of genes to calculate background scores is not enough, use predefined scores.\\n      if(is.null(syn.bg.scores)){\\n        message(\\"Not enough genes to build background. Using predefined values. (Mean = 0.279; SD = 0.13)\\")\\n        bg.mean = 0.279\\n        bg.sd = 0.13\\n      }else {\\n        if(nrow(syn.bg.scores) < nBgGenes){\\n          message(\\"Not enough genes to build background. Using predefined values. (Mean = 0.279; SD = 0.13)\\")\\n          bg.mean = 0.279\\n          bg.sd = 0.13\\n        }else{\\n          bg.mean = mean(syn.bg.scores$clusterScores)\\n          bg.sd = sd(syn.bg.scores$clusterScores)\\n          message(paste(\'Estimated background mean: \', bg.mean))\\n          message(paste(\'Estimated background SD: \', bg.sd))\\n        }\\n      }\\n    }\\n  }else{\\n    message(\\"Using predefined values for background. (Mean = 0.279; SD = 0.13)\\")\\n    bg.mean = 0.279\\n    bg.sd = 0.13\\n  }\\n\\n\\n\\n  #non-syn variants\\n  non.syn.maf = maf@data\\n  #in case user read maf without removing silent variants, remove theme here.\\n  silent = c(\\"3\'UTR\\", \\"5\'UTR\\", \\"3\'Flank\\", \\"Targeted_Region\\", \\"Silent\\", \\"Intron\\",\\n             \\"RNA\\", \\"IGR\\", \\"Splice_Region\\", \\"5\'Flank\\", \\"lincRNA\\")\\n  non.syn.maf = non.syn.maf[!Variant_Classification %in% silent] #Remove silent variants from main table\\n\\n  #Remove genes to ignore\\n  if(!is.null(ignoreGenes)){\\n    ignoreGenes.count = nrow(non.syn.maf[Hugo_Symbol %in% ignoreGenes])\\n    message(paste(\'Removed\', ignoreGenes.count, \'variants belonging to\', paste(ignoreGenes, collapse = \', \', sep=\',\')))\\n    non.syn.maf = non.syn.maf[!Hugo_Symbol %in% ignoreGenes]\\n  }\\n\\n  #Perform clustering and calculate cluster scores for nonsyn variants.\\n  message(\'Estimating cluster scores from non-syn variants..\')\\n  nonsyn.scores = parse_prot(dat = non.syn.maf, AACol = AACol, gl = gl, m = minMut, calBg = FALSE, nBg = nBgGenes)\\n\\n  if(pvalMethod == \'combined\'){\\n    message(\'Comapring with background model and estimating p-values..\')\\n    nonsyn.scores$zscore = (nonsyn.scores$clusterScores - bg.mean) / bg.sd\\n    nonsyn.scores$tPval = 1- pnorm(nonsyn.scores$zscore)\\n    nonsyn.scores$tFdr = p.adjust(nonsyn.scores$tPval, method = \'fdr\')\\n\\n    nonsyn.scores = merge(getGeneSummary(maf), nonsyn.scores, by = \'Hugo_Symbol\')\\n    nonsyn.scores[,fract_muts_in_clusters := muts_in_clusters/total]\\n\\n    counts.glm = glm(formula = total ~ protLen+clusters, family = poisson(link = identity), data = nonsyn.scores) #Poisson model\\n    nonsyn.scores$Expected = counts.glm$fitted.values #Get expected number of events (mutations) from the model\\n\\n    observed_mut_colIndex = which(colnames(nonsyn.scores) == \'total\')\\n    expected_mut_colIndex = which(colnames(nonsyn.scores) == \'Expected\')\\n\\n    #Poisson test to caluclate difference (p-value)\\n    nonsyn.scores$poissonPval = apply(nonsyn.scores, 1, function(x) {\\n      poisson.test(as.numeric(x[observed_mut_colIndex]), as.numeric(x[expected_mut_colIndex]))$p.value\\n    })\\n\\n    nonsyn.scores$poissonFdr = p.adjust(nonsyn.scores$poissonPval)\\n    nonsyn.scores = nonsyn.scores[order(poissonFdr)]\\n\\n    nonsyn.scores$fdr = apply(nonsyn.scores[,.(tFdr, poissonFdr)], MARGIN = 1, FUN = min)\\n\\n  } else if(pvalMethod == \'zscore\'){\\n    #Oncodrive clust way of caluclating pvalues\\n    #Calculate z scores; compare it to bg scores and estimate z-score, pvalues, corrected pvalues (fdr) (assumes normal distribution)\\n    message(\'Comapring with background model and estimating p-values..\')\\n    nonsyn.scores$zscore = (nonsyn.scores$clusterScores - bg.mean) / bg.sd\\n    nonsyn.scores$pval = 1- pnorm(nonsyn.scores$zscore)\\n    nonsyn.scores$fdr = p.adjust(nonsyn.scores$pval, method = \'fdr\')\\n\\n    nonsyn.scores = merge(getGeneSummary(maf), nonsyn.scores, by = \'Hugo_Symbol\')\\n    nonsyn.scores[,fract_muts_in_clusters := muts_in_clusters/total]\\n    #nonsyn.scores[,fract_MutatedSamples := MutatedSamples/numSamples]\\n    nonsyn.scores = nonsyn.scores[order(fdr)]\\n  }else{\\n    #Assuming poisson distribution of mutation counts\\n    #Now model observed number of mutations as a function of number of clusters and protein length. Calculate expected number of events based on poisson distribution.\\n    nonsyn.scores = merge(getGeneSummary(maf), nonsyn.scores, by = \'Hugo_Symbol\')\\n    nonsyn.scores[,fract_muts_in_clusters := muts_in_clusters/total]\\n\\n    counts.glm = glm(formula = total ~ protLen+clusters, family = poisson(link = identity), data = nonsyn.scores) #Poisson model\\n    nonsyn.scores$Expected = counts.glm$fitted.values #Get expected number of events (mutations) from the model\\n\\n    observed_mut_colIndex = which(colnames(nonsyn.scores) == \'total\')\\n    expected_mut_colIndex = which(colnames(nonsyn.scores) == \'Expected\')\\n\\n    #Poisson test to caluclate difference (p-value)\\n    nonsyn.scores$pval = apply(nonsyn.scores, 1, function(x) {\\n      poisson.test(as.numeric(x[observed_mut_colIndex]), as.numeric(x[expected_mut_colIndex]))$p.value\\n    })\\n\\n    nonsyn.scores$fdr = p.adjust(nonsyn.scores$pval)\\n    nonsyn.scores = nonsyn.scores[order(fdr)]\\n  }\\n  message(\'Done !\')\\n  return(nonsyn.scores)\\n}\\n" }\n'
line: b'{ "repo_name": "PoisonAlien/maftools", "ref": "refs/heads/master", "path": "R/titv.R", "content": "#\' Classifies SNPs into transitions and transversions\\n#\' @description takes output generated by read.maf and classifies Single Nucleotide Variants into Transitions and Transversions.\\n#\'\\n#\' @param maf an \\\\code{\\\\link{MAF}} object generated by \\\\code{\\\\link{read.maf}}\\n#\' @param useSyn Logical. Whether to include synonymous variants in analysis. Defaults to FALSE.\\n#\' @param plot plots a titv fractions. default TRUE.\\n#\' @param file basename for output file name. If given writes summaries to output file. Default NULL.\\n#\' @return list of \\\\code{data.frame}s with Transitions and Transversions summary.\\n#\' @seealso  \\\\code{\\\\link{plotTiTv}}\\n#\' @examples\\n#\' laml.maf <- system.file(\\"extdata\\", \\"tcga_laml.maf.gz\\", package = \\"maftools\\")\\n#\' laml <- read.maf(maf = laml.maf, removeSilent = TRUE, useAll = FALSE)\\n#\' laml.titv = titv(maf = laml, useSyn = TRUE)\\n#\'\\n#\' @export\\n\\n\\ntitv = function(maf, useSyn = FALSE, plot = TRUE, file = NULL)\\n{\\n\\n  #Synonymous variants\\n  maf.silent = maf@maf.silent\\n  #Main data\\n  maf = maf@data\\n\\n  #in case user read maf without removing silent variants, remove theme here.\\n  silent = c(\\"3\'UTR\\", \\"5\'UTR\\", \\"3\'Flank\\", \\"Targeted_Region\\", \\"Silent\\", \\"Intron\\",\\n             \\"RNA\\", \\"IGR\\", \\"Splice_Region\\", \\"5\'Flank\\", \\"lincRNA\\")\\n  maf = maf[!Variant_Classification %in% silent] #Remove silent variants from main table\\n\\n  if(useSyn){\\n    maf = rbind(maf, maf.silent, fill = TRUE)\\n  }\\n\\n  maf = maf[Variant_Type == \'SNP\']\\n\\n  #Some TCGA studies have Start_Position set to as \'position\'. Change if so.\\n  if(length(grep(pattern = \'Start_position\', x = colnames(maf))) > 0){\\n    colnames(maf)[which(colnames(maf) == \'Start_position\')] = \'Start_Position\'\\n  }\\n\\n  if(length(grep(pattern = \'End_position\', x = colnames(maf))) > 0){\\n    colnames(maf)[which(colnames(maf) == \'End_position\')] = \'End_Position\'\\n  }\\n\\n  maf = maf[,.(Hugo_Symbol, Start_Position, End_Position, Reference_Allele, Tumor_Seq_Allele2, Tumor_Sample_Barcode)]\\n  maf$con = paste(maf[,Reference_Allele], maf[,Tumor_Seq_Allele2], sep = \'-\')\\n\\n  maf.con.summary = maf[,.N, by = .(Tumor_Sample_Barcode, con)]\\n  maf.con.summary$con.class = suppressWarnings(as.character(factor(maf.con.summary$con, levels = c(\\"A-G\\", \\"T-C\\", \\"C-T\\", \\"G-A\\", \\"A-T\\", \\"T-A\\", \\"A-C\\", \\"T-G\\", \\"C-A\\", \\"G-T\\", \\"C-G\\", \\"G-C\\"),\\n                                                                   labels = c(\\"A-G\\", \\"A-G\\", \\"C-T\\", \\"C-T\\", \\"A-T\\", \\"A-T\\", \\"A-C\\", \\"A-C\\", \\"C-A\\", \\"C-A\\", \\"C-G\\", \\"C-G\\"))))\\n\\n\\n  maf.con.class.summary = maf.con.summary[,sum(N), by = .(Tumor_Sample_Barcode, con.class)]\\n  colnames(maf.con.class.summary)[ncol(maf.con.class.summary)] = \'nVars\'\\n  suppressWarnings(maf.con.class.summary[,fract := (nVars/sum(nVars))*100, by = .(Tumor_Sample_Barcode)])\\n\\n  maf.con.class.summary$con.class = factor(x = maf.con.class.summary$con.class,\\n                                           levels = c(\\"A-G\\", \\"C-T\\", \\"A-T\\", \\"A-C\\", \\"C-A\\", \\"C-G\\"))\\n  maf.con.class.summary$TiTv = suppressWarnings(as.character(factor(x = maf.con.class.summary$con.class,\\n                                                                    levels = c(\\"A-G\\", \\"C-T\\", \\"A-T\\", \\"A-C\\", \\"C-A\\", \\"C-G\\"), labels = c(\'Ti\', \'Ti\', \'Tv\', \'Tv\', \'Tv\', \'Tv\'))))\\n\\n  fract.classes = data.table::dcast(data = maf.con.class.summary, formula = Tumor_Sample_Barcode ~ con.class, value.var = \'fract\', fill = 0)\\n  raw.classes = data.table::dcast(data = maf.con.class.summary, formula = Tumor_Sample_Barcode ~ con.class, value.var = \'nVars\', fill = 0)\\n  titv.summary = maf.con.class.summary[,sum(fract), by = .(Tumor_Sample_Barcode, TiTv)]\\n  titv.summary = data.table::dcast(data = titv.summary, Tumor_Sample_Barcode ~ TiTv, value.var = \'V1\', fill = 0)\\n\\n  titv.res = list(fraction.contribution = fract.classes, raw.counts = raw.classes, TiTv.fractions = titv.summary)\\n\\n  if(plot){\\n    plotTiTv(res = titv.res)\\n  }\\n\\n  if(!is.null(file)){\\n    write.table(x = fract.classes,file = paste(file, \'_fraction_contribution.txt\', sep = \'\'), quote = FALSE, row.names = FALSE, sep = \'\\\\t\')\\n    write.table(x = raw.classes,file = paste(file, \'_fraction_counts.txt\', sep = \'\'), quote = FALSE, row.names = FALSE, sep = \'\\\\t\')\\n    write.table(x = titv.summary,file = paste(file, \'_TiTv_fractions.txt\', sep = \'\'), quote = FALSE, row.names = FALSE, sep = \'\\\\t\')\\n  }\\n\\n  return(titv.res)\\n}\\n" }\n'
line: b'{ "repo_name": "joyent/Rbunyan", "ref": "refs/heads/master", "path": "R/bunyanTracebackErrors.R", "content": "# Roxygen Comments bunyanTracebackErrors\\n#\' Count of errors above level threshold of 50 after setpoint\\n#\'\\n#\' Returns the number of ERROR/FATAL log messages\\n#\' encountered since bunyanSetpoint first called. Note that\\n#\' only the first call to bunyanSetpoint is used, subsequent\\n#\' calls are ignored. Use bunyanClearSetpoint to clear before\\n#\' setting a new setpoint.\\n#\'\\n#\'\\n#\' @keywords bunyan, setpoint\\n#\'\\n#\' @export\\nbunyanTracebackErrors <-\\nfunction() {\\n    return(bunyan_globals$errorssincemark)\\n}\\n" }\n'
line: b'{ "repo_name": "henkelstone/NPEL.Classification", "ref": "refs/heads/master", "path": "data-raw/constants.R", "content": "# Create package constants\\n\\n##### Scope Constants #####\\nsuppModels <- c(\'randomForest\',\'rfsrc\',\'fnn.FNN\',\'fnn.class\',\'kknn\',\'gbm\')          # Which packages (model classes) are supported by NPEL.Classification\\nprobModels <- c(\'randomForest\',\'rfsrc\',\'fnn.FNN\',\'fnn.class\',\'kknn\',\'gbm\')          # Which packages generate probabilities when passed categorical data\\ncontModels <- c(\'randomForest\',\'rfsrc\',\'kknn\',\'gbm\')                                # Which packages are able to handle continuous data }\\n\\n##### Generate Ecotype Groupings #####\\n# Create a shell to fill with data\\necoGroup <- list(); length(ecoGroup) <- 12;\\ndim(ecoGroup) <- c(4,3)                                                   # Three rows (different classifiers), and three columns (type of data we could extract)\\nrownames(ecoGroup)=c(\'identity\',\'domSpecies\',\'maxGranularity\',\'domGroup\') # Rows are different grouping scenarios\\ncolnames(ecoGroup)=c(\'transform\',\'labels\',\'colours\')                      # Cols are different things we could look up\\n\\n# Fill with data\\necoGroup[\'identity\',] <- list(c(1:27), paste0(\'BS\',1:27), 1:27)\\necoGroup[\'domSpecies\',] <- list( c(1,1,2,2,2,2,3,3,3,3,4,4,5,5,6,7,8,8,8,8,9,9,9,9,9,10,10),\\n                                 c(\'1\'=\\"Barren\\",\'2\'=\\"Pine\\",\'3\'=\\"Black Sp\\",\'4\'=\\"White Sp\\",\'5\'=\\"Birch\\",\'6\'=\\"Aspen\\",\'7\'=\\"Swamp\\",\'8\'=\\"Bog\\",\'9\'=\\"Fen\\",\'10\'=\\"Shore\\"),\\n                                 c(\'1\'=\\"gray\\",\'2\'=\\"tan1\\",\'3\'=\\"green4\\",\'4\'=\\"seagreen\\",\'5\'=\\"green3\\",\'6\'=\\"green2\\",\'7\'=\\"steelblue\\",\'8\'=\\"steelblue2\\",\'9\'=\\"steelblue4\\",\'10\'=\\"azure\\") )\\necoGroup[\'maxGranularity\',] <- list( c(1,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,15,16,17,17,18,18,19,20,20,21,21), paste0(\'BS\',1:21), 1:21)\\necoGroup[\'domGroup\',] <- list( c(1,1,2,2,2,2,2,2,2,2,2,2,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4),\\n                               c(\\"Barren\\",\\"Conifer\\",\\"Decid\\",\\"Wetland\\"),\\n                               c(\'1\'=\\"gray\\",\'2\'=\\"green4\\",\'3\'=\\"green2\\",\'4\'=\\"steelblue\\") )\\n" }\n'
line: b'{ "repo_name": "henkelstone/NPEL.Classification", "ref": "refs/heads/master", "path": "tests/testthat/test_package.R", "content": "# Automated testing of NPEL.Classification package\\n# Created by Jonathan Henkelman 16.Feb.2016\\n\\nlibrary(NPEL.Classification)\\ncontext(\\"Package\\")\\ncat(\'\\\\n\')\\n\\ntest_examples(\'../../man\')\\n# testthat::test_examples(\'./man\')\\n" }\n'
line: b'{ "repo_name": "henkelstone/NPEL.Classification", "ref": "refs/heads/master", "path": "tests/testthat/test_util.R", "content": " # Automated testing of the \'util.R\' file in the NPEL package\\n# Created by Jonathan Henkelman 27.Jan.2016\\n\\nlibrary(NPEL.Classification)\\nlibrary(testthat)\\ncontext(\\"Util Functions\\")\\ncat(\'\\\\n\')\\n\\n# Setup testing environment\\nfacA <- c(2,3,5,1);     lvlA <- c(3:5,1:2)\\nfacB <- c(12,13,15,11); lvlB <- c(13:15,11:12)\\ntFacA <- factor (facA, levels=lvlA)\\ntFacB <- factor (facB, levels=lvlB)\\nfacC <- factor(NULL)\\n\\ntest_that(\\"factorValues\\", {\\n  expect_equal(factorValues(tFacA), facA)\\n  expect_equal(factorValues(tFacB), facB)\\n  expect_equal(class(factorValues(facC)), \'numeric\')\\n  expect_equal(length(factorValues(facC)), 0)\\n})\\n\\ntest_that(\\"sortLevels\\", {\\n  sFac <- sortLevels(tFacA)\\n  expect_equal(factorValues(sFac), facA)\\n  expect_equal(as.numeric(levels(sFac)), c(1:5))\\n  sFac <- sortLevels(facC)\\n  expect_equal(class(sFac), \'factor\')\\n  expect_equal(length(sFac), 0)\\n})\\n\\ntest_that(\\"trimLevels\\", {\\n  sFac <- trimLevels(tFacA)\\n  expect_equal(factorValues(sFac), facA)\\n  expect_equal(as.numeric(levels(sFac)), c(3,5,1,2))\\n  sFac <- trimLevels(facC)\\n  expect_equal(class(sFac), \'factor\')\\n  expect_equal(length(sFac), 0)\\n})\\n\\ntest_that(\\"mergeLevels\\", {\\n  sFac <- mergeLevels(tFacA,tFacB)\\n  expect_equal(factorValues(sFac), facA)\\n  expect_equal(as.numeric(levels(sFac)), sort(c(lvlA,lvlB)))\\n  sFac <- mergeLevels(tFacA,facC)\\n  expect_equal(sFac, sortLevels(tFacA))\\n})\\n\\ntest_that(\\"fx2vars\\", {\\n  expect_error(fx2vars(),\\"fx2vars: needs some data; supply either a formula, x and y, and/or a list of names.\\")\\n  fx <- x <- y <- NULL; fx2vars(formula(\'a~b+c\'),x,y);                                   expect_equal(x,c(\'b\',\'c\')); expect_equal(y,\'a\')\\n  fx <- x <- y <- NULL; fx2vars(fx,c(\'b\',\'c\'),\'a\');                                      expect_equal(fx,formula(\'a~b+c\'))\\n  fx <- x <- y <- NULL; fx2vars(fx,x,y,names=c(\'a\',\'b\',\'c\'));                            expect_equal(fx,formula(\'a~b+c\')); expect_equal(x,c(\'b\',\'c\')); expect_equal(y,\'a\')\\n\\n  fx <- x <- y <- NULL; expect_error(fx2vars(fx,c(\'b\',\'c\'),\'a\',names=c(\'a\',\'b\')),        \\"fx2vars: a column specified for x does not occur in the dataset.\\")\\n  fx <- x <- y <- NULL; expect_error(fx2vars(formula(\'a~b+c\'),x,y,names=c(\'a\',\'b\')),     \\"fx2vars: a column specified for x does not occur in the dataset.\\")\\n  fx <- x <- y <- NULL; expect_error(fx2vars(formula(\'a+d~b+c\'),x,y),                    \\"fx2vars: multivariate analysis not supported; specify only a single variable for y.\\")\\n  fx <- x <- y <- NULL; expect_error(fx2vars(fx,c(\'b\',\'c\'),c(\'a\',\'d\')),                  \\"fx2vars: multivariate analysis not supported; specify only a single variable for y.\\")\\n  fx <- x <- y <- NULL; expect_error(fx2vars(formula(\'d~b+c\'),x,y,names=c(\'a\',\'b\',\'c\')), \\"fx2vars: variable specified for y does not occur in the dataset; check the column names.\\")\\n  fx <- x <- y <- NULL; expect_error(fx2vars(fx,c(\'b\',\'c\'),\'d\',names=c(\'a\',\'b\',\'c\')),    \\"fx2vars: variable specified for y does not occur in the dataset; check the column names.\\")\\n  fx <- x <- y <- NULL; expect_error(fx2vars(fx,x,\'a\'),                                  \\"fx2vars: cannot find a suitable default for x variables; provide fx and/or names.\\")\\n  fx <- x <- y <- NULL; expect_error(fx2vars(fx,c(\'b\',\'d\'),\'a\',names=c(\'a\',\'b\',\'c\')),    \\"fx2vars: a column specified for x does not occur in the dataset.\\")\\n  fx <- x <- y <- NULL; expect_error(fx2vars(fx,c(\'a\',\'b\',\'c\'),\'a\'),                     \\"fx2vars: x cannot contain the y variable.\\")\\n  fx <- x <- y <- NULL; expect_error(fx2vars(formula(\'a~a+b+c\'),x,y),                    \\"fx2vars: x cannot contain the y variable.\\")\\n})\\n" }\n'
line: b'{ "repo_name": "henkelstone/NPEL.Classification", "ref": "refs/heads/master", "path": "R/Plot.R", "content": "# Functions used to plot tiles and output\\n# Created 9.Oct.2015 from pre-existing code file started 6.Apr.2015\\n\\n##### plotTile.base #####\\n#\' Creates a base ggplot object of the raster object\\n#\'\\n#\' @param data a raster* object to plot\\n#\' @param layers (optional) the layers to include as facets\\n#\' @param title (optional) a title for the plot\\n#\' @param maxpixels (optional) the maximum number of pixels to output\\n#\' @param reduction (optional) is a reducing scale factor applied to both x and y axis; will use whichever is less: ncell/(reduction^2) or maxpixels\\n#\' @param ... other parameters for ggplot\\n#\' @return a ggplot object which can have geoms, scales, etc. added to it (see PlotTile).\\n#\' @export\\nplotTile.base <- function(data, layers=NULL, title=\\"\\", maxpixels=500000, reduction=1, ...) {\\n  if (!requireNamespace(\'ggplot2\')) stop(\\"ggplot2 package required for plotting\\")\\n  if (!is.null(layers)) data <- subset(data,layers)\\n\\n  # Theres a glitch in sampleRegular: we can\'t just simply ask for sR(data, number of cells, xy=TRUE) as it doesn\'t return the same values\\n  # for xy as if we do it step by step: x <- sR(data,#cells,xy=FALSE,raster=TRUE); xyFromCell(x,1:#cells). I don\'t know why this is, but it\\n  # causes artifacts in the final plot so it is necessary to do the work around shown here... It just means we have to extract the data\\n  # twice so it\'s a bit slower.\\n  colNames <- names(data)\\n  data <- raster::sampleRegular(data, size=min(raster::ncell(data)/(reduction*reduction),maxpixels),asRaster=TRUE)   # Return a raster so we can extract the coords afterwards.\\n  data <- data.frame(raster::xyFromCell(data,1:raster::ncell(data)), raster::getValues(data))\\n  names(data) <- c(\'x\',\'y\',colNames)\\n  dat <- reshape(data=data,direction=\'long\',idvar=1:2,varying=3:dim(data)[2],v.names=\'value\',timevar=\'type\',times=names(data)[3:dim(data)[2]]) # Massage into long form; over a GB for a full tile!\\n\\n  # Create a base plot object with some useful aesthetics and that holds the data; then if this is not the only plot the user wants, they should be able to output more without needing to resample the data\\n  return( ggplot2::ggplot(ggplot2::aes(x = \'x\', y = \'y\'), data = \'dat\', ...) +\\n            ggplot2::theme(axis.text.y=ggplot2::element_text(angle=90,hjust=0.5)) + ggplot2::coord_equal() +\\n            ggplot2::labs(title=title,x=\'Latitude (UTM)\',y=\'Longitude (UTM)\',fill=\'Value\') )\\n}\\n\\n##### plotTile #####\\n#\' Given a ggplot base object create a finished plot\\n#\' @param gp the base ggplot to which to add a colour scale and aesthetic (see plotTile.base)\\n#\' @param layers a character vector of the levels which to plot\\n#\' @param discrete force the plot to attempt a discrete fill axis\\n#\' @param colours specify the colours use; a list if discrete is TRUE, or, 2 or 3 colours for the gradient if discrete is FALSE\\n#\' @param labels if desired, specify the names of the labels for each class\\n#\' @param ... other parameters to pass\\n#\' @return the ggplot object with additional aes and scale\\nplotTile <- function(gp, layers, discrete, colours, labels=NULL,...){\\n\\n  # Note: ggplot saves the data in the object which both allows this, but also makes for a large object... like over a GB for a full tile -- I recommend against\\n  # saving the R database on exit without being sure you have time for this file to save/load.\\n\\n  if (!requireNamespace(\'ggplot2\')) stop(\\"ggplot2 package required for plotting\\")\\n  if (discrete) { if (is.null(colours)) stop (\\"plotTile: if discrete is specified, so must be colours\\") }\\n  else          { if (is.null(colours) || !sum(length(colours) == c(2,3))) stop(\\"plotTile: either 2 or 3 colours need to be specified for continuous gradient scales\\") }\\n\\n  gp <- ggplot2::`%+%`(gp, subset(gp$data,gp$data$type %in% layers))   # Subset the data. Necessary to do it this way so we don\'t have to specify the subset to every item, i.e. scale, geom, facet, etc.\\n  if (length(list(...))) gp <- gp + ...                   # Add any extra parameters the user may have supplied\\n  if (discrete) {                                         # If it\'s discrete then the user should have supplied the colours\\n    if (is.null(labels)) return( gp + ggplot2::aes(fill=\'factor(value)\') + ggplot2::geom_raster() + ggplot2::scale_fill_manual(values=colours) )\\n    else                 return( gp + ggplot2::aes(fill=\'factor(value)\') + ggplot2::geom_raster() + ggplot2::scale_fill_manual(values=colours, labels=labels) )\\n  } else {\\n    if (length (colours) == 2) return( gp + ggplot2::aes(fill=\'value\') + ggplot2::geom_raster() + ggplot2::scale_fill_gradient (low=colours[1], high=colours[2]) + ggplot2::facet_wrap(\'~type\') )\\n    if (length (colours) == 3) return( gp + ggplot2::aes(fill=\'value\') + ggplot2::geom_raster() + ggplot2::scale_fill_gradient2(low=colours[1], mid=colours[2], high=colours[3], midpoint=(max(gp$data$value,na.rm=TRUE)-min(gp$data$value,na.rm=TRUE))/2) + ggplot2::facet_wrap(\'~type\') )\\n  }\\n}\\n\\n##### plotTiles #####\\n#\' Outputs plots of multiple data files into a single folder\\n#\' @param path the path of the folder\\n#\' @param base the base part of the filename\\n#\' @param extension the filename \'extension\'; may contain more than the strict extension\\n#\' @param models a list of model names\\n#\' @param type (optional) type out output to use\\n#\' @param ... other parameters to pass to the device function\\nplotTiles <- function(path, base, extension, models, type=\'pdf\',...) {\\n  for (fType in models) {\\n    fName <- paste0(path,base,fType,extension)\\n    Tile <- raster::brick(paste0(fName,\'.tif\'))\\n    if (raster::nlayers(Tile) > 1) { names(Tile) <- c(\'Class\', \'Prob\', paste0(\'Prob.\',1:(raster::nlayers(Tile)-2))) } else { names(Tile) <- c(\'Class\') }\\n\\n    if (type==\'png\') png(paste0(fName,\'.png\'),...)\\n    else pdf(paste0(fName,\'.pdf\'),width=10.0,height=7.5,onefile=TRUE)\\n\\n    gp <- plotTile.base(Tile, layers=names(Tile), maxpixels=1000000)\\n    plotTile(gp, layers=\'Class\', discrete=TRUE, colours=NPEL.Classification::ecoGroup[[\'domSpecies\',\'colours\']]) #,aes(alpha=\'Prob\'))\\n    if (fType != \'fnn\') plotTile(gp, layers=c(\'Prob\',paste(\'Prob\',1:7,sep=\'.\')), discrete=FALSE, colours=c(\'grey17\',\'red\',\'yellow\'))\\n    #    Sys.sleep(15)\\n    dev.off(dev.cur())\\n  }\\n}\\n" }\n'
line: b'{ "repo_name": "rgriff23/btw", "ref": "refs/heads/master", "path": "R/killbt.R", "content": "killbt = function() {\\n\\tjobs = system(\\"pgrep BayesTraits\\", intern=T)\\n\\tfor (n in 1:length(jobs)) {system(paste(\\"kill\\", jobs[n]))}\\n}\\n\\n" }\n'
line: b'{ "repo_name": "fcharte/CursoCienciaDatosR", "ref": "refs/heads/master", "path": "instalaPaquetes.R", "content": "install.packages(\\"caret\\")\\n# LINUX: system(\'wajig install libgtk2.0-dev\') # Use \'sudo apt-get install wajig\' at the command line if needed, then install Gtk headers\\ninstall.packages(\'RGtk2\')\\ninstall.packages(\\"rattle\\")\\ninstall.packages(\\"neuralnet\\")\\ninstall.packages(\\"autoencoder\\")\\ninstall.packages(\\"ggplot2\\")\\ninstall.packages(\\"h2o\\", dependencies = c(\\"Depends\\", \\"Suggests\\"))\\ninstall.packages(\\"pROC\\")\\ninstall.packages(\\"arules\\")\\ninstall.packages(\\"arulesViz\\")\\ninstall.packages(\\"fpc\\")\\n\\nlibrary(rattle)  # WINDOWS: Choose to install Gtk\\nrattle()\\n\\nlibrary(h2o)\\nlocalH2O <- h2o.init() # It should indicate that H2O is initialized, as well as the Java VM version\\n\\n# Download https://github.com/fcharte/CursoCienciaDatosR/blob/master/data/datosTrabajo.RData and save it in working directory\\n\\nload(\'datosTrabajo.RData\')\\n# Look for the previous file at the \\"Files\\" panel, and click it to load it. All the data should appear in the Environment tab\\n\\nlibrary(caret)\\nlibrary(neuralnet)\\nlibrary(autoencoder)\\nlibrary(ggplot2)\\n" }\n'
line: b'{ "repo_name": "rich-iannone/PuffR", "ref": "refs/heads/master", "path": "R/calmet_get_ncdc_history.R", "content": "#\' Retrieve the NCDC history data file\\n#\' @description This function initiates a download of the NCDC surface station history file.\\n#\' @param replace.file selecting \'yes\' will overwrite history file if it exists in the working directory.\\n#\' @export calmet_get_ncdc_history\\n#\' @examples\\n#\' \\\\dontrun{\\n#\' # Obtain the NCDC history file\\n#\' calmet_get_ncdc_history()\\n#\'}\\n\\ncalmet_get_ncdc_history <- function(replace.file = FALSE) {\\n  \\n  # Get hourly surface data history CSV from NOAA/NCDC FTP\\n  file <- \\"ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-history.csv\\"\\n  \\n  if (replace.file == TRUE) {\\n    repeat {\\n      \\n      try(download.file(file, \\"ish-history.csv\\", quiet = TRUE))\\n      \\n      if (file.info(\\"ish-history.csv\\")$size > 0) { break }\\n      \\n    }\\n    \\n  } else { }\\n  \\n  # Check if file exists in working directory\\n  if (file.exists(\\"ish-history.csv\\") &\\n        file.info(\\"ish-history.csv\\")$size > 0 &\\n        replace.file == FALSE) { } else { \\n          \\n          repeat {\\n            \\n            try(download.file(file, \\"ish-history.csv\\", quiet = TRUE))\\n            \\n            if (file.info(\\"ish-history.csv\\")$size > 0) { break }\\n            \\n          }\\n          \\n        }\\n  \\n  # Read in the \\"ish-history\\" CSV file\\n  st <- read.csv(\\"ish-history.csv\\")\\n  \\n}\\n" }\n'
line: b'{ "repo_name": "rich-iannone/PuffR", "ref": "refs/heads/master", "path": "R/calpuff_add_area_sources.R", "content": "#\' Add area sources to a list for later use in CALPUFF\\n#\' @description Add area sources to a list for later use in CALPUFF\\n#\' @param src_name the name of the source emitting the species.\\n#\' @param species_name the name of the species undergoing emissions.\\n#\' @param lat_dec_deg a vector of 4 latitude values for the area source in units of decimal degrees.\\n#\' @param lon_dec_deg a vector of 4 longitude values for the area source in units of decimal degrees.\\n#\' @param x_coord_km a vector of 4 UTM easting values for the area source in km units.\\n#\' @param y_coord_km a vector of 4 UTM northing values for the area source in km units.\\n#\' @param UTM_zone the UTM zone for the area source.\\n#\' @param UTM_hemisphere the UTM hemisphere for the area source.\\n#\' @param effective_height the effective height of the area source in meters above ground level (m AGL).\\n#\' @param base_elev the ground elevation at the location of the area source in meters above sea level (m ASL).\\n#\' @param init_sigma_z the initial sigma z value for the area source in meters.\\n#\' @param emission_rate the rate of constant emissions from the area source; units are defined in the \'emission_units\' argument.\\n#\' @param emission_units the units applied to the value defined in the \'emission_rate\' argument. The possible selections are: (1) \\"g/m2/s\\", (2) \\"kg/m2/hr\\", (3) \\"lb/m2/hr\\", (4) \\"tons/m2/yr\\", (5) \\"Odour Unit * m/s\\", (6) \\"Odour Unit * m/min\\", (7) \\"metric tons/m2/yr\\", (8) \\"Bq/m2/s\\", and (9) \\"GBq/m2/yr\\".\\n#\' @export calpuff_add_area_sources\\n\\ncalpuff_add_area_sources <- function(src_name,\\n                                     species_name,\\n                                     lat_dec_deg = NULL,\\n                                     lon_dec_deg = NULL,\\n                                     x_coord_km = NULL,\\n                                     y_coord_km = NULL,\\n                                     UTM_zone = NULL,\\n                                     UTM_hemisphere = NULL,\\n                                     effective_height,\\n                                     base_elev,\\n                                     init_sigma_z,\\n                                     emission_rate,\\n                                     emission_units){\\n  \\n  # Add require statements\\n  require(rgdal)\\n  require(raster)\\n  require(stringr)\\n  require(plyr)\\n  \\n  # Get expected filename for area sources\\n  area_sources_filename <-\\n    paste0(unlist(str_split(getwd(),\\n                            pattern = \\"/\\"))[length(unlist(str_split(getwd(),\\n                                                                    pattern = \\"/\\")))],\\n           \\"--area_sources.txt\\")\\n  \\n  # Create area sources text file with header if it doesn\'t exist\\n  if (file.exists(area_sources_filename) == FALSE){\\n    \\n    # Create empty file in working folder\\n    file.create(area_sources_filename)\\n    \\n    # Add header row to new area sources file\\n    cat(paste0(\\"src_name\\", \\",\\",\\n               \\"species_name\\", \\",\\",\\n               \\"lat_dec_deg_1\\", \\",\\",\\n               \\"lon_dec_deg_1\\", \\",\\",\\n               \\"lat_dec_deg_2\\", \\",\\",\\n               \\"lon_dec_deg_2\\", \\",\\",\\n               \\"lat_dec_deg_3\\", \\",\\",\\n               \\"lon_dec_deg_3\\", \\",\\",\\n               \\"lat_dec_deg_4\\", \\",\\",\\n               \\"lon_dec_deg_4\\", \\",\\",\\n               \\"x_coord_km_1\\", \\",\\",\\n               \\"y_coord_km_1\\", \\",\\",\\n               \\"x_coord_km_2\\", \\",\\",\\n               \\"y_coord_km_2\\", \\",\\",\\n               \\"x_coord_km_3\\", \\",\\",\\n               \\"y_coord_km_3\\", \\",\\",\\n               \\"x_coord_km_4\\", \\",\\",\\n               \\"y_coord_km_4\\", \\",\\",\\n               \\"UTM_zone\\", \\",\\",\\n               \\"UTM_hemisphere\\", \\",\\",\\n               \\"effective_height\\", \\",\\",\\n               \\"base_elev\\", \\",\\",\\n               \\"init_sigma_z\\", \\",\\",\\n               \\"emission_rate\\", \\",\\",\\n               \\"emission_units\\"),\\n        sep = \\"\\\\n\\",\\n        file = area_sources_filename,\\n        append = TRUE)\\n    \\n  }\\n  \\n  # Determine whether lon/lat provided\\n  if (!is.null(lat_dec_deg) & !is.null(lon_dec_deg)){\\n    lon_lat_provided <- TRUE\\n  } else {\\n    lon_lat_provided <- FALSE\\n  }\\n  \\n  # Determine whether UTM coordinates and zone information provided\\n  if (!is.null(x_coord_km) & !is.null(y_coord_km)\\n      & !is.null(UTM_zone) & !is.null(UTM_hemisphere)){\\n    UTM_provided <- TRUE\\n  } else {\\n    UTM_provided <- FALSE\\n  }  \\n  \\n  # If both lon/lat provided, convert to UTM\\n  if (lon_lat_provided == TRUE & UTM_provided == FALSE){\\n    \\n    # Get matrix of longitude and latitude for source location\\n    lat_lon_dec_deg <- cbind(lon_dec_deg, lat_dec_deg)\\n    \\n    # Determine the UTM zone\\n    UTM_zone <- unique((floor((lon_dec_deg + 180)/6) %% 60) + 1)[1]\\n    \\n    # Determine whether source is in the Northern Hemisphere or the Southern Hemisphere\\n    UTM_hemisphere <- unique(ifelse(lat_dec_deg >= 0, \\"N\\", \\"S\\"))[1]\\n    \\n    # Define a PROJ.4 projection string for a lat/lon projection\\n    proj_string_longlat <- \\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\\"\\n    \\n    # Define a PROJ.4 projection string for a UTM projection\\n    proj_string_UTM <- paste0(\\"+proj=utm +zone=\\",\\n                              UTM_zone,\\n                              \\" +ellps=WGS84 +datum=WGS84 +units=m +no_defs\\")\\n    \\n    # Project as UTM coordinates from the determined UTM zone\\n    UTM_location <- project(lat_lon_dec_deg, proj_string_UTM)\\n    \\n    # Define the UTM x coordinates in km units\\n    x_coord_km <- UTM_location[,1] / 1000\\n    \\n    # Define the UTM y coordinates in km units\\n    y_coord_km <- UTM_location[,2] / 1000\\n    \\n  }\\n  \\n  # If UTM coordinates provided, convert to lon/lat\\n  if (lon_lat_provided == FALSE & UTM_provided == TRUE){\\n    \\n    # Define a PROJ.4 projection string for a lat/lon projection\\n    proj_string_longlat <- \\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\\"\\n    \\n    # Define a PROJ.4 projection string for a UTM projection\\n    proj_string_UTM <- paste0(\\"+proj=utm +zone=\\",\\n                              UTM_zone,\\n                              \\" +ellps=WGS84 +datum=WGS84 +units=m +no_defs\\")\\n    \\n    # Create a SpatialPoints object for the UTM coordinates\\n    UTM_m_SP <- SpatialPoints(matrix(c(x_coord_km * 1000,\\n                                       y_coord_km * 1000),\\n                                     nrow = 4,\\n                                     ncol = 2),\\n                              proj4string = CRS(proj_string_UTM))\\n    \\n    # Project as UTM coordinates from the determined UTM zone\\n    latlon_SP <- spTransform(UTM_m_SP, CRS(proj_string_longlat))\\n    \\n    # Extract the latitude values in decimal degrees from the SpatialPoints object\\n    lat_dec_deg <- latlon_SP@coords[,2]\\n    \\n    # Extract the longitude values in decimal degrees from the SpatialPoints object\\n    lon_dec_deg <- latlon_SP@coords[,1]\\n    \\n  }\\n  \\n  # Write the values to the file\\n  cat(paste0(src_name, \\",\\",\\n             species_name, \\",\\",\\n             format(lat_dec_deg[1], small.interval = 6), \\",\\",\\n             format(lon_dec_deg[1], small.interval = 6), \\",\\",\\n             format(lat_dec_deg[2], small.interval = 6), \\",\\",\\n             format(lon_dec_deg[2], small.interval = 6), \\",\\",\\n             format(lat_dec_deg[3], small.interval = 6), \\",\\",\\n             format(lon_dec_deg[3], small.interval = 6), \\",\\",\\n             format(lat_dec_deg[4], small.interval = 6), \\",\\",\\n             format(lon_dec_deg[4], small.interval = 6), \\",\\",\\n             format(x_coord_km[1], small.interval = 3), \\",\\",\\n             format(y_coord_km[1], small.interval = 3), \\",\\",\\n             format(x_coord_km[2], small.interval = 3), \\",\\",\\n             format(y_coord_km[2], small.interval = 3), \\",\\",\\n             format(x_coord_km[3], small.interval = 3), \\",\\",\\n             format(y_coord_km[3], small.interval = 3), \\",\\",\\n             format(x_coord_km[4], small.interval = 3), \\",\\",\\n             format(y_coord_km[4], small.interval = 3), \\",\\",\\n             UTM_zone, \\",\\",\\n             UTM_hemisphere, \\",\\",\\n             effective_height, \\",\\",\\n             base_elev, \\",\\",\\n             init_sigma_z, \\",\\",\\n             emission_rate, \\",\\",\\n             emission_units),\\n      sep = \\"\\\\n\\",\\n      file = area_sources_filename,\\n      append = TRUE)\\n  \\n}\\n" }\n'
line: b'{ "repo_name": "sfr/RStudio-Addin-Snippets", "ref": "refs/heads/master", "path": "tests/testthat/data/.foobar.R", "content": "# dummy method for copy.data unit testing\\n.foobar <- function( .x_y_z, a )\\n{\\na <- F # not indented on purpose\\n\\n    if (missing(.x_y_z)) {\\n        print(\'Please set a list .x_y_z.\')\\n    } else {\\n        print(.x_y_z$pi)\\n    }\\n\\n    a\\n}\\n\\n# dummy method for insert.pipe unit testing\\n.foobar2 <- function()\\n{\\n    select <- function()\\n    {\\n        NULL\\n    }\\n\\n    filter <- function()\\n    {\\n        NULL\\n    }\\n\\n    # test here\\n    a <- select() %   >  %\\n\\n\\n\\n\\n        filter()\\n\\n\\n    # expected\\n    a <- select() %>%\\n        filter() %>%\\n            as.data.frame()\\n\\n    a\\n}\\n\\ntop.context <- rstudioapi::getActiveDocumentContext()\\ntop.context[[\'selection\']][[1]] <- NULL\\nsave(top.context, file=\'.\\\\\\\\tests\\\\\\\\testthat\\\\\\\\.foobar.Rdata\')\\nrm(top.context)\\n" }\n'
line: b'{ "repo_name": "ellisp/ggseas", "ref": "refs/heads/master", "path": "pkg/tests/testthat.R", "content": "library(testthat)\\nlibrary(ggseas)\\n\\ntest_check(\\"ggseas\\")\\n" }\n'
line: b'{ "repo_name": "ellisp/ggseas", "ref": "refs/heads/master", "path": "prep/create_ldeaths_df.R", "content": "library(tidyr)\\nlibrary(dplyr)\\n\\nldeaths_df <- cbind(fdeaths, mdeaths, YearMon = time(fdeaths)) %>%\\n   as.data.frame() %>%\\n   gather(sex, deaths, -YearMon) %>%\\n   mutate(sex = ifelse(sex == \\"fdeaths\\", \\"female\\", \\"male\\"))\\n\\n\\nsave(ldeaths_df, file = \\"data/ldeaths_df.rda\\")" }\n'
line: b'{ "repo_name": "woobe/rApps", "ref": "refs/heads/master", "path": "oddsimiser/server.R", "content": "suppressMessages(library(shiny))\\nsuppressMessages(library(combinat))\\nsuppressMessages(library(GA))\\nsuppressMessages(library(MCMCpack))\\n\\nshinyServer(function(input, output) {\\n  \\n  ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n  ## Optimiser (Single)\\n  ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n  optimise_single <- reactive({\\n    \\n    odds_all <- c()  \\n    if (input$odds1 > 1) odds_all <- c(odds_all, input$odds1)\\n    if (input$odds2 > 1) odds_all <- c(odds_all, input$odds2)\\n    if (input$odds3 > 1) odds_all <- c(odds_all, input$odds3)\\n    if (input$odds4 > 1) odds_all <- c(odds_all, input$odds4)\\n    if (input$odds5 > 1) odds_all <- c(odds_all, input$odds5)\\n    if (input$odds6 > 1) odds_all <- c(odds_all, input$odds6)\\n    if (input$odds7 > 1) odds_all <- c(odds_all, input$odds7)\\n    if (input$odds8 > 1) odds_all <- c(odds_all, input$odds8)\\n    if (input$odds9 > 1) odds_all <- c(odds_all, input$odds9)\\n    if (input$odds10 > 1) odds_all <- c(odds_all, input$odds10)\\n    as.matrix(odds_all)\\n    \\n        \\n    ## Evaluate Function\\n    eval_odds <- function(inp_p) {\\n      total_p <- sum(inp_p)\\n      rtn <- odds_all * as.matrix(inp_p) - total_p\\n      #return(min(rtn))\\n      return(1-(max(rtn)-min(rtn))/max(rtn))\\n    }\\n    \\n    ## Define parameters for GA\\n    para_ga_min <- rep(0, length(odds_all))\\n    para_ga_max <- rep(1, length(odds_all))\\n    \\n    ## Optimise with GA\\n    set.seed(1234)\\n    model <- ga(type = \\"real-valued\\",\\n                fitness = eval_odds, monitor = FALSE,\\n                min = para_ga_min, max = para_ga_max,\\n                popSize = 10, maxiter = 1000)\\n    \\n    ## Normalise and save results\\n    best_p <- summary(model)$solution / sum(summary(model)$solution)\\n    \\n    ## Create summary df\\n    sum_df <- data.frame(matrix(NA, nrow = 4, ncol = length(odds_all)))\\n    colnames(sum_df) <- paste0(\\"\xe8\xb3\xbd\xe4\xba\x8b\\", 1:length(odds_all))\\n    rownames(sum_df) <- c(\\"\xe8\xb3\xa0\xe7\x8e\x87\\", \\"\xe6\xb3\xa8\xe7\xa2\xbc\\", \\"\xe5\x9b\x9e\xe5\xa0\xb1\\", \\"\xe5\x88\xa9\xe6\xbd\xa4\\")  \\n    sum_df[1,] <- odds_all\\n    sum_df[2,] <- best_p * input$stake\\n    sum_df[3,] <- odds_all * best_p * input$stake\\n    sum_df[4,] <- (odds_all * best_p - 1) * input$stake\\n    \\n    ## Return\\n    t(sum_df)\\n    \\n  })\\n  \\n  \\n  ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n  ## Optimiser Double\\n  ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n  optimise_double <- reactive({\\n    \\n    odds_all <- c()  \\n    if (input$odds1 > 1) odds_all <- c(odds_all, input$odds1)\\n    if (input$odds2 > 1) odds_all <- c(odds_all, input$odds2)\\n    if (input$odds3 > 1) odds_all <- c(odds_all, input$odds3)\\n    if (input$odds4 > 1) odds_all <- c(odds_all, input$odds4)\\n    if (input$odds5 > 1) odds_all <- c(odds_all, input$odds5)\\n    if (input$odds6 > 1) odds_all <- c(odds_all, input$odds6)\\n    if (input$odds7 > 1) odds_all <- c(odds_all, input$odds7)\\n    if (input$odds8 > 1) odds_all <- c(odds_all, input$odds8)\\n    if (input$odds9 > 1) odds_all <- c(odds_all, input$odds9)\\n    if (input$odds10 > 1) odds_all <- c(odds_all, input$odds10)\\n    as.matrix(odds_all)\\n      \\n    games_all <- paste0(\\"\xe8\xb3\xbd\xe4\xba\x8b\\", 1:length(odds_all))\\n    games_double <- combn(games_all, 2)\\n    \\n    cbn_double <- combn(odds_all, 2)\\n    odds_all <- cbn_double[1, ] * cbn_double[2, ]\\n    \\n    ## Evaluate Function\\n    eval_odds <- function(inp_p) {\\n      total_p <- sum(inp_p)\\n      rtn <- odds_all * as.matrix(inp_p) - total_p\\n      #return(min(rtn))\\n      return(1-(max(rtn)-min(rtn))/max(rtn))\\n    }\\n    \\n    ## Define parameters for GA\\n    para_ga_min <- rep(0, length(odds_all))\\n    para_ga_max <- rep(1, length(odds_all))\\n    \\n    ## Optimise with GA\\n    set.seed(1234)\\n    model <- ga(type = \\"real-valued\\",\\n                fitness = eval_odds, monitor = FALSE,\\n                min = para_ga_min, max = para_ga_max,\\n                popSize = 20, maxiter = 2000)\\n    \\n    ## Normalise and save results\\n    best_p <- summary(model)$solution / sum(summary(model)$solution)\\n    \\n    ## Create summary df\\n    sum_df <- data.frame(matrix(NA, nrow = 4, ncol = length(odds_all)))\\n    \\n    names_double <- c()\\n    for (n in 1:length(odds_all)) {\\n      names_double <- c(names_double, paste0(games_double[1,n], \\"x\\", games_double[2,n]))\\n    }\\n    colnames(sum_df) <- names_double\\n    \\n    rownames(sum_df) <- c(\\"\xe8\xb3\xa0\xe7\x8e\x87\\", \\"\xe6\xb3\xa8\xe7\xa2\xbc\\", \\"\xe5\x9b\x9e\xe5\xa0\xb1\\", \\"\xe5\x88\xa9\xe6\xbd\xa4\\")  \\n    sum_df[1,] <- odds_all\\n    sum_df[2,] <- best_p * input$stake\\n    sum_df[3,] <- odds_all * best_p * input$stake\\n    sum_df[4,] <- (odds_all * best_p - 1) * input$stake\\n    \\n    ## Return\\n    t(sum_df)\\n    \\n  })\\n  \\n  \\n  \\n  ## Outputs\\n  \\n  output$single <- renderTable({\\n    print(as.data.frame(optimise_single()))\\n  })\\n  \\n  output$double <- renderTable({\\n    print(as.data.frame(optimise_double()))\\n  })\\n  \\n  \\n})" }\n'
line: b'{ "repo_name": "fcocquemas/rdatastream", "ref": "refs/heads/master", "path": "R/help.R", "content": "#\' A R interface for Datastream and Thomson Dataworks Enterprise. \\n#\'\\n#\' @docType package\\n#\' @name RDatastream\\n#\' @aliases RDatastream\\n#\' @import XML RCurl\\nNULL" }\n'
line: b'{ "repo_name": "Robinlovelace/learning-shiny", "ref": "refs/heads/master", "path": "hi/server.R", "content": "library(shiny)\\n\\n# Define server logic required to draw a histogram\\nshinyServer(function(input, output) {\\n\\n  # Expression that generates a histogram. The expression is\\n  # wrapped in a call to renderPlot to indicate that:\\n  #\\n  #  1) It is \\"reactive\\" and therefore should be automatically\\n  #     re-executed when inputs change\\n  #  2) Its output type is a plot\\n\\n  output$distPlot <- renderPlot({\\n    x    <- faithful[, 2]  # Old Faithful Geyser data\\n    bins <- seq(min(x), max(x), length.out = input$bins + 1)\\n\\n    # draw the histogram with the specified number of bins\\n    hist(x, breaks = bins, col = \'darkgray\', border = \'white\')\\n  })\\n\\n})" }\n'
line: b'{ "repo_name": "zachmayer/cv.ts", "ref": "refs/heads/master", "path": "tests/testthat.R", "content": "library(testthat)\\nlibrary(cv.ts)\\n\\ntest_check(\\"cv.ts\\")\\n" }\n'
line: b'{ "repo_name": "RGLab/MAST", "ref": "refs/heads/summarizedExpt", "path": "R/ZlmFit-bootstrap.R", "content": "##\' Bootstrap a zlmfit\\n##\'\\n##\' Sample cells with replacement to find bootstrapped distribution of coefficients\\n##\' @param cl a \\\\code{cluster} object created by \\\\code{makeCluster}\\n##\' @param zlmfit class \\\\code{ZlmFit}\\n##\' @param R number of bootstrap replicates\\n##\' @return array of bootstrapped coefficients\\n##\' @export\\npbootVcov1<-function (cl,zlmfit, R = 99)\\n{\\n    sca <- zlmfit@sca\\n    N <- ncol(sca)\\n    LMlike <- zlmfit@LMlike\\n    parallel::clusterEvalQ(cl,require(MAST))\\n    ## clusterEvalQ(cl,require(abind))\\n    parallel::clusterExport(cl,\\"N\\",envir=environment())\\n    parallel::clusterExport(cl,\\"LMlike\\",envir=environment())\\n    parallel::clusterExport(cl,\\"sca\\",envir=environment())\\n    manyvc <- parallel::parSapply(cl,1:R, function(i,...){\\n        s <- sample(N, replace = TRUE)\\n        newsca <- sca[, s]\\n        LMlike <- update(LMlike, design=colData(newsca))\\n        zlm.SingleCellAssay(sca = newsca, LMlike = LMlike, onlyCoef=TRUE)\\n    })\\n  \\n    d<-dim(coef(zlmfit,\\"D\\"))\\n    manyvc<-aperm(array(manyvc,c(d,2,R)),c(4,1,2,3))\\n    dimnames(manyvc)<-c(list(NULL),dimnames(coef(zlmfit,\\"D\\")),list(c(\\"C\\",\\"D\\")))\\n    manyvc\\n}\\n\\n##\' Bootstrap a zlmfit\\n##\'\\n##\' Sample cells with replacement to find bootstrapped distribution of coefficients\\n##\' @param zlmfit class \\\\code{ZlmFit}\\n##\' @param R number of bootstrap replicates\\n##\' @return array of bootstrapped coefficients\\n##\' @importFrom plyr raply\\n##\' @export\\nbootVcov1 <- function(zlmfit, R=99){\\n    sca <- zlmfit@sca\\n    N <- ncol(sca)\\n    LMlike <- zlmfit@LMlike\\n    manyvc <- raply(R, {\\n        s <- sample(N, replace=TRUE)\\n        newsca <- sca[,s]\\n        LMlike <- update(LMlike, design=colData(newsca))\\n        zlm.SingleCellAssay(sca=newsca, LMlike=LMlike, onlyCoef=TRUE)\\n    })\\n\\n   manyvc\\n    \\n}\\n\\n" }\n'
line: b'{ "repo_name": "RGLab/MAST", "ref": "refs/heads/summarizedExpt", "path": "tests/testthat/test-lmWrapper-glmer.R", "content": "obj <- new(\'LMERlike\', design=colData(fd), formula=~Stim.Condition + (1|Subject.ID))\\n\\ncontext(\'LMERlike\')\\nif(require(lme4)){\\n    obj <- fit(obj, response=exprs(fd)[,2])\\nobjC <- lmer(obj@response ~Stim.Condition +  (1|Subject.ID), data=as.data.frame(obj@design), subset=obj@response>0, REML=FALSE)\\nobjD <- glmer(obj@response>0 ~Stim.Condition + (1|Subject.ID), data=as.data.frame(obj@design), family=binomial())\\nsource(\'common-lmWrapper-tests.R\', local=TRUE)\\n    try(detach(\'package:lme4\'), silent=TRUE)\\n}\\n\\n" }\n'
line: b'{ "repo_name": "RGLab/MAST", "ref": "refs/heads/summarizedExpt", "path": "tests/testthat/helper-vbeta-init.R", "content": "geneid=\\"Gene\\"\\nprimerid=\'Gene\'\\nmeasurement=\'et\'\\nidvars=c(\'Subject.ID\', \'Chip.Number\', \'Stim.Condition\', \'Population\', \'Well\', \'Number.of.Cells\')\\nphenovars=NULL\\ncellvars=\'Experiment.Number\'\\nfeaturevars=NULL\\nncells <- \'Number.of.Cells\'\\n\\n## Currently needed because devtools 1.11.0 has broken data()\\n## See https://github.com/mlr-org/mlr/pull/835\\nload(system.file(\'data/vbeta.RData\', package=\'MAST\'))\\ndata(vbeta)\\n\\nvbeta$et <- ifelse(is.na(vbeta$Ct), 0, 40-vbeta$Ct)\\n\\n\\nfd <- FromFlatDF(vbeta, idvars=idvars, primerid=primerid, measurement=measurement,cellvars=cellvars, geneid=geneid, ncells=\'Number.of.Cells\', class=\'FluidigmAssay\')\\n" }\n'
line: b'{ "repo_name": "terrytangyuan/dml", "ref": "refs/heads/master", "path": "tests/testthat/test_helper_functions.R", "content": "context(\'helper functions\')\\n\\ntest_that(\'package set up successfully\', {\\n  expect_that(sum(1,2), not(throws_error()))\\n})\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Getting_and_Cleaning_Data/Grouping_and_Chaining_with_dplyr/scripts/chain3.R", "content": "# Use filter() to select all rows for which size_mb is\\n# less than or equal to (<=) 0.5.\\n#\\n# If you want your results printed to the console, add\\n# print to the end of your chain.\\n\\ncran %>%\\n  select(ip_id, country, package, size) %>%\\n  mutate(size_mb = size / 2^20) %>%\\n  # Your call to filter() goes here\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Data_Analysis/Dispersion/initLesson.R", "content": "assign(\\"cars\\", openintro::cars, envir=globalenv())\\nassign(\\"mpg.midsize\\", cars[cars$type==\\"midsize\\",\\"mpgCity\\"], envir=globalenv())\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Exploratory_Data_Analysis/Dimension_Reduction/showRanMat.R", "content": "#par(mar=rep(0.2,4))\\nimage(1:10,1:40,t(dataMatrix)[,nrow(dataMatrix):1])\\n#par(mar=rep(0.2,4))\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Writing_swirl_Courses/R/yamlWriter.R", "content": "#\' DIRECTIONS: \\n#\' 1. Source this file and invoke newLesson(courseName, lessonName), where\\n#\' courseName and lessonName are strings of your choice such as \\"My Course\\" and \\n#\' \\"My Lesson 1\\". Spaces are allowed in course and lesson names.\\n#\' This will create a file such as My_Course/My_Lesson_1/lesson.yaml.\\n#\' The file should appear in the editor automatically. If not, open it manually.\\n#\' \\n#\' 2. Fill in the course meta-information, Author, Organization, etc. as indicated in \\n#\' lesson.yaml. \\n#\' \\n#\' 3. Append instructional units such as multiple-choice questions to lesson.yaml by\\n#\' invoking appropriate helper functions. Type hlp() for a brief list. Typing qmult(),\\n#\' for instance would append the following template to lesson.yaml:\\n#\' - Class: mult_question  \\n#\'   Output: ask the multiple choice question here\\n#\'   AnswerChoices: ANS;2;3\\n#\'   CorrectAnswer: ANS\\n#\'   AnswerTests: omnitest(correctVal= \'ANS\')\\n#\'   Hint: hint\\n#\' which might be manually edited as follows.\\n#\' - Class: mult_question  \\n#\'   Output: This demonstrates a multiple choice question. Which Scooby-Doo character wears an ascot?\\n#\'   AnswerChoices: Fred Jones;Velma Dinkley;Daphne Blake;Shaggy Rogers\\n#\'   CorrectAnswer: Fred Jones\\n#\'   AnswerTests: omnitest(correctVal= \'Fred Jones\')\\n#\'   Hint: This person doesn\'t say \\"Jinkies.\\"\\n#\'   \\n#\' 4. Save lesson.yaml whenever you append and edit a unit of instruction.\\n#\' \\n#\' 5. Strings may take more than one line provided they are surrounded by double quotes. For instance,\\n#\' in the following question the Output: field takes up two lines:\\n#\' - Class: text_question\\n#\'   Output: \\"This illustrates a question requiring a single phrase text answer. \\n#\' What is the name of the van which carries Scooby\'s gang?\\"\\n#\'   CorrectAnswer: Mystery Machine\\n#\'   AnswerTests: omnitest(correctVal=\'Mystery Machine\')\\n#\'   Hint: The gang generally solves a M------? It\'s the M------ Machine.\\n\\nnewLesson <- function(course, lesson){\\n  courseDir <- file.path(gsub(\\" \\", \\"_\\", course))\\n  lessonDir <<- file.path(courseDir, gsub(\\" \\", \\"_\\", lesson))\\n  if(!file.exists(lessonDir))dir.create(lessonDir, recursive=TRUE)\\n  # Check for existence of a manifest\\n  manifest <- file.path(courseDir, \\"MANIFEST\\")\\n  if(!file.exists(manifest)){\\n    file.create(manifest)\\n  }\\n  # Append the current lesson to the manifest\\n  cat(paste0(gsub(\\" \\", \\"_\\", lesson), \\"\\\\n\\"), file=manifest, append=TRUE)\\n  # The yaml faq, http://www.yaml.org/faq.html, encourages\\n  # use of the .yaml (as opposed to .yml) file extension\\n  # whenever possible.\\n  lessonFile <<- file.path(lessonDir, \\"lesson.yaml\\")\\n  writeLines(c(\\"- Class: meta\\", \\n               paste(\\"  Course:\\", course),\\n               paste(\\"  Lesson:\\", lesson),\\n               \\"  Author: your name goes here\\",\\n               \\"  Type: Standard\\",\\n               \\"  Organization: your organization\\",\\n               paste(\\"  Version: \\", packageDescription(\\"swirl\\")$Version)),\\n             lessonFile)\\n  # Create supporting files\\n  depends <- file.path(lessonDir, \\"dependson.txt\\")\\n  file.create(depends)\\n  init <- file.path(lessonDir, \\"initLesson.R\\")\\n  file.create(init)\\n  cat(\\"# Code placed in this file will be executed every time the\\n# lesson is started. Any variables created here will show up in\\n# the user\'s working directory and thus be accessible to them\\n# throughout the lesson.\\\\n\\", file=init)\\n  custom <- file.path(lessonDir, \\"customTests.R\\")\\n  file.create(custom)\\n  cat(\\"# Put custom tests in this file.\\n\\n# Uncommenting the following line of code will disable\\n# auto-detection of new variables and thus prevent swirl from\\n# executing every command twice, which can slow things down.\\n\\n# AUTO_DETECT_NEWVAR <- FALSE\\n\\n# However, this means that you should detect user-created\\n# variables when appropriate. The answer test, expr_creates_var()\\n# can be used for for the purpose, but it also re-evaluates the\\n# expression which the user entered, so care must be taken.\\\\n\\", \\n      file=custom)\\n  file.edit(lessonFile)\\n}\\n\\nsetLesson <- function(course, lesson){\\n  courseDir <- file.path(gsub(\\" \\", \\"_\\", course))\\n  lessonDir <- file.path(courseDir, gsub(\\" \\", \\"_\\", lesson))\\n  lessonFile <- file.path(lessonDir, \\"lesson.yaml\\")\\n  if(!file.exists(lessonFile)){\\n    stop(paste(\\"Sorry!\\", lessonFile, \\"doesn\'t exist. Check the path.\\"))\\n  } else {\\n    lessonDir <<- lessonDir\\n    lessonFile <<- lessonFile\\n    file.edit(lessonFile)\\n  }\\n}\\n\\n# yaml writer help\\nhlp <- function(){\\n  print(\\"txt -- just text, no question\\")\\n  print(\\"qmult -- multiple choice question\\")\\n  print(\\"qcmd -- command line question\\")\\n  print(\\"vid -- video\\")\\n  print(\\"fig -- figure\\")\\n  print(\\"qx -- question requiring exact numerical answer\\")\\n  print(\\"qtxt -- question requiring a short text answer\\")\\n  print(\\"qrng -- question requiring a numerical answer within a certain range.\\")\\n}\\n\\n# template for presentation without a question\\ntxt <- function(){\\n  cat(\\"\\\\n- Class: text\\n  Output: put your exposition here.\\\\n\\", file=lessonFile, append=TRUE)\\n  invisible()\\n}\\n\\n# template for multiple choice question\\nqmult <- function(){\\n  cat(\\"\\\\n- Class: mult_question  \\n  Output: ask the multiple choice question here\\n  AnswerChoices: ANS;2;3\\n  CorrectAnswer: ANS\\n  AnswerTests: omnitest(correctVal= \'ANS\')\\n  Hint: hint\\\\n\\", file=lessonFile, append=TRUE)\\n  invisible()\\n}\\n\\nqcmd <- function(){\\n  cat(\\"\\\\n- Class: cmd_question\\n  Output: explain what the user must do here\\n  CorrectAnswer: EXPR or VAL\\n  AnswerTests: omnitest(correctExpr=\'EXPR\', correctVal=VAL)\\n  Hint: hint\\\\n\\", file=lessonFile, append=TRUE)\\n  invisible()\\n}\\n\\nvid <- function(){\\n  cat(\\"\\\\n- Class: video\\n  Output: Would you like to watch a short video about ___?\\n  VideoLink: \'http://address.of.video\'\\\\n\\", file=lessonFile, append=TRUE)\\n  invisible()\\n}\\n\\nfig <- function(){\\n  cat(\\"\\\\n- Class: figure\\n  Output: explain the figure here\\n  Figure: sourcefile.R\\n  FigureType: new or add\\\\n\\", file=lessonFile, append=TRUE)\\n  invisible()\\n}\\n\\nqx<- function(){\\n  cat(\\"\\\\n- Class: exact_question\\n  Output: explain the question here\\n  CorrectAnswer: n\\n  AnswerTests: omnitest(correctVal=n)\\n  Hint: hint\\\\n\\", file=lessonFile, append=TRUE)\\ninvisible()\\n}\\n\\nqtxt <- function(){\\n  cat(\\"\\\\n- Class: text_question\\n  Output: explain the question here\\n  CorrectAnswer: answer\\n  AnswerTests: val_matches(\'regular_expression_which_matches_answer\')\\n  Hint: hint\\\\n\\", file=lessonFile, append=TRUE)\\ninvisible()\\n}\\n\\nqrng <- function(){\\n  cat(\\"\\\\n- Class: range_question\\n  Output: explain the question here\\n      CorrectAnswer: answer\\n      AnswerTests: requires a custom test\\n      Hint: hint\\\\n\\", file=lessonFile, append=TRUE)\\n  invisible()\\n}\\n\\n\\nreinstall <- function(){\\n  course <- dirname(lessonDir)\\n  # Uninstall_course\\n  try(swirl::uninstall_course(course), silent=TRUE)\\n  # Install course\\n  swirl::install_course_directory(gsub(\\" \\", \\"_\\", course))\\n}\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/Binary_Outcomes/initLesson.R", "content": "# For compatibility with 2.2.21\\n.get_course_path <- function(){\\n  tryCatch(swirl:::swirl_courses_dir(),\\n           error = function(c) {file.path(find.package(\\"swirl\\"),\\"Courses\\")}\\n  )\\n}\\n\\n# ravens data\\nravenData <- read.csv(file.path(.get_course_path(), \\n                                 \\"Regression_Models\\", \\"Binary_Outcomes\\", \\"ravens_data.csv\\"))\\nravenData <- ravenData[order(ravenData$ravenScore), 1:3]\\nrownames(ravenData) <- NULL\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/Overfitting_and_Underfitting/initLesson.R", "content": "# For compatibility with 2.2.21\\n.get_course_path <- function(){\\n  tryCatch(swirl:::swirl_courses_dir(),\\n           error = function(c) {file.path(find.package(\\"swirl\\"),\\"Courses\\")}\\n  )\\n}\\n\\nswiss <- datasets::swiss\\nfile.copy(from=file.path(.get_course_path(),\\n\\t\\"Regression_Models\\", \\"Overfitting_and_Underfitting\\",\\"fitting.R\\"), \\n          to=\\"fitting.R\\")\\nfile.edit(\\"fitting.R\\")\\nsource(\\"fitting.R\\", local=TRUE)\\nfit5 <- lm(Fertility ~ Agriculture + Examination + Education + Catholic, swiss)\\nfit6 <- lm(Fertility ~ ., swiss)\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Exploratory_Data_Analysis/Principles_of_Analytic_Graphs/MVD2.R", "content": "fname <- paste(path_to_course,\\"MVData2.jpeg\\",sep=\\"/\\")\\ntry(dev.off(),silent=TRUE)\\nplot.new()\\nplotArea=par(\'fig\')\\nrasterImage(readJPEG(fname),plotArea[1],plotArea[3],plotArea[2],plotArea[4],interpolate=FALSE)\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Getting_and_Cleaning_Data/Tidying_Data_with_tidyr/scripts/script8.R", "content": "# Accomplish the following three goals:\\n#\\n# 1. select() all columns that do NOT contain the word \\"total\\",\\n# since if we have the male and female data, we can always\\n# recreate the total count in a separate column, if we want it.\\n# Hint: Use the contains() function, which you\'ll\\n# find detailed in \'Special functions\' section of ?select.\\n#\\n# 2. gather() all columns EXCEPT score_range, using\\n# key = part_sex and value = count.\\n#\\n# 3. separate() part_sex into two separate variables (columns),\\n# called \\"part\\" and \\"sex\\", respectively. You may need to check\\n# the \'Examples\' section of ?separate to remember how the \'into\'\\n# argument should be phrased.\\n#\\nsat %>%\\n  select(-contains(###)) %>%\\n  gather(###, ###, -###) %>%\\n  ### <Your call to separate()> %>%\\n  print\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Getting_and_Cleaning_Data/Grouping_and_Chaining_with_dplyr/scripts/chain4.R", "content": "# arrange() the result by size_mb, in descending order.\\n#\\n# If you want your results printed to the console, add\\n# print to the end of your chain.\\n\\ncran %>%\\n  select(ip_id, country, package, size) %>%\\n  mutate(size_mb = size / 2^20) %>%\\n  filter(size_mb <= 0.5) %>%\\n  # Your call to arrange() goes here\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/Introduction_to_Multivariable_Regression/customTests.R", "content": "# So swirl does not repeat execution of plot commands\\nAUTO_DETECT_NEWVAR <- FALSE\\n\\n# Returns TRUE if e$expr matches any of the expressions given\\n# (as characters) in the argument.\\nANY_of_exprs <- function(...){\\n  e <- get(\\"e\\", parent.frame())\\n  any(sapply(c(...), function(expr)omnitest(expr)))\\n}\\n\\n# Returns TRUE if the user has created a specified lm model\\n# with a specified name.\\ncreates_lm_model <- function(correctExpr){\\n  e <- get(\\"e\\", parent.frame())\\n  # Do what the user should have done\\n  eSw <- cleanEnv(e$snapshot)\\n  mdlSw <- eval(parse(text=correctExpr), eSw)\\n  # Recreate what the user has done\\n  eUsr <- cleanEnv(e$snapshot)\\n  mdlUsr <- eval(e$expr, eUsr)\\n  # If the correct model is named:\\n  if(length(ls(eSw))>0){\\n    # Check whether the model\'s name is correct\\n    nameGood <- sum(ls(eUsr) %in% ls(eSw)) & sum(ls(eSw) %in% ls(eUsr))\\n    # If not, highlight the misspelling\\n    if(!nameGood){\\n      swirl_out(paste0(\\"You seem to have misspelled the model\'s name. I was expecting \\", ls(eSw), \\n                       \\" but you apparently typed \\", ls(eUsr), \\".\\"))\\n      return(FALSE)\\n    } else {\\n      # Append the result, as a list to e$delta for progress restoration\\n      e$delta <- c(e$delta, as.list(eUsr))\\n    }\\n  }\\n  # Check for effective equality of the models\\n  isTRUE(all.equal(sort(as.vector(mdlUsr$coefficients)), sort(as.vector(mdlSw$coefficients)))) &\\n    isTRUE(all.equal(mdlUsr$fitted.values, mdlSw$fitted.values))\\n}\\n\\n# Returns TRUE if the user has calculated a value equal to that calculated by the given expression.\\ncalculates_same_value <- function(expr){\\n  e <- get(\\"e\\", parent.frame())\\n  # Calculate what the user should have done.\\n  eSnap <- cleanEnv(e$snapshot)\\n  val <- eval(parse(text=expr), eSnap)\\n  isTRUE(all.equal(val, e$val))\\n}\\n\\n# Returns TRUE of the user has calculated a value equal to any of those computed by the given\\n# expressions.\\ncalculates_ANY_value <- function(...){\\n  e <- get(\\"e\\", parent.frame())\\n  any(sapply(c(...), function(expr)calculates_same_value(expr)))\\n}\\n\\n# Get the swirl state\\ngetState <- function(){\\n  # Whenever swirl is running, its callback is at the top of its call stack.\\n  # Swirl\'s state, named e, is stored in the environment of the callback.\\n  environment(sys.function(1))$e\\n}\\n\\n# Get the value which a user either entered directly or was computed\\n# by the command he or she entered.\\ngetVal <- function(){\\n  getState()$val\\n}\\n\\n# Get the last expression which the user entered at the R console.\\ngetExpr <- function(){\\n  getState()$expr\\n}\\n\\ncoursera_on_demand <- function(){\\n  selection <- getState()$val\\n  if(selection == \\"Yes\\"){\\n    email <- readline(\\"What is your email address? \\")\\n    token <- readline(\\"What is your assignment token? \\")\\n    \\n    payload <- sprintf(\'{  \\n      \\"assignmentKey\\": \\"hHsdF68wEeWxaw7Jay15BQ\\",\\n      \\"submitterEmail\\": \\"%s\\",  \\n      \\"secret\\": \\"%s\\",  \\n      \\"parts\\": {  \\n        \\"3wpmw\\": {  \\n          \\"output\\": \\"correct\\"  \\n        }  \\n      }  \\n    }\', email, token)\\n    url <- \'https://www.coursera.org/api/onDemandProgrammingScriptSubmissions.v1\'\\n  \\n    respone <- httr::POST(url, body = payload)\\n    if(respone$status_code >= 200 && respone$status_code < 300){\\n      message(\\"Grade submission succeeded!\\")\\n    } else {\\n      message(\\"Grade submission failed.\\")\\n      message(\\"Press ESC if you want to exit this lesson and you\\")\\n      message(\\"want to try to submit your grade at a later time.\\")\\n      return(FALSE)\\n    }\\n  }\\n  TRUE\\n}" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/Introduction/restore_1.R", "content": "plot(child ~ parent, galton)\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Statistical_Inference/Probability1/customTests.R", "content": "# So swirl does not repeat execution of plot commands\\nAUTO_DETECT_NEWVAR <- FALSE\\n\\n# Returns TRUE if e$expr matches any of the expressions given\\n# (as characters) in the argument.\\nANY_of_exprs <- function(...){\\n  e <- get(\\"e\\", parent.frame())\\n  any(sapply(c(...), function(expr)omnitest(expr)))\\n}\\n\\nequiv_val <- function(correctVal){\\n  e <- get(\\"e\\", parent.frame()) \\n  #print(paste(\\"User val is \\",e$val,\\"Correct ans is \\",correctVal))\\n  isTRUE(all.equal(correctVal,e$val))\\n  \\n}\\n\\n# Get the swirl state\\ngetState <- function(){\\n  # Whenever swirl is running, its callback is at the top of its call stack.\\n  # Swirl\'s state, named e, is stored in the environment of the callback.\\n  environment(sys.function(1))$e\\n}\\n\\n# Get the value which a user either entered directly or was computed\\n# by the command he or she entered.\\ngetVal <- function(){\\n  getState()$val\\n}\\n\\n# Get the last expression which the user entered at the R console.\\ngetExpr <- function(){\\n  getState()$expr\\n}\\n\\ncoursera_on_demand <- function(){\\n  selection <- getState()$val\\n  if(selection == \\"Yes\\"){\\n    email <- readline(\\"What is your email address? \\")\\n    token <- readline(\\"What is your assignment token? \\")\\n    \\n    payload <- sprintf(\'{  \\n      \\"assignmentKey\\": \\"BOLw8680EeWRRQpQejjiSw\\",\\n      \\"submitterEmail\\": \\"%s\\",  \\n      \\"secret\\": \\"%s\\",  \\n      \\"parts\\": {  \\n        \\"jwsCv\\": {  \\n          \\"output\\": \\"correct\\"  \\n        }  \\n      }  \\n    }\', email, token)\\n    url <- \'https://www.coursera.org/api/onDemandProgrammingScriptSubmissions.v1\'\\n  \\n    respone <- httr::POST(url, body = payload)\\n    if(respone$status_code >= 200 && respone$status_code < 300){\\n      message(\\"Grade submission succeeded!\\")\\n    } else {\\n      message(\\"Grade submission failed.\\")\\n      message(\\"Press ESC if you want to exit this lesson and you\\")\\n      message(\\"want to try to submit your grade at a later time.\\")\\n      return(FALSE)\\n    }\\n  }\\n  TRUE\\n}" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/MultiVar_Examples/customTests.R", "content": "# So swirl does not repeat execution of plot commands\\n#AUTO_DETECT_NEWVAR <- FALSE\\n\\n# Returns TRUE if e$expr matches any of the expressions given\\n# (as characters) in the argument.\\nANY_of_exprs <- function(...){\\n  e <- get(\\"e\\", parent.frame())\\n  any(sapply(c(...), function(expr)omnitest(expr)))\\n}\\n\\n# Returns TRUE if the user has created a specified lm model\\n# with a specified name.\\ncreates_lm_model <- function(correctExpr){\\n  e <- get(\\"e\\", parent.frame())\\n  # Do what the user should have done\\n  eSw <- cleanEnv(e$snapshot)\\n  mdlSw <- eval(parse(text=correctExpr), eSw)\\n  # Recreate what the user has done\\n  eUsr <- cleanEnv(e$snapshot)\\n  mdlUsr <- eval(e$expr, eUsr)\\n  # If the correct model is named:\\n  if(length(ls(eSw))>0){\\n    # Check whether the model\'s name is correct\\n    nameGood <- sum(ls(eUsr) %in% ls(eSw)) & sum(ls(eSw) %in% ls(eUsr))\\n    # If not, highlight the misspelling\\n    if(!nameGood){\\n      swirl_out(paste0(\\"You seem to have misspelled the model\'s name. I was expecting \\", ls(eSw), \\n                       \\" but you apparently typed \\", ls(eUsr), \\".\\"))\\n      return(FALSE)\\n    } else {\\n      # Append the result, as a list to e$delta for progress restoration\\n      e$delta <- c(e$delta, as.list(eUsr))\\n    }\\n  }\\n  # Check for effective equality of the models\\n  isTRUE(all.equal(sort(as.vector(mdlUsr$coefficients)), sort(as.vector(mdlSw$coefficients)))) &\\n    isTRUE(all.equal(mdlUsr$fitted.values, mdlSw$fitted.values))\\n}\\n\\n# Get the swirl state\\ngetState <- function(){\\n  # Whenever swirl is running, its callback is at the top of its call stack.\\n  # Swirl\'s state, named e, is stored in the environment of the callback.\\n  environment(sys.function(1))$e\\n}\\n\\n# Get the value which a user either entered directly or was computed\\n# by the command he or she entered.\\ngetVal <- function(){\\n  getState()$val\\n}\\n\\n# Get the last expression which the user entered at the R console.\\ngetExpr <- function(){\\n  getState()$expr\\n}\\n\\ncoursera_on_demand <- function(){\\n  selection <- getState()$val\\n  if(selection == \\"Yes\\"){\\n    email <- readline(\\"What is your email address? \\")\\n    token <- readline(\\"What is your assignment token? \\")\\n    \\n    payload <- sprintf(\'{  \\n      \\"assignmentKey\\": \\"iGMF3K8wEeWVdAqQVb1YyQ\\",\\n      \\"submitterEmail\\": \\"%s\\",  \\n      \\"secret\\": \\"%s\\",  \\n      \\"parts\\": {  \\n        \\"Dxjqj\\": {  \\n          \\"output\\": \\"correct\\"  \\n        }  \\n      }  \\n    }\', email, token)\\n    url <- \'https://www.coursera.org/api/onDemandProgrammingScriptSubmissions.v1\'\\n  \\n    respone <- httr::POST(url, body = payload)\\n    if(respone$status_code >= 200 && respone$status_code < 300){\\n      message(\\"Grade submission succeeded!\\")\\n    } else {\\n      message(\\"Grade submission failed.\\")\\n      message(\\"Press ESC if you want to exit this lesson and you\\")\\n      message(\\"want to try to submit your grade at a later time.\\")\\n      return(FALSE)\\n    }\\n  }\\n  TRUE\\n}" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Statistical_Inference/CommonDistros/stddev1.R", "content": "g <- ggplot(dat, aes(x = x, y = y)) + geom_line(size = 1.5)\\ng <- g + geom_ribbon(aes(x = ifelse(x > -1 & x < 1, x, 0), ymin = 0, ymax = dat$y), fill = \\"red\\", alpha = 1)\\ng <- g +  geom_ribbon(aes(x = ifelse(x > -2 & x < 2, x, 0), ymin = 0, ymax = dat$y), fill = \\"red\\", alpha = 0.5)\\ng <- g +  geom_ribbon(aes(x = ifelse(x > -3 & x < 3, x, 0), ymin = 0, ymax = dat$y), fill = \\"red\\", alpha = 0.35)\\nprint(g)\\n\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Getting_and_Cleaning_Data/Grouping_and_Chaining_with_dplyr/scripts/summarize2-correct.R", "content": "# Don\'t change any of the code below. Just type submit()\\n# when you think you understand it.\\n\\n# We\'ve already done this part, but we\'re repeating it\\n# here for clarity.\\n\\nby_package <- group_by(cran, package)\\npack_sum <- summarize(by_package,\\n                      count = n(),\\n                      unique = n_distinct(ip_id),\\n                      countries = n_distinct(country),\\n                      avg_bytes = mean(size))\\n\\n# Here\'s the new bit, but using the same approach we\'ve\\n# been using this whole time.\\n\\ntop_countries <- filter(pack_sum, countries > 60)\\nresult1 <- arrange(top_countries, desc(countries), avg_bytes)\\n\\n# Print the results to the console.\\nprint(result1)\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Data_Analysis/Central_Tendency/initLesson.R", "content": "  assign(\\"cars\\", openintro::cars, envir=globalenv())\\n  assign(\\"mpg.midsize\\", cars[cars$type==\\"midsize\\",\\"mpgCity\\"], envir=globalenv())\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/Residuals_Diagnostics_and_Variation/restore_4.R", "content": "plot(fit, which=2)" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "R_Programming_Alt/Functions/customTests.R", "content": "test_func1 <- function() {\\n  try({\\n    func <- get(\'boring_function\', globalenv())\\n    t1 <- identical(func(9), 9)\\n    t2 <- identical(func(4), 4)\\n    t3 <- identical(func(0), 0)\\n    ok <- all(t1, t2, t3)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}\\n\\ntest_func2 <- function() {\\n  try({\\n    func <- get(\'my_mean\', globalenv())\\n    t1 <- identical(func(9), mean(9))\\n    t2 <- identical(func(1:10), mean(1:10))\\n    t3 <- identical(func(c(-5, -2, 4, 10)), mean(c(-5, -2, 4, 10)))\\n    ok <- all(t1, t2, t3)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}\\n\\ntest_func3 <- function() {\\n  try({\\n    func <- get(\'remainder\', globalenv())\\n    t1 <- identical(func(9, 4), 9 %% 4)\\n    t2 <- identical(func(divisor = 5, num = 2), 2 %% 5)\\n    t3 <- identical(func(5), 5 %% 2)\\n    ok <- all(t1, t2, t3)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}\\n\\ntest_func4 <- function() {\\n  try({\\n    func <- get(\'evaluate\', globalenv())\\n    t1 <- identical(func(sum, c(2, 4, 7)), 13)\\n    t2 <- identical(func(median, c(9, 200, 100)), 100)\\n    t3 <- identical(func(floor, 12.1), 12)\\n    ok <- all(t1, t2, t3)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}\\n\\ntest_func5 <- function() {\\n  try({\\n    func <- get(\'telegram\', globalenv())\\n    t1 <- identical(func(\\"Good\\", \\"morning\\"), \\"START Good morning STOP\\")\\n    t2 <- identical(func(\\"hello\\", \\"there\\", \\"sir\\"), \\"START hello there sir STOP\\")\\n    t3 <- identical(func(), \\"START STOP\\")\\n    ok <- all(t1, t2, t3)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}\\n\\ntest_func6 <- function() {\\n  try({\\n    func <- get(\'mad_libs\', globalenv())\\n    t1 <- identical(func(place = \\"Baltimore\\", adjective = \\"smelly\\", noun = \\"Roger Peng statue\\"), \\"News from Baltimore today where smelly students took to the streets in protest of the new Roger Peng statue being installed on campus.\\")\\n    t2 <- identical(func(place = \\"Washington\\", adjective = \\"angry\\", noun = \\"Shake Shack\\"), \\"News from Washington today where angry students took to the streets in protest of the new Shake Shack being installed on campus.\\")\\n    ok <- all(t1, t2)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}\\n\\ntest_func7 <- function() {\\n  try({\\n    func <- get(\'%p%\', globalenv())\\n    t1 <- identical(func(\\"Good\\", \\"job!\\"), \\"Good job!\\")\\n    t2 <- identical(func(\\"one\\", func(\\"two\\", \\"three\\")), \\"one two three\\")\\n    ok <- all(t1, t2)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}\\n\\ntest_eval1 <- function(){\\n  try({\\n    e <- get(\\"e\\", parent.frame())\\n    expr <- e$expr\\n    t1 <- identical(expr[[3]], 6)\\n    expr[[3]] <- 7\\n    t2 <- identical(eval(expr), 8)\\n    ok <- all(t1, t2)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}\\n\\ntest_eval2 <- function(){\\n  try({\\n    e <- get(\\"e\\", parent.frame())\\n    expr <- e$expr\\n    t1 <- identical(expr[[3]], quote(c(8, 4, 0)))\\n    t2 <- identical(expr[[1]], quote(evaluate))\\n    expr[[3]] <- c(5, 6)\\n    t3 <- identical(eval(expr), 5)\\n    ok <- all(t1, t2, t3)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}\\n\\ntest_eval3 <- function(){\\n  try({\\n    e <- get(\\"e\\", parent.frame())\\n    expr <- e$expr\\n    t1 <- identical(expr[[3]], quote(c(8, 4, 0)))\\n    t2 <- identical(expr[[1]], quote(evaluate))\\n    expr[[3]] <- c(5, 6)\\n    t3 <- identical(eval(expr), 6)\\n    ok <- all(t1, t2, t3)\\n  }, silent = TRUE)\\n  exists(\'ok\') && isTRUE(ok)\\n}" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Exploratory_Data_Analysis/Working_with_Colors/customTests.R", "content": "# So swirl does not repeat execution of plot commands\\nAUTO_DETECT_NEWVAR <- FALSE\\n\\n# Returns TRUE if e$expr matches any of the expressions given\\n# (as characters) in the argument.\\nANY_of_exprs <- function(...){\\n  e <- get(\\"e\\", parent.frame())\\n  any(sapply(c(...), function(expr)omnitest(expr)))\\n}\\n\\nequiv_val <- function(correctVal){\\n  e <- get(\\"e\\", parent.frame()) \\n  #print(paste(\\"User val is \\",e$val,\\"Correct ans is \\",correctVal))\\n  isTRUE(all.equal(correctVal,e$val))\\n  \\n}\\n\\n# Get the swirl state\\ngetState <- function(){\\n  # Whenever swirl is running, its callback is at the top of its call stack.\\n  # Swirl\'s state, named e, is stored in the environment of the callback.\\n  environment(sys.function(1))$e\\n}\\n\\n# Get the value which a user either entered directly or was computed\\n# by the command he or she entered.\\ngetVal <- function(){\\n  getState()$val\\n}\\n\\n# Get the last expression which the user entered at the R console.\\ngetExpr <- function(){\\n  getState()$expr\\n}\\n\\ncoursera_on_demand <- function(){\\n  selection <- getState()$val\\n  if(selection == \\"Yes\\"){\\n    email <- readline(\\"What is your email address? \\")\\n    token <- readline(\\"What is your assignment token? \\")\\n    \\n    payload <- sprintf(\'{  \\n      \\"assignmentKey\\": \\"jbRkna8dEeWxIhKVGQB0WQ\\",\\n      \\"submitterEmail\\": \\"%s\\",  \\n      \\"secret\\": \\"%s\\",  \\n      \\"parts\\": {  \\n        \\"Lxfoi\\": {  \\n          \\"output\\": \\"correct\\"  \\n        }  \\n      }  \\n    }\', email, token)\\n    url <- \'https://www.coursera.org/api/onDemandProgrammingScriptSubmissions.v1\'\\n  \\n    respone <- httr::POST(url, body = payload)\\n    if(respone$status_code >= 200 && respone$status_code < 300){\\n      message(\\"Grade submission succeeded!\\")\\n    } else {\\n      message(\\"Grade submission failed.\\")\\n      message(\\"Press ESC if you want to exit this lesson and you\\")\\n      message(\\"want to try to submit your grade at a later time.\\")\\n      return(FALSE)\\n    }\\n  }\\n  TRUE\\n}" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Exploratory_Data_Analysis/Hierarchical_Clustering/dendro.R", "content": "try(dev.off(),silent=TRUE)\\nplot.new()\\nplot(as.dendrogram(hc))" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Exploratory_Data_Analysis/CaseStudy/initLesson.R", "content": "library(fields)\\n\\n# For compatibility with 2.2.21\\n.get_course_path <- function(){\\n  tryCatch(swirl:::swirl_courses_dir(),\\n           error = function(c) {file.path(find.package(\\"swirl\\"),\\"Courses\\")}\\n  )\\n}\\n\\n# Put initialization code in this file.\\npath_to_course <- file.path(.get_course_path(),\\n  \\"Exploratory_Data_Analysis\\",\\"CaseStudy\\")\\ntry(dev.off(),silent=TRUE)\\nplot.new()\\n\\npathtofile <- function(fileName){\\n  mypath <- file.path(.get_course_path(),\\n    \\"Exploratory_Data_Analysis\\",\\"CaseStudy\\",\\n                      fileName)\\n}\\nfxfer <- function(fileName){\\n  mypath <- pathtofile(fileName)\\n  file.copy(mypath,fileName)\\n}\\n\\nmyImage <- function(iname){\\n  par(mfrow=c(1,1))\\n  par(mar=c(8,10,8,10))\\n  image(t(iname)[,nrow(iname):1])\\n}\\nmyedit <- function(fname){\\n   #fxfer(fname)\\n   #file.edit(fname)\\n   mypath <- pathtofile(fname)\\n   file.edit(mypath)\\n}\\n\\nmdist <- function(x,y,cx,cy){\\n  distTmp <- matrix(NA,nrow=3,ncol=12)\\n  distTmp[1,] <- (x-cx[1])^2 + (y-cy[1])^2\\n  distTmp[2,] <- (x-cx[2])^2 + (y-cy[2])^2\\n  distTmp[3,] <- (x-cx[3])^2 + (y-cy[3])^2  \\n  return(distTmp)\\n}\\n\\nshowMe <- function(cv){\\n  myarg <- deparse(substitute(cv))\\n  z<- outer( 1:20,1:20, \\"+\\")\\n  obj<- list( x=1:20,y=1:20,z=z )\\n  image(obj, col=cv, main=myarg  )\\n}\\n#my1999 <- pathtofile(\\"RD_501_88101_1999-0.txt.gz\\")\\nmy1999 <- pathtofile(\\"airData1999.txt.gz\\")\\n#my2012 <- pathtofile(\\"RD_501_88101_2012-0.txt.gz\\")\\nmy2012 <- pathtofile(\\"airData2012.txt.gz\\")\\ncnames <- \\"# RD|Action Code|State Code|County Code|Site ID|Parameter|POC|Sample Duration|Unit|Method|Date|Start Time|Sample Value|Null Data Code|Sampling Frequency|Monitor Protocol (MP) ID|Qualifier - 1|Qualifier - 2|Qualifier - 3|Qualifier - 4|Qualifier - 5|Qualifier - 6|Qualifier - 7|Qualifier - 8|Qualifier - 9|Qualifier - 10|Alternate Method Detectable Limit|Uncertainty\\"\\npm0 <- read.table(my1999, comment.char = \\"#\\", header = FALSE, sep = \\"|\\", na.strings = \\"\\")\\npm1 <- read.table(my2012, comment.char = \\"#\\", header = FALSE, sep = \\"|\\", na.strings = \\"\\")\\nwcol <- c(3,4,5,11,13)\\n#pm0 <- pm0[,wcol]\\n#pm1 <- pm1[,wcol]\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Writing_swirl_Courses/Custom_Tests/initLesson.R", "content": "# Code placed in this file will be executed every time the\\n# lesson is started. Any variables created here will show up in\\n# the user\'s working directory and thus be accessible to them\\n# throughout the lesson.\\n\\n# Source utilities.R\\nsource(file.path(find.package(\\"swirl\\"), \\"Courses\\", \\"Writing_swirl_Courses\\", \\"R\\", \\"utilities.R\\"))\\n\\n# Display the customTests.R file.\\ndisplay_swirl_file(\\"customTests.R\\", \\"Writing_swirl_Courses\\", \\"Custom_Tests\\")\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/Least_Squares_Estimation/demofile.R", "content": "file.edit(fname)" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/MultiVar_Examples3/interactplot.R", "content": "plot(hunger$Year,hunger$Numeric,pch=19)\\npoints(hunger$Year,hunger$Numeric,pch=19,col=((hunger$Sex==\\"Male\\")*1+125))\\nabline(c(lmInter$coeff[1],lmInter$coeff[2]),col=\\"red\\",lwd=3)\\nabline(c(lmInter$coeff[1] + lmInter$coeff[3],lmInter$coeff[2] +lmInter$coeff[4]),col=\\"blue\\",lwd=3)" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "R_Programming/Functions/scripts/remainder.R", "content": "# Let me show you an example of a function I\'m going to make up called\\n# increment(). Most of the time I want to use this function to increase the\\n# value of a number by one. This function will take two arguments: \\"number\\" and\\n# \\"by\\" where \\"number\\" is the digit I want to increment and \\"by\\" is the amount I\\n# want to increment \\"number\\" by. I\'ve written the function below. \\n#\\n# increment <- function(number, by = 1){\\n#     number + by\\n# }\\n#\\n# If you take a look in between the parentheses you can see that I\'ve set\\n# \\"by\\" equal to 1. This means that the \\"by\\" argument will have the default\\n# value of 1.\\n#\\n# I can now use the increment function without providing a value for \\"by\\": \\n# increment(5) will evaluate to 6. \\n#\\n# However if I want to provide a value for the \\"by\\" argument I still can! The\\n# expression: increment(5, 2) will evaluate to 7. \\n# \\n# You\'re going to write a function called \\"remainder.\\" remainder() will take\\n# two arguments: \\"num\\" and \\"divisor\\" where \\"num\\" is divided by \\"divisor\\" and\\n# the remainder is returned. Imagine that you usually want to know the remainder\\n# when you divide by 2, so set the default value of \\"divisor\\" to 2. Please be\\n# sure that \\"num\\" is the first argument and \\"divisor\\" is the second argument.\\n#\\n# Hint #1: You can use the modulus operator %% to find the remainder.\\n#   Ex: 7 %% 4 evaluates to 3. \\n#\\n# Remember to set appropriate default values! Be sure to save this \\n# script and type submit() in the console after you write the function.\\n\\nremainder <- function(num, divisor) {\\n  # Write your code here!\\n  # Remember: the last expression evaluated will be returned! \\n}\\n" }\n'
line: b'{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Statistical_Inference/T_Confidence_Intervals/sleepPlot.R", "content": "g <- ggplot(sleep, aes(x = group, y = extra, group = factor(ID)))\\ng <- g + geom_line(size = 1, aes(colour = ID)) + geom_point(size =10, pch = 21, fill = \\"salmon\\", alpha = .5)\\nprint(g)" }\n'
line: b'{ "repo_name": "mariodeng/FirebrowseR", "ref": "refs/heads/master", "path": "R/Samples.Clinical.R", "content": "#\' Retrieve TCGA CDEs verbatim, i.e. not normalized by Firehose.\\n#\' \\n#\' This service returns patient clinical data from TCGA, verbatim. It differs from the Samples/Clinical_FH method by providing access to all TCGA CDEs in their original form, not merely the subset of CDEs normalized by Firehose for analyses.  Results may be selected by disease cohort, patient barcode or CDE name, but at least one cohort, barcode, or CDE must be provided. When filtering by CDE note that only when a patient record contains one or more of the selected CDEs will it be returned. Visit the Metadata/ClinicalNames api function to see the entire list of TCGA CDEs that may be queried via this method. For more information on how clinical data are processed, see our <a href=\\"https://confluence.broadinstitute.org/display/GDAC/Documentation#Documentation-ClinicalPipeline\\">pipeline documentation</a>.\\n#\'\\n#\' @param format Format of result. Default value is json. While json,tsv,csv are available. \\n#\' @param cohort Narrow search to one or more TCGA disease cohorts from the scrollable list. Multiple values are allowed ACC,BLCA,BRCA,CESC,CHOL,COAD,COADREAD,DLBC,ESCA,FPPP,GBM,GBMLGG,HNSC,KICH,KIPAN,KIRC,KIRP,LAML,LGG,LIHC,LUAD,LUSC,MESO,OV,PAAD,PCPG,PRAD,READ,SARC,SKCM,STAD,STES,TGCT,THCA,THYM,UCEC,UCS,UVM.\\n#\' @param tcga_participant_barcode Comma separated list of TCGA participant barcodes (e.g. TCGA-GF-A4EO). Multiple values are allowed .\\n#\' @param cde_name Retrieve results only for specified CDEs, per the Metadata/ClinicalNames function Multiple values are allowed .\\n#\' @param page Which page (slice) of entire results set should be returned.  Multiple values are allowed . Default value is 1.  \\n#\' @param page_size Number of records per page of results.  Max is 2000. Multiple values are allowed . Default value is 150.  \\n#\' @param sort_by Which column in the results should be used for sorting paginated results? Default value is cohort. While tcga_participant_barcode,cohort,cde_name are available. \\n#\' \\n#\' @export\\nSamples.Clinical = function(format = \\"json\\",\\n                             cohort = \\"\\",\\n                             tcga_participant_barcode = \\"\\",\\n                             cde_name = \\"\\",\\n                             page = \\"1\\",\\n                             page_size = \\"150\\",\\n                             sort_by = \\"cohort\\"\\n                             ){\\n                             \\n  parameters = list(format = format,\\n                    cohort = cohort,\\n                    tcga_participant_barcode = tcga_participant_barcode,\\n                    cde_name = cde_name,\\n                    page = page,\\n                    page_size = page_size,\\n                    sort_by = sort_by)\\n  to.Validate = c(\\"cohort\\",\\"tcga_participant_barcode\\",\\"cde_name\\")\\n  validate.Parameters(params = parameters, to.Validate = to.Validate)\\n\\n  url = build.Query(parameters = parameters,\\n                    invoker = \\"Samples\\",\\n                    method = \\"Clinical\\")\\n  ret = download.Data(url, format, page)\\n\\n  return(ret)\\n\\n}\\n" }\n'
line: b'{ "repo_name": "mariodeng/FirebrowseR", "ref": "refs/heads/master", "path": "R/Metadata.SampleTypes.R", "content": "#\' Return all TCGA sample type codes, both numeric and symbolic.\\n#\' \\n#\' \\n#\'\\n#\' @param format Format of result. Default value is json. While json,tsv,csv are available. \\n#\' \\n#\' @export\\nMetadata.SampleTypes = function(format = \\"json\\"\\n                             ){\\n                             \\n  parameters = list(format = format)\\n  \\n  validate.Parameters(params = parameters)\\n\\n  url = build.Query(parameters = parameters,\\n                    invoker = \\"Metadata\\",\\n                    method = \\"SampleTypes\\")\\n  ret = download.Data(url, format)\\n\\n  return(ret)\\n\\n}\\n" }\n'
line: b'{ "repo_name": "mariodeng/FirebrowseR", "ref": "refs/heads/master", "path": "tests/testthat/test.Samples.miRSeq.R", "content": "#library(FirebrowseR)\\ncontext(\\"Samples.miRSeq\\")\\n\\ntest_that(\\"miRSeq data is retrieved correctly\\", {\\n\\n  format = \\"json\\"\\n  mir = c(\\"hsa-mir-1285-3p\\",\\"hsa-mir-125a-5p\\",\\"hsa-mir-221-3p\\",\\"hsa-mir-10b-5p\\",\\"hsa-mir-608\\",\\"hsa-mir-324-5p\\")\\n  cohort = \\"BRCA\\"\\n  tcga_participant_barcode = \\"\\"\\n  tool = \\"miRseq_Mature_Preprocess\\"\\n  sample_type = \\"NT\\"\\n  page = 1\\n  page_size = 250\\n  sort_by = \\"mir\\"\\n\\n  obj = Samples.miRSeq(format = format,\\n                          mir = mir,\\n                          cohort = cohort,\\n                          tcga_participant_barcode = tcga_participant_barcode,\\n                          tool = tool,\\n                          sample_type = sample_type,\\n                          page = page,\\n                          page_size = page_size,\\n                          sort_by = sort_by)\\n  test.q = \\"http://firebrowse.org/api/v1/Samples/miRSeq?format=csv&mir=hsa-mir-1285-3p%2Chsa-mir-125a-5p%2Chsa-mir-221-3p%2Chsa-mir-10b-5p%2Chsa-mir-608%2Chsa-mir-324-5p&cohort=BRCA&tool=miRseq_Mature_Preprocess&sample_type=NT&page=1&page_size=250&sort_by=cohort\\"\\n  test.obj = read.table(test.q, header = T, sep = \\",\\", quote = \\"\\\\\\"\\")\\n  expect_is(obj, \\"list\\")\\n  expect_equal(length(obj[[1]]), nrow(test.obj))\\n  expect_equal(length(obj[[1]][[1]]), ncol(test.obj))\\n  \\n\\n  format = \\"csv\\"\\n  mir = c(\\"hsa-mir-1285-3p\\", \\"hsa-mir-125a-5p\\")\\n  obj = Samples.miRSeq(format = format,\\n                          mir = mir,\\n                          cohort = cohort,\\n                          tcga_participant_barcode = tcga_participant_barcode,\\n                          tool = tool,\\n                          sample_type = sample_type,\\n                          page = page,\\n                          page_size = page_size,\\n                          sort_by = sort_by)\\n  test.q = \\"http://firebrowse.org/api/v1/Samples/miRSeq?format=csv&mir=hsa-mir-1285-3p%2Chsa-mir-125a-5p&cohort=BRCA&tool=miRseq_Mature_Preprocess&sample_type=NT&page=1&page_size=250&sort_by=cohort\\"\\n  test.obj = read.table(test.q, header = T, sep = \\",\\", quote = \\"\\\\\\"\\")\\n  expect_equal(nrow(obj), nrow(test.obj))\\n  expect_equal(ncol(obj), ncol(test.obj))\\n\\n})\\n" }\n'
line: b'{ "repo_name": "mariodeng/FirebrowseR", "ref": "refs/heads/master", "path": "R/Metadata.Counts.R", "content": "#\' Retrieve sample counts.\\n#\' \\n#\' Returns the aliquot counts for each disease cohort, per sample type and data type.  The sample type designation of \\"Tumor\\" may be used to aggregate the count of all tumor aliquots into a single number per disease and data type. See the SampleTypes function for a complete description of sample types.\\n#\'\\n#\' @param format Format of result. Default value is json. While json,tsv,csv are available. \\n#\' @param date Select one or more date stamps. Multiple values are allowed 2016_01_28,2015_11_01,2015_08_21,2015_06_01,2015_04_02,2015_02_04,2014_12_06,2014_10_17,2014_09_02,2014_07_15,2014_05_18,2014_04_16,2014_03_16. Default value is 2016_01_28.  \\n#\' @param cohort Narrow search to one or more TCGA disease cohorts from the scrollable list. Multiple values are allowed ACC,BLCA,BRCA,CESC,CHOL,COAD,COADREAD,DLBC,ESCA,FPPP,GBM,GBMLGG,HNSC,KICH,KIPAN,KIRC,KIRP,LAML,LGG,LIHC,LUAD,LUSC,MESO,OV,PAAD,PCPG,PRAD,READ,SARC,SKCM,STAD,STES,TGCT,THCA,THYM,UCEC,UCS,UVM.\\n#\' @param sample_type Narrow search to one or more TCGA sample types from the scrollable list. Multiple values are allowed FFPE,NB,NBC,NBM,NT,TAM,TAP,TB,TM,TP,TR,Tumor.\\n#\' @param data_type Narrow search to one or more TCGA data types from the scrollable list. Multiple values are allowed bcr,clinical,cn,lowp,methylation,mrna,mrnaseq,mir,mirseq,rppa,maf,rawmaf.\\n#\' @param totals Output an entry providing the totals for each data type. Default value is TRUE. While  are available. \\n#\' @param sort_by Which column in the results should be used for sorting paginated results? Default value is cohort. While cohort are available. \\n#\' \\n#\' @export\\nMetadata.Counts = function(format = \\"json\\",\\n                             date = \\"2016_01_28\\",\\n                             cohort = \\"\\",\\n                             sample_type = \\"\\",\\n                             data_type = \\"\\",\\n                             totals = \\"TRUE\\",\\n                             sort_by = \\"cohort\\"\\n                             ){\\n                             \\n  parameters = list(format = format,\\n                    date = date,\\n                    cohort = cohort,\\n                    sample_type = sample_type,\\n                    data_type = data_type,\\n                    totals = totals,\\n                    sort_by = sort_by)\\n  \\n  validate.Parameters(params = parameters)\\n\\n  url = build.Query(parameters = parameters,\\n                    invoker = \\"Metadata\\",\\n                    method = \\"Counts\\")\\n  ret = download.Data(url, format)\\n\\n  return(ret)\\n\\n}\\n" }\n'
line: b'{ "repo_name": "mariodeng/FirebrowseR", "ref": "refs/heads/master", "path": "R/Metadata.Platforms.R", "content": "#\' Translate TCGA platform codes to full platform names.\\n#\' \\n#\' By default this function returns a table of all of the technology platforms used to sequence or characterize samples in TCGA--both their short platform codes and full names.  A subset of this table may be obtained by explicitly specifying one or more platform codes.\\n#\'\\n#\' @param format Format of result. Default value is json. While json,tsv,csv are available. \\n#\' @param platform Narrow search to one or more TCGA data generation platforms from the scrollable list. Multiple values are allowed 454,ABI,AgilentG4502A_07,AgilentG4502A_07_1,AgilentG4502A_07_2,AgilentG4502A_07_3,bio,biotab,CGH-1x1M_G4447A,diagnostic_images,fh_analyses,fh_reports,fh_stddata,Genome_Wide_SNP_6,GenomeWideSNP_5,H-miRNA_8x15K,H-miRNA_8x15Kv2,H-miRNA_EarlyAccess,H-miRNA_G4470A,HG-CGH-244A,HG-CGH-415K_G4124A,HG-U133_Plus_2,HG-U133A_2,HT_HG-U133A,HuEx-1_0-st-v2,Human1MDuo,HumanHap550,HumanMethylation27,HumanMethylation450,IlluminaDNAMethylation_OMA002_CPI,IlluminaDNAMethylation_OMA003_CPI,IlluminaGA_DNASeq,IlluminaGA_DNASeq_automated,IlluminaGA_DNASeq_Cont,IlluminaGA_DNASeq_Cont_automated,IlluminaGA_DNASeq_Cont_curated,IlluminaGA_DNASeq_curated,IlluminaGA_miRNASeq,IlluminaGA_mRNA_DGE,IlluminaGA_RNASeq,IlluminaGA_RNASeqV2,IlluminaGG,IlluminaHiSeq_DNASeq,IlluminaHiSeq_DNASeq_automated,IlluminaHiSeq_DNASeq_Cont,IlluminaHiSeq_DNASeq_Cont_automated,IlluminaHiSeq_DNASeq_Cont_curated,IlluminaHiSeq_DNASeq_curated,IlluminaHiSeq_DNASeqC,IlluminaHiSeq_miRNASeq,IlluminaHiSeq_mRNA_DGE,IlluminaHiSeq_RNASeq,IlluminaHiSeq_RNASeqV2,IlluminaHiSeq_TotalRNASeqV2,IlluminaHiSeq_WGBS,Mapping250K_Nsp,Mapping250K_Sty,MDA_RPPA_Core,microsat_i,minbio,minbiotab,Mixed_DNASeq,Mixed_DNASeq_automated,Mixed_DNASeq_Cont,Mixed_DNASeq_Cont_automated,Mixed_DNASeq_Cont_curated,Mixed_DNASeq_curated,pathology_reports,SOLiD_DNASeq,SOLiD_DNASeq_automated,SOLiD_DNASeq_Cont,SOLiD_DNASeq_Cont_automated,SOLiD_DNASeq_Cont_curated,SOLiD_DNASeq_curated,tissue_images,WHG-1x44K_G4112A,WHG-4x44K_G4112F,WHG-CGH_4x44B.\\n#\' \\n#\' @export\\nMetadata.Platforms = function(format = \\"json\\",\\n                             platform = \\"\\"\\n                             ){\\n                             \\n  parameters = list(format = format,\\n                    platform = platform)\\n  \\n  validate.Parameters(params = parameters)\\n\\n  url = build.Query(parameters = parameters,\\n                    invoker = \\"Metadata\\",\\n                    method = \\"Platforms\\")\\n  ret = download.Data(url, format)\\n\\n  return(ret)\\n\\n}\\n" }\n'
line: b'{ "repo_name": "alexholmes/hiped2", "ref": "refs/heads/master", "path": "src/main/R/wordcount_rhipe.R", "content": "#! /usr/bin/env Rscript\\nlibrary(Rhipe)\\n\\nrhinit(TRUE,TRUE)\\n\\nm <- expression({\\n  for(x in map.values){\\n    y <- strsplit(x,\\" +\\")[[1]]\\n    for(w in y) rhcollect(w,T)\\n  }\\n})\\n\\nz <- rhmr(map=m,inout=c(\\"text\\",\\"sequence\\"),\\n    ifolder=\\"stocks.txt\\",ofolder=\'/output\',mapred=list(mapred.reduce.tasks=5))\\n\\nrhex(z)" }\n'
line: b'{ "repo_name": "hredestig/pcaMethods", "ref": "refs/heads/master", "path": "R/llsImpute.R", "content": "##\' Missing value estimation using local least squares (LLS).  First,\\n##\' k variables (for Microarrya data usually the genes)  are selected\\n##\' by pearson, spearman or kendall correlation coefficients.  Then\\n##\' missing values are imputed by a linear combination of the k\\n##\' selected variables. The optimal combination is found by LLS\\n##\' regression.  The method was first described by Kim et al,\\n##\' Bioinformatics, 21(2),2005.\\n##\'\\n##\' Missing values are denoted as \\\\code{NA}\\\\cr It is not recommended\\n##\' to use this function directely but rather to use the nni() wrapper\\n##\' function. The methods provides two ways for missing value\\n##\' estimation, selected by the \\\\code{allVariables} option. The first\\n##\' one is to use only complete variables for the  regression. This is\\n##\' preferable when the number of incomplete variables is relatively\\n##\' small.\\n##\' \\n##\' The second way is to consider all variables as candidates for the\\n##\' regression.  Hereby missing values are initially replaced by the\\n##\' columns wise mean.  The method then iterates, using the current\\n##\' estimate as input for the regression until the change between new\\n##\' and old estimate falls below a threshold (0.001).\\n##\' \\n##\' @title LLSimpute algorithm\\n##\' @param Matrix \\\\code{matrix} -- Data containing the variables\\n##\' (genes) in columns and observations (samples) in rows. The data\\n##\' may contain missing values, denoted as \\\\code{NA}.\\n##\' @param k \\\\code{numeric} -- Cluster size, this is the number of\\n##\' similar genes used for regression.\\n##\' @param center \\\\code{boolean} -- Mean center the data if TRUE\\n##\' @param completeObs \\\\code{boolean} -- Return the estimated complete\\n##\' observations if  TRUE. This is the input data with NA values\\n##\' replaced by the estimated values.\\n##\' @param correlation \\\\code{character} -- How to calculate the\\n##\' distance between genes.  One out of pearson | kendall | spearman ,\\n##\' see also help(\\"cor\\").\\n##\' @param allVariables \\\\code{boolean} -- Use only complete genes to\\n##\' do the regression if TRUE, all genes if FALSE.\\n##\' @param maxSteps \\\\code{numeric} -- Maximum number of iteration\\n##\' steps if allGenes = TRUE.\\n##\' @param xval \\\\code{numeric} Use LLSimpute for cross\\n##\' validation. xval is the index of the gene to estimate, all other\\n##\' incomplete genes will be ignored if this parameter is set. We do\\n##\' not consider them in the cross-validation.\\n##\' @param verbose \\\\code{boolean} -- Print step number and relative\\n##\' change if TRUE and  allVariables = TRUE\\n##\' @param ... Reserved for parameters used in future version of the\\n##\' algorithm\\n##\' @note Each step the generalized inverse of a \\\\code{miss} x k\\n##\' matrix is calculated. Where \\\\code{miss} is the number of missing\\n##\' values in  variable j and \\\\code{k} the number of neighbours. This\\n##\' may be slow for large values of k and / or many missing\\n##\' values. See also help(\\"ginv\\").\\n##\' @return   \\\\item{nniRes}{Standard nni (nearest neighbour\\n##\' imputation) result object of this package. See\\n##\' \\\\code{\\\\link{nniRes}} for details.}\\n##\' @seealso \\\\code{\\\\link{pca}, \\\\link{nniRes}, \\\\link{nni}}.\\n##\' @examples\\n##\' ## Load a sample metabolite dataset (metaboliteData) with already 5\\\\% of\\n##\' ## data missing\\n##\' data(metaboliteData)\\n##\' ## Perform llsImpute using k = 10\\n##\' ## Set allVariables TRUE because there are very few complete variables\\n##\' result <- llsImpute(metaboliteData, k = 10, correlation=\\"pearson\\", allVariables=TRUE)\\n##\' ## Get the estimated complete observations\\n##\' cObs <- completeObs(result)\\n##\' @keywords multivariate\\n##\' @export\\n##\' @references Kim, H. and Golub, G.H. and Park, H.  - Missing value\\n##\' estimation for DNA microarray gene expression data: local least\\n##\' squares imputation.  \\\\emph{Bioinformatics, 2005; 21(2):187-198.}\\n##\' \\n##\' Troyanskaya O. and Cantor M. and Sherlock G. and Brown P. and\\n##\' Hastie T. and Tibshirani R. and Botstein D. and Altman RB.  -\\n##\' Missing value estimation methods for DNA microarrays.\\n##\' \\\\emph{Bioinformatics. 2001 Jun;17(6):520-525.}\\n##\' @author Wolfram Stacklies\\nllsImpute <- function(Matrix, k=10, center=FALSE, completeObs=TRUE,\\n                      correlation=\\"pearson\\", \\n                      allVariables=FALSE, maxSteps=100, xval=NULL,\\n                      verbose=FALSE, ...) {\\n\\n    threshold <- 0.001\\n\\n    correlation <- match.arg(correlation, c(\\"pearson\\", \\"kendall\\", \\"spearman\\"))\\n\\n    ## If the data is a data frame, convert it into a matrix\\n    Matrix <- as.matrix(Matrix, rownames.force=TRUE)\\n    ## And now check if everything is right...\\n    if ( !checkData(Matrix, verbose = interactive()) ) {\\n        stop(\\"Invalid data format! Use checkData(Matrix, verbose = TRUE) for details.\\\\n\\")\\n    }\\n\\n    ## Exit if number of neighbours exceeds number of columns\\n    if (k > ncol(Matrix))\\n        stop(\\"Cluster size larger than the number of columns, choose a k < ncol(Matrix)!\\")\\n \\n    ## Set allVariables TRUE if k exceeds number of complete genes\\n    ## Print warning messages in the first case and when less than 50% of all genes are complete\\n    ## and allVariables == FALSE\\n    cg <- sum( apply(is.na(Matrix), 2, sum) == 0)\\n    if ( (k > cg) && (!allVariables) ) {\\n        warning(\\"Cluster size larger than number of complete genes, using allVariables = TRUE\\")\\n        allVariables <- TRUE\\n    } else if ( (cg < (ncol(Matrix) / 2)) && (!allVariables) ) {\\n        warning(\\"Less than 50% of the genes are complete, consider using allVariables = TRUE\\")\\n    } else if (sum(is.na(Matrix)) == 0)\\n        stop(\\"No missing values, no need for missing value imputation :))\\")\\n\\n    ## Find all genes with missing values\\n    missing <- apply(is.na(Matrix), 2, sum) > 0\\n    missIx <- which(missing == TRUE)\\n    # For cross validation we want to only estimate one variable, the others\\n    # are not considered in the cross validation anyway\\n    if (!is.null(xval))\\n        missIx = xval\\n    obs <- Matrix    ## working copy of the data\\n    Ye <- Matrix     ## Estimated complete observations\\n\\n    ## Center the data column wise\\n    if (center) {\\n        obs   <- scale(Matrix, center = TRUE, scale = FALSE)\\n        Ye    <- obs\\n        means <- attr(Ye, \\"scaled:center\\")\\n    }\\n\\n    if (allVariables) {\\n        compIx <- 1:ncol(obs)\\n        ## Impute the row average\\n        rowMeans <- apply(obs, 1, mean, na.rm = TRUE)\\n        for (i in 1:nrow(obs)) {\\n            obs[i, is.na(Matrix[i,])] <- rowMeans[i]\\n        }\\n        ## distances between all genes, ignore the diagonal (correlation to itself)\\n        distance = abs(cor(obs, obs, method = correlation))\\n    } else {\\n        compIx <- which(missing == FALSE)\\n        ## missing genes are the rows, complete genes the columns\\n        distance = abs(cor(obs[,missIx, drop=FALSE], obs[,compIx, drop=FALSE], use=\\"pairwise.complete.obs\\",\\n                       method = correlation))\\n    }\\n\\n    change <- Inf\\n    step <- 0\\n    while ( (change > threshold) && (step < maxSteps) ) {\\n        step <- step + 1\\n        iteration <- 0\\n        \\n        ## Do the regression and imputation\\n        for (index in missIx) {\\n            iteration <- iteration + 1\\n\\t    if (allVariables) {\\n                similar <- sort(distance[iteration,], index.return = TRUE, decreasing = TRUE)\\n                simIx <- compIx[ similar$ix[similar$ix != iteration][1:k] ]\\n            } else {\\n                similar <- sort(distance[iteration,], index.return = TRUE, decreasing = TRUE)\\n                simIx <- compIx[ similar$ix[1:k] ]\\n            }\\n\\n            ##\\n            ## Do a regression against the k most similar genes\\n            ## See Kim et. al 2005 for details\\n            ##\\n            target <- obs[, index, drop = FALSE]\\n            tMiss <- is.na(Matrix[, index, drop = FALSE])\\n\\n            Apart <- obs[!tMiss, simIx, drop = FALSE]\\n            Bpart <- obs[tMiss, simIx, drop = FALSE]\\n            targetComplete <- target[!tMiss, , drop = FALSE]\\n            X <- MASS::ginv(Apart) %*% targetComplete\\n            estimate <- Bpart %*% X\\n\\n            ## Impute the estimate\\n            Ye[tMiss, index] <- estimate\\n        }\\n\\n        ## We do not want to iterate if allVariables == FALSE\\n        if (!allVariables || !is.null(xval)) {\\n            break\\n        } else {\\n            ## relative change in estimation\\n            change <- sqrt(sum( (obs - Ye)^2 ) / sum(obs^2))\\n            obs <- Ye\\n            if (verbose) {\\n                cat(\\"Step number     : \\", step, \'\\\\n\')\\n                cat(\\"Relative change : \\", change, \'\\\\n\')\\n                cat(\\"---------------\\", \'\\\\n\')\\n            }\\n        }\\n    }\\n\\n    ## Add the original mean\\n    if (center) {\\n        for(i in 1:ncol(Ye)) {\\n            Ye[,i] <- Ye[,i] + means[i]\\n        }\\n    }\\n\\n    ## Build the nniRes object\\n    ##\\n    result <- new(\\"nniRes\\")\\n\\n    if(completeObs) {\\n        Ye[!is.na(Matrix)] <- Matrix[!is.na(Matrix)]\\n        result@completeObs <- Ye\\n    }\\n    result@centered        <- center\\n    result@center          <- attr(scale(Matrix, center = TRUE, scale = FALSE), \\"scaled:center\\")\\n    result@nObs            <- nrow(Matrix)\\n    result@nVar            <- ncol(Matrix)\\n    result@method          <- \\"llsImpute\\"\\n    result@correlation     <- correlation\\n    result@k               <- k\\n    result@missing         <- sum(is.na(Matrix))\\n\\n    return(result)        \\n} \\n\\n" }\n'
line: b'{ "repo_name": "anqif/cvxr", "ref": "refs/heads/master", "path": "tests/testthat.R", "content": "library(testthat)\\nlibrary(cvxr)\\n\\ntest_check(\\"cvxr\\")\\n" }\n'
line: b'{ "repo_name": "anqif/cvxr", "ref": "refs/heads/master", "path": "tests/testthat/test_convolution.R", "content": "test_that(\\"test 1D convolution\\", {\\n  n <- 3\\n  x <- Variable(n)\\n  f <- as.matrix(c(1, 2, 3))\\n  g <- as.matrix(c(0, 1, 0.5))\\n  f_conv_g <- as.matrix(c(0, 1, 2.5, 4, 1.5))\\n  expr <- Conv(f, g)\\n  expect_true(is_constant(expr))\\n  expect_equal(size(expr), c(5, 1))\\n  \\n  expr <- Conv(f, x)\\n  expect_true(is_affine(expr))\\n  expect_equal(size(expr), c(5, 1))\\n  \\n  t <- Variable()\\n  prob <- Problem(Minimize(Pnorm(expr, 1)), list(x == g))\\n})\\n\\ntest_that(\\"test a problem with convolution\\", {\\n  N <- 5\\n  y <- matrix(rnorm(N), nrow = N, ncol = 1)\\n  h <- matrix(rnorm(2), nrow = 2, ncol = 1)\\n  x <- Variable(N)\\n  v <- Conv(h, x)\\n  # obj <- Minimize(SumEntries(MulElemwise(y, v[1:N])))\\n})\\n" }\n'
line: b'{ "repo_name": "philchalmers/mirtCAT", "ref": "refs/heads/master", "path": "R/PersonClass.R", "content": "Person <- setRefClass(\\"Person\\", \\n                      \\n                      fields = list(raw_responses = \'character\',\\n                                    responses = \'integer\',\\n                                    items_answered = \'integer\',\\n                                    thetas = \'matrix\',\\n                                    thetas_history = \'matrix\',\\n                                    thetas_SE_history = \'matrix\',\\n                                    info_thetas = \'matrix\',\\n                                    demographics = \'data.frame\',\\n                                    item_time = \'numeric\',\\n                                    valid_item = \'logical\',\\n                                    score = \'logical\'),\\n                      \\n                      methods = list(\\n                         initialize = function(nfact, nitems, thetas.start_in, score,\\n                                               theta_SEs){\\n                             \'Initialize the person object given background information\'\\n                             raw_responses <<- as.character(rep(NA, nitems))\\n                             responses <<- as.integer(rep(NA, nitems))\\n                             valid_item <<- rep(TRUE, nitems)\\n                             items_answered <<- as.integer(rep(NA, nitems))\\n                             thetas <<- matrix(numeric(nfact), nrow=1L)\\n                             thetas_SE_history <<- matrix(theta_SEs, 1L)\\n                             score <<- score\\n                             item_time <<- numeric(nitems)\\n                             if(!is.null(thetas.start_in))\\n                                thetas <<- matrix(thetas.start_in, nrow=1L)\\n                             thetas_history <<- matrix(thetas, 1L, nfact)\\n                             info_thetas <<- matrix(0, nfact, nfact)\\n                         })\\n                      \\n)\\n\\nPerson$methods(\\n    \\n    # Update thetas\\n    Update.thetas = function(design, test){\\n        \'Update the latent trait (theta) values using information \\n        from the design and test objects\'\\n        responses2 <- responses\\n        responses2[design@items_not_scored] <- NA\\n        if(score){\\n            method <- design@method\\n            if(last_item(items_answered) %in% design@items_not_scored)\\n                method <- \'fixed\'\\n            if(method == \'ML\'){\\n                if(length(unique(na.omit(responses2))) < 2L) method <- \'MAP\'\\n            }\\n            if(method != \'fixed\'){\\n                suppressWarnings(tmp <- fscores(test@mo, method=method, response.pattern=responses2,\\n                                   theta_lim=test@fscores_args$theta_lim,\\n                                   MI = test@fscores_args$MI, quadpts = test@quadpts, \\n                                   mean = test@fscores_args$mean, cov = test@fscores_args$cov,\\n                                   QMC=test@fscores_args$QMC, custom_den=test@fscores_args$custom_den))\\n                thetas <<- tmp[,paste0(\'F\', 1L:test@nfact), drop=FALSE]\\n                thetas_SE_history <<- rbind(thetas_SE_history, \\n                                            tmp[,paste0(\'SE_F\', 1L:test@nfact), drop=FALSE])\\n            } else {\\n                thetas_SE_history <<- rbind(thetas_SE_history, \\n                                            thetas_SE_history[nrow(thetas_SE_history),])\\n            }\\n            thetas_history <<- rbind(thetas_history, thetas)\\n            set <- c(\'Drule\', \'Trule\', \'Erule\', \'Wrule\', \'Arule\', \'APrule\',\\n                     \'DPrule\', \'TPrule\', \'EPrule\', \'WPrule\')\\n            if(test@nfact > 1L && design@criteria %in% set){\\n                pick <- which(!is.na(responses2))\\n                infos <- lapply(pick, function(x, thetas)\\n                    FI(extract.item(test@mo, x), Theta=thetas), thetas=thetas)\\n                tmp <- matrix(0, nrow(infos[[1L]]), ncol(infos[[1L]]))\\n                for(i in 1L:length(infos))\\n                    tmp <- tmp + infos[[i]]\\n                if(design@criteria %in% c(\'DPrule\', \'TPrule\', \'EPrule\', \'WPrule\', \'APrule\'))\\n                    tmp <- tmp + solve(test@gp$gcov)\\n                info_thetas <<- tmp\\n            }\\n        }\\n    }\\n)\\n" }\n'
line: b'{ "repo_name": "philchalmers/mirtCAT", "ref": "refs/heads/master", "path": "R/ShinyGUIClass.R", "content": "ShinyGUI <- setRefClass(\\"ShinyGUI\\", \\n                      \\n                      fields = list(title = \'character\',\\n                                    author = \'character\',\\n                                    questions = \'list\',\\n                                    df = \'list\',\\n                                    firstpage = \'list\',\\n                                    demographics = \'list\',\\n                                    lastpage = \'function\',\\n                                    instructions = \'character\',\\n                                    begin_message = \'character\',\\n                                    stem_locations = \'character\',\\n                                    demographic_inputIDs = \'character\',\\n                                    temp_file = \'character\',\\n                                    width = \'numeric\',\\n                                    height = \'numeric\',\\n                                    forced_choice = \'logical\',\\n                                    css = \'character\',\\n                                    stopApp = \'logical\',\\n                                    ui = \'function\'),\\n                      \\n                      methods = list(\\n                          initialize = function(questions, df, shinyGUI){\\n                              \'Initialize the shiny GUI given questions, df, and shinyGUI list\'\\n                              ui <<- default_UI\\n                              questions <<- questions\\n                              df <<- df\\n                              forced_choice <<- TRUE\\n                              stopApp <<- TRUE\\n                              if(is.null(shinyGUI$stem_locations)){\\n                                  stem_locations <<- as.character(rep(NA, length(questions)))\\n                              } else {\\n                                  stem_locations <<- as.character(sapply(shinyGUI$stem_locations, \\n                                    function(x){                                        \\n                                        ret <- if(!is.na(x)){\\n                                            org <- x\\n                                            exsts <- file.exists(x)\\n                                            if(!exsts){\\n                                                x <- paste0(getwd(), \'/\', x)\\n                                                exsts <- file.exists(x)\\n                                            }\\n                                            if(!exsts) \\n                                                stop(sprintf(\'The following file cannot be located: %s\', org), call.=FALSE)\\n                                            normalizePath(x, mustWork = TRUE)\\n                                        } else NA\\n                                        return(ret)\\n                                  }))\\n                              }\\n                              title <<- \'mirtCAT\'\\n                              author <<- \'Author information\'\\n                              instructions <<- c(\\"Instructions:\\",\\n                                                 \\"To progress through the interface, click on the action button below.\\",\\n                                                 \\"Next\\")\\n                              demographic_inputIDs <<- character(0)\\n                              begin_message <<- \\"Click the action button to begin.\\"\\n                              firstpage <<- list(h1(\'Welcome to the mirtCAT interface\'),\\n                                                 \'The following interface was created using the mirtCAT package. \\n                                                 To cite the package use citation(\\\\\'mirtCAT\\\\\') in R.\')\\n                              demographics <<- list()\\n                              lastpage <<- function(person) \\n                                            return(list(h5(\\"You have successfully completed the interface.\\n                                                   Click the action button to terminate the application.\\")))\\n                              if(!is.null(shinyGUI$stopApp) && !shinyGUI$stopApp)\\n                                  lastpage <<- function(person) \\n                                      return(list(h5(\\"You have successfully completed the interface.\\n                                                   Please close the tab/web browser to terminate the application.\\")))\\n                              temp_file <<- \'\'\\n                              css <<- \'\'\\n                                                 \\n                              if(length(shinyGUI)){\\n                                  dnames <- names(shinyGUI)\\n                                  gnames <- c(\'title\', \'authors\', \'instructions\', \'firstpage\', \'demographics\',\\n                                              \'demographics_inputIDs\', \'max_time\', \'temp_file\', \\n                                              \'lastpage\', \'css\', \'stem_dims\', \'forced_choice\', \'stem_locations\',\\n                                              \'begin_message\', \'stopApp\', \'ui\')\\n                                  if(!all(dnames %in% gnames))\\n                                      stop(\'The following inputs to shinyGUI are invalid: \',\\n                                           paste0(dnames[!(dnames %in% gnames)], \' \'), call.=FALSE)\\n                                  if(!is.null(shinyGUI$ui))\\n                                      ui <<- shinyGUI$ui\\n                                  if(!is.null(shinyGUI$instructions))\\n                                      instructions <<- shinyGUI$instructions\\n                                  if(!is.null(shinyGUI$begin_message))\\n                                      begin_message <<- shinyGUI$begin_message\\n                                  if(!is.null(shinyGUI$title))\\n                                      title <<- shinyGUI$title\\n                                  if(!is.null(shinyGUI$authors))\\n                                      author <<- shinyGUI$authors\\n                                  if(!is.null(shinyGUI$firstpage)) \\n                                      firstpage <<- shinyGUI$firstpage\\n                                  if(!is.null(shinyGUI$demographics)){\\n                                      demographics <<- shinyGUI$demographics\\n                                      demographic_inputIDs <<- shinyGUI$demographics_inputIDs\\n                                  }\\n                                  if(!is.null(shinyGUI$forced_choice))\\n                                      forced_choice <<- shinyGUI$forced_choice\\n                                  if(!is.null(shinyGUI$stopApp))\\n                                      stopApp <<- shinyGUI$stopApp\\n                                  if(!is.null(shinyGUI$lastpage)) \\n                                      lastpage <<- shinyGUI$lastpage\\n                                  if(!is.null(shinyGUI$temp_file))\\n                                      temp_file <<- shinyGUI$temp_file\\n                                  if(!is.null(shinyGUI$css))\\n                                      css <<- shinyGUI$css\\n                              }\\n                          })\\n                      \\n)" }\n'
line: b'{ "repo_name": "Netflix/Surus", "ref": "refs/heads/master", "path": "resources/R/RAD/R/anomaly_detection.R", "content": "#\' Time Series Anomaly Detection\\n#\' \\n#\' Fast C++ implementation of time series anomaly detection using Robust Principal Component Pursuit\\n#\' @param X a vector representing a time series, or a data frame where columns are time series.\\n#\' The length of this vector should be divisible by frequency.\\n#\' If X is a vector it will be cast to a matrix of dimension frequency by length(X)/frequency\\n#\' @param frequency the frequency of the seasonality of X\\n#\' @param dates optional vector of dates to be used as a time index in the output\\n#\' @param autodiff boolean. If true, use the Augmented Dickey Fuller Test to determine\\n#\' if differencing is needed to make X stationary\\n#\' @param forcediff boolean. If true, always compute differences\\n#\' @param scale boolean. If true normalize the time series to zero mean and unit variance\\n#\' @param L.penalty a scalar for the amount of thresholding in determining the low rank approximation for X.\\n#\' The default values are chosen to correspond to the smart thresholding values described in Candes\'\\n#\' Stable Principal Component Pursuit\\n#\' @param s.penalty a scalar for the amount of thresholding in determining the separation between noise and sparse outliers\\n#\' The default values are chosen to correspond to the smart thresholding values described in Zhou\'s\\n#\' Stable Principal Component Pursuit\\n#\' @param verbose boolean. If true print status updates while running optimization program\\n#\' @useDynLib RAD\\n#\' @importFrom tseries adf.test\\n#\' @details Robust Principal Component Pursuit is a matrix decomposition algorithm that seeks\\n#\' to separate a matrix X into the sum of three parts X = L + S + E. L is a low rank matrix representing\\n#\' a smooth X, S is a sparse matrix containing corrupted data, and E is noise. To convert a time series\\n#\' into the matrix X we take advantage of seasonality so that each column represents one full period, for\\n#\' example for weekly seasonality each row is a day of week and one column is one full week.\\n#\' \\n#\' While computing the low rank matrix L we take an SVD of X and soft threshold the singular values.\\n#\' This approach allows us to dampen all anomalies across the board simultaneously making the method\\n#\' robust to multiple anomalies. Most techniques such as time series regression and moving averages\\n#\' are not robust when there are two or more anomalies present.\\n#\' \\n#\' Empirical tests show that identifying anomalies is easier if X is stationary.\\n#\' The Augmented Dickey Fuller Test is used to test for stationarity - if X is not stationary\\n#\' then the time series is differenced before calling RPCP. While this test is abstracted away\\n#\' from the user differencing can be forced by setting the forcediff parameter.\\n#\' \\n#\' The thresholding values can be tuned for different applications, however we strongly\\n#\' recommend using the defaults which were proposed by Zhou.\\n#\' For more details on the choice of L.penalty and s.penalty\\n#\' please refer to Zhou\'s 2010 paper on Stable Principal Component Pursuit.\\n#\' \\n#\' The implementation of RPCP is done in C++ for high performance through RCpp.\\n#\' This function simply preprocesses the time series and calls RcppRPCP. \\n#\' @return \\n#\' \\\\itemize{\\n#\'   \\\\item X_transform. The transformation applied to the time series,\\n#\'   can be the identity or could be differencing\\n#\'   \\\\item L_transform. The low rank component in the transformed space\\n#\'   \\\\item S_transform. The sparse outliers in the transformed space\\n#\'   \\\\item E_transform. The noise in the transformed space\\n#\'   \\\\item X_original. The original time series\\n#\'   \\\\item time. The time index\\n#\'   \\\\item name. The name of the time series if X was a named data frame\\n#\' }\\n#\' @references\\n#\' The following are recommended educational material:\\n#\' \\\\itemize{\\n#\'   \\\\item Candes\' paper on RPCP \\\\url{http://statweb.stanford.edu/~candes/papers/RobustPCA.pdf}\\n#\'   \\\\item Zhou\'s follow up paper on Stable PCP \\\\url{http://arxiv.org/abs/1001.2363}\\n#\'   \\\\item Metamarkets Tech Blog on anomalies in time \\\\url{https://metamarkets.com/2012/algorithmic-trendspotting-the-meaning-of-interesting/}\\n#\' }\\n#\' @export\\n#\' @examples\\n#\' frequency = 7\\n#\' numPeriods = 10\\n#\' ts.sinusoidal = sin((2 * pi / frequency ) * 1:(numPeriods * frequency))\\n#\' ts = ts.sinusoidal\\n#\' ts = sin((2 * pi / frequency ) * 1:(numPeriods * frequency))\\n#\' ts[58:60] = 100\\n#\' ggplot_AnomalyDetection.rpca(AnomalyDetection.rpca(ts)) + ggplot2::theme_grey(base_size = 25)\\nAnomalyDetection.rpca = function(X, frequency=7, dates=NULL,\\n                                 autodiff = T,\\n                                 forcediff = F,\\n                                 scale = T,\\n                                 L.penalty = 1,\\n                                 s.penalty=1.4 / sqrt(max(frequency, ifelse(is.data.frame(X), nrow(X), length(X)) / frequency)),\\n                                 verbose=F) {\\n  if (is.vector(X) & !is.data.frame(X)) X = data.frame(y=X)\\n  time = if (is.null(dates)) 1:nrow(X) else dates\\n  \\n  #look through columns which are separate time series\\n  #transform each column vector into a matrix with nrow = observations per period\\n  #the number of columns will be equal to the number of periods\\n  rpca.ts = apply(X, 2, function(j) {\\n    j.init = j[1]\\n    useddiff = F\\n    if (forcediff) {\\n      useddiff = T\\n      j = c(0, diff(j))\\n    }\\n    else if (autodiff) {\\n      adf = suppressWarnings(tseries::adf.test(j))\\n      if (adf$p.value > .05) {useddiff = T; j = c(0, diff(j))}\\n    } \\n    \\n    if (scale) {\\n      j.global.mean = mean(j)\\n      j.global.sd = sd(j)\\n      j.matrix.standard.global = matrix((j - j.global.mean) / j.global.sd, nrow = frequency)\\n      j.matrix = j.matrix.standard.global  \\n    } else {\\n      j.global.mean = 0\\n      j.global.sd = 1\\n      j.matrix = matrix(j, nrow = frequency)\\n    }\\n    \\n    list(rpca = RcppRPCA(j.matrix, \\n                         Lpenalty = L.penalty, Spenalty = s.penalty, \\n                         verbose=verbose),\\n         mean = j.global.mean,\\n         sd = j.global.sd,\\n         diff = useddiff,\\n         j.init = j.init\\n    )\\n  })\\n  rpca.ts.stacked = lapply(rpca.ts, function(i) {\\n    if (i$diff) {\\n      X.orig = c(i$j.init + cumsum((as.vector(i$rpca$X)) * i$sd + i$mean))\\n      X.transform = (as.vector(i$rpca$X)) * i$sd + i$mean\\n      L.transform = (as.vector(i$rpca$L)) * i$sd + i$mean\\n      S.transform = (as.vector(i$rpca$S)) * i$sd\\n      E.transform = (as.vector(i$rpca$E)) * i$sd\\n      \\n      L.orig = cumsum(L.transform) + i$j.init\\n      X.rough = X.orig - L.orig\\n      \\n      #S.orig = cumsum(S.transform)\\n      #E.orig = X.orig - L.orig - S.orig\\n      \\n      ###       \\n      #\\n      #X.rough.rpca = RcppRPCA(matrix(X.rough, nrow(i$rpca$X), ncol(i$rpca$X)),\\n      #                        Lpenalty = 10,\\n      #                        Spenalty = 2 / sqrt(10))\\n      #S.orig = as.numeric(X.rough.rpca$S)\\n      #E.orig = X.orig - L.orig - S.orig\\n      \\n      ###\\n      S.orig = softThreshold(X.rough, 3 * (1/sqrt(2)) * sd(E.transform))\\n      E.orig = X.orig - (L.orig) - S.orig\\n      \\n      data.frame(X.transform = X.transform,\\n                 L.transform = L.transform,\\n                 S.transform = S.transform,\\n                 E.transform = E.transform,\\n                 X.orig = X.orig,\\n                 time = time)[-1,]\\n    }\\n    else {\\n      data.frame(X.transform = (as.vector(i$rpca$X)) * i$sd + i$mean,\\n                 L.transform = (as.vector(i$rpca$L)) * i$sd + i$mean,\\n                 S.transform = (as.vector(i$rpca$S)) * i$sd,\\n                 E.transform = (as.vector(i$rpca$E)) * i$sd,\\n                 X.orig = (as.vector(i$rpca$X)) * i$sd + i$mean,\\n                 time = time)\\n    }\\n  })\\n  names = unlist((mapply(function(df, name) { rep(name, nrow(df)) }, rpca.ts.stacked, names(rpca.ts))))\\n  #build a report containing anomaly data for all the columns found in X\\n  rpca.ts.stacked = cbind(do.call(\'rbind\', rpca.ts.stacked), name = as.vector(names))\\n  names(rpca.ts.stacked) = c(\\"X_transform\\", \\"L_transform\\", \\"S_transform\\", \\"E_transform\\",\\n                             \\"X_original\\",\\n                             \\"time\\", \\"name\\")\\n  \\n  return (rpca.ts.stacked)\\n}\\n\\n#\' ggplot for AnomalyDetection\\n#\' \\n#\' ggplot function which shows the low rank signal in blue, the random noise in green,\\n#\' and any outliers in red. If a transformation was applied, these signals will be plotted\\n#\' in the transformed space, along with the original time series\\n#\' @param anomalyDetection output from AnomalyDetection.rpca\\n#\' @import ggplot2\\n#\' @export\\n#\' @examples\\n#\' frequency = 7\\n#\' numPeriods = 10\\n#\' ts.sinusoidal = sin((2 * pi / frequency ) * 1:(numPeriods * frequency))\\n#\' ts = ts.sinusoidal\\n#\' ts = sin((2 * pi / frequency ) * 1:(numPeriods * frequency))\\n#\' ts[58:60] = 100\\n#\' ggplot_AnomalyDetection.rpca(AnomalyDetection.rpca(ts)) + ggplot2::theme_grey(base_size = 25)\\nggplot_AnomalyDetection.rpca = function(anomalyDetection) {\\n  ggplot2::ggplot(anomalyDetection, ggplot2::aes(time, X_original)) +\\n    ggplot2::geom_line(size = 1) +\\n    ggplot2::geom_line(ggplot2::aes(y = X_transform), size = 1, color = \\"black\\", linetype = \'dashed\') +\\n    ggplot2::geom_line(ggplot2::aes(y = L_transform), size = .5, color = \\"blue\\") +\\n    ggplot2::geom_line(ggplot2::aes(y = E_transform), size = .5, color = \\"green\\") +\\n    ggplot2::geom_point(data = subset(anomalyDetection, abs(S_transform) > 0), color = \\"red\\",\\n               ggplot2::aes(size = abs(S_transform))) +\\n    ggplot2::scale_size_continuous(range=c(4,6)) +\\n    ggplot2::facet_wrap(~name, scale = \\"free\\")    \\n}\\n\\nsoftThreshold = function(x, penalty) {\\n  sign(x) * pmax(abs(x) - penalty,0)\\n}\\n" }\n'
line: b'{ "repo_name": "Netflix/Surus", "ref": "refs/heads/master", "path": "resources/R/RAD/R/anomaly_detection_ma.R", "content": "AnomalyDetection.ma = function(X, frequency=7) {\\n  if (is.vector(X) & !is.data.frame(X)) X = data.frame(y=X)\\n  \\n  ma.ts = do.call(\'rbind\', apply(X, 2, function(j) {\\n    j.matrix = matrix(j, nrow= frequency)\\n    means = apply(j.matrix[,1:(ncol(j.matrix)-1)], 1, mean)\\n    sds = apply(j.matrix[,1:(ncol(j.matrix)-1)],1,sd)\\n    upperbounds = means + 1.6*sds\\n    lowerbounds = means - 1.6*sds\\n    anomalous = t(apply(cbind(upperbounds, lowerbounds, j.matrix), 1, function(i) {\\n      i[-(1:2)] > i[1] | i[-(1:2)] < i[2]\\n    }))\\n    data.frame(X = j,\\n               time = 1:length(j),\\n               anomaly = as.vector(anomalous))\\n  }))\\n  ma.ts = cbind(ma.ts, name = rep(names(X), each = nrow(X)))\\n  \\n  return (ma.ts)\\n}\\n\\nggplot_AnomalyDetection.ma = function(anomalyDetection) {\\n  ggplot(anomalyDetection,\\n         aes(x = time, y=X)) +\\n    geom_line(size = 1) +\\n    geom_point(data = subset(anomalyDetection, anomaly == T), color = \'red\', size = 6) +\\n    facet_wrap(~name, scale = \'free\')\\n}" }\n'
line: b'{ "repo_name": "AgResearch/KGD", "ref": "refs/heads/master", "path": "GBSRun.R", "content": "#!/bin/env Rscript\\n\\ngenofile <- \\"Example/HapMap.hmc.txt.gz\\"\\ngform <- \\"uneak\\"   # uneak (default), Tassel or chip\\n\\nsource(\\"GBS-Chip-Gmatrix.R\\")\\n\\nGfull      <- calcG()\\nGHWdgm.05  <- calcG(which(HWdis > -0.05), \\"HWdgm.05\\", npc = 4)  # recalculate using Hardy-Weinberg disequilibrium cut-off at -0.05\\n\\npedfile    <- \\"Example/Ped-GBS.csv\\"\\ngroupsfile <- \\"Example/Ped-Groups.csv\\"\\n\\nrel.thresh <- 0.2\\nGCheck     <- \\"GHWdgm.05$G5\\"\\nsource(\\"GBSPedAssign.R\\")\\n\\n# G5 <- GHWdgm.05$G5 save(G5,file=\'G5.RData\')\\n" }\n'
line: b'{ "repo_name": "gabraham/flashpca", "ref": "refs/heads/master", "path": "flashpcaR/tests/testthat/test_pca.R", "content": "context(\\"Checking PCA\\")\\n\\nn <- 200\\np <- 1000\\nndim <- 50\\nnextra <- 50\\n\\ntest_that(\\"Testing PCA with stand=\'binom\'\\", {\\n   X <- matrix(sample(0:2, n * p, replace=TRUE), n, p)\\n   q <- colMeans(X) / 2\\n   S <- scale(X, center=TRUE, scale=sqrt(q * (1 - q)))\\n\\n   f1 <- prcomp(S, center=FALSE, scale.=FALSE)\\n   f2 <- flashpca(X, ndim=ndim, mem=\\"low\\", nextra=nextra)\\n   f3 <- flashpca(X, ndim=ndim, mem=\\"high\\", nextra=nextra)\\n\\n   expect_equal(attr(S, \\"scaled:center\\"), f2$center)\\n   expect_equal(attr(S, \\"scaled:scale\\"), f2$scale)\\n   expect_equal(attr(S, \\"scaled:center\\"), f3$center)\\n   expect_equal(attr(S, \\"scaled:scale\\"), f3$scale)\\n\\n   r0 <- abs(diag(cor(f2$projection, f3$projection)))\\n   expect_equal(r0, rep(1.0, ndim), tolerance=1e-3)\\n\\n   r1 <- abs(diag(cor(f1$x[, 1:ndim], f2$projection)))\\n   expect_equal(r1, rep(1.0, ndim), tolerance=1e-3)\\n\\n   r2 <- abs(diag(cor(f1$x[, 1:ndim], f3$projection)))\\n   expect_equal(r2, rep(1.0, ndim), tolerance=1e-3)\\n})\\n\\n\\ntest_that(\\"Testing PCA with stand=\'sd\'\\", {\\n   X <- matrix(rnorm(n * p), n, p)\\n   S <- scale(X, center=TRUE, scale=TRUE)\\n\\n   f1 <- prcomp(S, center=FALSE, scale.=FALSE)\\n   f2 <- flashpca(X, ndim=ndim, stand=\\"sd\\", mem=\\"low\\", nextra=nextra)\\n   f3 <- flashpca(X, ndim=ndim, stand=\\"sd\\", mem=\\"high\\", nextra=nextra)\\n\\n   expect_equal(attr(S, \\"scaled:center\\"), f2$center)\\n   expect_equal(attr(S, \\"scaled:scale\\"), f2$scale)\\n   expect_equal(attr(S, \\"scaled:center\\"), f3$center)\\n   expect_equal(attr(S, \\"scaled:scale\\"), f3$scale)\\n\\n   r1 <- abs(diag(cor(f1$x[, 1:ndim], f2$projection)))\\n   expect_equal(r1, rep(1.0, ndim), tolerance=1e-3)\\n\\n   r2 <- abs(diag(cor(f1$x[, 1:ndim], f3$projection)))\\n   expect_equal(r2, rep(1.0, ndim), tolerance=1e-3)\\n})\\n\\ntest_that(\\"Testing PCA with stand=\'none\'\\", {\\n   X <- matrix(rnorm(n * p), n, p)\\n\\n   f1 <- prcomp(X, center=FALSE, scale.=FALSE)\\n   f2 <- flashpca(X, ndim=ndim, stand=\\"none\\", mem=\\"low\\", nextra=nextra)\\n   f3 <- flashpca(X, ndim=ndim, stand=\\"none\\", mem=\\"high\\", nextra=nextra)\\n\\n   expect_identical(numeric(0), f2$center)\\n   expect_identical(numeric(0), f2$scale)\\n   expect_identical(numeric(0), f3$center)\\n   expect_identical(numeric(0), f3$scale)\\n\\n   r1 <- abs(diag(cor(f1$x[, 1:ndim], f2$projection)))\\n   expect_equal(r1, rep(1.0, ndim), tolerance=1e-3)\\n\\n   r2 <- abs(diag(cor(f1$x[, 1:ndim], f3$projection)))\\n   expect_equal(r2, rep(1.0, ndim), tolerance=1e-3)\\n})\\n\\ntest_that(\\"Testing PCA with stand=\'center\'\\", {\\n   X <- matrix(rnorm(n * p), n, p)\\n   S <- scale(X, center=TRUE, scale=FALSE)\\n\\n   f1 <- prcomp(S, center=FALSE, scale.=FALSE)\\n   f2 <- flashpca(X, ndim=ndim, mem=\\"low\\", stand=\\"center\\", nextra=nextra)\\n   f3 <- flashpca(X, ndim=ndim, mem=\\"high\\", stand=\\"center\\", nextra=nextra)\\n\\n   expect_equal(attr(S, \\"scaled:center\\"), f2$center)\\n   expect_equal(rep(1, p), f2$scale)\\n   expect_equal(attr(S, \\"scaled:center\\"), f3$center)\\n   expect_equal(rep(1, p), f3$scale)\\n\\n   r1 <- abs(diag(cor(f1$x[, 1:ndim], f2$projection)))\\n   expect_equal(r1, rep(1.0, ndim), tolerance=1e-3)\\n\\n   r2 <- abs(diag(cor(f1$x[, 1:ndim], f3$projection)))\\n   expect_equal(r2, rep(1.0, ndim), tolerance=1e-3)\\n})\\n\\n" }\n'
line: b'{ "repo_name": "gabraham/flashpca", "ref": "refs/heads/master", "path": "test.R", "content": "\\nset.seed(32312)\\n\\nlibrary(flashpcaR)\\n\\nn <- 1000\\np <- 5000\\nX <- matrix(rnorm(n * p), n, p)\\nX <- scale(X, center=TRUE, scale=FALSE)\\n\\nsystem.time({\\n   S <- tcrossprod(X) / (nrow(X) - 1)\\n   e <- eigen(S)\\n})\\n\\nsystem.time({\\n   s <- svd(X)\\n})\\n\\nnthr <- 1\\ntol <- 1e-9\\nk <- 20\\nsystem.time({\\n   f1 <- flashpca(X, ndim=k, stand=\\"center\\", transpose=FALSE, tol=tol,\\n      num_threads=nthr, maxiter=100)\\n})\\nf2 <- flashpca(t(X), ndim=k, stand=\\"center\\", transpose=TRUE, tol=tol,\\n   num_threads=nthr, maxiter=100)\\nf3 <- flashpca(X, ndim=k, stand=\\"none\\", transpose=FALSE, tol=tol,\\n   num_threads=nthr, maxiter=100)\\nf4 <- flashpca(t(X), ndim=k, stand=\\"none\\", transpose=TRUE, tol=tol,\\n   num_threads=nthr, maxiter=100)\\n\\n(r <- cbind(\\n   e=e$val[1:k],\\n   f1=f1$val,\\n   f2=f2$val,\\n   f3=f3$val,\\n   f4=f4$val,\\n   s=s$d[1:k]^2 / (nrow(X) - 1)\\n))\\n\\ncor(r)\\n\\n" }\n'
line: b'{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "inst/ShinyStan/server_files/pages/diagnose/server/multitrace.R", "content": "# \\n# # multiparameter traceplots -----------------------------------------------\\n# calc_height_trace_plot <- reactive({\\n#   params <- input$multitrace_params\\n#   grid <- FALSE\\n#   if (!is.null(input$multitrace_layout)) {\\n#     if (input$multitrace_layout == \\"Grid\\") grid <- TRUE\\n#   }\\n#   params <- .update_params_with_groups(params, param_names)\\n#   LL <- length(params)\\n#   if (LL == 0) LL <- 4\\n#   if (LL == 1) LL <- 2\\n#   if (grid) {\\n#     if (LL > 5) return(30*LL)\\n#     if (LL < 5) return(60*LL)\\n#   }\\n#   round(100*LL)\\n# })\\n# \\n# # multitrace_plot\\n# multitrace_plot <- reactive({\\n#   validate(need(!is.null(input$multitrace_rect), message = \\"Loading...\\"))\\n#   x1 <- input$multi_xzoom[1]\\n#   x2 <- input$multi_xzoom[2]\\n#   dat <- samps_all[x1:x2,,,drop=FALSE]\\n#   # zoom <- \\"On\\"\\n#   do.call(\\".param_trace_multi\\", args = list(\\n#     params      = input$multitrace_params,\\n#     all_param_names = param_names,\\n#     dat         = dat,\\n#     chain       = input$multitrace_chain,\\n#     warmup_val  = warmup_val,\\n#     palette     = input$multitrace_palette ,\\n#     rect        = input$multitrace_rect,\\n#     rect_color  = \\"skyblue\\",\\n#     rect_alpha  = input$multitrace_rect_alpha,\\n#     layout      = input$multitrace_layout,\\n#     x1          = x1,\\n#     x2          = x2\\n#   ))\\n# })\\n# \\n# output$multitrace_plot_out <- renderPlot({\\n#   x <- multitrace_plot()\\n#   suppressWarnings(print(x)) # this avoids warnings about removing rows when using tracezoom feature\\n# }, height = calc_height_trace_plot, bg = \\"transparent\\")\\n# \\n# # download the plot\\n# output$download_multitrace <- downloadHandler(\\n#   filename = paste0(\'shinystan_multitrace.RData\'),\\n#   content = function(file) {\\n#     shinystan_multitrace <- multitrace_plot()\\n#     save(shinystan_multitrace, file = file)\\n#   }\\n# )\\n" }\n'
line: b'{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "R/generate_quantity.R", "content": "# shinystan is free software; you can redistribute it and/or modify it under the\\n# terms of the GNU General Public License as published by the Free Software\\n# Foundation; either version 3 of the License, or (at your option) any later\\n# version.\\n# \\n# shinystan is distributed in the hope that it will be useful, but WITHOUT ANY\\n# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR\\n# A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\\n# \\n# You should have received a copy of the GNU General Public License along with\\n# this program; if not, see <http://www.gnu.org/licenses/>.\\n\\n\\n#\' Add new quantity to shinystan object\\n#\' \\n#\' Add to shinystan object a new parameter as a function of one or two existing\\n#\' parameters.\\n#\' \\n#\' @export\\n#\' @template args-sso\\n#\' @param fun Function to call, i.e. \\\\code{function(param1)} or \\n#\'   \\\\code{function(param1,param2)}. See Examples, below.\\n#\' @param param1 Name of first parameter as character string.\\n#\' @param param2 Optional. Name of second paramter as character string.\\n#\' @param new_name Name for the new parameter as character string.\\n#\'   \\n#\' @return sso, updated. See Examples.\\n#\' \\n#\' @template seealso-drop_parameters\\n#\'\\n#\' @examples\\n#\' # Using example shinystan object \'eight_schools\'\\n#\' sso <- eight_schools\\n#\' sso <- generate_quantity(sso, fun = function(x) x^2, \\n#\'                          param1 = \\"tau\\", new_name = \\"tau_sq\\")\\n#\' sso <- generate_quantity(sso, fun = \\"-\\", \\n#\'                          param1 = \\"theta[1]\\", param2 = \\"theta[2]\\", \\n#\'                          new_name = \\"theta1minus2\\")\\n#\'                          \\ngenerate_quantity <- function(sso, param1, param2, fun, new_name) {\\n  sso_check(sso)\\n  if (isTRUE(new_name %in% slot(sso, \\"param_names\\")))\\n    stop(paste(\\"There is already a parameter named\\", new_name))\\n  \\n  message(\\"\\\\nThis might take a moment for large shinystan objects...\\")\\n  \\n  two_params <- !missing(param2)\\n  posterior <- slot(sso, \\"posterior_sample\\")\\n  dims <- dim(posterior)\\n  ndim <- length(dims)\\n  if (ndim == 3) {\\n    # i.e. multiple chains\\n    x_samp <- posterior[, , param1]\\n    if (two_params)\\n      y_samp <- posterior[, , param2]\\n  }\\n  if (ndim == 2) {\\n    # i.e. only 1 chain\\n    x_samp <- posterior[, param1]\\n    if (two_params)\\n      y_samp <- posterior[, param2]\\n  }\\n  \\n  arglist <- if (two_params)\\n    list(x_samp, y_samp) else list(x_samp)\\n  temp <- do.call(fun, args = arglist)\\n  \\n  new_dim <- dims\\n  new_dim[[ndim]] <- new_dim[[ndim]] + 1\\n  new_dim_names <- dimnames(posterior)\\n  new_dim_names[[ndim]] <- c(new_dim_names[[ndim]], new_name)\\n  posterior <-\\n    array(data = c(posterior, temp),\\n          dim = new_dim,\\n          dimnames = new_dim_names)\\n  \\n  param_dims_new <- slot(sso, \\"param_dims\\")\\n  param_dims_new[[new_name]] <- numeric(0)\\n  sso_new <- as.shinystan(\\n    posterior,\\n    model_name = slot(sso, \\"model_name\\"),\\n    burnin = slot(sso, \\"n_warmup\\"),\\n    param_dims = param_dims_new\\n  )\\n  slot(sso_new, \\"summary\\") <-\\n    shinystan_monitor(posterior, warmup = slot(sso, \\"n_warmup\\"))\\n  \\n  slot_names <- c(\\"sampler_params\\", \\"model_code\\", \\"user_model_info\\", \\"misc\\")\\n  for (sn in slot_names)\\n    slot(sso_new, sn) <- slot(sso, sn)\\n  \\n  sso_new\\n}\\n" }\n'
line: b'{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "R/sso-metadata.R", "content": "# shinystan is free software; you can redistribute it and/or modify it under the\\n# terms of the GNU General Public License as published by the Free Software\\n# Foundation; either version 3 of the License, or (at your option) any later\\n# version.\\n# \\n# shinystan is distributed in the hope that it will be useful, but WITHOUT ANY\\n# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR\\n# A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\\n# \\n# You should have received a copy of the GNU General Public License along with\\n# this program; if not, see <http://www.gnu.org/licenses/>.\\n\\n\\n#\' View or change metadata associated with a shinystan object\\n#\' \\n#\' @name shinystan-metadata\\n#\' @template args-sso\\n#\' \\n#\' @template seealso-as.shinystan\\n#\' @template seealso-drop_parameters\\n#\' @template seealso-generate_quantity\\n#\' \\n#\' @examples \\n#\' # use eight_schools example object\\n#\' sso <- eight_schools\\n#\' \\nNULL\\n\\n# sso_info ----------------------------------------------------------------\\n#\' @rdname shinystan-metadata\\n#\' @export\\n#\' \\n#\' @return \\\\code{sso_info} prints basic metadata including number of parameters, \\n#\'   chains, iterations, warmup iterations, etc. It does not return anything.\\n#\' \\n#\' @examples \\n#\' ################\\n#\' ### sso_info ###\\n#\' ################\\n#\' \\n#\' sso_info(sso)\\n#\'\\nsso_info <- function(sso) {\\n  sso_check(sso)\\n  sso_name <- deparse(substitute(sso))\\n  has_notes <-\\n    sso@user_model_info != \\"Use this space to store notes about your model\\"\\n  has_code <-\\n    sso@model_code != \\"Use this space to store your model code\\"\\n  \\n  cat(\\n    sso_name,\\n    \\"---------------------\\",\\n    paste(\\"Model name:\\", sso@model_name),\\n    paste(\\"Parameters:\\", length(sso@param_names)),\\n    paste(\\"Parameter groups:\\", length(names(sso@param_dims))),\\n    paste(\\"Chains:\\", sso@n_chain),\\n    paste(\\"Iterations:\\", sso@n_iter),\\n    paste(\\"Warmup:\\", sso@n_warmup),\\n    paste(\\"Has model code:\\", has_code),\\n    paste(\\"Has user notes:\\", has_notes),\\n    sep = \\"\\\\n\\"\\n  )\\n}\\n\\n\\n\\n# model_code --------------------------------------------------------------\\n#\' @rdname shinystan-metadata\\n#\' @export\\n#\' @param code A string, containing model code to be added, that can be\\n#\'   used as an argument to \\\\code{\\\\link{cat}}. See \\\\strong{Examples}.\\n#\'   \\n#\' @return \\\\code{model_code} returns or replaces model code stored in a \\n#\'   shinystan object. If \\\\code{code} is \\\\code{NULL} then any existing model\\n#\'   code stored in \\\\code{sso} is returned as a character string. If \\\\code{code}\\n#\'   is specified then an updated shinystan object is returned with \\\\code{code}\\n#\'   added. For shinystan objects created from stanfit (\\\\pkg{rstan}) and stanreg\\n#\'   (\\\\pkg{rstanarm}) objects, model code is automatically taken from that\\n#\'   object and does not need to be added manually. From within the ShinyStan\\n#\'   interface model code can be viewed on the \\\\strong{Model Code} page.\\n#\'\\n#\' @examples\\n#\' ##################\\n#\' ### model_code ###\\n#\' ##################\\n#\' \\n#\' # view model code in example shinystan object \'eight_schools\'\\n#\' cat(model_code(sso))\\n#\' \\n#\' # change the model code in sso \\n#\' # some jags style code\\n#\' my_code <- \\"\\n#\'  model {\\n#\'    for (i in 1:length(Y)) {\\n#\'      Y[i] ~ dpois(lambda[i])\\n#\'      log(lambda[i]) <- inprod(X[i,], theta[])\\n#\'    }\\n#\'    for (j in 1:J) {\\n#\'      theta[j] ~ dt(0.0, 1.0, 1.0)\\n#\'    }\\n#\'  }\\n#\' \\"\\n#\' sso <- model_code(sso, my_code)\\n#\' cat(model_code(sso))\\n#\'\\nmodel_code <- function(sso, code = NULL) {\\n  sso_check(sso)\\n  validate_model_code(code)\\n  \\n  if (is.null(code))\\n    return(slot(sso, \\"model_code\\"))\\n  \\n  slot(sso, \\"model_code\\") <- code\\n  message(\\n    paste0(\\n      \\"Successfully added code.\\",\\n      \\"\\\\nYou can view the code in the\\",\\n      \\"ShinyStan GUI on the \'Model Code\' page.\\"\\n    )\\n  )\\n  sso\\n}\\n\\nvalidate_model_code <- function(code) {\\n  if (is.null(code) || is.character(code)) {\\n    invisible(TRUE)\\n  } else {\\n    stop(\\"Model code should be NULL or a string\\", call. = FALSE)\\n  }\\n}\\n\\n\\n\\n# notes -------------------------------------------------------------------\\n#\' @rdname shinystan-metadata\\n#\' @export\\n#\' @param note A string containing a note to add to any existing notes\\n#\'   or replace existing notes, depending on the value of \\\\code{replace}.\\n#\' @param replace If \\\\code{TRUE} the existing notes are overwritten by \\n#\'   \\\\code{note} if \\\\code{note} is specified. If \\\\code{FALSE} (the default) \\n#\'   if \\\\code{note} is specified then its content is appended to the existing\\n#\'   notes.\\n#\'   \\n#\' @return \\\\code{notes} returns, amends, or replaces notes stored in a shinystan\\n#\'   object. If \\\\code{note} is \\\\code{NULL} then any existing notes stored in \\n#\'   \\\\code{sso} are returned as a character string. If \\\\code{note} is specified \\n#\'   then an updated shinystan object is returned with either \\\\code{note} added \\n#\'   to the previous notes (if \\\\code{replace=FALSE}) or overwritten by \\n#\'   \\\\code{note} (if \\\\code{replace = TRUE}). From within the ShinyStan\\n#\'   interface, notes are viewable on the \\\\strong{Notepad} page.\\n#\'   \\n#\' @examples \\n#\' #############\\n#\' ### notes ###\\n#\' #############\\n#\' \\n#\' # view existing notes\\n#\' notes(sso)\\n#\' \\n#\' # add a note to the existing notes\\n#\' sso <- notes(sso, \\"New note\\")\\n#\' notes(sso)\\n#\' cat(notes(sso))\\n#\' \\n#\' # replace existing notes\\n#\' sso <- notes(sso, \\"replacement note\\", replace = TRUE)\\n#\' notes(sso)\\n#\'  \\nnotes <- function(sso, note = NULL, replace = FALSE) {\\n  sso_check(sso)\\n  if (is.null(note))\\n    return(slot(sso, \\"user_model_info\\"))\\n  \\n  if (!is.character(note) || !isTRUE(length(note) == 1))\\n    stop(\\"\'note\' should be a single string\\")\\n  \\n  slot(sso, \\"user_model_info\\") <- if (replace)\\n    note else c(slot(sso, \\"user_model_info\\"), paste0(\\"\\\\n\\\\n\\", note))\\n  \\n  message(\\n    paste(\\n      \\"Successfully added note.\\",\\n      \\"\\\\nYou can view the notes in the\\",\\n      \\"ShinyStan GUI on the \'Notepad\' page.\\"\\n    )\\n  )\\n  sso\\n}\\n\\n\\n\\n# model_name (renaming) -----------------------------------------------------#\' \\n#\' @rdname shinystan-metadata\\n#\' @export\\n#\' @param name A string giving the new model name to use.\\n#\'   \\n#\' @return \\\\code{model_name} returns or replaces the model name associated with \\n#\'   a shinystan object. If \\\\code{name} is \\\\code{NULL} then the current model\\n#\'   name is returned. If \\\\code{name} is specified then \\\\code{sso} is returned\\n#\'   with an updated model name.\\n#\' \\n#\' @examples\\n#\' ##################\\n#\' ### model_name ###\\n#\' ##################\\n#\' \\n#\' # view model name\\n#\' model_name(sso)\\n#\' \\n#\' # change model name\\n#\' sso <- model_name(sso, \\"some other name\\")\\n#\' identical(model_name(sso), \\"some other name\\")\\n#\' \\nmodel_name <- function(sso, name = NULL) {\\n  sso_check(sso)\\n  if (is.null(name))\\n    return(slot(sso, \\"model_name\\"))\\n  \\n  if (!is.character(name) || !isTRUE(length(name) == 1))\\n    stop(\\"\'name\' should be a single string\\")\\n  \\n  slot(sso, \\"model_name\\") <- name\\n  message(paste(\\"Successfully changed model name to\\", name))\\n  sso\\n}\\n\\n\\n# nocov start\\n#\' rename_model (deprecated)\\n#\' \\n#\' This function is deprecated and will be removed in a future release. Please \\n#\' use the \\\\code{\\\\link{model_name}} function instead.\\n#\' \\n#\' @export\\n#\' @keywords internal\\n#\' @param sso,new_model_name Use the \\\\code{\\\\link{model_name}} function instead.\\n#\' \\nrename_model <- function(sso, new_model_name) {\\n  .Deprecated(\\"model_name()\\")\\n  model_name(sso, new_model_name)\\n}\\n# nocov end\\n" }\n'
line: b'{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "R/launch_shinystan.R", "content": "# shinystan is free software; you can redistribute it and/or modify it under the\\n# terms of the GNU General Public License as published by the Free Software\\n# Foundation; either version 3 of the License, or (at your option) any later\\n# version.\\n# \\n# shinystan is distributed in the hope that it will be useful, but WITHOUT ANY\\n# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR\\n# A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\\n# \\n# You should have received a copy of the GNU General Public License along with\\n# this program; if not, see <http://www.gnu.org/licenses/>.\\n\\n\\n#\' Launch the ShinyStan app\\n#\' \\n#\' Launch the ShinyStan app in the default web browser. RStudio users also have\\n#\' the option of launching the app in RStudio\'s pop-up Viewer.\\n#\' \\n#\' @export\\n#\' @param object An object of class shinystan, stanfit, or stanreg. To use other\\n#\'   types of objects first create a shinystan object using \\n#\'   \\\\code{\\\\link{as.shinystan}}.\\n#\' @param rstudio Only relevant for RStudio users. The default (\\\\code{FALSE}) is\\n#\'   to launch the app in the user\'s default web browser rather than RStudio\'s\\n#\'   pop-up Viewer. Users can change the default to \\\\code{TRUE} by setting the\\n#\'   global option \\\\code{options(shinystan.rstudio = TRUE)}.\\n#\' @param ... Optional arguments passed to \\\\code{\\\\link[shiny]{runApp}}.\\n#\' \\n#\' @return The \\\\code{launch_shinystan} function is used for the side effect of \\n#\'   starting the ShinyStan app, but it also returns a shinystan object, an\\n#\'   instance of S4 class \\\\code{\\"shinystan\\"}.\\n#\'   \\n#\' @template seealso-as.shinystan \\n#\' @template seealso-update_sso \\n#\' @template seealso-demo\\n#\'   \\n#\'   \\n#\' @examples\\n#\' \\\\dontrun{\\n#\' #######################################\\n#\' # Example 1: \'sso\' is a shinystan object\\n#\' #######################################\\n#\' \\n#\' # Just launch shinystan\\n#\' launch_shinystan(sso)\\n#\' \\n#\' # Launch shinystan and replace sso with an updated version of itself\\n#\' # if any changes are made to sso while using the app\\n#\' sso <- launch_shinystan(sso)\\n#\' \\n#\' # Launch shinystan but save any changes made to sso while running the app\\n#\' # in a new shinystan object sso2. sso will remained unchanged. \\n#\' sso2 <- launch_shinystan(sso) \\n#\' \\n#\' #######################################\\n#\' # Example 2: \'sf\' is a stanfit object\\n#\' #######################################\\n#\' \\n#\' # Just launch shinystan\\n#\' launch_shinystan(sf)\\n#\' \\n#\' # Launch shinystan and save the resulting shinystan object\\n#\' sf_sso <- launch_shinystan(sf)\\n#\' \\n#\' # Now sf_sso is a shinystan object and so Example 1 (above) applies when\\n#\' # using sf_sso. \\n#\' \\n#\' #######################################\\n#\' # Example 3: \'fit\' is an mcmc.list, array or list of matrices\\n#\' #######################################\\n#\'\\n#\' # First create shinystan object (see ?as.shinystan for full details)\\n#\' fit_sso <- as.shinystan(fit, model_name = \\"Example\\")\\n#\' \\n#\' # Now fit_sso is a shinystan object and so Example 1 (above) applies.\\n#\' }\\n#\'\\nlaunch_shinystan <- function(object, \\n                             rstudio = getOption(\\"shinystan.rstudio\\"), \\n                             ...) {\\n  if (is.shinystan(object)) {\\n    sso_check(object)\\n  } else if (is.stanreg(object) || is.stanfit(object)) {\\n    message(\\"\\\\nCreating shinystan object...\\")\\n    object <- as.shinystan(object)\\n  }\\n  if (!is.shinystan(object))\\n    stop(\\"\'object\' is not a valid input. See help(\'launch_shinystan\').\\")\\n  \\n  message(\\"\\\\nLaunching ShinyStan interface... \\",\\n          \\"for large models this  may take some time.\\")\\n  invisible(launch(object, rstudio, ...))\\n}\\n\\n\\n#\' ShinyStan demo\\n#\'\\n#\' @aliases eight_schools\\n#\' @export\\n#\' @inheritParams launch_shinystan\\n#\' @param demo_name The name of the demo. Currently \\\\code{\\"eight_schools\\"} is \\n#\'   the only option, but additional demos may be available in future releases.\\n#\'   \\\\describe{\\n#\'   \\\\item{\\\\code{eight_schools}}{Hierarchical meta-analysis model. See \\n#\'    \\\\emph{Meta Analysis} chapter of the Stan manual (chapter 11.2 in version\\n#\'    2.9), \\\\url{http://mc-stan.org/documentation/}.}\\n#\'   }\\n#\' @return An S4 shinystan object.\\n#\'   \\n#\' @template seealso-launch\\n#\' @template seealso-as.shinystan\\n#\' \\n#\' @examples\\n#\' \\\\dontrun{\\n#\' # launch demo but don\'t save a shinystan object\\n#\' launch_shinystan_demo() \\n#\' \\n#\' # launch demo and save the shinystan object for the demo \\n#\' sso_demo <- launch_shinystan_demo()\\n#\' }\\n#\'\\nlaunch_shinystan_demo <- function(demo_name = \\"eight_schools\\",\\n                                  rstudio = getOption(\\"shinystan.rstudio\\"),\\n                                  ...) {\\n  demo_name <- match.arg(demo_name)\\n  demo_object <- get(demo_name)\\n  invisible(launch(demo_object, rstudio = rstudio, ...))\\n}\\n\\n# Internal launch function \\n# @param sso shinystan object\\n# @param rstudio launch in rstudio viewer instead of web browser? \\n# @param ... passed to shiny::runApp\\nlaunch <- function(sso, rstudio = FALSE, ...) {\\n  launch.browser <- if (!rstudio) \\n    TRUE else getOption(\\"shiny.launch.browser\\", interactive())\\n  \\n  .sso_env$.SHINYSTAN_OBJECT <- sso  # see zzz.R for .sso_env\\n  on.exit(.sso_env$.SHINYSTAN_OBJECT <- NULL, add = TRUE)\\n  shiny::runApp(system.file(\\"ShinyStan\\", package = \\"shinystan\\"), \\n                launch.browser = launch.browser, ...)\\n}\\n" }\n'
line: b'{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "inst/ShinyStan/ui_files/model_code.R", "content": "sidebarLayout(\\n  sidebarPanel(\\n    width = 3,\\n    style = \\"height: 550px;\\",\\n    br(),\\n    h4(\\"Model Code\\"),\\n    helpText(\\n      style = \\"font-size: 12px;\\",\\n      p(\\n        \\"Model code will be displayed here each\\",\\n        \\"time you launch ShinyStan with this shinystan object.\\"\\n      )\\n    ),\\n    br(),\\n    actionButton(\\n      \\"save_user_model_code\\",\\n      label = \\"Save code\\",\\n      icon = icon(\\"save\\")\\n    ),\\n    div(style = \\"font-size: 11px;\\", textOutput(\\"user_code_saved\\")),\\n    conditionalPanel(\\n      condition = \\"input.save_user_model_code > 0\\",\\n      br(),\\n      save_and_close_reminder(\\"save_user_model_code_safe_quit\\")\\n    )\\n  ),\\n  mainPanel(\\n    width = 9,\\n    br(), br(),\\n    tags$textarea(\\n      id = \\"user_model_code\\",\\n      wrap = \\"off\\",\\n      cols = 80,\\n      rows = 20,\\n      .model_code\\n    )\\n  )\\n)\\n" }\n'
line: b'{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "inst/ShinyStan/server_utils.R", "content": "# function to suppress unnecessary warnings and messages generated by ggplot \\nsuppress_and_print <- function(x) {\\n  suppressMessages(suppressWarnings(print(x)))\\n}\\n" }\n'
line: b'{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "inst/ShinyStan/ui_files/help.R", "content": "div(\\n  class = \\"help-glossary-div\\",\\n  br(), br(),\\n  div(\\n    class = \\"help-glossary-nav-container\\",\\n    navlistPanel(\\n      well = TRUE,\\n      id = \\"help_navlist\\",\\n      \\"Topics\\",\\n      tabPanel(\\n        \\"Questions, bugs, and new features\\",\\n        div(\\n          class = \\"glossary-entry\\",\\n          h4(\\"Stan users group\\"),\\n          p(\\n            \\"To ask a question or suggest a new feature visit the\\",\\n            a(\\n              \\"Stan users message board.\\", \\n              href = \\"https://groups.google.com/forum/?fromgroups#!forum/stan-users\\"\\n            )\\n          ),\\n          br(),\\n          h4(\\"GitHub issue tracker\\"),\\n          p(\\n            \\"To report a bug  or suggest a new feature visit the\\",\\n            a(\\n              \\"GitHub issue tracker.\\", \\n              href = \\"https://github.com/stan-dev/shinystan/issues\\"\\n            )\\n          )\\n        )\\n      ),\\n      tabPanel(\\n        \\"Saving plots\\",\\n        div(\\n          class = \\"glossary-entry\\",\\n          h4(\\"Saving plots as ggplot2 objects\\"),\\n          p(\\n            \\"Clicking on a \'Save ggplot2 object\' button will be save an .RData\\n            file that you can load into your Global Environment using the\\",\\n            code(\\"load\\"),\\n            \\"function in R.\\n            You can then make changes to the plot using the functions in the\\n            ggplot2 package.\\"\\n          ),\\n          p(\\n            \\"Any plot that can be saved as a ggplot2 object can also be saved\\n            as a PDF.\\"\\n          )\\n      )), \\n      tabPanel(\\n        \\"Large models and launch speed\\",\\n        div(\\n          class = \\"glossary-entry\\",\\n          h4(\\"Launching ShinyStan faster\\"),\\n          p(\\n            \\"The\\", code(\\"drop_parameters\\"), \\"function in the\\", \\n            strong(\\"shinystan\\"), \\"R package will allow you to reduce the size\\", \\n            \\"of a shinystan object by removing parameters.\\", \\n            \\"See\\", code(\\"help(\'drop_parameters\', \'shinystan\')\\"), \\n            \\"for the documentation.\\"\\n          ),\\n          p(\\n            \\"Additionally, for large models, the\\", code(\\"launch_shinystan\\"),\\n            \\"function will launch the app faster when used with a\\",\\n            \\"shinystan object rather than a stanfit object\\",\\n            \\"(because no conversion is required).\\",\\n            \\"If ShinyStan takes a long time to launch for your\\",\\n            \\"model then it can help to first create a\\",\\n            \\"shinystan object using the\\", code(\\"as.shinystan\\"), \\"function.\\",\\n            \\"Alternatively, the first time you launch\\",\\n            \\"ShinyStan using a stanfit object, a shinystan\\",\\n            \\"object will be returned if you assign the value of\\",\\n            code(\\"launch_shinystan\\"),\\n            \\"to a name, e.g.\\"\\n          ),\\n          p(code(\\"sso <- launch_shinystan(stanfit)\\")),\\n          p(\\"rather than just\\"),\\n          p(code(\\"launch_shinystan(stanfit)\\")),\\n          p(\\n            \\"The next time you launch ShinyStan for the same\\",\\n            \\"model you can launch it using\\", code(\\"sso\\"), \\"rather than\\",\\n            code(\\"stanfit\\"), \\"and it should be quicker to launch.\\",\\n            \\"If it is still too slow then dropping some large parameters\\", \\n            \\"from the shinystan object is the best solution.\\"\\n          )\\n        )\\n      )\\n    )\\n  ),\\n  br(), br()\\n)\\n" }\n'
line: b'{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "inst/ShinyStan/ui_files/diagnostics_treedepth.R", "content": "# treedepth\\ndiv(\\n  class = \\"diagnostics-navlist-tabpanel\\",\\n  fluidRow(\\n    column(\\n      width = 7,\\n      help_dynamic,\\n      dygraphOutput_175px(\\"dynamic_trace_diagnostic_treedepth_out\\"),\\n      br(), br(),\\n      plotOutput(\\"treedepth_vs_lp_out\\", height = \\"150px\\")\\n    ),\\n    column(width = 5, plotOutput_400px(\\"treedepth_vs_accept_stat_out\\"))\\n  ),\\n  splitLayout(\\n    plotOutput(\\"treedepth_ndivergent_hist_out\\", height = \\"125px\\"),\\n    plotOutput(\\"treedepth_ndivergent0_hist_out\\", height = \\"125px\\"),\\n    plotOutput(\\"treedepth_ndivergent1_hist_out\\", height = \\"125px\\")\\n  ),\\n  br()\\n)\\n" }\n'
line: b'{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "man-roxygen/seealso-generate_quantity.R", "content": "#\' @seealso \\\\code{\\\\link{generate_quantity}} to add a new quantity to a shinystan\\n#\'   object.\\n" }\n'
line: b'{ "repo_name": "data-steve/data-steve.github.io", "ref": "refs/heads/master", "path": "_backlog/ggdumbbell2.R", "content": "pacman::p_load(dplyr, ggplot2, tidyr)\\nmkdat <- function(x,y,cat){\\n\\n}\\n\\ndat <-data.frame(\\"Jessica\\" = c(8.6, 9.3, 9.34, 9.1, 9),\\n                 \\"Francine\\" = c(9.4, 8.88, 9.2, 9.24, 9.34),\\n            \\"Events\\" = factor(1:5,labels=c(\\"sprint\\", \\"hurdles\\",\\"javelin\\", \\"discus\\", \\"hi-jump\\")))\\n\\nggdumbbell <- function(d\\n                       ,colors=c(\\"green4\\",\\"red3\\",\\"brown\\")\\n                       ,sz=17){\\n    nms <- names(d)\\n    names(d) <- c(\\"xvar\\",\\"yvar\\", \\"fvar\\")\\n    df <- d %>%\\n        dplyr::mutate(\\n               abs_diff = abs(xvar-yvar),\\n               pos_neg = sign(xvar-yvar),\\n               midpoint = abs_diff/2,\\n               clr = ifelse(pos_neg>0, colors[1], colors[2])\\n          ) %>%\\n      dplyr::rowwise() %>%\\n      dplyr::mutate(xmin = min(xvar,yvar),\\n             xmax = max(xvar,yvar)) %>% ungroup\\n\\n    df_tall <- df %>%\\n        tidyr::gather(keys, values, -c(fvar, xmin, xmax, midpoint, pos_neg, abs_diff, clr)) %>%\\n        mutate(keys=ifelse(keys==\\"xvar\\", nms[1], nms[2]))\\n\\n    l <-  as.character(df[[\\"fvar\\"]][as.integer(df[[\\"fvar\\"]])==max(as.integer(df[[\\"fvar\\"]]))])\\n    ll <- if(d[1,1]<d[1,2]) nms[1:2] else nms[2:1]\\nggplot(df_tall, aes(y=fvar, x=values) )+\\n  geom_segment(data=df, aes(x=xmin, xend=xmax, y=fvar, yend=fvar, alpha=abs_diff), color=df$clr, size=3) +\\n      geom_point(color = colors[3], size=5.5)  +\\n      geom_point(shape=21, color = colors[3], size=4.8, aes(fill=keys))  +\\n\\n      scale_fill_manual(values = c(\\"white\\", colors[3]), guide=FALSE) +\\n\\n      theme_bw()  +\\n  theme(text=element_text(size=sz),legend.position=\\"none\\", panel.grid = element_blank() )  +\\n  annotate(\\"text\\"\\n           , x = c(df[df$\\"fvar\\"==l,]$xmin,df[df$\\"fvar\\"==l,]$xmax)\\n           , y= c(df[df$\\"fvar\\"==l,]$fvar,df[df$\\"fvar\\"==l,]$fvar)\\n           , label=ll\\n           , color = colors[3], size=4.8 , vjust = 2.5) +\\n  labs(list(x=\\"Values\\",y=nms[3])) +\\n  annotate(\\"text\\", x=8.75, y=5, label=\\"Green/Red signifies\\\\nhigher/lower performance.\\\\n\\\\n Color intensity signifies\\\\nimporance of difference.\\", color=colors[3], size=7, vjust = .75)\\n}\\n\\nggdumbbell(dat) +\\n  # ggtitle(\\"Jessica\'s performance against Francine\\") +\\n  ggsave(\\"~/Documents/repos/data-steve.github.io/images/dumbbell.png\\")\\n" }\n'
line: b'{ "repo_name": "langcog/wordbank", "ref": "refs/heads/master", "path": "incoming_data/TEDS/convert_TEDS.R", "content": "# DEMOGRAPHIC INFORMATION\\n# Id_twin (unique twin identifier)\\n# Id_fam (family identifier, unique to each twin pair)\\n# twin   (twin birth order, 1=elder 2=younger)\\n# Random  (permits selecting one twin at random from each pair, to give independence of data, coded 0/1)\\n# Aethnic (ethnic category: 1=white, 0=other)\\n# Sex1 (gender of twin: 0=female, 1=male)\\n# Zygos (twin pair zygosity, 1=MZ 2=DZ)\\n# Amohqual (maternal educational qualifications, 8-point ordinal scale)\\n# Ases  (composite social class based on parental education and occupation, standardised with mean 0.0 and SD 1.0)\\n# \\n# AGE 2 MEASURES\\n# Brepage1  (age of twin on completion of parent-reported measures),\\n# converted to months and rounded down\\n# Bvc0011 to bvc1001  (100 item vocabulary checklist)\\n# Bvocab1 (vocabulary total on this scale)\\n# Bwu061  (does child combine words)\\n# Bs01s1 through bs12s1  (12 sentence complexity pairs)\\n# \\n# AGE 3 MEASURES\\n# Crepage1 (age of twin on completion of parent-reported measures),\\n# converted to months and rounded down\\n# Cvc0001 (not yet talking)\\n# Cvc0011 to cvc1001  (100 item vocabulary checklist)\\n# Cvocab1   (vocabulary total on this scale)\\n# Cs00s1 (does child combine words)\\n# Cs01s1 through cs12s1) (12 sentence complexity pairs)\\n# \\n# AGE 4 MEASURES\\n# Drepage1 (age of twin on completion of parent-reported measures),\\n# converted to months and rounded down\\n# Dvd011 through dvc481 (48 item vocabulary checklist)\\n# Dsay011  (overall evaluation of child\xe2\x80\x99s language/sentences, 6 point scale)\\n\\nlibrary(dplyr)\\nlibrary(haven)\\nlibrary(readr)\\nlibrary(foreign)\\nlibrary(tidyr)\\n\\nteds_raw <- read_spss(\\"TEDS dataset for WordBank 110516.sav\\")\\n\\nwrite_csv(teds_raw, \\"teds.csv\\")\\n\\n# make long form\\nteds_long <- teds_raw %>% \\n  gather(variable, value, btwoyear:dsay011) %>%\\n  separate(variable, into=c(\\"age\\", \\"variable\\"), sep = 1) %>%\\n  mutate(age = ifelse(age == \\"b\\", 2, ifelse(age == \\"c\\", 3, 4))) %>%\\n  filter(!(variable %in% c(\\"twoyear\\",\\"threeyr\\",\\"fouryr\\", \\"vocab1\\"))) %>%\\n  mutate(sex = ifelse(sex1 == 1, \\"male\\", \\"female\\"), \\n         ethnicity = ifelse(aethnic == 1, \\"white\\", \\"other\\"),\\n         zygosity = ifelse(zygos == 1, \\"MZ\\",\\"DZ\\"),\\n         ses = ases, \\n         mom_ed = amohqual) %>%\\n  select(-sex1, -aethnic, -zygos, -ases, -amohqual)\\n\\n# split out ages for later merging\\nages <- filter(teds_long, variable == \\"repagem1\\") %>%\\n  rename(age_months = value) %>%\\n  select(id_twin, age, age_months)\\n\\n# fours <- filter(d, variable != \\"repagem1\\", age == 4)\\n\\n# take only the two- and three-year-olds\\n# drop NAs for age, as this is missing data\\ntwos <- filter(teds_long, \\n               age == 2, \\n               variable != \\"repagem1\\") %>%\\n  spread(variable, value) %>%\\n  mutate(id_twin = factor(id_twin)) %>%\\n  left_join(ages %>% mutate(id_twin = factor(id_twin))) %>%\\n  filter(!is.na(age_months)) %>%\\n  select(-twin,-random, -age, -ses)\\n\\nthrees <- filter(teds_long, \\n               age == 3, \\n               variable != \\"repagem1\\") %>%\\n  spread(variable, value) %>%\\n  mutate(id_twin = factor(id_twin)) %>%\\n  left_join(ages %>% mutate(id_twin = factor(id_twin))) %>%\\n  filter(!is.na(age_months)) %>%\\n  select(-twin,-random, -age, -ses)\\n\\n# Either a single excel file (by convention called WugeseWS_Dax, but doesn\'t have to be), with sheets named data, fields, and values;\\n# Or three csv files, with the names foo_data, foo_fields, and foo_values (where foo is WugeseWS_Dax by convention, but doesn\'t have to be).\\n# For the data sheet/file:\\n#   The first row should be column labels (whatever they might be in this dataset).\\n# Each other row should be a single CDI administration.\\n\\nwrite_csv(twos, \\"../../raw_data/English_British_TEDS_2s/EnglishBritishTEDS2s_data.csv\\")\\nwrite_csv(threes, \\"../../raw_data/English_British_TEDS_3s/EnglishBritishTEDS3s_data.csv\\")\\n\\n# The fields sheet/file is a mapping from the dataset\'s column labels to Wordbank\'s fields, and should have the following columns:\\n#   column: column labels from the data sheet/file (modulo case sensitivity) that will be extracted\\n# field: what Wordbank field to map the column label to\\n# MUST include study_id and at least one of data_age and (date_of_birth and date_of_test)\\n# can also optionally have any of birth_order, ethnicity, mom_ed, sex\\n# the rest (everything in group=item) MUST be in this dataset\'s instrument definition file\'s itemID column\\n# this is how the dataset\'s fields get mapped \xe2\x80\x94 it\'s tricky and important to get right\\n# group: whether this field should be associated with the administration, the child, or the data table for the instrument\\n# one of admin, child, or item\\n# type: how to treat the value(s) of this field\\n# study_id, study_momed: value as is\\n# birth_order, data_age: value is made into an integer\\n# date_of_birth, date_of_test: value is made into date\\n# ethnicity, sex, mom_ed, any type in group=item: value is mapped using value mapping\\n\\nwrite_csv(data.frame(column = names(twos), \\n           field = names(twos), \\n           group = \\"item\\",\\n           type = \\"word\\"), \\n          \\"../../raw_data/English_British_TEDS_2s/EnglishBritishTEDS2s_fields.csv\\")\\n\\nwrite_csv(data.frame(column = names(threes), \\n                     field = names(threes), \\n                     group = \\"item\\",\\n                     type = \\"word\\"), \\n          \\"../../raw_data/English_British_TEDS_3s/EnglishBritishTEDS3s_fields.csv\\")\\n\\n# The values sheet/file is a mapping from the dataset\'s value to Wordbank\'s values, split by type, and should have the following columns:\\n#   type: one of the types in the field mapping sheet/file\\n# data_value: the value option in the dataset\\n# value: the short form (e.g. M) of the corresponding value option in Wordbank. The sets of value options in Wordbank are:\\n#   For ethnicity, defined in common/models.py\\n# ((\'A\', \'Asian\'), (\'B\', \'Black\'), (\'H\', \'Hispanic\'), (\'W\', \'White\'), (\'O\', \'Other/Mixed\'))\\n# For sex, defined in common/models.py\\n# ((\'M\', \'Male\'), (\'F\', \'Female\'), (\'O\', \'Other\'))\\n# For mom_ed, defined in common/management/commands/populate_momed.py\\n# {(1, \'None\'), (2, \'Primary\'), (3, \'Some Secondary\'), (4, \'Secondary\'), (5, \'Some College\'), (6, \'College\'), (7, \'Some Graduate\'), (8, \'Graduate\')}\\n# For all types in group=item, defined in e.g. instruments/schemas/Wugese_WS.py and equal to the choices for that type of item as given in the instrument definition file, e.g.\\n# [(u\'understands\', u\'understands\'), (u\'produces\', u\'produces\')]\\n\\n\\n\\n" }'
train bpe ...
regroup and select data for training bpe in /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/train.with_comments.tok.NoneGB ...
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.4.tok
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.2.tok
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.7.tok
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.6.tok
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.5.tok
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.3.tok
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.1.tok
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.0.tok
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.4.tok
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.2.tok
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.7.tok
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.6.tok
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.5.tok
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.3.tok
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.1.tok
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.0.tok
training bpe on /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/train.with_comments.tok.NoneGB...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.4.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.2.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.7.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.6.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.5.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.3.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.1.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.0.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.4.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.2.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.7.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.6.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.5.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.3.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.1.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.0.tok ...
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.4.tok ...\nRead 29539 words (1964 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.4.tok ...\nModified 29539 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.2.tok ...\nRead 39551 words (1900 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.2.tok ...\nModified 39551 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.7.tok ...\nRead 23937 words (1563 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.7.tok ...\nModified 23937 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.6.tok ...\nRead 23427 words (1854 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.6.tok ...\nModified 23427 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.5.tok ...\nRead 20556 words (1975 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.5.tok ...\nModified 20556 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.3.tok ...\nRead 53977 words (3261 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.3.tok ...\nModified 53977 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.1.tok ...\nRead 14118 words (1464 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.1.tok ...\nModified 14118 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.0.tok ...\nRead 18689 words (1735 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.0.tok ...\nModified 18689 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.4.tok ...\nRead 12475 words (1337 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.4.tok ...\nModified 12475 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.2.tok ...\nRead 8770 words (1123 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.2.tok ...\nModified 8770 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.7.tok ...\nRead 15082 words (1596 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.7.tok ...\nModified 15082 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.6.tok ...\nRead 11088 words (1168 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.6.tok ...\nModified 11088 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.5.tok ...\nRead 13921 words (1405 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.5.tok ...\nModified 13921 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.3.tok ...\nRead 11750 words (1127 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.3.tok ...\nModified 11750 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.1.tok ...\nRead 10408 words (1014 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.1.tok ...\nModified 10408 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.0.tok ...\nRead 15461 words (1484 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.0.tok ...\nModified 15461 words from text file.\n'
stdout: b''
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/test.with_comments.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/test.with_comments.tok ...
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/test.with_comments.tok ...\nRead 65232 words (3509 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/test.with_comments.tok ...\nModified 65232 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/test.with_comments.tok ...\nRead 4008 words (612 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/test.with_comments.tok ...\nModified 4008 words from text file.\n'
stdout: b''
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/valid.with_comments.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/valid.with_comments.tok ...
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/valid.with_comments.tok ...\nRead 22442 words (1621 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/valid.with_comments.tok ...\nModified 22442 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/valid.with_comments.tok ...\nRead 4717 words (710 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/valid.with_comments.tok ...\nModified 4717 words from text file.\n'
stdout: b''
get vocab ...
regroup and select data in /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/train.with_comments.bpe.NoneGB to get vocab ...
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.2.bpe
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.1.bpe
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.7.bpe
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.3.bpe
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.5.bpe
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.6.bpe
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.4.bpe
adding 9 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.0.bpe
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.2.bpe
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.4.bpe
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.7.bpe
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.6.bpe
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.0.bpe
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.1.bpe
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.3.bpe
adding 20 lines of /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.5.bpe
computing vocab on /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/train.with_comments.bpe.NoneGB...
binarize train.with_comments.[0123456789].bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.2.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.1.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.7.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.3.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.5.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.6.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.4.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.train.with_comments.0.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.2.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.4.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.7.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.6.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.0.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.1.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.3.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.train.with_comments.5.bpe ...
stderr: b'INFO - 04/17/21 17:25:57 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:25:57 - 0:00:00 - 40151 words (14447 unique) in 9 sentences.\n'
stderr: b'INFO - 04/17/21 17:25:59 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:25:59 - 0:00:00 - 14885 words (14447 unique) in 9 sentences.\n'
stderr: b'INFO - 04/17/21 17:25:59 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:25:59 - 0:00:00 - 24747 words (14447 unique) in 9 sentences.\n'
stderr: b'INFO - 04/17/21 17:26:00 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:26:00 - 0:00:00 - 55405 words (14447 unique) in 9 sentences.\n'
stderr: b'INFO - 04/17/21 17:26:01 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:26:01 - 0:00:00 - 21420 words (14447 unique) in 9 sentences.\n'
stderr: b'INFO - 04/17/21 17:26:01 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:26:02 - 0:00:00 - 23986 words (14447 unique) in 9 sentences.\n'
stderr: b'INFO - 04/17/21 17:26:02 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:26:02 - 0:00:00 - 30098 words (14447 unique) in 9 sentences.\n'
stderr: b'INFO - 04/17/21 17:26:03 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:26:03 - 0:00:00 - 19445 words (14447 unique) in 9 sentences.\n'
stderr: b'INFO - 04/17/21 17:26:03 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:26:03 - 0:00:00 - 9123 words (14447 unique) in 20 sentences.\n'
stderr: b'INFO - 04/17/21 17:26:04 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:26:04 - 0:00:00 - 12890 words (14447 unique) in 20 sentences.\n'
stderr: b'INFO - 04/17/21 17:26:05 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:26:05 - 0:00:00 - 15508 words (14447 unique) in 20 sentences.\n'
stderr: b'INFO - 04/17/21 17:26:05 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:26:05 - 0:00:00 - 11373 words (14447 unique) in 20 sentences.\n'
stderr: b'INFO - 04/17/21 17:26:06 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:26:06 - 0:00:00 - 15896 words (14447 unique) in 20 sentences.\n'
stderr: b'INFO - 04/17/21 17:26:07 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:26:07 - 0:00:00 - 10672 words (14447 unique) in 20 sentences.\n'
stderr: b'INFO - 04/17/21 17:26:07 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:26:08 - 0:00:00 - 12025 words (14447 unique) in 20 sentences.\n'
stderr: b'INFO - 04/17/21 17:26:08 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:26:08 - 0:00:00 - 14354 words (14447 unique) in 20 sentences.\n'
binarize test.with_comments.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.test.with_comments.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.test.with_comments.bpe ...
stderr: b'INFO - 04/17/21 17:26:09 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:26:09 - 0:00:00 - 77891 words (14447 unique) in 10 sentences.\nINFO - 04/17/21 17:26:09 - 0:00:00 - 5032 unknown words (478 unique), covering 6.46% of the data.\nINFO - 04/17/21 17:26:09 - 0:00:00 - _@@: 355\nINFO - 04/17/21 17:26:09 - 0:00:00 - g@@: 150\nINFO - 04/17/21 17:26:09 - 0:00:00 - k@@: 138\nINFO - 04/17/21 17:26:09 - 0:00:00 - s@@: 132\nINFO - 04/17/21 17:26:09 - 0:00:00 - w@@: 120\nINFO - 04/17/21 17:26:09 - 0:00:00 - "@@: 112\nINFO - 04/17/21 17:26:09 - 0:00:00 - Message: 81\nINFO - 04/17/21 17:26:09 - 0:00:00 - p@@: 78\nINFO - 04/17/21 17:26:09 - 0:00:00 - M@@: 77\nINFO - 04/17/21 17:26:09 - 0:00:00 - v@@: 76\nINFO - 04/17/21 17:26:09 - 0:00:00 - i@@: 76\nINFO - 04/17/21 17:26:09 - 0:00:00 - nt: 70\nINFO - 04/17/21 17:26:09 - 0:00:00 - ken@@: 69\nINFO - 04/17/21 17:26:09 - 0:00:00 - r@@: 69\nINFO - 04/17/21 17:26:09 - 0:00:00 - o@@: 67\nINFO - 04/17/21 17:26:09 - 0:00:00 - OS@@: 58\nINFO - 04/17/21 17:26:09 - 0:00:00 - E@@: 58\nINFO - 04/17/21 17:26:09 - 0:00:00 - metri@@: 56\nINFO - 04/17/21 17:26:09 - 0:00:00 - P@@: 55\nINFO - 04/17/21 17:26:09 - 0:00:00 - e@@: 53\nINFO - 04/17/21 17:26:09 - 0:00:00 - c@@: 49\nINFO - 04/17/21 17:26:09 - 0:00:00 - f@@: 48\nINFO - 04/17/21 17:26:09 - 0:00:00 - S@@: 47\nINFO - 04/17/21 17:26:09 - 0:00:00 - I@@: 46\nINFO - 04/17/21 17:26:09 - 0:00:00 - T@@: 45\nINFO - 04/17/21 17:26:09 - 0:00:00 - a@@: 43\nINFO - 04/17/21 17:26:09 - 0:00:00 - ASS: 42\nINFO - 04/17/21 17:26:09 - 0:00:00 - mt@@: 41\nINFO - 04/17/21 17:26:09 - 0:00:00 - 8@@: 40\nINFO - 04/17/21 17:26:09 - 0:00:00 - 3@@: 39\n'
stderr: b'INFO - 04/17/21 17:26:09 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:26:10 - 0:00:00 - 4437 words (14447 unique) in 10 sentences.\nINFO - 04/17/21 17:26:10 - 0:00:00 - 119 unknown words (59 unique), covering 2.68% of the data.\nINFO - 04/17/21 17:26:10 - 0:00:00 - l@@: 11\nINFO - 04/17/21 17:26:10 - 0:00:00 - k@@: 10\nINFO - 04/17/21 17:26:10 - 0:00:00 - m@@: 5\nINFO - 04/17/21 17:26:10 - 0:00:00 - .@@: 5\nINFO - 04/17/21 17:26:10 - 0:00:00 - r@@: 5\nINFO - 04/17/21 17:26:10 - 0:00:00 - /@@: 5\nINFO - 04/17/21 17:26:10 - 0:00:00 - i@@: 4\nINFO - 04/17/21 17:26:10 - 0:00:00 - 6@@: 3\nINFO - 04/17/21 17:26:10 - 0:00:00 - sert: 3\nINFO - 04/17/21 17:26:10 - 0:00:00 - S@@: 3\nINFO - 04/17/21 17:26:10 - 0:00:00 - P@@: 3\nINFO - 04/17/21 17:26:10 - 0:00:00 - sible@@: 3\nINFO - 04/17/21 17:26:10 - 0:00:00 - s@@: 3\nINFO - 04/17/21 17:26:10 - 0:00:00 - -@@: 2\nINFO - 04/17/21 17:26:10 - 0:00:00 - 1@@: 2\nINFO - 04/17/21 17:26:10 - 0:00:00 - 000: 2\nINFO - 04/17/21 17:26:10 - 0:00:00 - 0.2@@: 2\nINFO - 04/17/21 17:26:10 - 0:00:00 - c@@: 2\nINFO - 04/17/21 17:26:10 - 0:00:00 - ush: 2\nINFO - 04/17/21 17:26:10 - 0:00:00 - resul@@: 2\nINFO - 04/17/21 17:26:10 - 0:00:00 - top_@@: 2\nINFO - 04/17/21 17:26:10 - 0:00:00 - _@@: 2\nINFO - 04/17/21 17:26:10 - 0:00:00 - b@@: 2\nINFO - 04/17/21 17:26:10 - 0:00:00 - mod@@: 1\nINFO - 04/17/21 17:26:10 - 0:00:00 - ger@@: 1\nINFO - 04/17/21 17:26:10 - 0:00:00 - Inte@@: 1\nINFO - 04/17/21 17:26:10 - 0:00:00 - other@@: 1\nINFO - 04/17/21 17:26:10 - 0:00:00 - full_@@: 1\nINFO - 04/17/21 17:26:10 - 0:00:00 - ary/matri@@: 1\nINFO - 04/17/21 17:26:10 - 0:00:00 - Docum@@: 1\n'
binarize valid.with_comments.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/python.valid.with_comments.bpe ...
binarizing /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/r.valid.with_comments.bpe ...
stderr: b'INFO - 04/17/21 17:26:10 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:26:10 - 0:00:00 - 25744 words (14447 unique) in 10 sentences.\nINFO - 04/17/21 17:26:10 - 0:00:00 - 1284 unknown words (235 unique), covering 4.99% of the data.\nINFO - 04/17/21 17:26:10 - 0:00:00 - u@@: 68\nINFO - 04/17/21 17:26:10 - 0:00:00 - _@@: 51\nINFO - 04/17/21 17:26:10 - 0:00:00 - k@@: 46\nINFO - 04/17/21 17:26:10 - 0:00:00 - n@@: 44\nINFO - 04/17/21 17:26:10 - 0:00:00 - c@@: 41\nINFO - 04/17/21 17:26:10 - 0:00:00 - x@@: 39\nINFO - 04/17/21 17:26:10 - 0:00:00 - e@@: 36\nINFO - 04/17/21 17:26:10 - 0:00:00 - nap@@: 33\nINFO - 04/17/21 17:26:10 - 0:00:00 - l@@: 31\nINFO - 04/17/21 17:26:10 - 0:00:00 - S@@: 30\nINFO - 04/17/21 17:26:10 - 0:00:00 - tmp@@: 27\nINFO - 04/17/21 17:26:10 - 0:00:00 - module@@: 27\nINFO - 04/17/21 17:26:10 - 0:00:00 - W@@: 27\nINFO - 04/17/21 17:26:10 - 0:00:00 - g@@: 25\nINFO - 04/17/21 17:26:10 - 0:00:00 - d@@: 25\nINFO - 04/17/21 17:26:10 - 0:00:00 - G@@: 25\nINFO - 04/17/21 17:26:10 - 0:00:00 - D@@: 22\nINFO - 04/17/21 17:26:10 - 0:00:00 - h@@: 18\nINFO - 04/17/21 17:26:10 - 0:00:00 - script@@: 18\nINFO - 04/17/21 17:26:10 - 0:00:00 - a@@: 15\nINFO - 04/17/21 17:26:10 - 0:00:00 - r@@: 14\nINFO - 04/17/21 17:26:10 - 0:00:00 - nap: 13\nINFO - 04/17/21 17:26:10 - 0:00:00 - t@@: 13\nINFO - 04/17/21 17:26:10 - 0:00:00 - start_@@: 13\nINFO - 04/17/21 17:26:10 - 0:00:00 - purge_@@: 12\nINFO - 04/17/21 17:26:10 - 0:00:00 - X@@: 12\nINFO - 04/17/21 17:26:10 - 0:00:00 - L@@: 12\nINFO - 04/17/21 17:26:10 - 0:00:00 - ENT: 12\nINFO - 04/17/21 17:26:10 - 0:00:00 - T@@: 12\nINFO - 04/17/21 17:26:10 - 0:00:00 - GI@@: 11\n'
stderr: b'INFO - 04/17/21 17:26:11 - 0:00:00 - Read 14447 words from the vocabulary file.\n\nINFO - 04/17/21 17:26:11 - 0:00:00 - 5314 words (14447 unique) in 10 sentences.\nINFO - 04/17/21 17:26:11 - 0:00:00 - 240 unknown words (99 unique), covering 4.52% of the data.\nINFO - 04/17/21 17:26:11 - 0:00:00 - f@@: 14\nINFO - 04/17/21 17:26:11 - 0:00:00 - n@@: 14\nINFO - 04/17/21 17:26:11 - 0:00:00 - i@@: 10\nINFO - 04/17/21 17:26:11 - 0:00:00 - _@@: 9\nINFO - 04/17/21 17:26:11 - 0:00:00 - L@@: 7\nINFO - 04/17/21 17:26:11 - 0:00:00 - x@@: 7\nINFO - 04/17/21 17:26:11 - 0:00:00 - coe@@: 6\nINFO - 04/17/21 17:26:11 - 0:00:00 - y@@: 6\nINFO - 04/17/21 17:26:11 - 0:00:00 - c@@: 6\nINFO - 04/17/21 17:26:11 - 0:00:00 - r@@: 6\nINFO - 04/17/21 17:26:11 - 0:00:00 - G@@: 6\nINFO - 04/17/21 17:26:11 - 0:00:00 - O@@: 6\nINFO - 04/17/21 17:26:11 - 0:00:00 - s@@: 6\nINFO - 04/17/21 17:26:11 - 0:00:00 - vm: 4\nINFO - 04/17/21 17:26:11 - 0:00:00 - t@@: 4\nINFO - 04/17/21 17:26:11 - 0:00:00 - e@@: 4\nINFO - 04/17/21 17:26:11 - 0:00:00 - h@@: 4\nINFO - 04/17/21 17:26:11 - 0:00:00 - u@@: 4\nINFO - 04/17/21 17:26:11 - 0:00:00 - Y@@: 4\nINFO - 04/17/21 17:26:11 - 0:00:00 - Stu@@: 4\nINFO - 04/17/21 17:26:11 - 0:00:00 - R@@: 4\nINFO - 04/17/21 17:26:11 - 0:00:00 - ell: 3\nINFO - 04/17/21 17:26:11 - 0:00:00 - -@@: 3\nINFO - 04/17/21 17:26:11 - 0:00:00 - b@@: 3\nINFO - 04/17/21 17:26:11 - 0:00:00 - 7@@: 3\nINFO - 04/17/21 17:26:11 - 0:00:00 - .@@: 3\nINFO - 04/17/21 17:26:11 - 0:00:00 - SH@@: 3\nINFO - 04/17/21 17:26:11 - 0:00:00 - ption: 3\nINFO - 04/17/21 17:26:11 - 0:00:00 - k@@: 2\nINFO - 04/17/21 17:26:11 - 0:00:00 - anno@@: 2\n'r: split train, test and valid ... 
r: train for is 185 lines and 0.00042357295751571655 Go. 
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.4.tok
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.2.tok
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.7.tok
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.6.tok
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.5.tok
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.3.tok
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.1.tok
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.0.tok
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/test.with_comments.tok
extacting functions for /home/gcloud/TransCoder/data/test_dataset/r/valid.with_comments.tok

extract functions ... 
apply bpe on train ... 
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.1.functions_class.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.2.functions_class.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.7.functions_class.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.4.functions_class.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.0.functions_class.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.6.functions_class.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.6.functions_standalone.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.4.functions_standalone.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.5.functions_class.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.1.functions_standalone.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.0.functions_standalone.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.7.functions_standalone.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.3.functions_class.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.3.functions_standalone.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.5.functions_standalone.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.2.functions_standalone.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.1.functions_class.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.2.functions_class.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.7.functions_class.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.4.functions_class.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.0.functions_class.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.6.functions_class.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.6.functions_standalone.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.4.functions_standalone.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.5.functions_class.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.1.functions_standalone.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.0.functions_standalone.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.7.functions_standalone.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.3.functions_class.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.3.functions_standalone.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.5.functions_standalone.tok ...
apply bpe on /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.2.functions_standalone.tok ...
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.1.functions_class.tok ...\nRead 2168 words (358 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.1.functions_class.tok ...\nModified 2168 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.2.functions_class.tok ...\nRead 18694 words (816 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.2.functions_class.tok ...\nModified 18694 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.7.functions_class.tok ...\nRead 7964 words (746 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.7.functions_class.tok ...\nModified 7964 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.4.functions_class.tok ...\nRead 10428 words (838 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.4.functions_class.tok ...\nModified 10428 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.0.functions_class.tok ...\nRead 5300 words (566 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.0.functions_class.tok ...\nModified 5300 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.6.functions_class.tok ...\nRead 10365 words (843 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.6.functions_class.tok ...\nModified 10365 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.6.functions_standalone.tok ...\nRead 5660 words (677 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.6.functions_standalone.tok ...\nModified 5660 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.4.functions_standalone.tok ...\nRead 7130 words (889 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.4.functions_standalone.tok ...\nModified 7130 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.5.functions_class.tok ...\nRead 8664 words (814 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.5.functions_class.tok ...\nModified 8664 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.1.functions_standalone.tok ...\nRead 2368 words (329 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.1.functions_standalone.tok ...\nModified 2368 words from text file.\n'python: process ...
python: tokenizing 0 json files ...
python: train, test and valid for already exist. 
python: train for is 72 lines and 0.0006979554891586304 Go. 

stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.0.functions_standalone.tok ...\nRead 2410 words (373 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.0.functions_standalone.tok ...\nModified 2410 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.7.functions_standalone.tok ...\nRead 2691 words (269 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.7.functions_standalone.tok ...\nModified 2691 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.3.functions_class.tok ...\nRead 42246 words (2146 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.3.functions_class.tok ...\nModified 42246 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.3.functions_standalone.tok ...\nRead 2618 words (363 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.3.functions_standalone.tok ...\nModified 2618 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.5.functions_standalone.tok ...\nRead 6551 words (668 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.5.functions_standalone.tok ...\nModified 6551 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.2.functions_standalone.tok ...\nRead 8389 words (774 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/python/train.with_comments.2.functions_standalone.tok ...\nModified 8389 words from text file.\n'
stdout: b''
stderr: b'Loading codes from /home/gcloud/TransCoder/data/test_dataset/python-r-.with_comments/codes ...\nRead 20000 codes from the codes file.\nLoading vocabulary from /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.1.functions_class.tok ...\nRead 0 words (0 unique) from text file.\nApplying BPE to /home/gcloud/TransCoder/data/test_dataset/r/train.with_comments.1.functions_class.tok ...\nOutput memory map failed : 22.\n'
stdout: b''
