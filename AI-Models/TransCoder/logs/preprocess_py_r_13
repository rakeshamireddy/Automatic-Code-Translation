r: process ...
r: tokenizing 4 json files ...
input_path: /home/gcloud/TransCoder/data/test_dataset/r/r.004.json.gz
language: r
output_path: /home/gcloud/TransCoder/data/test_dataset/r/r.004.with_comments.tok
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Decimal-to-binary.R","content":"# Program to convert decimal number into binary number using recursive function\\nconvert_to_binary <- function(n) {\\n  if(n > 1) {\\n    convert_to_binary(as.integer(n/2))\\n  }\\n  cat(n %% 2)\\n}\\n\\nconvert_to_binary(52)"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Factorial.R","content":"# take input from the user\\nfactorial <- function(n) {\\nfactorial = 1\\n# check is the number is negative, positive or zero\\nif(num < 0) {\\n  print(\'Sorry factorial does not exist for negative numbers\')\\n} else if(num == 0) {\\n  print(\'The factorial of 0 is 1\')\\n} else {\\n  for(i in 1:num) {\\n    factorial = factorial * i\\n  }\\n  print(paste(\'The factorial of\', num ,\'is\',factorial))\\n}\\n}\\n\\nfactorial(4)"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/First-10-Fibonacci.R","content":"fibonacci <- function(n) {\\nFibonacci <- numeric(n)\\nFibonacci[1] <- Fibonacci[2] <- 1\\nfor (i in 3:10) Fibonacci[i] <- Fibonacci[i - 2] + Fibonacci[i - 1]\\nprint(\'First 10 Fibonacci numbers:\'\')\\nprint(Fibonacci)\\n}\\n\\nfibonacci(10)"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Max-Min-of-Vector.R","content":"max_min_vector <- function() {\\nnums = c(10, 20, 30, 40, 50, 60)\\nprint(\'Original vector:\')\\nprint(nums)   \\nprint(paste(\'Maximum value of the said vector:\',max(nums)))\\nprint(paste(\'Minimum value of the said vector:\',min(nums)))\\n}\\n\\nmax_min_vector()"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Odd-or-Even.R","content":"# Program to check if the input number is odd or even.\\n# A number is even if division by 2 give a remainder of 0.\\n# If remainder is 1, it is odd.\\nodd_or_even <- function(n){\\nnum = n\\nif((num %% 2) == 0) {\\n  print(paste(num,\'is Even\'))\\n} else {\\n  print(paste(num,\'is Odd\'))\\n}\\n}\\n\\nodd_or_even(4)"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Prime-Numbers.R","content":"prime_numbers <- function(n) {\\n  if (n >= 2) {\\n    x = seq(2, n)\\n    prime_nums = c()\\n    for (i in seq(2, n)) {\\n      if (any(x == i)) {\\n        prime_nums = c(prime_nums, i)\\n        x = c(x[(x %% i) != 0], i)\\n      }\\n    }\\n    return(prime_nums)\\n  }\\n  else \\n  {\\n    stop(\'Input number should be at least 2.\'\')\\n  }\\n} \\nprime_numbers(12)"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Read-csv-file.R","content":"read_csv_file <- function() {\\nmovie_data = read.csv(file=movies.csv, header=TRUE, sep=\',\')\\nprint(\'Content of the .csv file:\'\')\\nprint(movie_data)\\n}\\n\\nread_csv_file()"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/add1.r","content":"add1 <- function(n) {\\n   print( n+1)\\n}\\n\\nadd1(90)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/add2numbers.r","content":"add2nums <- function(num1, num2) {\\n   print( num1+num2)\\n}\\n\\nadd2nums(34, 6)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/areaofsquare.r","content":"areaSquare <- function(num) {\\n   msg1 <- \'Invalid measurement\'\\n   msg2 <- \'Area of the Square is: \'\\n   if  (num <= 0){\\n        print(msg1)\\n   }else {\\n       print (msg2)\\n       print(num*num)\\n   }\\n  \\n}\\n\\nareaSquare(6)"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/divisibleby10.r","content":"divisibleby10 <- function(num) {\\nif(num %% 10 == 0){\\n    print(\'True\')\\n}else{\\n    print(\'False\')\\n}\\n\\n}\\n\\ndivisibleby10(60)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/integertype.r","content":"integerType <- function(num){\\nif(num > 0) {\\nprint(\'Positive number\')\\n} else {\\nif(num == 0) {\\nprint(\'Zero\')\\n} else {\\nprint(\'Negative number\')\\n}\\n}\\n}\\n\\nintegerType(-90)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/lengthoflist.r","content":"\\nlengthofVector <- function(vector){\\ncount <- 0\\nfor (i in vector){\\n    count <- count + 1\\n}\\nprint(count)\\n}\\n\\narray <- c(3,4,5,1,6)\\nlengthofVector(array)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/palindrome.r","content":"\\npalindrome <- function(n){\\nrev = 0\\n    num = n\\n\\n    while (n > 0) {\\n      r = n %% 10\\n      rev = rev * 10 + r\\n      n = n %/% 10\\n    }\\n\\n    if (rev == num)\\n    {\\n      print(paste(\'Number is palindrome :\', rev))\\n    }\\n    else{\\n      print(paste(\'Number is not palindrome :\', rev))\\n    }\\n}\\npalindrome(121)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/sort.r","content":"integerType <- function(num){\\nif(num > 0) {\\nsortvector <- function(vector){\\nprint(sort(vector))\\n}\\n\\narray <- c(23,12,11,34,21)\\nsortvector(array)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/spmv.r","content":"\\nsum <- function(vector){\\nresult <- 0\\nfor(i in vector){\\n    result = result + i\\n}\\nprint(result)\\n}\\n\\nproduct <- function(vector){\\n    result <- 1\\n    for(i in vector){\\n        result = result*i\\n    }\\n    print(result)\\n}\\n\\narray <- c(1,2,3,4)\\nsum(array)\\nproduct(array)\\nmean(array, na.rm=TRUE)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/upperlower.r","content":"\\nupperCase <- function(word){\\nresult <- toupper(word)\\nprint(result)\\n}\\n\\nlowerCase <- function(word){\\nresult <- tolower(word)\\nprint(result)\\n}\\n\\nupperCase(\'Function\')\\nlowerCase(\'FUNCTION\')\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/armstrong.r","content":"\\n# take input from the user\\narmstrong <- function(){\\nnum = as.integer(readline(prompt=\'Enter a number: \'))\\n# initialize sum\\nsum = 0\\n# find the sum of the cube of each digit\\ntemp = num\\nwhile(temp > 0) {\\ndigit = temp %% 10\\nsum = sum + (digit ^ 3)\\ntemp = floor(temp / 10)\\n}\\n# display the result\\nif(num == sum) {\\nprint(paste(num, \'is an Armstrong number\'))\\n} else {\\nprint(paste(num, \'is not an Armstrong number\'))\\n}\\n}\\n" }\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/divisibleby10.r","content":"\\ndivisibleby10 <- function(num) {\\nif(num %% 10 == 0){\\n    print(\'True\')\\n}else{\\n    print(\'False\')\\n}\\n\\n}\\n\\ndivisibleby10(60)\\n" }\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/factors.r","content":"\\n#R program to print factors\\nfactors <- function()\\nn=14\\nfor(i in 1:n)\\n{\\nif((n%%i)==0)\\n{\\n  print(i)\\n}\\n}\\n}\\n" }\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/greatest3.r","content":"\\ngreatest3 <- function(){\\n    x <- 71\\n    y <- 87\\n    z <- 24\\n\\n    if (x > y && x > z) {\\n      print(paste(\'Greatest is :\', x))\\n    } else if (y > z) {\\n      print(paste(\'Greatest is :\', y))\\n    } else{\\n      print(paste(\'Greatest is :\', z))\\n    }\\n\\n}\\n"        }\n'python: process ...
python: tokenizing 4 json files ...
input_path: /home/gcloud/TransCoder/data/test_dataset/python/python.001.json.gz
language: python
output_path: /home/gcloud/TransCoder/data/test_dataset/python/python.001.with_comments.tok
line: b'{"repo_name":"anas-taji/knowledge","ref":"refs/heads/8.0","path":"attachment_edit/models/ir_attachment.py","content":"# -*- coding: utf-8 -*-\\n# \xc2\xa9 2015 Therp BV \\u003chttp://therp.nl\\u003e\\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl.html).\\n# -*- coding: utf-8 -*-\\nfrom openerp import models, fields, api\\n\\n\\nclass IrAttachment(models.Model):\\n    _inherit = \'ir.attachment\'\\n\\n    res_reference = fields.Reference(\\n        selection=\'_selection_res_reference\',\\n        string=\'Resource reference\', compute=\'_compute_res_reference\',\\n        inverse=\'_inverse_res_reference\')\\n\\n    @api.one\\n    @api.depends(\'res_id\', \'res_model\')\\n    def _compute_res_reference(self):\\n        if self.res_model and self.res_id:\\n            self.res_reference = \'%s,%s\' % (self.res_model, self.res_id)\\n\\n    @api.one\\n    def _inverse_res_reference(self):\\n        if self.res_reference:\\n            self.write({\\n                \'res_model\': self.res_reference._model._model,\\n                \'res_id\': self.res_reference.id,\\n            })\\n        else:\\n            self.write({\'res_model\': False, \'res_id\': False})\\n\\n    @api.model\\n    def _selection_res_reference(self):\\n        return self.env[\'ir.model\'].search([\\n            (\'osv_memory\', \'=\', False),\\n            (\'access_ids.group_id.users\', \'=\', self.env.uid)\\n        ]).mapped(lambda rec: (rec.model, rec.name))\\n"}\n'
line: b'{"repo_name":"apocquet/django","ref":"refs/heads/master","path":"django/contrib/gis/db/models/sql/conversion.py","content":"\\"\\"\\"\\nThis module holds simple classes to convert geospatial values from the\\ndatabase.\\n\\"\\"\\"\\n\\nfrom django.contrib.gis.db.models.fields import GeoSelectFormatMixin\\nfrom django.contrib.gis.geometry.backend import Geometry\\nfrom django.contrib.gis.measure import Area, Distance\\n\\n\\nclass BaseField(object):\\n    empty_strings_allowed = True\\n\\n    def get_db_converters(self, connection):\\n        return [self.from_db_value]\\n\\n    def select_format(self, compiler, sql, params):\\n        return sql, params\\n\\n\\nclass AreaField(BaseField):\\n    \\"Wrapper for Area values.\\"\\n    def __init__(self, area_att):\\n        self.area_att = area_att\\n\\n    def from_db_value(self, value, expression, connection, context):\\n        if value is not None:\\n            value = Area(**{self.area_att: value})\\n        return value\\n\\n    def get_internal_type(self):\\n        return \'AreaField\'\\n\\n\\nclass DistanceField(BaseField):\\n    \\"Wrapper for Distance values.\\"\\n    def __init__(self, distance_att):\\n        self.distance_att = distance_att\\n\\n    def from_db_value(self, value, expression, connection, context):\\n        if value is not None:\\n            value = Distance(**{self.distance_att: value})\\n        return value\\n\\n    def get_internal_type(self):\\n        return \'DistanceField\'\\n\\n\\nclass GeomField(GeoSelectFormatMixin, BaseField):\\n    \\"\\"\\"\\n    Wrapper for Geometry values.  It is a lightweight alternative to\\n    using GeometryField (which requires an SQL query upon instantiation).\\n    \\"\\"\\"\\n    # Hacky marker for get_db_converters()\\n    geom_type = None\\n\\n    def from_db_value(self, value, expression, connection, context):\\n        if value is not None:\\n            value = Geometry(value)\\n        return value\\n\\n    def get_internal_type(self):\\n        return \'GeometryField\'\\n\\n\\nclass GMLField(BaseField):\\n    \\"\\"\\"\\n    Wrapper for GML to be used by Oracle to ensure Database.LOB conversion.\\n    \\"\\"\\"\\n\\n    def get_internal_type(self):\\n        return \'GMLField\'\\n\\n    def from_db_value(self, value, expression, connection, context):\\n        return value\\n"}\n'
line: b'{"repo_name":"chenlian2015/skia_from_google","ref":"refs/heads/master","path":"tools/skp/page_sets/skia_youtube_desktop.py","content":"# Copyright 2014 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n# pylint: disable=W0401,W0614\\n\\n\\nfrom telemetry.page import page as page_module\\nfrom telemetry.page import page_set as page_set_module\\n\\n\\nclass SkiaBuildbotDesktopPage(page_module.Page):\\n\\n  def __init__(self, url, page_set):\\n    super(SkiaBuildbotDesktopPage, self).__init__(\\n        url=url,\\n        page_set=page_set,\\n        credentials_path=\'data/credentials.json\')\\n    self.user_agent_type = \'desktop\'\\n    self.archive_data_file = \'data/skia_youtube_desktop.json\'\\n\\n  def RunNavigateSteps(self, action_runner):\\n    action_runner.NavigateToPage(self)\\n    action_runner.Wait(25)\\n\\n\\nclass SkiaYoutubeDesktopPageSet(page_set_module.PageSet):\\n\\n  \\"\\"\\" Pages designed to represent the median, not highly optimized web \\"\\"\\"\\n\\n  def __init__(self):\\n    super(SkiaYoutubeDesktopPageSet, self).__init__(\\n      user_agent_type=\'desktop\',\\n      archive_data_file=\'data/skia_youtube_desktop.json\')\\n\\n    urls_list = [\\n      # Why: #3 (Alexa global)\\n      \'http://www.youtube.com/watch?v=PC57z-oDPLs\',\\n    ]\\n\\n    for url in urls_list:\\n      self.AddPage(SkiaBuildbotDesktopPage(url, self))\\n"}\n'
line: b'{"repo_name":"codeboy/projectile","ref":"refs/heads/master","path":"lib/treebeard/tests.py","content":"\\"Unit/Functional tests\\"\\n\\nimport functools\\nimport os\\nfrom django.contrib.admin.options import ModelAdmin\\nfrom django.contrib.admin.sites import AdminSite\\nfrom django.test import TestCase\\nfrom django.db import models, transaction\\nfrom django.contrib.auth.models import User\\nfrom django.db.models import Q\\nfrom django.conf import settings\\nfrom django import VERSION as DJANGO_VERSION\\n\\nfrom treebeard import numconv\\nfrom treebeard.exceptions import InvalidPosition, InvalidMoveToDescendant, \\\\\\n    PathOverflow, MissingNodeOrderBy\\nfrom treebeard.mp_tree import MP_Node\\nfrom treebeard.al_tree import AL_Node\\nfrom treebeard.ns_tree import NS_Node\\nfrom treebeard.forms import MoveNodeForm\\n\\n# ghetto app detection, there is probably some introspection method,\\n# but meh, this works\\nHAS_DJANGO_AUTH = \'django.contrib.auth\' in settings.INSTALLED_APPS\\n\\nBASE_DATA = [\\n  {\'data\':{\'desc\':\'1\'}},\\n  {\'data\':{\'desc\':\'2\'}, \'children\':[\\n    {\'data\':{\'desc\':\'21\'}},\\n    {\'data\':{\'desc\':\'22\'}},\\n    {\'data\':{\'desc\':\'23\'}, \'children\':[\\n      {\'data\':{\'desc\':\'231\'}},\\n    ]},\\n    {\'data\':{\'desc\':\'24\'}},\\n  ]},\\n  {\'data\':{\'desc\':\'3\'}},\\n  {\'data\':{\'desc\':\'4\'}, \'children\':[\\n    {\'data\':{\'desc\':\'41\'}},\\n  ]},\\n]\\n\\n\\nclass MP_TestNode(MP_Node):\\n    steplen = 3\\n\\n    desc = models.CharField(max_length=255)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass MP_TestNodeSomeDep(models.Model):\\n    node = models.ForeignKey(MP_TestNode)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass NS_TestNode(NS_Node):\\n    desc = models.CharField(max_length=255)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass NS_TestNodeSomeDep(models.Model):\\n    node = models.ForeignKey(NS_TestNode)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass AL_TestNode(AL_Node):\\n    parent = models.ForeignKey(\'self\',\\n                               related_name=\'children_set\',\\n                               null=True,\\n                               db_index=True)\\n    sib_order = models.PositiveIntegerField()\\n    desc = models.CharField(max_length=255)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass AL_TestNodeSomeDep(models.Model):\\n    node = models.ForeignKey(AL_TestNode)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass MP_TestNodeSorted(MP_Node):\\n    steplen = 1\\n    node_order_by = [\'val1\', \'val2\', \'desc\']\\n    val1 = models.IntegerField()\\n    val2 = models.IntegerField()\\n    desc = models.CharField(max_length=255)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass NS_TestNodeSorted(NS_Node):\\n    node_order_by = [\'val1\', \'val2\', \'desc\']\\n    val1 = models.IntegerField()\\n    val2 = models.IntegerField()\\n    desc = models.CharField(max_length=255)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass AL_TestNodeSorted(AL_Node):\\n    parent = models.ForeignKey(\'self\',\\n                               related_name=\'children_set\',\\n                               null=True,\\n                               db_index=True)\\n    node_order_by = [\'val1\', \'val2\', \'desc\']\\n    val1 = models.IntegerField()\\n    val2 = models.IntegerField()\\n    desc = models.CharField(max_length=255)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass MP_TestNodeAlphabet(MP_Node):\\n    steplen = 2\\n\\n    numval = models.IntegerField()\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass MP_TestNodeSmallStep(MP_Node):\\n    steplen = 1\\n    alphabet = \'0123456789\'\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass MP_TestNodeSortedAutoNow(MP_Node):\\n    desc = models.CharField(max_length=255)\\n    created = models.DateTimeField(auto_now_add=True)\\n\\n    node_order_by = [\'created\']\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n\\nclass MP_TestNodeShortPath(MP_Node):\\n    steplen = 1\\n    alphabet = \'01234\'\\n    desc = models.CharField(max_length=255)\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\n# This is how you change the default fields defined in a Django abstract class\\n# (in this case, MP_Node), since Django doesn\'t allow overriding fields, only\\n# mehods and attributes\\nMP_TestNodeShortPath._meta.get_field(\'path\').max_length = 4\\n\\n\\nif DJANGO_VERSION \\u003e= (1, 1):  # pragma: no cover\\n\\n    class MP_TestNode_Proxy(MP_TestNode):\\n        class Meta:\\n            proxy = True\\n\\n\\n    class NS_TestNode_Proxy(NS_TestNode):\\n        class Meta:\\n            proxy = True\\n\\n\\n    class AL_TestNode_Proxy(AL_TestNode):\\n        class Meta:\\n            proxy = True\\n\\n\\nclass MP_TestSortedNodeShortPath(MP_Node):\\n    steplen = 1\\n    alphabet = \'01234\'\\n    desc = models.CharField(max_length=255)\\n\\n    node_order_by = [\'desc\']\\n\\n    def __unicode__(self):  # pragma: no cover\\n        return \'Node %d\' % self.id\\n\\nMP_TestSortedNodeShortPath._meta.get_field(\'path\').max_length = 4\\n\\n\\nif HAS_DJANGO_AUTH:\\n\\n    class MP_TestIssue14(MP_Node):\\n        name = models.CharField(max_length=255)\\n        users = models.ManyToManyField(User)\\n\\n\\ndef testtype(treetype, proxy):\\n\\n    def decorator(f):\\n\\n        @functools.wraps(f)\\n        def _testtype(self):\\n            {\'MP\': self.set_MP,\\n             \'AL\': self.set_AL,\\n             \'NS\': self.set_NS}[treetype](proxy)\\n            try:\\n                f(self)\\n            finally:\\n                transaction.rollback()\\n                self.model = None\\n                self.sorted_model = None\\n                self.dep_model = None\\n        return _testtype\\n    return decorator\\n\\n\\ndef _load_test_methods(cls, proxy=True):\\n    if proxy and DJANGO_VERSION \\u003e= (1, 1):\\n        proxyopts = (False, True)\\n    else:\\n        proxyopts = (False,)\\n    for m in dir(cls):\\n        if not m.startswith(\'_multi_\'):\\n            continue\\n        for t in (\'MP\', \'AL\', \'NS\'):\\n            for p in proxyopts:\\n                deco = testtype(t, p)\\n                name = \'test_%s%s_%s\' % (t.lower(),\\n                                          \'_proxy\' if p else \'\',\\n                                          m.split(\'_\', 2)[2])\\n                setattr(cls, name, deco(getattr(cls, m)))\\n\\n\\nclass TestTreeBase(TestCase):\\n\\n    def setUp(self):\\n        self.set_MP()\\n        self.unchanged = [(u\'1\', 1, 0),\\n                          (u\'2\', 1, 4),\\n                          (u\'21\', 2, 0),\\n                          (u\'22\', 2, 0),\\n                          (u\'23\', 2, 1),\\n                          (u\'231\', 3, 0),\\n                          (u\'24\', 2, 0),\\n                          (u\'3\', 1, 0),\\n                          (u\'4\', 1, 1),\\n                          (u\'41\', 2, 0)]\\n\\n    def set_MP(self, proxy=False):\\n        if proxy and DJANGO_VERSION \\u003e= (1, 1):\\n            self.model = MP_TestNode_Proxy\\n        else:\\n            self.model = MP_TestNode\\n        self.sorted_model = MP_TestNodeSorted\\n        self.dep_model = MP_TestNodeSomeDep\\n\\n    def set_NS(self, proxy=False):\\n        if proxy and DJANGO_VERSION \\u003e= (1, 1):\\n            self.model = NS_TestNode_Proxy\\n        else:\\n            self.model = NS_TestNode\\n        self.sorted_model = NS_TestNodeSorted\\n        self.dep_model = NS_TestNodeSomeDep\\n\\n    def set_AL(self, proxy=False):\\n        if proxy and DJANGO_VERSION \\u003e= (1, 1):\\n            self.model = AL_TestNode_Proxy\\n        else:\\n            self.model = AL_TestNode\\n        self.sorted_model = AL_TestNodeSorted\\n        self.dep_model = AL_TestNodeSomeDep\\n\\n    def got(self):\\n        nsmodels = [NS_TestNode]\\n        if DJANGO_VERSION \\u003e= (1, 1):\\n            nsmodels.append(NS_TestNode_Proxy)\\n        if self.model in nsmodels:\\n            # this slows down nested sets tests quite a bit, but it has the\\n            # advantage that we\'ll check the node edges are correct\\n            d = {}\\n            for tree_id, lft, rgt in self.model.objects.values_list(\'tree_id\',\\n                                                                    \'lft\',\\n                                                                    \'rgt\'):\\n                d.setdefault(tree_id, []).extend([lft, rgt])\\n            for tree_id, got_edges in d.items():\\n                self.assertEqual(len(got_edges), max(got_edges))\\n                good_edges = range(1, len(got_edges) + 1)\\n                self.assertEqual(sorted(got_edges), good_edges)\\n\\n        return [(o.desc, o.get_depth(), o.get_children_count())\\n                for o in self.model.get_tree()]\\n\\n    def _assert_get_annotated_list(self, expected, parent=None):\\n        got = [\\n            (obj[0].desc, obj[1][\'open\'], obj[1][\'close\'], obj[1][\'level\'])\\n            for obj in self.model.get_annotated_list(parent)]\\n        self.assertEqual(expected, got)\\n\\n\\nclass TestEmptyTree(TestTreeBase):\\n\\n    def _multi_load_bulk_empty(self):\\n        ids = self.model.load_bulk(BASE_DATA)\\n        got_descs = [obj.desc\\n                     for obj in self.model.objects.filter(id__in=ids)]\\n        expected_descs = [x[0] for x in self.unchanged]\\n        self.assertEqual(sorted(got_descs), sorted(expected_descs))\\n        self.assertEqual(self.got(), self.unchanged)\\n\\n    def _multi_dump_bulk_empty(self):\\n        self.assertEqual(self.model.dump_bulk(), [])\\n\\n    def _multi_add_root_empty(self):\\n        self.model.add_root(desc=\'1\')\\n        expected = [(u\'1\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_get_root_nodes_empty(self):\\n        got = self.model.get_root_nodes()\\n        expected = []\\n        self.assertEqual([node.desc for node in got], expected)\\n\\n    def _multi_get_first_root_node_empty(self):\\n        got = self.model.get_first_root_node()\\n        self.assertEqual(got, None)\\n\\n    def _multi_get_last_root_node_empty(self):\\n        got = self.model.get_last_root_node()\\n        self.assertEqual(got, None)\\n\\n    def _multi_get_tree(self):\\n        got = list(self.model.get_tree())\\n        self.assertEqual(got, [])\\n\\n    def _multi_get_annotated_list(self):\\n        expected = []\\n        self._assert_get_annotated_list(expected)\\n\\n\\nclass TestNonEmptyTree(TestTreeBase):\\n\\n    def setUp(self):\\n        super(TestNonEmptyTree, self).setUp()\\n        MP_TestNode.load_bulk(BASE_DATA)\\n        AL_TestNode.load_bulk(BASE_DATA)\\n        NS_TestNode.load_bulk(BASE_DATA)\\n\\n\\nclass TestClassMethods(TestNonEmptyTree):\\n\\n    def setUp(self):\\n        super(TestClassMethods, self).setUp()\\n\\n    def _multi_load_bulk_existing(self):\\n\\n        # inserting on an existing node\\n\\n        node = self.model.objects.get(desc=u\'231\')\\n        ids = self.model.load_bulk(BASE_DATA, node)\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 4),\\n                    (u\'1\', 4, 0),\\n                    (u\'2\', 4, 4),\\n                    (u\'21\', 5, 0),\\n                    (u\'22\', 5, 0),\\n                    (u\'23\', 5, 1),\\n                    (u\'231\', 6, 0),\\n                    (u\'24\', 5, 0),\\n                    (u\'3\', 4, 0),\\n                    (u\'4\', 4, 1),\\n                    (u\'41\', 5, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        expected_descs = [u\'1\', u\'2\', u\'21\', u\'22\', u\'23\', u\'231\', u\'24\',\\n                          u\'3\', u\'4\', u\'41\']\\n        got_descs = [obj.desc\\n                     for obj in self.model.objects.filter(id__in=ids)]\\n        self.assertEqual(sorted(got_descs), sorted(expected_descs))\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_get_tree_all(self):\\n        got = [(o.desc, o.get_depth(), o.get_children_count())\\n                for o in self.model.get_tree()]\\n        self.assertEqual(got, self.unchanged)\\n\\n    def _multi_dump_bulk_all(self):\\n        self.assertEqual(self.model.dump_bulk(keep_ids=False), BASE_DATA)\\n\\n    def _multi_get_tree_node(self):\\n        node = self.model.objects.get(desc=u\'231\')\\n        self.model.load_bulk(BASE_DATA, node)\\n\\n        # the tree was modified by load_bulk, so we reload our node object\\n        node = self.model.objects.get(pk=node.id)\\n\\n        got = [(o.desc, o.get_depth(), o.get_children_count())\\n                for o in self.model.get_tree(node)]\\n        expected = [(u\'231\', 3, 4),\\n                    (u\'1\', 4, 0),\\n                    (u\'2\', 4, 4),\\n                    (u\'21\', 5, 0),\\n                    (u\'22\', 5, 0),\\n                    (u\'23\', 5, 1),\\n                    (u\'231\', 6, 0),\\n                    (u\'24\', 5, 0),\\n                    (u\'3\', 4, 0),\\n                    (u\'4\', 4, 1),\\n                    (u\'41\', 5, 0)]\\n        self.assertEqual(got, expected)\\n\\n    def _multi_get_tree_leaf(self):\\n        node = self.model.objects.get(desc=u\'1\')\\n\\n        self.assertEqual(0, node.get_children_count())\\n        got = [(o.desc, o.get_depth(), o.get_children_count())\\n                for o in self.model.get_tree(node)]\\n        expected = [(u\'1\', 1, 0)]\\n        self.assertEqual(got, expected)\\n\\n    def _multi_get_annotated_list_all(self):\\n        expected = [(u\'1\', True, [], 0), (u\'2\', False, [], 0),\\n                    (u\'21\', True, [], 1), (u\'22\', False, [], 1),\\n                    (u\'23\', False, [], 1), (u\'231\', True, [0], 2),\\n                    (u\'24\', False, [0], 1), (u\'3\', False, [], 0),\\n                    (u\'4\', False, [], 0), (u\'41\', True, [0, 1], 1)]\\n        self._assert_get_annotated_list(expected)\\n\\n    def _multi_get_annotated_list_node(self):\\n        node = self.model.objects.get(desc=u\'2\')\\n        expected = [(u\'2\', True, [], 0), (u\'21\', True, [], 1),\\n                    (u\'22\', False, [], 1), (u\'23\', False, [], 1),\\n                    (u\'231\', True, [0], 2), (u\'24\', False, [0, 1], 1)]\\n        self._assert_get_annotated_list(expected, node)\\n\\n    def _multi_get_annotated_list_leaf(self):\\n        node = self.model.objects.get(desc=u\'1\')\\n        expected = [(u\'1\', True, [0], 0)]\\n        self._assert_get_annotated_list(expected, node)\\n\\n    def _multi_dump_bulk_node(self):\\n        node = self.model.objects.get(desc=u\'231\')\\n        self.model.load_bulk(BASE_DATA, node)\\n\\n        # the tree was modified by load_bulk, so we reload our node object\\n        node = self.model.objects.get(pk=node.id)\\n\\n        got = self.model.dump_bulk(node, False)\\n        expected = [{\'data\':{\'desc\':u\'231\'}, \'children\':BASE_DATA}]\\n        self.assertEqual(got, expected)\\n\\n    def _multi_load_and_dump_bulk_keeping_ids(self):\\n        exp = self.model.dump_bulk(keep_ids=True)\\n        self.model.objects.all().delete()\\n        self.model.load_bulk(exp, None, True)\\n        got = self.model.dump_bulk(keep_ids=True)\\n        self.assertEqual(got, exp)\\n        # do we really have an unchaged tree after the dump/delete/load?\\n        got = [(o.desc, o.get_depth(), o.get_children_count())\\n                for o in self.model.get_tree()]\\n        self.assertEqual(got, self.unchanged)\\n\\n    def _multi_get_root_nodes(self):\\n        got = self.model.get_root_nodes()\\n        expected = [\'1\', \'2\', \'3\', \'4\']\\n        self.assertEqual([node.desc for node in got], expected)\\n\\n    def _multi_get_first_root_node(self):\\n        got = self.model.get_first_root_node()\\n        self.assertEqual(got.desc, \'1\')\\n\\n    def _multi_get_last_root_node(self):\\n        got = self.model.get_last_root_node()\\n        self.assertEqual(got.desc, \'4\')\\n\\n    def _multi_add_root(self):\\n        obj = self.model.add_root(desc=\'5\')\\n        self.assertEqual(obj.get_depth(), 1)\\n        self.assertEqual(self.model.get_last_root_node().desc, \'5\')\\n\\n\\nclass TestSimpleNodeMethods(TestNonEmptyTree):\\n\\n    def _multi_is_root(self):\\n        data = [\\n            (\'2\', True),\\n            (\'1\', True),\\n            (\'4\', True),\\n            (\'21\', False),\\n            (\'24\', False),\\n            (\'22\', False),\\n            (\'231\', False),\\n        ]\\n        for desc, expected in data:\\n            got = self.model.objects.get(desc=desc).is_root()\\n            self.assertEqual(got, expected)\\n\\n    def _multi_is_leaf(self):\\n        data = [\\n            (\'2\', False),\\n            (\'23\', False),\\n            (\'231\', True),\\n        ]\\n        for desc, expected in data:\\n            got = self.model.objects.get(desc=desc).is_leaf()\\n            self.assertEqual(got, expected)\\n\\n    def _multi_get_root(self):\\n        data = [\\n            (\'2\', \'2\'),\\n            (\'1\', \'1\'),\\n            (\'4\', \'4\'),\\n            (\'21\', \'2\'),\\n            (\'24\', \'2\'),\\n            (\'22\', \'2\'),\\n            (\'231\', \'2\'),\\n        ]\\n        for desc, expected in data:\\n            node = self.model.objects.get(desc=desc).get_root()\\n            self.assertEqual(node.desc, expected)\\n\\n    def _multi_get_parent(self):\\n        data = [\\n            (\'2\', None),\\n            (\'1\', None),\\n            (\'4\', None),\\n            (\'21\', \'2\'),\\n            (\'24\', \'2\'),\\n            (\'22\', \'2\'),\\n            (\'231\', \'23\'),\\n        ]\\n        data = dict(data)\\n        objs = {}\\n        for desc, expected in data.items():\\n            node = self.model.objects.get(desc=desc)\\n            parent = node.get_parent()\\n            if expected:\\n                self.assertEqual(parent.desc, expected)\\n            else:\\n                self.assertEqual(parent, None)\\n            objs[desc] = node\\n            # corrupt the objects\' parent cache\\n            node._parent_obj = \'CORRUPTED!!!\'\\n\\n        for desc, expected in data.items():\\n            node = objs[desc]\\n            # asking get_parent to not use the parent cache (since we\\n            # corrupted it in the previous loop)\\n            parent = node.get_parent(True)\\n            if expected:\\n                self.assertEqual(parent.desc, expected)\\n            else:\\n                self.assertEqual(parent, None)\\n\\n    def _multi_get_children(self):\\n        data = [\\n            (\'2\', [\'21\', \'22\', \'23\', \'24\']),\\n            (\'23\', [\'231\']),\\n            (\'231\', []),\\n        ]\\n        for desc, expected in data:\\n            children = self.model.objects.get(desc=desc).get_children()\\n            self.assertEqual([node.desc for node in children], expected)\\n\\n    def _multi_get_children_count(self):\\n        data = [\\n            (\'2\', 4),\\n            (\'23\', 1),\\n            (\'231\', 0),\\n        ]\\n        for desc, expected in data:\\n            got = self.model.objects.get(desc=desc).get_children_count()\\n            self.assertEqual(got, expected)\\n\\n    def _multi_get_siblings(self):\\n        data = [\\n            (\'2\', [\'1\', \'2\', \'3\', \'4\']),\\n            (\'21\', [\'21\', \'22\', \'23\', \'24\']),\\n            (\'231\', [\'231\']),\\n        ]\\n        for desc, expected in data:\\n            siblings = self.model.objects.get(desc=desc).get_siblings()\\n            self.assertEqual([node.desc for node in siblings], expected)\\n\\n    def _multi_get_first_sibling(self):\\n        data = [\\n            (\'2\', \'1\'),\\n            (\'1\', \'1\'),\\n            (\'4\', \'1\'),\\n            (\'21\', \'21\'),\\n            (\'24\', \'21\'),\\n            (\'22\', \'21\'),\\n            (\'231\', \'231\'),\\n        ]\\n        for desc, expected in data:\\n            node = self.model.objects.get(desc=desc).get_first_sibling()\\n            self.assertEqual(node.desc, expected)\\n\\n    def _multi_get_prev_sibling(self):\\n        data = [\\n            (\'2\', \'1\'),\\n            (\'1\', None),\\n            (\'4\', \'3\'),\\n            (\'21\', None),\\n            (\'24\', \'23\'),\\n            (\'22\', \'21\'),\\n            (\'231\', None),\\n        ]\\n        for desc, expected in data:\\n            node = self.model.objects.get(desc=desc).get_prev_sibling()\\n            if expected is None:\\n                self.assertEqual(node, None)\\n            else:\\n                self.assertEqual(node.desc, expected)\\n\\n    def _multi_get_next_sibling(self):\\n        data = [\\n            (\'2\', \'3\'),\\n            (\'1\', \'2\'),\\n            (\'4\', None),\\n            (\'21\', \'22\'),\\n            (\'24\', None),\\n            (\'22\', \'23\'),\\n            (\'231\', None),\\n        ]\\n        for desc, expected in data:\\n            node = self.model.objects.get(desc=desc).get_next_sibling()\\n            if expected is None:\\n                self.assertEqual(node, None)\\n            else:\\n                self.assertEqual(node.desc, expected)\\n\\n    def _multi_get_last_sibling(self):\\n        data = [\\n            (\'2\', \'4\'),\\n            (\'1\', \'4\'),\\n            (\'4\', \'4\'),\\n            (\'21\', \'24\'),\\n            (\'24\', \'24\'),\\n            (\'22\', \'24\'),\\n            (\'231\', \'231\'),\\n        ]\\n        for desc, expected in data:\\n            node = self.model.objects.get(desc=desc).get_last_sibling()\\n            self.assertEqual(node.desc, expected)\\n\\n    def _multi_get_first_child(self):\\n        data = [\\n            (\'2\', \'21\'),\\n            (\'21\', None),\\n            (\'23\', \'231\'),\\n            (\'231\', None),\\n        ]\\n        for desc, expected in data:\\n            node = self.model.objects.get(desc=desc).get_first_child()\\n            if expected is None:\\n                self.assertEqual(node, None)\\n            else:\\n                self.assertEqual(node.desc, expected)\\n\\n    def _multi_get_last_child(self):\\n        data = [\\n            (\'2\', \'24\'),\\n            (\'21\', None),\\n            (\'23\', \'231\'),\\n            (\'231\', None),\\n        ]\\n        for desc, expected in data:\\n            node = self.model.objects.get(desc=desc).get_last_child()\\n            if expected is None:\\n                self.assertEqual(node, None)\\n            else:\\n                self.assertEqual(node.desc, expected)\\n\\n    def _multi_get_ancestors(self):\\n        data = [\\n            (\'2\', []),\\n            (\'21\', [\'2\']),\\n            (\'231\', [\'2\', \'23\']),\\n        ]\\n        for desc, expected in data:\\n            nodes = self.model.objects.get(desc=desc).get_ancestors()\\n            self.assertEqual([node.desc for node in nodes], expected)\\n\\n    def _multi_get_descendants(self):\\n        data = [\\n            (\'2\', [\'21\', \'22\', \'23\', \'231\', \'24\']),\\n            (\'23\', [\'231\']),\\n            (\'231\', []),\\n            (\'1\', []),\\n            (\'4\', [\'41\']),\\n        ]\\n        for desc, expected in data:\\n            nodes = self.model.objects.get(desc=desc).get_descendants()\\n            self.assertEqual([node.desc for node in nodes], expected)\\n\\n    def _multi_get_descendant_count(self):\\n        data = [\\n            (\'2\', 5),\\n            (\'23\', 1),\\n            (\'231\', 0),\\n            (\'1\', 0),\\n            (\'4\', 1),\\n        ]\\n        for desc, expected in data:\\n            got = self.model.objects.get(desc=desc).get_descendant_count()\\n            self.assertEqual(got, expected)\\n\\n    def _multi_is_sibling_of(self):\\n        data = [\\n            (\'2\', \'2\', True),\\n            (\'2\', \'1\', True),\\n            (\'21\', \'2\', False),\\n            (\'231\', \'2\', False),\\n            (\'22\', \'23\', True),\\n            (\'231\', \'23\', False),\\n            (\'231\', \'231\', True),\\n        ]\\n        for desc1, desc2, expected in data:\\n            node1 = self.model.objects.get(desc=desc1)\\n            node2 = self.model.objects.get(desc=desc2)\\n            self.assertEqual(node1.is_sibling_of(node2), expected)\\n\\n    def _multi_is_child_of(self):\\n        data = [\\n            (\'2\', \'2\', False),\\n            (\'2\', \'1\', False),\\n            (\'21\', \'2\', True),\\n            (\'231\', \'2\', False),\\n            (\'231\', \'23\', True),\\n            (\'231\', \'231\', False),\\n        ]\\n        for desc1, desc2, expected in data:\\n            node1 = self.model.objects.get(desc=desc1)\\n            node2 = self.model.objects.get(desc=desc2)\\n            self.assertEqual(node1.is_child_of(node2), expected)\\n\\n    def _multi_is_descendant_of(self):\\n        data = [\\n            (\'2\', \'2\', False),\\n            (\'2\', \'1\', False),\\n            (\'21\', \'2\', True),\\n            (\'231\', \'2\', True),\\n            (\'231\', \'23\', True),\\n            (\'231\', \'231\', False),\\n        ]\\n        for desc1, desc2, expected in data:\\n            node1 = self.model.objects.get(desc=desc1)\\n            node2 = self.model.objects.get(desc=desc2)\\n            self.assertEqual(node1.is_descendant_of(node2), expected)\\n\\n\\nclass TestAddChild(TestNonEmptyTree):\\n\\n    def _multi_add_child_to_leaf(self):\\n        self.model.objects.get(desc=u\'231\').add_child(desc=\'2311\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 1),\\n                    (u\'2311\', 4, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_child_to_node(self):\\n        self.model.objects.get(desc=u\'2\').add_child(desc=\'25\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'25\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n\\nclass TestAddSibling(TestNonEmptyTree):\\n\\n    def _multi_add_sibling_invalid_pos(self):\\n        method = self.model.objects.get(desc=u\'231\').add_sibling\\n        self.assertRaises(InvalidPosition, method, \'invalid_pos\')\\n\\n    def _multi_add_sibling_missing_nodeorderby(self):\\n        node_wchildren = self.model.objects.get(desc=u\'2\')\\n        method = node_wchildren.add_sibling\\n        self.assertRaises(MissingNodeOrderBy, method, \'sorted-sibling\',\\n                          desc=\'aaa\')\\n\\n    def _multi_add_sibling_last_root(self):\\n        node_wchildren = self.model.objects.get(desc=u\'2\')\\n        obj = node_wchildren.add_sibling(\'last-sibling\', desc=\'5\')\\n        self.assertEqual(obj.get_depth(), 1)\\n        self.assertEqual(node_wchildren.get_last_sibling().desc, u\'5\')\\n\\n    def _multi_add_sibling_last(self):\\n        node = self.model.objects.get(desc=u\'231\')\\n        obj = node.add_sibling(\'last-sibling\', desc=\'232\')\\n        self.assertEqual(obj.get_depth(), 3)\\n        self.assertEqual(node.get_last_sibling().desc, u\'232\')\\n\\n    def _multi_add_sibling_first_root(self):\\n        node_wchildren = self.model.objects.get(desc=u\'2\')\\n        obj = node_wchildren.add_sibling(\'first-sibling\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 1)\\n        expected = [(u\'new\', 1, 0),\\n                    (u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_sibling_first(self):\\n        node_wchildren = self.model.objects.get(desc=u\'23\')\\n        obj = node_wchildren.add_sibling(\'first-sibling\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 2)\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'new\', 2, 0),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_sibling_left_root(self):\\n        node_wchildren = self.model.objects.get(desc=u\'2\')\\n        obj = node_wchildren.add_sibling(\'left\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 1)\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'new\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_sibling_left(self):\\n        node_wchildren = self.model.objects.get(desc=u\'23\')\\n        obj = node_wchildren.add_sibling(\'left\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 2)\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'new\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_sibling_left_noleft_root(self):\\n        node = self.model.objects.get(desc=u\'1\')\\n        obj = node.add_sibling(\'left\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 1)\\n        expected = [(u\'new\', 1, 0),\\n                    (u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_sibling_left_noleft(self):\\n        node = self.model.objects.get(desc=u\'231\')\\n        obj = node.add_sibling(\'left\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 3)\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 2),\\n                    (u\'new\', 3, 0),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_sibling_right_root(self):\\n        node_wchildren = self.model.objects.get(desc=u\'2\')\\n        obj = node_wchildren.add_sibling(\'right\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 1)\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'new\', 1, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_sibling_right(self):\\n        node_wchildren = self.model.objects.get(desc=u\'23\')\\n        obj = node_wchildren.add_sibling(\'right\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 2)\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'new\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_sibling_right_noright_root(self):\\n        node = self.model.objects.get(desc=u\'4\')\\n        obj = node.add_sibling(\'right\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 1)\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0),\\n                    (u\'new\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_sibling_right_noright(self):\\n        node = self.model.objects.get(desc=u\'231\')\\n        obj = node.add_sibling(\'right\', desc=\'new\')\\n        self.assertEqual(obj.get_depth(), 3)\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 2),\\n                    (u\'231\', 3, 0),\\n                    (u\'new\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n\\nclass TestDelete(TestNonEmptyTree):\\n\\n    def setUp(self):\\n        super(TestDelete, self).setUp()\\n        for node in self.model.objects.all():\\n            self.dep_model(node=node).save()\\n\\n    def _multi_delete_leaf(self):\\n        self.model.objects.get(desc=u\'231\').delete()\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_delete_node(self):\\n        self.model.objects.get(desc=u\'23\').delete()\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 3),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_delete_root(self):\\n        self.model.objects.get(desc=u\'2\').delete()\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_delete_filter_root_nodes(self):\\n        self.model.objects.filter(desc__in=(\'2\', \'3\')).delete()\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_delete_filter_children(self):\\n        self.model.objects.filter(\\n            desc__in=(\'2\', \'23\', \'231\')).delete()\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_delete_nonexistant_nodes(self):\\n        self.model.objects.filter(desc__in=(\'ZZZ\', \'XXX\')).delete()\\n        self.assertEqual(self.got(), self.unchanged)\\n\\n    def _multi_delete_same_node_twice(self):\\n        self.model.objects.filter(\\n            desc__in=(\'2\', \'2\')).delete()\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_delete_all_root_nodes(self):\\n        self.model.get_root_nodes().delete()\\n        count = self.model.objects.count()\\n        self.assertEqual(count, 0)\\n\\n    def _multi_delete_all_nodes(self):\\n        self.model.objects.all().delete()\\n        count = self.model.objects.count()\\n        self.assertEqual(count, 0)\\n\\n\\nclass TestMoveErrors(TestNonEmptyTree):\\n\\n    def _multi_move_invalid_pos(self):\\n        node = self.model.objects.get(desc=u\'231\')\\n        self.assertRaises(InvalidPosition, node.move, node, \'invalid_pos\')\\n\\n    def _multi_move_to_descendant(self):\\n        node = self.model.objects.get(desc=u\'2\')\\n        target = self.model.objects.get(desc=u\'231\')\\n        self.assertRaises(InvalidMoveToDescendant, node.move, target,\\n            \'first-sibling\')\\n\\n    def _multi_move_missing_nodeorderby(self):\\n        node = self.model.objects.get(desc=u\'231\')\\n        self.assertRaises(MissingNodeOrderBy, node.move, node,\\n                          \'sorted-child\')\\n        self.assertRaises(MissingNodeOrderBy, node.move, node,\\n                          \'sorted-sibling\')\\n\\n\\nclass TestMoveSortedErrors(TestNonEmptyTree):\\n\\n    def _multi_nonsorted_move_in_sorted(self):\\n        node = self.sorted_model.add_root(val1=3, val2=3, desc=\'zxy\')\\n        self.assertRaises(InvalidPosition, node.move, node, \'left\')\\n\\n\\nclass TestMoveLeafRoot(TestNonEmptyTree):\\n\\n    def _multi_move_leaf_last_sibling_root(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'2\'), \'last-sibling\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0),\\n                    (u\'231\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_first_sibling_root(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'2\'), \'first-sibling\')\\n        expected = [(u\'231\', 1, 0),\\n                    (u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_left_sibling_root(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'2\'), \'left\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'231\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_right_sibling_root(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'2\'), \'right\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'231\', 1, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_last_child_root(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'2\'), \'last-child\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'231\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_first_child_root(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'2\'), \'first-child\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'231\', 2, 0),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n\\nclass TestMoveLeaf(TestNonEmptyTree):\\n\\n    def _multi_move_leaf_last_sibling(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'22\'), \'last-sibling\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'231\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_first_sibling(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'22\'), \'first-sibling\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'231\', 2, 0),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_left_sibling(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'22\'), \'left\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'231\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_right_sibling(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'22\'), \'right\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'231\', 2, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_left_sibling_itself(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'231\'), \'left\')\\n        self.assertEqual(self.got(), self.unchanged)\\n\\n    def _multi_move_leaf_last_child(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'22\'), \'last-child\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_leaf_first_child(self):\\n        self.model.objects.get(desc=u\'231\').move(\\n            self.model.objects.get(desc=u\'22\'), \'first-child\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'23\', 2, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n\\nclass TestMoveBranchRoot(TestNonEmptyTree):\\n\\n    def _multi_move_branch_first_sibling_root(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'2\'), \'first-sibling\')\\n        expected = [(u\'4\', 1, 1),\\n                    (u\'41\', 2, 0),\\n                    (u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_last_sibling_root(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'2\'), \'last-sibling\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_left_sibling_root(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'2\'), \'left\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_right_sibling_root(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'2\'), \'right\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_left_noleft_sibling_root(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'2\').get_first_sibling(), \'left\')\\n        expected = [(u\'4\', 1, 1),\\n                    (u\'41\', 2, 0),\\n                    (u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_right_noright_sibling_root(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'2\').get_last_sibling(), \'right\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0),\\n                    (u\'4\', 1, 1),\\n                    (u\'41\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_first_child_root(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'2\'), \'first-child\')\\n        expected = [(u\'1\', 1, 0),\\n                   (u\'2\', 1, 5),\\n                   (u\'4\', 2, 1),\\n                   (u\'41\', 3, 0),\\n                   (u\'21\', 2, 0),\\n                   (u\'22\', 2, 0),\\n                   (u\'23\', 2, 1),\\n                   (u\'231\', 3, 0),\\n                   (u\'24\', 2, 0),\\n                   (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_last_child_root(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'2\'), \'last-child\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'4\', 2, 1),\\n                    (u\'41\', 3, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n\\nclass TestMoveBranch(TestNonEmptyTree):\\n\\n    def _multi_move_branch_first_sibling(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'23\'), \'first-sibling\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'4\', 2, 1),\\n                    (u\'41\', 3, 0),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_last_sibling(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'23\'), \'last-sibling\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'4\', 2, 1),\\n                    (u\'41\', 3, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_left_sibling(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'23\'), \'left\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'4\', 2, 1),\\n                    (u\'41\', 3, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_right_sibling(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'23\'), \'right\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'4\', 2, 1),\\n                    (u\'41\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_left_noleft_sibling(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'23\').get_first_sibling(), \'left\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'4\', 2, 1),\\n                    (u\'41\', 3, 0),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_right_noright_sibling(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'23\').get_last_sibling(), \'right\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 5),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 1),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'4\', 2, 1),\\n                    (u\'41\', 3, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_left_itself_sibling(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'4\'), \'left\')\\n        self.assertEqual(self.got(), self.unchanged)\\n\\n    def _multi_move_branch_first_child(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'23\'), \'first-child\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 2),\\n                    (u\'4\', 3, 1),\\n                    (u\'41\', 4, 0),\\n                    (u\'231\', 3, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_branch_last_child(self):\\n        self.model.objects.get(desc=\'4\').move(\\n            self.model.objects.get(desc=\'23\'), \'last-child\')\\n        expected = [(u\'1\', 1, 0),\\n                    (u\'2\', 1, 4),\\n                    (u\'21\', 2, 0),\\n                    (u\'22\', 2, 0),\\n                    (u\'23\', 2, 2),\\n                    (u\'231\', 3, 0),\\n                    (u\'4\', 3, 1),\\n                    (u\'41\', 4, 0),\\n                    (u\'24\', 2, 0),\\n                    (u\'3\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n\\nclass TestTreeSorted(TestTreeBase):\\n\\n    def got(self):\\n        return [(o.val1, o.val2, o.desc, o.get_depth(), o.get_children_count())\\n                 for o in self.sorted_model.get_tree()]\\n\\n    def _multi_add_root_sorted(self):\\n        self.sorted_model.add_root(val1=3, val2=3, desc=\'zxy\')\\n        self.sorted_model.add_root(val1=1, val2=4, desc=\'bcd\')\\n        self.sorted_model.add_root(val1=2, val2=5, desc=\'zxy\')\\n        self.sorted_model.add_root(val1=3, val2=3, desc=\'abc\')\\n        self.sorted_model.add_root(val1=4, val2=1, desc=\'fgh\')\\n        self.sorted_model.add_root(val1=3, val2=3, desc=\'abc\')\\n        self.sorted_model.add_root(val1=2, val2=2, desc=\'qwe\')\\n        self.sorted_model.add_root(val1=3, val2=2, desc=\'vcx\')\\n        expected = [(1, 4, u\'bcd\', 1, 0),\\n                    (2, 2, u\'qwe\', 1, 0),\\n                    (2, 5, u\'zxy\', 1, 0),\\n                    (3, 2, u\'vcx\', 1, 0),\\n                    (3, 3, u\'abc\', 1, 0),\\n                    (3, 3, u\'abc\', 1, 0),\\n                    (3, 3, u\'zxy\', 1, 0),\\n                    (4, 1, u\'fgh\', 1, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_child_root_sorted(self):\\n        root = self.sorted_model.add_root(val1=0, val2=0, desc=\'aaa\')\\n        root.add_child(val1=3, val2=3, desc=\'zxy\')\\n        root.add_child(val1=1, val2=4, desc=\'bcd\')\\n        root.add_child(val1=2, val2=5, desc=\'zxy\')\\n        root.add_child(val1=3, val2=3, desc=\'abc\')\\n        root.add_child(val1=4, val2=1, desc=\'fgh\')\\n        root.add_child(val1=3, val2=3, desc=\'abc\')\\n        root.add_child(val1=2, val2=2, desc=\'qwe\')\\n        root.add_child(val1=3, val2=2, desc=\'vcx\')\\n        expected = [(0, 0, u\'aaa\', 1, 8),\\n                    (1, 4, u\'bcd\', 2, 0),\\n                    (2, 2, u\'qwe\', 2, 0),\\n                    (2, 5, u\'zxy\', 2, 0),\\n                    (3, 2, u\'vcx\', 2, 0),\\n                    (3, 3, u\'abc\', 2, 0),\\n                    (3, 3, u\'abc\', 2, 0),\\n                    (3, 3, u\'zxy\', 2, 0),\\n                    (4, 1, u\'fgh\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_add_child_nonroot_sorted(self):\\n\\n        get_node = lambda node_id: self.sorted_model.objects.get(pk=node_id)\\n\\n        root_id = self.sorted_model.add_root(val1=0, val2=0, desc=\'a\').id\\n        node_id = get_node(root_id).add_child(val1=0, val2=0, desc=\'ac\').id\\n        get_node(root_id).add_child(val1=0, val2=0, desc=\'aa\')\\n        get_node(root_id).add_child(val1=0, val2=0, desc=\'av\')\\n        get_node(node_id).add_child(val1=0, val2=0, desc=\'aca\')\\n        get_node(node_id).add_child(val1=0, val2=0, desc=\'acc\')\\n        get_node(node_id).add_child(val1=0, val2=0, desc=\'acb\')\\n\\n        expected = [(0, 0, u\'a\', 1, 3),\\n                    (0, 0, u\'aa\', 2, 0),\\n                    (0, 0, u\'ac\', 2, 3),\\n                    (0, 0, u\'aca\', 3, 0),\\n                    (0, 0, u\'acb\', 3, 0),\\n                    (0, 0, u\'acc\', 3, 0),\\n                    (0, 0, u\'av\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n    def _multi_move_sorted(self):\\n        self.sorted_model.add_root(val1=3, val2=3, desc=\'zxy\')\\n        self.sorted_model.add_root(val1=1, val2=4, desc=\'bcd\')\\n        self.sorted_model.add_root(val1=2, val2=5, desc=\'zxy\')\\n        self.sorted_model.add_root(val1=3, val2=3, desc=\'abc\')\\n        self.sorted_model.add_root(val1=4, val2=1, desc=\'fgh\')\\n        self.sorted_model.add_root(val1=3, val2=3, desc=\'abc\')\\n        self.sorted_model.add_root(val1=2, val2=2, desc=\'qwe\')\\n        self.sorted_model.add_root(val1=3, val2=2, desc=\'vcx\')\\n        root_nodes = self.sorted_model.get_root_nodes()\\n        target = root_nodes[0]\\n        for node in root_nodes[1:]:\\n\\n            # because raw queries don\'t update django objects\\n            node = self.sorted_model.objects.get(pk=node.id)\\n            target = self.sorted_model.objects.get(pk=target.id)\\n\\n            node.move(target, \'sorted-child\')\\n        expected = [(1, 4, u\'bcd\', 1, 7),\\n                    (2, 2, u\'qwe\', 2, 0),\\n                    (2, 5, u\'zxy\', 2, 0),\\n                    (3, 2, u\'vcx\', 2, 0),\\n                    (3, 3, u\'abc\', 2, 0),\\n                    (3, 3, u\'abc\', 2, 0),\\n                    (3, 3, u\'zxy\', 2, 0),\\n                    (4, 1, u\'fgh\', 2, 0)]\\n        self.assertEqual(self.got(), expected)\\n\\n\\nclass TestMP_TreeAlphabet(TestCase):\\n\\n    def test_alphabet(self):\\n        if not os.getenv(\'TREEBEARD_TEST_ALPHABET\', False):\\n            # run this test only if the enviroment variable is set\\n            return\\n        basealpha = numconv.BASE85\\n        got_err = False\\n        last_good = None\\n        for alphabetlen in range(35, len(basealpha) + 1):\\n            alphabet = basealpha[0:alphabetlen]\\n            expected = [alphabet[0] + char for char in alphabet[1:]]\\n            expected.extend([alphabet[1] + char for char in alphabet])\\n            expected.append(alphabet[2] + alphabet[0])\\n\\n            # remove all nodes\\n            MP_TestNodeAlphabet.objects.all().delete()\\n\\n            # change the model\'s alphabet\\n            MP_TestNodeAlphabet.alphabet = alphabet\\n\\n            # insert root nodes\\n            for pos in range(len(alphabet) * 2):\\n                try:\\n                    MP_TestNodeAlphabet.add_root(numval=pos)\\n                except:\\n                    got_err = True\\n                    break\\n            if got_err:\\n                break\\n            got = [obj.path for obj in MP_TestNodeAlphabet.objects.all()]\\n            if got != expected:\\n                got_err = True\\n            last_good = alphabet\\n        print \'\\\\nThe best BASE85 based alphabet for your setup is: %s\' \\\\\\n            % (last_good, )\\n\\n\\nclass TestHelpers(TestTreeBase):\\n\\n    def setUp(self):\\n        for model in (MP_TestNode, AL_TestNode, NS_TestNode):\\n            model.load_bulk(BASE_DATA)\\n            for node in model.get_root_nodes():\\n                model.load_bulk(BASE_DATA, node)\\n            model.add_root(desc=\'5\')\\n\\n    def _multi_descendants_group_count_root(self):\\n        expected = [(o.desc, o.get_descendant_count())\\n                    for o in self.model.get_root_nodes()]\\n        got = [(o.desc, o.descendants_count)\\n               for o in self.model.get_descendants_group_count()]\\n        self.assertEqual(got, expected)\\n\\n    def _multi_descendants_group_count_node(self):\\n        parent = self.model.get_root_nodes().get(desc=\'2\')\\n        expected = [(o.desc, o.get_descendant_count())\\n                    for o in parent.get_children()]\\n        got = [(o.desc, o.descendants_count)\\n               for o in self.model.get_descendants_group_count(parent)]\\n        self.assertEqual(got, expected)\\n\\n\\nclass TestMP_TreeSortedAutoNow(TestCase):\\n    \\"\\"\\"\\n    The sorting mechanism used by treebeard when adding a node can fail if the\\n    ordering is using an \\"auto_now\\" field\\n    \\"\\"\\"\\n\\n    def test_sorted_by_autonow_workaround(self):\\n        \\"\\"\\"\\n        workaround\\n        \\"\\"\\"\\n        import datetime\\n        for i in range(1, 5):\\n            MP_TestNodeSortedAutoNow.add_root(desc=\'node%d\' % (i, ),\\n                                             created=datetime.datetime.now())\\n\\n    def test_sorted_by_autonow_FAIL(self):\\n        \\"\\"\\"\\n        This test asserts that we have a problem.\\n        fix this, somehow\\n        \\"\\"\\"\\n        MP_TestNodeSortedAutoNow.add_root(desc=\'node1\')\\n        self.assertRaises(ValueError, MP_TestNodeSortedAutoNow.add_root,\\n                          desc=\'node2\')\\n\\n\\nclass TestMP_TreeStepOverflow(TestCase):\\n\\n    def test_add_root(self):\\n        method = MP_TestNodeSmallStep.add_root\\n        for i in range(1, 10):\\n            method()\\n        self.assertRaises(PathOverflow, method)\\n\\n    def test_add_child(self):\\n        root = MP_TestNodeSmallStep.add_root()\\n        method = root.add_child\\n        for i in range(1, 10):\\n            method()\\n        self.assertRaises(PathOverflow, method)\\n\\n    def test_add_sibling(self):\\n        root = MP_TestNodeSmallStep.add_root()\\n        for i in range(1, 10):\\n            root.add_child()\\n        method = root.get_last_child().add_sibling\\n        positions = (\'first-sibling\', \'left\', \'right\', \'last-sibling\')\\n        for pos in positions:\\n            self.assertRaises(PathOverflow, method, pos)\\n\\n    def test_move(self):\\n        root = MP_TestNodeSmallStep.add_root()\\n        for i in range(1, 10):\\n            root.add_child()\\n        newroot = MP_TestNodeSmallStep.add_root()\\n        targets = [(root, [\'first-child\', \'last-child\']),\\n                   (root.get_first_child(), [\'first-sibling\',\\n                                            \'left\',\\n                                            \'right\',\\n                                            \'last-sibling\'])]\\n        for target, positions in targets:\\n            for pos in positions:\\n                self.assertRaises(PathOverflow, newroot.move, target, pos)\\n\\n\\nclass TestMP_TreeShortPath(TestCase):\\n    \\"\\"\\"\\n    Here we test a tree with a very small path field (max_length=4) and a\\n    steplen of 1\\n    \\"\\"\\"\\n\\n    def test_short_path(self):\\n        obj = MP_TestNodeShortPath.add_root()\\n        obj = obj.add_child().add_child().add_child()\\n        self.assertRaises(PathOverflow, obj.add_child)\\n\\n\\nclass TestMP_TreeFindProblems(TestTreeBase):\\n\\n    def test_find_problems(self):\\n        model = MP_TestNodeAlphabet\\n        model.alphabet = \'01234\'\\n        model(path=\'01\', depth=1, numchild=0, numval=0).save()\\n        model(path=\'1\', depth=1, numchild=0, numval=0).save()\\n        model(path=\'111\', depth=1, numchild=0, numval=0).save()\\n        model(path=\'abcd\', depth=1, numchild=0, numval=0).save()\\n        model(path=\'qa#$%!\', depth=1, numchild=0, numval=0).save()\\n        model(path=\'0201\', depth=2, numchild=0, numval=0).save()\\n        model(path=\'020201\', depth=3, numchild=0, numval=0).save()\\n        model(path=\'03\', depth=1, numchild=2, numval=0).save()\\n        model(path=\'0301\', depth=2, numchild=0, numval=0).save()\\n        model(path=\'030102\', depth=3, numchild=10, numval=0).save()\\n        model(path=\'04\', depth=10, numchild=1, numval=0).save()\\n        model(path=\'0401\', depth=20, numchild=0, numval=0).save()\\n\\n        evil_chars, bad_steplen, orphans, wrong_depth, wrong_numchild = \\\\\\n                                                        model.find_problems()\\n        self.assertEqual([\'abcd\', \'qa#$%!\'],\\n            [o.path for o in model.objects.filter(id__in=evil_chars)])\\n        self.assertEqual([\'1\', \'111\'],\\n            [o.path for o in model.objects.filter(id__in=bad_steplen)])\\n        self.assertEqual([\'0201\', \'020201\'],\\n            [o.path for o in model.objects.filter(id__in=orphans)])\\n        self.assertEqual([\'03\', \'0301\', \'030102\'],\\n            [o.path for o in model.objects.filter(id__in=wrong_numchild)])\\n        self.assertEqual([\'04\', \'0401\'],\\n            [o.path for o in model.objects.filter(id__in=wrong_depth)])\\n\\n\\nclass TestMP_TreeFix(TestTreeBase):\\n\\n    def setUp(self):\\n        super(TestMP_TreeFix, self).setUp()\\n        self.expected_no_holes = {\\n            MP_TestNodeShortPath: [\\n                (u\'1\', u\'b\', 1, 2),\\n                (u\'11\', u\'u\', 2, 1),\\n                (u\'111\', u\'i\', 3, 1),\\n                (u\'1111\', u\'e\', 4, 0),\\n                (u\'12\', u\'o\', 2, 0),\\n                (u\'2\', u\'d\', 1, 0),\\n                (u\'3\', u\'g\', 1, 0),\\n                (u\'4\', u\'a\', 1, 4),\\n                (u\'41\', u\'a\', 2, 0),\\n                (u\'42\', u\'a\', 2, 0),\\n                (u\'43\', u\'u\', 2, 1),\\n                (u\'431\', u\'i\', 3, 1),\\n                (u\'4311\', u\'e\', 4, 0),\\n                (u\'44\', u\'o\', 2, 0)],\\n            MP_TestSortedNodeShortPath: [\\n                (u\'1\', u\'a\', 1, 4),\\n                (u\'11\', u\'a\', 2, 0),\\n                (u\'12\', u\'a\', 2, 0),\\n                (u\'13\', u\'o\', 2, 0),\\n                (u\'14\', u\'u\', 2, 1),\\n                (u\'141\', u\'i\', 3, 1),\\n                (u\'1411\', u\'e\', 4, 0),\\n                (u\'2\', u\'b\', 1, 2),\\n                (u\'21\', u\'o\', 2, 0),\\n                (u\'22\', u\'u\', 2, 1),\\n                (u\'221\', u\'i\', 3, 1),\\n                (u\'2211\', u\'e\', 4, 0),\\n                (u\'3\', u\'d\', 1, 0),\\n                (u\'4\', u\'g\', 1, 0)]}\\n        self.expected_with_holes = {\\n            MP_TestNodeShortPath: [\\n                (u\'1\', u\'b\', 1L, 2L),\\n                (u\'13\', u\'u\', 2L, 1L),\\n                (u\'134\', u\'i\', 3L, 1L),\\n                (u\'1343\', u\'e\', 4L, 0L),\\n                (u\'14\', u\'o\', 2L, 0L),\\n                (u\'2\', u\'d\', 1L, 0L),\\n                (u\'3\', u\'g\', 1L, 0L),\\n                (u\'4\', u\'a\', 1L, 4L),\\n                (u\'41\', u\'a\', 2L, 0L),\\n                (u\'42\', u\'a\', 2L, 0L),\\n                (u\'43\', u\'u\', 2L, 1L),\\n                (u\'434\', u\'i\', 3L, 1L),\\n                (u\'4343\', u\'e\', 4L, 0L),\\n                (u\'44\', u\'o\', 2L, 0L)],\\n            MP_TestSortedNodeShortPath: [\\n                (u\'1\', u\'b\', 1L, 2L),\\n                (u\'13\', u\'u\', 2L, 1L),\\n                (u\'134\', u\'i\', 3L, 1L),\\n                (u\'1343\', u\'e\', 4L, 0L),\\n                (u\'14\', u\'o\', 2L, 0L),\\n                (u\'2\', u\'d\', 1L, 0L),\\n                (u\'3\', u\'g\', 1L, 0L),\\n                (u\'4\', u\'a\', 1L, 4L),\\n                (u\'41\', u\'a\', 2L, 0L),\\n                (u\'42\', u\'a\', 2L, 0L),\\n                (u\'43\', u\'u\', 2L, 1L),\\n                (u\'434\', u\'i\', 3L, 1L),\\n                (u\'4343\', u\'e\', 4L, 0L),\\n                (u\'44\', u\'o\', 2L, 0L)]}\\n\\n    def got(self, model):\\n        return [(o.path, o.desc, o.get_depth(), o.get_children_count())\\n                for o in model.get_tree()]\\n\\n    def add_broken_test_data(self, model):\\n        model(path=\'4\', depth=2, numchild=2, desc=\'a\').save()\\n        model(path=\'13\', depth=1000, numchild=0, desc=\'u\').save()\\n        model(path=\'14\', depth=4, numchild=500, desc=\'o\').save()\\n        model(path=\'134\', depth=321, numchild=543, desc=\'i\').save()\\n        model(path=\'1343\', depth=321, numchild=543, desc=\'e\').save()\\n        model(path=\'42\', depth=1, numchild=1, desc=\'a\').save()\\n        model(path=\'43\', depth=1000, numchild=0, desc=\'u\').save()\\n        model(path=\'44\', depth=4, numchild=500, desc=\'o\').save()\\n        model(path=\'434\', depth=321, numchild=543, desc=\'i\').save()\\n        model(path=\'4343\', depth=321, numchild=543, desc=\'e\').save()\\n        model(path=\'41\', depth=1, numchild=1, desc=\'a\').save()\\n        model(path=\'3\', depth=221, numchild=322, desc=\'g\').save()\\n        model(path=\'1\', depth=10, numchild=3, desc=\'b\').save()\\n        model(path=\'2\', depth=10, numchild=3, desc=\'d\').save()\\n\\n    def test_fix_tree_non_destructive(self):\\n\\n        for model in (MP_TestNodeShortPath, MP_TestSortedNodeShortPath):\\n            self.add_broken_test_data(model)\\n            model.fix_tree(destructive=False)\\n            self.assertEqual(self.got(model), self.expected_with_holes[model])\\n            model.find_problems()\\n\\n    def test_fix_tree_destructive(self):\\n\\n        for model in (MP_TestNodeShortPath, MP_TestSortedNodeShortPath):\\n            self.add_broken_test_data(model)\\n            model.fix_tree(destructive=True)\\n            self.assertEqual(self.got(model), self.expected_no_holes[model])\\n            model.find_problems()\\n\\n\\nclass TestIssues(TestCase):\\n    \\"test for http://code.google.com/p/django-treebeard/issues/detail?id=14\\"\\n\\n    def test_many_to_many_django_user_anonymous(self):\\n        if not HAS_DJANGO_AUTH:  # pragma: no cover\\n            self.fail(\'this test needs django.contrib.auth in INSTALLED_APPS\')\\n\\n        # Using AnonymousUser() in the querysets will expose non-treebeard\\n        # related problems in Django 1.0\\n        #\\n        # Postgres:\\n        #   ProgrammingError: can\'t adapt\\n        # SQLite:\\n        #   InterfaceError: Error binding parameter 4 - probably unsupported\\n        #   type.\\n        # MySQL compared a string to an integer field:\\n        #   `treebeard_mp_testissue14_users`.`user_id` = \'AnonymousUser\'\\n        #\\n        # Using a None field instead works (will be translated to IS NULL).\\n        #\\n        # anonuserobj = AnonymousUser()\\n        anonuserobj = None\\n\\n        def qs_check(qs, expected):\\n            self.assertEqual(\\n                [o.name for o in qs],\\n                expected)\\n\\n        user = User.objects.create_user(\'test_user\', \'test@example.com\',\\n                                        \'testpasswd\')\\n        user.save()\\n        root = MP_TestIssue14.add_root(name=\\"the root node\\")\\n\\n        root.add_child(name=\\"first\\")\\n        second = root.add_child(name=\\"second\\")\\n\\n        qs_check(root.get_children(), [\'first\', \'second\'])\\n        qs_check(root.get_children().filter(Q(name=\\"first\\")), [\'first\'])\\n        qs_check(root.get_children().filter(Q(users=user)), [])\\n        qs_check(\\n            root.get_children().filter(Q(name=\\"first\\") | Q(users=user)),\\n            [\'first\'])\\n\\n        user = anonuserobj\\n        qs_check(\\n            root.get_children().filter(Q(name=\\"first\\") | Q(users=user)),\\n            [\'first\', \'second\'])\\n\\n        user = User.objects.get(username=\\"test_user\\")\\n        second.users.add(user)\\n\\n        qs_check(\\n            root.get_children().filter(Q(name=\\"first\\") | Q(users=user)),\\n            [\'first\', \'second\'])\\n\\n        user = anonuserobj\\n        qs_check(\\n            root.get_children().filter(Q(name=\\"first\\") | Q(users=user)),\\n            [\'first\'])\\n\\n\\nclass TestModelAdmin(ModelAdmin):\\n    form = MoveNodeForm\\n\\n\\nclass TestMoveNodeForm(TestTreeBase):\\n\\n    tpl = (u\'\\u003ctr\\u003e\\u003cth\\u003e\\u003clabel for=\\"id__position\\"\\u003ePosition:\\u003c/label\\u003e\\u003c/th\\u003e\'\\n           \'\\u003ctd\\u003e\\u003cselect name=\\"_position\\" id=\\"id__position\\"\\u003e\\\\n\'\\n           \'\\u003coption value=\\"first-child\\"\\u003eFirst child of\\u003c/option\\u003e\\\\n\'\\n           \'\\u003coption value=\\"left\\"\\u003eBefore\\u003c/option\\u003e\\\\n\'\\n           \'\\u003coption value=\\"right\\"\\u003eAfter\\u003c/option\\u003e\\\\n\'\\n           \'\\u003c/select\\u003e\\u003c/td\\u003e\\u003c/tr\\u003e\\\\n\'\\n           \'\\u003ctr\\u003e\\u003cth\\u003e\\u003clabel for=\\"id__ref_node_id\\"\\u003eRelative to:\\u003c/label\\u003e\'\\n           \'\\u003c/th\\u003e\\u003ctd\\u003e\\u003cselect name=\\"_ref_node_id\\" id=\\"id__ref_node_id\\"\\u003e\\\\n\'\\n           \'\\u003coption value=\\"0\\"\\u003e-- root --\\u003c/option\\u003e\\\\n\')\\n\\n    def _multi_form_html_root_node(self):\\n        self.model.load_bulk(BASE_DATA)\\n        node = self.model.get_tree()[0]\\n        form = MoveNodeForm(instance=node)\\n        rtpl = self.tpl\\n        self.assertEqual([\'_position\', \'_ref_node_id\'],\\n                         form.base_fields.keys())\\n        for obj in self.model.get_tree():\\n            if node != obj or obj.is_descendant_of(node):\\n                rtpl += \'\\u003coption value=\\"%d\\"\\u003e%sNode %d\\u003c/option\\u003e\\\\n\' % (\\n                    obj.id, \'. . \' * (obj.get_depth() - 1), obj.id)\\n        rtpl += \'\\u003c/select\\u003e\\u003c/td\\u003e\\u003c/tr\\u003e\'\\n        formstr = unicode(form).replace(u\' selected=\\"selected\\"\', u\'\')\\n        self.assertEqual(rtpl, formstr)\\n\\n    def _multi_form_html_leaf_node(self):\\n        self.model.load_bulk(BASE_DATA)\\n        nodes = list(self.model.get_tree())\\n        node = nodes[-1]\\n        form = MoveNodeForm(instance=node)\\n        rtpl = self.tpl\\n        self.assertEqual([\'_position\', \'_ref_node_id\'],\\n                         form.base_fields.keys())\\n        for obj in self.model.get_tree():\\n            if node != obj or obj.is_descendant_of(node):\\n                rtpl += \'\\u003coption value=\\"%d\\"\\u003e%sNode %d\\u003c/option\\u003e\\\\n\' % (\\n                    obj.id, \'. . \' * (obj.get_depth() - 1), obj.id)\\n        rtpl += \'\\u003c/select\\u003e\\u003c/td\\u003e\\u003c/tr\\u003e\'\\n        formstr = unicode(form).replace(u\' selected=\\"selected\\"\', u\'\')\\n        self.assertEqual(rtpl, formstr)\\n\\n    def _multi_admin_html(self):\\n        tpl = (\'\\u003ctr\\u003e\\u003cth\\u003e\\u003clabel for=\\"id_desc\\"\\u003eDesc:\\u003c/label\\u003e\'\\n               \'\\u003c/th\\u003e\\u003ctd\\u003e\\u003cinput id=\\"id_desc\\" type=\\"text\\" class=\\"vTextField\\" \'\\n               \'name=\\"desc\\" maxlength=\\"255\\" /\\u003e\\u003c/td\\u003e\\u003c/tr\\u003e\\\\n\'\\n               \'\\u003ctr\\u003e\\u003cth\\u003e\\u003clabel for=\\"id__position\\"\\u003ePosition:\\u003c/label\\u003e\\u003c/th\\u003e\'\\n               \'\\u003ctd\\u003e\\u003cselect name=\\"_position\\" id=\\"id__position\\"\\u003e\\\\n\'\\n               \'\\u003coption value=\\"first-child\\"\\u003eFirst child of\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"left\\"\\u003eBefore\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"right\\"\\u003eAfter\\u003c/option\\u003e\\\\n\'\\n               \'\\u003c/select\\u003e\\u003c/td\\u003e\\u003c/tr\\u003e\\\\n\'\\n               \'\\u003ctr\\u003e\\u003cth\\u003e\\u003clabel for=\\"id__ref_node_id\\"\\u003eRelative to:\\u003c/label\\u003e\'\\n               \'\\u003c/th\\u003e\\u003ctd\\u003e\\u003cselect name=\\"_ref_node_id\\" id=\\"id__ref_node_id\\"\\u003e\\\\n\'\\n               \'\\u003coption value=\\"0\\"\\u003e-- root --\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003eNode %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003eNode %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003e. . Node %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003e. . Node %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003e. . Node %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003e. . . . Node %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003e. . Node %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003eNode %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003eNode %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003coption value=\\"%d\\"\\u003e. . Node %d\\u003c/option\\u003e\\\\n\'\\n               \'\\u003c/select\\u003e\\u003c/td\\u003e\\u003c/tr\\u003e\')\\n        request = None\\n        self.model.load_bulk(BASE_DATA)\\n        for node in self.model.objects.all():\\n            site = AdminSite()\\n            ma = TestModelAdmin(self.model, site)\\n            self.assertEqual(\\n                [\'desc\', \'_position\', \'_ref_node_id\'],\\n                ma.get_form(request).base_fields.keys())\\n            self.assertEqual(\\n                [(None, {\'fields\': [\'desc\', \'_position\', \'_ref_node_id\']})],\\n                ma.get_fieldsets(request))\\n            self.assertEqual(\\n                [(None, {\'fields\': [\'desc\', \'_position\', \'_ref_node_id\']})],\\n                ma.get_fieldsets(request, node))\\n            form = ma.get_form(request)()\\n            ids = []\\n            for obj in self.model.get_tree():\\n                ids.extend([obj.id] * 2)\\n            self.assertEqual(tpl % tuple(ids), unicode(form))\\n\\n\\n_load_test_methods(TestMoveNodeForm)\\n_load_test_methods(TestEmptyTree)\\n_load_test_methods(TestClassMethods)\\n_load_test_methods(TestSimpleNodeMethods)\\n_load_test_methods(TestAddChild)\\n_load_test_methods(TestAddSibling)\\n_load_test_methods(TestDelete)\\n_load_test_methods(TestMoveErrors)\\n_load_test_methods(TestMoveLeafRoot)\\n_load_test_methods(TestMoveLeaf)\\n_load_test_methods(TestMoveBranchRoot)\\n_load_test_methods(TestMoveBranch)\\n_load_test_methods(TestHelpers)\\n# we didn\'t create extra sorted-proxy models\\n_load_test_methods(TestMoveSortedErrors, proxy=False)\\n_load_test_methods(TestTreeSorted, proxy=False)\\n"}\n'
line: b'{"repo_name":"2014cdbg14/2014cdbg14","ref":"refs/heads/master","path":"wsgi/static/Brython2.1.0-20140419-113919/Lib/collections/abc.py","content":"# Copyright 2007 Google, Inc. All Rights Reserved.\\n# Licensed to PSF under a Contributor Agreement.\\n\\n\\"\\"\\"Abstract Base Classes (ABCs) for collections, according to PEP 3119.\\n\\nUnit tests are in test_collections.\\n\\"\\"\\"\\n\\nfrom abc import ABCMeta, abstractmethod\\nimport sys\\n\\n__all__ = [\\"Hashable\\", \\"Iterable\\", \\"Iterator\\",\\n           \\"Sized\\", \\"Container\\", \\"Callable\\",\\n           \\"Set\\", \\"MutableSet\\",\\n           \\"Mapping\\", \\"MutableMapping\\",\\n           \\"MappingView\\", \\"KeysView\\", \\"ItemsView\\", \\"ValuesView\\",\\n           \\"Sequence\\", \\"MutableSequence\\",\\n           \\"ByteString\\",\\n           ]\\n\\n# Private list of types that we want to register with the various ABCs\\n# so that they will pass tests like:\\n#       it = iter(somebytearray)\\n#       assert isinstance(it, Iterable)\\n# Note:  in other implementations, these types many not be distinct\\n# and they make have their own implementation specific types that\\n# are not included on this list.\\nbytes_iterator = type(iter(b\'\'))\\nbytearray_iterator = type(iter(bytearray()))\\n#callable_iterator = ???\\ndict_keyiterator = type(iter({}.keys()))\\ndict_valueiterator = type(iter({}.values()))\\ndict_itemiterator = type(iter({}.items()))\\nlist_iterator = type(iter([]))\\nlist_reverseiterator = type(iter(reversed([])))\\nrange_iterator = type(iter(range(0)))\\nset_iterator = type(iter(set()))\\nstr_iterator = type(iter(\\"\\"))\\ntuple_iterator = type(iter(()))\\nzip_iterator = type(iter(zip()))\\n## views ##\\ndict_keys = type({}.keys())\\ndict_values = type({}.values())\\ndict_items = type({}.items())\\n## misc ##\\nmappingproxy = type(type.__dict__)\\n\\n\\n### ONE-TRICK PONIES ###\\n\\nclass Hashable(metaclass=ABCMeta):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __hash__(self):\\n        return 0\\n\\n    @classmethod\\n    def __subclasshook__(cls, C):\\n        if cls is Hashable:\\n            for B in C.__mro__:\\n                if \\"__hash__\\" in B.__dict__:\\n                    if B.__dict__[\\"__hash__\\"]:\\n                        return True\\n                    break\\n        return NotImplemented\\n\\n\\nclass Iterable(metaclass=ABCMeta):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __iter__(self):\\n        while False:\\n            yield None\\n\\n    @classmethod\\n    def __subclasshook__(cls, C):\\n        if cls is Iterable:\\n            if any(\\"__iter__\\" in B.__dict__ for B in C.__mro__):\\n                return True\\n        return NotImplemented\\n\\n\\nclass Iterator(Iterable):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __next__(self):\\n        raise StopIteration\\n\\n    def __iter__(self):\\n        return self\\n\\n    @classmethod\\n    def __subclasshook__(cls, C):\\n        if cls is Iterator:\\n            if (any(\\"__next__\\" in B.__dict__ for B in C.__mro__) and\\n                any(\\"__iter__\\" in B.__dict__ for B in C.__mro__)):\\n                return True\\n        return NotImplemented\\n\\nIterator.register(bytes_iterator)\\nIterator.register(bytearray_iterator)\\n#Iterator.register(callable_iterator)\\nIterator.register(dict_keyiterator)\\nIterator.register(dict_valueiterator)\\nIterator.register(dict_itemiterator)\\nIterator.register(list_iterator)\\nIterator.register(list_reverseiterator)\\nIterator.register(range_iterator)\\nIterator.register(set_iterator)\\nIterator.register(str_iterator)\\nIterator.register(tuple_iterator)\\nIterator.register(zip_iterator)\\n\\nclass Sized(metaclass=ABCMeta):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __len__(self):\\n        return 0\\n\\n    @classmethod\\n    def __subclasshook__(cls, C):\\n        if cls is Sized:\\n            if any(\\"__len__\\" in B.__dict__ for B in C.__mro__):\\n                return True\\n        return NotImplemented\\n\\n\\nclass Container(metaclass=ABCMeta):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __contains__(self, x):\\n        return False\\n\\n    @classmethod\\n    def __subclasshook__(cls, C):\\n        if cls is Container:\\n            if any(\\"__contains__\\" in B.__dict__ for B in C.__mro__):\\n                return True\\n        return NotImplemented\\n\\n\\nclass Callable(metaclass=ABCMeta):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __call__(self, *args, **kwds):\\n        return False\\n\\n    @classmethod\\n    def __subclasshook__(cls, C):\\n        if cls is Callable:\\n            if any(\\"__call__\\" in B.__dict__ for B in C.__mro__):\\n                return True\\n        return NotImplemented\\n\\n\\n### SETS ###\\n\\n\\nclass Set(Sized, Iterable, Container):\\n\\n    \\"\\"\\"A set is a finite, iterable container.\\n\\n    This class provides concrete generic implementations of all\\n    methods except for __contains__, __iter__ and __len__.\\n\\n    To override the comparisons (presumably for speed, as the\\n    semantics are fixed), all you have to do is redefine __le__ and\\n    then the other operations will automatically follow suit.\\n    \\"\\"\\"\\n\\n    __slots__ = ()\\n\\n    def __le__(self, other):\\n        if not isinstance(other, Set):\\n            return NotImplemented\\n        if len(self) \\u003e len(other):\\n            return False\\n        for elem in self:\\n            if elem not in other:\\n                return False\\n        return True\\n\\n    def __lt__(self, other):\\n        if not isinstance(other, Set):\\n            return NotImplemented\\n        return len(self) \\u003c len(other) and self.__le__(other)\\n\\n    def __gt__(self, other):\\n        if not isinstance(other, Set):\\n            return NotImplemented\\n        return other \\u003c self\\n\\n    def __ge__(self, other):\\n        if not isinstance(other, Set):\\n            return NotImplemented\\n        return other \\u003c= self\\n\\n    def __eq__(self, other):\\n        if not isinstance(other, Set):\\n            return NotImplemented\\n        return len(self) == len(other) and self.__le__(other)\\n\\n    def __ne__(self, other):\\n        return not (self == other)\\n\\n    @classmethod\\n    def _from_iterable(cls, it):\\n        \'\'\'Construct an instance of the class from any iterable input.\\n\\n        Must override this method if the class constructor signature\\n        does not accept an iterable for an input.\\n        \'\'\'\\n        return cls(it)\\n\\n    def __and__(self, other):\\n        if not isinstance(other, Iterable):\\n            return NotImplemented\\n        return self._from_iterable(value for value in other if value in self)\\n\\n    def isdisjoint(self, other):\\n        for value in other:\\n            if value in self:\\n                return False\\n        return True\\n\\n    def __or__(self, other):\\n        if not isinstance(other, Iterable):\\n            return NotImplemented\\n        chain = (e for s in (self, other) for e in s)\\n        return self._from_iterable(chain)\\n\\n    def __sub__(self, other):\\n        if not isinstance(other, Set):\\n            if not isinstance(other, Iterable):\\n                return NotImplemented\\n            other = self._from_iterable(other)\\n        return self._from_iterable(value for value in self\\n                                   if value not in other)\\n\\n    def __xor__(self, other):\\n        if not isinstance(other, Set):\\n            if not isinstance(other, Iterable):\\n                return NotImplemented\\n            other = self._from_iterable(other)\\n        return (self - other) | (other - self)\\n\\n    def _hash(self):\\n        \\"\\"\\"Compute the hash value of a set.\\n\\n        Note that we don\'t define __hash__: not all sets are hashable.\\n        But if you define a hashable set type, its __hash__ should\\n        call this function.\\n\\n        This must be compatible __eq__.\\n\\n        All sets ought to compare equal if they contain the same\\n        elements, regardless of how they are implemented, and\\n        regardless of the order of the elements; so there\'s not much\\n        freedom for __eq__ or __hash__.  We match the algorithm used\\n        by the built-in frozenset type.\\n        \\"\\"\\"\\n        MAX = sys.maxsize\\n        MASK = 2 * MAX + 1\\n        n = len(self)\\n        h = 1927868237 * (n + 1)\\n        h \\u0026= MASK\\n        for x in self:\\n            hx = hash(x)\\n            h ^= (hx ^ (hx \\u003c\\u003c 16) ^ 89869747)  * 3644798167\\n            h \\u0026= MASK\\n        h = h * 69069 + 907133923\\n        h \\u0026= MASK\\n        if h \\u003e MAX:\\n            h -= MASK + 1\\n        if h == -1:\\n            h = 590923713\\n        return h\\n\\nSet.register(frozenset)\\n\\n\\nclass MutableSet(Set):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def add(self, value):\\n        \\"\\"\\"Add an element.\\"\\"\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def discard(self, value):\\n        \\"\\"\\"Remove an element.  Do not raise an exception if absent.\\"\\"\\"\\n        raise NotImplementedError\\n\\n    def remove(self, value):\\n        \\"\\"\\"Remove an element. If not a member, raise a KeyError.\\"\\"\\"\\n        if value not in self:\\n            raise KeyError(value)\\n        self.discard(value)\\n\\n    def pop(self):\\n        \\"\\"\\"Return the popped value.  Raise KeyError if empty.\\"\\"\\"\\n        it = iter(self)\\n        try:\\n            value = next(it)\\n        except StopIteration:\\n            raise KeyError\\n        self.discard(value)\\n        return value\\n\\n    def clear(self):\\n        \\"\\"\\"This is slow (creates N new iterators!) but effective.\\"\\"\\"\\n        try:\\n            while True:\\n                self.pop()\\n        except KeyError:\\n            pass\\n\\n    def __ior__(self, it):\\n        for value in it:\\n            self.add(value)\\n        return self\\n\\n    def __iand__(self, it):\\n        for value in (self - it):\\n            self.discard(value)\\n        return self\\n\\n    def __ixor__(self, it):\\n        if it is self:\\n            self.clear()\\n        else:\\n            if not isinstance(it, Set):\\n                it = self._from_iterable(it)\\n            for value in it:\\n                if value in self:\\n                    self.discard(value)\\n                else:\\n                    self.add(value)\\n        return self\\n\\n    def __isub__(self, it):\\n        if it is self:\\n            self.clear()\\n        else:\\n            for value in it:\\n                self.discard(value)\\n        return self\\n\\nMutableSet.register(set)\\n\\n\\n### MAPPINGS ###\\n\\n\\nclass Mapping(Sized, Iterable, Container):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __getitem__(self, key):\\n        raise KeyError\\n\\n    def get(self, key, default=None):\\n        try:\\n            return self[key]\\n        except KeyError:\\n            return default\\n\\n    def __contains__(self, key):\\n        try:\\n            self[key]\\n        except KeyError:\\n            return False\\n        else:\\n            return True\\n\\n    def keys(self):\\n        return KeysView(self)\\n\\n    def items(self):\\n        return ItemsView(self)\\n\\n    def values(self):\\n        return ValuesView(self)\\n\\n    def __eq__(self, other):\\n        if not isinstance(other, Mapping):\\n            return NotImplemented\\n        return dict(self.items()) == dict(other.items())\\n\\n    def __ne__(self, other):\\n        return not (self == other)\\n\\nMapping.register(mappingproxy)\\n\\n\\nclass MappingView(Sized):\\n\\n    def __init__(self, mapping):\\n        self._mapping = mapping\\n\\n    def __len__(self):\\n        return len(self._mapping)\\n\\n    def __repr__(self):\\n        return \'{0.__class__.__name__}({0._mapping!r})\'.format(self)\\n\\n\\nclass KeysView(MappingView, Set):\\n\\n    @classmethod\\n    def _from_iterable(self, it):\\n        return set(it)\\n\\n    def __contains__(self, key):\\n        return key in self._mapping\\n\\n    def __iter__(self):\\n        for key in self._mapping:\\n            yield key\\n\\nKeysView.register(dict_keys)\\n\\n\\nclass ItemsView(MappingView, Set):\\n\\n    @classmethod\\n    def _from_iterable(self, it):\\n        return set(it)\\n\\n    def __contains__(self, item):\\n        key, value = item\\n        try:\\n            v = self._mapping[key]\\n        except KeyError:\\n            return False\\n        else:\\n            return v == value\\n\\n    def __iter__(self):\\n        for key in self._mapping:\\n            yield (key, self._mapping[key])\\n\\nItemsView.register(dict_items)\\n\\n\\nclass ValuesView(MappingView):\\n\\n    def __contains__(self, value):\\n        for key in self._mapping:\\n            if value == self._mapping[key]:\\n                return True\\n        return False\\n\\n    def __iter__(self):\\n        for key in self._mapping:\\n            yield self._mapping[key]\\n\\nValuesView.register(dict_values)\\n\\n\\nclass MutableMapping(Mapping):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __setitem__(self, key, value):\\n        raise KeyError\\n\\n    @abstractmethod\\n    def __delitem__(self, key):\\n        raise KeyError\\n\\n    __marker = object()\\n\\n    def pop(self, key, default=__marker):\\n        try:\\n            value = self[key]\\n        except KeyError:\\n            if default is self.__marker:\\n                raise\\n            return default\\n        else:\\n            del self[key]\\n            return value\\n\\n    def popitem(self):\\n        try:\\n            key = next(iter(self))\\n        except StopIteration:\\n            raise KeyError\\n        value = self[key]\\n        del self[key]\\n        return key, value\\n\\n    def clear(self):\\n        try:\\n            while True:\\n                self.popitem()\\n        except KeyError:\\n            pass\\n\\n    def update(*args, **kwds):\\n        if len(args) \\u003e 2:\\n            raise TypeError(\\"update() takes at most 2 positional \\"\\n                            \\"arguments ({} given)\\".format(len(args)))\\n        elif not args:\\n            raise TypeError(\\"update() takes at least 1 argument (0 given)\\")\\n        self = args[0]\\n        other = args[1] if len(args) \\u003e= 2 else ()\\n\\n        if isinstance(other, Mapping):\\n            for key in other:\\n                self[key] = other[key]\\n        elif hasattr(other, \\"keys\\"):\\n            for key in other.keys():\\n                self[key] = other[key]\\n        else:\\n            for key, value in other:\\n                self[key] = value\\n        for key, value in kwds.items():\\n            self[key] = value\\n\\n    def setdefault(self, key, default=None):\\n        try:\\n            return self[key]\\n        except KeyError:\\n            self[key] = default\\n        return default\\n\\nMutableMapping.register(dict)\\n\\n\\n### SEQUENCES ###\\n\\n\\nclass Sequence(Sized, Iterable, Container):\\n\\n    \\"\\"\\"All the operations on a read-only sequence.\\n\\n    Concrete subclasses must override __new__ or __init__,\\n    __getitem__, and __len__.\\n    \\"\\"\\"\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __getitem__(self, index):\\n        raise IndexError\\n\\n    def __iter__(self):\\n        i = 0\\n        try:\\n            while True:\\n                v = self[i]\\n                yield v\\n                i += 1\\n        except IndexError:\\n            return\\n\\n    def __contains__(self, value):\\n        for v in self:\\n            if v == value:\\n                return True\\n        return False\\n\\n    def __reversed__(self):\\n        for i in reversed(range(len(self))):\\n            yield self[i]\\n\\n    def index(self, value):\\n        for i, v in enumerate(self):\\n            if v == value:\\n                return i\\n        raise ValueError\\n\\n    def count(self, value):\\n        return sum(1 for v in self if v == value)\\n\\nSequence.register(tuple)\\nSequence.register(str)\\nSequence.register(range)\\n\\n\\nclass ByteString(Sequence):\\n\\n    \\"\\"\\"This unifies bytes and bytearray.\\n\\n    XXX Should add all their methods.\\n    \\"\\"\\"\\n\\n    __slots__ = ()\\n\\nByteString.register(bytes)\\nByteString.register(bytearray)\\n\\n\\nclass MutableSequence(Sequence):\\n\\n    __slots__ = ()\\n\\n    @abstractmethod\\n    def __setitem__(self, index, value):\\n        raise IndexError\\n\\n    @abstractmethod\\n    def __delitem__(self, index):\\n        raise IndexError\\n\\n    @abstractmethod\\n    def insert(self, index, value):\\n        raise IndexError\\n\\n    def append(self, value):\\n        self.insert(len(self), value)\\n\\n    def clear(self):\\n        try:\\n            while True:\\n                self.pop()\\n        except IndexError:\\n            pass\\n\\n    def reverse(self):\\n        n = len(self)\\n        for i in range(n//2):\\n            self[i], self[n-i-1] = self[n-i-1], self[i]\\n\\n    def extend(self, values):\\n        for v in values:\\n            self.append(v)\\n\\n    def pop(self, index=-1):\\n        v = self[index]\\n        del self[index]\\n        return v\\n\\n    def remove(self, value):\\n        del self[self.index(value)]\\n\\n    def __iadd__(self, values):\\n        self.extend(values)\\n        return self\\n\\nMutableSequence.register(list)\\nMutableSequence.register(bytearray)  # Multiply inheriting, see ByteString\\n"}\n'
line: b'{"repo_name":"40223205/w16b_test-","ref":"refs/heads/master","path":"static/Brython3.1.1-20150328-091302/Lib/_socket.py","content":"\\"\\"\\"Implementation module for socket operations.\\n\\nSee the socket module for documentation.\\"\\"\\"\\n\\n\\nAF_APPLETALK = 16\\n\\nAF_DECnet = 12\\n\\nAF_INET = 2\\n\\nAF_INET6 = 23\\n\\nAF_IPX = 6\\n\\nAF_IRDA = 26\\n\\nAF_SNA = 11\\n\\nAF_UNSPEC = 0\\n\\nAI_ADDRCONFIG = 1024\\n\\nAI_ALL = 256\\n\\nAI_CANONNAME = 2\\n\\nAI_NUMERICHOST = 4\\n\\nAI_NUMERICSERV = 8\\n\\nAI_PASSIVE = 1\\n\\nAI_V4MAPPED = 2048\\n\\nCAPI = \'\\u003ccapsule object \\"_socket.CAPI\\" at 0x00BC4F38\\u003e\'\\n\\nEAI_AGAIN = 11002\\n\\nEAI_BADFLAGS = 10022\\n\\nEAI_FAIL = 11003\\n\\nEAI_FAMILY = 10047\\n\\nEAI_MEMORY = 8\\n\\nEAI_NODATA = 11001\\n\\nEAI_NONAME = 11001\\n\\nEAI_SERVICE = 10109\\n\\nEAI_SOCKTYPE = 10044\\n\\nINADDR_ALLHOSTS_GROUP = -536870911\\n\\nINADDR_ANY = 0\\n\\nINADDR_BROADCAST = -1\\n\\nINADDR_LOOPBACK = 2130706433\\n\\nINADDR_MAX_LOCAL_GROUP = -536870657\\n\\nINADDR_NONE = -1\\n\\nINADDR_UNSPEC_GROUP = -536870912\\n\\nIPPORT_RESERVED = 1024\\n\\nIPPORT_USERRESERVED = 5000\\n\\nIPPROTO_ICMP = 1\\n\\nIPPROTO_IP = 0\\n\\nIPPROTO_RAW = 255\\n\\nIPPROTO_TCP = 6\\n\\nIPPROTO_UDP = 17\\n\\nIPV6_CHECKSUM = 26\\n\\nIPV6_DONTFRAG = 14\\n\\nIPV6_HOPLIMIT = 21\\n\\nIPV6_HOPOPTS = 1\\n\\nIPV6_JOIN_GROUP = 12\\n\\nIPV6_LEAVE_GROUP = 13\\n\\nIPV6_MULTICAST_HOPS = 10\\n\\nIPV6_MULTICAST_IF = 9\\n\\nIPV6_MULTICAST_LOOP = 11\\n\\nIPV6_PKTINFO = 19\\n\\nIPV6_RECVRTHDR = 38\\n\\nIPV6_RECVTCLASS = 40\\n\\nIPV6_RTHDR = 32\\n\\nIPV6_TCLASS = 39\\n\\nIPV6_UNICAST_HOPS = 4\\n\\nIPV6_V6ONLY = 27\\n\\nIP_ADD_MEMBERSHIP = 12\\n\\nIP_DROP_MEMBERSHIP = 13\\n\\nIP_HDRINCL = 2\\n\\nIP_MULTICAST_IF = 9\\n\\nIP_MULTICAST_LOOP = 11\\n\\nIP_MULTICAST_TTL = 10\\n\\nIP_OPTIONS = 1\\n\\nIP_RECVDSTADDR = 25\\n\\nIP_TOS = 3\\n\\nIP_TTL = 4\\n\\nMSG_BCAST = 1024\\n\\nMSG_CTRUNC = 512\\n\\nMSG_DONTROUTE = 4\\n\\nMSG_MCAST = 2048\\n\\nMSG_OOB = 1\\n\\nMSG_PEEK = 2\\n\\nMSG_TRUNC = 256\\n\\nNI_DGRAM = 16\\n\\nNI_MAXHOST = 1025\\n\\nNI_MAXSERV = 32\\n\\nNI_NAMEREQD = 4\\n\\nNI_NOFQDN = 1\\n\\nNI_NUMERICHOST = 2\\n\\nNI_NUMERICSERV = 8\\n\\nRCVALL_MAX = 3\\n\\nRCVALL_OFF = 0\\n\\nRCVALL_ON = 1\\n\\nRCVALL_SOCKETLEVELONLY = 2\\n\\nSHUT_RD = 0\\n\\nSHUT_RDWR = 2\\n\\nSHUT_WR = 1\\n\\nSIO_KEEPALIVE_VALS = 2550136836\\n\\nSIO_RCVALL = 2550136833\\n\\nSOCK_DGRAM = 2\\n\\nSOCK_RAW = 3\\n\\nSOCK_RDM = 4\\n\\nSOCK_SEQPACKET = 5\\n\\nSOCK_STREAM = 1\\n\\nSOL_IP = 0\\n\\nSOL_SOCKET = 65535\\n\\nSOL_TCP = 6\\n\\nSOL_UDP = 17\\n\\nSOMAXCONN = 2147483647\\n\\nSO_ACCEPTCONN = 2\\n\\nSO_BROADCAST = 32\\n\\nSO_DEBUG = 1\\n\\nSO_DONTROUTE = 16\\n\\nSO_ERROR = 4103\\n\\nSO_EXCLUSIVEADDRUSE = -5\\n\\nSO_KEEPALIVE = 8\\n\\nSO_LINGER = 128\\n\\nSO_OOBINLINE = 256\\n\\nSO_RCVBUF = 4098\\n\\nSO_RCVLOWAT = 4100\\n\\nSO_RCVTIMEO = 4102\\n\\nSO_REUSEADDR = 4\\n\\nSO_SNDBUF = 4097\\n\\nSO_SNDLOWAT = 4099\\n\\nSO_SNDTIMEO = 4101\\n\\nSO_TYPE = 4104\\n\\nSO_USELOOPBACK = 64\\n\\nclass SocketType:\\n    pass\\n\\nTCP_MAXSEG = 4\\n\\nTCP_NODELAY = 1\\n\\n__loader__ = \'\\u003c_frozen_importlib.ExtensionFileLoader object at 0x00CA2D90\\u003e\'\\n\\ndef dup(*args,**kw):\\n    \\"\\"\\"dup(integer) -\\u003e integer    \\n    Duplicate an integer socket file descriptor.  This is like os.dup(), but for\\n    sockets; on some platforms os.dup() won\'t work for socket file descriptors.\\"\\"\\"\\n    pass\\n\\nclass error:\\n    pass\\n\\nclass gaierror:\\n    pass\\n\\ndef getaddrinfo(*args,**kw):\\n    \\"\\"\\"getaddrinfo(host, port [, family, socktype, proto, flags])        -\\u003e list of (family, socktype, proto, canonname, sockaddr)\\n    \\n    Resolve host and port into addrinfo struct.\\"\\"\\"\\n    pass\\n\\ndef getdefaulttimeout(*args,**kw):\\n    \\"\\"\\"getdefaulttimeout() -\\u003e timeout    \\n    Returns the default timeout in seconds (float) for new socket objects.\\n    A value of None indicates that new socket objects have no timeout.\\n    When the socket module is first imported, the default is None.\\"\\"\\"\\n    pass\\n\\ndef gethostbyaddr(*args,**kw):\\n    \\"\\"\\"gethostbyaddr(host) -\\u003e (name, aliaslist, addresslist)    \\n    Return the true host name, a list of aliases, and a list of IP addresses,\\n    for a host.  The host argument is a string giving a host name or IP number.\\"\\"\\"\\n    pass\\n\\ndef gethostbyname(*args,**kw):\\n    \\"\\"\\"gethostbyname(host) -\\u003e address    \\n    Return the IP address (a string of the form \'255.255.255.255\') for a host.\\"\\"\\"\\n    pass\\n\\ndef gethostbyname_ex(*args,**kw):\\n    \\"\\"\\"gethostbyname_ex(host) -\\u003e (name, aliaslist, addresslist)    \\n    Return the true host name, a list of aliases, and a list of IP addresses,\\n    for a host.  The host argument is a string giving a host name or IP number.\\"\\"\\"\\n    pass\\n\\ndef gethostname(*args,**kw):\\n    \\"\\"\\"gethostname() -\\u003e string    \\n    Return the current host name.\\"\\"\\"\\n    pass\\n\\ndef getnameinfo(*args,**kw):\\n    \\"\\"\\"getnameinfo(sockaddr, flags) --\\u003e (host, port)    \\n    Get host and port for a sockaddr.\\"\\"\\"\\n    pass\\n\\ndef getprotobyname(*args,**kw):\\n    \\"\\"\\"getprotobyname(name) -\\u003e integer    \\n    Return the protocol number for the named protocol.  (Rarely used.)\\"\\"\\"\\n    pass\\n\\ndef getservbyname(*args,**kw):\\n    \\"\\"\\"getservbyname(servicename[, protocolname]) -\\u003e integer    \\n    Return a port number from a service name and protocol name.\\n    The optional protocol name, if given, should be \'tcp\' or \'udp\',\\n    otherwise any protocol will match.\\"\\"\\"\\n    pass\\n\\ndef getservbyport(*args,**kw):\\n    \\"\\"\\"getservbyport(port[, protocolname]) -\\u003e string    \\n    Return the service name from a port number and protocol name.\\n    The optional protocol name, if given, should be \'tcp\' or \'udp\',\\n    otherwise any protocol will match.\\"\\"\\"\\n    pass\\n\\nhas_ipv6 = True\\n\\nclass herror:\\n    pass\\n\\ndef htonl(*args,**kw):\\n    \\"\\"\\"htonl(integer) -\\u003e integer    \\n    Convert a 32-bit integer from host to network byte order.\\"\\"\\"\\n    pass\\n\\ndef htons(*args,**kw):\\n    \\"\\"\\"htons(integer) -\\u003e integer    \\n    Convert a 16-bit integer from host to network byte order.\\"\\"\\"\\n    pass\\n\\ndef inet_aton(*args,**kw):\\n    \\"\\"\\"inet_aton(string) -\\u003e bytes giving packed 32-bit IP representation    \\n    Convert an IP address in string format (123.45.67.89) to the 32-bit packed\\n    binary format used in low-level network functions.\\"\\"\\"\\n    pass\\n\\ndef inet_ntoa(*args,**kw):\\n    \\"\\"\\"inet_ntoa(packed_ip) -\\u003e ip_address_string    \\n    Convert an IP address from 32-bit packed binary format to string format\\"\\"\\"\\n    pass\\n\\ndef ntohl(*args,**kw):\\n    \\"\\"\\"ntohl(integer) -\\u003e integer    \\n    Convert a 32-bit integer from network to host byte order.\\"\\"\\"\\n    pass\\n\\ndef ntohs(*args,**kw):\\n    \\"\\"\\"ntohs(integer) -\\u003e integer    \\n    Convert a 16-bit integer from network to host byte order.\\"\\"\\"\\n    pass\\n\\ndef setdefaulttimeout(*args,**kw):\\n    \\"\\"\\"setdefaulttimeout(timeout)    \\n    Set the default timeout in seconds (float) for new socket objects.\\n    A value of None indicates that new socket objects have no timeout.\\n    When the socket module is first imported, the default is None.\\"\\"\\"\\n    pass\\n\\nclass socket:\\n    def __init__(self,*args,**kw):\\n        pass\\n    def bind(self,*args,**kw):\\n        pass\\n    def close(self):\\n        pass\\n\\nclass timeout:\\n    pass\\n"}\n'
line: b'{"repo_name":"indevgr/django","ref":"refs/heads/master","path":"tests/servers/tests.py","content":"# -*- encoding: utf-8 -*-\\n\\"\\"\\"\\nTests for django.core.servers.\\n\\"\\"\\"\\nfrom __future__ import unicode_literals\\n\\nimport contextlib\\nimport errno\\nimport os\\nimport socket\\n\\nfrom django.core.exceptions import ImproperlyConfigured\\nfrom django.test import LiveServerTestCase, override_settings\\nfrom django.utils._os import upath\\nfrom django.utils.http import urlencode\\nfrom django.utils.six import text_type\\nfrom django.utils.six.moves.urllib.error import HTTPError\\nfrom django.utils.six.moves.urllib.request import urlopen\\n\\nfrom .models import Person\\n\\nTEST_ROOT = os.path.dirname(upath(__file__))\\nTEST_SETTINGS = {\\n    \'MEDIA_URL\': \'/media/\',\\n    \'MEDIA_ROOT\': os.path.join(TEST_ROOT, \'media\'),\\n    \'STATIC_URL\': \'/static/\',\\n    \'STATIC_ROOT\': os.path.join(TEST_ROOT, \'static\'),\\n}\\n\\n\\n@override_settings(ROOT_URLCONF=\'servers.urls\', **TEST_SETTINGS)\\nclass LiveServerBase(LiveServerTestCase):\\n\\n    available_apps = [\\n        \'servers\',\\n        \'django.contrib.auth\',\\n        \'django.contrib.contenttypes\',\\n        \'django.contrib.sessions\',\\n    ]\\n    fixtures = [\'testdata.json\']\\n\\n    def urlopen(self, url):\\n        return urlopen(self.live_server_url + url)\\n\\n\\nclass LiveServerAddress(LiveServerBase):\\n    \\"\\"\\"\\n    Ensure that the address set in the environment variable is valid.\\n    Refs #2879.\\n    \\"\\"\\"\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        # Backup original environment variable\\n        address_predefined = \'DJANGO_LIVE_TEST_SERVER_ADDRESS\' in os.environ\\n        old_address = os.environ.get(\'DJANGO_LIVE_TEST_SERVER_ADDRESS\')\\n\\n        # Just the host is not accepted\\n        cls.raises_exception(\'localhost\', ImproperlyConfigured)\\n\\n        # The host must be valid\\n        cls.raises_exception(\'blahblahblah:8081\', socket.error)\\n\\n        # The list of ports must be in a valid format\\n        cls.raises_exception(\'localhost:8081,\', ImproperlyConfigured)\\n        cls.raises_exception(\'localhost:8081,blah\', ImproperlyConfigured)\\n        cls.raises_exception(\'localhost:8081-\', ImproperlyConfigured)\\n        cls.raises_exception(\'localhost:8081-blah\', ImproperlyConfigured)\\n        cls.raises_exception(\'localhost:8081-8082-8083\', ImproperlyConfigured)\\n\\n        # Restore original environment variable\\n        if address_predefined:\\n            os.environ[\'DJANGO_LIVE_TEST_SERVER_ADDRESS\'] = old_address\\n        else:\\n            del os.environ[\'DJANGO_LIVE_TEST_SERVER_ADDRESS\']\\n\\n        # put it in a list to prevent descriptor lookups in test\\n        cls.live_server_url_test = [cls.live_server_url]\\n\\n    @classmethod\\n    def tearDownClass(cls):\\n        # skip it, as setUpClass doesn\'t call its parent either\\n        pass\\n\\n    @classmethod\\n    def raises_exception(cls, address, exception):\\n        os.environ[\'DJANGO_LIVE_TEST_SERVER_ADDRESS\'] = address\\n        try:\\n            super(LiveServerAddress, cls).setUpClass()\\n            raise Exception(\\"The line above should have raised an exception\\")\\n        except exception:\\n            pass\\n        finally:\\n            super(LiveServerAddress, cls).tearDownClass()\\n\\n    def test_live_server_url_is_class_property(self):\\n        self.assertIsInstance(self.live_server_url_test[0], text_type)\\n        self.assertEqual(self.live_server_url_test[0], self.live_server_url)\\n\\n\\nclass LiveServerViews(LiveServerBase):\\n    def test_404(self):\\n        \\"\\"\\"\\n        Ensure that the LiveServerTestCase serves 404s.\\n        Refs #2879.\\n        \\"\\"\\"\\n        try:\\n            self.urlopen(\'/\')\\n        except HTTPError as err:\\n            self.assertEqual(err.code, 404, \'Expected 404 response\')\\n        else:\\n            self.fail(\'Expected 404 response\')\\n\\n    def test_view(self):\\n        \\"\\"\\"\\n        Ensure that the LiveServerTestCase serves views.\\n        Refs #2879.\\n        \\"\\"\\"\\n        with contextlib.closing(self.urlopen(\'/example_view/\')) as f:\\n            self.assertEqual(f.read(), b\'example view\')\\n\\n    def test_static_files(self):\\n        \\"\\"\\"\\n        Ensure that the LiveServerTestCase serves static files.\\n        Refs #2879.\\n        \\"\\"\\"\\n        with contextlib.closing(self.urlopen(\'/static/example_static_file.txt\')) as f:\\n            self.assertEqual(f.read().rstrip(b\'\\\\r\\\\n\'), b\'example static file\')\\n\\n    def test_no_collectstatic_emulation(self):\\n        \\"\\"\\"\\n        Test that LiveServerTestCase reports a 404 status code when HTTP client\\n        tries to access a static file that isn\'t explicitly put under\\n        STATIC_ROOT.\\n        \\"\\"\\"\\n        try:\\n            self.urlopen(\'/static/another_app/another_app_static_file.txt\')\\n        except HTTPError as err:\\n            self.assertEqual(err.code, 404, \'Expected 404 response\')\\n        else:\\n            self.fail(\'Expected 404 response (got %d)\' % err.code)\\n\\n    def test_media_files(self):\\n        \\"\\"\\"\\n        Ensure that the LiveServerTestCase serves media files.\\n        Refs #2879.\\n        \\"\\"\\"\\n        with contextlib.closing(self.urlopen(\'/media/example_media_file.txt\')) as f:\\n            self.assertEqual(f.read().rstrip(b\'\\\\r\\\\n\'), b\'example media file\')\\n\\n    def test_environ(self):\\n        with contextlib.closing(self.urlopen(\'/environ_view/?%s\' % urlencode({\'q\': \'\xd1\x82\xd0\xb5\xd1\x81\xd1\x82\'}))) as f:\\n            self.assertIn(b\\"QUERY_STRING: \'q=%D1%82%D0%B5%D1%81%D1%82\'\\", f.read())\\n\\n\\nclass LiveServerDatabase(LiveServerBase):\\n\\n    def test_fixtures_loaded(self):\\n        \\"\\"\\"\\n        Ensure that fixtures are properly loaded and visible to the\\n        live server thread.\\n        Refs #2879.\\n        \\"\\"\\"\\n        with contextlib.closing(self.urlopen(\'/model_view/\')) as f:\\n            self.assertEqual(f.read().splitlines(), [b\'jane\', b\'robert\'])\\n\\n    def test_database_writes(self):\\n        \\"\\"\\"\\n        Ensure that data written to the database by a view can be read.\\n        Refs #2879.\\n        \\"\\"\\"\\n        self.urlopen(\'/create_model_instance/\')\\n        self.assertQuerysetEqual(\\n            Person.objects.all().order_by(\'pk\'),\\n            [\'jane\', \'robert\', \'emily\'],\\n            lambda b: b.name\\n        )\\n\\n\\nclass LiveServerPort(LiveServerBase):\\n\\n    def test_port_bind(self):\\n        \\"\\"\\"\\n        Each LiveServerTestCase binds to a unique port or fails to start a\\n        server thread when run concurrently (#26011).\\n        \\"\\"\\"\\n        TestCase = type(str(\\"TestCase\\"), (LiveServerBase,), {})\\n        try:\\n            TestCase.setUpClass()\\n        except socket.error as e:\\n            if e.errno == errno.EADDRINUSE:\\n                # We\'re out of ports, LiveServerTestCase correctly fails with\\n                # a socket error.\\n                return\\n            # Unexpected error.\\n            raise\\n        try:\\n            # We\'ve acquired a port, ensure our server threads acquired\\n            # different addresses.\\n            self.assertNotEqual(\\n                self.live_server_url, TestCase.live_server_url,\\n                \\"Acquired duplicate server addresses for server threads: %s\\" % self.live_server_url\\n            )\\n        finally:\\n            TestCase.tearDownClass()\\n"}\n'
line: b'{"repo_name":"bdupharm/sqlalchemy","ref":"refs/heads/master","path":"lib/sqlalchemy/ext/declarative/api.py","content":"# ext/declarative/api.py\\n# Copyright (C) 2005-2016 the SQLAlchemy authors and contributors\\n# \\u003csee AUTHORS file\\u003e\\n#\\n# This module is part of SQLAlchemy and is released under\\n# the MIT License: http://www.opensource.org/licenses/mit-license.php\\n\\"\\"\\"Public API functions and helpers for declarative.\\"\\"\\"\\n\\n\\nfrom ...schema import Table, MetaData, Column\\nfrom ...orm import synonym as _orm_synonym, \\\\\\n    comparable_property,\\\\\\n    interfaces, properties, attributes\\nfrom ...orm.util import polymorphic_union\\nfrom ...orm.base import _mapper_or_none\\nfrom ...util import OrderedDict, hybridmethod, hybridproperty\\nfrom ... import util\\nfrom ... import exc\\nimport weakref\\n\\nfrom .base import _as_declarative, \\\\\\n    _declarative_constructor,\\\\\\n    _DeferredMapperConfig, _add_attribute\\nfrom .clsregistry import _class_resolver\\n\\n\\ndef instrument_declarative(cls, registry, metadata):\\n    \\"\\"\\"Given a class, configure the class declaratively,\\n    using the given registry, which can be any dictionary, and\\n    MetaData object.\\n\\n    \\"\\"\\"\\n    if \'_decl_class_registry\' in cls.__dict__:\\n        raise exc.InvalidRequestError(\\n            \\"Class %r already has been \\"\\n            \\"instrumented declaratively\\" % cls)\\n    cls._decl_class_registry = registry\\n    cls.metadata = metadata\\n    _as_declarative(cls, cls.__name__, cls.__dict__)\\n\\n\\ndef has_inherited_table(cls):\\n    \\"\\"\\"Given a class, return True if any of the classes it inherits from has a\\n    mapped table, otherwise return False.\\n    \\"\\"\\"\\n    for class_ in cls.__mro__[1:]:\\n        if getattr(class_, \'__table__\', None) is not None:\\n            return True\\n    return False\\n\\n\\nclass DeclarativeMeta(type):\\n    def __init__(cls, classname, bases, dict_):\\n        if \'_decl_class_registry\' not in cls.__dict__:\\n            _as_declarative(cls, classname, cls.__dict__)\\n        type.__init__(cls, classname, bases, dict_)\\n\\n    def __setattr__(cls, key, value):\\n        _add_attribute(cls, key, value)\\n\\n\\ndef synonym_for(name, map_column=False):\\n    \\"\\"\\"Decorator, make a Python @property a query synonym for a column.\\n\\n    A decorator version of :func:`~sqlalchemy.orm.synonym`. The function being\\n    decorated is the \'descriptor\', otherwise passes its arguments through to\\n    synonym()::\\n\\n      @synonym_for(\'col\')\\n      @property\\n      def prop(self):\\n          return \'special sauce\'\\n\\n    The regular ``synonym()`` is also usable directly in a declarative setting\\n    and may be convenient for read/write properties::\\n\\n      prop = synonym(\'col\', descriptor=property(_read_prop, _write_prop))\\n\\n    \\"\\"\\"\\n    def decorate(fn):\\n        return _orm_synonym(name, map_column=map_column, descriptor=fn)\\n    return decorate\\n\\n\\ndef comparable_using(comparator_factory):\\n    \\"\\"\\"Decorator, allow a Python @property to be used in query criteria.\\n\\n    This is a  decorator front end to\\n    :func:`~sqlalchemy.orm.comparable_property` that passes\\n    through the comparator_factory and the function being decorated::\\n\\n      @comparable_using(MyComparatorType)\\n      @property\\n      def prop(self):\\n          return \'special sauce\'\\n\\n    The regular ``comparable_property()`` is also usable directly in a\\n    declarative setting and may be convenient for read/write properties::\\n\\n      prop = comparable_property(MyComparatorType)\\n\\n    \\"\\"\\"\\n    def decorate(fn):\\n        return comparable_property(comparator_factory, fn)\\n    return decorate\\n\\n\\nclass declared_attr(interfaces._MappedAttribute, property):\\n    \\"\\"\\"Mark a class-level method as representing the definition of\\n    a mapped property or special declarative member name.\\n\\n    @declared_attr turns the attribute into a scalar-like\\n    property that can be invoked from the uninstantiated class.\\n    Declarative treats attributes specifically marked with\\n    @declared_attr as returning a construct that is specific\\n    to mapping or declarative table configuration.  The name\\n    of the attribute is that of what the non-dynamic version\\n    of the attribute would be.\\n\\n    @declared_attr is more often than not applicable to mixins,\\n    to define relationships that are to be applied to different\\n    implementors of the class::\\n\\n        class ProvidesUser(object):\\n            \\"A mixin that adds a \'user\' relationship to classes.\\"\\n\\n            @declared_attr\\n            def user(self):\\n                return relationship(\\"User\\")\\n\\n    It also can be applied to mapped classes, such as to provide\\n    a \\"polymorphic\\" scheme for inheritance::\\n\\n        class Employee(Base):\\n            id = Column(Integer, primary_key=True)\\n            type = Column(String(50), nullable=False)\\n\\n            @declared_attr\\n            def __tablename__(cls):\\n                return cls.__name__.lower()\\n\\n            @declared_attr\\n            def __mapper_args__(cls):\\n                if cls.__name__ == \'Employee\':\\n                    return {\\n                            \\"polymorphic_on\\":cls.type,\\n                            \\"polymorphic_identity\\":\\"Employee\\"\\n                    }\\n                else:\\n                    return {\\"polymorphic_identity\\":cls.__name__}\\n\\n    .. versionchanged:: 0.8 :class:`.declared_attr` can be used with\\n       non-ORM or extension attributes, such as user-defined attributes\\n       or :func:`.association_proxy` objects, which will be assigned\\n       to the class at class construction time.\\n\\n\\n    \\"\\"\\"\\n\\n    def __init__(self, fget, cascading=False):\\n        super(declared_attr, self).__init__(fget)\\n        self.__doc__ = fget.__doc__\\n        self._cascading = cascading\\n\\n    def __get__(desc, self, cls):\\n        reg = cls.__dict__.get(\'_sa_declared_attr_reg\', None)\\n        if reg is None:\\n            manager = attributes.manager_of_class(cls)\\n            if manager is None:\\n                util.warn(\\n                    \\"Unmanaged access of declarative attribute %s from \\"\\n                    \\"non-mapped class %s\\" %\\n                    (desc.fget.__name__, cls.__name__))\\n            return desc.fget(cls)\\n\\n        if reg is None:\\n            return desc.fget(cls)\\n        elif desc in reg:\\n            return reg[desc]\\n        else:\\n            reg[desc] = obj = desc.fget(cls)\\n            return obj\\n\\n    @hybridmethod\\n    def _stateful(cls, **kw):\\n        return _stateful_declared_attr(**kw)\\n\\n    @hybridproperty\\n    def cascading(cls):\\n        \\"\\"\\"Mark a :class:`.declared_attr` as cascading.\\n\\n        This is a special-use modifier which indicates that a column\\n        or MapperProperty-based declared attribute should be configured\\n        distinctly per mapped subclass, within a mapped-inheritance scenario.\\n\\n        Below, both MyClass as well as MySubClass will have a distinct\\n        ``id`` Column object established::\\n\\n            class HasSomeAttribute(object):\\n                @declared_attr.cascading\\n                def some_id(cls):\\n                    if has_inherited_table(cls):\\n                        return Column(\\n                            ForeignKey(\'myclass.id\'), primary_key=True)\\n                    else:\\n                        return Column(Integer, primary_key=True)\\n\\n                    return Column(\'id\', Integer, primary_key=True)\\n\\n            class MyClass(HasSomeAttribute, Base):\\n                \\"\\"\\n                # ...\\n\\n            class MySubClass(MyClass):\\n                \\"\\"\\n                # ...\\n\\n        The behavior of the above configuration is that ``MySubClass``\\n        will refer to both its own ``id`` column as well as that of\\n        ``MyClass`` underneath the attribute named ``some_id``.\\n\\n        .. seealso::\\n\\n            :ref:`declarative_inheritance`\\n\\n            :ref:`mixin_inheritance_columns`\\n\\n\\n        \\"\\"\\"\\n        return cls._stateful(cascading=True)\\n\\n\\nclass _stateful_declared_attr(declared_attr):\\n    def __init__(self, **kw):\\n        self.kw = kw\\n\\n    def _stateful(self, **kw):\\n        new_kw = self.kw.copy()\\n        new_kw.update(kw)\\n        return _stateful_declared_attr(**new_kw)\\n\\n    def __call__(self, fn):\\n        return declared_attr(fn, **self.kw)\\n\\n\\ndef declarative_base(bind=None, metadata=None, mapper=None, cls=object,\\n                     name=\'Base\', constructor=_declarative_constructor,\\n                     class_registry=None,\\n                     metaclass=DeclarativeMeta):\\n    \\"\\"\\"Construct a base class for declarative class definitions.\\n\\n    The new base class will be given a metaclass that produces\\n    appropriate :class:`~sqlalchemy.schema.Table` objects and makes\\n    the appropriate :func:`~sqlalchemy.orm.mapper` calls based on the\\n    information provided declaratively in the class and any subclasses\\n    of the class.\\n\\n    :param bind: An optional\\n      :class:`~sqlalchemy.engine.Connectable`, will be assigned\\n      the ``bind`` attribute on the :class:`~sqlalchemy.schema.MetaData`\\n      instance.\\n\\n    :param metadata:\\n      An optional :class:`~sqlalchemy.schema.MetaData` instance.  All\\n      :class:`~sqlalchemy.schema.Table` objects implicitly declared by\\n      subclasses of the base will share this MetaData.  A MetaData instance\\n      will be created if none is provided.  The\\n      :class:`~sqlalchemy.schema.MetaData` instance will be available via the\\n      `metadata` attribute of the generated declarative base class.\\n\\n    :param mapper:\\n      An optional callable, defaults to :func:`~sqlalchemy.orm.mapper`. Will\\n      be used to map subclasses to their Tables.\\n\\n    :param cls:\\n      Defaults to :class:`object`. A type to use as the base for the generated\\n      declarative base class. May be a class or tuple of classes.\\n\\n    :param name:\\n      Defaults to ``Base``.  The display name for the generated\\n      class.  Customizing this is not required, but can improve clarity in\\n      tracebacks and debugging.\\n\\n    :param constructor:\\n      Defaults to\\n      :func:`~sqlalchemy.ext.declarative._declarative_constructor`, an\\n      __init__ implementation that assigns \\\\**kwargs for declared\\n      fields and relationships to an instance.  If ``None`` is supplied,\\n      no __init__ will be provided and construction will fall back to\\n      cls.__init__ by way of the normal Python semantics.\\n\\n    :param class_registry: optional dictionary that will serve as the\\n      registry of class names-\\u003e mapped classes when string names\\n      are used to identify classes inside of :func:`.relationship`\\n      and others.  Allows two or more declarative base classes\\n      to share the same registry of class names for simplified\\n      inter-base relationships.\\n\\n    :param metaclass:\\n      Defaults to :class:`.DeclarativeMeta`.  A metaclass or __metaclass__\\n      compatible callable to use as the meta type of the generated\\n      declarative base class.\\n\\n    .. seealso::\\n\\n        :func:`.as_declarative`\\n\\n    \\"\\"\\"\\n    lcl_metadata = metadata or MetaData()\\n    if bind:\\n        lcl_metadata.bind = bind\\n\\n    if class_registry is None:\\n        class_registry = weakref.WeakValueDictionary()\\n\\n    bases = not isinstance(cls, tuple) and (cls,) or cls\\n    class_dict = dict(_decl_class_registry=class_registry,\\n                      metadata=lcl_metadata)\\n\\n    if constructor:\\n        class_dict[\'__init__\'] = constructor\\n    if mapper:\\n        class_dict[\'__mapper_cls__\'] = mapper\\n\\n    return metaclass(name, bases, class_dict)\\n\\n\\ndef as_declarative(**kw):\\n    \\"\\"\\"\\n    Class decorator for :func:`.declarative_base`.\\n\\n    Provides a syntactical shortcut to the ``cls`` argument\\n    sent to :func:`.declarative_base`, allowing the base class\\n    to be converted in-place to a \\"declarative\\" base::\\n\\n        from sqlalchemy.ext.declarative import as_declarative\\n\\n        @as_declarative()\\n        class Base(object):\\n            @declared_attr\\n            def __tablename__(cls):\\n                return cls.__name__.lower()\\n            id = Column(Integer, primary_key=True)\\n\\n        class MyMappedClass(Base):\\n            # ...\\n\\n    All keyword arguments passed to :func:`.as_declarative` are passed\\n    along to :func:`.declarative_base`.\\n\\n    .. versionadded:: 0.8.3\\n\\n    .. seealso::\\n\\n        :func:`.declarative_base`\\n\\n    \\"\\"\\"\\n    def decorate(cls):\\n        kw[\'cls\'] = cls\\n        kw[\'name\'] = cls.__name__\\n        return declarative_base(**kw)\\n\\n    return decorate\\n\\n\\nclass ConcreteBase(object):\\n    \\"\\"\\"A helper class for \'concrete\' declarative mappings.\\n\\n    :class:`.ConcreteBase` will use the :func:`.polymorphic_union`\\n    function automatically, against all tables mapped as a subclass\\n    to this class.   The function is called via the\\n    ``__declare_last__()`` function, which is essentially\\n    a hook for the :meth:`.after_configured` event.\\n\\n    :class:`.ConcreteBase` produces a mapped\\n    table for the class itself.  Compare to :class:`.AbstractConcreteBase`,\\n    which does not.\\n\\n    Example::\\n\\n        from sqlalchemy.ext.declarative import ConcreteBase\\n\\n        class Employee(ConcreteBase, Base):\\n            __tablename__ = \'employee\'\\n            employee_id = Column(Integer, primary_key=True)\\n            name = Column(String(50))\\n            __mapper_args__ = {\\n                            \'polymorphic_identity\':\'employee\',\\n                            \'concrete\':True}\\n\\n        class Manager(Employee):\\n            __tablename__ = \'manager\'\\n            employee_id = Column(Integer, primary_key=True)\\n            name = Column(String(50))\\n            manager_data = Column(String(40))\\n            __mapper_args__ = {\\n                            \'polymorphic_identity\':\'manager\',\\n                            \'concrete\':True}\\n\\n    .. seealso::\\n\\n        :class:`.AbstractConcreteBase`\\n\\n        :ref:`concrete_inheritance`\\n\\n        :ref:`inheritance_concrete_helpers`\\n\\n\\n    \\"\\"\\"\\n\\n    @classmethod\\n    def _create_polymorphic_union(cls, mappers):\\n        return polymorphic_union(OrderedDict(\\n            (mp.polymorphic_identity, mp.local_table)\\n            for mp in mappers\\n        ), \'type\', \'pjoin\')\\n\\n    @classmethod\\n    def __declare_first__(cls):\\n        m = cls.__mapper__\\n        if m.with_polymorphic:\\n            return\\n\\n        mappers = list(m.self_and_descendants)\\n        pjoin = cls._create_polymorphic_union(mappers)\\n        m._set_with_polymorphic((\\"*\\", pjoin))\\n        m._set_polymorphic_on(pjoin.c.type)\\n\\n\\nclass AbstractConcreteBase(ConcreteBase):\\n    \\"\\"\\"A helper class for \'concrete\' declarative mappings.\\n\\n    :class:`.AbstractConcreteBase` will use the :func:`.polymorphic_union`\\n    function automatically, against all tables mapped as a subclass\\n    to this class.   The function is called via the\\n    ``__declare_last__()`` function, which is essentially\\n    a hook for the :meth:`.after_configured` event.\\n\\n    :class:`.AbstractConcreteBase` does produce a mapped class\\n    for the base class, however it is not persisted to any table; it\\n    is instead mapped directly to the \\"polymorphic\\" selectable directly\\n    and is only used for selecting.  Compare to :class:`.ConcreteBase`,\\n    which does create a persisted table for the base class.\\n\\n    Example::\\n\\n        from sqlalchemy.ext.declarative import AbstractConcreteBase\\n\\n        class Employee(AbstractConcreteBase, Base):\\n            pass\\n\\n        class Manager(Employee):\\n            __tablename__ = \'manager\'\\n            employee_id = Column(Integer, primary_key=True)\\n            name = Column(String(50))\\n            manager_data = Column(String(40))\\n\\n            __mapper_args__ = {\\n                \'polymorphic_identity\':\'manager\',\\n                \'concrete\':True}\\n\\n    The abstract base class is handled by declarative in a special way;\\n    at class configuration time, it behaves like a declarative mixin\\n    or an ``__abstract__`` base class.   Once classes are configured\\n    and mappings are produced, it then gets mapped itself, but\\n    after all of its decscendants.  This is a very unique system of mapping\\n    not found in any other SQLAlchemy system.\\n\\n    Using this approach, we can specify columns and properties\\n    that will take place on mapped subclasses, in the way that\\n    we normally do as in :ref:`declarative_mixins`::\\n\\n        class Company(Base):\\n            __tablename__ = \'company\'\\n            id = Column(Integer, primary_key=True)\\n\\n        class Employee(AbstractConcreteBase, Base):\\n            employee_id = Column(Integer, primary_key=True)\\n\\n            @declared_attr\\n            def company_id(cls):\\n                return Column(ForeignKey(\'company.id\'))\\n\\n            @declared_attr\\n            def company(cls):\\n                return relationship(\\"Company\\")\\n\\n        class Manager(Employee):\\n            __tablename__ = \'manager\'\\n\\n            name = Column(String(50))\\n            manager_data = Column(String(40))\\n\\n            __mapper_args__ = {\\n                \'polymorphic_identity\':\'manager\',\\n                \'concrete\':True}\\n\\n    When we make use of our mappings however, both ``Manager`` and\\n    ``Employee`` will have an independently usable ``.company`` attribute::\\n\\n        session.query(Employee).filter(Employee.company.has(id=5))\\n\\n    .. versionchanged:: 1.0.0 - The mechanics of :class:`.AbstractConcreteBase`\\n       have been reworked to support relationships established directly\\n       on the abstract base, without any special configurational steps.\\n\\n    .. seealso::\\n\\n        :class:`.ConcreteBase`\\n\\n        :ref:`concrete_inheritance`\\n\\n        :ref:`inheritance_concrete_helpers`\\n\\n    \\"\\"\\"\\n\\n    __no_table__ = True\\n\\n    @classmethod\\n    def __declare_first__(cls):\\n        cls._sa_decl_prepare_nocascade()\\n\\n    @classmethod\\n    def _sa_decl_prepare_nocascade(cls):\\n        if getattr(cls, \'__mapper__\', None):\\n            return\\n\\n        to_map = _DeferredMapperConfig.config_for_cls(cls)\\n\\n        # can\'t rely on \'self_and_descendants\' here\\n        # since technically an immediate subclass\\n        # might not be mapped, but a subclass\\n        # may be.\\n        mappers = []\\n        stack = list(cls.__subclasses__())\\n        while stack:\\n            klass = stack.pop()\\n            stack.extend(klass.__subclasses__())\\n            mn = _mapper_or_none(klass)\\n            if mn is not None:\\n                mappers.append(mn)\\n        pjoin = cls._create_polymorphic_union(mappers)\\n\\n        # For columns that were declared on the class, these\\n        # are normally ignored with the \\"__no_table__\\" mapping,\\n        # unless they have a different attribute key vs. col name\\n        # and are in the properties argument.\\n        # In that case, ensure we update the properties entry\\n        # to the correct column from the pjoin target table.\\n        declared_cols = set(to_map.declared_columns)\\n        for k, v in list(to_map.properties.items()):\\n            if v in declared_cols:\\n                to_map.properties[k] = pjoin.c[v.key]\\n\\n        to_map.local_table = pjoin\\n\\n        m_args = to_map.mapper_args_fn or dict\\n\\n        def mapper_args():\\n            args = m_args()\\n            args[\'polymorphic_on\'] = pjoin.c.type\\n            return args\\n        to_map.mapper_args_fn = mapper_args\\n\\n        m = to_map.map()\\n\\n        for scls in cls.__subclasses__():\\n            sm = _mapper_or_none(scls)\\n            if sm and sm.concrete and cls in scls.__bases__:\\n                sm._set_concrete_base(m)\\n\\n\\nclass DeferredReflection(object):\\n    \\"\\"\\"A helper class for construction of mappings based on\\n    a deferred reflection step.\\n\\n    Normally, declarative can be used with reflection by\\n    setting a :class:`.Table` object using autoload=True\\n    as the ``__table__`` attribute on a declarative class.\\n    The caveat is that the :class:`.Table` must be fully\\n    reflected, or at the very least have a primary key column,\\n    at the point at which a normal declarative mapping is\\n    constructed, meaning the :class:`.Engine` must be available\\n    at class declaration time.\\n\\n    The :class:`.DeferredReflection` mixin moves the construction\\n    of mappers to be at a later point, after a specific\\n    method is called which first reflects all :class:`.Table`\\n    objects created so far.   Classes can define it as such::\\n\\n        from sqlalchemy.ext.declarative import declarative_base\\n        from sqlalchemy.ext.declarative import DeferredReflection\\n        Base = declarative_base()\\n\\n        class MyClass(DeferredReflection, Base):\\n            __tablename__ = \'mytable\'\\n\\n    Above, ``MyClass`` is not yet mapped.   After a series of\\n    classes have been defined in the above fashion, all tables\\n    can be reflected and mappings created using\\n    :meth:`.prepare`::\\n\\n        engine = create_engine(\\"someengine://...\\")\\n        DeferredReflection.prepare(engine)\\n\\n    The :class:`.DeferredReflection` mixin can be applied to individual\\n    classes, used as the base for the declarative base itself,\\n    or used in a custom abstract class.   Using an abstract base\\n    allows that only a subset of classes to be prepared for a\\n    particular prepare step, which is necessary for applications\\n    that use more than one engine.  For example, if an application\\n    has two engines, you might use two bases, and prepare each\\n    separately, e.g.::\\n\\n        class ReflectedOne(DeferredReflection, Base):\\n            __abstract__ = True\\n\\n        class ReflectedTwo(DeferredReflection, Base):\\n            __abstract__ = True\\n\\n        class MyClass(ReflectedOne):\\n            __tablename__ = \'mytable\'\\n\\n        class MyOtherClass(ReflectedOne):\\n            __tablename__ = \'myothertable\'\\n\\n        class YetAnotherClass(ReflectedTwo):\\n            __tablename__ = \'yetanothertable\'\\n\\n        # ... etc.\\n\\n    Above, the class hierarchies for ``ReflectedOne`` and\\n    ``ReflectedTwo`` can be configured separately::\\n\\n        ReflectedOne.prepare(engine_one)\\n        ReflectedTwo.prepare(engine_two)\\n\\n    .. versionadded:: 0.8\\n\\n    \\"\\"\\"\\n    @classmethod\\n    def prepare(cls, engine):\\n        \\"\\"\\"Reflect all :class:`.Table` objects for all current\\n        :class:`.DeferredReflection` subclasses\\"\\"\\"\\n\\n        to_map = _DeferredMapperConfig.classes_for_base(cls)\\n        for thingy in to_map:\\n            cls._sa_decl_prepare(thingy.local_table, engine)\\n            thingy.map()\\n            mapper = thingy.cls.__mapper__\\n            metadata = mapper.class_.metadata\\n            for rel in mapper._props.values():\\n                if isinstance(rel, properties.RelationshipProperty) and \\\\\\n                        rel.secondary is not None:\\n                    if isinstance(rel.secondary, Table):\\n                        cls._reflect_table(rel.secondary, engine)\\n                    elif isinstance(rel.secondary, _class_resolver):\\n                        rel.secondary._resolvers += (\\n                            cls._sa_deferred_table_resolver(engine, metadata),\\n                        )\\n\\n    @classmethod\\n    def _sa_deferred_table_resolver(cls, engine, metadata):\\n        def _resolve(key):\\n            t1 = Table(key, metadata)\\n            cls._reflect_table(t1, engine)\\n            return t1\\n        return _resolve\\n\\n    @classmethod\\n    def _sa_decl_prepare(cls, local_table, engine):\\n        # autoload Table, which is already\\n        # present in the metadata.  This\\n        # will fill in db-loaded columns\\n        # into the existing Table object.\\n        if local_table is not None:\\n            cls._reflect_table(local_table, engine)\\n\\n    @classmethod\\n    def _reflect_table(cls, table, engine):\\n        Table(table.name,\\n              table.metadata,\\n              extend_existing=True,\\n              autoload_replace=False,\\n              autoload=True,\\n              autoload_with=engine,\\n              schema=table.schema)\\n"}\n'
line: b'{"repo_name":"tianweizhang/nova","ref":"refs/heads/v0","path":"nova/cmd/network.py","content":"# Copyright 2010 United States Government as represented by the\\n# Administrator of the National Aeronautics and Space Administration.\\n# All Rights Reserved.\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\n\\"\\"\\"Starter script for Nova Network.\\"\\"\\"\\n\\nimport sys\\nimport traceback\\n\\nfrom oslo.config import cfg\\n\\nfrom nova.conductor import rpcapi as conductor_rpcapi\\nfrom nova import config\\nimport nova.db.api\\nfrom nova import exception\\nfrom nova.i18n import _\\nfrom nova import objects\\nfrom nova.objects import base as objects_base\\nfrom nova.openstack.common import log as logging\\nfrom nova.openstack.common.report import guru_meditation_report as gmr\\nfrom nova import service\\nfrom nova import utils\\nfrom nova import version\\n\\nCONF = cfg.CONF\\nCONF.import_opt(\'network_topic\', \'nova.network.rpcapi\')\\nCONF.import_opt(\'use_local\', \'nova.conductor.api\', group=\'conductor\')\\n\\n\\ndef block_db_access():\\n    class NoDB(object):\\n        def __getattr__(self, attr):\\n            return self\\n\\n        def __call__(self, *args, **kwargs):\\n            stacktrace = \\"\\".join(traceback.format_stack())\\n            LOG = logging.getLogger(\'nova.network\')\\n            LOG.error(_(\'No db access allowed in nova-network: %s\'),\\n                      stacktrace)\\n            raise exception.DBNotAllowed(\'nova-network\')\\n\\n    nova.db.api.IMPL = NoDB()\\n\\n\\ndef main():\\n    config.parse_args(sys.argv)\\n    logging.setup(\\"nova\\")\\n    utils.monkey_patch()\\n    objects.register_all()\\n\\n    gmr.TextGuruMeditation.setup_autorun(version)\\n\\n    if not CONF.conductor.use_local:\\n        block_db_access()\\n        objects_base.NovaObject.indirection_api = \\\\\\n            conductor_rpcapi.ConductorAPI()\\n\\n    server = service.Service.create(binary=\'nova-network\',\\n                                    topic=CONF.network_topic,\\n                                    db_allowed=CONF.conductor.use_local)\\n    service.serve(server)\\n    service.wait()\\n"}\n'
line: b'{"repo_name":"apollo13/ansible","ref":"refs/heads/devel","path":"lib/ansible/modules/cloud/amazon/elasticache.py","content":"#!/usr/bin/python\\n#\\n# Copyright (c) 2017 Ansible Project\\n#\\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\\nANSIBLE_METADATA = {\'metadata_version\': \'1.1\',\\n                    \'status\': [\'preview\'],\\n                    \'supported_by\': \'community\'}\\n\\n\\nDOCUMENTATION = \\"\\"\\"\\n---\\nmodule: elasticache\\nshort_description: Manage cache clusters in Amazon Elasticache.\\ndescription:\\n  - Manage cache clusters in Amazon Elasticache.\\n  - Returns information about the specified cache cluster.\\nversion_added: \\"1.4\\"\\nrequirements: [ boto3 ]\\nauthor: \\"Jim Dalton (@jsdalton)\\"\\noptions:\\n  state:\\n    description:\\n      - C(absent) or C(present) are idempotent actions that will create or destroy a cache cluster as needed. C(rebooted) will reboot the cluster,\\n        resulting in a momentary outage.\\n    choices: [\'present\', \'absent\', \'rebooted\']\\n    required: true\\n  name:\\n    description:\\n      - The cache cluster identifier\\n    required: true\\n  engine:\\n    description:\\n      - Name of the cache engine to be used.\\n    default: memcached\\n    choices: [\'redis\', \'memcached\']\\n  cache_engine_version:\\n    description:\\n      - The version number of the cache engine\\n  node_type:\\n    description:\\n      - The compute and memory capacity of the nodes in the cache cluster\\n    default: cache.m1.small\\n  num_nodes:\\n    description:\\n      - The initial number of cache nodes that the cache cluster will have. Required when state=present.\\n  cache_port:\\n    description:\\n      - The port number on which each of the cache nodes will accept connections\\n  cache_parameter_group:\\n    description:\\n      - The name of the cache parameter group to associate with this cache cluster. If this argument is omitted, the default cache parameter group\\n        for the specified engine will be used.\\n    version_added: \\"2.0\\"\\n    aliases: [ \'parameter_group\' ]\\n  cache_subnet_group:\\n    description:\\n      - The subnet group name to associate with. Only use if inside a vpc. Required if inside a vpc\\n    version_added: \\"2.0\\"\\n  security_group_ids:\\n    description:\\n      - A list of vpc security group names to associate with this cache cluster. Only use if inside a vpc\\n    version_added: \\"1.6\\"\\n  cache_security_groups:\\n    description:\\n      - A list of cache security group names to associate with this cache cluster. Must be an empty list if inside a vpc\\n  zone:\\n    description:\\n      - The EC2 Availability Zone in which the cache cluster will be created\\n  wait:\\n    description:\\n      - Wait for cache cluster result before returning\\n    type: bool\\n    default: \'yes\'\\n  hard_modify:\\n    description:\\n      - Whether to destroy and recreate an existing cache cluster if necessary in order to modify its state\\n    type: bool\\n    default: \'no\'\\nextends_documentation_fragment:\\n    - aws\\n    - ec2\\n\\"\\"\\"\\n\\nEXAMPLES = \\"\\"\\"\\n# Note: None of these examples set aws_access_key, aws_secret_key, or region.\\n# It is assumed that their matching environment variables are set.\\n\\n# Basic example\\n- elasticache:\\n    name: \\"test-please-delete\\"\\n    state: present\\n    engine: memcached\\n    cache_engine_version: 1.4.14\\n    node_type: cache.m1.small\\n    num_nodes: 1\\n    cache_port: 11211\\n    cache_security_groups:\\n      - default\\n    zone: us-east-1d\\n\\n\\n# Ensure cache cluster is gone\\n- elasticache:\\n    name: \\"test-please-delete\\"\\n    state: absent\\n\\n# Reboot cache cluster\\n- elasticache:\\n    name: \\"test-please-delete\\"\\n    state: rebooted\\n\\n\\"\\"\\"\\nfrom time import sleep\\nfrom traceback import format_exc\\nfrom ansible.module_utils.basic import AnsibleModule\\nfrom ansible.module_utils.ec2 import ec2_argument_spec, get_aws_connection_info, boto3_conn, HAS_BOTO3, camel_dict_to_snake_dict\\n\\ntry:\\n    import boto3\\n    import botocore\\nexcept ImportError:\\n    pass  # will be detected by imported HAS_BOTO3\\n\\n\\nclass ElastiCacheManager(object):\\n\\n    \\"\\"\\"Handles elasticache creation and destruction\\"\\"\\"\\n\\n    EXIST_STATUSES = [\'available\', \'creating\', \'rebooting\', \'modifying\']\\n\\n    def __init__(self, module, name, engine, cache_engine_version, node_type,\\n                 num_nodes, cache_port, cache_parameter_group, cache_subnet_group,\\n                 cache_security_groups, security_group_ids, zone, wait,\\n                 hard_modify, region, **aws_connect_kwargs):\\n        self.module = module\\n        self.name = name\\n        self.engine = engine.lower()\\n        self.cache_engine_version = cache_engine_version\\n        self.node_type = node_type\\n        self.num_nodes = num_nodes\\n        self.cache_port = cache_port\\n        self.cache_parameter_group = cache_parameter_group\\n        self.cache_subnet_group = cache_subnet_group\\n        self.cache_security_groups = cache_security_groups\\n        self.security_group_ids = security_group_ids\\n        self.zone = zone\\n        self.wait = wait\\n        self.hard_modify = hard_modify\\n\\n        self.region = region\\n        self.aws_connect_kwargs = aws_connect_kwargs\\n\\n        self.changed = False\\n        self.data = None\\n        self.status = \'gone\'\\n        self.conn = self._get_elasticache_connection()\\n        self._refresh_data()\\n\\n    def ensure_present(self):\\n        \\"\\"\\"Ensure cache cluster exists or create it if not\\"\\"\\"\\n        if self.exists():\\n            self.sync()\\n        else:\\n            self.create()\\n\\n    def ensure_absent(self):\\n        \\"\\"\\"Ensure cache cluster is gone or delete it if not\\"\\"\\"\\n        self.delete()\\n\\n    def ensure_rebooted(self):\\n        \\"\\"\\"Ensure cache cluster is gone or delete it if not\\"\\"\\"\\n        self.reboot()\\n\\n    def exists(self):\\n        \\"\\"\\"Check if cache cluster exists\\"\\"\\"\\n        return self.status in self.EXIST_STATUSES\\n\\n    def create(self):\\n        \\"\\"\\"Create an ElastiCache cluster\\"\\"\\"\\n        if self.status == \'available\':\\n            return\\n        if self.status in [\'creating\', \'rebooting\', \'modifying\']:\\n            if self.wait:\\n                self._wait_for_status(\'available\')\\n            return\\n        if self.status == \'deleting\':\\n            if self.wait:\\n                self._wait_for_status(\'gone\')\\n            else:\\n                msg = \\"\'%s\' is currently deleting. Cannot create.\\"\\n                self.module.fail_json(msg=msg % self.name)\\n\\n        kwargs = dict(CacheClusterId=self.name,\\n                      NumCacheNodes=self.num_nodes,\\n                      CacheNodeType=self.node_type,\\n                      Engine=self.engine,\\n                      EngineVersion=self.cache_engine_version,\\n                      CacheSecurityGroupNames=self.cache_security_groups,\\n                      SecurityGroupIds=self.security_group_ids,\\n                      CacheParameterGroupName=self.cache_parameter_group,\\n                      CacheSubnetGroupName=self.cache_subnet_group)\\n        if self.cache_port is not None:\\n            kwargs[\'Port\'] = self.cache_port\\n        if self.zone is not None:\\n            kwargs[\'PreferredAvailabilityZone\'] = self.zone\\n\\n        try:\\n            self.conn.create_cache_cluster(**kwargs)\\n\\n        except botocore.exceptions.ClientError as e:\\n            self.module.fail_json(msg=e.message, exception=format_exc(),\\n                                  **camel_dict_to_snake_dict(e.response))\\n\\n        self._refresh_data()\\n\\n        self.changed = True\\n        if self.wait:\\n            self._wait_for_status(\'available\')\\n        return True\\n\\n    def delete(self):\\n        \\"\\"\\"Destroy an ElastiCache cluster\\"\\"\\"\\n        if self.status == \'gone\':\\n            return\\n        if self.status == \'deleting\':\\n            if self.wait:\\n                self._wait_for_status(\'gone\')\\n            return\\n        if self.status in [\'creating\', \'rebooting\', \'modifying\']:\\n            if self.wait:\\n                self._wait_for_status(\'available\')\\n            else:\\n                msg = \\"\'%s\' is currently %s. Cannot delete.\\"\\n                self.module.fail_json(msg=msg % (self.name, self.status))\\n\\n        try:\\n            response = self.conn.delete_cache_cluster(CacheClusterId=self.name)\\n        except botocore.exceptions.ClientError as e:\\n            self.module.fail_json(msg=e.message, exception=format_exc(),\\n                                  **camel_dict_to_snake_dict(e.response))\\n\\n        cache_cluster_data = response[\'CacheCluster\']\\n        self._refresh_data(cache_cluster_data)\\n\\n        self.changed = True\\n        if self.wait:\\n            self._wait_for_status(\'gone\')\\n\\n    def sync(self):\\n        \\"\\"\\"Sync settings to cluster if required\\"\\"\\"\\n        if not self.exists():\\n            msg = \\"\'%s\' is %s. Cannot sync.\\"\\n            self.module.fail_json(msg=msg % (self.name, self.status))\\n\\n        if self.status in [\'creating\', \'rebooting\', \'modifying\']:\\n            if self.wait:\\n                self._wait_for_status(\'available\')\\n            else:\\n                # Cluster can only be synced if available. If we can\'t wait\\n                # for this, then just be done.\\n                return\\n\\n        if self._requires_destroy_and_create():\\n            if not self.hard_modify:\\n                msg = \\"\'%s\' requires destructive modification. \'hard_modify\' must be set to true to proceed.\\"\\n                self.module.fail_json(msg=msg % self.name)\\n            if not self.wait:\\n                msg = \\"\'%s\' requires destructive modification. \'wait\' must be set to true.\\"\\n                self.module.fail_json(msg=msg % self.name)\\n            self.delete()\\n            self.create()\\n            return\\n\\n        if self._requires_modification():\\n            self.modify()\\n\\n    def modify(self):\\n        \\"\\"\\"Modify the cache cluster. Note it\'s only possible to modify a few select options.\\"\\"\\"\\n        nodes_to_remove = self._get_nodes_to_remove()\\n        try:\\n            self.conn.modify_cache_cluster(CacheClusterId=self.name,\\n                                           NumCacheNodes=self.num_nodes,\\n                                           CacheNodeIdsToRemove=nodes_to_remove,\\n                                           CacheSecurityGroupNames=self.cache_security_groups,\\n                                           CacheParameterGroupName=self.cache_parameter_group,\\n                                           SecurityGroupIds=self.security_group_ids,\\n                                           ApplyImmediately=True,\\n                                           EngineVersion=self.cache_engine_version)\\n        except botocore.exceptions.ClientError as e:\\n            self.module.fail_json(msg=e.message, exception=format_exc(),\\n                                  **camel_dict_to_snake_dict(e.response))\\n\\n        self._refresh_data()\\n\\n        self.changed = True\\n        if self.wait:\\n            self._wait_for_status(\'available\')\\n\\n    def reboot(self):\\n        \\"\\"\\"Reboot the cache cluster\\"\\"\\"\\n        if not self.exists():\\n            msg = \\"\'%s\' is %s. Cannot reboot.\\"\\n            self.module.fail_json(msg=msg % (self.name, self.status))\\n        if self.status == \'rebooting\':\\n            return\\n        if self.status in [\'creating\', \'modifying\']:\\n            if self.wait:\\n                self._wait_for_status(\'available\')\\n            else:\\n                msg = \\"\'%s\' is currently %s. Cannot reboot.\\"\\n                self.module.fail_json(msg=msg % (self.name, self.status))\\n\\n        # Collect ALL nodes for reboot\\n        cache_node_ids = [cn[\'CacheNodeId\'] for cn in self.data[\'CacheNodes\']]\\n        try:\\n            self.conn.reboot_cache_cluster(CacheClusterId=self.name,\\n                                           CacheNodeIdsToReboot=cache_node_ids)\\n        except botocore.exceptions.ClientError as e:\\n            self.module.fail_json(msg=e.message, exception=format_exc(),\\n                                  **camel_dict_to_snake_dict(e.response))\\n\\n        self._refresh_data()\\n\\n        self.changed = True\\n        if self.wait:\\n            self._wait_for_status(\'available\')\\n\\n    def get_info(self):\\n        \\"\\"\\"Return basic info about the cache cluster\\"\\"\\"\\n        info = {\\n            \'name\': self.name,\\n            \'status\': self.status\\n        }\\n        if self.data:\\n            info[\'data\'] = self.data\\n        return info\\n\\n    def _wait_for_status(self, awaited_status):\\n        \\"\\"\\"Wait for status to change from present status to awaited_status\\"\\"\\"\\n        status_map = {\\n            \'creating\': \'available\',\\n            \'rebooting\': \'available\',\\n            \'modifying\': \'available\',\\n            \'deleting\': \'gone\'\\n        }\\n        if self.status == awaited_status:\\n            # No need to wait, we\'re already done\\n            return\\n        if status_map[self.status] != awaited_status:\\n            msg = \\"Invalid awaited status. \'%s\' cannot transition to \'%s\'\\"\\n            self.module.fail_json(msg=msg % (self.status, awaited_status))\\n\\n        if awaited_status not in set(status_map.values()):\\n            msg = \\"\'%s\' is not a valid awaited status.\\"\\n            self.module.fail_json(msg=msg % awaited_status)\\n\\n        while True:\\n            sleep(1)\\n            self._refresh_data()\\n            if self.status == awaited_status:\\n                break\\n\\n    def _requires_modification(self):\\n        \\"\\"\\"Check if cluster requires (nondestructive) modification\\"\\"\\"\\n        # Check modifiable data attributes\\n        modifiable_data = {\\n            \'NumCacheNodes\': self.num_nodes,\\n            \'EngineVersion\': self.cache_engine_version\\n        }\\n        for key, value in modifiable_data.items():\\n            if value is not None and value and self.data[key] != value:\\n                return True\\n\\n        # Check cache security groups\\n        cache_security_groups = []\\n        for sg in self.data[\'CacheSecurityGroups\']:\\n            cache_security_groups.append(sg[\'CacheSecurityGroupName\'])\\n        if set(cache_security_groups) != set(self.cache_security_groups):\\n            return True\\n\\n        # check vpc security groups\\n        if self.security_group_ids:\\n            vpc_security_groups = []\\n            security_groups = self.data[\'SecurityGroups\'] or []\\n            for sg in security_groups:\\n                vpc_security_groups.append(sg[\'SecurityGroupId\'])\\n            if set(vpc_security_groups) != set(self.security_group_ids):\\n                return True\\n\\n        return False\\n\\n    def _requires_destroy_and_create(self):\\n        \\"\\"\\"\\n        Check whether a destroy and create is required to synchronize cluster.\\n        \\"\\"\\"\\n        unmodifiable_data = {\\n            \'node_type\': self.data[\'CacheNodeType\'],\\n            \'engine\': self.data[\'Engine\'],\\n            \'cache_port\': self._get_port()\\n        }\\n        # Only check for modifications if zone is specified\\n        if self.zone is not None:\\n            unmodifiable_data[\'zone\'] = self.data[\'PreferredAvailabilityZone\']\\n        for key, value in unmodifiable_data.items():\\n            if getattr(self, key) is not None and getattr(self, key) != value:\\n                return True\\n        return False\\n\\n    def _get_elasticache_connection(self):\\n        \\"\\"\\"Get an elasticache connection\\"\\"\\"\\n        region, ec2_url, aws_connect_params = get_aws_connection_info(self.module, boto3=True)\\n        if region:\\n            return boto3_conn(self.module, conn_type=\'client\', resource=\'elasticache\',\\n                              region=region, endpoint=ec2_url, **aws_connect_params)\\n        else:\\n            self.module.fail_json(msg=\\"region must be specified\\")\\n\\n    def _get_port(self):\\n        \\"\\"\\"Get the port. Where this information is retrieved from is engine dependent.\\"\\"\\"\\n        if self.data[\'Engine\'] == \'memcached\':\\n            return self.data[\'ConfigurationEndpoint\'][\'Port\']\\n        elif self.data[\'Engine\'] == \'redis\':\\n            # Redis only supports a single node (presently) so just use\\n            # the first and only\\n            return self.data[\'CacheNodes\'][0][\'Endpoint\'][\'Port\']\\n\\n    def _refresh_data(self, cache_cluster_data=None):\\n        \\"\\"\\"Refresh data about this cache cluster\\"\\"\\"\\n\\n        if cache_cluster_data is None:\\n            try:\\n                response = self.conn.describe_cache_clusters(CacheClusterId=self.name, ShowCacheNodeInfo=True)\\n            except botocore.exceptions.ClientError as e:\\n                if e.response[\'Error\'][\'Code\'] == \'CacheClusterNotFound\':\\n                    self.data = None\\n                    self.status = \'gone\'\\n                    return\\n                else:\\n                    self.module.fail_json(msg=e.message, exception=format_exc(),\\n                                          **camel_dict_to_snake_dict(e.response))\\n            cache_cluster_data = response[\'CacheClusters\'][0]\\n        self.data = cache_cluster_data\\n        self.status = self.data[\'CacheClusterStatus\']\\n\\n        # The documentation for elasticache lies -- status on rebooting is set\\n        # to \'rebooting cache cluster nodes\' instead of \'rebooting\'. Fix it\\n        # here to make status checks etc. more sane.\\n        if self.status == \'rebooting cache cluster nodes\':\\n            self.status = \'rebooting\'\\n\\n    def _get_nodes_to_remove(self):\\n        \\"\\"\\"If there are nodes to remove, it figures out which need to be removed\\"\\"\\"\\n        num_nodes_to_remove = self.data[\'NumCacheNodes\'] - self.num_nodes\\n        if num_nodes_to_remove \\u003c= 0:\\n            return []\\n\\n        if not self.hard_modify:\\n            msg = \\"\'%s\' requires removal of cache nodes. \'hard_modify\' must be set to true to proceed.\\"\\n            self.module.fail_json(msg=msg % self.name)\\n\\n        cache_node_ids = [cn[\'CacheNodeId\'] for cn in self.data[\'CacheNodes\']]\\n        return cache_node_ids[-num_nodes_to_remove:]\\n\\n\\ndef main():\\n    \\"\\"\\" elasticache ansible module \\"\\"\\"\\n    argument_spec = ec2_argument_spec()\\n    argument_spec.update(dict(\\n        state=dict(required=True, choices=[\'present\', \'absent\', \'rebooted\']),\\n        name=dict(required=True),\\n        engine=dict(default=\'memcached\'),\\n        cache_engine_version=dict(default=\\"\\"),\\n        node_type=dict(default=\'cache.t2.small\'),\\n        num_nodes=dict(default=1, type=\'int\'),\\n        # alias for compat with the original PR 1950\\n        cache_parameter_group=dict(default=\\"\\", aliases=[\'parameter_group\']),\\n        cache_port=dict(type=\'int\'),\\n        cache_subnet_group=dict(default=\\"\\"),\\n        cache_security_groups=dict(default=[], type=\'list\'),\\n        security_group_ids=dict(default=[], type=\'list\'),\\n        zone=dict(),\\n        wait=dict(default=True, type=\'bool\'),\\n        hard_modify=dict(type=\'bool\')\\n    ))\\n\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n    )\\n\\n    if not HAS_BOTO3:\\n        module.fail_json(msg=\'boto3 required for this module\')\\n\\n    region, ec2_url, aws_connect_kwargs = get_aws_connection_info(module)\\n\\n    name = module.params[\'name\']\\n    state = module.params[\'state\']\\n    engine = module.params[\'engine\']\\n    cache_engine_version = module.params[\'cache_engine_version\']\\n    node_type = module.params[\'node_type\']\\n    num_nodes = module.params[\'num_nodes\']\\n    cache_port = module.params[\'cache_port\']\\n    cache_subnet_group = module.params[\'cache_subnet_group\']\\n    cache_security_groups = module.params[\'cache_security_groups\']\\n    security_group_ids = module.params[\'security_group_ids\']\\n    zone = module.params[\'zone\']\\n    wait = module.params[\'wait\']\\n    hard_modify = module.params[\'hard_modify\']\\n    cache_parameter_group = module.params[\'cache_parameter_group\']\\n\\n    if cache_subnet_group and cache_security_groups:\\n        module.fail_json(msg=\\"Can\'t specify both cache_subnet_group and cache_security_groups\\")\\n\\n    if state == \'present\' and not num_nodes:\\n        module.fail_json(msg=\\"\'num_nodes\' is a required parameter. Please specify num_nodes \\u003e 0\\")\\n\\n    elasticache_manager = ElastiCacheManager(module, name, engine,\\n                                             cache_engine_version, node_type,\\n                                             num_nodes, cache_port,\\n                                             cache_parameter_group,\\n                                             cache_subnet_group,\\n                                             cache_security_groups,\\n                                             security_group_ids, zone, wait,\\n                                             hard_modify, region, **aws_connect_kwargs)\\n\\n    if state == \'present\':\\n        elasticache_manager.ensure_present()\\n    elif state == \'absent\':\\n        elasticache_manager.ensure_absent()\\n    elif state == \'rebooted\':\\n        elasticache_manager.ensure_rebooted()\\n\\n    facts_result = dict(changed=elasticache_manager.changed,\\n                        elasticache=elasticache_manager.get_info())\\n\\n    module.exit_json(**facts_result)\\n\\n\\nif __name__ == \'__main__\':\\n    main()\\n"}\n'
line: b'{"repo_name":"jasonseminara/OpenSourceFinal","ref":"refs/heads/master","path":"myvenv/lib/python3.5/site-packages/django/template/smartif.py","content":"\\"\\"\\"\\nParser and utilities for the smart \'if\' tag\\n\\"\\"\\"\\nimport warnings\\n\\nfrom django.utils.deprecation import RemovedInDjango110Warning\\n\\n\\n# Using a simple top down parser, as described here:\\n#    http://effbot.org/zone/simple-top-down-parsing.htm.\\n# \'led\' = left denotation\\n# \'nud\' = null denotation\\n# \'bp\' = binding power (left = lbp, right = rbp)\\n\\nclass TokenBase(object):\\n    \\"\\"\\"\\n    Base class for operators and literals, mainly for debugging and for throwing\\n    syntax errors.\\n    \\"\\"\\"\\n    id = None  # node/token type name\\n    value = None  # used by literals\\n    first = second = None  # used by tree nodes\\n\\n    def nud(self, parser):\\n        # Null denotation - called in prefix context\\n        raise parser.error_class(\\n            \\"Not expecting \'%s\' in this position in if tag.\\" % self.id\\n        )\\n\\n    def led(self, left, parser):\\n        # Left denotation - called in infix context\\n        raise parser.error_class(\\n            \\"Not expecting \'%s\' as infix operator in if tag.\\" % self.id\\n        )\\n\\n    def display(self):\\n        \\"\\"\\"\\n        Returns what to display in error messages for this node\\n        \\"\\"\\"\\n        return self.id\\n\\n    def __repr__(self):\\n        out = [str(x) for x in [self.id, self.first, self.second] if x is not None]\\n        return \\"(\\" + \\" \\".join(out) + \\")\\"\\n\\n\\ndef infix(bp, func):\\n    \\"\\"\\"\\n    Creates an infix operator, given a binding power and a function that\\n    evaluates the node\\n    \\"\\"\\"\\n    class Operator(TokenBase):\\n        lbp = bp\\n\\n        def led(self, left, parser):\\n            self.first = left\\n            self.second = parser.expression(bp)\\n            return self\\n\\n        def eval(self, context):\\n            try:\\n                return func(context, self.first, self.second)\\n            except Exception:\\n                # Templates shouldn\'t throw exceptions when rendering.  We are\\n                # most likely to get exceptions for things like {% if foo in bar\\n                # %} where \'bar\' does not support \'in\', so default to False\\n                return False\\n\\n    return Operator\\n\\n\\ndef prefix(bp, func):\\n    \\"\\"\\"\\n    Creates a prefix operator, given a binding power and a function that\\n    evaluates the node.\\n    \\"\\"\\"\\n    class Operator(TokenBase):\\n        lbp = bp\\n\\n        def nud(self, parser):\\n            self.first = parser.expression(bp)\\n            self.second = None\\n            return self\\n\\n        def eval(self, context):\\n            try:\\n                return func(context, self.first)\\n            except Exception:\\n                return False\\n\\n    return Operator\\n\\n\\n# Operator precedence follows Python.\\n# NB - we can get slightly more accurate syntax error messages by not using the\\n# same object for \'==\' and \'=\'.\\n# We defer variable evaluation to the lambda to ensure that terms are\\n# lazily evaluated using Python\'s boolean parsing logic.\\nOPERATORS = {\\n    \'or\': infix(6, lambda context, x, y: x.eval(context) or y.eval(context)),\\n    \'and\': infix(7, lambda context, x, y: x.eval(context) and y.eval(context)),\\n    \'not\': prefix(8, lambda context, x: not x.eval(context)),\\n    \'in\': infix(9, lambda context, x, y: x.eval(context) in y.eval(context)),\\n    \'not in\': infix(9, lambda context, x, y: x.eval(context) not in y.eval(context)),\\n    # This should be removed in Django 1.10:\\n    \'=\': infix(10, lambda context, x, y: x.eval(context) == y.eval(context)),\\n    \'==\': infix(10, lambda context, x, y: x.eval(context) == y.eval(context)),\\n    \'!=\': infix(10, lambda context, x, y: x.eval(context) != y.eval(context)),\\n    \'\\u003e\': infix(10, lambda context, x, y: x.eval(context) \\u003e y.eval(context)),\\n    \'\\u003e=\': infix(10, lambda context, x, y: x.eval(context) \\u003e= y.eval(context)),\\n    \'\\u003c\': infix(10, lambda context, x, y: x.eval(context) \\u003c y.eval(context)),\\n    \'\\u003c=\': infix(10, lambda context, x, y: x.eval(context) \\u003c= y.eval(context)),\\n}\\n\\n# Assign \'id\' to each:\\nfor key, op in OPERATORS.items():\\n    op.id = key\\n\\n\\nclass Literal(TokenBase):\\n    \\"\\"\\"\\n    A basic self-resolvable object similar to a Django template variable.\\n    \\"\\"\\"\\n    # IfParser uses Literal in create_var, but TemplateIfParser overrides\\n    # create_var so that a proper implementation that actually resolves\\n    # variables, filters etc is used.\\n    id = \\"literal\\"\\n    lbp = 0\\n\\n    def __init__(self, value):\\n        self.value = value\\n\\n    def display(self):\\n        return repr(self.value)\\n\\n    def nud(self, parser):\\n        return self\\n\\n    def eval(self, context):\\n        return self.value\\n\\n    def __repr__(self):\\n        return \\"(%s %r)\\" % (self.id, self.value)\\n\\n\\nclass EndToken(TokenBase):\\n    lbp = 0\\n\\n    def nud(self, parser):\\n        raise parser.error_class(\\"Unexpected end of expression in if tag.\\")\\n\\nEndToken = EndToken()\\n\\n\\nclass IfParser(object):\\n    error_class = ValueError\\n\\n    def __init__(self, tokens):\\n        # pre-pass necessary to turn  \'not\',\'in\' into single token\\n        l = len(tokens)\\n        mapped_tokens = []\\n        i = 0\\n        while i \\u003c l:\\n            token = tokens[i]\\n            if token == \\"not\\" and i + 1 \\u003c l and tokens[i + 1] == \\"in\\":\\n                token = \\"not in\\"\\n                i += 1  # skip \'in\'\\n            mapped_tokens.append(self.translate_token(token))\\n            i += 1\\n\\n        self.tokens = mapped_tokens\\n        self.pos = 0\\n        self.current_token = self.next_token()\\n\\n    def translate_token(self, token):\\n        try:\\n            op = OPERATORS[token]\\n        except (KeyError, TypeError):\\n            return self.create_var(token)\\n        else:\\n            if token == \'=\':\\n                warnings.warn(\\n                    \\"Operator \'=\' is deprecated and will be removed in Django 1.10. Use \'==\' instead.\\",\\n                    RemovedInDjango110Warning, stacklevel=2\\n                )\\n            return op()\\n\\n    def next_token(self):\\n        if self.pos \\u003e= len(self.tokens):\\n            return EndToken\\n        else:\\n            retval = self.tokens[self.pos]\\n            self.pos += 1\\n            return retval\\n\\n    def parse(self):\\n        retval = self.expression()\\n        # Check that we have exhausted all the tokens\\n        if self.current_token is not EndToken:\\n            raise self.error_class(\\"Unused \'%s\' at end of if expression.\\" %\\n                                   self.current_token.display())\\n        return retval\\n\\n    def expression(self, rbp=0):\\n        t = self.current_token\\n        self.current_token = self.next_token()\\n        left = t.nud(self)\\n        while rbp \\u003c self.current_token.lbp:\\n            t = self.current_token\\n            self.current_token = self.next_token()\\n            left = t.led(left, self)\\n        return left\\n\\n    def create_var(self, value):\\n        return Literal(value)\\n"}\n'
line: b'{"repo_name":"shsingh/ansible","ref":"refs/heads/devel","path":"lib/ansible/modules/cloud/azure/azure_rm_securitygroup.py","content":"#!/usr/bin/python\\n#\\n# Copyright (c) 2016 Matt Davis, \\u003cmdavis@ansible.com\\u003e\\n#                    Chris Houseknecht, \\u003chouse@redhat.com\\u003e\\n#\\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\\n\\nfrom __future__ import absolute_import, division, print_function\\n__metaclass__ = type\\n\\n\\nANSIBLE_METADATA = {\'metadata_version\': \'1.1\',\\n                    \'status\': [\'preview\'],\\n                    \'supported_by\': \'community\'}\\n\\n\\nDOCUMENTATION = \'\'\'\\n---\\nmodule: azure_rm_securitygroup\\nversion_added: \\"2.1\\"\\nshort_description: Manage Azure network security groups.\\ndescription:\\n    - Create, update or delete a network security group. A security group contains Access Control List (ACL) rules\\n      that allow or deny network traffic to subnets or individual network interfaces. A security group is created\\n      with a set of default security rules and an empty set of security rules. Shape traffic flow by adding\\n      rules to the empty set of security rules.\\n\\noptions:\\n    default_rules:\\n        description:\\n            - The set of default rules automatically added to a security group at creation. In general default\\n              rules will not be modified. Modify rules to shape the flow of traffic to or from a subnet or NIC. See\\n              rules below for the makeup of a rule dict.\\n    location:\\n        description:\\n            - Valid azure location. Defaults to location of the resource group.\\n    name:\\n        description:\\n            - Name of the security group to operate on.\\n    purge_default_rules:\\n        description:\\n            - Remove any existing rules not matching those defined in the default_rules parameter.\\n        type: bool\\n        default: \'no\'\\n    purge_rules:\\n        description:\\n            - Remove any existing rules not matching those defined in the rules parameters.\\n        type: bool\\n        default: \'no\'\\n    resource_group:\\n        description:\\n            - Name of the resource group the security group belongs to.\\n        required: true\\n    rules:\\n        description:\\n            - Set of rules shaping traffic flow to or from a subnet or NIC. Each rule is a dictionary.\\n        suboptions:\\n            name:\\n                description:\\n                  - Unique name for the rule.\\n                required: true\\n            description:\\n                description:\\n                  - Short description of the rule\'s purpose.\\n            protocol:\\n                description: Accepted traffic protocol.\\n                choices:\\n                  - Udp\\n                  - Tcp\\n                  - \\"*\\"\\n                default: \\"*\\"\\n            source_port_range:\\n                description:\\n                  - Port or range of ports from which traffic originates.\\n                  - It can accept string type or a list of string type.\\n                default: \\"*\\"\\n            destination_port_range:\\n                description:\\n                  - Port or range of ports to which traffic is headed.\\n                  - It can accept string type or a list of string type.\\n                default: \\"*\\"\\n            source_address_prefix:\\n                description:\\n                  - The CIDR or source IP range.\\n                  - Asterisk C(*) can also be used to match all source IPs.\\n                  - Default tags such as C(VirtualNetwork), C(AzureLoadBalancer) and C(Internet) can also be used.\\n                  - If this is an ingress rule, specifies where network traffic originates from.\\n                  - It can accept string type or a list of string type.\\n                default: \\"*\\"\\n            destination_address_prefix:\\n                description:\\n                  - The destination address prefix.\\n                  - CIDR or destination IP range.\\n                  - Asterisk C(*) can also be used to match all source IPs.\\n                  - Default tags such as C(VirtualNetwork), C(AzureLoadBalancer) and C(Internet) can also be used.\\n                  - It can accept string type or a list of string type.\\n                default: \\"*\\"\\n            access:\\n                description:\\n                  - Whether or not to allow the traffic flow.\\n                choices:\\n                  - Allow\\n                  - Deny\\n                default: Allow\\n            priority:\\n                description:\\n                  - Order in which to apply the rule. Must a unique integer between 100 and 4096 inclusive.\\n                required: true\\n            direction:\\n                description:\\n                  - Indicates the direction of the traffic flow.\\n                choices:\\n                  - Inbound\\n                  - Outbound\\n                default: Inbound\\n    state:\\n        description:\\n            - Assert the state of the security group. Set to C(present) to create or update a security group. Set to\\n              C(absent) to remove a security group.\\n        default: present\\n        choices:\\n            - absent\\n            - present\\n\\nextends_documentation_fragment:\\n    - azure\\n    - azure_tags\\n\\nauthor:\\n    - \\"Chris Houseknecht (@chouseknecht)\\"\\n    - \\"Matt Davis (@nitzmahone)\\"\\n\\n\'\'\'\\n\\nEXAMPLES = \'\'\'\\n\\n# Create a security group\\n- azure_rm_securitygroup:\\n      resource_group: myResourceGroup\\n      name: mysecgroup\\n      purge_rules: yes\\n      rules:\\n          - name: DenySSH\\n            protocol: Tcp\\n            destination_port_range: 22\\n            access: Deny\\n            priority: 100\\n            direction: Inbound\\n          - name: \'AllowSSH\'\\n            protocol: Tcp\\n            source_address_prefix:\\n              - \'174.109.158.0/24\'\\n              - \'174.109.159.0/24\'\\n            destination_port_range: 22\\n            access: Allow\\n            priority: 101\\n            direction: Inbound\\n          - name: \'AllowMultiplePorts\'\\n            protocol: Tcp\\n            source_address_prefix:\\n              - \'174.109.158.0/24\'\\n              - \'174.109.159.0/24\'\\n            destination_port_range:\\n              - 80\\n              - 443\\n            access: Allow\\n            priority: 102\\n\\n# Update rules on existing security group\\n- azure_rm_securitygroup:\\n      resource_group: myResourceGroup\\n      name: mysecgroup\\n      rules:\\n          - name: DenySSH\\n            protocol: Tcp\\n            destination_port_range: 22-23\\n            access: Deny\\n            priority: 100\\n            direction: Inbound\\n          - name: AllowSSHFromHome\\n            protocol: Tcp\\n            source_address_prefix: \'174.109.158.0/24\'\\n            destination_port_range: 22-23\\n            access: Allow\\n            priority: 102\\n            direction: Inbound\\n      tags:\\n          testing: testing\\n          delete: on-exit\\n\\n# Delete security group\\n- azure_rm_securitygroup:\\n      resource_group: myResourceGroup\\n      name: mysecgroup\\n      state: absent\\n\'\'\'\\n\\nRETURN = \'\'\'\\nstate:\\n    description: Current state of the security group.\\n    returned: always\\n    type: dict\\n    sample: {\\n        \\"default_rules\\": [\\n            {\\n                \\"access\\": \\"Allow\\",\\n                \\"description\\": \\"Allow inbound traffic from all VMs in VNET\\",\\n                \\"destination_address_prefix\\": \\"VirtualNetwork\\",\\n                \\"destination_port_range\\": \\"*\\",\\n                \\"direction\\": \\"Inbound\\",\\n                \\"etag\\": \'W/\\"edf48d56-b315-40ca-a85d-dbcb47f2da7d\\"\',\\n                \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroup/myResourceGroup/providers/Microsoft.Network/networkSecurityGroups/mysecgroup/defaultSecurityRules/AllowVnetInBound\\",\\n                \\"name\\": \\"AllowVnetInBound\\",\\n                \\"priority\\": 65000,\\n                \\"protocol\\": \\"*\\",\\n                \\"provisioning_state\\": \\"Succeeded\\",\\n                \\"source_address_prefix\\": \\"VirtualNetwork\\",\\n                \\"source_port_range\\": \\"*\\"\\n            },\\n            {\\n                \\"access\\": \\"Allow\\",\\n                \\"description\\": \\"Allow inbound traffic from azure load balancer\\",\\n                \\"destination_address_prefix\\": \\"*\\",\\n                \\"destination_port_range\\": \\"*\\",\\n                \\"direction\\": \\"Inbound\\",\\n                \\"etag\\": \'W/\\"edf48d56-b315-40ca-a85d-dbcb47f2da7d\\"\',\\n                \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroup/myResourceGroup/providers/Microsoft.Network/networkSecurityGroups/mysecgroup/defaultSecurityRules/AllowAzureLoadBalancerInBound\\",\\n                \\"name\\": \\"AllowAzureLoadBalancerInBound\\",\\n                \\"priority\\": 65001,\\n                \\"protocol\\": \\"*\\",\\n                \\"provisioning_state\\": \\"Succeeded\\",\\n                \\"source_address_prefix\\": \\"AzureLoadBalancer\\",\\n                \\"source_port_range\\": \\"*\\"\\n            },\\n            {\\n                \\"access\\": \\"Deny\\",\\n                \\"description\\": \\"Deny all inbound traffic\\",\\n                \\"destination_address_prefix\\": \\"*\\",\\n                \\"destination_port_range\\": \\"*\\",\\n                \\"direction\\": \\"Inbound\\",\\n                \\"etag\\": \'W/\\"edf48d56-b315-40ca-a85d-dbcb47f2da7d\\"\',\\n                \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroup/myResourceGroup/providers/Microsoft.Network/networkSecurityGroups/mysecgroup/defaultSecurityRules/DenyAllInBound\\",\\n                \\"name\\": \\"DenyAllInBound\\",\\n                \\"priority\\": 65500,\\n                \\"protocol\\": \\"*\\",\\n                \\"provisioning_state\\": \\"Succeeded\\",\\n                \\"source_address_prefix\\": \\"*\\",\\n                \\"source_port_range\\": \\"*\\"\\n            },\\n            {\\n                \\"access\\": \\"Allow\\",\\n                \\"description\\": \\"Allow outbound traffic from all VMs to all VMs in VNET\\",\\n                \\"destination_address_prefix\\": \\"VirtualNetwork\\",\\n                \\"destination_port_range\\": \\"*\\",\\n                \\"direction\\": \\"Outbound\\",\\n                \\"etag\\": \'W/\\"edf48d56-b315-40ca-a85d-dbcb47f2da7d\\"\',\\n                \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroup/myResourceGroup/providers/Microsoft.Network/networkSecurityGroups/mysecgroup/defaultSecurityRules/AllowVnetOutBound\\",\\n                \\"name\\": \\"AllowVnetOutBound\\",\\n                \\"priority\\": 65000,\\n                \\"protocol\\": \\"*\\",\\n                \\"provisioning_state\\": \\"Succeeded\\",\\n                \\"source_address_prefix\\": \\"VirtualNetwork\\",\\n                \\"source_port_range\\": \\"*\\"\\n            },\\n            {\\n                \\"access\\": \\"Allow\\",\\n                \\"description\\": \\"Allow outbound traffic from all VMs to Internet\\",\\n                \\"destination_address_prefix\\": \\"Internet\\",\\n                \\"destination_port_range\\": \\"*\\",\\n                \\"direction\\": \\"Outbound\\",\\n                \\"etag\\": \'W/\\"edf48d56-b315-40ca-a85d-dbcb47f2da7d\\"\',\\n                \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroup/myResourceGroup/providers/Microsoft.Network/networkSecurityGroups/mysecgroup/defaultSecurityRules/AllowInternetOutBound\\",\\n                \\"name\\": \\"AllowInternetOutBound\\",\\n                \\"priority\\": 65001,\\n                \\"protocol\\": \\"*\\",\\n                \\"provisioning_state\\": \\"Succeeded\\",\\n                \\"source_address_prefix\\": \\"*\\",\\n                \\"source_port_range\\": \\"*\\"\\n            },\\n            {\\n                \\"access\\": \\"Deny\\",\\n                \\"description\\": \\"Deny all outbound traffic\\",\\n                \\"destination_address_prefix\\": \\"*\\",\\n                \\"destination_port_range\\": \\"*\\",\\n                \\"direction\\": \\"Outbound\\",\\n                \\"etag\\": \'W/\\"edf48d56-b315-40ca-a85d-dbcb47f2da7d\\"\',\\n                \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroup/myResourceGroup/providers/Microsoft.Network/networkSecurityGroups/mysecgroup/defaultSecurityRules/DenyAllOutBound\\",\\n                \\"name\\": \\"DenyAllOutBound\\",\\n                \\"priority\\": 65500,\\n                \\"protocol\\": \\"*\\",\\n                \\"provisioning_state\\": \\"Succeeded\\",\\n                \\"source_address_prefix\\": \\"*\\",\\n                \\"source_port_range\\": \\"*\\"\\n            }\\n        ],\\n        \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroup/myResourceGroup/providers/Microsoft.Network/networkSecurityGroups/mysecgroup\\",\\n        \\"location\\": \\"westus\\",\\n        \\"name\\": \\"mysecgroup\\",\\n        \\"network_interfaces\\": [],\\n        \\"rules\\": [\\n            {\\n                \\"access\\": \\"Deny\\",\\n                \\"description\\": null,\\n                \\"destination_address_prefix\\": \\"*\\",\\n                \\"destination_port_range\\": \\"22\\",\\n                \\"direction\\": \\"Inbound\\",\\n                \\"etag\\": \'W/\\"edf48d56-b315-40ca-a85d-dbcb47f2da7d\\"\',\\n                \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroup/myResourceGroup/providers/Microsoft.Network/networkSecurityGroups/mysecgroup/securityRules/DenySSH\\",\\n                \\"name\\": \\"DenySSH\\",\\n                \\"priority\\": 100,\\n                \\"protocol\\": \\"Tcp\\",\\n                \\"provisioning_state\\": \\"Succeeded\\",\\n                \\"source_address_prefix\\": \\"*\\",\\n                \\"source_port_range\\": \\"*\\"\\n            },\\n            {\\n                \\"access\\": \\"Allow\\",\\n                \\"description\\": null,\\n                \\"destination_address_prefix\\": \\"*\\",\\n                \\"destination_port_range\\": \\"22\\",\\n                \\"direction\\": \\"Inbound\\",\\n                \\"etag\\": \'W/\\"edf48d56-b315-40ca-a85d-dbcb47f2da7d\\"\',\\n                \\"id\\": \\"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroup/myResourceGroup/providers/Microsoft.Network/networkSecurityGroups/mysecgroup/securityRules/AllowSSH\\",\\n                \\"name\\": \\"AllowSSH\\",\\n                \\"priority\\": 101,\\n                \\"protocol\\": \\"Tcp\\",\\n                \\"provisioning_state\\": \\"Succeeded\\",\\n                \\"source_address_prefix\\": \\"174.109.158.0/24\\",\\n                \\"source_port_range\\": \\"*\\"\\n            }\\n        ],\\n        \\"subnets\\": [],\\n        \\"tags\\": {\\n            \\"delete\\": \\"on-exit\\",\\n            \\"foo\\": \\"bar\\",\\n            \\"testing\\": \\"testing\\"\\n        },\\n        \\"type\\": \\"Microsoft.Network/networkSecurityGroups\\"\\n    }\\n\'\'\'  # NOQA\\n\\ntry:\\n    from msrestazure.azure_exceptions import CloudError\\n    from azure.mgmt.network import NetworkManagementClient\\nexcept ImportError:\\n    # This is handled in azure_rm_common\\n    pass\\n\\nfrom ansible.module_utils.azure_rm_common import AzureRMModuleBase\\nfrom ansible.module_utils.six import integer_types\\nfrom ansible.module_utils._text import to_native\\n\\n\\ndef validate_rule(self, rule, rule_type=None):\\n    \'\'\'\\n    Apply defaults to a rule dictionary and check that all values are valid.\\n\\n    :param rule: rule dict\\n    :param rule_type: Set to \'default\' if the rule is part of the default set of rules.\\n    :return: None\\n    \'\'\'\\n    priority = rule.get(\'priority\', 0)\\n    if rule_type != \'default\' and (priority \\u003c 100 or priority \\u003e 4096):\\n        raise Exception(\\"Rule priority must be between 100 and 4096\\")\\n\\n    def check_plural(src, dest):\\n        if isinstance(rule.get(src), list):\\n            rule[dest] = rule[src]\\n            rule[src] = None\\n\\n    check_plural(\'destination_address_prefix\', \'destination_address_prefixes\')\\n    check_plural(\'source_address_prefix\', \'source_address_prefixes\')\\n    check_plural(\'source_port_range\', \'source_port_ranges\')\\n    check_plural(\'destination_port_range\', \'destination_port_ranges\')\\n\\n\\ndef compare_rules_change(old_list, new_list, purge_list):\\n    old_list = old_list or []\\n    new_list = new_list or []\\n    changed = False\\n\\n    for old_rule in old_list:\\n        matched = next((x for x in new_list if x[\'name\'] == old_rule[\'name\']), [])\\n        if matched:  # if the new one is in the old list, check whether it is updated\\n            changed = changed or compare_rules(old_rule, matched)\\n        elif not purge_list:  # keep this rule\\n            new_list.append(old_rule)\\n        else:  # one rule is removed\\n            changed = True\\n    # Compare new list and old list is the same? here only compare names\\n    if not changed:\\n        new_names = [to_native(x[\'name\']) for x in new_list]\\n        old_names = [to_native(x[\'name\']) for x in old_list]\\n        changed = (set(new_names) != set(old_names))\\n    return changed, new_list\\n\\n\\ndef compare_rules(old_rule, rule):\\n    changed = False\\n    if old_rule[\'name\'] != rule[\'name\']:\\n        changed = True\\n    if rule.get(\'description\', None) != old_rule[\'description\']:\\n        changed = True\\n    if rule[\'protocol\'] != old_rule[\'protocol\']:\\n        changed = True\\n    if str(rule[\'source_port_range\']) != str(old_rule[\'source_port_range\']):\\n        changed = True\\n    if str(rule[\'destination_port_range\']) != str(old_rule[\'destination_port_range\']):\\n        changed = True\\n    if rule[\'access\'] != old_rule[\'access\']:\\n        changed = True\\n    if rule[\'priority\'] != old_rule[\'priority\']:\\n        changed = True\\n    if rule[\'direction\'] != old_rule[\'direction\']:\\n        changed = True\\n    if str(rule[\'source_address_prefix\']) != str(old_rule[\'source_address_prefix\']):\\n        changed = True\\n    if str(rule[\'destination_address_prefix\']) != str(old_rule[\'destination_address_prefix\']):\\n        changed = True\\n    if set(rule.get(\'source_address_prefixes\') or []) != set(old_rule.get(\'source_address_prefixes\') or []):\\n        changed = True\\n    if set(rule.get(\'destination_address_prefixes\') or []) != set(old_rule.get(\'destination_address_prefixes\') or []):\\n        changed = True\\n    if set(rule.get(\'source_port_ranges\') or []) != set(old_rule.get(\'source_port_ranges\') or []):\\n        changed = True\\n    if set(rule.get(\'destination_port_ranges\') or []) != set(old_rule.get(\'destination_port_ranges\') or []):\\n        changed = True\\n    return changed\\n\\n\\ndef create_rule_instance(self, rule):\\n    \'\'\'\\n    Create an instance of SecurityRule from a dict.\\n\\n    :param rule: dict\\n    :return: SecurityRule\\n    \'\'\'\\n    return self.nsg_models.SecurityRule(\\n        description=rule.get(\'description\', None),\\n        protocol=rule.get(\'protocol\', None),\\n        source_port_range=rule.get(\'source_port_range\', None),\\n        destination_port_range=rule.get(\'destination_port_range\', None),\\n        source_address_prefix=rule.get(\'source_address_prefix\', None),\\n        source_address_prefixes=rule.get(\'source_address_prefixes\', None),\\n        destination_address_prefix=rule.get(\'destination_address_prefix\', None),\\n        destination_address_prefixes=rule.get(\'destination_address_prefixes\', None),\\n        source_port_ranges=rule.get(\'source_port_ranges\', None),\\n        destination_port_ranges=rule.get(\'destination_port_ranges\', None),\\n        access=rule.get(\'access\', None),\\n        priority=rule.get(\'priority\', None),\\n        direction=rule.get(\'direction\', None),\\n        provisioning_state=rule.get(\'provisioning_state\', None),\\n        name=rule.get(\'name\', None),\\n        etag=rule.get(\'etag\', None)\\n    )\\n\\n\\ndef create_rule_dict_from_obj(rule):\\n    \'\'\'\\n    Create a dict from an instance of a SecurityRule.\\n\\n    :param rule: SecurityRule\\n    :return: dict\\n    \'\'\'\\n    return dict(\\n        id=rule.id,\\n        name=rule.name,\\n        description=rule.description,\\n        protocol=rule.protocol,\\n        source_port_range=rule.source_port_range,\\n        destination_port_range=rule.destination_port_range,\\n        source_address_prefix=rule.source_address_prefix,\\n        destination_address_prefix=rule.destination_address_prefix,\\n        source_port_ranges=rule.source_port_ranges,\\n        destination_port_ranges=rule.destination_port_ranges,\\n        source_address_prefixes=rule.source_address_prefixes,\\n        destination_address_prefixes=rule.destination_address_prefixes,\\n        access=rule.access,\\n        priority=rule.priority,\\n        direction=rule.direction,\\n        provisioning_state=rule.provisioning_state,\\n        etag=rule.etag\\n    )\\n\\n\\ndef create_network_security_group_dict(nsg):\\n    results = dict(\\n        id=nsg.id,\\n        name=nsg.name,\\n        type=nsg.type,\\n        location=nsg.location,\\n        tags=nsg.tags,\\n    )\\n    results[\'rules\'] = []\\n    if nsg.security_rules:\\n        for rule in nsg.security_rules:\\n            results[\'rules\'].append(create_rule_dict_from_obj(rule))\\n\\n    results[\'default_rules\'] = []\\n    if nsg.default_security_rules:\\n        for rule in nsg.default_security_rules:\\n            results[\'default_rules\'].append(create_rule_dict_from_obj(rule))\\n\\n    results[\'network_interfaces\'] = []\\n    if nsg.network_interfaces:\\n        for interface in nsg.network_interfaces:\\n            results[\'network_interfaces\'].append(interface.id)\\n\\n    results[\'subnets\'] = []\\n    if nsg.subnets:\\n        for subnet in nsg.subnets:\\n            results[\'subnets\'].append(subnet.id)\\n\\n    return results\\n\\n\\nrule_spec = dict(\\n    name=dict(type=\'str\', required=True),\\n    description=dict(type=\'str\'),\\n    protocol=dict(type=\'str\', choices=[\'Udp\', \'Tcp\', \'*\'], default=\'*\'),\\n    source_port_range=dict(type=\'raw\', default=\'*\'),\\n    destination_port_range=dict(type=\'raw\', default=\'*\'),\\n    source_address_prefix=dict(type=\'raw\', default=\'*\'),\\n    destination_address_prefix=dict(type=\'raw\', default=\'*\'),\\n    access=dict(type=\'str\', choices=[\'Allow\', \'Deny\'], default=\'Allow\'),\\n    priority=dict(type=\'int\', required=True),\\n    direction=dict(type=\'str\', choices=[\'Inbound\', \'Outbound\'], default=\'Inbound\')\\n)\\n\\n\\nclass AzureRMSecurityGroup(AzureRMModuleBase):\\n\\n    def __init__(self):\\n\\n        self.module_arg_spec = dict(\\n            default_rules=dict(type=\'list\', elements=\'dict\', options=rule_spec),\\n            location=dict(type=\'str\'),\\n            name=dict(type=\'str\', required=True),\\n            purge_default_rules=dict(type=\'bool\', default=False),\\n            purge_rules=dict(type=\'bool\', default=False),\\n            resource_group=dict(required=True, type=\'str\'),\\n            rules=dict(type=\'list\', elements=\'dict\', options=rule_spec),\\n            state=dict(type=\'str\', default=\'present\', choices=[\'present\', \'absent\']),\\n        )\\n\\n        self.default_rules = None\\n        self.location = None\\n        self.name = None\\n        self.purge_default_rules = None\\n        self.purge_rules = None\\n        self.resource_group = None\\n        self.rules = None\\n        self.state = None\\n        self.tags = None\\n        self.nsg_models = None  # type: azure.mgmt.network.models\\n\\n        self.results = dict(\\n            changed=False,\\n            state=dict()\\n        )\\n\\n        super(AzureRMSecurityGroup, self).__init__(self.module_arg_spec,\\n                                                   supports_check_mode=True)\\n\\n    def exec_module(self, **kwargs):\\n        # tighten up poll interval for security groups; default 30s is an eternity\\n        # this value is still overridden by the response Retry-After header (which is set on the initial operation response to 10s)\\n        self.network_client.config.long_running_operation_timeout = 3\\n        self.nsg_models = self.network_client.network_security_groups.models\\n\\n        for key in list(self.module_arg_spec.keys()) + [\'tags\']:\\n            setattr(self, key, kwargs[key])\\n\\n        changed = False\\n        results = dict()\\n\\n        resource_group = self.get_resource_group(self.resource_group)\\n        if not self.location:\\n            # Set default location\\n            self.location = resource_group.location\\n\\n        if self.rules:\\n            for rule in self.rules:\\n                try:\\n                    validate_rule(self, rule)\\n                except Exception as exc:\\n                    self.fail(\\"Error validating rule {0} - {1}\\".format(rule, str(exc)))\\n\\n        if self.default_rules:\\n            for rule in self.default_rules:\\n                try:\\n                    validate_rule(self, rule, \'default\')\\n                except Exception as exc:\\n                    self.fail(\\"Error validating default rule {0} - {1}\\".format(rule, str(exc)))\\n\\n        try:\\n            nsg = self.network_client.network_security_groups.get(self.resource_group, self.name)\\n            results = create_network_security_group_dict(nsg)\\n            self.log(\\"Found security group:\\")\\n            self.log(results, pretty_print=True)\\n            self.check_provisioning_state(nsg, self.state)\\n            if self.state == \'present\':\\n                pass\\n            elif self.state == \'absent\':\\n                self.log(\\"CHANGED: security group found but state is \'absent\'\\")\\n                changed = True\\n        except CloudError:  # TODO: actually check for ResourceMissingError\\n            if self.state == \'present\':\\n                self.log(\\"CHANGED: security group not found and state is \'present\'\\")\\n                changed = True\\n\\n        if self.state == \'present\' and not changed:\\n            # update the security group\\n            self.log(\\"Update security group {0}\\".format(self.name))\\n\\n            update_tags, results[\'tags\'] = self.update_tags(results[\'tags\'])\\n            if update_tags:\\n                changed = True\\n\\n            rule_changed, new_rule = compare_rules_change(results[\'rules\'], self.rules, self.purge_rules)\\n            if rule_changed:\\n                changed = True\\n                results[\'rules\'] = new_rule\\n            rule_changed, new_rule = compare_rules_change(results[\'default_rules\'], self.default_rules, self.purge_default_rules)\\n            if rule_changed:\\n                changed = True\\n                results[\'default_rules\'] = new_rule\\n\\n            self.results[\'changed\'] = changed\\n            self.results[\'state\'] = results\\n            if not self.check_mode and changed:\\n                self.results[\'state\'] = self.create_or_update(results)\\n\\n        elif self.state == \'present\' and changed:\\n            # create the security group\\n            self.log(\\"Create security group {0}\\".format(self.name))\\n\\n            if not self.location:\\n                self.fail(\\"Parameter error: location required when creating a security group.\\")\\n\\n            results[\'name\'] = self.name\\n            results[\'location\'] = self.location\\n            results[\'rules\'] = []\\n            results[\'default_rules\'] = []\\n            results[\'tags\'] = {}\\n\\n            if self.rules:\\n                results[\'rules\'] = self.rules\\n            if self.default_rules:\\n                results[\'default_rules\'] = self.default_rules\\n            if self.tags:\\n                results[\'tags\'] = self.tags\\n\\n            self.results[\'changed\'] = changed\\n            self.results[\'state\'] = results\\n            if not self.check_mode:\\n                self.results[\'state\'] = self.create_or_update(results)\\n\\n        elif self.state == \'absent\' and changed:\\n            self.log(\\"Delete security group {0}\\".format(self.name))\\n            self.results[\'changed\'] = changed\\n            self.results[\'state\'] = dict()\\n            if not self.check_mode:\\n                self.delete()\\n                # the delete does not actually return anything. if no exception, then we\'ll assume\\n                # it worked.\\n                self.results[\'state\'][\'status\'] = \'Deleted\'\\n\\n        return self.results\\n\\n    def create_or_update(self, results):\\n        parameters = self.nsg_models.NetworkSecurityGroup()\\n        if results.get(\'rules\'):\\n            parameters.security_rules = []\\n            for rule in results.get(\'rules\'):\\n                parameters.security_rules.append(create_rule_instance(self, rule))\\n        if results.get(\'default_rules\'):\\n            parameters.default_security_rules = []\\n            for rule in results.get(\'default_rules\'):\\n                parameters.default_security_rules.append(create_rule_instance(self, rule))\\n        parameters.tags = results.get(\'tags\')\\n        parameters.location = results.get(\'location\')\\n\\n        try:\\n            poller = self.network_client.network_security_groups.create_or_update(resource_group_name=self.resource_group,\\n                                                                                  network_security_group_name=self.name,\\n                                                                                  parameters=parameters)\\n            result = self.get_poller_result(poller)\\n        except CloudError as exc:\\n            self.fail(\\"Error creating/updating security group {0} - {1}\\".format(self.name, str(exc)))\\n        return create_network_security_group_dict(result)\\n\\n    def delete(self):\\n        try:\\n            poller = self.network_client.network_security_groups.delete(resource_group_name=self.resource_group, network_security_group_name=self.name)\\n            result = self.get_poller_result(poller)\\n        except CloudError as exc:\\n            raise Exception(\\"Error deleting security group {0} - {1}\\".format(self.name, str(exc)))\\n        return result\\n\\n\\ndef main():\\n    AzureRMSecurityGroup()\\n\\n\\nif __name__ == \'__main__\':\\n    main()\\n"}\n'
line: b'{"repo_name":"srvg/ansible","ref":"refs/heads/devel","path":"lib/ansible/modules/network/netvisor/pn_vtep.py","content":"#!/usr/bin/python\\n# Copyright: (c) 2018, Pluribus Networks\\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\\n\\nfrom __future__ import absolute_import, division, print_function\\n__metaclass__ = type\\n\\n\\nANSIBLE_METADATA = {\'metadata_version\': \'1.1\',\\n                    \'status\': [\'preview\'],\\n                    \'supported_by\': \'community\'}\\n\\n\\nDOCUMENTATION = \\"\\"\\"\\n---\\nmodule: pn_vtep\\nauthor: \\"Pluribus Networks (@rajaspachipulusu17)\\"\\nversion_added: \\"2.9\\"\\nshort_description: CLI command to create/delete vtep\\ndescription:\\n  - This module can be used to create a vtep and delete a vtep.\\noptions:\\n  pn_cliswitch:\\n    description:\\n      - Target switch to run the CLI on.\\n    required: false\\n    type: str\\n  state:\\n    description:\\n      - vtep configuration command.\\n    required: false\\n    choices: [\'present\', \'absent\']\\n    type: str\\n    default: \'present\'\\n  pn_name:\\n    description:\\n      - vtep name.\\n    required: false\\n    type: str\\n  pn_ip:\\n    description:\\n      - Primary IP address.\\n    required: false\\n    type: str\\n  pn_vrouter_name:\\n    description:\\n      - name of the vrouter service.\\n    required: false\\n    type: str\\n  pn_virtual_ip:\\n    description:\\n      - Virtual/Secondary IP address.\\n    required: false\\n    type: str\\n  pn_location:\\n    description:\\n      - switch name.\\n    required: false\\n    type: str\\n  pn_switch_in_cluster:\\n    description:\\n      - Tells whether switch in cluster or not.\\n    required: false\\n    type: bool\\n    default: True\\n\\"\\"\\"\\n\\nEXAMPLES = \\"\\"\\"\\n- name: create vtep\\n  pn_vtep:\\n    pn_cliswitch: \'sw01\'\\n    pn_name: \'foo\'\\n    pn_vrouter_name: \'foo-vrouter\'\\n    pn_ip: \'22.22.22.2\'\\n    pn_location: \'sw01\'\\n    pn_virtual_ip: \\"22.22.22.1\\"\\n\\n- name: delete vtep\\n  pn_vtep:\\n    pn_cliswitch: \'sw01\'\\n    state: \'absent\'\\n    pn_name: \'foo\'\\n\\"\\"\\"\\n\\nRETURN = \\"\\"\\"\\ncommand:\\n  description: the CLI command run on the target node.\\n  returned: always\\n  type: str\\nstdout:\\n  description: set of responses from the vtep command.\\n  returned: always\\n  type: list\\nstderr:\\n  description: set of error responses from the vtep command.\\n  returned: on error\\n  type: list\\nchanged:\\n  description: indicates whether the CLI caused changes on the target.\\n  returned: always\\n  type: bool\\n\\"\\"\\"\\n\\nfrom ansible.module_utils.basic import AnsibleModule\\nfrom ansible.module_utils.network.netvisor.pn_nvos import pn_cli, run_cli\\nfrom ansible.module_utils.network.netvisor.netvisor import run_commands\\n\\n\\ndef check_cli(module, cli):\\n    \\"\\"\\"\\n    This method checks for idempotency using the vtep-show command.\\n    If a name exists, return True if name exists else False.\\n    :param module: The Ansible module to fetch input parameters\\n    :param cli: The CLI string\\n    \\"\\"\\"\\n    name = module.params[\'pn_name\']\\n\\n    cli += \' vtep-show format name no-show-headers\'\\n    out = run_commands(module, cli)[1]\\n\\n    if out:\\n        out = out.split()\\n\\n    return True if name in out else False\\n\\n\\ndef main():\\n    \\"\\"\\" This section is for arguments parsing \\"\\"\\"\\n\\n    state_map = dict(\\n        present=\'vtep-create\',\\n        absent=\'vtep-delete\'\\n    )\\n\\n    argument_spec = dict(\\n        pn_cliswitch=dict(required=False, type=\'str\'),\\n        state=dict(required=False, type=\'str\', choices=state_map.keys(), default=\'present\'),\\n        pn_name=dict(required=False, type=\'str\'),\\n        pn_ip=dict(required=False, type=\'str\'),\\n        pn_vrouter_name=dict(required=False, type=\'str\'),\\n        pn_virtual_ip=dict(required=False, type=\'str\'),\\n        pn_location=dict(required=False, type=\'str\'),\\n        pn_switch_in_cluster=dict(required=False, type=\'bool\', default=\'True\')\\n    )\\n\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        required_if=(\\n            [\\"state\\", \\"present\\", [\\"pn_name\\", \\"pn_ip\\", \\"pn_vrouter_name\\", \\"pn_location\\"]],\\n            [\\"state\\", \\"absent\\", [\\"pn_name\\"]],\\n        ),\\n    )\\n\\n    # Accessing the arguments\\n    cliswitch = module.params[\'pn_cliswitch\']\\n    state = module.params[\'state\']\\n    name = module.params[\'pn_name\']\\n    ip = module.params[\'pn_ip\']\\n    vrouter_name = module.params[\'pn_vrouter_name\']\\n    virtual_ip = module.params[\'pn_virtual_ip\']\\n    location = module.params[\'pn_location\']\\n    switch_in_cluster = module.params[\'pn_switch_in_cluster\']\\n\\n    if switch_in_cluster and not virtual_ip and state == \'present\':\\n        module.exit_json(\\n            failed=True,\\n            msg=\'virtual ip is required when switch is in cluster\'\\n        )\\n\\n    command = state_map[state]\\n\\n    # Building the CLI command string\\n    cli = pn_cli(module, cliswitch)\\n\\n    NAME_EXISTS = check_cli(module, cli)\\n\\n    cli += \' %s name %s \' % (command, name)\\n\\n    if command == \'vtep-delete\':\\n        if NAME_EXISTS is False:\\n            module.exit_json(\\n                skipped=True,\\n                msg=\'vtep with name %s does not exist\' % name\\n            )\\n\\n    if command == \'vtep-create\':\\n        if NAME_EXISTS is True:\\n            module.exit_json(\\n                skipped=True,\\n                msg=\'vtpe with name %s already exists\' % name\\n            )\\n\\n        cli += \'vrouter-name %s \' % vrouter_name\\n        cli += \'ip %s \' % ip\\n        cli += \'location %s \' % location\\n\\n        if virtual_ip:\\n            cli += \'virtual-ip %s \' % virtual_ip\\n\\n    run_cli(module, cli, state_map)\\n\\n\\nif __name__ == \'__main__\':\\n    main()\\n"}\n'
line: b'{"repo_name":"dragonpt/Kernel_3.4.67_KK_Wiko_DarkMoon","ref":"refs/heads/master","path":"tools/perf/scripts/python/futex-contention.py","content":"# futex contention\\n# (c) 2010, Arnaldo Carvalho de Melo \\u003cacme@redhat.com\\u003e\\n# Licensed under the terms of the GNU GPL License version 2\\n#\\n# Translation of:\\n#\\n# http://sourceware.org/systemtap/wiki/WSFutexContention\\n#\\n# to perf python scripting.\\n#\\n# Measures futex contention\\n\\nimport os, sys\\nsys.path.append(os.environ[\'PERF_EXEC_PATH\'] + \'/scripts/python/Perf-Trace-Util/lib/Perf/Trace\')\\nfrom Util import *\\n\\nprocess_names = {}\\nthread_thislock = {}\\nthread_blocktime = {}\\n\\nlock_waits = {} # long-lived stats on (tid,lock) blockage elapsed time\\nprocess_names = {} # long-lived pid-to-execname mapping\\n\\ndef syscalls__sys_enter_futex(event, ctxt, cpu, s, ns, tid, comm,\\n\\t\\t\\t      nr, uaddr, op, val, utime, uaddr2, val3):\\n\\tcmd = op \\u0026 FUTEX_CMD_MASK\\n\\tif cmd != FUTEX_WAIT:\\n\\t\\treturn # we don\'t care about originators of WAKE events\\n\\n\\tprocess_names[tid] = comm\\n\\tthread_thislock[tid] = uaddr\\n\\tthread_blocktime[tid] = nsecs(s, ns)\\n\\ndef syscalls__sys_exit_futex(event, ctxt, cpu, s, ns, tid, comm,\\n\\t\\t\\t     nr, ret):\\n\\tif thread_blocktime.has_key(tid):\\n\\t\\telapsed = nsecs(s, ns) - thread_blocktime[tid]\\n\\t\\tadd_stats(lock_waits, (tid, thread_thislock[tid]), elapsed)\\n\\t\\tdel thread_blocktime[tid]\\n\\t\\tdel thread_thislock[tid]\\n\\ndef trace_begin():\\n\\tprint \\"Press control+C to stop and show the summary\\"\\n\\ndef trace_end():\\n\\tfor (tid, lock) in lock_waits:\\n\\t\\tmin, max, avg, count = lock_waits[tid, lock]\\n\\t\\tprint \\"%s[%d] lock %x contended %d times, %d avg ns\\" % \\\\\\n\\t\\t      (process_names[tid], tid, lock, count, avg)\\n\\n"}\n'
line: b'{"repo_name":"bert9bert/statsmodels","ref":"refs/heads/master","path":"statsmodels/sandbox/nonparametric/kernel_extras.py","content":"\\"\\"\\"\\nMultivariate Conditional and Unconditional Kernel Density Estimation\\nwith Mixed Data Types\\n\\nReferences\\n----------\\n[1] Racine, J., Li, Q. Nonparametric econometrics: theory and practice.\\n    Princeton University Press. (2007)\\n[2] Racine, Jeff. \\"Nonparametric Econometrics: A Primer,\\" Foundation\\n    and Trends in Econometrics: Vol 3: No 1, pp1-88. (2008)\\n    http://dx.doi.org/10.1561/0800000009\\n[3] Racine, J., Li, Q. \\"Nonparametric Estimation of Distributions\\n    with Categorical and Continuous Data.\\" Working Paper. (2000)\\n[4] Racine, J. Li, Q. \\"Kernel Estimation of Multivariate Conditional\\n    Distributions Annals of Economics and Finance 5, 211-235 (2004)\\n[5] Liu, R., Yang, L. \\"Kernel estimation of multivariate\\n    cumulative distribution function.\\"\\n    Journal of Nonparametric Statistics (2008)\\n[6] Li, R., Ju, G. \\"Nonparametric Estimation of Multivariate CDF\\n    with Categorical and Continuous Data.\\" Working Paper\\n[7] Li, Q., Racine, J. \\"Cross-validated local linear nonparametric\\n    regression\\" Statistica Sinica 14(2004), pp. 485-512\\n[8] Racine, J.: \\"Consistent Significance Testing for Nonparametric\\n        Regression\\" Journal of Business \\u0026 Economics Statistics\\n[9] Racine, J., Hart, J., Li, Q., \\"Testing the Significance of\\n        Categorical Predictor Variables in Nonparametric Regression\\n        Models\\", 2006, Econometric Reviews 25, 523-544\\n\\n\\"\\"\\"\\n\\n# TODO: make default behavior efficient=True above a certain n_obs\\n\\nfrom statsmodels.compat.python import range, next\\nimport numpy as np\\nfrom scipy import optimize\\nfrom scipy.stats.mstats import mquantiles\\n\\nfrom statsmodels.nonparametric.api import KDEMultivariate, KernelReg\\nfrom statsmodels.nonparametric._kernel_base import \\\\\\n    gpke, LeaveOneOut, _get_type_pos, _adjust_shape\\n\\n\\n__all__ = [\'SingleIndexModel\', \'SemiLinear\', \'TestFForm\']\\n\\n\\nclass TestFForm(object):\\n    \\"\\"\\"\\n    Nonparametric test for functional form.\\n\\n    Parameters\\n    ----------\\n    endog: list\\n        Dependent variable (training set)\\n    exog: list of array_like objects\\n        The independent (right-hand-side) variables\\n    bw: array_like, str\\n        Bandwidths for exog or specify method for bandwidth selection\\n    fform: function\\n        The functional form ``y = g(b, x)`` to be tested. Takes as inputs\\n        the RHS variables `exog` and the coefficients ``b`` (betas)\\n        and returns a fitted ``y_hat``.\\n    var_type: str\\n        The type of the independent `exog` variables:\\n\\n            - c: continuous\\n            - o: ordered\\n            - u: unordered\\n\\n    estimator: function\\n        Must return the estimated coefficients b (betas). Takes as inputs\\n        ``(endog, exog)``.  E.g. least square estimator::\\n\\n            lambda (x,y): np.dot(np.pinv(np.dot(x.T, x)), np.dot(x.T, y))\\n\\n    References\\n    ----------\\n    See Racine, J.: \\"Consistent Significance Testing for Nonparametric\\n    Regression\\" Journal of Business \\\\\\u0026 Economics Statistics.\\n\\n    See chapter 12 in [1]  pp. 355-357.\\n\\n    \\"\\"\\"\\n    def __init__(self, endog, exog, bw, var_type, fform, estimator, nboot=100):\\n        self.endog = endog\\n        self.exog = exog\\n        self.var_type = var_type\\n        self.fform = fform\\n        self.estimator = estimator\\n        self.nboot = nboot\\n        self.bw = KDEMultivariate(exog, bw=bw, var_type=var_type).bw\\n        self.sig = self._compute_sig()\\n\\n    def _compute_sig(self):\\n        Y = self.endog\\n        X = self.exog\\n        b = self.estimator(Y, X)\\n        m = self.fform(X, b)\\n        n = np.shape(X)[0]\\n        resid = Y - m\\n        resid = resid - np.mean(resid)  # center residuals\\n        self.test_stat = self._compute_test_stat(resid)\\n        sqrt5 = np.sqrt(5.)\\n        fct1 = (1 - sqrt5) / 2.\\n        fct2 = (1 + sqrt5) / 2.\\n        u1 = fct1 * resid\\n        u2 = fct2 * resid\\n        r = fct2 / sqrt5\\n        I_dist = np.empty((self.nboot,1))\\n        for j in range(self.nboot):\\n            u_boot = u2.copy()\\n\\n            prob = np.random.uniform(0,1, size = (n,))\\n            ind = prob \\u003c r\\n            u_boot[ind] = u1[ind]\\n            Y_boot = m + u_boot\\n            b_hat = self.estimator(Y_boot, X)\\n            m_hat = self.fform(X, b_hat)\\n            u_boot_hat = Y_boot - m_hat\\n            I_dist[j] = self._compute_test_stat(u_boot_hat)\\n\\n        self.boots_results = I_dist\\n        sig = \\"Not Significant\\"\\n        if self.test_stat \\u003e mquantiles(I_dist, 0.9):\\n            sig = \\"*\\"\\n        if self.test_stat \\u003e mquantiles(I_dist, 0.95):\\n            sig = \\"**\\"\\n        if self.test_stat \\u003e mquantiles(I_dist, 0.99):\\n            sig = \\"***\\"\\n        return sig\\n\\n    def _compute_test_stat(self, u):\\n        n = np.shape(u)[0]\\n        XLOO = LeaveOneOut(self.exog)\\n        uLOO = LeaveOneOut(u[:,None]).__iter__()\\n        I = 0\\n        S2 = 0\\n        for i, X_not_i in enumerate(XLOO):\\n            u_j = next(uLOO)\\n            u_j = np.squeeze(u_j)\\n            # See Bootstrapping procedure on p. 357 in [1]\\n            K = gpke(self.bw, data=-X_not_i, data_predict=-self.exog[i, :],\\n                     var_type=self.var_type, tosum=False)\\n            f_i = (u[i] * u_j * K)\\n            assert u_j.shape == K.shape\\n            I += f_i.sum()  # See eq. 12.7 on p. 355 in [1]\\n            S2 += (f_i**2).sum()  # See Theorem 12.1 on p.356 in [1]\\n            assert np.size(I) == 1\\n            assert np.size(S2) == 1\\n\\n        I *= 1. / (n * (n - 1))\\n        ix_cont = _get_type_pos(self.var_type)[0]\\n        hp = self.bw[ix_cont].prod()\\n        S2 *= 2 * hp / (n * (n - 1))\\n        T = n * I * np.sqrt(hp / S2)\\n        return T\\n\\n\\nclass SingleIndexModel(KernelReg):\\n    \\"\\"\\"\\n    Single index semiparametric model ``y = g(X * b) + e``.\\n\\n    Parameters\\n    ----------\\n    endog: array_like\\n        The dependent variable\\n    exog: array_like\\n        The independent variable(s)\\n    var_type: str\\n        The type of variables in X:\\n\\n            - c: continuous\\n            - o: ordered\\n            - u: unordered\\n\\n    Attributes\\n    ----------\\n    b: array_like\\n        The linear coefficients b (betas)\\n    bw: array_like\\n        Bandwidths\\n\\n    Methods\\n    -------\\n    fit(): Computes the fitted values ``E[Y|X] = g(X * b)``\\n           and the marginal effects ``dY/dX``.\\n\\n    References\\n    ----------\\n    See chapter on semiparametric models in [1]\\n\\n    Notes\\n    -----\\n    This model resembles the binary choice models. The user knows\\n    that X and b interact linearly, but ``g(X * b)`` is unknown.\\n    In the parametric binary choice models the user usually assumes\\n    some distribution of g() such as normal or logistic.\\n\\n    \\"\\"\\"\\n    def __init__(self, endog, exog, var_type):\\n        self.var_type = var_type\\n        self.K = len(var_type)\\n        self.var_type = self.var_type[0]\\n        self.endog = _adjust_shape(endog, 1)\\n        self.exog = _adjust_shape(exog, self.K)\\n        self.nobs = np.shape(self.exog)[0]\\n        self.data_type = self.var_type\\n        self.func = self._est_loc_linear\\n\\n        self.b, self.bw = self._est_b_bw()\\n\\n    def _est_b_bw(self):\\n        params0 = np.random.uniform(size=(self.K + 1, ))\\n        b_bw = optimize.fmin(self.cv_loo, params0, disp=0)\\n        b = b_bw[0:self.K]\\n        bw = b_bw[self.K:]\\n        bw = self._set_bw_bounds(bw)\\n        return b, bw\\n\\n    def cv_loo(self, params):\\n        # See p. 254 in Textbook\\n        params = np.asarray(params)\\n        b = params[0 : self.K]\\n        bw = params[self.K:]\\n        LOO_X = LeaveOneOut(self.exog)\\n        LOO_Y = LeaveOneOut(self.endog).__iter__()\\n        L = 0\\n        for i, X_not_i in enumerate(LOO_X):\\n            Y = next(LOO_Y)\\n            #print b.shape, np.dot(self.exog[i:i+1, :], b).shape, bw,\\n            G = self.func(bw, endog=Y, exog=-np.dot(X_not_i, b)[:,None],\\n                          #data_predict=-b*self.exog[i, :])[0]\\n                          data_predict=-np.dot(self.exog[i:i+1, :], b))[0]\\n            #print G.shape\\n            L += (self.endog[i] - G) ** 2\\n\\n        # Note: There might be a way to vectorize this. See p.72 in [1]\\n        return L / self.nobs\\n\\n    def fit(self, data_predict=None):\\n        if data_predict is None:\\n            data_predict = self.exog\\n        else:\\n            data_predict = _adjust_shape(data_predict, self.K)\\n\\n        N_data_predict = np.shape(data_predict)[0]\\n        mean = np.empty((N_data_predict,))\\n        mfx = np.empty((N_data_predict, self.K))\\n        for i in range(N_data_predict):\\n            mean_mfx = self.func(self.bw, self.endog,\\n                                 np.dot(self.exog, self.b)[:,None],\\n                                 data_predict=np.dot(data_predict[i:i+1, :],self.b))\\n            mean[i] = mean_mfx[0]\\n            mfx_c = np.squeeze(mean_mfx[1])\\n            mfx[i, :] = mfx_c\\n\\n        return mean, mfx\\n\\n    def __repr__(self):\\n        \\"\\"\\"Provide something sane to print.\\"\\"\\"\\n        repr = \\"Single Index Model \\\\n\\"\\n        repr += \\"Number of variables: K = \\" + str(self.K) + \\"\\\\n\\"\\n        repr += \\"Number of samples:   nobs = \\" + str(self.nobs) + \\"\\\\n\\"\\n        repr += \\"Variable types:      \\" + self.var_type + \\"\\\\n\\"\\n        repr += \\"BW selection method: cv_ls\\" + \\"\\\\n\\"\\n        repr += \\"Estimator type: local constant\\" + \\"\\\\n\\"\\n        return repr\\n\\n\\nclass SemiLinear(KernelReg):\\n    \\"\\"\\"\\n    Semiparametric partially linear model, ``Y = Xb + g(Z) + e``.\\n\\n    Parameters\\n    ----------\\n    endog: array_like\\n        The dependent variable\\n    exog: array_like\\n        The linear component in the regression\\n    exog_nonparametric: array_like\\n        The nonparametric component in the regression\\n    var_type: str\\n        The type of the variables in the nonparametric component;\\n\\n            - c: continuous\\n            - o: ordered\\n            - u: unordered\\n\\n    k_linear : int\\n        The number of variables that comprise the linear component.\\n\\n    Attributes\\n    ----------\\n    bw: array_like\\n        Bandwidths for the nonparametric component exog_nonparametric\\n    b: array_like\\n        Coefficients in the linear component\\n    nobs : int\\n        The number of observations.\\n    k_linear : int\\n        The number of variables that comprise the linear component.\\n\\n    Methods\\n    -------\\n    fit(): Returns the fitted mean and marginal effects dy/dz\\n\\n    Notes\\n    -----\\n    This model uses only the local constant regression estimator\\n\\n    References\\n    ----------\\n    See chapter on Semiparametric Models in [1]\\n    \\"\\"\\"\\n\\n    def __init__(self, endog, exog, exog_nonparametric, var_type, k_linear):\\n        self.endog = _adjust_shape(endog, 1)\\n        self.exog = _adjust_shape(exog, k_linear)\\n        self.K = len(var_type)\\n        self.exog_nonparametric = _adjust_shape(exog_nonparametric, self.K)\\n        self.k_linear = k_linear\\n        self.nobs = np.shape(self.exog)[0]\\n        self.var_type = var_type\\n        self.data_type = self.var_type\\n        self.func = self._est_loc_linear\\n\\n        self.b, self.bw = self._est_b_bw()\\n\\n    def _est_b_bw(self):\\n        \\"\\"\\"\\n        Computes the (beta) coefficients and the bandwidths.\\n\\n        Minimizes ``cv_loo`` with respect to ``b`` and ``bw``.\\n        \\"\\"\\"\\n        params0 = np.random.uniform(size=(self.k_linear + self.K, ))\\n        b_bw = optimize.fmin(self.cv_loo, params0, disp=0)\\n        b = b_bw[0 : self.k_linear]\\n        bw = b_bw[self.k_linear:]\\n        #bw = self._set_bw_bounds(np.asarray(bw))\\n        return b, bw\\n\\n    def cv_loo(self, params):\\n        \\"\\"\\"\\n        Similar to the cross validation leave-one-out estimator.\\n\\n        Modified to reflect the linear components.\\n\\n        Parameters\\n        ----------\\n        params: array_like\\n            Vector consisting of the coefficients (b) and the bandwidths (bw).\\n            The first ``k_linear`` elements are the coefficients.\\n\\n        Returns\\n        -------\\n        L: float\\n            The value of the objective function\\n\\n        References\\n        ----------\\n        See p.254 in [1]\\n        \\"\\"\\"\\n        params = np.asarray(params)\\n        b = params[0 : self.k_linear]\\n        bw = params[self.k_linear:]\\n        LOO_X = LeaveOneOut(self.exog)\\n        LOO_Y = LeaveOneOut(self.endog).__iter__()\\n        LOO_Z = LeaveOneOut(self.exog_nonparametric).__iter__()\\n        Xb = np.dot(self.exog, b)[:,None]\\n        L = 0\\n        for ii, X_not_i in enumerate(LOO_X):\\n            Y = next(LOO_Y)\\n            Z = next(LOO_Z)\\n            Xb_j = np.dot(X_not_i, b)[:,None]\\n            Yx = Y - Xb_j\\n            G = self.func(bw, endog=Yx, exog=-Z,\\n                          data_predict=-self.exog_nonparametric[ii, :])[0]\\n            lt = Xb[ii, :] #.sum()  # linear term\\n            L += (self.endog[ii] - lt - G) ** 2\\n\\n        return L\\n\\n    def fit(self, exog_predict=None, exog_nonparametric_predict=None):\\n        \\"\\"\\"Computes fitted values and marginal effects\\"\\"\\"\\n\\n        if exog_predict is None:\\n            exog_predict = self.exog\\n        else:\\n            exog_predict = _adjust_shape(exog_predict, self.k_linear)\\n\\n        if exog_nonparametric_predict is None:\\n            exog_nonparametric_predict = self.exog_nonparametric\\n        else:\\n            exog_nonparametric_predict = _adjust_shape(exog_nonparametric_predict, self.K)\\n\\n        N_data_predict = np.shape(exog_nonparametric_predict)[0]\\n        mean = np.empty((N_data_predict,))\\n        mfx = np.empty((N_data_predict, self.K))\\n        Y = self.endog - np.dot(exog_predict, self.b)[:,None]\\n        for i in range(N_data_predict):\\n            mean_mfx = self.func(self.bw, Y, self.exog_nonparametric,\\n                                 data_predict=exog_nonparametric_predict[i, :])\\n            mean[i] = mean_mfx[0]\\n            mfx_c = np.squeeze(mean_mfx[1])\\n            mfx[i, :] = mfx_c\\n\\n        return mean, mfx\\n\\n    def __repr__(self):\\n        \\"\\"\\"Provide something sane to print.\\"\\"\\"\\n        repr = \\"Semiparamatric Partially Linear Model \\\\n\\"\\n        repr += \\"Number of variables: K = \\" + str(self.K) + \\"\\\\n\\"\\n        repr += \\"Number of samples:   N = \\" + str(self.nobs) + \\"\\\\n\\"\\n        repr += \\"Variable types:      \\" + self.var_type + \\"\\\\n\\"\\n        repr += \\"BW selection method: cv_ls\\" + \\"\\\\n\\"\\n        repr += \\"Estimator type: local constant\\" + \\"\\\\n\\"\\n        return repr\\n"}\n'
line: b'{"repo_name":"peonycredit/peonycredit","ref":"refs/heads/master","path":"qa/pull-tester/pull-tester.py","content":"#!/usr/bin/python\\n# Copyright (c) 2013 The Bitcoin Core developers\\n# Distributed under the MIT/X11 software license, see the accompanying\\n# file COPYING or http://www.opensource.org/licenses/mit-license.php.\\n#\\nimport json\\nfrom urllib import urlopen\\nimport requests\\nimport getpass\\nfrom string import Template\\nimport sys\\nimport os\\nimport subprocess\\n\\nclass RunError(Exception):\\n    def __init__(self, value):\\n        self.value = value\\n    def __str__(self):\\n        return repr(self.value)\\n\\ndef run(command, **kwargs):\\n    fail_hard = kwargs.pop(\\"fail_hard\\", True)\\n    # output to /dev/null by default:\\n    kwargs.setdefault(\\"stdout\\", open(\'/dev/null\', \'w\'))\\n    kwargs.setdefault(\\"stderr\\", open(\'/dev/null\', \'w\'))\\n    command = Template(command).substitute(os.environ)\\n    if \\"TRACE\\" in os.environ:\\n        if \'cwd\' in kwargs:\\n            print(\\"[cwd=%s] %s\\"%(kwargs[\'cwd\'], command))\\n        else: print(command)\\n    try:\\n        process = subprocess.Popen(command.split(\' \'), **kwargs)\\n        process.wait()\\n    except KeyboardInterrupt:\\n        process.terminate()\\n        raise\\n    if process.returncode != 0 and fail_hard:\\n        raise RunError(\\"Failed: \\"+command)\\n    return process.returncode\\n\\ndef checkout_pull(clone_url, commit, out):\\n    # Init\\n    build_dir=os.environ[\\"BUILD_DIR\\"]\\n    run(\\"umount ${CHROOT_COPY}/proc\\", fail_hard=False)\\n    run(\\"rsync --delete -apv ${CHROOT_MASTER}/ ${CHROOT_COPY}\\")\\n    run(\\"rm -rf ${CHROOT_COPY}${SCRIPTS_DIR}\\")\\n    run(\\"cp -a ${SCRIPTS_DIR} ${CHROOT_COPY}${SCRIPTS_DIR}\\")\\n    # Merge onto upstream/master\\n    run(\\"rm -rf ${BUILD_DIR}\\")\\n    run(\\"mkdir -p ${BUILD_DIR}\\")\\n    run(\\"git clone ${CLONE_URL} ${BUILD_DIR}\\")\\n    run(\\"git remote add pull \\"+clone_url, cwd=build_dir, stdout=out, stderr=out)\\n    run(\\"git fetch pull\\", cwd=build_dir, stdout=out, stderr=out)\\n    if run(\\"git merge \\"+ commit, fail_hard=False, cwd=build_dir, stdout=out, stderr=out) != 0:\\n        return False\\n    run(\\"chown -R ${BUILD_USER}:${BUILD_GROUP} ${BUILD_DIR}\\", stdout=out, stderr=out)\\n    run(\\"mount --bind /proc ${CHROOT_COPY}/proc\\")\\n    return True\\n\\ndef commentOn(commentUrl, success, inMerge, needTests, linkUrl):\\n    common_message = \\"\\"\\"\\nThis test script verifies pulls every time they are updated. It, however, dies sometimes and fails to test properly.  If you are waiting on a test, please check timestamps to verify that the test.log is moving at http://jenkins.bluematt.me/pull-tester/current/\\nContact BlueMatt on freenode if something looks broken.\\"\\"\\"\\n\\n    # Remove old BitcoinPullTester comments (I\'m being lazy and not paginating here)\\n    recentcomments = requests.get(commentUrl+\\"?sort=created\\u0026direction=desc\\",\\n                                  auth=(os.environ[\'GITHUB_USER\'], os.environ[\\"GITHUB_AUTH_TOKEN\\"])).json\\n    for comment in recentcomments:\\n        if comment[\\"user\\"][\\"login\\"] == os.environ[\\"GITHUB_USER\\"] and common_message in comment[\\"body\\"]:\\n            requests.delete(comment[\\"url\\"],\\n                                  auth=(os.environ[\'GITHUB_USER\'], os.environ[\\"GITHUB_AUTH_TOKEN\\"]))\\n\\n    if success == True:\\n        if needTests:\\n            message = \\"Automatic sanity-testing: PLEASE ADD TEST-CASES, though technically passed. See \\" + linkUrl + \\" for binaries and test log.\\"\\n        else:\\n            message = \\"Automatic sanity-testing: PASSED, see \\" + linkUrl + \\" for binaries and test log.\\"\\n\\n        post_data = { \\"body\\" : message + common_message}\\n    elif inMerge:\\n        post_data = { \\"body\\" : \\"Automatic sanity-testing: FAILED MERGE, see \\" + linkUrl + \\" for test log.\\" + \\"\\"\\"\\n\\nThis pull does not merge cleanly onto current master\\"\\"\\" + common_message}\\n    else:\\n        post_data = { \\"body\\" : \\"Automatic sanity-testing: FAILED BUILD/TEST, see \\" + linkUrl + \\" for binaries and test log.\\" + \\"\\"\\"\\n\\nThis could happen for one of several reasons:\\n1. It chanages changes build scripts in a way that made them incompatible with the automated testing scripts (please tweak those patches in qa/pull-tester)\\n2. It adds/modifies tests which test network rules (thanks for doing that), which conflicts with a patch applied at test time\\n3. It does not build on either Linux i386 or Win32 (via MinGW cross compile)\\n4. The test suite fails on either Linux i386 or Win32\\n5. The block test-cases failed (lookup the first bNN identifier which failed in https://github.com/TheBlueMatt/test-scripts/blob/master/FullBlockTestGenerator.java)\\n\\nIf you believe this to be in error, please ping BlueMatt on freenode or TheBlueMatt here.\\n\\"\\"\\" + common_message}\\n\\n    resp = requests.post(commentUrl, json.dumps(post_data), auth=(os.environ[\'GITHUB_USER\'], os.environ[\\"GITHUB_AUTH_TOKEN\\"]))\\n\\ndef testpull(number, comment_url, clone_url, commit):\\n    print(\\"Testing pull %d: %s : %s\\"%(number, clone_url,commit))\\n\\n    dir = os.environ[\\"RESULTS_DIR\\"] + \\"/\\" + commit + \\"/\\"\\n    print(\\" ouput to %s\\"%dir)\\n    if os.path.exists(dir):\\n        os.system(\\"rm -r \\" + dir)\\n    os.makedirs(dir)\\n    currentdir = os.environ[\\"RESULTS_DIR\\"] + \\"/current\\"\\n    os.system(\\"rm -r \\"+currentdir)\\n    os.system(\\"ln -s \\" + dir + \\" \\" + currentdir)\\n    out = open(dir + \\"test.log\\", \'w+\')\\n\\n    resultsurl = os.environ[\\"RESULTS_URL\\"] + commit\\n    checkedout = checkout_pull(clone_url, commit, out)\\n    if checkedout != True:\\n        print(\\"Failed to test pull - sending comment to: \\" + comment_url)\\n        commentOn(comment_url, False, True, False, resultsurl)\\n        open(os.environ[\\"TESTED_DB\\"], \\"a\\").write(commit + \\"\\\\n\\")\\n        return\\n\\n    run(\\"rm -rf ${CHROOT_COPY}/${OUT_DIR}\\", fail_hard=False);\\n    run(\\"mkdir -p ${CHROOT_COPY}/${OUT_DIR}\\", fail_hard=False);\\n    run(\\"chown -R ${BUILD_USER}:${BUILD_GROUP} ${CHROOT_COPY}/${OUT_DIR}\\", fail_hard=False)\\n\\n    script = os.environ[\\"BUILD_PATH\\"]+\\"/qa/pull-tester/pull-tester.sh\\"\\n    script += \\" ${BUILD_PATH} ${MINGW_DEPS_DIR} ${SCRIPTS_DIR}/BitcoindComparisonTool_jar/BitcoindComparisonTool.jar 0 6 ${OUT_DIR}\\"\\n    returncode = run(\\"chroot ${CHROOT_COPY} sudo -u ${BUILD_USER} -H timeout ${TEST_TIMEOUT} \\"+script,\\n                     fail_hard=False, stdout=out, stderr=out)\\n\\n    run(\\"mv ${CHROOT_COPY}/${OUT_DIR} \\" + dir)\\n    run(\\"mv ${BUILD_DIR} \\" + dir)\\n\\n    if returncode == 42:\\n        print(\\"Successfully tested pull (needs tests) - sending comment to: \\" + comment_url)\\n        commentOn(comment_url, True, False, True, resultsurl)\\n    elif returncode != 0:\\n        print(\\"Failed to test pull - sending comment to: \\" + comment_url)\\n        commentOn(comment_url, False, False, False, resultsurl)\\n    else:\\n        print(\\"Successfully tested pull - sending comment to: \\" + comment_url)\\n        commentOn(comment_url, True, False, False, resultsurl)\\n    open(os.environ[\\"TESTED_DB\\"], \\"a\\").write(commit + \\"\\\\n\\")\\n\\ndef environ_default(setting, value):\\n    if not setting in os.environ:\\n        os.environ[setting] = value\\n\\nif getpass.getuser() != \\"root\\":\\n\\tprint(\\"Run me as root!\\")\\n\\tsys.exit(1)\\n\\nif \\"GITHUB_USER\\" not in os.environ or \\"GITHUB_AUTH_TOKEN\\" not in os.environ:\\n    print(\\"GITHUB_USER and/or GITHUB_AUTH_TOKEN environment variables not set\\")\\n    sys.exit(1)\\n\\nenviron_default(\\"CLONE_URL\\", \\"https://github.com/bitcoin/bitcoin.git\\")\\nenviron_default(\\"MINGW_DEPS_DIR\\", \\"/mnt/w32deps\\")\\nenviron_default(\\"SCRIPTS_DIR\\", \\"/mnt/test-scripts\\")\\nenviron_default(\\"CHROOT_COPY\\", \\"/mnt/chroot-tmp\\")\\nenviron_default(\\"CHROOT_MASTER\\", \\"/mnt/chroot\\")\\nenviron_default(\\"OUT_DIR\\", \\"/mnt/out\\")\\nenviron_default(\\"BUILD_PATH\\", \\"/mnt/bitcoin\\")\\nos.environ[\\"BUILD_DIR\\"] = os.environ[\\"CHROOT_COPY\\"] + os.environ[\\"BUILD_PATH\\"]\\nenviron_default(\\"RESULTS_DIR\\", \\"/mnt/www/pull-tester\\")\\nenviron_default(\\"RESULTS_URL\\", \\"http://jenkins.bluematt.me/pull-tester/\\")\\nenviron_default(\\"GITHUB_REPO\\", \\"bitcoin/bitcoin\\")\\nenviron_default(\\"TESTED_DB\\", \\"/mnt/commits-tested.txt\\")\\nenviron_default(\\"BUILD_USER\\", \\"matt\\")\\nenviron_default(\\"BUILD_GROUP\\", \\"matt\\")\\nenviron_default(\\"TEST_TIMEOUT\\", str(60*60*2))\\n\\nprint(\\"Optional usage: pull-tester.py 2112\\")\\n\\nf = open(os.environ[\\"TESTED_DB\\"])\\ntested = set( line.rstrip() for line in f.readlines() )\\nf.close()\\n\\nif len(sys.argv) \\u003e 1:\\n    pull = requests.get(\\"https://api.github.com/repos/\\"+os.environ[\\"GITHUB_REPO\\"]+\\"/pulls/\\"+sys.argv[1],\\n                        auth=(os.environ[\'GITHUB_USER\'], os.environ[\\"GITHUB_AUTH_TOKEN\\"])).json\\n    testpull(pull[\\"number\\"], pull[\\"_links\\"][\\"comments\\"][\\"href\\"],\\n             pull[\\"head\\"][\\"repo\\"][\\"clone_url\\"], pull[\\"head\\"][\\"sha\\"])\\n\\nelse:\\n    for page in range(1,100):\\n        result = requests.get(\\"https://api.github.com/repos/\\"+os.environ[\\"GITHUB_REPO\\"]+\\"/pulls?state=open\\u0026page=%d\\"%(page,),\\n                              auth=(os.environ[\'GITHUB_USER\'], os.environ[\\"GITHUB_AUTH_TOKEN\\"])).json\\n        if len(result) == 0: break;\\n        for pull in result:\\n            if pull[\\"head\\"][\\"sha\\"] in tested:\\n                print(\\"Pull %d already tested\\"%(pull[\\"number\\"],))\\n                continue\\n            testpull(pull[\\"number\\"], pull[\\"_links\\"][\\"comments\\"][\\"href\\"],\\n                     pull[\\"head\\"][\\"repo\\"][\\"clone_url\\"], pull[\\"head\\"][\\"sha\\"])\\n"}\n'
line: b'{"repo_name":"haad/ansible","ref":"refs/heads/devel","path":"lib/ansible/modules/cloud/lxd/lxd_profile.py","content":"#!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n\\n# (c) 2016, Hiroaki Nakamura \\u003chnakamur@gmail.com\\u003e\\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\\n\\nfrom __future__ import absolute_import, division, print_function\\n__metaclass__ = type\\n\\n\\nANSIBLE_METADATA = {\'metadata_version\': \'1.1\',\\n                    \'status\': [\'preview\'],\\n                    \'supported_by\': \'community\'}\\n\\n\\nDOCUMENTATION = \'\'\'\\n---\\nmodule: lxd_profile\\nshort_description: Manage LXD profiles\\nversion_added: \\"2.2\\"\\ndescription:\\n  - Management of LXD profiles\\nauthor: \\"Hiroaki Nakamura (@hnakamur)\\"\\noptions:\\n    name:\\n        description:\\n          - Name of a profile.\\n        required: true\\n    description:\\n        description:\\n          - Description of the profile.\\n        version_added: \\"2.5\\"\\n    config:\\n        description:\\n          - \'The config for the container (e.g. {\\"limits.memory\\": \\"4GB\\"}).\\n            See U(https://github.com/lxc/lxd/blob/master/doc/rest-api.md#patch-3)\'\\n          - If the profile already exists and its \\"config\\" value in metadata\\n            obtained from\\n            GET /1.0/profiles/\\u003cname\\u003e\\n            U(https://github.com/lxc/lxd/blob/master/doc/rest-api.md#get-19)\\n            are different, they this module tries to apply the configurations.\\n          - Not all config values are supported to apply the existing profile.\\n            Maybe you need to delete and recreate a profile.\\n        required: false\\n    devices:\\n        description:\\n          - \'The devices for the profile\\n            (e.g. {\\"rootfs\\": {\\"path\\": \\"/dev/kvm\\", \\"type\\": \\"unix-char\\"}).\\n            See U(https://github.com/lxc/lxd/blob/master/doc/rest-api.md#patch-3)\'\\n        required: false\\n    new_name:\\n        description:\\n          - A new name of a profile.\\n          - If this parameter is specified a profile will be renamed to this name.\\n            See U(https://github.com/lxc/lxd/blob/master/doc/rest-api.md#post-11)\\n        required: false\\n    state:\\n        choices:\\n          - present\\n          - absent\\n        description:\\n          - Define the state of a profile.\\n        required: false\\n        default: present\\n    url:\\n        description:\\n          - The unix domain socket path or the https URL for the LXD server.\\n        required: false\\n        default: unix:/var/lib/lxd/unix.socket\\n    key_file:\\n        description:\\n          - The client certificate key file path.\\n        required: false\\n        default: \'\\"{}/.config/lxc/client.key\\" .format(os.environ[\\"HOME\\"])\'\\n    cert_file:\\n        description:\\n          - The client certificate file path.\\n        required: false\\n        default: \'\\"{}/.config/lxc/client.crt\\" .format(os.environ[\\"HOME\\"])\'\\n    trust_password:\\n        description:\\n          - The client trusted password.\\n          - You need to set this password on the LXD server before\\n            running this module using the following command.\\n            lxc config set core.trust_password \\u003csome random password\\u003e\\n            See U(https://www.stgraber.org/2016/04/18/lxd-api-direct-interaction/)\\n          - If trust_password is set, this module send a request for\\n            authentication before sending any requests.\\n        required: false\\nnotes:\\n  - Profiles must have a unique name. If you attempt to create a profile\\n    with a name that already existed in the users namespace the module will\\n    simply return as \\"unchanged\\".\\n\'\'\'\\n\\nEXAMPLES = \'\'\'\\n# An example for creating a profile\\n- hosts: localhost\\n  connection: local\\n  tasks:\\n    - name: Create a profile\\n      lxd_profile:\\n        name: macvlan\\n        state: present\\n        config: {}\\n        description: my macvlan profile\\n        devices:\\n          eth0:\\n            nictype: macvlan\\n            parent: br0\\n            type: nic\\n\\n# An example for creating a profile via http connection\\n- hosts: localhost\\n  connection: local\\n  tasks:\\n  - name: create macvlan profile\\n    lxd_profile:\\n      url: https://127.0.0.1:8443\\n      # These cert_file and key_file values are equal to the default values.\\n      #cert_file: \\"{{ lookup(\'env\', \'HOME\') }}/.config/lxc/client.crt\\"\\n      #key_file: \\"{{ lookup(\'env\', \'HOME\') }}/.config/lxc/client.key\\"\\n      trust_password: mypassword\\n      name: macvlan\\n      state: present\\n      config: {}\\n      description: my macvlan profile\\n      devices:\\n        eth0:\\n          nictype: macvlan\\n          parent: br0\\n          type: nic\\n\\n# An example for deleting a profile\\n- hosts: localhost\\n  connection: local\\n  tasks:\\n    - name: Delete a profile\\n      lxd_profile:\\n        name: macvlan\\n        state: absent\\n\\n# An example for renaming a profile\\n- hosts: localhost\\n  connection: local\\n  tasks:\\n    - name: Rename a profile\\n      lxd_profile:\\n        name: macvlan\\n        new_name: macvlan2\\n        state: present\\n\'\'\'\\n\\nRETURN = \'\'\'\\nold_state:\\n  description: The old state of the profile\\n  returned: success\\n  type: string\\n  sample: \\"absent\\"\\nlogs:\\n  description: The logs of requests and responses.\\n  returned: when ansible-playbook is invoked with -vvvv.\\n  type: list\\n  sample: \\"(too long to be placed here)\\"\\nactions:\\n  description: List of actions performed for the profile.\\n  returned: success\\n  type: list\\n  sample: \'[\\"create\\"]\'\\n\'\'\'\\n\\nimport os\\n\\nfrom ansible.module_utils.basic import AnsibleModule\\nfrom ansible.module_utils.lxd import LXDClient, LXDClientException\\n\\n\\n# PROFILE_STATES is a list for states supported\\nPROFILES_STATES = [\\n    \'present\', \'absent\'\\n]\\n\\n# CONFIG_PARAMS is a list of config attribute names.\\nCONFIG_PARAMS = [\\n    \'config\', \'description\', \'devices\'\\n]\\n\\n\\nclass LXDProfileManagement(object):\\n    def __init__(self, module):\\n        \\"\\"\\"Management of LXC containers via Ansible.\\n\\n        :param module: Processed Ansible Module.\\n        :type module: ``object``\\n        \\"\\"\\"\\n        self.module = module\\n        self.name = self.module.params[\'name\']\\n        self._build_config()\\n        self.state = self.module.params[\'state\']\\n        self.new_name = self.module.params.get(\'new_name\', None)\\n\\n        self.url = self.module.params[\'url\']\\n        self.key_file = self.module.params.get(\'key_file\', None)\\n        self.cert_file = self.module.params.get(\'cert_file\', None)\\n        self.debug = self.module._verbosity \\u003e= 4\\n        try:\\n            self.client = LXDClient(\\n                self.url, key_file=self.key_file, cert_file=self.cert_file,\\n                debug=self.debug\\n            )\\n        except LXDClientException as e:\\n            self.module.fail_json(msg=e.msg)\\n        self.trust_password = self.module.params.get(\'trust_password\', None)\\n        self.actions = []\\n\\n    def _build_config(self):\\n        self.config = {}\\n        for attr in CONFIG_PARAMS:\\n            param_val = self.module.params.get(attr, None)\\n            if param_val is not None:\\n                self.config[attr] = param_val\\n\\n    def _get_profile_json(self):\\n        return self.client.do(\\n            \'GET\', \'/1.0/profiles/{0}\'.format(self.name),\\n            ok_error_codes=[404]\\n        )\\n\\n    @staticmethod\\n    def _profile_json_to_module_state(resp_json):\\n        if resp_json[\'type\'] == \'error\':\\n            return \'absent\'\\n        return \'present\'\\n\\n    def _update_profile(self):\\n        if self.state == \'present\':\\n            if self.old_state == \'absent\':\\n                if self.new_name is None:\\n                    self._create_profile()\\n                else:\\n                    self.module.fail_json(\\n                        msg=\'new_name must not be set when the profile does not exist and the specified state is present\',\\n                        changed=False)\\n            else:\\n                if self.new_name is not None and self.new_name != self.name:\\n                    self._rename_profile()\\n                if self._needs_to_apply_profile_configs():\\n                    self._apply_profile_configs()\\n        elif self.state == \'absent\':\\n            if self.old_state == \'present\':\\n                if self.new_name is None:\\n                    self._delete_profile()\\n                else:\\n                    self.module.fail_json(\\n                        msg=\'new_name must not be set when the profile exists and the specified state is absent\',\\n                        changed=False)\\n\\n    def _create_profile(self):\\n        config = self.config.copy()\\n        config[\'name\'] = self.name\\n        self.client.do(\'POST\', \'/1.0/profiles\', config)\\n        self.actions.append(\'create\')\\n\\n    def _rename_profile(self):\\n        config = {\'name\': self.new_name}\\n        self.client.do(\'POST\', \'/1.0/profiles/{}\'.format(self.name), config)\\n        self.actions.append(\'rename\')\\n        self.name = self.new_name\\n\\n    def _needs_to_change_profile_config(self, key):\\n        if key not in self.config:\\n            return False\\n        old_configs = self.old_profile_json[\'metadata\'].get(key, None)\\n        return self.config[key] != old_configs\\n\\n    def _needs_to_apply_profile_configs(self):\\n        return (\\n            self._needs_to_change_profile_config(\'config\') or\\n            self._needs_to_change_profile_config(\'description\') or\\n            self._needs_to_change_profile_config(\'devices\')\\n        )\\n\\n    def _apply_profile_configs(self):\\n        config = self.old_profile_json.copy()\\n        for k, v in self.config.items():\\n            config[k] = v\\n        self.client.do(\'PUT\', \'/1.0/profiles/{}\'.format(self.name), config)\\n        self.actions.append(\'apply_profile_configs\')\\n\\n    def _delete_profile(self):\\n        self.client.do(\'DELETE\', \'/1.0/profiles/{}\'.format(self.name))\\n        self.actions.append(\'delete\')\\n\\n    def run(self):\\n        \\"\\"\\"Run the main method.\\"\\"\\"\\n\\n        try:\\n            if self.trust_password is not None:\\n                self.client.authenticate(self.trust_password)\\n\\n            self.old_profile_json = self._get_profile_json()\\n            self.old_state = self._profile_json_to_module_state(self.old_profile_json)\\n            self._update_profile()\\n\\n            state_changed = len(self.actions) \\u003e 0\\n            result_json = {\\n                \'changed\': state_changed,\\n                \'old_state\': self.old_state,\\n                \'actions\': self.actions\\n            }\\n            if self.client.debug:\\n                result_json[\'logs\'] = self.client.logs\\n            self.module.exit_json(**result_json)\\n        except LXDClientException as e:\\n            state_changed = len(self.actions) \\u003e 0\\n            fail_params = {\\n                \'msg\': e.msg,\\n                \'changed\': state_changed,\\n                \'actions\': self.actions\\n            }\\n            if self.client.debug:\\n                fail_params[\'logs\'] = e.kwargs[\'logs\']\\n            self.module.fail_json(**fail_params)\\n\\n\\ndef main():\\n    \\"\\"\\"Ansible Main module.\\"\\"\\"\\n\\n    module = AnsibleModule(\\n        argument_spec=dict(\\n            name=dict(\\n                type=\'str\',\\n                required=True\\n            ),\\n            new_name=dict(\\n                type=\'str\',\\n            ),\\n            config=dict(\\n                type=\'dict\',\\n            ),\\n            description=dict(\\n                type=\'str\',\\n            ),\\n            devices=dict(\\n                type=\'dict\',\\n            ),\\n            state=dict(\\n                choices=PROFILES_STATES,\\n                default=\'present\'\\n            ),\\n            url=dict(\\n                type=\'str\',\\n                default=\'unix:/var/lib/lxd/unix.socket\'\\n            ),\\n            key_file=dict(\\n                type=\'str\',\\n                default=\'{}/.config/lxc/client.key\'.format(os.environ[\'HOME\'])\\n            ),\\n            cert_file=dict(\\n                type=\'str\',\\n                default=\'{}/.config/lxc/client.crt\'.format(os.environ[\'HOME\'])\\n            ),\\n            trust_password=dict(type=\'str\', no_log=True)\\n        ),\\n        supports_check_mode=False,\\n    )\\n\\n    lxd_manage = LXDProfileManagement(module=module)\\n    lxd_manage.run()\\n\\n\\nif __name__ == \'__main__\':\\n    main()\\n"}\n'
line: b'{"repo_name":"michalkurka/h2o-3","ref":"refs/heads/master","path":"h2o-py/tests/testdir_algos/pca/pyunit_pubdev_4961_pca_implementations.py","content":"from __future__ import print_function\\nfrom builtins import str\\nfrom builtins import range\\nimport sys\\nsys.path.insert(1,\\"../../../\\")\\nimport h2o\\nfrom tests import pyunit_utils\\nfrom h2o.estimators.pca import H2OPrincipalComponentAnalysisEstimator as H2OPCA\\n\\n\\ndef pca_arrests():\\n  print(\\"Importing USArrests.csv data...\\")\\n  arrestsH2O = h2o.upload_file(pyunit_utils.locate(\\"smalldata/pca_test/USArrests.csv\\"))\\n\\n  print(\\"Testing to see whether the trained PCA are essentially the same using different implementation...\\")\\n  \\n  eigenvector_standard = None\\n  for impl in [\\"MTJ_EVD_DENSEMATRIX\\", \\"MTJ_EVD_SYMMMATRIX\\", \\"MTJ_SVD_DENSEMATRIX\\", \\"JAMA\\"]:\\n    print(\\"Run PCA with implementation: \\" + impl)\\n    model = H2OPCA(k = 4, pca_impl=impl, seed=1234)\\n    model.train(x=list(range(4)), training_frame=arrestsH2O)\\n    eigenvectors = model._model_json[\\"output\\"][\\"eigenvectors\\"]\\n    if eigenvector_standard is not None:\\n      # Compare to see if they are fundamentally the same\\n      pyunit_utils.assert_H2OTwoDimTable_equal(\\n        eigenvector_standard,\\n        eigenvectors,\\n        model._model_json[\\"output\\"][\\"names\\"],\\n        tolerance=1e-6,\\n        check_sign=True,\\n        check_all=False)\\n    else:\\n      eigenvector_standard = eigenvectors\\n\\nif __name__ == \\"__main__\\":\\n  pyunit_utils.standalone_test(pca_arrests)\\nelse:\\n  pca_arrests()\\n"}\n'
line: b'{"repo_name":"carragom/modoboa","ref":"refs/heads/master","path":"modoboa/admin/models/domain_alias.py","content":"\\"\\"\\"Models related to domain aliases management.\\"\\"\\"\\n\\nfrom django.db import models\\nfrom django.utils.encoding import python_2_unicode_compatible, smart_text\\nfrom django.utils.translation import ugettext as _, ugettext_lazy\\n\\nfrom django.contrib.contenttypes.fields import GenericRelation\\n\\nfrom reversion import revisions as reversion\\n\\nfrom modoboa.core import models as core_models\\nfrom modoboa.core import signals as core_signals\\nfrom modoboa.lib.exceptions import BadRequest, Conflict\\n\\nfrom .base import AdminObject\\nfrom .domain import Domain\\n\\n\\nclass DomainAliasManager(models.Manager):\\n\\n    def get_for_admin(self, admin):\\n        \\"\\"\\"Return the domain aliases belonging to this admin.\\n\\n        The result is a ``QuerySet`` object, so this function can be used\\n        to fill ``ModelChoiceField`` objects.\\n        \\"\\"\\"\\n        if admin.is_superuser:\\n            return self.get_queryset()\\n        return self.get_queryset().filter(owners__user=admin)\\n\\n\\n@python_2_unicode_compatible\\nclass DomainAlias(AdminObject):\\n\\n    \\"\\"\\"Domain aliases.\\"\\"\\"\\n\\n    name = models.CharField(ugettext_lazy(\\"name\\"), max_length=100, unique=True,\\n                            help_text=ugettext_lazy(\\"The alias name\\"))\\n    target = models.ForeignKey(\\n        Domain, verbose_name=ugettext_lazy(\'target\'),\\n        help_text=ugettext_lazy(\\"The domain this alias points to\\")\\n    )\\n    enabled = models.BooleanField(\\n        ugettext_lazy(\'enabled\'),\\n        help_text=ugettext_lazy(\\"Check to activate this alias\\"),\\n        default=True\\n    )\\n\\n    owners = GenericRelation(core_models.ObjectAccess)\\n\\n    objects = DomainAliasManager()\\n\\n    class Meta:\\n        permissions = (\\n            (\\"view_domaliases\\", \\"View domain aliases\\"),\\n        )\\n        app_label = \\"admin\\"\\n\\n    def __str__(self):\\n        return smart_text(self.name)\\n\\n    def from_csv(self, user, row):\\n        \\"\\"\\"Create a domain alias from a CSV row\\n\\n        Expected format: [\\"domainalias\\", domain alias name, targeted domain, enabled]\\n\\n        :param user: a ``User`` object\\n        :param row: a list containing the alias definition\\n        \\"\\"\\"\\n        if len(row) \\u003c 4:\\n            raise BadRequest(_(\\"Invalid line\\"))\\n        self.name = row[1].strip()\\n        for model in [DomainAlias, Domain]:\\n            if model.objects.filter(name=self.name).exists():\\n                raise Conflict\\n        domname = row[2].strip()\\n        try:\\n            self.target = Domain.objects.get(name=domname)\\n        except Domain.DoesNotExist:\\n            raise BadRequest(_(\\"Unknown domain %s\\") % domname)\\n        core_signals.can_create_object.send(\\n            sender=\\"import\\", context=self.target, object_type=\\"domain_aliases\\")\\n        self.enabled = row[3].strip() in [\\"True\\", \\"1\\", \\"yes\\", \\"y\\"]\\n        self.save(creator=user)\\n\\n    def to_csv(self, csvwriter):\\n        \\"\\"\\"Export a domain alias using CSV format\\n\\n        :param csvwriter: a ``csv.writer`` object\\n        \\"\\"\\"\\n        csvwriter.writerow([\\"domainalias\\", self.name,\\n                            self.target.name, self.enabled])\\n\\nreversion.register(DomainAlias)\\n"}\n'
line: b'{"repo_name":"roandelyf/iTerm2","ref":"refs/heads/master","path":"tests/esctest/tests/el.py","content":"from esc import NUL, blank\\nimport escargs\\nimport esccmd\\nimport escio\\nfrom esctypes import Point, Rect\\nfrom escutil import AssertEQ, AssertScreenCharsInRectEqual, GetCursorPosition, knownBug\\n\\nclass ELTests(object):\\n  def prepare(self):\\n    \\"\\"\\"Initializes the screen to abcdefghij on the first line with the cursor\\n    on the \'e\'.\\"\\"\\"\\n    esccmd.CUP(Point(1, 1))\\n    escio.Write(\\"abcdefghij\\")\\n    esccmd.CUP(Point(5, 1))\\n\\n  def test_EL_Default(self):\\n    \\"\\"\\"Should erase to right of cursor.\\"\\"\\"\\n    self.prepare()\\n    esccmd.EL()\\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\\n                                 [ \\"abcd\\" + 6 * NUL ])\\n\\n  def test_EL_0(self):\\n    \\"\\"\\"Should erase to right of cursor.\\"\\"\\"\\n    self.prepare()\\n    esccmd.EL(0)\\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\\n                                 [ \\"abcd\\" + 6 * NUL ])\\n\\n  def test_EL_1(self):\\n    \\"\\"\\"Should erase to left of cursor.\\"\\"\\"\\n    self.prepare()\\n    esccmd.EL(1)\\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\\n                                 [ 5 * blank() + \\"fghij\\" ])\\n\\n  def test_EL_2(self):\\n    \\"\\"\\"Should erase whole line.\\"\\"\\"\\n    self.prepare()\\n    esccmd.EL(2)\\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\\n                                 [ 10 * NUL ])\\n\\n  def test_EL_IgnoresScrollRegion(self):\\n    \\"\\"\\"Should erase whole line.\\"\\"\\"\\n    self.prepare()\\n    esccmd.DECSET(esccmd.DECLRMM)\\n    esccmd.DECSLRM(2, 4)\\n    esccmd.CUP(Point(5, 1))\\n    esccmd.EL(2)\\n    esccmd.DECRESET(esccmd.DECLRMM)\\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\\n                                 [ 10 * NUL ])\\n\\n  def test_EL_doesNotRespectDECProtection(self):\\n    \\"\\"\\"EL respects DECSCA.\\"\\"\\"\\n    escio.Write(\\"a\\")\\n    escio.Write(\\"b\\")\\n    esccmd.DECSCA(1)\\n    escio.Write(\\"c\\")\\n    esccmd.DECSCA(0)\\n    esccmd.CUP(Point(1, 1))\\n    esccmd.EL(2)\\n    AssertScreenCharsInRectEqual(Rect(1, 1, 3, 1),\\n                                 [ NUL * 3 ])\\n\\n  @knownBug(terminal=\\"iTerm2\\",\\n            reason=\\"Protection not implemented.\\")\\n  def test_EL_respectsISOProtection(self):\\n    \\"\\"\\"EL respects SPA/EPA.\\"\\"\\"\\n    escio.Write(\\"a\\")\\n    escio.Write(\\"b\\")\\n    esccmd.SPA()\\n    escio.Write(\\"c\\")\\n    esccmd.EPA()\\n    esccmd.CUP(Point(1, 1))\\n    esccmd.EL(2)\\n    AssertScreenCharsInRectEqual(Rect(1, 1, 3, 1),\\n                                 [ blank() * 2 + \\"c\\" ])\\n\\n"}\n'
line: b'{"repo_name":"yatinkumbhare/openstack-nova","ref":"refs/heads/master","path":"nova/db/sqlalchemy/migrate_repo/versions/284_placeholder.py","content":"#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\n# This is a placeholder for Kilo backports.\\n# Do not use this number for new Liberty work.  New work starts after\\n# all the placeholders.\\n#\\n# See this for more information:\\n# http://lists.openstack.org/pipermail/openstack-dev/2013-March/006827.html\\n\\n\\ndef upgrade(migrate_engine):\\n    pass\\n"}\n'
line: b'{"repo_name":"tacaswell/datamuxer","ref":"refs/heads/master","path":"datamuxer.py","content":"# ######################################################################\\n# Copyright (c) 2014, Brookhaven Science Associates, Brookhaven        #\\n# National Laboratory. All rights reserved.                            #\\n#                                                                      #\\n# Redistribution and use in source and binary forms, with or without   #\\n# modification, are permitted provided that the following conditions   #\\n# are met:                                                             #\\n#                                                                      #\\n# * Redistributions of source code must retain the above copyright     #\\n#   notice, this list of conditions and the following disclaimer.      #\\n#                                                                      #\\n# * Redistributions in binary form must reproduce the above copyright  #\\n#   notice this list of conditions and the following disclaimer in     #\\n#   the documentation and/or other materials provided with the         #\\n#   distribution.                                                      #\\n#                                                                      #\\n# * Neither the name of the Brookhaven Science Associates, Brookhaven  #\\n#   National Laboratory nor the names of its contributors may be used  #\\n#   to endorse or promote products derived from this software without  #\\n#   specific prior written permission.                                 #\\n#                                                                      #\\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS  #\\n# \\"AS IS\\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT    #\\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS    #\\n# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE       #\\n# COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,           #\\n# INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES   #\\n# (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR   #\\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)   #\\n# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,  #\\n# STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OTHERWISE) ARISING   #\\n# IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE   #\\n# POSSIBILITY OF SUCH DAMAGE.                                          #\\n########################################################################\\nfrom __future__ import (absolute_import, division, print_function,\\n                        unicode_literals)\\nimport six\\nfrom collections import namedtuple, deque\\nimport logging\\nimport pandas as pd\\nimport tzlocal\\nimport numpy as np\\nfrom scipy.interpolate import interp1d\\nimport pandas.core.groupby  # to get custom exception\\n\\n\\nlogger = logging.getLogger(__name__)\\n__all__ = [\'DataMuxer\', \'dataframe_to_dict\']\\n\\nTZ = str(tzlocal.get_localzone())\\n\\n\\nclass BinningError(Exception):\\n    \\"\\"\\"\\n    An exception to raise if there are insufficient sampling rules to\\n    upsampling or downsample a data column into specified bins.\\n    \\"\\"\\"\\n    pass\\n\\n\\nclass BadDownsamplerError(Exception):\\n    \\"\\"\\"\\n    An exception to raise if a downsampler produces unexpected output.\\n    \\"\\"\\"\\n    pass\\n\\n\\nclass ColSpec(namedtuple(\\n              \'ColSpec\', [\'name\', \'ndim\', \'shape\', \'upsample\', \'downsample\'])):\\n    \\"\\"\\"\\n    Named-tuple sub-class to validate the column specifications for the\\n    DataMuxer\\n\\n    Parameters\\n    ----------\\n    name : hashable\\n    ndim : uint\\n        Dimensionality of the data stored in the column\\n    shape : tuple or None\\n        like ndarray.shape, where 0 or None are scalar\\n    upsample : {None, \'linear\', \'nearest\', \'zero\', \'slinear\', \'quadratic\', \'cubic\', \'ffill\', \'bfill\'}\\n        None means that each time bin must have at least one value.\\n        The names refer to kinds of scipy.interpolator. See documentation\\n        link below.\\n    downsample : None or a function\\n        None if the data cannot be downsampled (reduced). Otherwise,\\n        any callable that reduces multiple data points (of whatever dimension)\\n        to a single data point.\\n\\n    References\\n    ----------\\n    http://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html\\n    \\"\\"\\"\\n    # These reflect the \'method\' argument of pandas.DataFrame.fillna\\n    upsampling_methods = {\'None\', \'linear\', \'nearest\', \'zero\', \'slinear\',\\n                          \'quadratic\', \'cubic\', \'ffill\', \'bfill\'}\\n    downsampling_methods = {\'None\', \'last\', \'first\', \'median\', \'mean\', \'sum\',\\n                            \'min\', \'max\'}\\n    _downsample_mapping = {\'last\': lambda x: x[-1],\\n                           \'first\': lambda x: x[0],\\n                           # new in np 1.9\\n                           \'median\': lambda x: np.median(x, 0),\\n                           \'mean\': lambda x: np.mean(x, 0),\\n                           \'sum\': lambda x: np.sum(x, 0),\\n                           \'min\': lambda x: np.min(x, 0),\\n                           \'max\': lambda x: np.max(x, 0)}\\n\\n    __slots__ = ()\\n\\n    def __new__(cls, name, ndim, shape, upsample, downsample):\\n        # Validations\\n        upsample = _validate_upsample(upsample)\\n        downsample = _validate_downsample(downsample)\\n        if int(ndim) \\u003c 0:\\n            raise ValueError(\\"ndim must be positive not {}\\".format(ndim))\\n        if shape is not None:\\n            shape = tuple(shape)\\n\\n        return super(ColSpec, cls).__new__(\\n            cls, name, int(ndim), shape, upsample, downsample)\\n\\n\\ndef _validate_upsample(input):\\n    # TODO The upsampling method could be any callable.\\n    if input is None or input == \'None\':\\n        return \'None\'\\n    if not (input in ColSpec.upsampling_methods):\\n        raise ValueError(\\"{} is not a valid upsampling method. It \\"\\n                         \\"must be one of {}\\".format(\\n                             input, ColSpec.upsampling_methods))\\n    return input.lower()\\n\\n\\ndef _validate_downsample(input):\\n    # TODO The downsampling methods could have string aliases like \'mean\'.\\n    if (input is not None) and (not (callable(input) or\\n                                     input in ColSpec.downsampling_methods)):\\n        raise ValueError(\\"The downsampling method must be a callable, None, \\"\\n                         \\"or one of {}.\\".format(ColSpec.downsampling_methods))\\n    if input is None:\\n        return \'None\'\\n    return input\\n\\n\\nclass DataMuxer(object):\\n    \\"\\"\\"\\n    This class provides a wrapper layer of signals and slots\\n    around a pandas DataFrame to make plugging stuff in for live\\n    view easier.\\n\\n    The data collection/event model being used is all measurements\\n    (that is values that come off of the hardware) are time stamped\\n    to ring time.\\n\\n    The language being used through out is that of pandas data frames.\\n\\n    The data model is that of a sparse table keyed on time stamps which\\n    is \'densified\' on demand by propagating measurements forwards.  Not\\n    all measurements (ex images) can be filled.  This behavior is controlled\\n    by the `col_info` tuple.\\n\\n\\n    Parameters\\n    ----------\\n    events : list\\n        list of Events (any object with the expected attributes will do)\\n    \\"\\"\\"\\n    class Planner(object):\\n        def __init__(self, dm):\\n            self.dm = dm\\n\\n        def determine_upsample(self, interpolation=None, use_cols=None):\\n            \\"Resolve (and if necessary validate) upsampling rules.\\"\\n            if interpolation is None:\\n                interpolation = dict()\\n            if use_cols is None:\\n                use_cols = self.dm.columns\\n            rules = dict()\\n            for name in use_cols:\\n                col_info = self.dm.col_info[name]\\n                rule = _validate_upsample(\\n                    interpolation.get(name, col_info.upsample))\\n                rule = _normalize_string_none(rule)\\n                if (rule is not None) and (col_info.ndim \\u003e 0):\\n                    raise NotImplementedError(\\n                        \\"Only scalar data can be upsampled. \\"\\n                        \\"The {0}-dimensional source {1} was given the \\"\\n                        \\"upsampling rule {2}.\\".format(\\n                            col_info.ndim, name, rule))\\n                rules[name] = rule\\n            return rules\\n\\n        def determine_downsample(self, agg=None, use_cols=None):\\n            \\"Resolve (and if necessary validate) sampling rules.\\"\\n            if agg is None:\\n                agg = dict()\\n            if use_cols is None:\\n                use_cols = self.dm.columns\\n            rules = dict()\\n            for name in use_cols:\\n                col_info = self.dm.col_info[name]\\n                rule = _validate_downsample(agg.get(name, col_info.downsample))\\n                rule = _normalize_string_none(rule)\\n                rules[name] = rule\\n            return rules\\n\\n        def bin_by_edges(self, bin_edges, bin_anchors, interpolation=None,\\n                         agg=None, use_cols=None):\\n            \\"\\"\\"Explain operation of DataMuxer.bin_by_edges\\n\\n            Parameters\\n            ----------\\n            bin_edges : list\\n                list of two-element items like [(t1, t2), (t3, t4), ...]\\n            bin_anchors : list\\n                These are time points where interpolated values will be\\n                evaluated. Bin centers are usually a good choice.\\n            interpolation : dict, optional\\n                Override the default interpolation (upsampling) behavior of any\\n                data source by passing a dictionary of source names mapped onto\\n                one of the following interpolation methods.\\n\\n                {None, \'linear\', \'nearest\', \'zero\', \'slinear\', \'quadratic\',\\n                \'cubic\', \'ffill\', \'bfill\'}\\n\\n                None means that each time bin must have at least one value.\\n                See scipy.interpolator for more on the other methods.\\n            agg : dict, optional\\n                Override the default reduction (downsampling) behavior of any\\n                data source by passing a dictionary of source names mapped onto\\n                any callable that reduces multiple data points (of whatever\\n                dimension) to a single data point.\\n            use_cols : list, optional\\n                List of columns to include in binning; use all columns by\\n                default.\\n\\n            Returns\\n            -------\\n            df : pandas.DataFrame\\n                table giving upsample and downsample rules for each data column\\n                and indicating whether those rules are applicable\\n\\n            References\\n            ----------\\n            http://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html\\n            \\"\\"\\"\\n            bin_anchors, binning = self.dm._bin_by_edges(bin_anchors, bin_edges)\\n            # TODO Cache the grouping for reuse by resample.\\n            grouped = self.dm._dataframe.groupby(binning)\\n            counts = grouped.count()\\n            df = pd.DataFrame.from_dict(_is_resampling_applicable(counts))\\n            df[\'upsample\'] = self.determine_upsample(interpolation, use_cols)\\n            df[\'downsample\'] = self.determine_downsample(agg, use_cols)\\n            return df\\n\\n        def bin_on(self, source_name, interpolation=None, agg=None,\\n                   use_cols=None):\\n            \\"\\"\\"Explain operation of DataMuxer.bin_on.\\n\\n            Parameters\\n            ----------\\n            source_name : string\\n            interpolation : dict, optional\\n                Override the default interpolation (upsampling) behavior of any\\n                data source by passing a dictionary of source names mapped onto\\n                one of the following interpolation methods.\\n\\n                {None, \'linear\', \'nearest\', \'zero\', \'slinear\', \'quadratic\',\\n                \'cubic\'}\\n\\n                None means that each time bin must have at least one value.\\n                See scipy.interpolator for more on the other methods.\\n            agg : dict, optional\\n                Override the default reduction (downsampling) behavior of any\\n                data source by passing a dictionary of source names mapped onto\\n                any callable that reduces multiple data points (of whatever\\n                dimension) to a single data point.\\n            use_cols : list, optional\\n                List of columns to include in binning; use all columns by\\n                default.\\n\\n            Returns\\n            -------\\n            df : pandas.DataFrame\\n                table giving upsample and downsample rules for each data column\\n                and indicating whether those rules are applicable\\n\\n            References\\n            ----------\\n            http://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html\\n            \\"\\"\\"\\n            centers, bin_edges = self.dm._bin_on(source_name)\\n            bin_anchors, binning = self.dm._bin_by_edges(centers, bin_edges)\\n            # TODO Cache the grouping for reuse by resample.\\n            grouped = self.dm._dataframe.groupby(binning)\\n            counts = grouped.count()\\n            df = pd.DataFrame.from_dict(_is_resampling_applicable(counts))\\n            df[\'upsample\'] = self.determine_upsample(interpolation, use_cols)\\n            df[\'downsample\'] = self.determine_downsample(agg, use_cols)\\n            return df\\n\\n    default_upsample = None\\n    default_downsample = None\\n\\n    def __init__(self):\\n        self.sources = {}\\n        self.col_info = {}\\n        self.col_info[\'time\'] = ColSpec(\'time\', 0, [], \'linear\', \'mean\')\\n\\n        self._data = deque()\\n        self._time = deque()\\n        self._timestamps = deque()\\n\\n        self._timestamps_as_data = set()\\n        self._known_events = set()\\n        self._known_descriptors = set()\\n        self._stale = True\\n\\n        self.plan = self.Planner(self)\\n        self.convert_times = True\\n        self._reference_time = None\\n\\n    @property\\n    def reference_time(self):\\n        return self._reference_time\\n\\n    @reference_time.setter\\n    def reference_time(self, val):\\n        self._reference_time = pd.Timestamp(val, unit=\'s\')\\n\\n    @property\\n    def columns(self):\\n        \\"The columns of DataFrames returned by methods that return DataFrames.\\"\\n        return set(self.sources) | self._time_columns\\n\\n    @property\\n    def _time_columns(self):\\n        ts_names = [name + \'_timestamp\' for name in self._timestamps_as_data]\\n        return {\'time\'} | set(ts_names)\\n\\n    @classmethod\\n    def from_events(cls, events, verbose=False):\\n        \\"\\"\\"\\n        Create a DataMuxer from a list of Events.\\n\\n        Parameters\\n        ----------\\n        events : list\\n            list of Events (any objects with the expected attributes will do)\\n        \\"\\"\\"\\n        \\n        instance = cls()\\n        instance.append_events(events, verbose)\\n        return instance\\n\\n    def append_events(self, events, verbose=False):\\n        \\"\\"\\"Add a list of events to the DataMuxer.\\n\\n        Parameters\\n        ----------\\n        events : list\\n            list of Events (any objects with the expected attributes will do)\\n        \\"\\"\\"\\n        for idx, event in enumerate(events):\\n            if verbose and idx % 25 == 0:\\n                print(\'loading event %s\' % idx),\\n            self.append_event(event)\\n\\n    def append_event(self, event):\\n        \\"\\"\\"Add an event to the DataMuxer.\\n\\n        Parameters\\n        ----------\\n        event : Event\\n            Event Document or any object with the expected attributes\\n\\n        Returns\\n        -------\\n        is_new : bool\\n            True if event was added, False is it has already been added\\n        \\"\\"\\"\\n        if event.uid in self._known_events:\\n            return False\\n        self._known_events.add(event.uid)\\n        self._stale = True\\n        if event.descriptor.uid not in self._known_descriptors:\\n            self._process_new_descriptor(event.descriptor)\\n        # Both scalar and nonscalar data will get stored in the DataFrame.\\n        # This may be optimized later, but it might not actually help much.\\n        self._data.append(\\n            {name: data for name, data in six.iteritems(event.data)})\\n        self._timestamps.append(\\n            {name: ts for name, ts in six.iteritems(event.timestamps)})\\n        self._time.append(event.time)\\n        return True\\n\\n    def _process_new_descriptor(self, descriptor):\\n        \\"Build a ColSpec and update state.\\"\\n        for name, description in six.iteritems(descriptor.data_keys):\\n\\n            # If we already have this source name, the unique source\\n            # identifiers must match. Ambiguous names are not allowed.\\n            if name in self.sources:\\n                if self.sources[name] != description[\'source\']:\\n                    raise ValueError(\\"In a previously loaded descriptor, \\"\\n                                     \\"\'{0}\' refers to {1} but in Event \\"\\n                                     \\"Descriptor {2} it refers to {3}.\\".format(\\n                                         name, self.sources[name],\\n                                         descriptor.uid,\\n                                         description[\'source\']))\\n                if name == \'time\':\\n                    # We can argue later about how best to handle this corner\\n                    # case, but anything is better than silently mislabeling\\n                    # data.\\n                    raise ValueError(\\"The name \'time\' is reserved and cannot \\"\\n                                     \\"be used as an alias.\\")\\n\\n            # If it is a new name, determine a ColSpec.\\n            else:\\n                self.sources[name] = description[\'source\']\\n                if \'external\' in description and \'shape\' in description:\\n                    shape = description[\'shape\']\\n                    ndim = len(shape)\\n                else:\\n                    # External data can be scalar. Nonscalar data must\\n                    # have a specified shape. Thus, if no shape is given,\\n                    # assume scalar.\\n                    shape = None\\n                    ndim = 0\\n                upsample = self.default_upsample\\n                if ndim \\u003e 0:\\n                    upsample = None\\n\\n                col_info = ColSpec(name, ndim, shape, upsample,\\n                                   self.default_downsample)  # defaults\\n                # TODO Look up source-specific default in a config file\\n                # or some other source of reference data.\\n                self.col_info[name] = col_info\\n        self._known_descriptors.add(descriptor.uid)\\n\\n    @property\\n    def _dataframe(self):\\n        \\"See also to_sparse_dataframe, the public version of this.\\"\\n        # Rebuild the DataFrame if more data has been added.\\n        if self._stale:\\n            df = pd.DataFrame(list(self._data))\\n            df[\'time\'] = list(self._time)\\n            if self._timestamps_as_data:\\n                # Only build this if we need it.\\n                # TODO: We shouldn\'t have to build\\n                # the whole thing, but there is already a lot of trickiness\\n                # here so we\'ll worry about optimization later.\\n                timestamps = pd.DataFrame(list(self._timestamps))\\n            for source_name in self._timestamps_as_data:\\n                col_name = _timestamp_col_name(source_name)\\n                df[col_name] = timestamps[source_name]\\n                logger.debug(\\"Including %s timestamps as data\\", source_name)\\n            self._df = df.sort(\'time\').reset_index(drop=True)\\n            self._stale = False\\n        return self._df\\n\\n    def to_sparse_dataframe(self, include_all_timestamps=False):\\n        \\"\\"\\"Obtain all measurements in a DataFrame, one row per Event time.\\n\\n        Parameters\\n        ----------\\n        include_all_timestamps : bool\\n            The result will always contain a \'time\' column but, by default,\\n            not timestamps for individual data sources like \'motor_timestamp\'.\\n            Set this to True to export timestamp columns for each data column\\n\\n        Returns\\n        -------\\n        df : pandas.DataFrame\\n        \\"\\"\\"\\n        if include_all_timestamps:\\n            raise NotImplementedError(\\"TODO\\")\\n\\n        result = self._dataframe.copy()\\n        for col_name in self._time_columns:\\n            result[col_name] = self._maybe_convert_times(result[col_name])\\n        return result\\n\\n    def _maybe_convert_times(self, data):\\n        if self.convert_times:\\n            t = pd.to_datetime(data, unit=\'s\', utc=True).dt.tz_localize(TZ)\\n            if self.reference_time is None:\\n                return t\\n            else:\\n                return t - self.reference_time\\n        return data  # no-op\\n\\n    def include_timestamp_data(self, source_name):\\n        \\"\\"\\"Add the exact timing of a data source as a data column.\\n\\n        Parameters\\n        ----------\\n        source_name : string\\n            one of the source names in DataMuxer.sources\\n        \\"\\"\\"\\n        # self._timestamps_as_data is a set of sources who timestamps\\n        # should be treated as data in the _dataframe method above.\\n        self._timestamps_as_data.add(source_name)\\n        name = _timestamp_col_name(source_name)\\n        self.col_info[name] = ColSpec(name, 0, None, None, np.mean)\\n        self._stale = True\\n\\n    def remove_timestamp_data(self, source_name):\\n        \\"\\"\\"Remove the exact timing of a data source from the data columns.\\n\\n        Parameters\\n        ----------\\n        source_name : string\\n            one of the source names in DataMuxer.sources\\n        \\"\\"\\"\\n        self._timestamps_as_data.remove(source_name)\\n        # Do not force a rebuilt (i.e., self._stale). Just remove it here.\\n        del self._df[_timestamp_col_name(source_name)]\\n\\n    def bin_on(self, source_name, interpolation=None, agg=None, use_cols=None):\\n        \\"\\"\\"\\n        Return data resampled to align with the data from a particular source.\\n\\n        Parameters\\n        ----------\\n        source_name : string\\n        interpolation : dict, optional\\n            Override the default interpolation (upsampling) behavior of any\\n            data source by passing a dictionary of source names mapped onto\\n            one of the following interpolation methods.\\n\\n            {None, \'linear\', \'nearest\', \'zero\', \'slinear\', \'quadratic\',\\n             \'cubic\'}\\n\\n            None means that each time bin must have at least one value.\\n            See scipy.interpolator for more on the other methods.\\n        agg : dict, optional\\n            Override the default reduction (downsampling) behavior of any data\\n            source by passing a dictionary of source names mapped onto any\\n            callable that reduces multiple data points (of whatever dimension)\\n            to a single data point.\\n        use_cols : list, optional\\n            List of columns to include in binning; use all columns by default.\\n\\n        Returns\\n        -------\\n        resampled_df : pandas.DataFrame\\n\\n        References\\n        ----------\\n        http://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html\\n        \\"\\"\\"\\n        centers, bin_edges = self._bin_on(source_name)\\n        return self.bin_by_edges(bin_edges, bin_anchors=centers,\\n                                 interpolation=interpolation, agg=agg,\\n                                 use_cols=use_cols)\\n\\n    def _bin_on(self, source_name):\\n        \\"Compute bin edges spaced around centers defined by source_name points.\\"\\n        col = self._dataframe[source_name]\\n        centers = self._dataframe[\'time\'].reindex_like(col.dropna()).values\\n\\n        # [2, 4, 6] -\\u003e [-inf, 3, 5, inf]\\n        bin_edges = np.mean([centers[1:], centers[:-1]], 0)\\n        # [-inf, 3, 5, inf] -\\u003e [(-inf, 3), (3, 5), (5, inf)]\\n        bin_edges = [-np.inf] + list(np.repeat(bin_edges, 2)) + [np.inf]\\n        bin_edges = np.reshape(bin_edges, (-1, 2))\\n        return centers, bin_edges\\n\\n    def bin_by_edges(self, bin_edges, bin_anchors, interpolation=None, agg=None,\\n                     use_cols=None):\\n        \\"\\"\\"\\n        Return data resampled into bins with the specified edges.\\n\\n        Parameters\\n        ----------\\n        bin_edges : list\\n            list of two-element items like [(t1, t2), (t3, t4), ...]\\n        bin_anchors : list\\n            These are time points where interpolated values will be evaluated.\\n            Bin centers are usually a good choice.\\n        interpolation : dict, optional\\n            Override the default interpolation (upsampling) behavior of any\\n            data source by passing a dictionary of source names mapped onto\\n            one of the following interpolation methods.\\n\\n            {None, \'linear\', \'nearest\', \'zero\', \'slinear\', \'quadratic\',\\n             \'cubic\'}\\n\\n            None means that each time bin must have at least one value.\\n            See scipy.interpolator for more on the other methods.\\n        agg : dict, optional\\n            Override the default reduction (downsampling) behavior of any data\\n            source by passing a dictionary of source names mapped onto any\\n            callable that reduces multiple data points (of whatever dimension)\\n            to a single data point.\\n        use_cols : list, optional\\n            List of columns to include in binning; use all columns by default.\\n\\n        Returns\\n        -------\\n        resampled_df : pandas.DataFrame\\n\\n        References\\n        ----------\\n        http://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html\\n        \\"\\"\\"\\n        bin_anchors, binning = self._bin_by_edges(bin_anchors, bin_edges)\\n        return self.resample(bin_anchors, binning, interpolation, agg,\\n                             use_cols=use_cols)\\n\\n    def _bin_by_edges(self, bin_anchors, bin_edges):\\n        \\"Compute bin assignment and, if needed, bin_anchors.\\"\\n        time = self._dataframe[\'time\'].values\\n        # Get edges into 1D array[L, R, L, R, ...]\\n        edges_as_pairs = np.reshape(bin_edges, (-1, 2))\\n        all_edges = np.ravel(edges_as_pairs)\\n        if not np.all(np.diff(all_edges) \\u003e= 0):\\n            raise ValueError(\\"Illegal binning: the left edge must be less \\"\\n                             \\"than the right edge.\\")\\n        # Sort out where the array each time would be inserted.\\n        binning = np.searchsorted(all_edges, time).astype(float)\\n        # Times that would get inserted at even positions are between bins.\\n        # Mark them\\n        binning[binning % 2 == 0] = np.nan\\n        binning //= 2  # Make bin number sequential, not odds only.\\n        if bin_anchors is None:\\n            bin_anchors = np.mean(edges_as_pairs, axis=1)  # bin centers\\n        else:\\n            if len(bin_anchors) != len(bin_edges):\\n                raise ValueError(\\"There are {0} bin_anchors but {1} pairs of \\"\\n                                 \\"bin_edges. These must match.\\".format(\\n                                     len(bin_anchors), len(bin_edges)))\\n        return bin_anchors, binning\\n\\n    def resample(self, bin_anchors, binning, interpolation=None, agg=None,\\n                 verify_integrity=True, use_cols=None):\\n        \\"\\"\\"\\n        Return data resampled into bins with the specified edges.\\n\\n        Parameters\\n        ----------\\n        bin_anchors : list\\n            These are time points where interpolated values will be evaluated.\\n            Bin centers are usually a good choice.\\n        bin_anchors : list\\n            Bin assignment. Example: [1, 1, 2, 2, 3, 3] puts six data points\\n            into three bins with two points each.\\n        interpolation : dict, optional\\n            Override the default interpolation (upsampling) behavior of any\\n            data source by passing a dictionary of source names mapped onto\\n            one of the following interpolation methods.\\n\\n            {None, \'linear\', \'nearest\', \'zero\', \'slinear\', \'quadratic\',\\n             \'cubic\'}\\n\\n            None means that each time bin must have at least one value.\\n            See scipy.interpolator for more on the other methods.\\n        agg : dict, optional\\n            Override the default reduction (downsampling) behavior of any data\\n            source by passing a dictionary of source names mapped onto any\\n            callable that reduces multiple data points (of whatever dimension)\\n            to a single data point.\\n        verify_integrity : bool, optional\\n            For a cost in performance, verify that the downsampling function\\n            produces data of the expected shape. True by default.\\n        use_cols : list, optional\\n            List of columns to include in binning; use all columns by default.\\n\\n        Returns\\n        -------\\n        resampled_df : pandas.DataFrame\\n\\n        References\\n        ----------\\n        http://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html\\n        \\"\\"\\"\\n        if use_cols is None:\\n            use_cols = self.columns\\n        plan = self.Planner(self)\\n        upsampling_rules = plan.determine_upsample(interpolation, use_cols)\\n        downsampling_rules = plan.determine_downsample(agg, use_cols)\\n        grouped = self._dataframe.groupby(binning)\\n        first_point = grouped.first()\\n        counts = grouped.count()\\n        resampling_requirements = _is_resampling_applicable(counts)\\n        index = np.arange(len(bin_anchors))\\n        result = {}  # dict of DataFrames, to become one MultiIndexed DataFrame\\n        for name in use_cols:\\n            upsample = upsampling_rules[name]\\n            downsample = downsampling_rules[name]\\n            upsampling_possible = resampling_requirements[\'upsampling_possible\'][name]\\n            downsampling_needed = resampling_requirements[\'downsampling_needed\'][name]\\n            result[name] = pd.DataFrame(index=index)\\n            # Put the first (maybe only) value into a Series.\\n            # We will overwrite as needed below.\\n            result[name][\'val\'] = pd.Series(data=first_point[name])\\n\\n            # Short-circuit if we are done.\\n            if not (upsampling_possible or downsampling_needed):\\n                logger.debug(\\"%s has exactly one data point per bin\\", name)\\n                continue\\n\\n            result[name][\'count\'] = counts[name]\\n\\n            # If any bin has no data, use the upsampling rule to interpolate\\n            # at the center of the empty bins. If there is no rule, simply\\n            # leave some bins empty. Do not raise an error.\\n            if upsampling_possible and (upsample is not None):\\n                if upsample in (\'ffill\', \'bfill\'):\\n                    result[name][\'val\'].fillna(method=upsample, inplace=True)\\n                else:\\n                    dense_col = self._dataframe[name].dropna()\\n                    y = dense_col.values\\n                    x = self._dataframe[\'time\'].reindex_like(dense_col).values\\n                    interpolator = interp1d(x, y, kind=upsample)\\n                    # Outside the limits of the data, the interpolator will\\n                    # fail.  Leave any such entires empty.\\n                    is_safe = ((bin_anchors \\u003e np.min(x)) \\u0026\\n                               (bin_anchors \\u003c np.max(x)))\\n                    safe_times = bin_anchors[is_safe]\\n                    safe_bins = index[is_safe]\\n                    interp_points = pd.Series(interpolator(safe_times),\\n                                              index=safe_bins)\\n                    logger.debug(\\"Interpolating to fill %d of %d \\"\\n                                 \\"empty bins in %s\\",\\n                                 len(safe_bins), (counts[name] == 0).sum(),\\n                                 name)\\n                    result[name][\'val\'].fillna(interp_points, inplace=True)\\n\\n            # Short-circuit if we are done.\\n            if not downsampling_needed:\\n                logger.debug(\\"%s has at most one data point per bin\\", name)\\n                continue\\n\\n            # Multi-valued bins must be downsampled (reduced). If there is no\\n            # rule for downsampling, we have no recourse: we must raise.\\n            if (downsample is None):\\n                raise BinningError(\\"The specified binning puts multiple \\"\\n                                   \\"\'{0}\' measurements in at least one bin, \\"\\n                                   \\"and there is no rule for downsampling \\"\\n                                   \\"(i.e., reducing) it.\\".format(name))\\n            if verify_integrity and callable(downsample):\\n                downsample = _build_verified_downsample(\\n                    downsample, self.col_info[name].shape)\\n\\n            g = grouped[name]  # for brevity\\n            if self.col_info[name].ndim == 0:\\n                logger.debug(\\"The scalar column %s must be downsampled.\\", name)\\n                # For scalars, pandas knows what to do.\\n                downsampled = g.agg(downsample)\\n                std_series = g.std()\\n                max_series = g.max()\\n                min_series = g.min()\\n            else:\\n                # For nonscalars, we are abusing groupby and must go to a\\n                # a little more trouble to guarantee success.\\n                logger.debug(\\"The nonscalar column %s must be downsampled.\\",\\n                             name)\\n                if not callable(downsample):\\n                    # Do this lookup here so that strings can be passed\\n                    # in the call to resample.\\n                    downsample = ColSpec._downsample_mapping[downsample]\\n                downsampled = g.apply(lambda x: downsample(np.asarray(x.dropna())))\\n                std_series = g.apply(lambda x: np.std(np.asarray(x.dropna()), 0))\\n                max_series = g.apply(lambda x: np.max(np.asarray(x.dropna()), 0))\\n                min_series = g.apply(lambda x: np.min(np.asarray(x.dropna()), 0))\\n\\n            # This (counts[name] \\u003e 1) is redundant, but there is no clean way to\\n            # pass it here without refactoring. Not a huge cost.\\n            result[name][\'val\'].where(~(counts[name] \\u003e 1), downsampled, inplace=True)\\n            result[name][\'std\'] = std_series\\n            result[name][\'max\'] = max_series\\n            result[name][\'min\'] = min_series\\n\\n        result = pd.concat(result, axis=1)  # one MultiIndexed DataFrame\\n        result.index.name = \'bin\'\\n\\n        # Convert time timestamp or timedelta, depending on the state of\\n        # self.convert_times and self.reference_time.\\n        for col_name in self._time_columns:\\n            if isinstance(result[col_name], pd.DataFrame):\\n                subcols = result[col_name].columns\\n                for subcol in subcols \\u0026 {\'max\', \'min\', \'val\'}:\\n                    result[(col_name, subcol)] = self._maybe_convert_times(\\n                            result[(col_name, subcol)])\\n                for subcol in subcols \\u0026 {\'std\'}:\\n                    result[(col_name, subcol)] = pd.to_timedelta(\\n                            result[(col_name, subcol)], unit=\'s\')\\n            else:\\n                result[col_name] = self._maybe_convert_times(\\n                        result[col_name])\\n        return result\\n\\n    def __getitem__(self, source_name):\\n        if source_name not in list(self.col_info.keys()) + [\'time\']:\\n            raise KeyError(\\"No data from a source called \'{0}\' has been \\"\\n                           \\"added.\\".format(source_name))\\n        # Unlike output from binning functions, this is indexed\\n        # on time.\\n        result = self._dataframe[source_name].dropna()\\n        result.index = self._dataframe[\'time\'].reindex_like(result)\\n        return result\\n\\n    def __getattr__(self, attr):\\n        # Developer beware: if any properties raise an AttributeError,\\n        # this will mask it. Comment this magic method to debug properties.\\n        if attr in self.col_info.keys():\\n            return self[attr]\\n        else:\\n            raise AttributeError(\\"DataMuxer has no attribute {0} and no \\"\\n                                  \\"data source named \'{0}\'\\".format(attr))\\n\\n    @property\\n    def ncols(self):\\n        \\"\\"\\"\\n        The number of columns that the DataMuxer contains\\n        \\"\\"\\"\\n        return len(self.col_info)\\n\\n    @property\\n    def col_info_by_ndim(self):\\n        \\"\\"\\"Dictionary mapping dimensionality (ndim) onto a list of ColSpecs\\"\\"\\"\\n\\n        result = {}\\n        for name, col_spec in six.iteritems(self.col_info):\\n            try:\\n                result[col_spec.ndim]\\n            except KeyError:\\n                result[col_spec.ndim] = []\\n            result[col_spec.ndim].append(col_spec)\\n        return result\\n\\n\\ndef dataframe_to_dict(df):\\n    \\"\\"\\"\\n    Turn a DataFrame into a dict of lists.\\n\\n    Parameters\\n    ----------\\n    df : DataFrame\\n\\n    Returns\\n    -------\\n    index : ndarray\\n        The index of the data frame\\n    data : dict\\n        Dictionary keyed on column name of the column.  The value is\\n        one of (ndarray, list, pd.Series)\\n    \\"\\"\\"\\n    dict_of_lists = {col: df[col].to_list() for col in df.columns}\\n    return df.index.values, dict_of_lists\\n\\n\\ndef _build_verified_downsample(downsample, expected_shape):\\n    # Ensure two things:\\n    # 1. The downsampling function shouldn\'t touch bins with only one point.\\n    # 2. The result of downsample should have the right shape.\\n    def _downsample(data):\\n        if len(data) == 1:\\n            return data\\n        downsampled = downsample(data)\\n        if (expected_shape is None or expected_shape == 0):\\n            if not np.isscalar(downsampled):\\n                raise BadDownsamplerError(\\"The \'agg\' (downsampling) function \\"\\n                                          \\"for {0} is expected to produce \\"\\n                                          \\"a scalar from the data in each \\"\\n                                          \\"bin.\\".format(downsampled))\\n        elif downsampled.shape != expected_shape:\\n            raise BadDownsamplerError(\\"An \'agg\' (downsampling) function \\"\\n                                      \\"returns data shaped {0} but the \\"\\n                                      \\"shape {1} is expected.\\".format(\\n                                          downsampled.shape, expected_shape))\\n        return downsampled\\n    return _downsample\\n\\n\\ndef _timestamp_col_name(source_name):\\n    return \'{0}_timestamp\'.format(source_name)\\n\\n\\ndef _normalize_string_none(val):\\n    \\"Replay passes \'None\' to mean None.\\"\\n    try:\\n        lowercase_val = val.lower()\\n    except AttributeError:\\n        return val\\n    if lowercase_val == \'none\':\\n        return None\\n    else:\\n        return val\\n\\n\\ndef _is_resampling_applicable(counts):\\n    has_no_points = counts == 0\\n    has_multiple_points = counts \\u003e 1\\n    upsampling_possible = has_no_points.any()\\n    downsampling_needed = has_multiple_points.any()\\n    result = {}\\n    result[\'upsampling_possible\'] = upsampling_possible.to_dict()\\n    result[\'downsampling_needed\'] = downsampling_needed.to_dict()\\n    return result\\n"}\n'
line: b'{"repo_name":"glaubitz/fs-uae-debian","ref":"refs/heads/master","path":"launcher/launcher/game_paths.py","content":"import os\\n\\nimport fsui\\nfrom fsbc.paths import Paths\\nfrom fsgs.FSGSDirectories import FSGSDirectories\\nfrom .launcher_config import LauncherConfig\\nfrom .launcher_settings import LauncherSettings\\nfrom .ui.Constants import Constants\\n\\n\\nclass GamePaths(object):\\n    @staticmethod\\n    def current():\\n        model = LauncherConfig.get(\\"amiga_model\\")\\n        if model.startswith(\\"CD32\\"):\\n            platform = \\"CD32\\"\\n        elif model == \\"CDTV\\":\\n            platform = \\"CDTV\\"\\n        else:\\n            platform = \\"Amiga\\"\\n        name = LauncherSettings.get(\\"config_name\\")\\n        uuid = LauncherConfig.get(\\"x_game_uuid\\")\\n        return GamePaths(name, platform, uuid)\\n\\n    def __init__(self, name, platform, uuid):\\n        self.uuid = uuid\\n\\n        self.config_name = name\\n        if \\"(\\" in name:\\n            parts = name.split(\\"(\\", 1)\\n            self.name, self.variant = parts\\n            self.name = self.name.strip()\\n            self.variant = self.variant.strip()\\n            if self.variant.endswith(\\")\\"):\\n                self.variant = self.variant[:-1]\\n            self.variant = self.variant.replace(\\") (\\", \\", \\")\\n            self.variant = self.variant.replace(\\")(\\", \\", \\")\\n        else:\\n            self.name = name\\n            self.variant = \\"\\"\\n        self.platform = platform\\n\\n    def get_name(self):\\n        return self.name\\n\\n    def get_variant(self):\\n        return self.variant\\n\\n    @staticmethod\\n    def get_override_path(name):\\n        path = LauncherConfig.get(name)\\n        if not path:\\n            return \\"\\"\\n        path = Paths.expand_path(path)\\n        return path\\n\\n    def get_screenshot_path(self, number):\\n        if number == 0:\\n            sha1 = LauncherConfig.get(\\"title_sha1\\")\\n        else:\\n            sha1 = LauncherConfig.get(\\"screen{0}_sha1\\".format(number))\\n        if sha1:\\n            return \\"sha1:\\" + sha1\\n\\n        if number == 0:\\n            path = self.get_override_path(\\"title_image\\")\\n        else:\\n            path = self.get_override_path(\\"screen{0}_image\\".format(number))\\n        if path and os.path.exists(path):\\n            return path\\n        if self.uuid:\\n            if number == 0:\\n                name = \\"title.png\\"\\n            else:\\n                name = \\"screen{0}.png\\".format(number)\\n            paths = FSGSDirectories.get_images_dirs()\\n            for dir_ in paths:\\n                p = os.path.join(dir_, self.platform, \\"Images\\",\\n                                 self.uuid[:2], self.uuid, name)\\n                if os.path.exists(p):\\n                    return p\\n                p = os.path.join(dir_, self.platform, \\"Thumbnails\\",\\n                                 self.uuid[:2], self.uuid, name)\\n                if os.path.exists(p):\\n                    return p\\n        letter = self.get_letter(self.name)\\n        if not letter:\\n            return None\\n        name = self.name\\n        if number == 0:\\n            override_dir = LauncherConfig.get(\\"titles_dir\\")\\n            if override_dir:\\n                paths = [Paths.expand_path(override_dir)]\\n            else:\\n                paths = FSGSDirectories.get_titles_dirs()\\n        else:\\n            override_dir = LauncherConfig.get(\\"screenshots_dir\\")\\n            if override_dir:\\n                paths = [Paths.expand_path(override_dir)]\\n            else:\\n                paths = FSGSDirectories.get_screenshots_dirs()\\n        if number \\u003e= 2:\\n            name = \\"{0}_{1}\\".format(name, number)\\n        for dir_ in paths:\\n            path = os.path.join(dir_, letter, name + \\".png\\")\\n            if os.path.exists(path):\\n                return path\\n            path = os.path.join(dir_, letter, name + \\".gif\\")\\n            if os.path.exists(path):\\n                return path\\n            path = os.path.join(dir_, name + \\".png\\")\\n            if os.path.exists(path):\\n                return path\\n            path = os.path.join(dir_, letter, name + \\".gif\\")\\n            if os.path.exists(path):\\n                return path\\n        return None\\n\\n    def load_screenshot(self, number):\\n        path = self.get_screenshot_path(number)\\n        if path:\\n            return fsui.Image(path)\\n\\n    def load_screenshot_preview(self, number):\\n        image = self.load_screenshot(number)\\n        if image is None:\\n            return image\\n        if image.size == Constants.SCREEN_SIZE:\\n            return image\\n        if image.size[0] \\u003c 400:\\n            image.resize((image.size[0] * 2, image.size[1] * 2),\\n                         fsui.Image.NEAREST)\\n        image.resize(Constants.SCREEN_SIZE)\\n        return image\\n\\n    def get_cover_path(self):\\n        sha1 = LauncherConfig.get(\\"front_sha1\\")\\n        if sha1:\\n            return \\"sha1:\\" + sha1\\n\\n        path = self.get_override_path(\\"cover_image\\")\\n        if path and os.path.exists(path):\\n            return path\\n        if self.uuid:\\n            paths = FSGSDirectories.get_images_dirs()\\n            for dir_ in paths:\\n                p = os.path.join(dir_, self.platform, \\"Images\\",\\n                                 self.uuid[:2], self.uuid, \\"front.png\\")\\n                if os.path.exists(p):\\n                    return p\\n                p = os.path.join(dir_, self.platform, \\"Thumbnails\\",\\n                                 self.uuid[:2], self.uuid, \\"front.png\\")\\n                if os.path.exists(p):\\n                    return p\\n        letter = self.get_letter(self.name)\\n        if not letter:\\n            return None\\n        name = self.name\\n        override_dir = LauncherConfig.get(\\"covers_dir\\")\\n        if override_dir:\\n            paths = [Paths.expand_path(override_dir)]\\n        else:\\n            paths = FSGSDirectories.get_covers_dirs()\\n        for dir_ in paths:\\n            path = os.path.join(dir_, letter, name + \\".jpg\\")\\n            if os.path.exists(path):\\n                return path\\n            path = os.path.join(dir_, letter, name + \\".png\\")\\n            if os.path.exists(path):\\n                return path\\n            path = os.path.join(dir_, name + \\".jpg\\")\\n            if os.path.exists(path):\\n                return path\\n            path = os.path.join(dir_, name + \\".png\\")\\n            if os.path.exists(path):\\n                return path\\n        return None\\n\\n    def load_cover(self):\\n        path = self.get_cover_path()\\n        print(path)\\n        if path:\\n            return fsui.Image(path)\\n\\n    def load_cover_preview(self):\\n        image = self.load_cover()\\n        if image is None:\\n            return image\\n        image.resize(Constants.COVER_SIZE)\\n        return image\\n\\n    def get_theme_path(self):\\n        letter = self.get_letter(self.name)\\n        if not letter:\\n            return None\\n        paths = FSGSDirectories.get_themes_dirs()\\n        for dir_ in paths:\\n            path = os.path.join(dir_, letter, self.name)\\n            if os.path.exists(path):\\n                return path\\n        return None\\n\\n    def _get_state_dir(self):\\n        config_name = self.config_name\\n        if not config_name:\\n            config_name = \\"Default\\"\\n\\n        # use a temporary state dir, for now, to avoid problems with\\n        # floppy overlays etc interfering with net play\\n        from .netplay.netplay import Netplay\\n        if Netplay.current():\\n            # it is possible to manually specify the state dir\\n            config_name = LauncherConfig.get(\\"__netplay_state_dir_name\\")\\n            if not config_name:\\n                # this is the default behavior, create a clean state\\n                # dir for the net play session\\n                netplay_game = LauncherConfig.get(\\"__netplay_game\\")\\n                if netplay_game:\\n                    config_name = \\"Net Play ({0})\\".format(netplay_game)\\n\\n        letter = self.get_letter(config_name)\\n        if not letter:\\n            config_name = \\"Default\\"\\n            letter = self.get_letter(config_name)\\n        # we use an existing state dir in a \\"letter\\" dir if it exists\\n        # (legacy support).\\n        path = os.path.join(FSGSDirectories.get_save_states_dir(), letter,\\n                            config_name)\\n        if os.path.exists(path):\\n            return path\\n        # if not, we use a direct sub-folder of save states dir\\n        path = os.path.join(FSGSDirectories.get_save_states_dir(),\\n                            config_name)\\n        return path\\n\\n    def get_state_dir(self):\\n        state_dir = self._get_state_dir()\\n        if not os.path.exists(state_dir):\\n            os.makedirs(state_dir)\\n        return state_dir\\n\\n    @staticmethod\\n    def get_letter(name):\\n        letter_name = name.upper()\\n        if letter_name.startswith(\\"THE \\"):\\n            letter_name = letter_name[4:]\\n        if letter_name.startswith(\\"A \\"):\\n            letter_name = letter_name[2:]\\n        for i in range(len(letter_name)):\\n            letter = letter_name[i]\\n            if letter in \\"01234567890\\":\\n                letter = \\"0\\"\\n                break\\n            if letter in \\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\\":\\n                break\\n        else:\\n            return None\\n        return letter\\n"}\n'
line: b'{"repo_name":"Nirlendu/Dummy-Search-Engine","ref":"refs/heads/master","path":"tornado-3.2/build/lib.win32-2.7/tornado/wsgi.py","content":"#!/usr/bin/env python\\n#\\n# Copyright 2009 Facebook\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n# not use this file except in compliance with the License. You may obtain\\n# a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n# License for the specific language governing permissions and limitations\\n# under the License.\\n\\n\\"\\"\\"WSGI support for the Tornado web framework.\\n\\nWSGI is the Python standard for web servers, and allows for interoperability\\nbetween Tornado and other Python web frameworks and servers.  This module\\nprovides WSGI support in two ways:\\n\\n* `WSGIApplication` is a version of `tornado.web.Application` that can run\\n  inside a WSGI server.  This is useful for running a Tornado app on another\\n  HTTP server, such as Google App Engine.  See the `WSGIApplication` class\\n  documentation for limitations that apply.\\n* `WSGIContainer` lets you run other WSGI applications and frameworks on the\\n  Tornado HTTP server.  For example, with this class you can mix Django\\n  and Tornado handlers in a single server.\\n\\"\\"\\"\\n\\nfrom __future__ import absolute_import, division, print_function, with_statement\\n\\nimport sys\\nimport time\\nimport copy\\nimport tornado\\n\\nfrom tornado import escape\\nfrom tornado import httputil\\nfrom tornado.log import access_log\\nfrom tornado import web\\nfrom tornado.escape import native_str, parse_qs_bytes\\nfrom tornado.util import bytes_type, unicode_type\\n\\ntry:\\n    from io import BytesIO  # python 3\\nexcept ImportError:\\n    from cStringIO import StringIO as BytesIO  # python 2\\n\\ntry:\\n    import Cookie  # py2\\nexcept ImportError:\\n    import http.cookies as Cookie  # py3\\n\\ntry:\\n    import urllib.parse as urllib_parse  # py3\\nexcept ImportError:\\n    import urllib as urllib_parse\\n\\n# PEP 3333 specifies that WSGI on python 3 generally deals with byte strings\\n# that are smuggled inside objects of type unicode (via the latin1 encoding).\\n# These functions are like those in the tornado.escape module, but defined\\n# here to minimize the temptation to use them in non-wsgi contexts.\\nif str is unicode_type:\\n    def to_wsgi_str(s):\\n        assert isinstance(s, bytes_type)\\n        return s.decode(\'latin1\')\\n\\n    def from_wsgi_str(s):\\n        assert isinstance(s, str)\\n        return s.encode(\'latin1\')\\nelse:\\n    def to_wsgi_str(s):\\n        assert isinstance(s, bytes_type)\\n        return s\\n\\n    def from_wsgi_str(s):\\n        assert isinstance(s, str)\\n        return s\\n\\n\\nclass WSGIApplication(web.Application):\\n    \\"\\"\\"A WSGI equivalent of `tornado.web.Application`.\\n\\n    `WSGIApplication` is very similar to `tornado.web.Application`,\\n    except no asynchronous methods are supported (since WSGI does not\\n    support non-blocking requests properly). If you call\\n    ``self.flush()`` or other asynchronous methods in your request\\n    handlers running in a `WSGIApplication`, we throw an exception.\\n\\n    Example usage::\\n\\n        import tornado.web\\n        import tornado.wsgi\\n        import wsgiref.simple_server\\n\\n        class MainHandler(tornado.web.RequestHandler):\\n            def get(self):\\n                self.write(\\"Hello, world\\")\\n\\n        if __name__ == \\"__main__\\":\\n            application = tornado.wsgi.WSGIApplication([\\n                (r\\"/\\", MainHandler),\\n            ])\\n            server = wsgiref.simple_server.make_server(\'\', 8888, application)\\n            server.serve_forever()\\n\\n    See the `appengine demo\\n    \\u003chttps://github.com/facebook/tornado/tree/master/demos/appengine\\u003e`_\\n    for an example of using this module to run a Tornado app on Google\\n    App Engine.\\n\\n    WSGI applications use the same `.RequestHandler` class, but not\\n    ``@asynchronous`` methods or ``flush()``.  This means that it is\\n    not possible to use `.AsyncHTTPClient`, or the `tornado.auth` or\\n    `tornado.websocket` modules.\\n    \\"\\"\\"\\n    def __init__(self, handlers=None, default_host=\\"\\", **settings):\\n        web.Application.__init__(self, handlers, default_host, transforms=[],\\n                                 wsgi=True, **settings)\\n\\n    def __call__(self, environ, start_response):\\n        handler = web.Application.__call__(self, HTTPRequest(environ))\\n        assert handler._finished\\n        reason = handler._reason\\n        status = str(handler._status_code) + \\" \\" + reason\\n        headers = list(handler._headers.get_all())\\n        if hasattr(handler, \\"_new_cookie\\"):\\n            for cookie in handler._new_cookie.values():\\n                headers.append((\\"Set-Cookie\\", cookie.OutputString(None)))\\n        start_response(status,\\n                       [(native_str(k), native_str(v)) for (k, v) in headers])\\n        return handler._write_buffer\\n\\n\\nclass HTTPRequest(object):\\n    \\"\\"\\"Mimics `tornado.httpserver.HTTPRequest` for WSGI applications.\\"\\"\\"\\n    def __init__(self, environ):\\n        \\"\\"\\"Parses the given WSGI environment to construct the request.\\"\\"\\"\\n        self.method = environ[\\"REQUEST_METHOD\\"]\\n        self.path = urllib_parse.quote(from_wsgi_str(environ.get(\\"SCRIPT_NAME\\", \\"\\")))\\n        self.path += urllib_parse.quote(from_wsgi_str(environ.get(\\"PATH_INFO\\", \\"\\")))\\n        self.uri = self.path\\n        self.arguments = {}\\n        self.query_arguments = {}\\n        self.body_arguments = {}\\n        self.query = environ.get(\\"QUERY_STRING\\", \\"\\")\\n        if self.query:\\n            self.uri += \\"?\\" + self.query\\n            self.arguments = parse_qs_bytes(native_str(self.query),\\n                                            keep_blank_values=True)\\n            self.query_arguments = copy.deepcopy(self.arguments)\\n        self.version = \\"HTTP/1.1\\"\\n        self.headers = httputil.HTTPHeaders()\\n        if environ.get(\\"CONTENT_TYPE\\"):\\n            self.headers[\\"Content-Type\\"] = environ[\\"CONTENT_TYPE\\"]\\n        if environ.get(\\"CONTENT_LENGTH\\"):\\n            self.headers[\\"Content-Length\\"] = environ[\\"CONTENT_LENGTH\\"]\\n        for key in environ:\\n            if key.startswith(\\"HTTP_\\"):\\n                self.headers[key[5:].replace(\\"_\\", \\"-\\")] = environ[key]\\n        if self.headers.get(\\"Content-Length\\"):\\n            self.body = environ[\\"wsgi.input\\"].read(\\n                int(self.headers[\\"Content-Length\\"]))\\n        else:\\n            self.body = \\"\\"\\n        self.protocol = environ[\\"wsgi.url_scheme\\"]\\n        self.remote_ip = environ.get(\\"REMOTE_ADDR\\", \\"\\")\\n        if environ.get(\\"HTTP_HOST\\"):\\n            self.host = environ[\\"HTTP_HOST\\"]\\n        else:\\n            self.host = environ[\\"SERVER_NAME\\"]\\n\\n        # Parse request body\\n        self.files = {}\\n        httputil.parse_body_arguments(self.headers.get(\\"Content-Type\\", \\"\\"),\\n                                      self.body, self.body_arguments, self.files)\\n\\n        for k, v in self.body_arguments.items():\\n            self.arguments.setdefault(k, []).extend(v)\\n\\n        self._start_time = time.time()\\n        self._finish_time = None\\n\\n    def supports_http_1_1(self):\\n        \\"\\"\\"Returns True if this request supports HTTP/1.1 semantics\\"\\"\\"\\n        return self.version == \\"HTTP/1.1\\"\\n\\n    @property\\n    def cookies(self):\\n        \\"\\"\\"A dictionary of Cookie.Morsel objects.\\"\\"\\"\\n        if not hasattr(self, \\"_cookies\\"):\\n            self._cookies = Cookie.SimpleCookie()\\n            if \\"Cookie\\" in self.headers:\\n                try:\\n                    self._cookies.load(\\n                        native_str(self.headers[\\"Cookie\\"]))\\n                except Exception:\\n                    self._cookies = None\\n        return self._cookies\\n\\n    def full_url(self):\\n        \\"\\"\\"Reconstructs the full URL for this request.\\"\\"\\"\\n        return self.protocol + \\"://\\" + self.host + self.uri\\n\\n    def request_time(self):\\n        \\"\\"\\"Returns the amount of time it took for this request to execute.\\"\\"\\"\\n        if self._finish_time is None:\\n            return time.time() - self._start_time\\n        else:\\n            return self._finish_time - self._start_time\\n\\n\\nclass WSGIContainer(object):\\n    r\\"\\"\\"Makes a WSGI-compatible function runnable on Tornado\'s HTTP server.\\n\\n    Wrap a WSGI function in a `WSGIContainer` and pass it to `.HTTPServer` to\\n    run it. For example::\\n\\n        def simple_app(environ, start_response):\\n            status = \\"200 OK\\"\\n            response_headers = [(\\"Content-type\\", \\"text/plain\\")]\\n            start_response(status, response_headers)\\n            return [\\"Hello world!\\\\n\\"]\\n\\n        container = tornado.wsgi.WSGIContainer(simple_app)\\n        http_server = tornado.httpserver.HTTPServer(container)\\n        http_server.listen(8888)\\n        tornado.ioloop.IOLoop.instance().start()\\n\\n    This class is intended to let other frameworks (Django, web.py, etc)\\n    run on the Tornado HTTP server and I/O loop.\\n\\n    The `tornado.web.FallbackHandler` class is often useful for mixing\\n    Tornado and WSGI apps in the same server.  See\\n    https://github.com/bdarnell/django-tornado-demo for a complete example.\\n    \\"\\"\\"\\n    def __init__(self, wsgi_application):\\n        self.wsgi_application = wsgi_application\\n\\n    def __call__(self, request):\\n        data = {}\\n        response = []\\n\\n        def start_response(status, response_headers, exc_info=None):\\n            data[\\"status\\"] = status\\n            data[\\"headers\\"] = response_headers\\n            return response.append\\n        app_response = self.wsgi_application(\\n            WSGIContainer.environ(request), start_response)\\n        try:\\n            response.extend(app_response)\\n            body = b\\"\\".join(response)\\n        finally:\\n            if hasattr(app_response, \\"close\\"):\\n                app_response.close()\\n        if not data:\\n            raise Exception(\\"WSGI app did not call start_response\\")\\n\\n        status_code = int(data[\\"status\\"].split()[0])\\n        headers = data[\\"headers\\"]\\n        header_set = set(k.lower() for (k, v) in headers)\\n        body = escape.utf8(body)\\n        if status_code != 304:\\n            if \\"content-length\\" not in header_set:\\n                headers.append((\\"Content-Length\\", str(len(body))))\\n            if \\"content-type\\" not in header_set:\\n                headers.append((\\"Content-Type\\", \\"text/html; charset=UTF-8\\"))\\n        if \\"server\\" not in header_set:\\n            headers.append((\\"Server\\", \\"TornadoServer/%s\\" % tornado.version))\\n\\n        parts = [escape.utf8(\\"HTTP/1.1 \\" + data[\\"status\\"] + \\"\\\\r\\\\n\\")]\\n        for key, value in headers:\\n            parts.append(escape.utf8(key) + b\\": \\" + escape.utf8(value) + b\\"\\\\r\\\\n\\")\\n        parts.append(b\\"\\\\r\\\\n\\")\\n        parts.append(body)\\n        request.write(b\\"\\".join(parts))\\n        request.finish()\\n        self._log(status_code, request)\\n\\n    @staticmethod\\n    def environ(request):\\n        \\"\\"\\"Converts a `tornado.httpserver.HTTPRequest` to a WSGI environment.\\n        \\"\\"\\"\\n        hostport = request.host.split(\\":\\")\\n        if len(hostport) == 2:\\n            host = hostport[0]\\n            port = int(hostport[1])\\n        else:\\n            host = request.host\\n            port = 443 if request.protocol == \\"https\\" else 80\\n        environ = {\\n            \\"REQUEST_METHOD\\": request.method,\\n            \\"SCRIPT_NAME\\": \\"\\",\\n            \\"PATH_INFO\\": to_wsgi_str(escape.url_unescape(\\n                request.path, encoding=None, plus=False)),\\n            \\"QUERY_STRING\\": request.query,\\n            \\"REMOTE_ADDR\\": request.remote_ip,\\n            \\"SERVER_NAME\\": host,\\n            \\"SERVER_PORT\\": str(port),\\n            \\"SERVER_PROTOCOL\\": request.version,\\n            \\"wsgi.version\\": (1, 0),\\n            \\"wsgi.url_scheme\\": request.protocol,\\n            \\"wsgi.input\\": BytesIO(escape.utf8(request.body)),\\n            \\"wsgi.errors\\": sys.stderr,\\n            \\"wsgi.multithread\\": False,\\n            \\"wsgi.multiprocess\\": True,\\n            \\"wsgi.run_once\\": False,\\n        }\\n        if \\"Content-Type\\" in request.headers:\\n            environ[\\"CONTENT_TYPE\\"] = request.headers.pop(\\"Content-Type\\")\\n        if \\"Content-Length\\" in request.headers:\\n            environ[\\"CONTENT_LENGTH\\"] = request.headers.pop(\\"Content-Length\\")\\n        for key, value in request.headers.items():\\n            environ[\\"HTTP_\\" + key.replace(\\"-\\", \\"_\\").upper()] = value\\n        return environ\\n\\n    def _log(self, status_code, request):\\n        if status_code \\u003c 400:\\n            log_method = access_log.info\\n        elif status_code \\u003c 500:\\n            log_method = access_log.warning\\n        else:\\n            log_method = access_log.error\\n        request_time = 1000.0 * request.request_time()\\n        summary = request.method + \\" \\" + request.uri + \\" (\\" + \\\\\\n            request.remote_ip + \\")\\"\\n        log_method(\\"%d %s %.2fms\\", status_code, summary, request_time)\\n"}\n'
line: b'{"repo_name":"octacoin-project/beta","ref":"refs/heads/master","path":"share/qt/extract_strings_qt.py","content":"#!/usr/bin/python\\n\'\'\'\\nExtract _(\\"...\\") strings for translation and convert to Qt4 stringdefs so that\\nthey can be picked up by Qt linguist.\\n\'\'\'\\nfrom subprocess import Popen, PIPE\\nimport glob\\nimport operator\\nimport os\\nimport sys\\n\\nOUT_CPP=\\"qt/bitcoinstrings.cpp\\"\\nEMPTY=[\'\\"\\"\']\\n\\ndef parse_po(text):\\n    \\"\\"\\"\\n    Parse \'po\' format produced by xgettext.\\n    Return a list of (msgid,msgstr) tuples.\\n    \\"\\"\\"\\n    messages = []\\n    msgid = []\\n    msgstr = []\\n    in_msgid = False\\n    in_msgstr = False\\n\\n    for line in text.split(\'\\\\n\'):\\n        line = line.rstrip(\'\\\\r\')\\n        if line.startswith(\'msgid \'):\\n            if in_msgstr:\\n                messages.append((msgid, msgstr))\\n                in_msgstr = False\\n            # message start\\n            in_msgid = True\\n            \\n            msgid = [line[6:]]\\n        elif line.startswith(\'msgstr \'):\\n            in_msgid = False\\n            in_msgstr = True\\n            msgstr = [line[7:]]\\n        elif line.startswith(\'\\"\'):\\n            if in_msgid:\\n                msgid.append(line)\\n            if in_msgstr:\\n                msgstr.append(line)\\n\\n    if in_msgstr:\\n        messages.append((msgid, msgstr))\\n\\n    return messages\\n\\nfiles = sys.argv[1:]\\n\\n# xgettext -n --keyword=_ $FILES\\nXGETTEXT=os.getenv(\'XGETTEXT\', \'xgettext\')\\nchild = Popen([XGETTEXT,\'--output=-\',\'-n\',\'--keyword=_\'] + files, stdout=PIPE)\\n(out, err) = child.communicate()\\n\\nmessages = parse_po(out) \\n\\nf = open(OUT_CPP, \'w\')\\nf.write(\\"\\"\\"\\n\\n#include \\u003cQtGlobal\\u003e\\n\\n// Automatically generated by extract_strings.py\\n#ifdef __GNUC__\\n#define UNUSED __attribute__((unused))\\n#else\\n#define UNUSED\\n#endif\\n\\"\\"\\")\\nf.write(\'static const char UNUSED *bitcoin_strings[] = {\\\\n\')\\nmessages.sort(key=operator.itemgetter(0))\\nfor (msgid, msgstr) in messages:\\n    if msgid != EMPTY:\\n        f.write(\'QT_TRANSLATE_NOOP(\\"bitcoin-core\\", %s),\\\\n\' % (\'\\\\n\'.join(msgid)))\\nf.write(\'};\\\\n\')\\nf.close()\\n"}\n'
line: b'{"repo_name":"FHannes/intellij-community","ref":"refs/heads/master","path":"python/testData/intentions/returnTypeInPy3Annotation2_after.py","content":"def my_func(p1=1) -\\u003e object:\\n    return p1\\n\\nd = my_func(1)"}\n'
line: b'{"repo_name":"BhallaLab/moose","ref":"refs/heads/master","path":"moose-examples/passive/passive_soma.py","content":"\\"\\"\\" passive_soma.py: \\n\\nIn this script, we simulate a single compartment soma in MOOSE.\\n\\nThis soma does not have any ion-channels, only passive properties. It should\\nbehave like a RC circuit. A current is injected into soma.\\n\\n\\"\\"\\"\\n    \\n__author__           = \\"Dilawar Singh\\"\\n__copyright__        = \\"Copyright 2015, Dilawar Singh and NCBS Bangalore\\"\\n__credits__          = [\\"NCBS Bangalore\\"]\\n__license__          = \\"GNU GPL\\"\\n__version__          = \\"1.0.0\\"\\n__maintainer__       = \\"Dilawar Singh\\"\\n__email__            = \\"dilawars@ncbs.res.in\\"\\n__status__           = \\"Development\\"\\n\\n   \\nimport moose\\nimport pylab\\n\\n\\nmodel = None\\nsoma = None \\nvmtab = None\\n\\ndef buildModel():\\n    global model \\n    global soma\\n    model = moose.Neutral(\'/model\')\\n    soma = moose.Compartment(\'/model/soma\')\\n    soma.Em = -60e-3\\n    soma.Rm = 1e10\\n    soma.Cm = 1e-10\\n    return model\\n\\ndef stimulus():\\n    global soma\\n    global vmtab\\n    pulse = moose.PulseGen(\'/model/pulse\')\\n    pulse.delay[0] = 50e-3\\n    pulse.width[0] = 100e-3\\n    pulse.level[0] = 1e-9\\n    pulse.delay[1] = 1e9\\n    vmtab = moose.Table(\'/soma_Vm\')\\n    moose.connect(pulse, \'output\', soma, \'injectMsg\')\\n    moose.connect(vmtab, \'requestOut\', soma , \'getVm\')\\n\\ndef main():\\n    global vmtab\\n    buildModel()\\n    stimulus()\\n    moose.reinit()\\n    t = 500e-2\\n    moose.start(t)\\n    time_vector = pylab.linspace(0, t, len(vmtab.vector))\\n    pylab.plot(time_vector, vmtab.vector)\\n    pylab.show( )\\n    # pylab.savefig(\'soma_passive.png\')\\n\\nif __name__ == \'__main__\':\\n    main()\\n"}\n'
line: b'{"repo_name":"tommy-u/chaco","ref":"refs/heads/master","path":"chaco/base_plot_container.py","content":"\\"\\"\\" Defines the BasePlotContainer class.\\n\\"\\"\\"\\nimport warnings\\n\\n# Enthought library imports\\nfrom enable.api import Container\\nfrom traits.api import Bool, Instance, Property, Str, Tuple\\n\\n# Local, relative imports\\nfrom plot_component import DEFAULT_DRAWING_ORDER, PlotComponent\\n\\n\\nclass BasePlotContainer(Container):\\n    \\"\\"\\"\\n    A container for PlotComponents that conforms to being laid out by\\n    PlotFrames.  Serves as the base class for other PlotContainers.\\n\\n    PlotContainers define a layout, i.e., a spatial relationship between\\n    their contained components.  (BasePlotContainer doesn\'t define one,\\n    but its various subclasses do.)\\n\\n    BasePlotContainer is a subclass of Enable Container, so it is possible to\\n    insert Enable-level components into it.  However, because Enable\\n    components don\'t have the correct interfaces to participate in layout,\\n    the visual results will probably be incorrect.\\n    \\"\\"\\"\\n\\n    # Redefine the container layers to name the main layer as \\"plot\\" instead\\n    # of the Enable default of \\"mainlayer\\"\\n    container_under_layers = Tuple(\\"background\\", \\"image\\", \\"underlay\\", \\"plot\\")\\n\\n    #------------------------------------------------------------------------\\n    # Duplicate trait declarations from PlotComponent.  We don\'t subclass\\n    # PlotComponent to avoid MRO complications with trait handlers and property\\n    # getters/setters.\\n    #------------------------------------------------------------------------\\n\\n    draw_order = Instance(list, args=(DEFAULT_DRAWING_ORDER,))\\n    draw_layer = Str(\\"plot\\")\\n\\n    #------------------------------------------------------------------------\\n    # Deprecated traits\\n    #------------------------------------------------------------------------\\n\\n    # Deprecated flag to indicate that a component needed to do old-style\\n    # drawing.  Unused by any recent Chaco component.\\n    use_draw_order = Bool(True)\\n\\n    # Deprecated property for accessing the components in the container.\\n    plot_components = Property\\n\\n    def _get_plot_components(self):\\n        warnings.warn(\\"Use of plot_components attribute deprecated.\\" \\\\\\n                      \\"Use components attribute instead.\\", DeprecationWarning)\\n        return self._components\\n\\n    def _set_plot_components(self, new):\\n        warnings.warn(\\"Use of plot_components attribute deprecated.\\" \\\\\\n                      \\"Use components attribute instead.\\", DeprecationWarning)\\n        self._components = new\\n\\n    def _use_draw_order_changed(self, old, new):\\n        \\"\\"\\" Handler to catch the case when someone is trying to use the\\n        old-style drawing mechanism, which is now unsupported.\\n        \\"\\"\\"\\n        if new == False:\\n            raise RuntimeError(\\"The old-style drawing mechanism is no longer \\" \\\\\\n                    \\"supported in Chaco.\\")\\n\\n# EOF\\n"}\n'
line: b'{"repo_name":"factorybuild/stbgui","ref":"refs/heads/master","path":"lib/python/Screens/ChannelSelection.py","content":"from Tools.Profile import profile\\n\\nfrom Screen import Screen\\nimport Screens.InfoBar\\nimport Components.ParentalControl\\nfrom Components.Button import Button\\nfrom Components.ServiceList import ServiceList, refreshServiceList\\nfrom Components.ActionMap import NumberActionMap, ActionMap, HelpableActionMap\\nfrom Components.MenuList import MenuList\\nfrom Components.ServiceEventTracker import ServiceEventTracker, InfoBarBase\\nprofile(\\"ChannelSelection.py 1\\")\\nfrom EpgSelection import EPGSelection\\nfrom enigma import eServiceReference, eEPGCache, eServiceCenter, eRCInput, eTimer, eDVBDB, iPlayableService, iServiceInformation, getPrevAsciiCode, eEnv\\nfrom Components.config import config, configfile, ConfigSubsection, ConfigText, ConfigYesNo\\nfrom Tools.NumericalTextInput import NumericalTextInput\\nprofile(\\"ChannelSelection.py 2\\")\\nfrom Components.NimManager import nimmanager\\nprofile(\\"ChannelSelection.py 2.1\\")\\nfrom Components.Sources.RdsDecoder import RdsDecoder\\nprofile(\\"ChannelSelection.py 2.2\\")\\nfrom Components.Sources.ServiceEvent import ServiceEvent\\nfrom Components.Sources.Event import Event\\nprofile(\\"ChannelSelection.py 2.3\\")\\nfrom Components.Input import Input\\nprofile(\\"ChannelSelection.py 3\\")\\nfrom Components.ChoiceList import ChoiceList, ChoiceEntryComponent\\nfrom Components.SystemInfo import SystemInfo\\nfrom Screens.InputBox import PinInput\\nfrom Screens.VirtualKeyBoard import VirtualKeyBoard\\nfrom Screens.MessageBox import MessageBox\\nfrom Screens.ServiceInfo import ServiceInfo\\nfrom Screens.Hotkey import InfoBarHotkey, hotkeyActionMap, getHotkeyFunctions\\nprofile(\\"ChannelSelection.py 4\\")\\nfrom Screens.PictureInPicture import PictureInPicture\\nfrom Screens.RdsDisplay import RassInteractive\\nfrom ServiceReference import ServiceReference\\nfrom Tools.BoundFunction import boundFunction\\nfrom Tools import Notifications\\nfrom Tools.Alternatives import CompareWithAlternatives, GetWithAlternative\\nfrom Tools.Directories import fileExists\\nfrom Plugins.Plugin import PluginDescriptor\\nfrom Components.PluginComponent import plugins\\nfrom Screens.ChoiceBox import ChoiceBox\\nfrom Screens.EventView import EventViewEPGSelect\\nimport os, unicodedata\\nprofile(\\"ChannelSelection.py after imports\\")\\n\\nFLAG_SERVICE_NEW_FOUND = 64\\nFLAG_IS_DEDICATED_3D = 128\\nFLAG_HIDE_VBI = 512 #define in lib/dvb/idvb.h as dxNewFound = 64 and dxIsDedicated3D = 128\\n\\nclass BouquetSelector(Screen):\\n\\tdef __init__(self, session, bouquets, selectedFunc, enableWrapAround=True):\\n\\t\\tScreen.__init__(self, session)\\n\\t\\tself.setTitle(_(\\"Choose bouquet\\"))\\n\\n\\t\\tself.selectedFunc=selectedFunc\\n\\n\\t\\tself[\\"actions\\"] = ActionMap([\\"OkCancelActions\\"],\\n\\t\\t\\t{\\n\\t\\t\\t\\t\\"ok\\": self.okbuttonClick,\\n\\t\\t\\t\\t\\"cancel\\": self.cancelClick\\n\\t\\t\\t})\\n\\t\\tentrys = [ (x[0], x[1]) for x in bouquets ]\\n\\t\\tself[\\"menu\\"] = MenuList(entrys, enableWrapAround)\\n\\n\\tdef getCurrent(self):\\n\\t\\tcur = self[\\"menu\\"].getCurrent()\\n\\t\\treturn cur and cur[1]\\n\\n\\tdef okbuttonClick(self):\\n\\t\\tself.selectedFunc(self.getCurrent())\\n\\n\\tdef up(self):\\n\\t\\tself[\\"menu\\"].up()\\n\\n\\tdef down(self):\\n\\t\\tself[\\"menu\\"].down()\\n\\n\\tdef cancelClick(self):\\n\\t\\tself.close(False)\\n\\nclass SilentBouquetSelector:\\n\\tdef __init__(self, bouquets, enableWrapAround=False, current=0):\\n\\t\\tself.bouquets = [b[1] for b in bouquets]\\n\\t\\tself.pos = current\\n\\t\\tself.count = len(bouquets)\\n\\t\\tself.enableWrapAround = enableWrapAround\\n\\n\\tdef up(self):\\n\\t\\tif self.pos \\u003e 0 or self.enableWrapAround:\\n\\t\\t\\tself.pos = (self.pos - 1) % self.count\\n\\n\\tdef down(self):\\n\\t\\tif self.pos \\u003c (self.count - 1) or self.enableWrapAround:\\n\\t\\t\\tself.pos = (self.pos + 1) % self.count\\n\\n\\tdef getCurrent(self):\\n\\t\\treturn self.bouquets[self.pos]\\n\\n# csel.bouquet_mark_edit values\\nOFF = 0\\nEDIT_BOUQUET = 1\\nEDIT_ALTERNATIVES = 2\\n\\ndef append_when_current_valid(current, menu, args, level=0, key=\\"\\"):\\n\\tif current and current.valid() and level \\u003c= config.usage.setup_level.index:\\n\\t\\tmenu.append(ChoiceEntryComponent(key, args))\\n\\ndef removed_userbouquets_available():\\n\\tfor file in os.listdir(\\"/etc/enigma2/\\"):\\n\\t\\tif file.startswith(\\"userbouquet\\") and file.endswith(\\".del\\"):\\n\\t\\t\\treturn True\\n\\treturn False\\n\\nclass ChannelContextMenu(Screen):\\n\\tdef __init__(self, session, csel):\\n\\n\\t\\tScreen.__init__(self, session)\\n\\t\\tself.csel = csel\\n\\t\\tself.bsel = None\\n\\t\\tif self.isProtected():\\n\\t\\t\\tself.onFirstExecBegin.append(boundFunction(self.session.openWithCallback, self.protectResult, PinInput, pinList=[x.value for x in config.ParentalControl.servicepin], triesEntry=config.ParentalControl.retries.servicepin, title=_(\\"Please enter the correct pin code\\"), windowTitle=_(\\"Enter pin code\\")))\\n\\n\\t\\tself[\\"actions\\"] = ActionMap([\\"OkCancelActions\\", \\"ColorActions\\", \\"NumberActions\\", \\"MenuActions\\"],\\n\\t\\t\\t{\\n\\t\\t\\t\\t\\"ok\\": self.okbuttonClick,\\n\\t\\t\\t\\t\\"cancel\\": self.cancelClick,\\n\\t\\t\\t\\t\\"blue\\": self.showServiceInPiP,\\n\\t\\t\\t\\t\\"red\\": self.playMain,\\n\\t\\t\\t\\t\\"menu\\": self.openSetup,\\n\\t\\t\\t\\t\\"2\\": self.renameEntry,\\n\\t\\t\\t\\t\\"3\\": self.findCurrentlyPlayed,\\n\\t\\t\\t\\t\\"5\\": self.addServiceToBouquetOrAlternative,\\n\\t\\t\\t\\t\\"6\\": self.toggleMoveModeSelect,\\n\\t\\t\\t\\t\\"8\\": self.removeEntry\\n\\t\\t\\t})\\n\\t\\tmenu = [ ]\\n\\n\\t\\tself.removeFunction = False\\n\\t\\tself.addFunction = False\\n\\t\\tcurrent = csel.getCurrentSelection()\\n\\t\\tcurrent_root = csel.getRoot()\\n\\t\\tcurrent_sel_path = current.getPath()\\n\\t\\tcurrent_sel_flags = current.flags\\n\\t\\tinBouquetRootList = current_root and \'FROM BOUQUET \\"bouquets.\' in current_root.getPath() #FIXME HACK\\n\\t\\tinAlternativeList = current_root and \'FROM BOUQUET \\"alternatives\' in current_root.getPath()\\n\\t\\tself.inBouquet = csel.getMutableList() is not None\\n\\t\\thaveBouquets = config.usage.multibouquet.value\\n\\t\\tfrom Components.ParentalControl import parentalControl\\n\\t\\tself.parentalControl = parentalControl\\n\\t\\tself.parentalControlEnabled = config.ParentalControl.servicepin[0].value and config.ParentalControl.servicepinactive.value\\n\\t\\tif not (current_sel_path or current_sel_flags \\u0026 (eServiceReference.isDirectory|eServiceReference.isMarker)) or current_sel_flags \\u0026 eServiceReference.isGroup:\\n\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"show transponder info\\"), self.showServiceInformations), level=2)\\n\\t\\tif csel.bouquet_mark_edit == OFF and not csel.entry_marked:\\n\\t\\t\\tif not inBouquetRootList:\\n\\t\\t\\t\\tisPlayable = not (current_sel_flags \\u0026 (eServiceReference.isMarker|eServiceReference.isDirectory))\\n\\t\\t\\t\\tif isPlayable:\\n\\t\\t\\t\\t\\tfor p in plugins.getPlugins(PluginDescriptor.WHERE_CHANNEL_CONTEXT_MENU):\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (p.name, boundFunction(self.runPlugin, p)), key=\\"bullet\\")\\n\\t\\t\\t\\t\\tif config.servicelist.startupservice.value == current.toString():\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"stop using as startup service\\"), self.unsetStartupService), level=0)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"set as startup service\\"), self.setStartupService), level=0)\\n\\t\\t\\t\\t\\tif self.parentalControlEnabled:\\n\\t\\t\\t\\t\\t\\tif self.parentalControl.getProtectionLevel(current.toCompareString()) == -1:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"add to parental protection\\"), boundFunction(self.addParentalProtection, current)), level=0)\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tif self.parentalControl.isServiceProtectionBouquet(current.toCompareString()):\\n\\t\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"service is in bouquet parental protection\\"), self.cancelClick), level=0)\\n\\t\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove from parental protection\\"), boundFunction(self.removeParentalProtection, current)), level=0)\\n\\t\\t\\t\\t\\t\\tif config.ParentalControl.hideBlacklist.value and not parentalControl.sessionPinCached and config.ParentalControl.storeservicepin.value != \\"never\\":\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"Unhide parental control services\\"), self.unhideParentalServices), level=0, key=\\"1\\")\\n\\t\\t\\t\\t\\tif SystemInfo[\\"3DMode\\"] and  fileExists(\\"/usr/lib/enigma2/python/Plugins/SystemPlugins/OSD3DSetup/plugin.py\\"):\\n\\t\\t\\t\\t\\t\\tif eDVBDB.getInstance().getFlag(eServiceReference(current.toString())) \\u0026 FLAG_IS_DEDICATED_3D:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"Unmark service as dedicated 3D service\\"), self.removeDedicated3DFlag), level=0)\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"Mark service as dedicated 3D service\\"), self.addDedicated3DFlag), level=0)\\n\\t\\t\\t\\t\\tif not (current_sel_path):\\n\\t\\t\\t\\t\\t\\tif eDVBDB.getInstance().getFlag(eServiceReference(current.toString())) \\u0026 FLAG_HIDE_VBI:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"Uncover dashed flickering line for this service\\"), self.removeHideVBIFlag), level=0)\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"Cover dashed flickering line for this service\\"), self.addHideVBIFlag), level=0)\\n\\t\\t\\t\\t\\tif haveBouquets:\\n\\t\\t\\t\\t\\t\\tbouquets = self.csel.getBouquetList()\\n\\t\\t\\t\\t\\t\\tif bouquets is None:\\n\\t\\t\\t\\t\\t\\t\\tbouquetCnt = 0\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tbouquetCnt = len(bouquets)\\n\\t\\t\\t\\t\\t\\tif not self.inBouquet or bouquetCnt \\u003e 1:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"add service to bouquet\\"), self.addServiceToBouquetSelected), level=0, key=\\"5\\")\\n\\t\\t\\t\\t\\t\\t\\tself.addFunction = self.addServiceToBouquetSelected\\n\\t\\t\\t\\t\\t\\tif not self.inBouquet:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove entry\\"), self.removeEntry), level = 0, key=\\"8\\")\\n\\t\\t\\t\\t\\t\\t\\tself.removeFunction = self.removeSatelliteService\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tif not self.inBouquet:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"add service to favourites\\"), self.addServiceToBouquetSelected), level=0, key=\\"5\\")\\n\\t\\t\\t\\t\\t\\t\\tself.addFunction = self.addServiceToBouquetSelected\\n\\t\\t\\t\\t\\tif SystemInfo[\\"PIPAvailable\\"]:\\n\\t\\t\\t\\t\\t\\tif not self.parentalControlEnabled or self.parentalControl.getProtectionLevel(current.toCompareString()) == -1:\\n\\t\\t\\t\\t\\t\\t\\tif self.csel.dopipzap:\\n\\t\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"play in mainwindow\\"), self.playMain), level=0, key=\\"red\\")\\n\\t\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"play as picture in picture\\"), self.showServiceInPiP), level=0, key=\\"blue\\")\\n\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"find currently played service\\"), self.findCurrentlyPlayed), level=0, key=\\"3\\")\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tif \'FROM SATELLITES\' in current_root.getPath() and current and _(\\"Services\\") in eServiceCenter.getInstance().info(current).getName(current):\\n\\t\\t\\t\\t\\t\\tunsigned_orbpos = current.getUnsignedData(4) \\u003e\\u003e 16\\n\\t\\t\\t\\t\\t\\tif unsigned_orbpos == 0xFFFF:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove cable services\\"), self.removeSatelliteServices), level = 0)\\n\\t\\t\\t\\t\\t\\telif unsigned_orbpos == 0xEEEE:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove terrestrial services\\"), self.removeSatelliteServices), level = 0)\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove selected satellite\\"), self.removeSatelliteServices), level = 0)\\n\\t\\t\\t\\t\\tif haveBouquets:\\n\\t\\t\\t\\t\\t\\tif not self.inBouquet and not \\"PROVIDERS\\" in current_sel_path:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"copy to bouquets\\"), self.copyCurrentToBouquetList), level=0)\\n\\t\\t\\t\\t\\tif (\\"flags == %d\\" %(FLAG_SERVICE_NEW_FOUND)) in current_sel_path:\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove all new found flags\\"), self.removeAllNewFoundFlags), level=0)\\n\\t\\t\\t\\tif self.inBouquet:\\n\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"rename entry\\"), self.renameEntry), level=0, key=\\"2\\")\\n\\t\\t\\t\\t\\tif not inAlternativeList:\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove entry\\"), self.removeEntry), level=0, key=\\"8\\")\\n\\t\\t\\t\\t\\t\\tself.removeFunction = self.removeCurrentService\\n\\t\\t\\t\\tif current_root and (\\"flags == %d\\" %(FLAG_SERVICE_NEW_FOUND)) in current_root.getPath():\\n\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove new found flag\\"), self.removeNewFoundFlag), level=0)\\n\\t\\t\\telse:\\n\\t\\t\\t\\t\\tif self.parentalControlEnabled:\\n\\t\\t\\t\\t\\t\\tif self.parentalControl.getProtectionLevel(current.toCompareString()) == -1:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"add bouquet to parental protection\\"), boundFunction(self.addParentalProtection, current)), level=0)\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove bouquet from parental protection\\"), boundFunction(self.removeParentalProtection, current)), level=0)\\n\\t\\t\\t\\t\\tmenu.append(ChoiceEntryComponent(text=(_(\\"add bouquet\\"), self.showBouquetInputBox)))\\n\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"rename entry\\"), self.renameEntry), level=0, key=\\"2\\")\\n\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove entry\\"), self.removeEntry), level=0, key=\\"8\\")\\n\\t\\t\\t\\t\\tself.removeFunction = self.removeBouquet\\n\\t\\t\\t\\t\\tif removed_userbouquets_available():\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"purge deleted userbouquets\\"), self.purgeDeletedBouquets), level=0)\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"restore deleted userbouquets\\"), self.restoreDeletedBouquets), level=0)\\n\\t\\tif self.inBouquet: # current list is editable?\\n\\t\\t\\tif csel.bouquet_mark_edit == OFF:\\n\\t\\t\\t\\tif csel.movemode:\\n\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"disable move mode\\"), self.toggleMoveMode), level=0, key=\\"6\\")\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"enable move mode\\"), self.toggleMoveMode), level=1, key=\\"6\\")\\n\\t\\t\\t\\tif not csel.entry_marked and not inBouquetRootList and current_root and not (current_root.flags \\u0026 eServiceReference.isGroup):\\n\\t\\t\\t\\t\\tif current.type != -1:\\n\\t\\t\\t\\t\\t\\tmenu.append(ChoiceEntryComponent(text=(_(\\"add marker\\"), self.showMarkerInputBox)))\\n\\t\\t\\t\\t\\tif not csel.movemode:\\n\\t\\t\\t\\t\\t\\tif haveBouquets:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"enable bouquet edit\\"), self.bouquetMarkStart), level=0)\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"enable favourite edit\\"), self.bouquetMarkStart), level=0)\\n\\t\\t\\t\\t\\tif current_sel_flags \\u0026 eServiceReference.isGroup:\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"edit alternatives\\"), self.editAlternativeServices), level=2)\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"show alternatives\\"), self.showAlternativeServices), level=2)\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove all alternatives\\"), self.removeAlternativeServices), level=2)\\n\\t\\t\\t\\t\\telif not current_sel_flags \\u0026 eServiceReference.isMarker:\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"add alternatives\\"), self.addAlternativeServices), level=2)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tif csel.bouquet_mark_edit == EDIT_BOUQUET:\\n\\t\\t\\t\\t\\tif haveBouquets:\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"end bouquet edit\\"), self.bouquetMarkEnd), level=0)\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"abort bouquet edit\\"), self.bouquetMarkAbort), level=0)\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"end favourites edit\\"), self.bouquetMarkEnd), level=0)\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"abort favourites edit\\"), self.bouquetMarkAbort), level=0)\\n\\t\\t\\t\\t\\tif current_sel_flags \\u0026 eServiceReference.isMarker:\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"rename entry\\"), self.renameEntry), level=0, key=\\"2\\")\\n\\t\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"remove entry\\"), self.removeEntry), level=0, key=\\"8\\")\\n\\t\\t\\t\\t\\t\\tself.removeFunction = self.removeCurrentService\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"end alternatives edit\\"), self.bouquetMarkEnd), level=0)\\n\\t\\t\\t\\t\\tappend_when_current_valid(current, menu, (_(\\"abort alternatives edit\\"), self.bouquetMarkAbort), level=0)\\n\\n\\t\\tmenu.append(ChoiceEntryComponent(\\"menu\\", (_(\\"Configuration...\\"), self.openSetup)))\\n\\t\\tself[\\"menu\\"] = ChoiceList(menu)\\n\\n\\tdef set3DMode(self, value):\\n\\t\\tplayingref = self.session.nav.getCurrentlyPlayingServiceReference()\\n\\t\\tif config.plugins.OSD3DSetup.mode.value == \\"auto\\" and (playingref and playingref == self.csel.getCurrentSelection()):\\n\\t\\t\\tfrom Plugins.SystemPlugins.OSD3DSetup.plugin import applySettings\\n\\t\\t\\tapplySettings(value and \\"sidebyside\\" or config.plugins.OSD3DSetup.mode.value)\\n\\n\\tdef addDedicated3DFlag(self):\\n\\t\\teDVBDB.getInstance().addFlag(eServiceReference(self.csel.getCurrentSelection().toString()), FLAG_IS_DEDICATED_3D)\\n\\t\\teDVBDB.getInstance().reloadBouquets()\\n\\t\\tself.set3DMode(True)\\n\\t\\tself.close()\\n\\n\\tdef removeDedicated3DFlag(self):\\n\\t\\teDVBDB.getInstance().removeFlag(eServiceReference(self.csel.getCurrentSelection().toString()), FLAG_IS_DEDICATED_3D)\\n\\t\\teDVBDB.getInstance().reloadBouquets()\\n\\t\\tself.set3DMode(False)\\n\\t\\tself.close()\\n\\n\\tdef addHideVBIFlag(self):\\n\\t\\teDVBDB.getInstance().addFlag(eServiceReference(self.csel.getCurrentSelection().toString()), FLAG_HIDE_VBI)\\n\\t\\teDVBDB.getInstance().reloadBouquets()\\n\\t\\tScreens.InfoBar.InfoBar.instance.showHideVBI()\\n\\t\\tself.close()\\n\\n\\tdef removeHideVBIFlag(self):\\n\\t\\teDVBDB.getInstance().removeFlag(eServiceReference(self.csel.getCurrentSelection().toString()), FLAG_HIDE_VBI)\\n\\t\\teDVBDB.getInstance().reloadBouquets()\\n\\t\\tScreens.InfoBar.InfoBar.instance.showHideVBI()\\n\\t\\tself.close()\\n\\n\\tdef isProtected(self):\\n\\t\\treturn self.csel.protectContextMenu and config.ParentalControl.setuppinactive.value and config.ParentalControl.config_sections.context_menus.value\\n\\n\\tdef protectResult(self, answer):\\n\\t\\tif answer:\\n\\t\\t\\tself.csel.protectContextMenu = False\\n\\t\\telif answer is not None:\\n\\t\\t\\tself.session.openWithCallback(self.close, MessageBox, _(\\"The pin code you entered is wrong.\\"), MessageBox.TYPE_ERROR)\\n\\t\\telse:\\n\\t\\t\\tself.close()\\n\\n\\tdef addServiceToBouquetOrAlternative(self):\\n\\t\\tif self.addFunction:\\n\\t\\t\\tself.addFunction()\\n\\t\\telse:\\n\\t\\t\\treturn 0\\n\\n\\tdef getCurrentSelectionName(self):\\n\\t\\tcur = self.csel.getCurrentSelection()\\n\\t\\tif cur and cur.valid():\\n\\t\\t\\tname = eServiceCenter.getInstance().info(cur).getName(cur) or ServiceReference(cur).getServiceName() or \\"\\"\\n\\t\\t\\tname = name.replace(\'\\\\xc2\\\\x86\', \'\').replace(\'\\\\xc2\\\\x87\', \'\')\\n\\t\\t\\treturn name\\n\\t\\treturn \\"\\"\\n\\n\\tdef removeEntry(self):\\n\\t\\tif self.removeFunction and self.csel.servicelist.getCurrent() and self.csel.servicelist.getCurrent().valid():\\n\\t\\t\\tif self.csel.confirmRemove:\\n\\t\\t\\t\\tlist = [(_(\\"yes\\"), True), (_(\\"no\\"), False), (_(\\"yes\\") + \\" \\" + _(\\"and never ask again this session again\\"), \\"never\\")]\\n\\t\\t\\t\\tself.session.openWithCallback(self.removeFunction, MessageBox, _(\\"Are you sure to remove this entry?\\") + \\"\\\\n%s\\" % self.getCurrentSelectionName(), list=list)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.removeFunction(True)\\n\\t\\telse:\\n\\t\\t\\treturn 0\\n\\n\\tdef removeCurrentService(self, answer):\\n\\t\\tif answer:\\n\\t\\t\\tif answer == \\"never\\":\\n\\t\\t\\t\\tself.csel.confirmRemove = False\\n\\t\\t\\tself.csel.removeCurrentService()\\n\\t\\t\\tself.close()\\n\\n\\tdef removeSatelliteService(self, answer):\\n\\t\\tif answer:\\n\\t\\t\\tif answer == \\"never\\":\\n\\t\\t\\t\\tself.csel.confirmRemove = False\\n\\t\\t\\tself.csel.removeSatelliteService()\\n\\t\\t\\tself.close()\\n\\n\\tdef removeBouquet(self, answer):\\n\\t\\tif answer:\\n\\t\\t\\tself.csel.removeBouquet()\\n\\t\\t\\teDVBDB.getInstance().reloadBouquets()\\n\\t\\t\\tself.close()\\n\\n\\tdef purgeDeletedBouquets(self):\\n\\t\\tself.session.openWithCallback(self.purgeDeletedBouquetsCallback, MessageBox, _(\\"Are you sure to purge all deleted userbouquets?\\"))\\n\\n\\tdef purgeDeletedBouquetsCallback(self, answer):\\n\\t\\tif answer:\\n\\t\\t\\tfor file in os.listdir(\\"/etc/enigma2/\\"):\\n\\t\\t\\t\\tif file.startswith(\\"userbouquet\\") and file.endswith(\\".del\\"):\\n\\t\\t\\t\\t\\tfile = \\"/etc/enigma2/\\" + file\\n\\t\\t\\t\\t\\tprint \\"permantly remove file \\", file\\n\\t\\t\\t\\t\\tos.remove(file)\\n\\t\\t\\tself.close()\\n\\n\\tdef restoreDeletedBouquets(self):\\n\\t\\tfor file in os.listdir(\\"/etc/enigma2/\\"):\\n\\t\\t\\tif file.startswith(\\"userbouquet\\") and file.endswith(\\".del\\"):\\n\\t\\t\\t\\tfile = \\"/etc/enigma2/\\" + file\\n\\t\\t\\t\\tprint \\"restore file \\", file[:-4]\\n\\t\\t\\t\\tos.rename(file, file[:-4])\\n\\t\\teDVBDBInstance = eDVBDB.getInstance()\\n\\t\\teDVBDBInstance.setLoadUnlinkedUserbouquets(True)\\n\\t\\teDVBDBInstance.reloadBouquets()\\n\\t\\teDVBDBInstance.setLoadUnlinkedUserbouquets(config.misc.load_unlinked_userbouquets.value)\\n\\t\\trefreshServiceList()\\n\\t\\tself.csel.showFavourites()\\n\\t\\tself.close()\\n\\n\\tdef playMain(self):\\n\\t\\tsel = self.csel.getCurrentSelection()\\n\\t\\tif sel and sel.valid() and self.csel.dopipzap and (not self.parentalControlEnabled or self.parentalControl.getProtectionLevel(self.csel.getCurrentSelection().toCompareString()) == -1):\\n\\t\\t\\tself.csel.zap()\\n\\t\\t\\tself.csel.setCurrentSelection(sel)\\n\\t\\t\\tself.close(True)\\n\\t\\telse:\\n\\t\\t\\treturn 0\\n\\n\\tdef okbuttonClick(self):\\n\\t\\tself[\\"menu\\"].getCurrent()[0][1]()\\n\\n\\tdef openSetup(self):\\n\\t\\tfrom Screens.Setup import Setup\\n\\t\\tself.session.openWithCallback(self.cancelClick, Setup, \\"userinterface\\")\\n\\n\\tdef cancelClick(self, dummy=False):\\n\\t\\tself.close(False)\\n\\n\\tdef showServiceInformations(self):\\n\\t\\tcurrent = self.csel.getCurrentSelection()\\n\\t\\tif current.flags \\u0026 eServiceReference.isGroup:\\n\\t\\t\\tplayingref = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\t\\tif playingref and playingref == current:\\n\\t\\t\\t\\tcurrent = self.session.nav.getCurrentlyPlayingServiceReference()\\n\\t\\t\\telse:\\n\\t\\t\\t\\tcurrent = eServiceReference(GetWithAlternative(current.toString()))\\n\\t\\tself.session.open(ServiceInfo, current)\\n\\t\\tself.close()\\n\\n\\tdef setStartupService(self):\\n\\t\\tself.session.openWithCallback(self.setStartupServiceCallback, MessageBox, _(\\"Set startup service\\"), list = [(_(\\"Only on startup\\"), \\"startup\\"), (_(\\"Also on standby\\"), \\"standby\\")])\\n\\n\\tdef setStartupServiceCallback(self, answer):\\n\\t\\tif answer:\\n\\t\\t\\tconfig.servicelist.startupservice.value = self.csel.getCurrentSelection().toString()\\n\\t\\t\\tpath = \';\'.join([i.toString() for i in self.csel.servicePath])\\n\\t\\t\\tconfig.servicelist.startuproot.value = path\\n\\t\\t\\tconfig.servicelist.startupmode.value = config.servicelist.lastmode.value\\n\\t\\t\\tconfig.servicelist.startupservice_onstandby.value = answer == \\"standby\\"\\n\\t\\t\\tconfig.servicelist.save()\\n\\t\\t\\tconfigfile.save()\\n\\t\\t\\tself.close()\\n\\n\\tdef unsetStartupService(self):\\n\\t\\tconfig.servicelist.startupservice.value = \'\'\\n\\t\\tconfig.servicelist.startupservice_onstandby.value = False\\n\\t\\tconfig.servicelist.save()\\n\\t\\tconfigfile.save()\\n\\t\\tself.close()\\n\\n\\tdef showBouquetInputBox(self):\\n\\t\\tself.session.openWithCallback(self.bouquetInputCallback, VirtualKeyBoard, title=_(\\"Please enter a name for the new bouquet\\"), text=\\"bouquetname\\", maxSize=False, visible_width=56, type=Input.TEXT)\\n\\n\\tdef bouquetInputCallback(self, bouquet):\\n\\t\\tif bouquet is not None:\\n\\t\\t\\tself.csel.addBouquet(bouquet, None)\\n\\t\\tself.close()\\n\\n\\tdef addParentalProtection(self, service):\\n\\t\\tself.parentalControl.protectService(service.toCompareString())\\n\\t\\tif config.ParentalControl.hideBlacklist.value and not self.parentalControl.sessionPinCached:\\n\\t\\t\\tself.csel.servicelist.resetRoot()\\n\\t\\tself.close()\\n\\n\\tdef removeParentalProtection(self, service):\\n\\t\\tself.session.openWithCallback(boundFunction(self.pinEntered, service.toCompareString()), PinInput, pinList=[config.ParentalControl.servicepin[0].value], triesEntry=config.ParentalControl.retries.servicepin, title=_(\\"Enter the service pin\\"), windowTitle=_(\\"Enter pin code\\"))\\n\\n\\tdef pinEntered(self, service, answer):\\n\\t\\tif answer:\\n\\t\\t\\tself.parentalControl.unProtectService(service)\\n\\t\\t\\tself.close()\\n\\t\\telif answer is not None:\\n\\t\\t\\tself.session.openWithCallback(self.close, MessageBox, _(\\"The pin code you entered is wrong.\\"), MessageBox.TYPE_ERROR)\\n\\t\\telse:\\n\\t\\t\\tself.close()\\n\\n\\tdef unhideParentalServices(self):\\n\\t\\tif self.csel.protectContextMenu:\\n\\t\\t\\tself.session.openWithCallback(self.unhideParentalServicesCallback, PinInput, pinList=[config.ParentalControl.servicepin[0].value], triesEntry=config.ParentalControl.retries.servicepin, title=_(\\"Enter the service pin\\"), windowTitle=_(\\"Enter pin code\\"))\\n\\t\\telse:\\n\\t\\t\\tself.unhideParentalServicesCallback(True)\\n\\n\\tdef unhideParentalServicesCallback(self, answer):\\n\\t\\tif answer:\\n\\t\\t\\tservice = self.csel.servicelist.getCurrent()\\n\\t\\t\\tself.parentalControl.setSessionPinCached()\\n\\t\\t\\tself.parentalControl.hideBlacklist()\\n\\t\\t\\tself.csel.servicelist.resetRoot()\\n\\t\\t\\tself.csel.servicelist.setCurrent(service)\\n\\t\\t\\tself.close()\\n\\t\\telif answer is not None:\\n\\t\\t\\tself.session.openWithCallback(self.close, MessageBox, _(\\"The pin code you entered is wrong.\\"), MessageBox.TYPE_ERROR)\\n\\t\\telse:\\n\\t\\t\\tself.close()\\n\\n\\tdef showServiceInPiP(self):\\n\\t\\tif self.csel.dopipzap or (self.parentalControlEnabled and not self.parentalControl.getProtectionLevel(self.csel.getCurrentSelection().toCompareString()) == -1):\\n\\t\\t\\treturn 0\\n\\t\\tif self.session.pipshown:\\n\\t\\t\\tdel self.session.pip\\n\\t\\tself.session.pip = self.session.instantiateDialog(PictureInPicture)\\n\\t\\tself.session.pip.show()\\n\\t\\tnewservice = self.csel.servicelist.getCurrent()\\n\\t\\tcurrentBouquet = self.csel.servicelist and self.csel.servicelist.getRoot()\\n\\t\\tif newservice and newservice.valid():\\n\\t\\t\\tif self.session.pip.playService(newservice):\\n\\t\\t\\t\\tself.session.pipshown = True\\n\\t\\t\\t\\tself.session.pip.servicePath = self.csel.getCurrentServicePath()\\n\\t\\t\\t\\tself.session.pip.servicePath[1] = currentBouquet\\n\\t\\t\\t\\tself.close(True)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.session.pipshown = False\\n\\t\\t\\t\\tdel self.session.pip\\n\\t\\t\\t\\tself.session.openWithCallback(self.close, MessageBox, _(\\"Could not open Picture in Picture\\"), MessageBox.TYPE_ERROR)\\n\\t\\telse:\\n\\t\\t\\treturn 0\\n\\n\\tdef addServiceToBouquetSelected(self):\\n\\t\\tbouquets = self.csel.getBouquetList()\\n\\t\\tif bouquets is None:\\n\\t\\t\\tcnt = 0\\n\\t\\telse:\\n\\t\\t\\tcnt = len(bouquets)\\n\\t\\tif cnt \\u003e 1: # show bouquet list\\n\\t\\t\\tself.bsel = self.session.openWithCallback(self.bouquetSelClosed, BouquetSelector, bouquets, self.addCurrentServiceToBouquet)\\n\\t\\telif cnt == 1: # add to only one existing bouquet\\n\\t\\t\\tself.addCurrentServiceToBouquet(bouquets[0][1], closeBouquetSelection=False)\\n\\n\\tdef bouquetSelClosed(self, recursive):\\n\\t\\tself.bsel = None\\n\\t\\tif recursive:\\n\\t\\t\\tself.close(False)\\n\\n\\tdef removeSatelliteServices(self):\\n\\t\\tself.csel.removeSatelliteServices()\\n\\t\\tself.close()\\n\\n\\tdef copyCurrentToBouquetList(self):\\n\\t\\tself.csel.copyCurrentToBouquetList()\\n\\t\\tself.close()\\n\\n\\tdef showMarkerInputBox(self):\\n\\t\\tself.session.openWithCallback(self.markerInputCallback, VirtualKeyBoard, title=_(\\"Please enter a name for the new marker\\"), text=\\"markername\\", maxSize=False, visible_width=56, type=Input.TEXT)\\n\\n\\tdef markerInputCallback(self, marker):\\n\\t\\tif marker is not None:\\n\\t\\t\\tself.csel.addMarker(marker)\\n\\t\\tself.close()\\n\\n\\tdef addCurrentServiceToBouquet(self, dest, closeBouquetSelection=True):\\n\\t\\tself.csel.addServiceToBouquet(dest)\\n\\t\\tif self.bsel is not None:\\n\\t\\t\\tself.bsel.close(True)\\n\\t\\telse:\\n\\t\\t\\tself.close(closeBouquetSelection) # close bouquet selection\\n\\n\\tdef renameEntry(self):\\n\\t\\tif self.inBouquet and self.csel.servicelist.getCurrent() and self.csel.servicelist.getCurrent().valid() and not self.csel.entry_marked:\\n\\t\\t\\tself.csel.renameEntry()\\n\\t\\t\\tself.close()\\n\\t\\telse:\\n\\t\\t\\treturn 0\\n\\n\\tdef toggleMoveMode(self):\\n\\t\\tif self.inBouquet and self.csel.servicelist.getCurrent() and self.csel.servicelist.getCurrent().valid():\\n\\t\\t\\tself.csel.toggleMoveMode()\\n\\t\\t\\tself.close()\\n\\t\\telse:\\n\\t\\t\\treturn 0\\n\\n\\tdef toggleMoveModeSelect(self):\\n\\t\\tif self.inBouquet and self.csel.servicelist.getCurrent() and self.csel.servicelist.getCurrent().valid():\\n\\t\\t\\tself.csel.toggleMoveMode(True)\\n\\t\\t\\tself.close()\\n\\t\\telse:\\n\\t\\t\\treturn 0\\n\\n\\tdef bouquetMarkStart(self):\\n\\t\\tself.csel.startMarkedEdit(EDIT_BOUQUET)\\n\\t\\tself.close()\\n\\n\\tdef bouquetMarkEnd(self):\\n\\t\\tself.csel.endMarkedEdit(abort=False)\\n\\t\\tself.close()\\n\\n\\tdef bouquetMarkAbort(self):\\n\\t\\tself.csel.endMarkedEdit(abort=True)\\n\\t\\tself.close()\\n\\n\\tdef removeNewFoundFlag(self):\\n\\t\\teDVBDB.getInstance().removeFlag(self.csel.getCurrentSelection(), FLAG_SERVICE_NEW_FOUND)\\n\\t\\tself.close()\\n\\n\\tdef removeAllNewFoundFlags(self):\\n\\t\\tcurpath = self.csel.getCurrentSelection().getPath()\\n\\t\\tidx = curpath.find(\\"satellitePosition == \\")\\n\\t\\tif idx != -1:\\n\\t\\t\\ttmp = curpath[idx+21:]\\n\\t\\t\\tidx = tmp.find(\')\')\\n\\t\\t\\tif idx != -1:\\n\\t\\t\\t\\tsatpos = int(tmp[:idx])\\n\\t\\t\\t\\teDVBDB.getInstance().removeFlags(FLAG_SERVICE_NEW_FOUND, -1, -1, -1, satpos)\\n\\t\\tself.close()\\n\\n\\tdef editAlternativeServices(self):\\n\\t\\tself.csel.startMarkedEdit(EDIT_ALTERNATIVES)\\n\\t\\tself.close()\\n\\n\\tdef showAlternativeServices(self):\\n\\t\\tself.csel[\\"Service\\"].editmode = True\\n\\t\\tself.csel.enterPath(self.csel.getCurrentSelection())\\n\\t\\tself.close()\\n\\n\\tdef removeAlternativeServices(self):\\n\\t\\tself.csel.removeAlternativeServices()\\n\\t\\tself.close()\\n\\n\\tdef addAlternativeServices(self):\\n\\t\\tself.csel.addAlternativeServices()\\n\\t\\tself.csel.startMarkedEdit(EDIT_ALTERNATIVES)\\n\\t\\tself.close()\\n\\n\\tdef findCurrentlyPlayed(self):\\n\\t\\tsel = self.csel.getCurrentSelection()\\n\\t\\tif sel and sel.valid() and not self.csel.entry_marked:\\n\\t\\t\\tcurrentPlayingService = (hasattr(self.csel, \\"dopipzap\\") and self.csel.dopipzap) and self.session.pip.getCurrentService() or self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\t\\tself.csel.servicelist.setCurrent(currentPlayingService, adjust=False)\\n\\t\\t\\tif self.csel.getCurrentSelection() != currentPlayingService:\\n\\t\\t\\t\\tself.csel.setCurrentSelection(sel)\\n\\t\\t\\tself.close()\\n\\t\\telse:\\n\\t\\t\\treturn 0\\n\\n\\tdef runPlugin(self, plugin):\\n\\t\\tplugin(session=self.session, service=self.csel.getCurrentSelection())\\n\\t\\tself.close()\\n\\nclass SelectionEventInfo:\\n\\tdef __init__(self):\\n\\t\\tself[\\"Service\\"] = self[\\"ServiceEvent\\"] = ServiceEvent()\\n\\t\\tself[\\"Event\\"] = Event()\\n\\t\\tself.servicelist.connectSelChanged(self.__selectionChanged)\\n\\t\\tself.timer = eTimer()\\n\\t\\tself.timer.callback.append(self.updateEventInfo)\\n\\t\\tself.onShown.append(self.__selectionChanged)\\n\\n\\tdef __selectionChanged(self):\\n\\t\\tif self.execing:\\n\\t\\t\\tself.timer.start(100, True)\\n\\n\\tdef updateEventInfo(self):\\n\\t\\tcur = self.getCurrentSelection()\\n\\t\\tservice = self[\\"Service\\"]\\n\\t\\tservice.newService(cur)\\n\\t\\tself[\\"Event\\"].newEvent(service.event)\\n\\nclass ChannelSelectionEPG(InfoBarHotkey):\\n\\tdef __init__(self):\\n\\t\\tself.hotkeys = [(\\"Info (EPG)\\", \\"info\\", \\"Infobar/openEventView\\"),\\n\\t\\t\\t(\\"Info (EPG)\\" + \\" \\" + _(\\"long\\"), \\"info_long\\", \\"Infobar/showEventInfoPlugins\\"),\\n\\t\\t\\t(\\"Epg/Guide\\", \\"epg\\", \\"Plugins/Extensions/GraphMultiEPG/1\\"),\\n\\t\\t\\t(\\"Epg/Guide\\" + \\" \\" + _(\\"long\\"), \\"epg_long\\", \\"Infobar/showEventInfoPlugins\\")]\\n\\t\\tself[\\"ChannelSelectEPGActions\\"] = hotkeyActionMap([\\"ChannelSelectEPGActions\\"], dict((x[1], self.hotkeyGlobal) for x in self.hotkeys))\\n\\t\\tself.eventViewEPG = self.start_bouquet = self.epg_bouquet = None\\n\\t\\tself.currentSavedPath = []\\n\\n\\tdef getKeyFunctions(self, key):\\n\\t\\tselection = eval(\\"config.misc.hotkey.\\" + key + \\".value.split(\',\')\\")\\n\\t\\tselected = []\\n\\t\\tfor x in selection:\\n\\t\\t\\tfunction = list(function for function in getHotkeyFunctions() if function[1] == x and function[2] == \\"EPG\\")\\n\\t\\t\\tif function:\\n\\t\\t\\t\\tselected.append(function[0])\\n\\t\\treturn selected\\n\\n\\tdef runPlugin(self, plugin):\\n\\t\\tScreens.InfoBar.InfoBar.instance.runPlugin(plugin)\\n\\n\\tdef getEPGPluginList(self, getAll=False):\\n\\t\\tpluginlist = [(p.name, boundFunction(self.runPlugin, p), p.path) for p in plugins.getPlugins(where = PluginDescriptor.WHERE_EVENTINFO) \\\\\\n\\t\\t\\t\\tif \'selectedevent\' not in p.__call__.func_code.co_varnames] or []\\n\\t\\tfrom Components.ServiceEventTracker import InfoBarCount\\n\\t\\tif getAll or InfoBarCount == 1:\\n\\t\\t\\tpluginlist.append((_(\\"Show EPG for current channel...\\"), self.openSingleServiceEPG, \\"current_channel\\"))\\n\\t\\tpluginlist.append((_(\\"Multi EPG\\"), self.openMultiServiceEPG, \\"multi_epg\\"))\\n\\t\\tpluginlist.append((_(\\"Current event EPG\\"), self.openEventView, \\"event_epg\\"))\\n\\t\\treturn pluginlist\\n\\n\\tdef showEventInfoPlugins(self):\\n\\t\\tpluginlist = self.getEPGPluginList()\\n\\t\\tif pluginlist:\\n\\t\\t\\tself.session.openWithCallback(self.EventInfoPluginChosen, ChoiceBox, title=_(\\"Please choose an extension...\\"), list = pluginlist, skin_name = \\"EPGExtensionsList\\")\\n\\t\\telse:\\n\\t\\t\\tself.openSingleServiceEPG()\\n\\n\\tdef EventInfoPluginChosen(self, answer):\\n\\t\\tif answer is not None:\\n\\t\\t\\tanswer[1]()\\n\\n\\tdef openEventView(self):\\n\\t\\tepglist = [ ]\\n\\t\\tself.epglist = epglist\\n\\t\\tref = self.getCurrentSelection()\\n\\t\\tepg = eEPGCache.getInstance()\\n\\t\\tnow_event = epg.lookupEventTime(ref, -1, 0)\\n\\t\\tif now_event:\\n\\t\\t\\tepglist.append(now_event)\\n\\t\\t\\tnext_event = epg.lookupEventTime(ref, -1, 1)\\n\\t\\t\\tif next_event:\\n\\t\\t\\t\\tepglist.append(next_event)\\n\\t\\tif epglist:\\n\\t\\t\\tself.eventViewEPG = self.session.openWithCallback(self.eventViewEPGClosed, EventViewEPGSelect, epglist[0], ServiceReference(ref), self.eventViewEPGCallback, self.openSingleServiceEPG, self.openMultiServiceEPG, self.openSimilarList)\\n\\n\\tdef eventViewEPGCallback(self, setEvent, setService, val):\\n\\t\\tepglist = self.epglist\\n\\t\\tif len(epglist) \\u003e 1:\\n\\t\\t\\ttmp = epglist[0]\\n\\t\\t\\tepglist[0] = epglist[1]\\n\\t\\t\\tepglist[1] = tmp\\n\\t\\t\\tsetEvent(epglist[0])\\n\\n\\tdef eventViewEPGClosed(self, ret=False):\\n\\t\\tself.eventViewEPG = None\\n\\t\\tif ret:\\n\\t\\t\\tself.close()\\n\\n\\tdef openMultiServiceEPG(self):\\n\\t\\tref = self.getCurrentSelection()\\n\\t\\tif ref:\\n\\t\\t\\tself.start_bouquet = self.epg_bouquet = self.servicelist.getRoot()\\n\\t\\t\\tself.savedService = ref\\n\\t\\t\\tself.currentSavedPath = self.servicePath[:]\\n\\t\\t\\tservices = self.getServicesList(self.servicelist.getRoot())\\n\\t\\t\\tself.session.openWithCallback(self.SingleMultiEPGClosed, EPGSelection, services, self.zapToService, None, bouquetChangeCB=self.changeBouquetForMultiEPG)\\n\\n\\tdef openSingleServiceEPG(self):\\n\\t\\tref = self.getCurrentSelection()\\n\\t\\tif ref:\\n\\t\\t\\tself.start_bouquet = self.epg_bouquet = self.servicelist.getRoot()\\n\\t\\t\\tself.savedService = ref\\n\\t\\t\\tself.currentSavedPath = self.servicePath[:]\\n\\t\\t\\tself.session.openWithCallback(self.SingleMultiEPGClosed, EPGSelection, ref, self.zapToService, serviceChangeCB=self.changeServiceCB, bouquetChangeCB=self.changeBouquetForSingleEPG)\\n\\n\\tdef openSimilarList(self, eventid, refstr):\\n\\t\\tself.session.open(EPGSelection, refstr, None, eventid)\\n\\n\\tdef getServicesList(self, root):\\n\\t\\tservices = [ ]\\n\\t\\tservicelist = root and eServiceCenter.getInstance().list(root)\\n\\t\\tif not servicelist is None:\\n\\t\\t\\twhile True:\\n\\t\\t\\t\\tservice = servicelist.getNext()\\n\\t\\t\\t\\tif not service.valid():\\n\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\tif service.flags \\u0026 (eServiceReference.isDirectory | eServiceReference.isMarker):\\n\\t\\t\\t\\t\\tcontinue\\n\\t\\t\\t\\tservices.append(ServiceReference(service))\\n\\t\\treturn services\\n\\n\\tdef SingleMultiEPGClosed(self, ret=False):\\n\\t\\tif ret:\\n\\t\\t\\tservice = self.getCurrentSelection()\\n\\t\\t\\tif self.eventViewEPG:\\n\\t\\t\\t\\tself.eventViewEPG.close(service)\\n\\t\\t\\telif service is not None:\\n\\t\\t\\t\\tself.close()\\n\\t\\telse:\\n\\t\\t\\tif self.start_bouquet != self.epg_bouquet and len(self.currentSavedPath) \\u003e 0:\\n\\t\\t\\t\\tself.clearPath()\\n\\t\\t\\t\\tself.enterPath(self.bouquet_root)\\n\\t\\t\\t\\tself.epg_bouquet = self.start_bouquet\\n\\t\\t\\t\\tself.enterPath(self.epg_bouquet)\\n\\t\\t\\tself.setCurrentSelection(self.savedService)\\n\\n\\tdef changeBouquetForSingleEPG(self, direction, epg):\\n\\t\\tif config.usage.multibouquet.value:\\n\\t\\t\\tinBouquet = self.getMutableList() is not None\\n\\t\\t\\tif inBouquet and len(self.servicePath) \\u003e 1:\\n\\t\\t\\t\\tself.pathUp()\\n\\t\\t\\t\\tif direction \\u003c 0:\\n\\t\\t\\t\\t\\tself.moveUp()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself.moveDown()\\n\\t\\t\\t\\tcur = self.getCurrentSelection()\\n\\t\\t\\t\\tself.enterPath(cur)\\n\\t\\t\\t\\tself.epg_bouquet = self.servicelist.getRoot()\\n\\t\\t\\t\\tepg.setService(ServiceReference(self.getCurrentSelection()))\\n\\n\\tdef changeBouquetForMultiEPG(self, direction, epg):\\n\\t\\tif config.usage.multibouquet.value:\\n\\t\\t\\tinBouquet = self.getMutableList() is not None\\n\\t\\t\\tif inBouquet and len(self.servicePath) \\u003e 1:\\n\\t\\t\\t\\tself.pathUp()\\n\\t\\t\\t\\tif direction \\u003c 0:\\n\\t\\t\\t\\t\\tself.moveUp()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself.moveDown()\\n\\t\\t\\t\\tcur = self.getCurrentSelection()\\n\\t\\t\\t\\tself.enterPath(cur)\\n\\t\\t\\t\\tself.epg_bouquet = self.servicelist.getRoot()\\n\\t\\t\\t\\tservices = self.getServicesList(self.epg_bouquet)\\n\\t\\t\\t\\tepg.setServices(services)\\n\\n\\tdef changeServiceCB(self, direction, epg):\\n\\t\\tbeg = self.getCurrentSelection()\\n\\t\\twhile True:\\n\\t\\t\\tif direction \\u003e 0:\\n\\t\\t\\t\\tself.moveDown()\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.moveUp()\\n\\t\\t\\tcur = self.getCurrentSelection()\\n\\t\\t\\tif cur == beg or not (cur.flags \\u0026 eServiceReference.isMarker):\\n\\t\\t\\t\\tbreak\\n\\t\\tepg.setService(ServiceReference(self.getCurrentSelection()))\\n\\n\\tdef zapToService(self, service, preview=False, zapback=False):\\n\\t\\tif self.startServiceRef is None:\\n\\t\\t\\tself.startServiceRef = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\tif service is not None:\\n\\t\\t\\tif self.servicelist.getRoot() != self.epg_bouquet:\\n\\t\\t\\t\\tself.servicelist.clearPath()\\n\\t\\t\\t\\tif self.servicelist.bouquet_root != self.epg_bouquet:\\n\\t\\t\\t\\t\\tself.servicelist.enterPath(self.servicelist.bouquet_root)\\n\\t\\t\\t\\tself.servicelist.enterPath(self.epg_bouquet)\\n\\t\\t\\tself.servicelist.setCurrent(service)\\n\\t\\tif not zapback or preview:\\n\\t\\t\\tself.zap(enable_pipzap=True)\\n\\t\\tif (self.dopipzap or zapback) and not preview:\\n\\t\\t\\tself.zapBack()\\n\\t\\tif not preview:\\n\\t\\t\\tself.startServiceRef = None\\n\\t\\t\\tself.startRoot = None\\n\\t\\t\\tself.revertMode = None\\n\\nclass ChannelSelectionEdit:\\n\\tdef __init__(self):\\n\\t\\tself.entry_marked = False\\n\\t\\tself.bouquet_mark_edit = OFF\\n\\t\\tself.mutableList = None\\n\\t\\tself.__marked = [ ]\\n\\t\\tself.saved_title = None\\n\\t\\tself.saved_root = None\\n\\t\\tself.current_ref = None\\n\\t\\tself.editMode = False\\n\\t\\tself.confirmRemove = True\\n\\n\\t\\tclass ChannelSelectionEditActionMap(ActionMap):\\n\\t\\t\\tdef __init__(self, csel, contexts=[ ], actions={ }, prio=0):\\n\\t\\t\\t\\tActionMap.__init__(self, contexts, actions, prio)\\n\\t\\t\\t\\tself.csel = csel\\n\\n\\t\\t\\tdef action(self, contexts, action):\\n\\t\\t\\t\\tif action == \\"cancel\\":\\n\\t\\t\\t\\t\\tself.csel.handleEditCancel()\\n\\t\\t\\t\\t\\treturn 0 # fall-trough\\n\\t\\t\\t\\telif action == \\"ok\\":\\n\\t\\t\\t\\t\\treturn 0 # fall-trough\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\treturn ActionMap.action(self, contexts, action)\\n\\n\\t\\tself[\\"ChannelSelectEditActions\\"] = ChannelSelectionEditActionMap(self, [\\"ChannelSelectEditActions\\", \\"OkCancelActions\\"],\\n\\t\\t\\t{\\n\\t\\t\\t\\t\\"contextMenu\\": self.doContext,\\n\\t\\t\\t})\\n\\n\\tdef getMutableList(self, root=eServiceReference()):\\n\\t\\tif not self.mutableList is None:\\n\\t\\t\\treturn self.mutableList\\n\\t\\tserviceHandler = eServiceCenter.getInstance()\\n\\t\\tif not root.valid():\\n\\t\\t\\troot=self.getRoot()\\n\\t\\tlist = root and serviceHandler.list(root)\\n\\t\\tif list is not None:\\n\\t\\t\\treturn list.startEdit()\\n\\t\\treturn None\\n\\n\\tdef buildBouquetID(self, name):\\n\\t\\tname = unicodedata.normalize(\'NFKD\', unicode(name, \'utf_8\', errors=\'ignore\')).encode(\'ASCII\', \'ignore\').translate(None, \'\\u003c\\u003e:\\"/\\\\|?*() \')\\n\\t\\twhile os.path.isfile((self.mode == MODE_TV and \\"/etc/enigma2/userbouquet.%s.tv\\" or \\"/etc/enigma2/userbouquet.%s.radio\\") % name):\\n\\t\\t\\tname = name.rsplit(\\"_\\", 1)\\n\\t\\t\\tname = \\"_\\".join((name[0], len(name) == 2 and name[1].isdigit() and str(int(name[1]) + 1) or \\"1\\"))\\n\\t\\treturn name\\n\\n\\tdef renameEntry(self):\\n\\t\\tself.editMode = True\\n\\t\\tcur = self.getCurrentSelection()\\n\\t\\tif cur and cur.valid():\\n\\t\\t\\tname = eServiceCenter.getInstance().info(cur).getName(cur) or ServiceReference(cur).getServiceName() or \\"\\"\\n\\t\\t\\tname = name.replace(\'\\\\xc2\\\\x86\', \'\').replace(\'\\\\xc2\\\\x87\', \'\')\\n\\t\\t\\tif name:\\n\\t\\t\\t\\tself.session.openWithCallback(self.renameEntryCallback, VirtualKeyBoard, title=_(\\"Please enter new name:\\"), text=name)\\n\\t\\telse:\\n\\t\\t\\treturn 0\\n\\n\\tdef renameEntryCallback(self, name):\\n\\t\\tif name:\\n\\t\\t\\tmutableList = self.getMutableList()\\n\\t\\t\\tif mutableList:\\n\\t\\t\\t\\tcurrent = self.servicelist.getCurrent()\\n\\t\\t\\t\\tcurrent.setName(name)\\n\\t\\t\\t\\tindex = self.servicelist.getCurrentIndex()\\n\\t\\t\\t\\tmutableList.removeService(current, False)\\n\\t\\t\\t\\tmutableList.addService(current)\\n\\t\\t\\t\\tmutableList.moveService(current, index)\\n\\t\\t\\t\\tmutableList.flushChanges()\\n\\t\\t\\t\\tself.servicelist.addService(current, True)\\n\\t\\t\\t\\tself.servicelist.removeCurrent()\\n\\t\\t\\t\\tif not self.servicelist.atEnd():\\n\\t\\t\\t\\t\\tself.servicelist.moveUp()\\n\\n\\tdef addMarker(self, name):\\n\\t\\tcurrent = self.servicelist.getCurrent()\\n\\t\\tmutableList = self.getMutableList()\\n\\t\\tcnt = 0\\n\\t\\twhile mutableList:\\n\\t\\t\\tstr = \'1:64:%d:0:0:0:0:0:0:0::%s\'%(cnt, name)\\n\\t\\t\\tref = eServiceReference(str)\\n\\t\\t\\tif current and current.valid():\\n\\t\\t\\t\\tif not mutableList.addService(ref, current):\\n\\t\\t\\t\\t\\tself.servicelist.addService(ref, True)\\n\\t\\t\\t\\t\\tmutableList.flushChanges()\\n\\t\\t\\t\\t\\tbreak\\n\\t\\t\\telif not mutableList.addService(ref):\\n\\t\\t\\t\\tself.servicelist.addService(ref, True)\\n\\t\\t\\t\\tmutableList.flushChanges()\\n\\t\\t\\t\\tbreak\\n\\t\\t\\tcnt+=1\\n\\n\\tdef addAlternativeServices(self):\\n\\t\\tcur_service = ServiceReference(self.getCurrentSelection())\\n\\t\\tend = self.atEnd()\\n\\t\\troot = self.getRoot()\\n\\t\\tcur_root = root and ServiceReference(root)\\n\\t\\tmutableBouquet = cur_root.list().startEdit()\\n\\t\\tif mutableBouquet:\\n\\t\\t\\tname = cur_service.getServiceName()\\n\\t\\t\\trefstr = \'_\'.join(cur_service.ref.toString().split(\':\'))\\n\\t\\t\\tif self.mode == MODE_TV:\\n\\t\\t\\t\\tstr = \'1:134:1:0:0:0:0:0:0:0:FROM BOUQUET \\\\\\"alternatives.%s.tv\\\\\\" ORDER BY bouquet\'%(refstr)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tstr = \'1:134:2:0:0:0:0:0:0:0:FROM BOUQUET \\\\\\"alternatives.%s.radio\\\\\\" ORDER BY bouquet\'%(refstr)\\n\\t\\t\\tnew_ref = ServiceReference(str)\\n\\t\\t\\tif not mutableBouquet.addService(new_ref.ref, cur_service.ref):\\n\\t\\t\\t\\tmutableBouquet.removeService(cur_service.ref)\\n\\t\\t\\t\\tmutableBouquet.flushChanges()\\n\\t\\t\\t\\teDVBDB.getInstance().reloadBouquets()\\n\\t\\t\\t\\tmutableAlternatives = new_ref.list().startEdit()\\n\\t\\t\\t\\tif mutableAlternatives:\\n\\t\\t\\t\\t\\tmutableAlternatives.setListName(name)\\n\\t\\t\\t\\t\\tif mutableAlternatives.addService(cur_service.ref):\\n\\t\\t\\t\\t\\t\\tprint \\"add\\", cur_service.ref.toString(), \\"to new alternatives failed\\"\\n\\t\\t\\t\\t\\tmutableAlternatives.flushChanges()\\n\\t\\t\\t\\t\\tself.servicelist.addService(new_ref.ref, True)\\n\\t\\t\\t\\t\\tself.servicelist.removeCurrent()\\n\\t\\t\\t\\t\\tif not end:\\n\\t\\t\\t\\t\\t\\tself.servicelist.moveUp()\\n\\t\\t\\t\\t\\tif cur_service.ref.toString() == self.lastservice.value:\\n\\t\\t\\t\\t\\t\\tself.saveChannel(new_ref.ref)\\n\\t\\t\\t\\t\\tif self.startServiceRef and cur_service.ref == self.startServiceRef:\\n\\t\\t\\t\\t\\t\\tself.startServiceRef = new_ref.ref\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tprint \\"get mutable list for new created alternatives failed\\"\\n\\t\\t\\telse:\\n\\t\\t\\t\\tprint \\"add\\", str, \\"to\\", cur_root.getServiceName(), \\"failed\\"\\n\\t\\telse:\\n\\t\\t\\tprint \\"bouquetlist is not editable\\"\\n\\n\\tdef addBouquet(self, bName, services):\\n\\t\\tserviceHandler = eServiceCenter.getInstance()\\n\\t\\tmutableBouquetList = serviceHandler.list(self.bouquet_root).startEdit()\\n\\t\\tif mutableBouquetList:\\n\\t\\t\\tbName = self.buildBouquetID(bName)\\n\\t\\t\\tnew_bouquet_ref = eServiceReference((self.mode == MODE_TV and \'1:7:1:0:0:0:0:0:0:0:FROM BOUQUET \\"userbouquet.%s.tv\\" ORDER BY bouquet\' or \'1:7:2:0:0:0:0:0:0:0:FROM BOUQUET \\"userbouquet.%s.radio\\" ORDER BY bouquet\') % bName)\\n\\t\\t\\tif not mutableBouquetList.addService(new_bouquet_ref):\\n\\t\\t\\t\\tmutableBouquetList.flushChanges()\\n\\t\\t\\t\\teDVBDB.getInstance().reloadBouquets()\\n\\t\\t\\t\\tmutableBouquet = serviceHandler.list(new_bouquet_ref).startEdit()\\n\\t\\t\\t\\tif mutableBouquet:\\n\\t\\t\\t\\t\\tmutableBouquet.setListName(bName)\\n\\t\\t\\t\\t\\tif services is not None:\\n\\t\\t\\t\\t\\t\\tfor service in services:\\n\\t\\t\\t\\t\\t\\t\\tif mutableBouquet.addService(service):\\n\\t\\t\\t\\t\\t\\t\\t\\tprint \\"add\\", service.toString(), \\"to new bouquet failed\\"\\n\\t\\t\\t\\t\\tmutableBouquet.flushChanges()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tprint \\"get mutable list for new created bouquet failed\\"\\n\\t\\t\\t\\t# do some voodoo to check if current_root is equal to bouquet_root\\n\\t\\t\\t\\tcur_root = self.getRoot();\\n\\t\\t\\t\\tstr1 = cur_root and cur_root.toString()\\n\\t\\t\\t\\tpos1 = str1 and str1.find(\\"FROM BOUQUET\\") or -1\\n\\t\\t\\t\\tpos2 = self.bouquet_rootstr.find(\\"FROM BOUQUET\\")\\n\\t\\t\\t\\tif pos1 != -1 and pos2 != -1 and str1[pos1:] == self.bouquet_rootstr[pos2:]:\\n\\t\\t\\t\\t\\tself.servicelist.addService(new_bouquet_ref)\\n\\t\\t\\t\\t\\tself.servicelist.resetRoot()\\n\\t\\t\\telse:\\n\\t\\t\\t\\tprint \\"add\\", str, \\"to bouquets failed\\"\\n\\t\\telse:\\n\\t\\t\\tprint \\"bouquetlist is not editable\\"\\n\\n\\tdef copyCurrentToBouquetList(self):\\n\\t\\tprovider = ServiceReference(self.getCurrentSelection())\\n\\t\\tproviderName = provider.getServiceName()\\n\\t\\tserviceHandler = eServiceCenter.getInstance()\\n\\t\\tservices = serviceHandler.list(provider.ref)\\n\\t\\tself.addBouquet(providerName, services and services.getContent(\'R\', True))\\n\\n\\tdef removeAlternativeServices(self):\\n\\t\\tcur_service = ServiceReference(self.getCurrentSelection())\\n\\t\\tend = self.atEnd()\\n\\t\\troot = self.getRoot()\\n\\t\\tcur_root = root and ServiceReference(root)\\n\\t\\tlist = cur_service.list()\\n\\t\\tfirst_in_alternative = list and list.getNext()\\n\\t\\tif first_in_alternative:\\n\\t\\t\\tedit_root = cur_root and cur_root.list().startEdit()\\n\\t\\t\\tif edit_root:\\n\\t\\t\\t\\tif not edit_root.addService(first_in_alternative, cur_service.ref):\\n\\t\\t\\t\\t\\tself.servicelist.addService(first_in_alternative, True)\\n\\t\\t\\t\\t\\tif cur_service.ref.toString() == self.lastservice.value:\\n\\t\\t\\t\\t\\t\\tself.saveChannel(first_in_alternative)\\n\\t\\t\\t\\t\\tif self.startServiceRef and cur_service.ref == self.startServiceRef:\\n\\t\\t\\t\\t\\t\\tself.startServiceRef = first_in_alternative\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tprint \\"couldn\'t add first alternative service to current root\\"\\n\\t\\t\\telse:\\n\\t\\t\\t\\tprint \\"couldn\'t edit current root!!\\"\\n\\t\\telse:\\n\\t\\t\\tprint \\"remove empty alternative list !!\\"\\n\\t\\tself.removeBouquet()\\n\\t\\tif not end:\\n\\t\\t\\tself.servicelist.moveUp()\\n\\n\\tdef removeBouquet(self):\\n\\t\\trefstr = self.getCurrentSelection().toString()\\n\\t\\tprint \\"removeBouquet\\", refstr\\n\\t\\tpos = refstr.find(\'FROM BOUQUET \\"\')\\n\\t\\tfilename = None\\n\\t\\tself.removeCurrentService(bouquet=True)\\n\\n\\tdef removeSatelliteService(self):\\n\\t\\tcurrent = self.getCurrentSelection()\\n\\t\\teDVBDB.getInstance().removeService(current)\\n\\t\\trefreshServiceList()\\n\\t\\tif not self.atEnd():\\n\\t\\t\\tself.servicelist.moveUp()\\n\\n\\tdef removeSatelliteServices(self):\\n\\t\\tcurrent = self.getCurrentSelection()\\n\\t\\tunsigned_orbpos = current.getUnsignedData(4) \\u003e\\u003e 16\\n\\t\\tif unsigned_orbpos == 0xFFFF:\\n\\t\\t\\tmessageText = _(\\"Are you sure to remove all cable services?\\")\\n\\t\\telif unsigned_orbpos == 0xEEEE:\\n\\t\\t\\tmessageText = _(\\"Are you sure to remove all terrestrial services?\\")\\n\\t\\telse:\\n\\t\\t\\tif unsigned_orbpos \\u003e 1800:\\n\\t\\t\\t\\tunsigned_orbpos = 3600 - unsigned_orbpos\\n\\t\\t\\t\\tdirection = _(\\"W\\")\\n\\t\\t\\telse:\\n\\t\\t\\t\\tdirection = _(\\"E\\")\\n\\t\\t\\tmessageText = _(\\"Are you sure to remove all %d.%d%s%s services?\\") % (unsigned_orbpos/10, unsigned_orbpos%10, \\"\\\\xc2\\\\xb0\\", direction)\\n\\t\\tself.session.openWithCallback(self.removeSatelliteServicesCallback, MessageBox, messageText)\\n\\n\\tdef removeSatelliteServicesCallback(self, answer):\\n\\t\\tif answer:\\n\\t\\t\\tcurrentIndex = self.servicelist.getCurrentIndex()\\n\\t\\t\\tcurrent = self.getCurrentSelection()\\n\\t\\t\\tunsigned_orbpos = current.getUnsignedData(4) \\u003e\\u003e 16\\n\\t\\t\\tif unsigned_orbpos == 0xFFFF:\\n\\t\\t\\t\\teDVBDB.getInstance().removeServices(int(\\"0xFFFF0000\\", 16) - 0x100000000)\\n\\t\\t\\telif unsigned_orbpos == 0xEEEE:\\n\\t\\t\\t\\teDVBDB.getInstance().removeServices(int(\\"0xEEEE0000\\", 16) - 0x100000000)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tcurpath = current.getPath()\\n\\t\\t\\t\\tidx = curpath.find(\\"satellitePosition == \\")\\n\\t\\t\\t\\tif idx != -1:\\n\\t\\t\\t\\t\\ttmp = curpath[idx + 21:]\\n\\t\\t\\t\\t\\tidx = tmp.find(\')\')\\n\\t\\t\\t\\t\\tif idx != -1:\\n\\t\\t\\t\\t\\t\\tsatpos = int(tmp[:idx])\\n\\t\\t\\t\\t\\t\\teDVBDB.getInstance().removeServices(-1, -1, -1, satpos)\\n\\t\\t\\trefreshServiceList()\\n\\t\\t\\tif hasattr(self, \'showSatellites\'):\\n\\t\\t\\t\\tself.showSatellites()\\n\\t\\t\\t\\tself.servicelist.moveToIndex(currentIndex)\\n\\t\\t\\t\\tif currentIndex != self.servicelist.getCurrentIndex():\\n\\t\\t\\t\\t\\tself.servicelist.instance.moveSelection(self.servicelist.instance.moveEnd)\\n\\n#  multiple marked entry stuff ( edit mode, later multiepg selection )\\n\\tdef startMarkedEdit(self, type):\\n\\t\\tself.savedPath = self.servicePath[:]\\n\\t\\tif type == EDIT_ALTERNATIVES:\\n\\t\\t\\tself.current_ref = self.getCurrentSelection()\\n\\t\\t\\tself.enterPath(self.current_ref)\\n\\t\\tself.mutableList = self.getMutableList()\\n\\t\\t# add all services from the current list to internal marked set in listboxservicecontent\\n\\t\\tself.clearMarks() # this clears the internal marked set in the listboxservicecontent\\n\\t\\tself.saved_title = self.getTitle()\\n\\t\\tpos = self.saved_title.find(\')\')\\n\\t\\tnew_title = self.saved_title[:pos+1]\\n\\t\\tif type == EDIT_ALTERNATIVES:\\n\\t\\t\\tself.bouquet_mark_edit = EDIT_ALTERNATIVES\\n\\t\\t\\tnew_title += \' \' + _(\\"[alternative edit]\\")\\n\\t\\telse:\\n\\t\\t\\tself.bouquet_mark_edit = EDIT_BOUQUET\\n\\t\\t\\tif config.usage.multibouquet.value:\\n\\t\\t\\t\\tnew_title += \' \' + _(\\"[bouquet edit]\\")\\n\\t\\t\\telse:\\n\\t\\t\\t\\tnew_title += \' \' + _(\\"[favourite edit]\\")\\n\\t\\tself.setTitle(new_title)\\n\\t\\tself.__marked = self.servicelist.getRootServices()\\n\\t\\tfor x in self.__marked:\\n\\t\\t\\tself.servicelist.addMarked(eServiceReference(x))\\n\\t\\tself[\\"Service\\"].editmode = True\\n\\n\\tdef endMarkedEdit(self, abort):\\n\\t\\tif not abort and self.mutableList is not None:\\n\\t\\t\\tnew_marked = set(self.servicelist.getMarked())\\n\\t\\t\\told_marked = set(self.__marked)\\n\\t\\t\\tremoved = old_marked - new_marked\\n\\t\\t\\tadded = new_marked - old_marked\\n\\t\\t\\tchanged = False\\n\\t\\t\\tfor x in removed:\\n\\t\\t\\t\\tchanged = True\\n\\t\\t\\t\\tself.mutableList.removeService(eServiceReference(x))\\n\\t\\t\\tfor x in added:\\n\\t\\t\\t\\tchanged = True\\n\\t\\t\\t\\tself.mutableList.addService(eServiceReference(x))\\n\\t\\t\\tif changed:\\n\\t\\t\\t\\tif self.bouquet_mark_edit == EDIT_ALTERNATIVES and not new_marked and self.__marked:\\n\\t\\t\\t\\t\\tself.mutableList.addService(eServiceReference(self.__marked[0]))\\n\\t\\t\\t\\tself.mutableList.flushChanges()\\n\\t\\tself.__marked = []\\n\\t\\tself.clearMarks()\\n\\t\\tself.bouquet_mark_edit = OFF\\n\\t\\tself.mutableList = None\\n\\t\\tself.setTitle(self.saved_title)\\n\\t\\tself.saved_title = None\\n\\t\\t# self.servicePath is just a reference to servicePathTv or Radio...\\n\\t\\t# so we never ever do use the asignment operator in self.servicePath\\n\\t\\tdel self.servicePath[:] # remove all elements\\n\\t\\tself.servicePath += self.savedPath # add saved elements\\n\\t\\tdel self.savedPath\\n\\t\\tself.setRoot(self.servicePath[-1])\\n\\t\\tif self.current_ref:\\n\\t\\t\\tself.setCurrentSelection(self.current_ref)\\n\\t\\t\\tself.current_ref = None\\n\\n\\tdef clearMarks(self):\\n\\t\\tself.servicelist.clearMarks()\\n\\n\\tdef doMark(self):\\n\\t\\tref = self.servicelist.getCurrent()\\n\\t\\tif self.servicelist.isMarked(ref):\\n\\t\\t\\tself.servicelist.removeMarked(ref)\\n\\t\\telse:\\n\\t\\t\\tself.servicelist.addMarked(ref)\\n\\n\\tdef removeCurrentEntry(self, bouquet=False):\\n\\t\\tif self.confirmRemove:\\n\\t\\t\\tlist = [(_(\\"yes\\"), True), (_(\\"no\\"), False), (_(\\"yes\\") + \\" \\" + _(\\"and never ask again this session again\\"), \\"never\\")]\\n\\t\\t\\tself.session.openWithCallback(boundFunction(self.removeCurrentEntryCallback, bouquet), MessageBox, _(\\"Are you sure to remove this entry?\\"), list=list)\\n\\t\\telse:\\n\\t\\t\\tself.removeCurrentEntryCallback(bouquet, True)\\n\\n\\tdef removeCurrentEntryCallback(self, bouquet, answer):\\n\\t\\tif answer:\\n\\t\\t\\tif answer == \\"never\\":\\n\\t\\t\\t\\tself.confirmRemove = False\\n\\t\\t\\tif bouquet:\\n\\t\\t\\t\\tself.removeBouquet()\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.removeCurrentService()\\n\\n\\tdef removeCurrentService(self, bouquet=False):\\n\\t\\tself.editMode = True\\n\\t\\tref = self.servicelist.getCurrent()\\n\\t\\tmutableList = self.getMutableList()\\n\\t\\tif ref.valid() and mutableList is not None:\\n\\t\\t\\tif not mutableList.removeService(ref):\\n\\t\\t\\t\\tmutableList.flushChanges() #FIXME dont flush on each single removed service\\n\\t\\t\\t\\tself.servicelist.removeCurrent()\\n\\t\\t\\t\\tself.servicelist.resetRoot()\\n\\t\\t\\t\\tplayingref = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\t\\t\\tif not bouquet and playingref and ref == playingref:\\n\\t\\t\\t\\t\\tself.channelSelected(doClose=False)\\n\\n\\tdef addServiceToBouquet(self, dest, service=None):\\n\\t\\tmutableList = self.getMutableList(dest)\\n\\t\\tif not mutableList is None:\\n\\t\\t\\tif service is None: #use current selected service\\n\\t\\t\\t\\tservice = self.servicelist.getCurrent()\\n\\t\\t\\tif not mutableList.addService(service):\\n\\t\\t\\t\\tmutableList.flushChanges()\\n\\t\\t\\t\\t# do some voodoo to check if current_root is equal to dest\\n\\t\\t\\t\\tcur_root = self.getRoot();\\n\\t\\t\\t\\tstr1 = cur_root and cur_root.toString() or -1\\n\\t\\t\\t\\tstr2 = dest.toString()\\n\\t\\t\\t\\tpos1 = str1.find(\\"FROM BOUQUET\\")\\n\\t\\t\\t\\tpos2 = str2.find(\\"FROM BOUQUET\\")\\n\\t\\t\\t\\tif pos1 != -1 and pos2 != -1 and str1[pos1:] == str2[pos2:]:\\n\\t\\t\\t\\t\\tself.servicelist.addService(service)\\n\\t\\t\\t\\tself.servicelist.resetRoot()\\n\\n\\tdef toggleMoveMode(self, select=False):\\n\\t\\tself.editMode = True\\n\\t\\tif self.movemode:\\n\\t\\t\\tif self.entry_marked:\\n\\t\\t\\t\\tself.toggleMoveMarked() # unmark current entry\\n\\t\\t\\tself.movemode = False\\n\\t\\t\\tself.mutableList.flushChanges() # FIXME add check if changes was made\\n\\t\\t\\tself.mutableList = None\\n\\t\\t\\tself.setTitle(self.saved_title)\\n\\t\\t\\tself.saved_title = None\\n\\t\\t\\tself.servicelist.resetRoot()\\n\\t\\t\\tself.servicelist.l.setHideNumberMarker(config.usage.hide_number_markers.value)\\n\\t\\t\\tself.setCurrentSelection(self.servicelist.getCurrent())\\n\\t\\telse:\\n\\t\\t\\tself.mutableList = self.getMutableList()\\n\\t\\t\\tself.movemode = True\\n\\t\\t\\tselect and self.toggleMoveMarked()\\n\\t\\t\\tself.saved_title = self.getTitle()\\n\\t\\t\\tpos = self.saved_title.find(\')\')\\n\\t\\t\\tself.setTitle(self.saved_title[:pos+1] + \' \' + _(\\"[move mode]\\") + self.saved_title[pos+1:]);\\n\\t\\t\\tself.servicelist.l.setHideNumberMarker(False)\\n\\t\\t\\tself.setCurrentSelection(self.servicelist.getCurrent())\\n\\t\\tself[\\"Service\\"].editmode = True\\n\\n\\tdef handleEditCancel(self):\\n\\t\\tif self.movemode: #movemode active?\\n\\t\\t\\tself.toggleMoveMode() # disable move mode\\n\\t\\telif self.bouquet_mark_edit != OFF:\\n\\t\\t\\tself.endMarkedEdit(True) # abort edit mode\\n\\n\\tdef toggleMoveMarked(self):\\n\\t\\tif self.entry_marked:\\n\\t\\t\\tself.servicelist.setCurrentMarked(False)\\n\\t\\t\\tself.entry_marked = False\\n\\t\\t\\tself.pathChangeDisabled = False # re-enable path change\\n\\t\\telse:\\n\\t\\t\\tself.servicelist.setCurrentMarked(True)\\n\\t\\t\\tself.entry_marked = True\\n\\t\\t\\tself.pathChangeDisabled = True # no path change allowed in movemod\\n\\n\\tdef doContext(self):\\n\\t\\tself.session.openWithCallback(self.exitContext, ChannelContextMenu, self)\\n\\n\\tdef exitContext(self, close=False):\\n\\t\\tif close:\\n\\t\\t\\tself.cancel()\\n\\nMODE_TV = 0\\nMODE_RADIO = 1\\n\\n# type 1 = digital television service\\n# type 4 = nvod reference service (NYI)\\n# type 17 = MPEG-2 HD digital television service\\n# type 22 = advanced codec SD digital television\\n# type 24 = advanced codec SD NVOD reference service (NYI)\\n# type 25 = advanced codec HD digital television\\n# type 27 = advanced codec HD NVOD reference service (NYI)\\n# type 2 = digital radio sound service\\n# type 10 = advanced codec digital radio sound service\\n# type 31 = High Efficiency Video Coing digital television\\n\\nservice_types_tv = \'1:7:1:0:0:0:0:0:0:0:(type == 1) || (type == 17) || (type == 22) || (type == 25) || (type == 31) || (type == 134) || (type == 195)\'\\nservice_types_radio = \'1:7:2:0:0:0:0:0:0:0:(type == 2) || (type == 10)\'\\n\\nclass ChannelSelectionBase(Screen):\\n\\tdef __init__(self, session):\\n\\t\\tScreen.__init__(self, session)\\n\\t\\tself.setScreenPathMode(None)\\n\\t\\tself[\\"key_red\\"] = Button(_(\\"All\\"))\\n\\t\\tself[\\"key_green\\"] = Button(_(\\"Satellites\\"))\\n\\t\\tself[\\"key_yellow\\"] = Button(_(\\"Provider\\"))\\n\\t\\tself[\\"key_blue\\"] = Button(_(\\"Favourites\\"))\\n\\n\\t\\tself[\\"list\\"] = ServiceList(self)\\n\\t\\tself.servicelist = self[\\"list\\"]\\n\\n\\t\\tself.numericalTextInput = NumericalTextInput(handleTimeout=False)\\n\\t\\tself.numericalTextInput.setUseableChars(u\'1234567890ABCDEFGHIJKLMNOPQRSTUVWXYZ\')\\n\\n\\t\\tself.servicePathTV = [ ]\\n\\t\\tself.servicePathRadio = [ ]\\n\\t\\tself.servicePath = [ ]\\n\\t\\tself.history = [ ]\\n\\t\\tself.rootChanged = False\\n\\t\\tself.startRoot = None\\n\\t\\tself.selectionNumber = \\"\\"\\n\\t\\tself.clearNumberSelectionNumberTimer = eTimer()\\n\\t\\tself.clearNumberSelectionNumberTimer.callback.append(self.clearNumberSelectionNumber)\\n\\t\\tself.protectContextMenu = True\\n\\n\\t\\tself.mode = MODE_TV\\n\\t\\tself.dopipzap = False\\n\\t\\tself.pathChangeDisabled = False\\n\\t\\tself.movemode = False\\n\\t\\tself.showSatDetails = False\\n\\n\\t\\tself[\\"ChannelSelectBaseActions\\"] = NumberActionMap([\\"ChannelSelectBaseActions\\", \\"NumberActions\\", \\"InputAsciiActions\\"],\\n\\t\\t\\t{\\n\\t\\t\\t\\t\\"showFavourites\\": self.showFavourites,\\n\\t\\t\\t\\t\\"showAllServices\\": self.showAllServices,\\n\\t\\t\\t\\t\\"showProviders\\": self.showProviders,\\n\\t\\t\\t\\t\\"showSatellites\\": boundFunction(self.showSatellites, changeMode=True),\\n\\t\\t\\t\\t\\"nextBouquet\\": self.nextBouquet,\\n\\t\\t\\t\\t\\"prevBouquet\\": self.prevBouquet,\\n\\t\\t\\t\\t\\"nextMarker\\": self.nextMarker,\\n\\t\\t\\t\\t\\"prevMarker\\": self.prevMarker,\\n\\t\\t\\t\\t\\"gotAsciiCode\\": self.keyAsciiCode,\\n\\t\\t\\t\\t\\"keyLeft\\": self.keyLeft,\\n\\t\\t\\t\\t\\"keyRight\\": self.keyRight,\\n\\t\\t\\t\\t\\"keyRecord\\": self.keyRecord,\\n\\t\\t\\t\\t\\"1\\": self.keyNumberGlobal,\\n\\t\\t\\t\\t\\"2\\": self.keyNumberGlobal,\\n\\t\\t\\t\\t\\"3\\": self.keyNumberGlobal,\\n\\t\\t\\t\\t\\"4\\": self.keyNumberGlobal,\\n\\t\\t\\t\\t\\"5\\": self.keyNumberGlobal,\\n\\t\\t\\t\\t\\"6\\": self.keyNumberGlobal,\\n\\t\\t\\t\\t\\"7\\": self.keyNumberGlobal,\\n\\t\\t\\t\\t\\"8\\": self.keyNumberGlobal,\\n\\t\\t\\t\\t\\"9\\": self.keyNumberGlobal,\\n\\t\\t\\t\\t\\"0\\": self.keyNumber0\\n\\t\\t\\t}, -2)\\n\\t\\tself.maintitle = _(\\"Channel selection\\")\\n\\t\\tself.recallBouquetMode()\\n\\n\\tdef getBouquetNumOffset(self, bouquet):\\n\\t\\tif not config.usage.multibouquet.value:\\n\\t\\t\\treturn 0\\n\\t\\tstr = bouquet.toString()\\n\\t\\toffset = 0\\n\\t\\tif \'userbouquet.\' in bouquet.toCompareString():\\n\\t\\t\\tserviceHandler = eServiceCenter.getInstance()\\n\\t\\t\\tservicelist = serviceHandler.list(bouquet)\\n\\t\\t\\tif not servicelist is None:\\n\\t\\t\\t\\twhile True:\\n\\t\\t\\t\\t\\tserviceIterator = servicelist.getNext()\\n\\t\\t\\t\\t\\tif not serviceIterator.valid(): #check if end of list\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\t\\tnumber = serviceIterator.getChannelNum()\\n\\t\\t\\t\\t\\tif number \\u003e 0:\\n\\t\\t\\t\\t\\t\\toffset = number - 1\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\treturn offset\\n\\n\\tdef recallBouquetMode(self):\\n\\t\\tif self.mode == MODE_TV:\\n\\t\\t\\tself.service_types = service_types_tv\\n\\t\\t\\tif config.usage.multibouquet.value:\\n\\t\\t\\t\\tself.bouquet_rootstr = \'1:7:1:0:0:0:0:0:0:0:FROM BOUQUET \\"bouquets.tv\\" ORDER BY bouquet\'\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.bouquet_rootstr = \'%s FROM BOUQUET \\"userbouquet.favourites.tv\\" ORDER BY bouquet\'%(self.service_types)\\n\\t\\telse:\\n\\t\\t\\tself.service_types = service_types_radio\\n\\t\\t\\tif config.usage.multibouquet.value:\\n\\t\\t\\t\\tself.bouquet_rootstr = \'1:7:1:0:0:0:0:0:0:0:FROM BOUQUET \\"bouquets.radio\\" ORDER BY bouquet\'\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.bouquet_rootstr = \'%s FROM BOUQUET \\"userbouquet.favourites.radio\\" ORDER BY bouquet\'%(self.service_types)\\n\\t\\tself.bouquet_root = eServiceReference(self.bouquet_rootstr)\\n\\n\\tdef setTvMode(self):\\n\\t\\tself.mode = MODE_TV\\n\\t\\tself.servicePath = self.servicePathTV\\n\\t\\tself.recallBouquetMode()\\n\\t\\ttitle = self.maintitle\\n\\t\\tpos = title.find(\\" (\\")\\n\\t\\tif pos != -1:\\n\\t\\t\\ttitle = title[:pos]\\n\\t\\ttitle += _(\\" (TV)\\")\\n\\t\\tself.setTitle(title)\\n\\n\\tdef setRadioMode(self):\\n\\t\\tself.mode = MODE_RADIO\\n\\t\\tself.servicePath = self.servicePathRadio\\n\\t\\tself.recallBouquetMode()\\n\\t\\ttitle = self.maintitle\\n\\t\\tpos = title.find(\\" (\\")\\n\\t\\tif pos != -1:\\n\\t\\t\\ttitle = title[:pos]\\n\\t\\ttitle += _(\\" (Radio)\\")\\n\\t\\tself.setTitle(title)\\n\\n\\tdef setRoot(self, root, justSet=False):\\n\\t\\tif self.startRoot is None:\\n\\t\\t\\tself.startRoot = self.getRoot()\\n\\t\\tpath = root.getPath()\\n\\t\\tisBouquet = \'FROM BOUQUET\' in path and (root.flags \\u0026 eServiceReference.isDirectory)\\n\\t\\tinBouquetRootList = \'FROM BOUQUET \\"bouquets.\' in path #FIXME HACK\\n\\t\\tif not inBouquetRootList and isBouquet:\\n\\t\\t\\tself.servicelist.setMode(ServiceList.MODE_FAVOURITES)\\n\\t\\telse:\\n\\t\\t\\tself.servicelist.setMode(ServiceList.MODE_NORMAL)\\n\\t\\tself.servicelist.setRoot(root, justSet)\\n\\t\\tself.rootChanged = True\\n\\t\\tself.buildTitleString()\\n\\n\\tdef removeModeStr(self, str):\\n\\t\\tif self.mode == MODE_TV:\\n\\t\\t\\tpos = str.find(_(\\" (TV)\\"))\\n\\t\\telse:\\n\\t\\t\\tpos = str.find(_(\\" (Radio)\\"))\\n\\t\\tif pos != -1:\\n\\t\\t\\treturn str[:pos]\\n\\t\\treturn str\\n\\n\\tdef getServiceName(self, ref):\\n\\t\\tstr = self.removeModeStr(ServiceReference(ref).getServiceName())\\n\\t\\tif \'bouquets\' in str.lower():\\n\\t\\t\\treturn _(\\"User - bouquets\\")\\n\\t\\tif not str:\\n\\t\\t\\tpathstr = ref.getPath()\\n\\t\\t\\tif \'FROM PROVIDERS\' in pathstr:\\n\\t\\t\\t\\treturn _(\\"Provider\\")\\n\\t\\t\\tif \'FROM SATELLITES\' in pathstr:\\n\\t\\t\\t\\treturn _(\\"Satellites\\")\\n\\t\\t\\tif \') ORDER BY name\' in pathstr:\\n\\t\\t\\t\\treturn _(\\"All\\")\\n\\t\\treturn str\\n\\n\\tdef buildTitleString(self):\\n\\t\\ttitleStr = self.getTitle()\\n\\t\\tpos = titleStr.find(\']\')\\n\\t\\tif pos == -1:\\n\\t\\t\\tpos = titleStr.find(\')\')\\n\\t\\tif pos != -1:\\n\\t\\t\\ttitleStr = titleStr[:pos+1]\\n\\t\\t\\tLen = len(self.servicePath)\\n\\t\\t\\tif Len \\u003e 0:\\n\\t\\t\\t\\tbase_ref = self.servicePath[0]\\n\\t\\t\\t\\tif Len \\u003e 1:\\n\\t\\t\\t\\t\\tend_ref = self.servicePath[Len-1]\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tend_ref = None\\n\\t\\t\\t\\tnameStr = self.getServiceName(base_ref)\\n\\t\\t\\t\\ttitleStr += \' - \' + nameStr\\n\\t\\t\\t\\tif end_ref is not None:\\n\\t\\t\\t\\t\\tif Len \\u003e 2:\\n\\t\\t\\t\\t\\t\\ttitleStr += \'/../\'\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\ttitleStr += \'/\'\\n\\t\\t\\t\\t\\tnameStr = self.getServiceName(end_ref)\\n\\t\\t\\t\\t\\ttitleStr += nameStr\\n\\t\\t\\t\\tself.setTitle(titleStr)\\n\\n\\tdef moveUp(self):\\n\\t\\tself.servicelist.moveUp()\\n\\n\\tdef moveDown(self):\\n\\t\\tself.servicelist.moveDown()\\n\\n\\tdef clearPath(self):\\n\\t\\tdel self.servicePath[:]\\n\\n\\tdef enterPath(self, ref, justSet=False):\\n\\t\\tself.servicePath.append(ref)\\n\\t\\tself.setRoot(ref, justSet)\\n\\n\\tdef enterUserbouquet(self, root, save_root=True):\\n\\t\\tself.clearPath()\\n\\t\\tself.recallBouquetMode()\\n\\t\\tif self.bouquet_root:\\n\\t\\t\\tself.enterPath(self.bouquet_root)\\n\\t\\tself.enterPath(root)\\n\\t\\tself.startRoot = None\\n\\t\\tif save_root:\\n\\t\\t\\tself.saveRoot()\\n\\n\\tdef pathUp(self, justSet=False):\\n\\t\\tprev = self.servicePath.pop()\\n\\t\\tif self.servicePath:\\n\\t\\t\\tcurrent = self.servicePath[-1]\\n\\t\\t\\tself.setRoot(current, justSet)\\n\\t\\t\\tif not justSet:\\n\\t\\t\\t\\tself.setCurrentSelection(prev)\\n\\t\\treturn prev\\n\\n\\tdef isBasePathEqual(self, ref):\\n\\t\\tif len(self.servicePath) \\u003e 1 and self.servicePath[0] == ref:\\n\\t\\t\\treturn True\\n\\t\\treturn False\\n\\n\\tdef isPrevPathEqual(self, ref):\\n\\t\\tlength = len(self.servicePath)\\n\\t\\tif length \\u003e 1 and self.servicePath[length-2] == ref:\\n\\t\\t\\treturn True\\n\\t\\treturn False\\n\\n\\tdef preEnterPath(self, refstr):\\n\\t\\treturn False\\n\\n\\tdef showAllServices(self):\\n\\t\\tif not self.pathChangeDisabled:\\n\\t\\t\\trefstr = \'%s ORDER BY name\'%(self.service_types)\\n\\t\\t\\tif not self.preEnterPath(refstr):\\n\\t\\t\\t\\tref = eServiceReference(refstr)\\n\\t\\t\\t\\tcurrentRoot = self.getRoot()\\n\\t\\t\\t\\tif currentRoot is None or currentRoot != ref:\\n\\t\\t\\t\\t\\tself.clearPath()\\n\\t\\t\\t\\t\\tself.enterPath(ref)\\n\\t\\t\\t\\t\\tplayingref = self.session.nav.getCurrentlyPlayingServiceReference()\\n\\t\\t\\t\\t\\tif playingref:\\n\\t\\t\\t\\t\\t\\tself.setCurrentSelectionAlternative(playingref)\\n\\n\\tdef showSatellites(self, changeMode=False):\\n\\t\\tif not self.pathChangeDisabled:\\n\\t\\t\\trefstr = \'%s FROM SATELLITES ORDER BY satellitePosition\'%(self.service_types)\\n\\t\\t\\tif not self.preEnterPath(refstr):\\n\\t\\t\\t\\tref = eServiceReference(refstr)\\n\\t\\t\\t\\tjustSet=False\\n\\t\\t\\t\\tprev = None\\n\\n\\t\\t\\t\\tif self.isBasePathEqual(ref):\\n\\t\\t\\t\\t\\tif self.isPrevPathEqual(ref):\\n\\t\\t\\t\\t\\t\\tjustSet=True\\n\\t\\t\\t\\t\\tprev = self.pathUp(justSet)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tcurrentRoot = self.getRoot()\\n\\t\\t\\t\\t\\tif currentRoot is None or currentRoot != ref:\\n\\t\\t\\t\\t\\t\\tjustSet=True\\n\\t\\t\\t\\t\\t\\tself.clearPath()\\n\\t\\t\\t\\t\\t\\tself.enterPath(ref, True)\\n\\t\\t\\t\\t\\tif changeMode and currentRoot and currentRoot == ref:\\n\\t\\t\\t\\t\\t\\tself.showSatDetails = not self.showSatDetails\\n\\t\\t\\t\\t\\t\\tjustSet = True\\n\\t\\t\\t\\t\\t\\tself.clearPath()\\n\\t\\t\\t\\t\\t\\tself.enterPath(ref, True)\\n\\t\\t\\t\\tif justSet:\\n\\t\\t\\t\\t\\taddCableAndTerrestrialLater = []\\n\\t\\t\\t\\t\\tserviceHandler = eServiceCenter.getInstance()\\n\\t\\t\\t\\t\\tservicelist = serviceHandler.list(ref)\\n\\t\\t\\t\\t\\tif not servicelist is None:\\n\\t\\t\\t\\t\\t\\twhile True:\\n\\t\\t\\t\\t\\t\\t\\tservice = servicelist.getNext()\\n\\t\\t\\t\\t\\t\\t\\tif not service.valid(): #check if end of list\\n\\t\\t\\t\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\t\\t\\t\\tunsigned_orbpos = service.getUnsignedData(4) \\u003e\\u003e 16\\n\\t\\t\\t\\t\\t\\t\\torbpos = service.getData(4) \\u003e\\u003e 16\\n\\t\\t\\t\\t\\t\\t\\tif orbpos \\u003c 0:\\n\\t\\t\\t\\t\\t\\t\\t\\torbpos += 3600\\n\\t\\t\\t\\t\\t\\t\\tif \\"FROM PROVIDER\\" in service.getPath():\\n\\t\\t\\t\\t\\t\\t\\t\\tservice_type = self.showSatDetails and _(\\"Providers\\")\\n\\t\\t\\t\\t\\t\\t\\telif (\\"flags == %d\\" %(FLAG_SERVICE_NEW_FOUND)) in service.getPath():\\n\\t\\t\\t\\t\\t\\t\\t\\tservice_type = self.showSatDetails and _(\\"New\\")\\n\\t\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\t\\tservice_type = _(\\"Services\\")\\n\\t\\t\\t\\t\\t\\t\\tif service_type:\\n\\t\\t\\t\\t\\t\\t\\t\\tif unsigned_orbpos == 0xFFFF: #Cable\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tservice_name = _(\\"Cable\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\taddCableAndTerrestrialLater.append((\\"%s - %s\\" % (service_name, service_type), service.toString()))\\n\\t\\t\\t\\t\\t\\t\\t\\telif unsigned_orbpos == 0xEEEE: #Terrestrial\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tservice_name = _(\\"Terrestrial\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\taddCableAndTerrestrialLater.append((\\"%s - %s\\" % (service_name, service_type), service.toString()))\\n\\t\\t\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tservice_name = str(nimmanager.getSatDescription(orbpos))\\n\\t\\t\\t\\t\\t\\t\\t\\t\\texcept:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tif orbpos \\u003e 1800: # west\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\torbpos = 3600 - orbpos\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\th = _(\\"W\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\th = _(\\"E\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tservice_name = (\\"%d.%d\\" + h) % (orbpos / 10, orbpos % 10)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tservice.setName(\\"%s - %s\\" % (service_name, service_type))\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tself.servicelist.addService(service)\\n\\t\\t\\t\\t\\t\\tcur_ref = self.session.nav.getCurrentlyPlayingServiceReference()\\n\\t\\t\\t\\t\\t\\tself.servicelist.l.sort()\\n\\t\\t\\t\\t\\t\\tif cur_ref:\\n\\t\\t\\t\\t\\t\\t\\tpos = self.service_types.rfind(\':\')\\n\\t\\t\\t\\t\\t\\t\\trefstr = \'%s (channelID == %08x%04x%04x) \\u0026\\u0026 %s ORDER BY name\' %(self.service_types[:pos+1],\\n\\t\\t\\t\\t\\t\\t\\t\\tcur_ref.getUnsignedData(4), # NAMESPACE\\n\\t\\t\\t\\t\\t\\t\\t\\tcur_ref.getUnsignedData(2), # TSID\\n\\t\\t\\t\\t\\t\\t\\t\\tcur_ref.getUnsignedData(3), # ONID\\n\\t\\t\\t\\t\\t\\t\\t\\tself.service_types[pos+1:])\\n\\t\\t\\t\\t\\t\\t\\tref = eServiceReference(refstr)\\n\\t\\t\\t\\t\\t\\t\\tref.setName(_(\\"Current transponder\\"))\\n\\t\\t\\t\\t\\t\\t\\tself.servicelist.addService(ref, beforeCurrent=True)\\n\\t\\t\\t\\t\\t\\tfor (service_name, service_ref) in addCableAndTerrestrialLater:\\n\\t\\t\\t\\t\\t\\t\\tref = eServiceReference(service_ref)\\n\\t\\t\\t\\t\\t\\t\\tref.setName(service_name)\\n\\t\\t\\t\\t\\t\\t\\tself.servicelist.addService(ref, beforeCurrent=True)\\n\\t\\t\\t\\t\\t\\tself.servicelist.l.FillFinished()\\n\\t\\t\\t\\t\\t\\tif prev is not None:\\n\\t\\t\\t\\t\\t\\t\\tself.setCurrentSelection(prev)\\n\\t\\t\\t\\t\\t\\telif cur_ref:\\n\\t\\t\\t\\t\\t\\t\\trefstr = cur_ref.toString()\\n\\t\\t\\t\\t\\t\\t\\top = \\"\\".join(refstr.split(\':\', 10)[6:7])\\n\\t\\t\\t\\t\\t\\t\\tif len(op) \\u003e= 4:\\n\\t\\t\\t\\t\\t\\t\\t\\thop = int(op[:-4],16)\\n\\t\\t\\t\\t\\t\\t\\t\\tif len(op) \\u003e= 7 and not op.endswith(\'0000\'):\\n\\t\\t\\t\\t\\t\\t\\t\\t\\top = op[:-4] + \'0000\'\\n\\t\\t\\t\\t\\t\\t\\t\\trefstr = \'1:7:0:0:0:0:%s:0:0:0:(satellitePosition == %s) \\u0026\\u0026 %s ORDER BY name\' % (op, hop, self.service_types[self.service_types.rfind(\':\')+1:])\\n\\t\\t\\t\\t\\t\\t\\t\\tself.setCurrentSelectionAlternative(eServiceReference(refstr))\\n\\n\\tdef showProviders(self):\\n\\t\\tif not self.pathChangeDisabled:\\n\\t\\t\\trefstr = \'%s FROM PROVIDERS ORDER BY name\'%(self.service_types)\\n\\t\\t\\tif not self.preEnterPath(refstr):\\n\\t\\t\\t\\tref = eServiceReference(refstr)\\n\\t\\t\\t\\tif self.isBasePathEqual(ref):\\n\\t\\t\\t\\t\\tself.pathUp()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tcurrentRoot = self.getRoot()\\n\\t\\t\\t\\t\\tif currentRoot is None or currentRoot != ref:\\n\\t\\t\\t\\t\\t\\tself.clearPath()\\n\\t\\t\\t\\t\\t\\tself.enterPath(ref)\\n\\t\\t\\t\\t\\t\\tservice = self.session.nav.getCurrentService()\\n\\t\\t\\t\\t\\t\\tif service:\\n\\t\\t\\t\\t\\t\\t\\tinfo = service.info()\\n\\t\\t\\t\\t\\t\\t\\tif info:\\n\\t\\t\\t\\t\\t\\t\\t\\tprovider = info.getInfoString(iServiceInformation.sProvider)\\n\\t\\t\\t\\t\\t\\t\\t\\trefstr = \'1:7:0:0:0:0:0:0:0:0:(provider == \\\\\\"%s\\\\\\") \\u0026\\u0026 %s ORDER BY name:%s\' % (provider, self.service_types[self.service_types.rfind(\':\')+1:],provider)\\n\\t\\t\\t\\t\\t\\t\\t\\tself.setCurrentSelectionAlternative(eServiceReference(refstr))\\n\\n\\tdef changeBouquet(self, direction):\\n\\t\\tif not self.pathChangeDisabled:\\n\\t\\t\\tif len(self.servicePath) \\u003e 1:\\n\\t\\t\\t\\t#when enter satellite root list we must do some magic stuff..\\n\\t\\t\\t\\tref = eServiceReference(\'%s FROM SATELLITES ORDER BY satellitePosition\'%(self.service_types))\\n\\t\\t\\t\\tif self.isBasePathEqual(ref):\\n\\t\\t\\t\\t\\tself.showSatellites()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself.pathUp()\\n\\t\\t\\t\\tif direction \\u003c 0:\\n\\t\\t\\t\\t\\tself.moveUp()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself.moveDown()\\n\\t\\t\\t\\tref = self.getCurrentSelection()\\n\\t\\t\\t\\tself.enterPath(ref)\\n\\n\\tdef inBouquet(self):\\n\\t\\tif self.servicePath and self.servicePath[0] == self.bouquet_root:\\n\\t\\t\\treturn True\\n\\t\\treturn False\\n\\n\\tdef atBegin(self):\\n\\t\\treturn self.servicelist.atBegin()\\n\\n\\tdef atEnd(self):\\n\\t\\treturn self.servicelist.atEnd()\\n\\n\\tdef nextBouquet(self):\\n\\t\\tif self.shown and config.usage.oldstyle_channel_select_controls.value:\\n\\t\\t\\tself.servicelist.instance.moveSelection(self.servicelist.instance.pageUp)\\n\\t\\telif \\"reverseB\\" in config.usage.servicelist_cursor_behavior.value:\\n\\t\\t\\tself.changeBouquet(-1)\\n\\t\\telse:\\n\\t\\t\\tself.changeBouquet(+1)\\n\\n\\tdef prevBouquet(self):\\n\\t\\tif self.shown and config.usage.oldstyle_channel_select_controls.value:\\n\\t\\t\\tself.servicelist.instance.moveSelection(self.servicelist.instance.pageDown)\\n\\t\\telif \\"reverseB\\" in config.usage.servicelist_cursor_behavior.value:\\n\\t\\t\\tself.changeBouquet(+1)\\n\\t\\telse:\\n\\t\\t\\tself.changeBouquet(-1)\\n\\n\\tdef keyLeft(self):\\n\\t\\tif config.usage.oldstyle_channel_select_controls.value:\\n\\t\\t\\tself.changeBouquet(-1)\\n\\t\\telse:\\n\\t\\t\\tself.servicelist.instance.moveSelection(self.servicelist.instance.pageUp)\\n\\n\\tdef keyRight(self):\\n\\t\\tif config.usage.oldstyle_channel_select_controls.value:\\n\\t\\t\\tself.changeBouquet(+1)\\n\\t\\telse:\\n\\t\\t\\tself.servicelist.instance.moveSelection(self.servicelist.instance.pageDown)\\n\\n\\tdef keyRecord(self):\\n\\t\\tref = self.getCurrentSelection()\\n\\t\\tif ref and not(ref.flags \\u0026 (eServiceReference.isMarker|eServiceReference.isDirectory)):\\n\\t\\t\\tScreens.InfoBar.InfoBar.instance.instantRecord(serviceRef=ref)\\n\\n\\tdef showFavourites(self):\\n\\t\\tif not self.pathChangeDisabled:\\n\\t\\t\\tif not self.preEnterPath(self.bouquet_rootstr):\\n\\t\\t\\t\\tif self.isBasePathEqual(self.bouquet_root):\\n\\t\\t\\t\\t\\tself.pathUp()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tcurrentRoot = self.getRoot()\\n\\t\\t\\t\\t\\tif currentRoot is None or currentRoot != self.bouquet_root:\\n\\t\\t\\t\\t\\t\\tself.clearPath()\\n\\t\\t\\t\\t\\t\\tself.enterPath(self.bouquet_root)\\n\\n\\tdef keyNumber0(self, number):\\n\\t\\tif len(self.servicePath) \\u003e 1 and not self.selectionNumber:\\n\\t\\t\\tself.keyGoUp()\\n\\t\\telse:\\n\\t\\t\\tself.keyNumberGlobal(number)\\n\\n\\tdef keyNumberGlobal(self, number):\\n\\t\\tif self.isBasePathEqual(self.bouquet_root):\\n\\t\\t\\tif hasattr(self, \\"editMode\\") and self.editMode:\\n\\t\\t\\t\\tif number == 2:\\n\\t\\t\\t\\t\\tself.renameEntry()\\n\\t\\t\\t\\tif number == 6:\\n\\t\\t\\t\\t\\tself.toggleMoveMode(select=True)\\n\\t\\t\\t\\tif number == 8:\\n\\t\\t\\t\\t\\tself.removeCurrentEntry(bouquet=False)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.numberSelectionActions(number)\\n\\t\\telse:\\n\\t\\t\\tcurrent_root = self.getRoot()\\n\\t\\t\\tif  current_root and \'FROM BOUQUET \\"bouquets.\' in current_root.getPath():\\n\\t\\t\\t\\tif hasattr(self, \\"editMode\\") and self.editMode:\\n\\t\\t\\t\\t\\tif number == 2:\\n\\t\\t\\t\\t\\t\\tself.renameEntry()\\n\\t\\t\\t\\t\\tif number == 6:\\n\\t\\t\\t\\t\\t\\tself.toggleMoveMode(select=True)\\n\\t\\t\\t\\t\\tif number == 8:\\n\\t\\t\\t\\t\\t\\tself.removeCurrentEntry(bouquet=True)\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself.numberSelectionActions(number)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tunichar = self.numericalTextInput.getKey(number)\\n\\t\\t\\t\\tcharstr = unichar.encode(\\"utf-8\\")\\n\\t\\t\\t\\tif len(charstr) == 1:\\n\\t\\t\\t\\t\\tself.servicelist.moveToChar(charstr[0])\\n\\n\\tdef numberSelectionActions(self, number):\\n\\t\\tif not(hasattr(self, \\"movemode\\") and self.movemode):\\n\\t\\t\\tif len(self.selectionNumber)\\u003e4:\\n\\t\\t\\t\\tself.clearNumberSelectionNumber()\\n\\t\\t\\tself.selectionNumber = self.selectionNumber + str(number)\\n\\t\\t\\tref, bouquet = Screens.InfoBar.InfoBar.instance.searchNumber(int(self.selectionNumber), bouquet=self.getRoot())\\n\\t\\t\\tif ref:\\n\\t\\t\\t\\tif not ref.flags \\u0026 eServiceReference.isMarker:\\n\\t\\t\\t\\t\\tself.enterUserbouquet(bouquet, save_root=False)\\n\\t\\t\\t\\t\\tself.setCurrentSelection(ref)\\n\\t\\t\\t\\tself.clearNumberSelectionNumberTimer.start(1000, True)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.clearNumberSelectionNumber()\\n\\n\\tdef clearNumberSelectionNumber(self):\\n\\t\\tself.clearNumberSelectionNumberTimer.stop()\\n\\t\\tself.selectionNumber = \\"\\"\\n\\n\\tdef keyAsciiCode(self):\\n\\t\\tunichar = unichr(getPrevAsciiCode())\\n\\t\\tcharstr = unichar.encode(\\"utf-8\\")\\n\\t\\tif len(charstr) == 1:\\n\\t\\t\\tself.servicelist.moveToChar(charstr[0])\\n\\n\\tdef getRoot(self):\\n\\t\\treturn self.servicelist.getRoot()\\n\\n\\tdef getCurrentSelection(self):\\n\\t\\treturn self.servicelist.getCurrent()\\n\\n\\tdef setCurrentSelection(self, service):\\n\\t\\tif service:\\n\\t\\t\\tself.servicelist.setCurrent(service, adjust=False)\\n\\n\\tdef setCurrentSelectionAlternative(self, ref):\\n\\t\\tif self.bouquet_mark_edit == EDIT_ALTERNATIVES and not (ref.flags \\u0026 eServiceReference.isDirectory):\\n\\t\\t\\tfor markedService in self.servicelist.getMarked():\\n\\t\\t\\t\\tmarkedService = eServiceReference(markedService)\\n\\t\\t\\t\\tself.setCurrentSelection(markedService)\\n\\t\\t\\t\\tif markedService == self.getCurrentSelection():\\n\\t\\t\\t\\t\\treturn\\n\\t\\tself.setCurrentSelection(ref)\\n\\n\\tdef getBouquetList(self):\\n\\t\\tbouquets = [ ]\\n\\t\\tserviceHandler = eServiceCenter.getInstance()\\n\\t\\tif config.usage.multibouquet.value:\\n\\t\\t\\tlist = serviceHandler.list(self.bouquet_root)\\n\\t\\t\\tif list:\\n\\t\\t\\t\\twhile True:\\n\\t\\t\\t\\t\\ts = list.getNext()\\n\\t\\t\\t\\t\\tif not s.valid():\\n\\t\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\t\\tif s.flags \\u0026 eServiceReference.isDirectory and not s.flags \\u0026 eServiceReference.isInvisible:\\n\\t\\t\\t\\t\\t\\tinfo = serviceHandler.info(s)\\n\\t\\t\\t\\t\\t\\tif info:\\n\\t\\t\\t\\t\\t\\t\\tbouquets.append((info.getName(s), s))\\n\\t\\t\\t\\treturn bouquets\\n\\t\\telse:\\n\\t\\t\\tinfo = serviceHandler.info(self.bouquet_root)\\n\\t\\t\\tif info:\\n\\t\\t\\t\\tbouquets.append((info.getName(self.bouquet_root), self.bouquet_root))\\n\\t\\t\\treturn bouquets\\n\\t\\treturn None\\n\\n\\tdef keyGoUp(self):\\n\\t\\tif len(self.servicePath) \\u003e 1:\\n\\t\\t\\tif self.isBasePathEqual(self.bouquet_root):\\n\\t\\t\\t\\tself.showFavourites()\\n\\t\\t\\telse:\\n\\t\\t\\t\\tref = eServiceReference(\'%s FROM SATELLITES ORDER BY satellitePosition\'%(self.service_types))\\n\\t\\t\\t\\tif self.isBasePathEqual(ref):\\n\\t\\t\\t\\t\\tself.showSatellites()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tref = eServiceReference(\'%s FROM PROVIDERS ORDER BY name\'%(self.service_types))\\n\\t\\t\\t\\t\\tif self.isBasePathEqual(ref):\\n\\t\\t\\t\\t\\t\\tself.showProviders()\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tself.showAllServices()\\n\\n\\tdef nextMarker(self):\\n\\t\\tself.servicelist.moveToNextMarker()\\n\\n\\tdef prevMarker(self):\\n\\t\\tself.servicelist.moveToPrevMarker()\\n\\n\\tdef gotoCurrentServiceOrProvider(self, ref):\\n\\t\\tstr = ref.toString()\\n\\t\\tif _(\\"Providers\\") in str:\\n\\t\\t\\tservice = self.session.nav.getCurrentService()\\n\\t\\t\\tif service:\\n\\t\\t\\t\\tinfo = service.info()\\n\\t\\t\\t\\tif info:\\n\\t\\t\\t\\t\\tprovider = info.getInfoString(iServiceInformation.sProvider)\\n\\t\\t\\t\\t\\top = int(self.session.nav.getCurrentlyPlayingServiceOrGroup().toString().split(\':\')[6][:-4] or \\"0\\",16)\\n\\t\\t\\t\\t\\trefstr = \'1:7:0:0:0:0:0:0:0:0:(provider == \\\\\\"%s\\\\\\") \\u0026\\u0026 (satellitePosition == %s) \\u0026\\u0026 %s ORDER BY name:%s\' % (provider, op, self.service_types[self.service_types.rfind(\':\')+1:],provider)\\n\\t\\t\\t\\t\\tself.setCurrentSelection(eServiceReference(refstr))\\n\\t\\telif not self.isBasePathEqual(self.bouquet_root) or self.bouquet_mark_edit == EDIT_ALTERNATIVES:\\n\\t\\t\\tplayingref = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\t\\tif playingref:\\n\\t\\t\\t\\tself.setCurrentSelectionAlternative(playingref)\\n\\nHISTORYSIZE = 20\\n\\n#config for lastservice\\nconfig.tv = ConfigSubsection()\\nconfig.tv.lastservice = ConfigText()\\nconfig.tv.lastroot = ConfigText()\\nconfig.radio = ConfigSubsection()\\nconfig.radio.lastservice = ConfigText()\\nconfig.radio.lastroot = ConfigText()\\nconfig.servicelist = ConfigSubsection()\\nconfig.servicelist.lastmode = ConfigText(default = \\"tv\\")\\nconfig.servicelist.startupservice = ConfigText()\\nconfig.servicelist.startupservice_onstandby = ConfigYesNo(default = False)\\nconfig.servicelist.startuproot = ConfigText()\\nconfig.servicelist.startupmode = ConfigText(default = \\"tv\\")\\n\\nclass ChannelSelection(ChannelSelectionBase, ChannelSelectionEdit, ChannelSelectionEPG, SelectionEventInfo):\\n\\tdef __init__(self, session):\\n\\t\\tChannelSelectionBase.__init__(self,session)\\n\\t\\tChannelSelectionEdit.__init__(self)\\n\\t\\tChannelSelectionEPG.__init__(self)\\n\\t\\tSelectionEventInfo.__init__(self)\\n\\n\\t\\tself[\\"actions\\"] = ActionMap([\\"OkCancelActions\\", \\"TvRadioActions\\"],\\n\\t\\t\\t{\\n\\t\\t\\t\\t\\"cancel\\": self.cancel,\\n\\t\\t\\t\\t\\"ok\\": self.channelSelected,\\n\\t\\t\\t\\t\\"keyRadio\\": self.doRadioButton,\\n\\t\\t\\t\\t\\"keyTV\\": self.doTVButton,\\n\\t\\t\\t})\\n\\n\\t\\tself.__event_tracker = ServiceEventTracker(screen=self, eventmap=\\n\\t\\t\\t{\\n\\t\\t\\t\\tiPlayableService.evStart: self.__evServiceStart,\\n\\t\\t\\t\\tiPlayableService.evEnd: self.__evServiceEnd\\n\\t\\t\\t})\\n\\n\\t\\tself.startServiceRef = None\\n\\n\\t\\tself.history = [ ]\\n\\t\\tself.history_pos = 0\\n\\n\\t\\tif config.servicelist.startupservice.value and config.servicelist.startuproot.value:\\n\\t\\t\\tconfig.servicelist.lastmode.value = config.servicelist.startupmode.value\\n\\t\\t\\tif config.servicelist.lastmode.value == \\"tv\\":\\n\\t\\t\\t\\tconfig.tv.lastservice.value = config.servicelist.startupservice.value\\n\\t\\t\\t\\tconfig.tv.lastroot.value = config.servicelist.startuproot.value\\n\\t\\t\\telif config.servicelist.lastmode.value == \\"radio\\":\\n\\t\\t\\t\\tconfig.radio.lastservice.value = config.servicelist.startupservice.value\\n\\t\\t\\t\\tconfig.radio.lastroot.value = config.servicelist.startuproot.value\\n\\n\\t\\tself.lastservice = config.tv.lastservice\\n\\t\\tself.lastroot = config.tv.lastroot\\n\\t\\tself.revertMode = None\\n\\t\\tconfig.usage.multibouquet.addNotifier(self.multibouquet_config_changed)\\n\\t\\tself.new_service_played = False\\n\\t\\tself.dopipzap = False\\n\\t\\tself.onExecBegin.append(self.asciiOn)\\n\\t\\tself.mainScreenMode = None\\n\\t\\tself.mainScreenRoot = None\\n\\n\\t\\tself.lastChannelRootTimer = eTimer()\\n\\t\\tself.lastChannelRootTimer.callback.append(self.__onCreate)\\n\\t\\tself.lastChannelRootTimer.start(100,True)\\n\\t\\tself.pipzaptimer = eTimer()\\n\\n\\tdef asciiOn(self):\\n\\t\\trcinput = eRCInput.getInstance()\\n\\t\\trcinput.setKeyboardMode(rcinput.kmAscii)\\n\\n\\tdef asciiOff(self):\\n\\t\\trcinput = eRCInput.getInstance()\\n\\t\\trcinput.setKeyboardMode(rcinput.kmNone)\\n\\n\\tdef multibouquet_config_changed(self, val):\\n\\t\\tself.recallBouquetMode()\\n\\n\\tdef __evServiceStart(self):\\n\\t\\tif self.dopipzap and hasattr(self.session, \'pip\'):\\n\\t\\t\\tself.servicelist.setPlayableIgnoreService(self.session.pip.getCurrentServiceReference() or eServiceReference())\\n\\t\\telse:\\n\\t\\t\\tservice = self.session.nav.getCurrentService()\\n\\t\\t\\tif service:\\n\\t\\t\\t\\tinfo = service.info()\\n\\t\\t\\t\\tif info:\\n\\t\\t\\t\\t\\trefstr = info.getInfoString(iServiceInformation.sServiceref)\\n\\t\\t\\t\\t\\tself.servicelist.setPlayableIgnoreService(eServiceReference(refstr))\\n\\n\\tdef __evServiceEnd(self):\\n\\t\\tself.servicelist.setPlayableIgnoreService(eServiceReference())\\n\\n\\tdef setMode(self):\\n\\t\\tself.rootChanged = True\\n\\t\\tself.restoreRoot()\\n\\t\\tlastservice = eServiceReference(self.lastservice.value)\\n\\t\\tif lastservice.valid():\\n\\t\\t\\tself.setCurrentSelection(lastservice)\\n\\n\\tdef doTVButton(self):\\n\\t\\tif self.mode == MODE_TV:\\n\\t\\t\\tself.channelSelected(doClose = False)\\n\\t\\telse:\\n\\t\\t\\tself.setModeTv()\\n\\n\\tdef setModeTv(self):\\n\\t\\tif self.revertMode is None:\\n\\t\\t\\tself.revertMode = self.mode\\n\\t\\tself.lastservice = config.tv.lastservice\\n\\t\\tself.lastroot = config.tv.lastroot\\n\\t\\tconfig.servicelist.lastmode.value = \\"tv\\"\\n\\t\\tself.setTvMode()\\n\\t\\tself.setMode()\\n\\n\\tdef doRadioButton(self):\\n\\t\\tif self.mode == MODE_RADIO:\\n\\t\\t\\tself.channelSelected(doClose=False)\\n\\t\\telse:\\n\\t\\t\\tself.setModeRadio()\\n\\n\\tdef setModeRadio(self):\\n\\t\\tif self.revertMode is None:\\n\\t\\t\\tself.revertMode = self.mode\\n\\t\\tif config.usage.e1like_radio_mode.value:\\n\\t\\t\\tself.lastservice = config.radio.lastservice\\n\\t\\t\\tself.lastroot = config.radio.lastroot\\n\\t\\t\\tconfig.servicelist.lastmode.value = \\"radio\\"\\n\\t\\t\\tself.setRadioMode()\\n\\t\\t\\tself.setMode()\\n\\n\\tdef __onCreate(self):\\n\\t\\tif config.usage.e1like_radio_mode.value:\\n\\t\\t\\tif config.servicelist.lastmode.value == \\"tv\\":\\n\\t\\t\\t\\tself.setModeTv()\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.setModeRadio()\\n\\t\\telse:\\n\\t\\t\\tself.setModeTv()\\n\\t\\tlastservice = eServiceReference(self.lastservice.value)\\n\\t\\tif lastservice.valid():\\n\\t\\t\\tself.zap()\\n\\n\\tdef channelSelected(self, doClose = True):\\n\\t\\tplayingref = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\tif config.usage.channelselection_preview.value and (playingref is None or self.getCurrentSelection() and self.getCurrentSelection() != playingref):\\n\\t\\t\\tdoClose = False\\n\\t\\tif not self.startServiceRef and not doClose:\\n\\t\\t\\tself.startServiceRef = playingref\\n\\t\\tref = self.getCurrentSelection()\\n\\t\\tif self.movemode and (self.isBasePathEqual(self.bouquet_root) or \\"userbouquet.\\" in ref.toString()):\\n\\t\\t\\tself.toggleMoveMarked()\\n\\t\\telif (ref.flags \\u0026 eServiceReference.flagDirectory) == eServiceReference.flagDirectory:\\n\\t\\t\\tif Components.ParentalControl.parentalControl.isServicePlayable(ref, self.bouquetParentalControlCallback, self.session):\\n\\t\\t\\t\\tself.enterPath(ref)\\n\\t\\t\\t\\tself.gotoCurrentServiceOrProvider(ref)\\n\\t\\t\\t\\tself.revertMode = None\\n\\t\\telif self.bouquet_mark_edit != OFF:\\n\\t\\t\\tif not (self.bouquet_mark_edit == EDIT_ALTERNATIVES and ref.flags \\u0026 eServiceReference.isGroup):\\n\\t\\t\\t\\tself.doMark()\\n\\t\\telif not (ref.flags \\u0026 eServiceReference.isMarker or ref.type == -1):\\n\\t\\t\\troot = self.getRoot()\\n\\t\\t\\tif not root or not (root.flags \\u0026 eServiceReference.isGroup):\\n\\t\\t\\t\\tself.zap(enable_pipzap=doClose, preview_zap=not doClose)\\n\\t\\t\\t\\tself.asciiOff()\\n\\t\\t\\t\\tif doClose:\\n\\t\\t\\t\\t\\tif self.dopipzap:\\n\\t\\t\\t\\t\\t\\tself.zapBack()\\n\\t\\t\\t\\t\\tself.startServiceRef = None\\n\\t\\t\\t\\t\\tself.startRoot = None\\n\\t\\t\\t\\t\\tself.correctChannelNumber()\\n\\t\\t\\t\\t\\tself.movemode and self.toggleMoveMode()\\n\\t\\t\\t\\t\\tself.editMode = False\\n\\t\\t\\t\\t\\tself.protectContextMenu = True\\n\\t\\t\\t\\t\\tself.close(ref)\\n\\n\\tdef bouquetParentalControlCallback(self, ref):\\n\\t\\tself.enterPath(ref)\\n\\t\\tself.gotoCurrentServiceOrProvider(ref)\\n\\t\\tself.revertMode = None\\n\\n\\tdef togglePipzap(self):\\n\\t\\tassert(self.session.pip)\\n\\t\\ttitle = self.instance.getTitle()\\n\\t\\tpos = title.find(\\" (\\")\\n\\t\\tif pos != -1:\\n\\t\\t\\ttitle = title[:pos]\\n\\t\\tif self.dopipzap:\\n\\t\\t\\t# Mark PiP as inactive and effectively deactivate pipzap\\n\\t\\t\\tself.hidePipzapMessage()\\n\\t\\t\\tself.dopipzap = False\\n\\n\\t\\t\\t# Disable PiP if not playing a service\\n\\t\\t\\tif self.session.pip.pipservice is None:\\n\\t\\t\\t\\tself.session.pipshown = False\\n\\t\\t\\t\\tdel self.session.pip\\n\\t\\t\\tself.__evServiceStart()\\n\\t\\t\\t# Move to playing service\\n\\t\\t\\tlastservice = eServiceReference(self.lastservice.value)\\n\\t\\t\\tif lastservice.valid() and self.getCurrentSelection() != lastservice:\\n\\t\\t\\t\\tself.setCurrentSelection(lastservice)\\n\\t\\t\\t\\tif self.getCurrentSelection() != lastservice:\\n\\t\\t\\t\\t\\tself.servicelist.setCurrent(lastservice)\\n\\n\\t\\t\\ttitle += _(\\" (TV)\\")\\n\\t\\telse:\\n\\t\\t\\t# Mark PiP as active and effectively active pipzap\\n\\t\\t\\tself.showPipzapMessage()\\n\\t\\t\\tself.dopipzap = True\\n\\t\\t\\tself.__evServiceStart()\\n\\t\\t\\t# Move to service playing in pip (will not work with subservices)\\n\\t\\t\\tself.setCurrentSelection(self.session.pip.getCurrentService())\\n\\n\\t\\t\\ttitle += _(\\" (PiP)\\")\\n\\t\\tself.setTitle(title)\\n\\t\\tself.buildTitleString()\\n\\n\\tdef showPipzapMessage(self):\\n\\t\\ttime = config.usage.infobar_timeout.index\\n\\t\\tif time:\\n\\t\\t\\tself.pipzaptimer.callback.append(self.hidePipzapMessage)\\n\\t\\t\\tself.pipzaptimer.startLongTimer(time)\\n\\t\\tself.session.pip.active()\\n\\n\\tdef hidePipzapMessage(self):\\n\\t\\tif self.pipzaptimer.isActive():\\n\\t\\t\\tself.pipzaptimer.callback.remove(self.hidePipzapMessage)\\n\\t\\t\\tself.pipzaptimer.stop()\\n\\t\\tself.session.pip.inactive()\\n\\n\\t#called from infoBar and channelSelected\\n\\tdef zap(self, enable_pipzap=False, preview_zap=False, checkParentalControl=True, ref=None):\\n\\t\\tself.curRoot = self.startRoot\\n\\t\\tnref = ref or self.getCurrentSelection()\\n\\t\\tref = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\tif enable_pipzap and self.dopipzap:\\n\\t\\t\\tref = self.session.pip.getCurrentService()\\n\\t\\t\\tif ref is None or ref != nref:\\n\\t\\t\\t\\tnref = self.session.pip.resolveAlternatePipService(nref)\\n\\t\\t\\t\\tif nref and (not checkParentalControl or Components.ParentalControl.parentalControl.isServicePlayable(nref, boundFunction(self.zap, enable_pipzap=True, checkParentalControl=False))):\\n\\t\\t\\t\\t\\tself.session.pip.playService(nref)\\n\\t\\t\\t\\t\\tself.__evServiceStart()\\n\\t\\t\\t\\t\\tself.showPipzapMessage()\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself.setStartRoot(self.curRoot)\\n\\t\\t\\t\\t\\tself.setCurrentSelection(ref)\\n\\t\\telif ref is None or ref != nref:\\n\\t\\t\\tScreens.InfoBar.InfoBar.instance.checkTimeshiftRunning(boundFunction(self.zapCheckTimeshiftCallback, enable_pipzap, preview_zap, nref))\\n\\t\\telif not preview_zap:\\n\\t\\t\\tself.saveRoot()\\n\\t\\t\\tself.saveChannel(nref)\\n\\t\\t\\tconfig.servicelist.lastmode.save()\\n\\t\\t\\tself.setCurrentSelection(nref)\\n\\t\\t\\tif self.startServiceRef is None or nref != self.startServiceRef:\\n\\t\\t\\t\\tself.addToHistory(nref)\\n\\t\\t\\tself.rootChanged = False\\n\\t\\t\\tself.revertMode = None\\n\\n\\tdef zapCheckTimeshiftCallback(self, enable_pipzap, preview_zap, nref, answer):\\n\\t\\tif answer:\\n\\t\\t\\tself.new_service_played = True\\n\\t\\t\\tself.session.nav.playService(nref)\\n\\t\\t\\tif not preview_zap:\\n\\t\\t\\t\\tself.saveRoot()\\n\\t\\t\\t\\tself.saveChannel(nref)\\n\\t\\t\\t\\tconfig.servicelist.lastmode.save()\\n\\t\\t\\t\\tif self.startServiceRef is None or nref != self.startServiceRef:\\n\\t\\t\\t\\t\\tself.addToHistory(nref)\\n\\t\\t\\t\\tif self.dopipzap:\\n\\t\\t\\t\\t\\tself.setCurrentSelection(self.session.pip.getCurrentService())\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tself.mainScreenMode = config.servicelist.lastmode.value\\n\\t\\t\\t\\t\\tself.mainScreenRoot = self.getRoot()\\n\\t\\t\\t\\tself.revertMode = None\\n\\t\\t\\telse:\\n\\t\\t\\t\\tNotifications.RemovePopup(\\"Parental control\\")\\n\\t\\t\\t\\tself.setCurrentSelection(nref)\\n\\t\\telse:\\n\\t\\t\\tself.setStartRoot(self.curRoot)\\n\\t\\t\\tself.setCurrentSelection(self.session.nav.getCurrentlyPlayingServiceOrGroup())\\n\\t\\tif not preview_zap:\\n\\t\\t\\tself.hide()\\n\\n\\tdef newServicePlayed(self):\\n\\t\\tret = self.new_service_played\\n\\t\\tself.new_service_played = False\\n\\t\\treturn ret\\n\\n\\tdef addToHistory(self, ref):\\n\\t\\tif self.servicePath is not None:\\n\\t\\t\\ttmp=self.servicePath[:]\\n\\t\\t\\ttmp.append(ref)\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tdel self.history[self.history_pos+1:]\\n\\t\\t\\texcept:\\n\\t\\t\\t\\tpass\\n\\t\\t\\tself.history.append(tmp)\\n\\t\\t\\thlen = len(self.history)\\n\\t\\t\\tif hlen \\u003e HISTORYSIZE:\\n\\t\\t\\t\\tdel self.history[0]\\n\\t\\t\\t\\thlen -= 1\\n\\t\\t\\tself.history_pos = hlen-1\\n\\n\\tdef historyBack(self):\\n\\t\\thlen = len(self.history)\\n\\t\\tcurrentPlayedRef = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\tif hlen \\u003e 0 and currentPlayedRef and self.history[self.history_pos][-1] != currentPlayedRef:\\n\\t\\t\\tself.addToHistory(currentPlayedRef)\\n\\t\\t\\thlen = len(self.history)\\n\\t\\tif hlen \\u003e 1 and self.history_pos \\u003e 0:\\n\\t\\t\\tself.history_pos -= 1\\n\\t\\t\\tself.setHistoryPath()\\n\\n\\tdef historyNext(self):\\n\\t\\thlen = len(self.history)\\n\\t\\tif hlen \\u003e 1 and self.history_pos \\u003c (hlen-1):\\n\\t\\t\\tself.history_pos += 1\\n\\t\\t\\tself.setHistoryPath()\\n\\n\\tdef setHistoryPath(self, doZap=True):\\n\\t\\tpath = self.history[self.history_pos][:]\\n\\t\\tref = path.pop()\\n\\t\\tdel self.servicePath[:]\\n\\t\\tself.servicePath += path\\n\\t\\tself.saveRoot()\\n\\t\\troot = path[-1]\\n\\t\\tcur_root = self.getRoot()\\n\\t\\tif cur_root and cur_root != root:\\n\\t\\t\\tself.setRoot(root)\\n\\t\\tif doZap:\\n\\t\\t\\tself.session.nav.playService(ref, adjust=False)\\n\\t\\tif self.dopipzap:\\n\\t\\t\\tself.setCurrentSelection(self.session.pip.getCurrentService())\\n\\t\\telse:\\n\\t\\t\\tself.setCurrentSelection(ref)\\n\\t\\tself.saveChannel(ref)\\n\\n\\tdef saveRoot(self):\\n\\t\\tpath = \'\'\\n\\t\\tfor i in self.servicePath:\\n\\t\\t\\tpath += i.toString()\\n\\t\\t\\tpath += \';\'\\n\\t\\tif path and path != self.lastroot.value:\\n\\t\\t\\tif self.mode == MODE_RADIO and \'FROM BOUQUET \\"bouquets.tv\\"\' in path:\\n\\t\\t\\t\\tself.setModeTv()\\n\\t\\t\\telif self.mode == MODE_TV and \'FROM BOUQUET \\"bouquets.radio\\"\' in path:\\n\\t\\t\\t\\tself.setModeRadio()\\n\\t\\t\\tself.lastroot.value = path\\n\\t\\t\\tself.lastroot.save()\\n\\n\\tdef restoreRoot(self):\\n\\t\\ttmp = [x for x in self.lastroot.value.split(\';\') if x != \'\']\\n\\t\\tcurrent = [x.toString() for x in self.servicePath]\\n\\t\\tif tmp != current or self.rootChanged:\\n\\t\\t\\tself.clearPath()\\n\\t\\t\\tcnt = 0\\n\\t\\t\\tfor i in tmp:\\n\\t\\t\\t\\tself.servicePath.append(eServiceReference(i))\\n\\t\\t\\t\\tcnt += 1\\n\\t\\t\\tif cnt:\\n\\t\\t\\t\\tpath = self.servicePath.pop()\\n\\t\\t\\t\\tself.enterPath(path)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.showFavourites()\\n\\t\\t\\t\\tself.saveRoot()\\n\\t\\t\\tself.rootChanged = False\\n\\n\\tdef preEnterPath(self, refstr):\\n\\t\\tif self.servicePath and self.servicePath[0] != eServiceReference(refstr):\\n\\t\\t\\tpathstr = self.lastroot.value\\n\\t\\t\\tif pathstr is not None and refstr in pathstr:\\n\\t\\t\\t\\tself.restoreRoot()\\n\\t\\t\\t\\tlastservice = eServiceReference(self.lastservice.value)\\n\\t\\t\\t\\tif lastservice.valid():\\n\\t\\t\\t\\t\\tself.setCurrentSelection(lastservice)\\n\\t\\t\\t\\treturn True\\n\\t\\treturn False\\n\\n\\tdef saveChannel(self, ref):\\n\\t\\tif ref is not None:\\n\\t\\t\\trefstr = ref.toString()\\n\\t\\telse:\\n\\t\\t\\trefstr = \\"\\"\\n\\t\\tif refstr != self.lastservice.value and not Components.ParentalControl.parentalControl.isProtected(ref):\\n\\t\\t\\tself.lastservice.value = refstr\\n\\t\\t\\tself.lastservice.save()\\n\\n\\tdef setCurrentServicePath(self, path, doZap=True):\\n\\t\\thlen = len(self.history)\\n\\t\\tif not hlen:\\n\\t\\t\\tself.history.append(path)\\n\\t\\t\\tself.history_pos = 0\\n\\t\\tif hlen == 1:\\n\\t\\t\\tself.history[self.history_pos] = path\\n\\t\\telse:\\n\\t\\t\\tif path in self.history:\\n\\t\\t\\t\\tself.history.remove(path)\\n\\t\\t\\t\\tself.history_pos -= 1\\n\\t\\t\\ttmp = self.history[self.history_pos][:]\\n\\t\\t\\tself.history.append(tmp)\\n\\t\\t\\tself.history_pos += 1\\n\\t\\t\\tself.history[self.history_pos] = path\\n\\t\\tself.setHistoryPath(doZap)\\n\\n\\tdef getCurrentServicePath(self):\\n\\t\\tif self.history:\\n\\t\\t\\treturn self.history[self.history_pos]\\n\\t\\treturn None\\n\\n\\tdef recallPrevService(self):\\n\\t\\thlen = len(self.history)\\n\\t\\tcurrentPlayedRef = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\tif hlen \\u003e 0 and currentPlayedRef and self.history[self.history_pos][-1] != currentPlayedRef:\\n\\t\\t\\tself.addToHistory(currentPlayedRef)\\n\\t\\t\\thlen = len(self.history)\\n\\t\\tif hlen \\u003e 1:\\n\\t\\t\\tif self.history_pos == hlen-1:\\n\\t\\t\\t\\ttmp = self.history[self.history_pos]\\n\\t\\t\\t\\tself.history[self.history_pos] = self.history[self.history_pos-1]\\n\\t\\t\\t\\tself.history[self.history_pos-1] = tmp\\n\\t\\t\\telse:\\n\\t\\t\\t\\ttmp = self.history[self.history_pos+1]\\n\\t\\t\\t\\tself.history[self.history_pos+1] = self.history[self.history_pos]\\n\\t\\t\\t\\tself.history[self.history_pos] = tmp\\n\\t\\t\\tself.setHistoryPath()\\n\\n\\tdef cancel(self):\\n\\t\\tif self.revertMode is None:\\n\\t\\t\\tself.restoreRoot()\\n\\t\\t\\tif self.dopipzap:\\n\\t\\t\\t\\t# This unfortunately won\'t work with subservices\\n\\t\\t\\t\\tself.setCurrentSelection(self.session.pip.getCurrentService())\\n\\t\\t\\telse:\\n\\t\\t\\t\\tlastservice = eServiceReference(self.lastservice.value)\\n\\t\\t\\t\\tif lastservice.valid() and self.getCurrentSelection() != lastservice:\\n\\t\\t\\t\\t\\tself.setCurrentSelection(lastservice)\\n\\t\\tself.asciiOff()\\n\\t\\tself.zapBack()\\n\\t\\tself.correctChannelNumber()\\n\\t\\tself.editMode = False\\n\\t\\tself.protectContextMenu = True\\n\\t\\tself.close(None)\\n\\n\\tdef zapBack(self):\\n\\t\\tplayingref = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\tif self.startServiceRef and (playingref is None or playingref != self.startServiceRef):\\n\\t\\t\\tself.setStartRoot(self.startRoot)\\n\\t\\t\\tself.new_service_played = True\\n\\t\\t\\tself.session.nav.playService(self.startServiceRef)\\n\\t\\t\\tself.saveChannel(self.startServiceRef)\\n\\t\\telse:\\n\\t\\t\\tself.restoreMode()\\n\\t\\tself.startServiceRef = None\\n\\t\\tself.startRoot = None\\n\\t\\tif self.dopipzap:\\n\\t\\t\\t# This unfortunately won\'t work with subservices\\n\\t\\t\\tself.setCurrentSelection(self.session.pip.getCurrentService())\\n\\t\\telse:\\n\\t\\t\\tlastservice = eServiceReference(self.lastservice.value)\\n\\t\\t\\tif lastservice.valid() and self.getCurrentSelection() == lastservice:\\n\\t\\t\\t\\tpass\\t# keep current selection\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.setCurrentSelection(playingref)\\n\\n\\tdef setStartRoot(self, root):\\n\\t\\tif root:\\n\\t\\t\\tif self.revertMode == MODE_TV:\\n\\t\\t\\t\\tself.setModeTv()\\n\\t\\t\\telif self.revertMode == MODE_RADIO:\\n\\t\\t\\t\\tself.setModeRadio()\\n\\t\\t\\tself.revertMode = None\\n\\t\\t\\tself.enterUserbouquet(root)\\n\\n\\tdef restoreMode(self):\\n\\t\\tif self.revertMode == MODE_TV:\\n\\t\\t\\tself.setModeTv()\\n\\t\\telif self.revertMode == MODE_RADIO:\\n\\t\\t\\tself.setModeRadio()\\n\\t\\tself.revertMode = None\\n\\n\\tdef correctChannelNumber(self):\\n\\t\\tcurrent_ref = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\tif self.dopipzap:\\n\\t\\t\\ttmp_mode = config.servicelist.lastmode.value\\n\\t\\t\\ttmp_root = self.getRoot()\\n\\t\\t\\ttmp_ref = self.getCurrentSelection()\\n\\t\\t\\tpip_ref = self.session.pip.getCurrentService()\\n\\t\\t\\tif tmp_ref and pip_ref and tmp_ref != pip_ref:\\n\\t\\t\\t\\tself.revertMode = None\\n\\t\\t\\t\\treturn\\n\\t\\t\\tif self.mainScreenMode == \\"tv\\":\\n\\t\\t\\t\\tself.setModeTv()\\n\\t\\t\\telif self.mainScreenMode == \\"radio\\":\\n\\t\\t\\t\\tself.setModeRadio()\\n\\t\\t\\tif self.mainScreenRoot:\\n\\t\\t\\t\\tself.setRoot(self.mainScreenRoot)\\n\\t\\t\\t\\tself.setCurrentSelection(current_ref)\\n\\t\\tselected_ref = self.getCurrentSelection()\\n\\t\\tif selected_ref and current_ref and selected_ref.getChannelNum() != current_ref.getChannelNum():\\n\\t\\t\\toldref = self.session.nav.currentlyPlayingServiceReference\\n\\t\\t\\tif oldref and selected_ref == oldref or (oldref != current_ref and selected_ref == current_ref):\\n\\t\\t\\t\\tself.session.nav.currentlyPlayingServiceOrGroup = selected_ref\\n\\t\\t\\t\\tself.session.nav.pnav.navEvent(iPlayableService.evStart)\\n\\t\\tif self.dopipzap:\\n\\t\\t\\tif tmp_mode == \\"tv\\":\\n\\t\\t\\t\\tself.setModeTv()\\n\\t\\t\\telif tmp_mode == \\"radio\\":\\n\\t\\t\\t\\tself.setModeRadio()\\n\\t\\t\\tself.enterUserbouquet(tmp_root)\\n\\t\\t\\ttitle = self.instance.getTitle()\\n\\t\\t\\tpos = title.find(\\" (\\")\\n\\t\\t\\tif pos != -1:\\n\\t\\t\\t\\ttitle = title[:pos]\\n\\t\\t\\t\\ttitle += _(\\" (PiP)\\")\\n\\t\\t\\t\\tself.setTitle(title)\\n\\t\\t\\t\\tself.buildTitleString()\\n\\t\\t\\tif tmp_ref and pip_ref and tmp_ref.getChannelNum() != pip_ref.getChannelNum():\\n\\t\\t\\t\\tself.session.pip.currentService = tmp_ref\\n\\t\\t\\tself.setCurrentSelection(tmp_ref)\\n\\t\\tself.revertMode = None\\n\\nclass RadioInfoBar(Screen):\\n\\tdef __init__(self, session):\\n\\t\\tScreen.__init__(self, session)\\n\\t\\tself[\\"RdsDecoder\\"] = RdsDecoder(self.session.nav)\\n\\nclass ChannelSelectionRadio(ChannelSelectionBase, ChannelSelectionEdit, ChannelSelectionEPG, InfoBarBase, SelectionEventInfo):\\n\\tALLOW_SUSPEND = True\\n\\n\\tdef __init__(self, session, infobar):\\n\\t\\tChannelSelectionBase.__init__(self, session)\\n\\t\\tChannelSelectionEdit.__init__(self)\\n\\t\\tChannelSelectionEPG.__init__(self)\\n\\t\\tInfoBarBase.__init__(self)\\n\\t\\tSelectionEventInfo.__init__(self)\\n\\t\\tself.infobar = infobar\\n\\t\\tself.startServiceRef = None\\n\\t\\tself.onLayoutFinish.append(self.onCreate)\\n\\n\\t\\tself.info = session.instantiateDialog(RadioInfoBar) # our simple infobar\\n\\n\\t\\tself[\\"actions\\"] = ActionMap([\\"OkCancelActions\\", \\"TvRadioActions\\"],\\n\\t\\t\\t{\\n\\t\\t\\t\\t\\"keyTV\\": self.cancel,\\n\\t\\t\\t\\t\\"keyRadio\\": self.cancel,\\n\\t\\t\\t\\t\\"cancel\\": self.cancel,\\n\\t\\t\\t\\t\\"ok\\": self.channelSelected,\\n\\t\\t\\t})\\n\\n\\t\\tself.__event_tracker = ServiceEventTracker(screen=self, eventmap=\\n\\t\\t\\t{\\n\\t\\t\\t\\tiPlayableService.evStart: self.__evServiceStart,\\n\\t\\t\\t\\tiPlayableService.evEnd: self.__evServiceEnd\\n\\t\\t\\t})\\n\\n########## RDS Radiotext / Rass Support BEGIN\\n\\t\\tself.infobar = infobar # reference to real infobar (the one and only)\\n\\t\\tself[\\"RdsDecoder\\"] = self.info[\\"RdsDecoder\\"]\\n\\t\\tself[\\"RdsActions\\"] = HelpableActionMap(self, \\"InfobarRdsActions\\",\\n\\t\\t{\\n\\t\\t\\t\\"startRassInteractive\\": (self.startRassInteractive, _(\\"View Rass interactive...\\"))\\n\\t\\t},-1)\\n\\t\\tself[\\"RdsActions\\"].setEnabled(False)\\n\\t\\tinfobar.rds_display.onRassInteractivePossibilityChanged.append(self.RassInteractivePossibilityChanged)\\n\\t\\tself.onClose.append(self.__onClose)\\n\\t\\tself.onExecBegin.append(self.__onExecBegin)\\n\\t\\tself.onExecEnd.append(self.__onExecEnd)\\n\\n\\tdef __onClose(self):\\n\\t\\tlastservice = eServiceReference(config.tv.lastservice.value)\\n\\t\\tself.session.nav.playService(lastservice)\\n\\n\\tdef startRassInteractive(self):\\n\\t\\tself.info.hide();\\n\\t\\tself.infobar.rass_interactive = self.session.openWithCallback(self.RassInteractiveClosed, RassInteractive)\\n\\n\\tdef RassInteractiveClosed(self):\\n\\t\\tself.info.show()\\n\\t\\tself.infobar.rass_interactive = None\\n\\t\\tself.infobar.RassSlidePicChanged()\\n\\n\\tdef RassInteractivePossibilityChanged(self, state):\\n\\t\\tself[\\"RdsActions\\"].setEnabled(state)\\n########## RDS Radiotext / Rass Support END\\n\\n\\tdef __onExecBegin(self):\\n\\t\\tself.info.show()\\n\\n\\tdef __onExecEnd(self):\\n\\t\\tself.info.hide()\\n\\n\\tdef cancel(self):\\n\\t\\tself.infobar.rds_display.onRassInteractivePossibilityChanged.remove(self.RassInteractivePossibilityChanged)\\n\\t\\tself.info.hide()\\n\\t\\t#set previous tv service\\n\\t\\tself.close(None)\\n\\n\\tdef __evServiceStart(self):\\n\\t\\tservice = self.session.nav.getCurrentService()\\n\\t\\tif service:\\n\\t\\t\\tinfo = service.info()\\n\\t\\t\\tif info:\\n\\t\\t\\t\\trefstr = info.getInfoString(iServiceInformation.sServiceref)\\n\\t\\t\\t\\tself.servicelist.setPlayableIgnoreService(eServiceReference(refstr))\\n\\n\\tdef __evServiceEnd(self):\\n\\t\\tself.servicelist.setPlayableIgnoreService(eServiceReference())\\n\\n\\tdef saveRoot(self):\\n\\t\\tpath = \'\'\\n\\t\\tfor i in self.servicePathRadio:\\n\\t\\t\\tpath += i.toString()\\n\\t\\t\\tpath += \';\'\\n\\t\\tif path and path != config.radio.lastroot.value:\\n\\t\\t\\tconfig.radio.lastroot.value = path\\n\\t\\t\\tconfig.radio.lastroot.save()\\n\\n\\tdef restoreRoot(self):\\n\\t\\ttmp = [x for x in config.radio.lastroot.value.split(\';\') if x != \'\']\\n\\t\\tcurrent = [x.toString() for x in self.servicePath]\\n\\t\\tif tmp != current or self.rootChanged:\\n\\t\\t\\tcnt = 0\\n\\t\\t\\tfor i in tmp:\\n\\t\\t\\t\\tself.servicePathRadio.append(eServiceReference(i))\\n\\t\\t\\t\\tcnt += 1\\n\\t\\t\\tif cnt:\\n\\t\\t\\t\\tpath = self.servicePathRadio.pop()\\n\\t\\t\\t\\tself.enterPath(path)\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.showFavourites()\\n\\t\\t\\t\\tself.saveRoot()\\n\\t\\t\\tself.rootChanged = False\\n\\n\\tdef preEnterPath(self, refstr):\\n\\t\\tif self.servicePathRadio and self.servicePathRadio[0] != eServiceReference(refstr):\\n\\t\\t\\tpathstr = config.radio.lastroot.value\\n\\t\\t\\tif pathstr is not None and refstr in pathstr:\\n\\t\\t\\t\\tself.restoreRoot()\\n\\t\\t\\t\\tlastservice = eServiceReference(config.radio.lastservice.value)\\n\\t\\t\\t\\tif lastservice.valid():\\n\\t\\t\\t\\t\\tself.setCurrentSelection(lastservice)\\n\\t\\t\\t\\treturn True\\n\\t\\treturn False\\n\\n\\tdef onCreate(self):\\n\\t\\tself.setRadioMode()\\n\\t\\tself.restoreRoot()\\n\\t\\tlastservice = eServiceReference(config.radio.lastservice.value)\\n\\t\\tif lastservice.valid():\\n\\t\\t\\tself.servicelist.setCurrent(lastservice)\\n\\t\\t\\tself.session.nav.playService(lastservice)\\n\\t\\telse:\\n\\t\\t\\tself.session.nav.stopService()\\n\\t\\tself.info.show()\\n\\n\\tdef channelSelected(self, doClose=False): # just return selected service\\n\\t\\tref = self.getCurrentSelection()\\n\\t\\tif self.movemode:\\n\\t\\t\\tself.toggleMoveMarked()\\n\\t\\telif (ref.flags \\u0026 eServiceReference.flagDirectory) == eServiceReference.flagDirectory:\\n\\t\\t\\tself.enterPath(ref)\\n\\t\\t\\tself.gotoCurrentServiceOrProvider(ref)\\n\\t\\telif self.bouquet_mark_edit != OFF:\\n\\t\\t\\tif not (self.bouquet_mark_edit == EDIT_ALTERNATIVES and ref.flags \\u0026 eServiceReference.isGroup):\\n\\t\\t\\t\\tself.doMark()\\n\\t\\telif not (ref.flags \\u0026 eServiceReference.isMarker): # no marker\\n\\t\\t\\tcur_root = self.getRoot()\\n\\t\\t\\tif not cur_root or not (cur_root.flags \\u0026 eServiceReference.isGroup):\\n\\t\\t\\t\\tplayingref = self.session.nav.getCurrentlyPlayingServiceOrGroup()\\n\\t\\t\\t\\tif playingref is None or playingref != ref:\\n\\t\\t\\t\\t\\tself.session.nav.playService(ref)\\n\\t\\t\\t\\t\\tconfig.radio.lastservice.value = ref.toString()\\n\\t\\t\\t\\t\\tconfig.radio.lastservice.save()\\n\\t\\t\\t\\tself.saveRoot()\\n\\n\\tdef zapBack(self):\\n\\t\\tself.channelSelected()\\n\\nclass SimpleChannelSelection(ChannelSelectionBase, SelectionEventInfo):\\n\\tdef __init__(self, session, title, currentBouquet=False, returnBouquet=False, setService=None, setBouquet=None):\\n\\t\\tChannelSelectionBase.__init__(self, session)\\n\\t\\tSelectionEventInfo.__init__(self)\\n\\t\\tself[\\"actions\\"] = ActionMap([\\"OkCancelActions\\", \\"TvRadioActions\\"],\\n\\t\\t\\t{\\n\\t\\t\\t\\t\\"cancel\\": self.close,\\n\\t\\t\\t\\t\\"ok\\": self.channelSelected,\\n\\t\\t\\t\\t\\"keyRadio\\": self.setModeRadio,\\n\\t\\t\\t\\t\\"keyTV\\": self.setModeTv,\\n\\t\\t\\t})\\n\\t\\tself.bouquet_mark_edit = OFF\\n\\t\\tif isinstance(title, str):\\n\\t\\t\\tself.maintitle = title\\n\\t\\tself.currentBouquet = currentBouquet\\n\\t\\tself.returnBouquet = returnBouquet\\n\\t\\tself.setService = setService\\n\\t\\tself.setBouquet = setBouquet\\n\\t\\tself.onLayoutFinish.append(self.layoutFinished)\\n\\n\\tdef layoutFinished(self):\\n\\t\\tself.setModeTv()\\n\\t\\tif self.currentBouquet or self.setBouquet:\\n\\t\\t\\tref = self.setBouquet or Screens.InfoBar.InfoBar.instance.servicelist.getRoot()\\n\\t\\t\\tif ref:\\n\\t\\t\\t\\tself.enterPath(ref)\\n\\t\\t\\t\\tself.gotoCurrentServiceOrProvider(ref)\\n\\t\\tif self.setService:\\n\\t\\t\\tself.setCurrentSelection(self.setService)\\n\\n\\tdef saveRoot(self):\\n\\t\\tpass\\n\\n\\tdef keyRecord(self):\\n\\t\\treturn 0\\n\\n\\tdef channelSelected(self): # just return selected service\\n\\t\\tref = self.getCurrentSelection()\\n\\t\\tif (ref.flags \\u0026 eServiceReference.flagDirectory) == eServiceReference.flagDirectory:\\n\\t\\t\\tself.enterPath(ref)\\n\\t\\t\\tself.gotoCurrentServiceOrProvider(ref)\\n\\t\\telif not (ref.flags \\u0026 eServiceReference.isMarker):\\n\\t\\t\\tref = self.getCurrentSelection()\\n\\t\\t\\tif self.returnBouquet and len(self.servicePath):\\n\\t\\t\\t\\tself.close(ref, self.servicePath[-1])\\n\\t\\t\\telse:\\n\\t\\t\\t\\tself.close(ref)\\n\\n\\tdef setModeTv(self):\\n\\t\\tself.setTvMode()\\n\\t\\tself.showFavourites()\\n\\n\\tdef setModeRadio(self):\\n\\t\\tself.setRadioMode()\\n\\t\\tself.showFavourites()\\n"}\n'
line: b'{"repo_name":"giorgiop/scipy","ref":"refs/heads/master","path":"scipy/interpolate/tests/test_fitpack2.py","content":"#!/usr/bin/env python\\n# Created by Pearu Peterson, June 2003\\nfrom __future__ import division, print_function, absolute_import\\n\\nimport warnings\\n\\nimport numpy as np\\nfrom numpy.testing import (assert_equal, assert_almost_equal, assert_array_equal,\\n        assert_array_almost_equal, assert_allclose, assert_raises, TestCase,\\n        run_module_suite)\\nfrom numpy import array, diff, linspace, meshgrid, ones, pi, shape\\nfrom scipy.interpolate.fitpack import bisplrep, bisplev\\nfrom scipy.interpolate.fitpack2 import (UnivariateSpline,\\n        LSQUnivariateSpline, InterpolatedUnivariateSpline,\\n        LSQBivariateSpline, SmoothBivariateSpline, RectBivariateSpline,\\n        LSQSphereBivariateSpline, SmoothSphereBivariateSpline,\\n        RectSphereBivariateSpline)\\n\\n\\nclass TestUnivariateSpline(TestCase):\\n    def test_linear_constant(self):\\n        x = [1,2,3]\\n        y = [3,3,3]\\n        lut = UnivariateSpline(x,y,k=1)\\n        assert_array_almost_equal(lut.get_knots(),[1,3])\\n        assert_array_almost_equal(lut.get_coeffs(),[3,3])\\n        assert_almost_equal(lut.get_residual(),0.0)\\n        assert_array_almost_equal(lut([1,1.5,2]),[3,3,3])\\n\\n    def test_preserve_shape(self):\\n        x = [1, 2, 3]\\n        y = [0, 2, 4]\\n        lut = UnivariateSpline(x, y, k=1)\\n        arg = 2\\n        assert_equal(shape(arg), shape(lut(arg)))\\n        assert_equal(shape(arg), shape(lut(arg, nu=1)))\\n        arg = [1.5, 2, 2.5]\\n        assert_equal(shape(arg), shape(lut(arg)))\\n        assert_equal(shape(arg), shape(lut(arg, nu=1)))\\n\\n    def test_linear_1d(self):\\n        x = [1,2,3]\\n        y = [0,2,4]\\n        lut = UnivariateSpline(x,y,k=1)\\n        assert_array_almost_equal(lut.get_knots(),[1,3])\\n        assert_array_almost_equal(lut.get_coeffs(),[0,4])\\n        assert_almost_equal(lut.get_residual(),0.0)\\n        assert_array_almost_equal(lut([1,1.5,2]),[0,1,2])\\n\\n    def test_subclassing(self):\\n        # See #731\\n\\n        class ZeroSpline(UnivariateSpline):\\n            def __call__(self, x):\\n                return 0*array(x)\\n\\n        sp = ZeroSpline([1,2,3,4,5], [3,2,3,2,3], k=2)\\n        assert_array_equal(sp([1.5, 2.5]), [0., 0.])\\n\\n    def test_empty_input(self):\\n        # Test whether empty input returns an empty output. Ticket 1014\\n        x = [1,3,5,7,9]\\n        y = [0,4,9,12,21]\\n        spl = UnivariateSpline(x, y, k=3)\\n        assert_array_equal(spl([]), array([]))\\n\\n    def test_resize_regression(self):\\n        \\"\\"\\"Regression test for #1375.\\"\\"\\"\\n        x = [-1., -0.65016502, -0.58856235, -0.26903553, -0.17370892,\\n             -0.10011001, 0., 0.10011001, 0.17370892, 0.26903553, 0.58856235,\\n             0.65016502, 1.]\\n        y = [1.,0.62928599, 0.5797223, 0.39965815, 0.36322694, 0.3508061,\\n             0.35214793, 0.3508061, 0.36322694, 0.39965815, 0.5797223,\\n             0.62928599, 1.]\\n        w = [1.00000000e+12, 6.88875973e+02, 4.89314737e+02, 4.26864807e+02,\\n             6.07746770e+02, 4.51341444e+02, 3.17480210e+02, 4.51341444e+02,\\n             6.07746770e+02, 4.26864807e+02, 4.89314737e+02, 6.88875973e+02,\\n             1.00000000e+12]\\n        spl = UnivariateSpline(x=x, y=y, w=w, s=None)\\n        desired = array([0.35100374, 0.51715855, 0.87789547, 0.98719344])\\n        assert_allclose(spl([0.1, 0.5, 0.9, 0.99]), desired, atol=5e-4)\\n\\n    def test_out_of_range_regression(self):\\n        # Test different extrapolation modes. See ticket 3557\\n        x = np.arange(5, dtype=float)\\n        y = x**3\\n\\n        xp = linspace(-8, 13, 100)\\n        xp_zeros = xp.copy()\\n        xp_zeros[np.logical_or(xp_zeros \\u003c 0., xp_zeros \\u003e 4.)] = 0\\n        xp_clip = xp.copy()\\n        xp_clip[xp_clip \\u003c x[0]] = x[0]\\n        xp_clip[xp_clip \\u003e x[-1]] = x[-1]\\n\\n        for cls in [UnivariateSpline, InterpolatedUnivariateSpline]:\\n            spl = cls(x=x, y=y)\\n            for ext in [0, \'extrapolate\']:\\n                assert_allclose(spl(xp, ext=ext), xp**3, atol=1e-16)\\n                assert_allclose(cls(x, y, ext=ext)(xp), xp**3, atol=1e-16)\\n            for ext in [1, \'zeros\']:\\n                assert_allclose(spl(xp, ext=ext), xp_zeros**3, atol=1e-16)\\n                assert_allclose(cls(x, y, ext=ext)(xp), xp_zeros**3, atol=1e-16)\\n            for ext in [2, \'raise\']:\\n                assert_raises(ValueError, spl, xp, **dict(ext=ext))\\n            for ext in [3, \'const\']:\\n                assert_allclose(spl(xp, ext=ext), xp_clip**3, atol=1e-16)\\n                assert_allclose(cls(x, y, ext=ext)(xp), xp_clip**3, atol=1e-16)\\n\\n        # also test LSQUnivariateSpline [which needs explicit knots]\\n        t = spl.get_knots()[3:4]  # interior knots w/ default k=3\\n        spl = LSQUnivariateSpline(x, y, t)\\n        assert_allclose(spl(xp, ext=0), xp**3, atol=1e-16)\\n        assert_allclose(spl(xp, ext=1), xp_zeros**3, atol=1e-16)\\n        assert_raises(ValueError, spl, xp, **dict(ext=2))\\n        assert_allclose(spl(xp, ext=3), xp_clip**3, atol=1e-16)\\n\\n        # also make sure that unknown values for `ext` are caught early\\n        for ext in [-1, \'unknown\']:\\n            spl = UnivariateSpline(x, y)\\n            assert_raises(ValueError, spl, xp, **dict(ext=ext))\\n            assert_raises(ValueError, UnivariateSpline,\\n                    **dict(x=x, y=y, ext=ext))\\n\\n    def test_lsq_fpchec(self):\\n        xs = np.arange(100) * 1.\\n        ys = np.arange(100) * 1.\\n        knots = np.linspace(0, 99, 10)\\n        bbox = (-1, 101)\\n        assert_raises(ValueError, LSQUnivariateSpline, xs, ys, knots,\\n                      bbox=bbox)\\n\\n    def test_derivative_and_antiderivative(self):\\n        # Thin wrappers to splder/splantider, so light smoke test only.\\n        x = np.linspace(0, 1, 70)**3\\n        y = np.cos(x)\\n\\n        spl = UnivariateSpline(x, y, s=0)\\n        spl2 = spl.antiderivative(2).derivative(2)\\n        assert_allclose(spl(0.3), spl2(0.3))\\n\\n        spl2 = spl.antiderivative(1)\\n        assert_allclose(spl2(0.6) - spl2(0.2),\\n                        spl.integral(0.2, 0.6))\\n\\n    def test_nan(self):\\n        # bail out early if the input data contains nans\\n        x = np.arange(10, dtype=float)\\n        y = x**3\\n        for z in [np.nan, np.inf, -np.inf]:\\n            y[-1] = z\\n            assert_raises(ValueError, UnivariateSpline,\\n                    **dict(x=x, y=y, check_finite=True))\\n\\n\\nclass TestLSQBivariateSpline(TestCase):\\n    # NOTE: The systems in this test class are rank-deficient\\n    def test_linear_constant(self):\\n        x = [1,1,1,2,2,2,3,3,3]\\n        y = [1,2,3,1,2,3,1,2,3]\\n        z = [3,3,3,3,3,3,3,3,3]\\n        s = 0.1\\n        tx = [1+s,3-s]\\n        ty = [1+s,3-s]\\n        with warnings.catch_warnings(record=True):  # coefficients of the ...\\n            lut = LSQBivariateSpline(x,y,z,tx,ty,kx=1,ky=1)\\n\\n        assert_almost_equal(lut(2,2), 3.)\\n\\n    def test_bilinearity(self):\\n        x = [1,1,1,2,2,2,3,3,3]\\n        y = [1,2,3,1,2,3,1,2,3]\\n        z = [0,7,8,3,4,7,1,3,4]\\n        s = 0.1\\n        tx = [1+s,3-s]\\n        ty = [1+s,3-s]\\n        with warnings.catch_warnings():\\n            # This seems to fail (ier=1, see ticket 1642).\\n            warnings.simplefilter(\'ignore\', UserWarning)\\n            lut = LSQBivariateSpline(x,y,z,tx,ty,kx=1,ky=1)\\n\\n        tx, ty = lut.get_knots()\\n        for xa, xb in zip(tx[:-1], tx[1:]):\\n            for ya, yb in zip(ty[:-1], ty[1:]):\\n                for t in [0.1, 0.5, 0.9]:\\n                    for s in [0.3, 0.4, 0.7]:\\n                        xp = xa*(1-t) + xb*t\\n                        yp = ya*(1-s) + yb*s\\n                        zp = (+ lut(xa, ya)*(1-t)*(1-s)\\n                              + lut(xb, ya)*t*(1-s)\\n                              + lut(xa, yb)*(1-t)*s\\n                              + lut(xb, yb)*t*s)\\n                        assert_almost_equal(lut(xp,yp), zp)\\n\\n    def test_integral(self):\\n        x = [1,1,1,2,2,2,8,8,8]\\n        y = [1,2,3,1,2,3,1,2,3]\\n        z = array([0,7,8,3,4,7,1,3,4])\\n\\n        s = 0.1\\n        tx = [1+s,3-s]\\n        ty = [1+s,3-s]\\n        with warnings.catch_warnings(record=True):  # coefficients of the ...\\n            lut = LSQBivariateSpline(x,y,z,tx,ty,kx=1,ky=1)\\n        tx, ty = lut.get_knots()\\n        tz = lut(tx, ty)\\n        trpz = .25*(diff(tx)[:,None]*diff(ty)[None,:]\\n                    * (tz[:-1,:-1]+tz[1:,:-1]+tz[:-1,1:]+tz[1:,1:])).sum()\\n\\n        assert_almost_equal(lut.integral(tx[0], tx[-1], ty[0], ty[-1]),\\n                            trpz)\\n\\n    def test_empty_input(self):\\n        # Test whether empty inputs returns an empty output. Ticket 1014\\n        x = [1,1,1,2,2,2,3,3,3]\\n        y = [1,2,3,1,2,3,1,2,3]\\n        z = [3,3,3,3,3,3,3,3,3]\\n        s = 0.1\\n        tx = [1+s,3-s]\\n        ty = [1+s,3-s]\\n        with warnings.catch_warnings(record=True):  # coefficients of the ...\\n            lut = LSQBivariateSpline(x,y,z,tx,ty,kx=1,ky=1)\\n\\n        assert_array_equal(lut([], []), np.zeros((0,0)))\\n        assert_array_equal(lut([], [], grid=False), np.zeros((0,)))\\n\\n\\nclass TestSmoothBivariateSpline(TestCase):\\n    def test_linear_constant(self):\\n        x = [1,1,1,2,2,2,3,3,3]\\n        y = [1,2,3,1,2,3,1,2,3]\\n        z = [3,3,3,3,3,3,3,3,3]\\n        lut = SmoothBivariateSpline(x,y,z,kx=1,ky=1)\\n        assert_array_almost_equal(lut.get_knots(),([1,1,3,3],[1,1,3,3]))\\n        assert_array_almost_equal(lut.get_coeffs(),[3,3,3,3])\\n        assert_almost_equal(lut.get_residual(),0.0)\\n        assert_array_almost_equal(lut([1,1.5,2],[1,1.5]),[[3,3],[3,3],[3,3]])\\n\\n    def test_linear_1d(self):\\n        x = [1,1,1,2,2,2,3,3,3]\\n        y = [1,2,3,1,2,3,1,2,3]\\n        z = [0,0,0,2,2,2,4,4,4]\\n        lut = SmoothBivariateSpline(x,y,z,kx=1,ky=1)\\n        assert_array_almost_equal(lut.get_knots(),([1,1,3,3],[1,1,3,3]))\\n        assert_array_almost_equal(lut.get_coeffs(),[0,0,4,4])\\n        assert_almost_equal(lut.get_residual(),0.0)\\n        assert_array_almost_equal(lut([1,1.5,2],[1,1.5]),[[0,0],[1,1],[2,2]])\\n\\n    def test_integral(self):\\n        x = [1,1,1,2,2,2,4,4,4]\\n        y = [1,2,3,1,2,3,1,2,3]\\n        z = array([0,7,8,3,4,7,1,3,4])\\n\\n        with warnings.catch_warnings():\\n            # This seems to fail (ier=1, see ticket 1642).\\n            warnings.simplefilter(\'ignore\', UserWarning)\\n            lut = SmoothBivariateSpline(x, y, z, kx=1, ky=1, s=0)\\n\\n        tx = [1,2,4]\\n        ty = [1,2,3]\\n\\n        tz = lut(tx, ty)\\n        trpz = .25*(diff(tx)[:,None]*diff(ty)[None,:]\\n                    * (tz[:-1,:-1]+tz[1:,:-1]+tz[:-1,1:]+tz[1:,1:])).sum()\\n        assert_almost_equal(lut.integral(tx[0], tx[-1], ty[0], ty[-1]), trpz)\\n\\n        lut2 = SmoothBivariateSpline(x, y, z, kx=2, ky=2, s=0)\\n        assert_almost_equal(lut2.integral(tx[0], tx[-1], ty[0], ty[-1]), trpz,\\n                            decimal=0)  # the quadratures give 23.75 and 23.85\\n\\n        tz = lut(tx[:-1], ty[:-1])\\n        trpz = .25*(diff(tx[:-1])[:,None]*diff(ty[:-1])[None,:]\\n                    * (tz[:-1,:-1]+tz[1:,:-1]+tz[:-1,1:]+tz[1:,1:])).sum()\\n        assert_almost_equal(lut.integral(tx[0], tx[-2], ty[0], ty[-2]), trpz)\\n\\n    def test_rerun_lwrk2_too_small(self):\\n        # in this setting, lwrk2 is too small in the default run. Here we\\n        # check for equality with the bisplrep/bisplev output because there,\\n        # an automatic re-run of the spline representation is done if ier\\u003e10.\\n        x = np.linspace(-2, 2, 80)\\n        y = np.linspace(-2, 2, 80)\\n        z = x + y\\n        xi = np.linspace(-1, 1, 100)\\n        yi = np.linspace(-2, 2, 100)\\n        tck = bisplrep(x, y, z)\\n        res1 = bisplev(xi, yi, tck)\\n        interp_ = SmoothBivariateSpline(x, y, z)\\n        res2 = interp_(xi, yi)\\n        assert_almost_equal(res1, res2)\\n\\n\\nclass TestLSQSphereBivariateSpline(TestCase):\\n    def setUp(self):\\n        # define the input data and coordinates\\n        ntheta, nphi = 70, 90\\n        theta = linspace(0.5/(ntheta - 1), 1 - 0.5/(ntheta - 1), ntheta) * pi\\n        phi = linspace(0.5/(nphi - 1), 1 - 0.5/(nphi - 1), nphi) * 2. * pi\\n        data = ones((theta.shape[0], phi.shape[0]))\\n        # define knots and extract data values at the knots\\n        knotst = theta[::5]\\n        knotsp = phi[::5]\\n        knotdata = data[::5, ::5]\\n        # calculate spline coefficients\\n        lats, lons = meshgrid(theta, phi)\\n        lut_lsq = LSQSphereBivariateSpline(lats.ravel(), lons.ravel(),\\n                                           data.T.ravel(), knotst, knotsp)\\n        self.lut_lsq = lut_lsq\\n        self.data = knotdata\\n        self.new_lons, self.new_lats = knotsp, knotst\\n\\n    def test_linear_constant(self):\\n        assert_almost_equal(self.lut_lsq.get_residual(), 0.0)\\n        assert_array_almost_equal(self.lut_lsq(self.new_lats, self.new_lons),\\n                                  self.data)\\n\\n    def test_empty_input(self):\\n        assert_array_almost_equal(self.lut_lsq([], []), np.zeros((0,0)))\\n        assert_array_almost_equal(self.lut_lsq([], [], grid=False), np.zeros((0,)))\\n\\n\\nclass TestSmoothSphereBivariateSpline(TestCase):\\n    def setUp(self):\\n        theta = array([.25*pi, .25*pi, .25*pi, .5*pi, .5*pi, .5*pi, .75*pi,\\n                       .75*pi, .75*pi])\\n        phi = array([.5 * pi, pi, 1.5 * pi, .5 * pi, pi, 1.5 * pi, .5 * pi, pi,\\n                     1.5 * pi])\\n        r = array([3, 3, 3, 3, 3, 3, 3, 3, 3])\\n        self.lut = SmoothSphereBivariateSpline(theta, phi, r, s=1E10)\\n\\n    def test_linear_constant(self):\\n        assert_almost_equal(self.lut.get_residual(), 0.)\\n        assert_array_almost_equal(self.lut([1, 1.5, 2],[1, 1.5]),\\n                                  [[3, 3], [3, 3], [3, 3]])\\n\\n    def test_empty_input(self):\\n        assert_array_almost_equal(self.lut([], []), np.zeros((0,0)))\\n        assert_array_almost_equal(self.lut([], [], grid=False), np.zeros((0,)))\\n\\n\\nclass TestRectBivariateSpline(TestCase):\\n    def test_defaults(self):\\n        x = array([1,2,3,4,5])\\n        y = array([1,2,3,4,5])\\n        z = array([[1,2,1,2,1],[1,2,1,2,1],[1,2,3,2,1],[1,2,2,2,1],[1,2,1,2,1]])\\n        lut = RectBivariateSpline(x,y,z)\\n        assert_array_almost_equal(lut(x,y),z)\\n\\n    def test_evaluate(self):\\n        x = array([1,2,3,4,5])\\n        y = array([1,2,3,4,5])\\n        z = array([[1,2,1,2,1],[1,2,1,2,1],[1,2,3,2,1],[1,2,2,2,1],[1,2,1,2,1]])\\n        lut = RectBivariateSpline(x,y,z)\\n\\n        xi = [1, 2.3, 5.3, 0.5, 3.3, 1.2, 3]\\n        yi = [1, 3.3, 1.2, 4.0, 5.0, 1.0, 3]\\n        zi = lut.ev(xi, yi)\\n        zi2 = array([lut(xp, yp)[0,0] for xp, yp in zip(xi, yi)])\\n\\n        assert_almost_equal(zi, zi2)\\n\\n    def test_derivatives_grid(self):\\n        x = array([1,2,3,4,5])\\n        y = array([1,2,3,4,5])\\n        z = array([[1,2,1,2,1],[1,2,1,2,1],[1,2,3,2,1],[1,2,2,2,1],[1,2,1,2,1]])\\n        dx = array([[0,0,-20,0,0],[0,0,13,0,0],[0,0,4,0,0],\\n            [0,0,-11,0,0],[0,0,4,0,0]])/6.\\n        dy = array([[4,-1,0,1,-4],[4,-1,0,1,-4],[0,1.5,0,-1.5,0],\\n            [2,.25,0,-.25,-2],[4,-1,0,1,-4]])\\n        dxdy = array([[40,-25,0,25,-40],[-26,16.25,0,-16.25,26],\\n            [-8,5,0,-5,8],[22,-13.75,0,13.75,-22],[-8,5,0,-5,8]])/6.\\n        lut = RectBivariateSpline(x,y,z)\\n        assert_array_almost_equal(lut(x,y,dx=1),dx)\\n        assert_array_almost_equal(lut(x,y,dy=1),dy)\\n        assert_array_almost_equal(lut(x,y,dx=1,dy=1),dxdy)\\n\\n    def test_derivatives(self):\\n        x = array([1,2,3,4,5])\\n        y = array([1,2,3,4,5])\\n        z = array([[1,2,1,2,1],[1,2,1,2,1],[1,2,3,2,1],[1,2,2,2,1],[1,2,1,2,1]])\\n        dx = array([0,0,2./3,0,0])\\n        dy = array([4,-1,0,-.25,-4])\\n        dxdy = array([160,65,0,55,32])/24.\\n        lut = RectBivariateSpline(x,y,z)\\n        assert_array_almost_equal(lut(x,y,dx=1,grid=False),dx)\\n        assert_array_almost_equal(lut(x,y,dy=1,grid=False),dy)\\n        assert_array_almost_equal(lut(x,y,dx=1,dy=1,grid=False),dxdy)\\n\\n    def test_broadcast(self):\\n        x = array([1,2,3,4,5])\\n        y = array([1,2,3,4,5])\\n        z = array([[1,2,1,2,1],[1,2,1,2,1],[1,2,3,2,1],[1,2,2,2,1],[1,2,1,2,1]])\\n        lut = RectBivariateSpline(x,y,z)\\n        assert_allclose(lut(x, y), lut(x[:,None], y[None,:], grid=False))\\n\\n\\nclass TestRectSphereBivariateSpline(TestCase):\\n    def test_defaults(self):\\n        y = linspace(0.01, 2*pi-0.01, 7)\\n        x = linspace(0.01, pi-0.01, 7)\\n        z = array([[1,2,1,2,1,2,1],[1,2,1,2,1,2,1],[1,2,3,2,1,2,1],\\n                   [1,2,2,2,1,2,1],[1,2,1,2,1,2,1],[1,2,2,2,1,2,1],\\n                   [1,2,1,2,1,2,1]])\\n        lut = RectSphereBivariateSpline(x,y,z)\\n        assert_array_almost_equal(lut(x,y),z)\\n\\n    def test_evaluate(self):\\n        y = linspace(0.01, 2*pi-0.01, 7)\\n        x = linspace(0.01, pi-0.01, 7)\\n        z = array([[1,2,1,2,1,2,1],[1,2,1,2,1,2,1],[1,2,3,2,1,2,1],\\n                   [1,2,2,2,1,2,1],[1,2,1,2,1,2,1],[1,2,2,2,1,2,1],\\n                   [1,2,1,2,1,2,1]])\\n        lut = RectSphereBivariateSpline(x,y,z)\\n        yi = [0.2, 1, 2.3, 2.35, 3.0, 3.99, 5.25]\\n        xi = [1.5, 0.4, 1.1, 0.45, 0.2345, 1., 0.0001]\\n        zi = lut.ev(xi, yi)\\n        zi2 = array([lut(xp, yp)[0,0] for xp, yp in zip(xi, yi)])\\n        assert_almost_equal(zi, zi2)\\n\\n    def test_derivatives_grid(self):\\n        y = linspace(0.01, 2*pi-0.01, 7)\\n        x = linspace(0.01, pi-0.01, 7)\\n        z = array([[1,2,1,2,1,2,1],[1,2,1,2,1,2,1],[1,2,3,2,1,2,1],\\n                   [1,2,2,2,1,2,1],[1,2,1,2,1,2,1],[1,2,2,2,1,2,1],\\n                   [1,2,1,2,1,2,1]])\\n\\n        lut = RectSphereBivariateSpline(x,y,z)\\n\\n        y = linspace(0.02, 2*pi-0.02, 7)\\n        x = linspace(0.02, pi-0.02, 7)\\n\\n        assert_allclose(lut(x, y, dtheta=1), _numdiff_2d(lut, x, y, dx=1),\\n                        rtol=1e-4, atol=1e-4)\\n        assert_allclose(lut(x, y, dphi=1), _numdiff_2d(lut, x, y, dy=1),\\n                        rtol=1e-4, atol=1e-4)\\n        assert_allclose(lut(x, y, dtheta=1, dphi=1), _numdiff_2d(lut, x, y, dx=1, dy=1, eps=1e-6),\\n                        rtol=1e-3, atol=1e-3)\\n\\n    def test_derivatives(self):\\n        y = linspace(0.01, 2*pi-0.01, 7)\\n        x = linspace(0.01, pi-0.01, 7)\\n        z = array([[1,2,1,2,1,2,1],[1,2,1,2,1,2,1],[1,2,3,2,1,2,1],\\n                   [1,2,2,2,1,2,1],[1,2,1,2,1,2,1],[1,2,2,2,1,2,1],\\n                   [1,2,1,2,1,2,1]])\\n\\n        lut = RectSphereBivariateSpline(x,y,z)\\n\\n        y = linspace(0.02, 2*pi-0.02, 7)\\n        x = linspace(0.02, pi-0.02, 7)\\n\\n        assert_equal(lut(x, y, dtheta=1, grid=False).shape, x.shape)\\n        assert_allclose(lut(x, y, dtheta=1, grid=False),\\n                        _numdiff_2d(lambda x,y: lut(x,y,grid=False), x, y, dx=1),\\n                        rtol=1e-4, atol=1e-4)\\n        assert_allclose(lut(x, y, dphi=1, grid=False),\\n                        _numdiff_2d(lambda x,y: lut(x,y,grid=False), x, y, dy=1),\\n                        rtol=1e-4, atol=1e-4)\\n        assert_allclose(lut(x, y, dtheta=1, dphi=1, grid=False),\\n                        _numdiff_2d(lambda x,y: lut(x,y,grid=False), x, y, dx=1, dy=1, eps=1e-6),\\n                        rtol=1e-3, atol=1e-3)\\n\\n\\ndef _numdiff_2d(func, x, y, dx=0, dy=0, eps=1e-8):\\n    if dx == 0 and dy == 0:\\n        return func(x, y)\\n    elif dx == 1 and dy == 0:\\n        return (func(x + eps, y) - func(x - eps, y)) / (2*eps)\\n    elif dx == 0 and dy == 1:\\n        return (func(x, y + eps) - func(x, y - eps)) / (2*eps)\\n    elif dx == 1 and dy == 1:\\n        return (func(x + eps, y + eps) - func(x - eps, y + eps)\\n                - func(x + eps, y - eps) + func(x - eps, y - eps)) / (2*eps)**2\\n    else:\\n        raise ValueError(\\"invalid derivative order\\")\\n\\nif __name__ == \\"__main__\\":\\n    run_module_suite()\\n"}\n'
line: b'{"repo_name":"rpmcpp/Audacity","ref":"refs/heads/master","path":"lib-src/lv2/lv2/plugins/eg03-metro.lv2/waflib/Tools/vala.py","content":"#! /usr/bin/env python\\n# encoding: utf-8\\n# WARNING! Do not edit! http://waf.googlecode.com/git/docs/wafbook/single.html#_obtaining_the_waf_file\\n\\nimport os.path,shutil,re\\nfrom waflib import Context,Task,Utils,Logs,Options,Errors\\nfrom waflib.TaskGen import extension,taskgen_method\\nfrom waflib.Configure import conf\\nclass valac(Task.Task):\\n\\tvars=[\\"VALAC\\",\\"VALAC_VERSION\\",\\"VALAFLAGS\\"]\\n\\text_out=[\'.h\']\\n\\tdef run(self):\\n\\t\\tcmd=[self.env[\'VALAC\']]+self.env[\'VALAFLAGS\']\\n\\t\\tcmd.extend([a.abspath()for a in self.inputs])\\n\\t\\tret=self.exec_command(cmd,cwd=self.outputs[0].parent.abspath())\\n\\t\\tif ret:\\n\\t\\t\\treturn ret\\n\\t\\tfor x in self.outputs:\\n\\t\\t\\tif id(x.parent)!=id(self.outputs[0].parent):\\n\\t\\t\\t\\tshutil.move(self.outputs[0].parent.abspath()+os.sep+x.name,x.abspath())\\n\\t\\tif self.generator.dump_deps_node:\\n\\t\\t\\tself.generator.dump_deps_node.write(\'\\\\n\'.join(self.generator.packages))\\n\\t\\treturn ret\\nvalac=Task.update_outputs(valac)\\n@taskgen_method\\ndef init_vala_task(self):\\n\\tself.profile=getattr(self,\'profile\',\'gobject\')\\n\\tif self.profile==\'gobject\':\\n\\t\\tself.uselib=Utils.to_list(getattr(self,\'uselib\',[]))\\n\\t\\tif not\'GOBJECT\'in self.uselib:\\n\\t\\t\\tself.uselib.append(\'GOBJECT\')\\n\\tdef addflags(flags):\\n\\t\\tself.env.append_value(\'VALAFLAGS\',flags)\\n\\tif self.profile:\\n\\t\\taddflags(\'--profile=%s\'%self.profile)\\n\\tif hasattr(self,\'threading\'):\\n\\t\\tif self.profile==\'gobject\':\\n\\t\\t\\tif not\'GTHREAD\'in self.uselib:\\n\\t\\t\\t\\tself.uselib.append(\'GTHREAD\')\\n\\t\\telse:\\n\\t\\t\\tLogs.warn(\\"Profile %s means no threading support\\"%self.profile)\\n\\t\\t\\tself.threading=False\\n\\t\\tif self.threading:\\n\\t\\t\\taddflags(\'--threading\')\\n\\tvalatask=self.valatask\\n\\tself.is_lib=\'cprogram\'not in self.features\\n\\tif self.is_lib:\\n\\t\\taddflags(\'--library=%s\'%self.target)\\n\\t\\th_node=self.path.find_or_declare(\'%s.h\'%self.target)\\n\\t\\tvalatask.outputs.append(h_node)\\n\\t\\taddflags(\'--header=%s\'%h_node.name)\\n\\t\\tvalatask.outputs.append(self.path.find_or_declare(\'%s.vapi\'%self.target))\\n\\t\\tif getattr(self,\'gir\',None):\\n\\t\\t\\tgir_node=self.path.find_or_declare(\'%s.gir\'%self.gir)\\n\\t\\t\\taddflags(\'--gir=%s\'%gir_node.name)\\n\\t\\t\\tvalatask.outputs.append(gir_node)\\n\\tself.vala_target_glib=getattr(self,\'vala_target_glib\',getattr(Options.options,\'vala_target_glib\',None))\\n\\tif self.vala_target_glib:\\n\\t\\taddflags(\'--target-glib=%s\'%self.vala_target_glib)\\n\\taddflags([\'--define=%s\'%x for x in getattr(self,\'vala_defines\',[])])\\n\\tpackages_private=Utils.to_list(getattr(self,\'packages_private\',[]))\\n\\taddflags([\'--pkg=%s\'%x for x in packages_private])\\n\\tdef _get_api_version():\\n\\t\\tapi_version=\'1.0\'\\n\\t\\tif hasattr(Context.g_module,\'API_VERSION\'):\\n\\t\\t\\tversion=Context.g_module.API_VERSION.split(\\".\\")\\n\\t\\t\\tif version[0]==\\"0\\":\\n\\t\\t\\t\\tapi_version=\\"0.\\"+version[1]\\n\\t\\t\\telse:\\n\\t\\t\\t\\tapi_version=version[0]+\\".0\\"\\n\\t\\treturn api_version\\n\\tself.includes=Utils.to_list(getattr(self,\'includes\',[]))\\n\\tself.uselib=self.to_list(getattr(self,\'uselib\',[]))\\n\\tvalatask.install_path=getattr(self,\'install_path\',\'\')\\n\\tvalatask.vapi_path=getattr(self,\'vapi_path\',\'${DATAROOTDIR}/vala/vapi\')\\n\\tvalatask.pkg_name=getattr(self,\'pkg_name\',self.env[\'PACKAGE\'])\\n\\tvalatask.header_path=getattr(self,\'header_path\',\'${INCLUDEDIR}/%s-%s\'%(valatask.pkg_name,_get_api_version()))\\n\\tvalatask.install_binding=getattr(self,\'install_binding\',True)\\n\\tself.packages=packages=Utils.to_list(getattr(self,\'packages\',[]))\\n\\tself.vapi_dirs=vapi_dirs=Utils.to_list(getattr(self,\'vapi_dirs\',[]))\\n\\tincludes=[]\\n\\tif hasattr(self,\'use\'):\\n\\t\\tlocal_packages=Utils.to_list(self.use)[:]\\n\\t\\tseen=[]\\n\\t\\twhile len(local_packages)\\u003e0:\\n\\t\\t\\tpackage=local_packages.pop()\\n\\t\\t\\tif package in seen:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tseen.append(package)\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tpackage_obj=self.bld.get_tgen_by_name(package)\\n\\t\\t\\texcept Errors.WafError:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tpackage_name=package_obj.target\\n\\t\\t\\tpackage_node=package_obj.path\\n\\t\\t\\tpackage_dir=package_node.path_from(self.path)\\n\\t\\t\\tfor task in package_obj.tasks:\\n\\t\\t\\t\\tfor output in task.outputs:\\n\\t\\t\\t\\t\\tif output.name==package_name+\\".vapi\\":\\n\\t\\t\\t\\t\\t\\tvalatask.set_run_after(task)\\n\\t\\t\\t\\t\\t\\tif package_name not in packages:\\n\\t\\t\\t\\t\\t\\t\\tpackages.append(package_name)\\n\\t\\t\\t\\t\\t\\tif package_dir not in vapi_dirs:\\n\\t\\t\\t\\t\\t\\t\\tvapi_dirs.append(package_dir)\\n\\t\\t\\t\\t\\t\\tif package_dir not in includes:\\n\\t\\t\\t\\t\\t\\t\\tincludes.append(package_dir)\\n\\t\\t\\tif hasattr(package_obj,\'use\'):\\n\\t\\t\\t\\tlst=self.to_list(package_obj.use)\\n\\t\\t\\t\\tlst.reverse()\\n\\t\\t\\t\\tlocal_packages=[pkg for pkg in lst if pkg not in seen]+local_packages\\n\\taddflags([\'--pkg=%s\'%p for p in packages])\\n\\tfor vapi_dir in vapi_dirs:\\n\\t\\tv_node=self.path.find_dir(vapi_dir)\\n\\t\\tif not v_node:\\n\\t\\t\\tLogs.warn(\'Unable to locate Vala API directory: %r\'%vapi_dir)\\n\\t\\telse:\\n\\t\\t\\taddflags(\'--vapidir=%s\'%v_node.abspath())\\n\\t\\t\\taddflags(\'--vapidir=%s\'%v_node.get_bld().abspath())\\n\\tself.dump_deps_node=None\\n\\tif self.is_lib and self.packages:\\n\\t\\tself.dump_deps_node=self.path.find_or_declare(\'%s.deps\'%self.target)\\n\\t\\tvalatask.outputs.append(self.dump_deps_node)\\n\\tself.includes.append(self.bld.srcnode.abspath())\\n\\tself.includes.append(self.bld.bldnode.abspath())\\n\\tfor include in includes:\\n\\t\\ttry:\\n\\t\\t\\tself.includes.append(self.path.find_dir(include).abspath())\\n\\t\\t\\tself.includes.append(self.path.find_dir(include).get_bld().abspath())\\n\\t\\texcept AttributeError:\\n\\t\\t\\tLogs.warn(\\"Unable to locate include directory: \'%s\'\\"%include)\\n\\tif self.is_lib and valatask.install_binding:\\n\\t\\theaders_list=[o for o in valatask.outputs if o.suffix()==\\".h\\"]\\n\\t\\ttry:\\n\\t\\t\\tself.install_vheader.source=headers_list\\n\\t\\texcept AttributeError:\\n\\t\\t\\tself.install_vheader=self.bld.install_files(valatask.header_path,headers_list,self.env)\\n\\t\\tvapi_list=[o for o in valatask.outputs if(o.suffix()in(\\".vapi\\",\\".deps\\"))]\\n\\t\\ttry:\\n\\t\\t\\tself.install_vapi.source=vapi_list\\n\\t\\texcept AttributeError:\\n\\t\\t\\tself.install_vapi=self.bld.install_files(valatask.vapi_path,vapi_list,self.env)\\n\\t\\tgir_list=[o for o in valatask.outputs if o.suffix()==\'.gir\']\\n\\t\\ttry:\\n\\t\\t\\tself.install_gir.source=gir_list\\n\\t\\texcept AttributeError:\\n\\t\\t\\tself.install_gir=self.bld.install_files(getattr(self,\'gir_path\',\'${DATAROOTDIR}/gir-1.0\'),gir_list,self.env)\\n@extension(\'.vala\',\'.gs\')\\ndef vala_file(self,node):\\n\\ttry:\\n\\t\\tvalatask=self.valatask\\n\\texcept AttributeError:\\n\\t\\tvalatask=self.valatask=self.create_task(\'valac\')\\n\\t\\tself.init_vala_task()\\n\\tvalatask.inputs.append(node)\\n\\tc_node=node.change_ext(\'.c\')\\n\\tvalatask.outputs.append(c_node)\\n\\tself.source.append(c_node)\\n@conf\\ndef find_valac(self,valac_name,min_version):\\n\\tvalac=self.find_program(valac_name,var=\'VALAC\')\\n\\ttry:\\n\\t\\toutput=self.cmd_and_log(valac+\' --version\')\\n\\texcept Exception:\\n\\t\\tvalac_version=None\\n\\telse:\\n\\t\\tver=re.search(r\'\\\\d+.\\\\d+.\\\\d+\',output).group(0).split(\'.\')\\n\\t\\tvalac_version=tuple([int(x)for x in ver])\\n\\tself.msg(\'Checking for %s version \\u003e= %r\'%(valac_name,min_version),valac_version,valac_version and valac_version\\u003e=min_version)\\n\\tif valac and valac_version\\u003cmin_version:\\n\\t\\tself.fatal(\\"%s version %r is too old, need \\u003e= %r\\"%(valac_name,valac_version,min_version))\\n\\tself.env[\'VALAC_VERSION\']=valac_version\\n\\treturn valac\\n@conf\\ndef check_vala(self,min_version=(0,8,0),branch=None):\\n\\tif not branch:\\n\\t\\tbranch=min_version[:2]\\n\\ttry:\\n\\t\\tfind_valac(self,\'valac-%d.%d\'%(branch[0],branch[1]),min_version)\\n\\texcept self.errors.ConfigurationError:\\n\\t\\tfind_valac(self,\'valac\',min_version)\\n@conf\\ndef check_vala_deps(self):\\n\\tif not self.env[\'HAVE_GOBJECT\']:\\n\\t\\tpkg_args={\'package\':\'gobject-2.0\',\'uselib_store\':\'GOBJECT\',\'args\':\'--cflags --libs\'}\\n\\t\\tif getattr(Options.options,\'vala_target_glib\',None):\\n\\t\\t\\tpkg_args[\'atleast_version\']=Options.options.vala_target_glib\\n\\t\\tself.check_cfg(**pkg_args)\\n\\tif not self.env[\'HAVE_GTHREAD\']:\\n\\t\\tpkg_args={\'package\':\'gthread-2.0\',\'uselib_store\':\'GTHREAD\',\'args\':\'--cflags --libs\'}\\n\\t\\tif getattr(Options.options,\'vala_target_glib\',None):\\n\\t\\t\\tpkg_args[\'atleast_version\']=Options.options.vala_target_glib\\n\\t\\tself.check_cfg(**pkg_args)\\ndef configure(self):\\n\\tself.load(\'gnu_dirs\')\\n\\tself.check_vala_deps()\\n\\tself.check_vala()\\n\\tself.env.VALAFLAGS=[\'-C\',\'--quiet\']\\ndef options(opt):\\n\\topt.load(\'gnu_dirs\')\\n\\tvalaopts=opt.add_option_group(\'Vala Compiler Options\')\\n\\tvalaopts.add_option(\'--vala-target-glib\',default=None,dest=\'vala_target_glib\',metavar=\'MAJOR.MINOR\',help=\'Target version of glib for Vala GObject code generation\')\\n"}\n'
line: b'{"repo_name":"dvliman/jaikuengine","ref":"refs/heads/master","path":".google_appengine/lib/django-1.5/django/contrib/comments/views/moderation.py","content":"from __future__ import absolute_import\\n\\nfrom django import template\\nfrom django.conf import settings\\nfrom django.contrib import comments\\nfrom django.contrib.auth.decorators import login_required, permission_required\\nfrom django.contrib.comments import signals\\nfrom django.contrib.comments.views.utils import next_redirect, confirmation_view\\nfrom django.shortcuts import get_object_or_404, render_to_response\\nfrom django.views.decorators.csrf import csrf_protect\\n\\n\\n@csrf_protect\\n@login_required\\ndef flag(request, comment_id, next=None):\\n    \\"\\"\\"\\n    Flags a comment. Confirmation on GET, action on POST.\\n\\n    Templates: :template:`comments/flag.html`,\\n    Context:\\n        comment\\n            the flagged `comments.comment` object\\n    \\"\\"\\"\\n    comment = get_object_or_404(comments.get_model(), pk=comment_id, site__pk=settings.SITE_ID)\\n\\n    # Flag on POST\\n    if request.method == \'POST\':\\n        perform_flag(request, comment)\\n        return next_redirect(request, fallback=next or \'comments-flag-done\',\\n            c=comment.pk)\\n\\n    # Render a form on GET\\n    else:\\n        return render_to_response(\'comments/flag.html\',\\n            {\'comment\': comment, \\"next\\": next},\\n            template.RequestContext(request)\\n        )\\n\\n@csrf_protect\\n@permission_required(\\"comments.can_moderate\\")\\ndef delete(request, comment_id, next=None):\\n    \\"\\"\\"\\n    Deletes a comment. Confirmation on GET, action on POST. Requires the \\"can\\n    moderate comments\\" permission.\\n\\n    Templates: :template:`comments/delete.html`,\\n    Context:\\n        comment\\n            the flagged `comments.comment` object\\n    \\"\\"\\"\\n    comment = get_object_or_404(comments.get_model(), pk=comment_id, site__pk=settings.SITE_ID)\\n\\n    # Delete on POST\\n    if request.method == \'POST\':\\n        # Flag the comment as deleted instead of actually deleting it.\\n        perform_delete(request, comment)\\n        return next_redirect(request, fallback=next or \'comments-delete-done\',\\n            c=comment.pk)\\n\\n    # Render a form on GET\\n    else:\\n        return render_to_response(\'comments/delete.html\',\\n            {\'comment\': comment, \\"next\\": next},\\n            template.RequestContext(request)\\n        )\\n\\n@csrf_protect\\n@permission_required(\\"comments.can_moderate\\")\\ndef approve(request, comment_id, next=None):\\n    \\"\\"\\"\\n    Approve a comment (that is, mark it as public and non-removed). Confirmation\\n    on GET, action on POST. Requires the \\"can moderate comments\\" permission.\\n\\n    Templates: :template:`comments/approve.html`,\\n    Context:\\n        comment\\n            the `comments.comment` object for approval\\n    \\"\\"\\"\\n    comment = get_object_or_404(comments.get_model(), pk=comment_id, site__pk=settings.SITE_ID)\\n\\n    # Delete on POST\\n    if request.method == \'POST\':\\n        # Flag the comment as approved.\\n        perform_approve(request, comment)\\n        return next_redirect(request, fallback=next or \'comments-approve-done\',\\n            c=comment.pk)\\n\\n    # Render a form on GET\\n    else:\\n        return render_to_response(\'comments/approve.html\',\\n            {\'comment\': comment, \\"next\\": next},\\n            template.RequestContext(request)\\n        )\\n\\n# The following functions actually perform the various flag/aprove/delete\\n# actions. They\'ve been broken out into separate functions to that they\\n# may be called from admin actions.\\n\\ndef perform_flag(request, comment):\\n    \\"\\"\\"\\n    Actually perform the flagging of a comment from a request.\\n    \\"\\"\\"\\n    flag, created = comments.models.CommentFlag.objects.get_or_create(\\n        comment = comment,\\n        user    = request.user,\\n        flag    = comments.models.CommentFlag.SUGGEST_REMOVAL\\n    )\\n    signals.comment_was_flagged.send(\\n        sender  = comment.__class__,\\n        comment = comment,\\n        flag    = flag,\\n        created = created,\\n        request = request,\\n    )\\n\\ndef perform_delete(request, comment):\\n    flag, created = comments.models.CommentFlag.objects.get_or_create(\\n        comment = comment,\\n        user    = request.user,\\n        flag    = comments.models.CommentFlag.MODERATOR_DELETION\\n    )\\n    comment.is_removed = True\\n    comment.save()\\n    signals.comment_was_flagged.send(\\n        sender  = comment.__class__,\\n        comment = comment,\\n        flag    = flag,\\n        created = created,\\n        request = request,\\n    )\\n\\n\\ndef perform_approve(request, comment):\\n    flag, created = comments.models.CommentFlag.objects.get_or_create(\\n        comment = comment,\\n        user    = request.user,\\n        flag    = comments.models.CommentFlag.MODERATOR_APPROVAL,\\n    )\\n\\n    comment.is_removed = False\\n    comment.is_public = True\\n    comment.save()\\n\\n    signals.comment_was_flagged.send(\\n        sender  = comment.__class__,\\n        comment = comment,\\n        flag    = flag,\\n        created = created,\\n        request = request,\\n    )\\n\\n# Confirmation views.\\n\\nflag_done = confirmation_view(\\n    template = \\"comments/flagged.html\\",\\n    doc = \'Displays a \\"comment was flagged\\" success page.\'\\n)\\ndelete_done = confirmation_view(\\n    template = \\"comments/deleted.html\\",\\n    doc = \'Displays a \\"comment was deleted\\" success page.\'\\n)\\napprove_done = confirmation_view(\\n    template = \\"comments/approved.html\\",\\n    doc = \'Displays a \\"comment was approved\\" success page.\'\\n)\\n"}\n'
line: b'{"repo_name":"mjtamlyn/django","ref":"refs/heads/master","path":"django/contrib/auth/hashers.py","content":"import base64\\nimport binascii\\nimport functools\\nimport hashlib\\nimport importlib\\nimport warnings\\nfrom collections import OrderedDict\\n\\nfrom django.conf import settings\\nfrom django.core.exceptions import ImproperlyConfigured\\nfrom django.core.signals import setting_changed\\nfrom django.dispatch import receiver\\nfrom django.utils.crypto import (\\n    constant_time_compare, get_random_string, pbkdf2,\\n)\\nfrom django.utils.encoding import force_bytes, force_text\\nfrom django.utils.module_loading import import_string\\nfrom django.utils.translation import gettext_noop as _\\n\\nUNUSABLE_PASSWORD_PREFIX = \'!\'  # This will never be a valid encoded hash\\nUNUSABLE_PASSWORD_SUFFIX_LENGTH = 40  # number of random chars to add after UNUSABLE_PASSWORD_PREFIX\\n\\n\\ndef is_password_usable(encoded):\\n    if encoded is None or encoded.startswith(UNUSABLE_PASSWORD_PREFIX):\\n        return False\\n    try:\\n        identify_hasher(encoded)\\n    except ValueError:\\n        return False\\n    return True\\n\\n\\ndef check_password(password, encoded, setter=None, preferred=\'default\'):\\n    \\"\\"\\"\\n    Return a boolean of whether the raw password matches the three\\n    part encoded digest.\\n\\n    If setter is specified, it\'ll be called when you need to\\n    regenerate the password.\\n    \\"\\"\\"\\n    if password is None or not is_password_usable(encoded):\\n        return False\\n\\n    preferred = get_hasher(preferred)\\n    hasher = identify_hasher(encoded)\\n\\n    hasher_changed = hasher.algorithm != preferred.algorithm\\n    must_update = hasher_changed or preferred.must_update(encoded)\\n    is_correct = hasher.verify(password, encoded)\\n\\n    # If the hasher didn\'t change (we don\'t protect against enumeration if it\\n    # does) and the password should get updated, try to close the timing gap\\n    # between the work factor of the current encoded password and the default\\n    # work factor.\\n    if not is_correct and not hasher_changed and must_update:\\n        hasher.harden_runtime(password, encoded)\\n\\n    if setter and is_correct and must_update:\\n        setter(password)\\n    return is_correct\\n\\n\\ndef make_password(password, salt=None, hasher=\'default\'):\\n    \\"\\"\\"\\n    Turn a plain-text password into a hash for database storage\\n\\n    Same as encode() but generate a new random salt. If password is None then\\n    return a concatenation of UNUSABLE_PASSWORD_PREFIX and a random string,\\n    which disallows logins. Additional random string reduces chances of gaining\\n    access to staff or superuser accounts. See ticket #20079 for more info.\\n    \\"\\"\\"\\n    if password is None:\\n        return UNUSABLE_PASSWORD_PREFIX + get_random_string(UNUSABLE_PASSWORD_SUFFIX_LENGTH)\\n    hasher = get_hasher(hasher)\\n\\n    if not salt:\\n        salt = hasher.salt()\\n\\n    return hasher.encode(password, salt)\\n\\n\\n@functools.lru_cache()\\ndef get_hashers():\\n    hashers = []\\n    for hasher_path in settings.PASSWORD_HASHERS:\\n        hasher_cls = import_string(hasher_path)\\n        hasher = hasher_cls()\\n        if not getattr(hasher, \'algorithm\'):\\n            raise ImproperlyConfigured(\\"hasher doesn\'t specify an \\"\\n                                       \\"algorithm name: %s\\" % hasher_path)\\n        hashers.append(hasher)\\n    return hashers\\n\\n\\n@functools.lru_cache()\\ndef get_hashers_by_algorithm():\\n    return {hasher.algorithm: hasher for hasher in get_hashers()}\\n\\n\\n@receiver(setting_changed)\\ndef reset_hashers(**kwargs):\\n    if kwargs[\'setting\'] == \'PASSWORD_HASHERS\':\\n        get_hashers.cache_clear()\\n        get_hashers_by_algorithm.cache_clear()\\n\\n\\ndef get_hasher(algorithm=\'default\'):\\n    \\"\\"\\"\\n    Return an instance of a loaded password hasher.\\n\\n    If algorithm is \'default\', return the default hasher. Lazily import hashers\\n    specified in the project\'s settings file if needed.\\n    \\"\\"\\"\\n    if hasattr(algorithm, \'algorithm\'):\\n        return algorithm\\n\\n    elif algorithm == \'default\':\\n        return get_hashers()[0]\\n\\n    else:\\n        hashers = get_hashers_by_algorithm()\\n        try:\\n            return hashers[algorithm]\\n        except KeyError:\\n            raise ValueError(\\"Unknown password hashing algorithm \'%s\'. \\"\\n                             \\"Did you specify it in the PASSWORD_HASHERS \\"\\n                             \\"setting?\\" % algorithm)\\n\\n\\ndef identify_hasher(encoded):\\n    \\"\\"\\"\\n    Return an instance of a loaded password hasher.\\n\\n    Identify hasher algorithm by examining encoded hash, and call\\n    get_hasher() to return hasher. Raise ValueError if\\n    algorithm cannot be identified, or if hasher is not loaded.\\n    \\"\\"\\"\\n    # Ancient versions of Django created plain MD5 passwords and accepted\\n    # MD5 passwords with an empty salt.\\n    if ((len(encoded) == 32 and \'$\' not in encoded) or\\n            (len(encoded) == 37 and encoded.startswith(\'md5$$\'))):\\n        algorithm = \'unsalted_md5\'\\n    # Ancient versions of Django accepted SHA1 passwords with an empty salt.\\n    elif len(encoded) == 46 and encoded.startswith(\'sha1$$\'):\\n        algorithm = \'unsalted_sha1\'\\n    else:\\n        algorithm = encoded.split(\'$\', 1)[0]\\n    return get_hasher(algorithm)\\n\\n\\ndef mask_hash(hash, show=6, char=\\"*\\"):\\n    \\"\\"\\"\\n    Return the given hash, with only the first ``show`` number shown. The\\n    rest are masked with ``char`` for security reasons.\\n    \\"\\"\\"\\n    masked = hash[:show]\\n    masked += char * len(hash[show:])\\n    return masked\\n\\n\\nclass BasePasswordHasher:\\n    \\"\\"\\"\\n    Abstract base class for password hashers\\n\\n    When creating your own hasher, you need to override algorithm,\\n    verify(), encode() and safe_summary().\\n\\n    PasswordHasher objects are immutable.\\n    \\"\\"\\"\\n    algorithm = None\\n    library = None\\n\\n    def _load_library(self):\\n        if self.library is not None:\\n            if isinstance(self.library, (tuple, list)):\\n                name, mod_path = self.library\\n            else:\\n                mod_path = self.library\\n            try:\\n                module = importlib.import_module(mod_path)\\n            except ImportError as e:\\n                raise ValueError(\\"Couldn\'t load %r algorithm library: %s\\" %\\n                                 (self.__class__.__name__, e))\\n            return module\\n        raise ValueError(\\"Hasher %r doesn\'t specify a library attribute\\" %\\n                         self.__class__.__name__)\\n\\n    def salt(self):\\n        \\"\\"\\"Generate a cryptographically secure nonce salt in ASCII.\\"\\"\\"\\n        return get_random_string()\\n\\n    def verify(self, password, encoded):\\n        \\"\\"\\"Check if the given password is correct.\\"\\"\\"\\n        raise NotImplementedError(\'subclasses of BasePasswordHasher must provide a verify() method\')\\n\\n    def encode(self, password, salt):\\n        \\"\\"\\"\\n        Create an encoded database value.\\n\\n        The result is normally formatted as \\"algorithm$salt$hash\\" and\\n        must be fewer than 128 characters.\\n        \\"\\"\\"\\n        raise NotImplementedError(\'subclasses of BasePasswordHasher must provide an encode() method\')\\n\\n    def safe_summary(self, encoded):\\n        \\"\\"\\"\\n        Return a summary of safe values.\\n\\n        The result is a dictionary and will be used where the password field\\n        must be displayed to construct a safe representation of the password.\\n        \\"\\"\\"\\n        raise NotImplementedError(\'subclasses of BasePasswordHasher must provide a safe_summary() method\')\\n\\n    def must_update(self, encoded):\\n        return False\\n\\n    def harden_runtime(self, password, encoded):\\n        \\"\\"\\"\\n        Bridge the runtime gap between the work factor supplied in `encoded`\\n        and the work factor suggested by this hasher.\\n\\n        Taking PBKDF2 as an example, if `encoded` contains 20000 iterations and\\n        `self.iterations` is 30000, this method should run password through\\n        another 10000 iterations of PBKDF2. Similar approaches should exist\\n        for any hasher that has a work factor. If not, this method should be\\n        defined as a no-op to silence the warning.\\n        \\"\\"\\"\\n        warnings.warn(\'subclasses of BasePasswordHasher should provide a harden_runtime() method\')\\n\\n\\nclass PBKDF2PasswordHasher(BasePasswordHasher):\\n    \\"\\"\\"\\n    Secure password hashing using the PBKDF2 algorithm (recommended)\\n\\n    Configured to use PBKDF2 + HMAC + SHA256.\\n    The result is a 64 byte binary string.  Iterations may be changed\\n    safely but you must rename the algorithm if you change SHA256.\\n    \\"\\"\\"\\n    algorithm = \\"pbkdf2_sha256\\"\\n    iterations = 100000\\n    digest = hashlib.sha256\\n\\n    def encode(self, password, salt, iterations=None):\\n        assert password is not None\\n        assert salt and \'$\' not in salt\\n        if not iterations:\\n            iterations = self.iterations\\n        hash = pbkdf2(password, salt, iterations, digest=self.digest)\\n        hash = base64.b64encode(hash).decode(\'ascii\').strip()\\n        return \\"%s$%d$%s$%s\\" % (self.algorithm, iterations, salt, hash)\\n\\n    def verify(self, password, encoded):\\n        algorithm, iterations, salt, hash = encoded.split(\'$\', 3)\\n        assert algorithm == self.algorithm\\n        encoded_2 = self.encode(password, salt, int(iterations))\\n        return constant_time_compare(encoded, encoded_2)\\n\\n    def safe_summary(self, encoded):\\n        algorithm, iterations, salt, hash = encoded.split(\'$\', 3)\\n        assert algorithm == self.algorithm\\n        return OrderedDict([\\n            (_(\'algorithm\'), algorithm),\\n            (_(\'iterations\'), iterations),\\n            (_(\'salt\'), mask_hash(salt)),\\n            (_(\'hash\'), mask_hash(hash)),\\n        ])\\n\\n    def must_update(self, encoded):\\n        algorithm, iterations, salt, hash = encoded.split(\'$\', 3)\\n        return int(iterations) != self.iterations\\n\\n    def harden_runtime(self, password, encoded):\\n        algorithm, iterations, salt, hash = encoded.split(\'$\', 3)\\n        extra_iterations = self.iterations - int(iterations)\\n        if extra_iterations \\u003e 0:\\n            self.encode(password, salt, extra_iterations)\\n\\n\\nclass PBKDF2SHA1PasswordHasher(PBKDF2PasswordHasher):\\n    \\"\\"\\"\\n    Alternate PBKDF2 hasher which uses SHA1, the default PRF\\n    recommended by PKCS #5. This is compatible with other\\n    implementations of PBKDF2, such as openssl\'s\\n    PKCS5_PBKDF2_HMAC_SHA1().\\n    \\"\\"\\"\\n    algorithm = \\"pbkdf2_sha1\\"\\n    digest = hashlib.sha1\\n\\n\\nclass Argon2PasswordHasher(BasePasswordHasher):\\n    \\"\\"\\"\\n    Secure password hashing using the argon2 algorithm.\\n\\n    This is the winner of the Password Hashing Competition 2013-2015\\n    (https://password-hashing.net). It requires the argon2-cffi library which\\n    depends on native C code and might cause portability issues.\\n    \\"\\"\\"\\n    algorithm = \'argon2\'\\n    library = \'argon2\'\\n\\n    time_cost = 2\\n    memory_cost = 512\\n    parallelism = 2\\n\\n    def encode(self, password, salt):\\n        argon2 = self._load_library()\\n        data = argon2.low_level.hash_secret(\\n            force_bytes(password),\\n            force_bytes(salt),\\n            time_cost=self.time_cost,\\n            memory_cost=self.memory_cost,\\n            parallelism=self.parallelism,\\n            hash_len=argon2.DEFAULT_HASH_LENGTH,\\n            type=argon2.low_level.Type.I,\\n        )\\n        return self.algorithm + data.decode(\'ascii\')\\n\\n    def verify(self, password, encoded):\\n        argon2 = self._load_library()\\n        algorithm, rest = encoded.split(\'$\', 1)\\n        assert algorithm == self.algorithm\\n        try:\\n            return argon2.low_level.verify_secret(\\n                force_bytes(\'$\' + rest),\\n                force_bytes(password),\\n                type=argon2.low_level.Type.I,\\n            )\\n        except argon2.exceptions.VerificationError:\\n            return False\\n\\n    def safe_summary(self, encoded):\\n        (algorithm, variety, version, time_cost, memory_cost, parallelism,\\n            salt, data) = self._decode(encoded)\\n        assert algorithm == self.algorithm\\n        return OrderedDict([\\n            (_(\'algorithm\'), algorithm),\\n            (_(\'variety\'), variety),\\n            (_(\'version\'), version),\\n            (_(\'memory cost\'), memory_cost),\\n            (_(\'time cost\'), time_cost),\\n            (_(\'parallelism\'), parallelism),\\n            (_(\'salt\'), mask_hash(salt)),\\n            (_(\'hash\'), mask_hash(data)),\\n        ])\\n\\n    def must_update(self, encoded):\\n        (algorithm, variety, version, time_cost, memory_cost, parallelism,\\n            salt, data) = self._decode(encoded)\\n        assert algorithm == self.algorithm\\n        argon2 = self._load_library()\\n        return (\\n            argon2.low_level.ARGON2_VERSION != version or\\n            self.time_cost != time_cost or\\n            self.memory_cost != memory_cost or\\n            self.parallelism != parallelism\\n        )\\n\\n    def harden_runtime(self, password, encoded):\\n        # The runtime for Argon2 is too complicated to implement a sensible\\n        # hardening algorithm.\\n        pass\\n\\n    def _decode(self, encoded):\\n        \\"\\"\\"\\n        Split an encoded hash and return: (\\n            algorithm, variety, version, time_cost, memory_cost,\\n            parallelism, salt, data,\\n        ).\\n        \\"\\"\\"\\n        bits = encoded.split(\'$\')\\n        if len(bits) == 5:\\n            # Argon2 \\u003c 1.3\\n            algorithm, variety, raw_params, salt, data = bits\\n            version = 0x10\\n        else:\\n            assert len(bits) == 6\\n            algorithm, variety, raw_version, raw_params, salt, data = bits\\n            assert raw_version.startswith(\'v=\')\\n            version = int(raw_version[len(\'v=\'):])\\n        params = dict(bit.split(\'=\', 1) for bit in raw_params.split(\',\'))\\n        assert len(params) == 3 and all(x in params for x in (\'t\', \'m\', \'p\'))\\n        time_cost = int(params[\'t\'])\\n        memory_cost = int(params[\'m\'])\\n        parallelism = int(params[\'p\'])\\n        return (\\n            algorithm, variety, version, time_cost, memory_cost, parallelism,\\n            salt, data,\\n        )\\n\\n\\nclass BCryptSHA256PasswordHasher(BasePasswordHasher):\\n    \\"\\"\\"\\n    Secure password hashing using the bcrypt algorithm (recommended)\\n\\n    This is considered by many to be the most secure algorithm but you\\n    must first install the bcrypt library.  Please be warned that\\n    this library depends on native C code and might cause portability\\n    issues.\\n    \\"\\"\\"\\n    algorithm = \\"bcrypt_sha256\\"\\n    digest = hashlib.sha256\\n    library = (\\"bcrypt\\", \\"bcrypt\\")\\n    rounds = 12\\n\\n    def salt(self):\\n        bcrypt = self._load_library()\\n        return bcrypt.gensalt(self.rounds)\\n\\n    def encode(self, password, salt):\\n        bcrypt = self._load_library()\\n        # Hash the password prior to using bcrypt to prevent password\\n        # truncation as described in #20138.\\n        if self.digest is not None:\\n            # Use binascii.hexlify() because a hex encoded bytestring is str.\\n            password = binascii.hexlify(self.digest(force_bytes(password)).digest())\\n        else:\\n            password = force_bytes(password)\\n\\n        data = bcrypt.hashpw(password, salt)\\n        return \\"%s$%s\\" % (self.algorithm, force_text(data))\\n\\n    def verify(self, password, encoded):\\n        algorithm, data = encoded.split(\'$\', 1)\\n        assert algorithm == self.algorithm\\n        encoded_2 = self.encode(password, force_bytes(data))\\n        return constant_time_compare(encoded, encoded_2)\\n\\n    def safe_summary(self, encoded):\\n        algorithm, empty, algostr, work_factor, data = encoded.split(\'$\', 4)\\n        assert algorithm == self.algorithm\\n        salt, checksum = data[:22], data[22:]\\n        return OrderedDict([\\n            (_(\'algorithm\'), algorithm),\\n            (_(\'work factor\'), work_factor),\\n            (_(\'salt\'), mask_hash(salt)),\\n            (_(\'checksum\'), mask_hash(checksum)),\\n        ])\\n\\n    def must_update(self, encoded):\\n        algorithm, empty, algostr, rounds, data = encoded.split(\'$\', 4)\\n        return int(rounds) != self.rounds\\n\\n    def harden_runtime(self, password, encoded):\\n        _, data = encoded.split(\'$\', 1)\\n        salt = data[:29]  # Length of the salt in bcrypt.\\n        rounds = data.split(\'$\')[2]\\n        # work factor is logarithmic, adding one doubles the load.\\n        diff = 2**(self.rounds - int(rounds)) - 1\\n        while diff \\u003e 0:\\n            self.encode(password, force_bytes(salt))\\n            diff -= 1\\n\\n\\nclass BCryptPasswordHasher(BCryptSHA256PasswordHasher):\\n    \\"\\"\\"\\n    Secure password hashing using the bcrypt algorithm\\n\\n    This is considered by many to be the most secure algorithm but you\\n    must first install the bcrypt library.  Please be warned that\\n    this library depends on native C code and might cause portability\\n    issues.\\n\\n    This hasher does not first hash the password which means it is subject to\\n    the 72 character bcrypt password truncation, most use cases should prefer\\n    the BCryptSHA256PasswordHasher.\\n\\n    See: https://code.djangoproject.com/ticket/20138\\n    \\"\\"\\"\\n    algorithm = \\"bcrypt\\"\\n    digest = None\\n\\n\\nclass SHA1PasswordHasher(BasePasswordHasher):\\n    \\"\\"\\"\\n    The SHA1 password hashing algorithm (not recommended)\\n    \\"\\"\\"\\n    algorithm = \\"sha1\\"\\n\\n    def encode(self, password, salt):\\n        assert password is not None\\n        assert salt and \'$\' not in salt\\n        hash = hashlib.sha1(force_bytes(salt + password)).hexdigest()\\n        return \\"%s$%s$%s\\" % (self.algorithm, salt, hash)\\n\\n    def verify(self, password, encoded):\\n        algorithm, salt, hash = encoded.split(\'$\', 2)\\n        assert algorithm == self.algorithm\\n        encoded_2 = self.encode(password, salt)\\n        return constant_time_compare(encoded, encoded_2)\\n\\n    def safe_summary(self, encoded):\\n        algorithm, salt, hash = encoded.split(\'$\', 2)\\n        assert algorithm == self.algorithm\\n        return OrderedDict([\\n            (_(\'algorithm\'), algorithm),\\n            (_(\'salt\'), mask_hash(salt, show=2)),\\n            (_(\'hash\'), mask_hash(hash)),\\n        ])\\n\\n    def harden_runtime(self, password, encoded):\\n        pass\\n\\n\\nclass MD5PasswordHasher(BasePasswordHasher):\\n    \\"\\"\\"\\n    The Salted MD5 password hashing algorithm (not recommended)\\n    \\"\\"\\"\\n    algorithm = \\"md5\\"\\n\\n    def encode(self, password, salt):\\n        assert password is not None\\n        assert salt and \'$\' not in salt\\n        hash = hashlib.md5(force_bytes(salt + password)).hexdigest()\\n        return \\"%s$%s$%s\\" % (self.algorithm, salt, hash)\\n\\n    def verify(self, password, encoded):\\n        algorithm, salt, hash = encoded.split(\'$\', 2)\\n        assert algorithm == self.algorithm\\n        encoded_2 = self.encode(password, salt)\\n        return constant_time_compare(encoded, encoded_2)\\n\\n    def safe_summary(self, encoded):\\n        algorithm, salt, hash = encoded.split(\'$\', 2)\\n        assert algorithm == self.algorithm\\n        return OrderedDict([\\n            (_(\'algorithm\'), algorithm),\\n            (_(\'salt\'), mask_hash(salt, show=2)),\\n            (_(\'hash\'), mask_hash(hash)),\\n        ])\\n\\n    def harden_runtime(self, password, encoded):\\n        pass\\n\\n\\nclass UnsaltedSHA1PasswordHasher(BasePasswordHasher):\\n    \\"\\"\\"\\n    Very insecure algorithm that you should *never* use; store SHA1 hashes\\n    with an empty salt.\\n\\n    This class is implemented because Django used to accept such password\\n    hashes. Some older Django installs still have these values lingering\\n    around so we need to handle and upgrade them properly.\\n    \\"\\"\\"\\n    algorithm = \\"unsalted_sha1\\"\\n\\n    def salt(self):\\n        return \'\'\\n\\n    def encode(self, password, salt):\\n        assert salt == \'\'\\n        hash = hashlib.sha1(force_bytes(password)).hexdigest()\\n        return \'sha1$$%s\' % hash\\n\\n    def verify(self, password, encoded):\\n        encoded_2 = self.encode(password, \'\')\\n        return constant_time_compare(encoded, encoded_2)\\n\\n    def safe_summary(self, encoded):\\n        assert encoded.startswith(\'sha1$$\')\\n        hash = encoded[6:]\\n        return OrderedDict([\\n            (_(\'algorithm\'), self.algorithm),\\n            (_(\'hash\'), mask_hash(hash)),\\n        ])\\n\\n    def harden_runtime(self, password, encoded):\\n        pass\\n\\n\\nclass UnsaltedMD5PasswordHasher(BasePasswordHasher):\\n    \\"\\"\\"\\n    Incredibly insecure algorithm that you should *never* use; stores unsalted\\n    MD5 hashes without the algorithm prefix, also accepts MD5 hashes with an\\n    empty salt.\\n\\n    This class is implemented because Django used to store passwords this way\\n    and to accept such password hashes. Some older Django installs still have\\n    these values lingering around so we need to handle and upgrade them\\n    properly.\\n    \\"\\"\\"\\n    algorithm = \\"unsalted_md5\\"\\n\\n    def salt(self):\\n        return \'\'\\n\\n    def encode(self, password, salt):\\n        assert salt == \'\'\\n        return hashlib.md5(force_bytes(password)).hexdigest()\\n\\n    def verify(self, password, encoded):\\n        if len(encoded) == 37 and encoded.startswith(\'md5$$\'):\\n            encoded = encoded[5:]\\n        encoded_2 = self.encode(password, \'\')\\n        return constant_time_compare(encoded, encoded_2)\\n\\n    def safe_summary(self, encoded):\\n        return OrderedDict([\\n            (_(\'algorithm\'), self.algorithm),\\n            (_(\'hash\'), mask_hash(encoded, show=3)),\\n        ])\\n\\n    def harden_runtime(self, password, encoded):\\n        pass\\n\\n\\nclass CryptPasswordHasher(BasePasswordHasher):\\n    \\"\\"\\"\\n    Password hashing using UNIX crypt (not recommended)\\n\\n    The crypt module is not supported on all platforms.\\n    \\"\\"\\"\\n    algorithm = \\"crypt\\"\\n    library = \\"crypt\\"\\n\\n    def salt(self):\\n        return get_random_string(2)\\n\\n    def encode(self, password, salt):\\n        crypt = self._load_library()\\n        assert len(salt) == 2\\n        data = crypt.crypt(password, salt)\\n        assert data is not None  # A platform like OpenBSD with a dummy crypt module.\\n        # we don\'t need to store the salt, but Django used to do this\\n        return \\"%s$%s$%s\\" % (self.algorithm, \'\', data)\\n\\n    def verify(self, password, encoded):\\n        crypt = self._load_library()\\n        algorithm, salt, data = encoded.split(\'$\', 2)\\n        assert algorithm == self.algorithm\\n        return constant_time_compare(data, crypt.crypt(password, data))\\n\\n    def safe_summary(self, encoded):\\n        algorithm, salt, data = encoded.split(\'$\', 2)\\n        assert algorithm == self.algorithm\\n        return OrderedDict([\\n            (_(\'algorithm\'), algorithm),\\n            (_(\'salt\'), salt),\\n            (_(\'hash\'), mask_hash(data, show=3)),\\n        ])\\n\\n    def harden_runtime(self, password, encoded):\\n        pass\\n"}\n'
line: b'{"repo_name":"nikolay-fedotov/tempest","ref":"refs/heads/master","path":"tempest/api/object_storage/test_account_services_negative.py","content":"# Copyright (C) 2013 eNovance SAS \\u003clicensing@enovance.com\\u003e\\n#\\n# Author: Joe H. Rahme \\u003cjoe.hakim.rahme@enovance.com\\u003e\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\nfrom tempest.api.object_storage import base\\nfrom tempest import clients\\nfrom tempest import exceptions\\nfrom tempest import test\\n\\n\\nclass AccountNegativeTest(base.BaseObjectTest):\\n\\n    @test.attr(type=[\'negative\', \'gate\'])\\n    def test_list_containers_with_non_authorized_user(self):\\n        # list containers using non-authorized user\\n\\n        # create user\\n        self.data.setup_test_user()\\n        test_os = clients.Manager(self.data.test_credentials)\\n        test_auth_provider = test_os.auth_provider\\n        # Get auth for the test user\\n        test_auth_provider.auth_data\\n\\n        # Get fresh auth for test user and set it to next auth request for\\n        # custom_account_client\\n        delattr(test_auth_provider, \'auth_data\')\\n        test_auth_new_data = test_auth_provider.auth_data\\n        self.custom_account_client.auth_provider.set_alt_auth_data(\\n            request_part=\'headers\',\\n            auth_data=test_auth_new_data\\n        )\\n\\n        params = {\'format\': \'json\'}\\n        # list containers with non-authorized user token\\n        self.assertRaises(exceptions.Unauthorized,\\n                          self.custom_account_client.list_account_containers,\\n                          params=params)\\n"}\n'
line: b'{"repo_name":"adobe/chromium","ref":"refs/heads/master","path":"third_party/closure_linter/closure_linter/error_fixer.py","content":"#!/usr/bin/env python\\n#\\n# Copyright 2007 The Closure Linter Authors. All Rights Reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#      http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS-IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\n\\"\\"\\"Main class responsible for automatically fixing simple style violations.\\"\\"\\"\\n\\n__author__ = \'robbyw@google.com (Robert Walker)\'\\n\\nimport re\\n\\nimport gflags as flags\\nfrom closure_linter import errors\\nfrom closure_linter import javascriptstatetracker\\nfrom closure_linter import javascripttokens\\nfrom closure_linter import requireprovidesorter\\nfrom closure_linter import tokenutil\\nfrom closure_linter.common import errorhandler\\n\\n# Shorthand\\nToken = javascripttokens.JavaScriptToken\\nType = javascripttokens.JavaScriptTokenType\\n\\nEND_OF_FLAG_TYPE = re.compile(r\'(}?\\\\s*)$\')\\n\\n# Regex to represent common mistake inverting author name and email as\\n# @author User Name (user@company)\\nINVERTED_AUTHOR_SPEC = re.compile(r\'(?P\\u003cleading_whitespace\\u003e\\\\s*)\'\\n                                  \'(?P\\u003cname\\u003e[^(]+)\'\\n                                  \'(?P\\u003cwhitespace_after_name\\u003e\\\\s+)\'\\n                                  \'\\\\(\'\\n                                  \'(?P\\u003cemail\\u003e[^\\\\s]+@[^)\\\\s]+)\'\\n                                  \'\\\\)\'\\n                                  \'(?P\\u003ctrailing_characters\\u003e.*)\')\\n\\nFLAGS = flags.FLAGS\\nflags.DEFINE_boolean(\'disable_indentation_fixing\', False,\\n                     \'Whether to disable automatic fixing of indentation.\')\\n\\n\\nclass ErrorFixer(errorhandler.ErrorHandler):\\n  \\"\\"\\"Object that fixes simple style errors.\\"\\"\\"\\n\\n  def __init__(self, external_file=None):\\n    \\"\\"\\"Initialize the error fixer.\\n\\n    Args:\\n      external_file: If included, all output will be directed to this file\\n          instead of overwriting the files the errors are found in.\\n    \\"\\"\\"\\n    errorhandler.ErrorHandler.__init__(self)\\n\\n    self._file_name = None\\n    self._file_token = None\\n    self._external_file = external_file\\n\\n  def HandleFile(self, filename, first_token):\\n    \\"\\"\\"Notifies this ErrorPrinter that subsequent errors are in filename.\\n\\n    Args:\\n      filename: The name of the file about to be checked.\\n      first_token: The first token in the file.\\n    \\"\\"\\"\\n    self._file_name = filename\\n    self._file_token = first_token\\n    self._file_fix_count = 0\\n    self._file_changed_lines = set()\\n\\n  def _AddFix(self, tokens):\\n    \\"\\"\\"Adds the fix to the internal count.\\n\\n    Args:\\n      tokens: The token or sequence of tokens changed to fix an error.\\n    \\"\\"\\"\\n    self._file_fix_count += 1\\n    if hasattr(tokens, \'line_number\'):\\n      self._file_changed_lines.add(tokens.line_number)\\n    else:\\n      for token in tokens:\\n        self._file_changed_lines.add(token.line_number)\\n\\n  def HandleError(self, error):\\n    \\"\\"\\"Attempts to fix the error.\\n\\n    Args:\\n      error: The error object\\n    \\"\\"\\"\\n    code = error.code\\n    token = error.token\\n\\n    if code == errors.JSDOC_PREFER_QUESTION_TO_PIPE_NULL:\\n      iterator = token.attached_object.type_start_token\\n      if iterator.type == Type.DOC_START_BRACE or iterator.string.isspace():\\n        iterator = iterator.next\\n\\n      leading_space = len(iterator.string) - len(iterator.string.lstrip())\\n      iterator.string = \'%s?%s\' % (\' \' * leading_space,\\n                                   iterator.string.lstrip())\\n\\n      # Cover the no outer brace case where the end token is part of the type.\\n      while iterator and iterator != token.attached_object.type_end_token.next:\\n        iterator.string = iterator.string.replace(\\n            \'null|\', \'\').replace(\'|null\', \'\')\\n        iterator = iterator.next\\n\\n      # Create a new flag object with updated type info.\\n      token.attached_object = javascriptstatetracker.JsDocFlag(token)\\n      self._AddFix(token)\\n\\n    elif code == errors.JSDOC_MISSING_OPTIONAL_TYPE:\\n      iterator = token.attached_object.type_end_token\\n      if iterator.type == Type.DOC_END_BRACE or iterator.string.isspace():\\n        iterator = iterator.previous\\n\\n      ending_space = len(iterator.string) - len(iterator.string.rstrip())\\n      iterator.string = \'%s=%s\' % (iterator.string.rstrip(),\\n                                   \' \' * ending_space)\\n\\n      # Create a new flag object with updated type info.\\n      token.attached_object = javascriptstatetracker.JsDocFlag(token)\\n      self._AddFix(token)\\n\\n    elif code in (errors.MISSING_SEMICOLON_AFTER_FUNCTION,\\n                  errors.MISSING_SEMICOLON):\\n      semicolon_token = Token(\';\', Type.SEMICOLON, token.line,\\n                              token.line_number)\\n      tokenutil.InsertTokenAfter(semicolon_token, token)\\n      token.metadata.is_implied_semicolon = False\\n      semicolon_token.metadata.is_implied_semicolon = False\\n      self._AddFix(token)\\n\\n    elif code in (errors.ILLEGAL_SEMICOLON_AFTER_FUNCTION,\\n                  errors.REDUNDANT_SEMICOLON,\\n                  errors.COMMA_AT_END_OF_LITERAL):\\n      tokenutil.DeleteToken(token)\\n      self._AddFix(token)\\n\\n    elif code == errors.INVALID_JSDOC_TAG:\\n      if token.string == \'@returns\':\\n        token.string = \'@return\'\\n        self._AddFix(token)\\n\\n    elif code == errors.FILE_MISSING_NEWLINE:\\n      # This error is fixed implicitly by the way we restore the file\\n      self._AddFix(token)\\n\\n    elif code == errors.MISSING_SPACE:\\n      if error.position:\\n        if error.position.IsAtBeginning():\\n          tokenutil.InsertSpaceTokenAfter(token.previous)\\n        elif error.position.IsAtEnd(token.string):\\n          tokenutil.InsertSpaceTokenAfter(token)\\n        else:\\n          token.string = error.position.Set(token.string, \' \')\\n        self._AddFix(token)\\n\\n    elif code == errors.EXTRA_SPACE:\\n      if error.position:\\n        token.string = error.position.Set(token.string, \'\')\\n        self._AddFix(token)\\n\\n    elif code == errors.JSDOC_TAG_DESCRIPTION_ENDS_WITH_INVALID_CHARACTER:\\n      token.string = error.position.Set(token.string, \'.\')\\n      self._AddFix(token)\\n\\n    elif code == errors.MISSING_LINE:\\n      if error.position.IsAtBeginning():\\n        tokenutil.InsertBlankLineAfter(token.previous)\\n      else:\\n        tokenutil.InsertBlankLineAfter(token)\\n      self._AddFix(token)\\n\\n    elif code == errors.EXTRA_LINE:\\n      tokenutil.DeleteToken(token)\\n      self._AddFix(token)\\n\\n    elif code == errors.WRONG_BLANK_LINE_COUNT:\\n      if not token.previous:\\n        # TODO(user): Add an insertBefore method to tokenutil.\\n        return\\n\\n      num_lines = error.fix_data\\n      should_delete = False\\n\\n      if num_lines \\u003c 0:\\n        num_lines *= -1\\n        should_delete = True\\n\\n      for i in xrange(1, num_lines + 1):\\n        if should_delete:\\n          # TODO(user): DeleteToken should update line numbers.\\n          tokenutil.DeleteToken(token.previous)\\n        else:\\n          tokenutil.InsertBlankLineAfter(token.previous)\\n        self._AddFix(token)\\n\\n    elif code == errors.UNNECESSARY_DOUBLE_QUOTED_STRING:\\n      end_quote = tokenutil.Search(token, Type.DOUBLE_QUOTE_STRING_END)\\n      if end_quote:\\n        single_quote_start = Token(\\n            \\"\'\\", Type.SINGLE_QUOTE_STRING_START, token.line, token.line_number)\\n        single_quote_end = Token(\\n            \\"\'\\", Type.SINGLE_QUOTE_STRING_START, end_quote.line,\\n            token.line_number)\\n\\n        tokenutil.InsertTokenAfter(single_quote_start, token)\\n        tokenutil.InsertTokenAfter(single_quote_end, end_quote)\\n        tokenutil.DeleteToken(token)\\n        tokenutil.DeleteToken(end_quote)\\n        self._AddFix([token, end_quote])\\n\\n    elif code == errors.MISSING_BRACES_AROUND_TYPE:\\n      fixed_tokens = []\\n      start_token = token.attached_object.type_start_token\\n\\n      if start_token.type != Type.DOC_START_BRACE:\\n        leading_space = (\\n            len(start_token.string) - len(start_token.string.lstrip()))\\n        if leading_space:\\n          start_token = tokenutil.SplitToken(start_token, leading_space)\\n          # Fix case where start and end token were the same.\\n          if token.attached_object.type_end_token == start_token.previous:\\n            token.attached_object.type_end_token = start_token\\n\\n        new_token = Token(\'{\', Type.DOC_START_BRACE, start_token.line,\\n                          start_token.line_number)\\n        tokenutil.InsertTokenAfter(new_token, start_token.previous)\\n        token.attached_object.type_start_token = new_token\\n        fixed_tokens.append(new_token)\\n\\n      end_token = token.attached_object.type_end_token\\n      if end_token.type != Type.DOC_END_BRACE:\\n        # If the start token was a brace, the end token will be a\\n        # FLAG_ENDING_TYPE token, if there wasn\'t a starting brace then\\n        # the end token is the last token of the actual type.\\n        last_type = end_token\\n        if not fixed_tokens:\\n          last_type = end_token.previous\\n\\n        while last_type.string.isspace():\\n          last_type = last_type.previous\\n\\n        # If there was no starting brace then a lone end brace wouldn\'t have\\n        # been type end token. Now that we\'ve added any missing start brace,\\n        # see if the last effective type token was an end brace.\\n        if last_type.type != Type.DOC_END_BRACE:\\n          trailing_space = (len(last_type.string) -\\n                            len(last_type.string.rstrip()))\\n          if trailing_space:\\n            tokenutil.SplitToken(last_type,\\n                                 len(last_type.string) - trailing_space)\\n\\n          new_token = Token(\'}\', Type.DOC_END_BRACE, last_type.line,\\n                            last_type.line_number)\\n          tokenutil.InsertTokenAfter(new_token, last_type)\\n          token.attached_object.type_end_token = new_token\\n          fixed_tokens.append(new_token)\\n\\n      self._AddFix(fixed_tokens)\\n\\n    elif code == errors.GOOG_REQUIRES_NOT_ALPHABETIZED:\\n      require_start_token = error.fix_data\\n      sorter = requireprovidesorter.RequireProvideSorter()\\n      sorter.FixRequires(require_start_token)\\n\\n      self._AddFix(require_start_token)\\n\\n    elif code == errors.GOOG_PROVIDES_NOT_ALPHABETIZED:\\n      provide_start_token = error.fix_data\\n      sorter = requireprovidesorter.RequireProvideSorter()\\n      sorter.FixProvides(provide_start_token)\\n\\n      self._AddFix(provide_start_token)\\n\\n    elif code == errors.UNNECESSARY_BRACES_AROUND_INHERIT_DOC:\\n      if token.previous.string == \'{\' and token.next.string == \'}\':\\n        tokenutil.DeleteToken(token.previous)\\n        tokenutil.DeleteToken(token.next)\\n        self._AddFix([token])\\n\\n    elif code == errors.INVALID_AUTHOR_TAG_DESCRIPTION:\\n      match = INVERTED_AUTHOR_SPEC.match(token.string)\\n      if match:\\n        token.string = \'%s%s%s(%s)%s\' % (match.group(\'leading_whitespace\'),\\n                                         match.group(\'email\'),\\n                                         match.group(\'whitespace_after_name\'),\\n                                         match.group(\'name\'),\\n                                         match.group(\'trailing_characters\'))\\n        self._AddFix(token)\\n\\n    elif (code == errors.WRONG_INDENTATION and\\n          not FLAGS.disable_indentation_fixing):\\n      token = tokenutil.GetFirstTokenInSameLine(token)\\n      actual = error.position.start\\n      expected = error.position.length\\n\\n      if token.type in (Type.WHITESPACE, Type.PARAMETERS) and actual != 0:\\n        token.string = token.string.lstrip() + (\' \' * expected)\\n        self._AddFix([token])\\n      else:\\n        # We need to add indentation.\\n        new_token = Token(\' \' * expected, Type.WHITESPACE,\\n                          token.line, token.line_number)\\n        # Note that we\'ll never need to add indentation at the first line,\\n        # since it will always not be indented.  Therefore it\'s safe to assume\\n        # token.previous exists.\\n        tokenutil.InsertTokenAfter(new_token, token.previous)\\n        self._AddFix([token])\\n\\n    elif code in [errors.MALFORMED_END_OF_SCOPE_COMMENT,\\n                  errors.MISSING_END_OF_SCOPE_COMMENT]:\\n      # Only fix cases where }); is found with no trailing content on the line\\n      # other than a comment. Value of \'token\' is set to } for this error.\\n      if (token.type == Type.END_BLOCK and\\n          token.next.type == Type.END_PAREN and\\n          token.next.next.type == Type.SEMICOLON):\\n        current_token = token.next.next.next\\n        removed_tokens = []\\n        while current_token and current_token.line_number == token.line_number:\\n          if current_token.IsAnyType(Type.WHITESPACE,\\n                                     Type.START_SINGLE_LINE_COMMENT,\\n                                     Type.COMMENT):\\n            removed_tokens.append(current_token)\\n            current_token = current_token.next\\n          else:\\n            return\\n\\n        if removed_tokens:\\n          tokenutil.DeleteTokens(removed_tokens[0], len(removed_tokens))\\n\\n        whitespace_token = Token(\'  \', Type.WHITESPACE, token.line,\\n                                 token.line_number)\\n        start_comment_token = Token(\'//\', Type.START_SINGLE_LINE_COMMENT,\\n                                    token.line, token.line_number)\\n        comment_token = Token(\' goog.scope\', Type.COMMENT, token.line,\\n                              token.line_number)\\n        insertion_tokens = [whitespace_token, start_comment_token,\\n                            comment_token]\\n\\n        tokenutil.InsertTokensAfter(insertion_tokens, token.next.next)\\n        self._AddFix(removed_tokens + insertion_tokens)\\n\\n    elif code in [errors.EXTRA_GOOG_PROVIDE, errors.EXTRA_GOOG_REQUIRE]:\\n      tokens_in_line = tokenutil.GetAllTokensInSameLine(token)\\n      tokenutil.DeleteTokens(tokens_in_line[0], len(tokens_in_line))\\n      self._AddFix(tokens_in_line)\\n\\n    elif code in [errors.MISSING_GOOG_PROVIDE, errors.MISSING_GOOG_REQUIRE]:\\n      is_provide = code == errors.MISSING_GOOG_PROVIDE\\n      is_require = code == errors.MISSING_GOOG_REQUIRE\\n\\n      missing_namespaces = error.fix_data[0]\\n      need_blank_line = error.fix_data[1]\\n\\n      if need_blank_line is None:\\n        # TODO(user): This happens when there are no existing\\n        # goog.provide or goog.require statements to position new statements\\n        # relative to. Consider handling this case with a heuristic.\\n        return\\n\\n      insert_location = token.previous\\n\\n      # If inserting a missing require with no existing requires, insert a\\n      # blank line first.\\n      if need_blank_line and is_require:\\n        tokenutil.InsertBlankLineAfter(insert_location)\\n        insert_location = insert_location.next\\n\\n      for missing_namespace in missing_namespaces:\\n        new_tokens = self._GetNewRequireOrProvideTokens(\\n            is_provide, missing_namespace, insert_location.line_number + 1)\\n        tokenutil.InsertLineAfter(insert_location, new_tokens)\\n        insert_location = new_tokens[-1]\\n        self._AddFix(new_tokens)\\n\\n      # If inserting a missing provide with no existing provides, insert a\\n      # blank line after.\\n      if need_blank_line and is_provide:\\n        tokenutil.InsertBlankLineAfter(insert_location)\\n\\n  def _GetNewRequireOrProvideTokens(self, is_provide, namespace, line_number):\\n    \\"\\"\\"Returns a list of tokens to create a goog.require/provide statement.\\n\\n    Args:\\n      is_provide: True if getting tokens for a provide, False for require.\\n      namespace: The required or provided namespaces to get tokens for.\\n      line_number: The line number the new require or provide statement will be\\n          on.\\n\\n    Returns:\\n      Tokens to create a new goog.require or goog.provide statement.\\n    \\"\\"\\"\\n    string = \'goog.require\'\\n    if is_provide:\\n      string = \'goog.provide\'\\n    line_text = string + \'(\\\\\'\' + namespace + \'\\\\\');\\\\n\'\\n    return [\\n        Token(string, Type.IDENTIFIER, line_text, line_number),\\n        Token(\'(\', Type.START_PAREN, line_text, line_number),\\n        Token(\'\\\\\'\', Type.SINGLE_QUOTE_STRING_START, line_text, line_number),\\n        Token(namespace, Type.STRING_TEXT, line_text, line_number),\\n        Token(\'\\\\\'\', Type.SINGLE_QUOTE_STRING_END, line_text, line_number),\\n        Token(\')\', Type.END_PAREN, line_text, line_number),\\n        Token(\';\', Type.SEMICOLON, line_text, line_number)\\n        ]\\n\\n  def FinishFile(self):\\n    \\"\\"\\"Called when the current file has finished style checking.\\n\\n    Used to go back and fix any errors in the file.\\n    \\"\\"\\"\\n    if self._file_fix_count:\\n      f = self._external_file\\n      if not f:\\n        print \'Fixed %d errors in %s\' % (self._file_fix_count, self._file_name)\\n        f = open(self._file_name, \'w\')\\n\\n      token = self._file_token\\n      char_count = 0\\n      while token:\\n        f.write(token.string)\\n        char_count += len(token.string)\\n\\n        if token.IsLastInLine():\\n          f.write(\'\\\\n\')\\n          if char_count \\u003e 80 and token.line_number in self._file_changed_lines:\\n            print \'WARNING: Line %d of %s is now longer than 80 characters.\' % (\\n                token.line_number, self._file_name)\\n\\n          char_count = 0\\n\\n        token = token.next\\n\\n      if not self._external_file:\\n        # Close the file if we created it\\n        f.close()\\n"}\n'
line: b'{"repo_name":"ryfeus/lambda-packs","ref":"refs/heads/master","path":"Skimage_numpy/source/PIL/JpegImagePlugin.py","content":"#\\n# The Python Imaging Library.\\n# $Id$\\n#\\n# JPEG (JFIF) file handling\\n#\\n# See \\"Digital Compression and Coding of Continuous-Tone Still Images,\\n# Part 1, Requirements and Guidelines\\" (CCITT T.81 / ISO 10918-1)\\n#\\n# History:\\n# 1995-09-09 fl   Created\\n# 1995-09-13 fl   Added full parser\\n# 1996-03-25 fl   Added hack to use the IJG command line utilities\\n# 1996-05-05 fl   Workaround Photoshop 2.5 CMYK polarity bug\\n# 1996-05-28 fl   Added draft support, JFIF version (0.1)\\n# 1996-12-30 fl   Added encoder options, added progression property (0.2)\\n# 1997-08-27 fl   Save mode 1 images as BW (0.3)\\n# 1998-07-12 fl   Added YCbCr to draft and save methods (0.4)\\n# 1998-10-19 fl   Don\'t hang on files using 16-bit DQT\'s (0.4.1)\\n# 2001-04-16 fl   Extract DPI settings from JFIF files (0.4.2)\\n# 2002-07-01 fl   Skip pad bytes before markers; identify Exif files (0.4.3)\\n# 2003-04-25 fl   Added experimental EXIF decoder (0.5)\\n# 2003-06-06 fl   Added experimental EXIF GPSinfo decoder\\n# 2003-09-13 fl   Extract COM markers\\n# 2009-09-06 fl   Added icc_profile support (from Florian Hoech)\\n# 2009-03-06 fl   Changed CMYK handling; always use Adobe polarity (0.6)\\n# 2009-03-08 fl   Added subsampling support (from Justin Huff).\\n#\\n# Copyright (c) 1997-2003 by Secret Labs AB.\\n# Copyright (c) 1995-1996 by Fredrik Lundh.\\n#\\n# See the README file for information on usage and redistribution.\\n#\\n\\nfrom __future__ import print_function\\n\\nimport array\\nimport struct\\nimport io\\nimport warnings\\nfrom struct import unpack_from\\nfrom PIL import Image, ImageFile, TiffImagePlugin, _binary\\nfrom PIL.JpegPresets import presets\\nfrom PIL._util import isStringType\\n\\ni8 = _binary.i8\\no8 = _binary.o8\\ni16 = _binary.i16be\\ni32 = _binary.i32be\\n\\n__version__ = \\"0.6\\"\\n\\n\\n#\\n# Parser\\n\\ndef Skip(self, marker):\\n    n = i16(self.fp.read(2))-2\\n    ImageFile._safe_read(self.fp, n)\\n\\n\\ndef APP(self, marker):\\n    #\\n    # Application marker.  Store these in the APP dictionary.\\n    # Also look for well-known application markers.\\n\\n    n = i16(self.fp.read(2))-2\\n    s = ImageFile._safe_read(self.fp, n)\\n\\n    app = \\"APP%d\\" % (marker \\u0026 15)\\n\\n    self.app[app] = s  # compatibility\\n    self.applist.append((app, s))\\n\\n    if marker == 0xFFE0 and s[:4] == b\\"JFIF\\":\\n        # extract JFIF information\\n        self.info[\\"jfif\\"] = version = i16(s, 5)  # version\\n        self.info[\\"jfif_version\\"] = divmod(version, 256)\\n        # extract JFIF properties\\n        try:\\n            jfif_unit = i8(s[7])\\n            jfif_density = i16(s, 8), i16(s, 10)\\n        except:\\n            pass\\n        else:\\n            if jfif_unit == 1:\\n                self.info[\\"dpi\\"] = jfif_density\\n            self.info[\\"jfif_unit\\"] = jfif_unit\\n            self.info[\\"jfif_density\\"] = jfif_density\\n    elif marker == 0xFFE1 and s[:5] == b\\"Exif\\\\0\\":\\n        # extract Exif information (incomplete)\\n        self.info[\\"exif\\"] = s  # FIXME: value will change\\n    elif marker == 0xFFE2 and s[:5] == b\\"FPXR\\\\0\\":\\n        # extract FlashPix information (incomplete)\\n        self.info[\\"flashpix\\"] = s  # FIXME: value will change\\n    elif marker == 0xFFE2 and s[:12] == b\\"ICC_PROFILE\\\\0\\":\\n        # Since an ICC profile can be larger than the maximum size of\\n        # a JPEG marker (64K), we need provisions to split it into\\n        # multiple markers. The format defined by the ICC specifies\\n        # one or more APP2 markers containing the following data:\\n        #   Identifying string      ASCII \\"ICC_PROFILE\\\\0\\"  (12 bytes)\\n        #   Marker sequence number  1, 2, etc (1 byte)\\n        #   Number of markers       Total of APP2\'s used (1 byte)\\n        #   Profile data            (remainder of APP2 data)\\n        # Decoders should use the marker sequence numbers to\\n        # reassemble the profile, rather than assuming that the APP2\\n        # markers appear in the correct sequence.\\n        self.icclist.append(s)\\n    elif marker == 0xFFEE and s[:5] == b\\"Adobe\\":\\n        self.info[\\"adobe\\"] = i16(s, 5)\\n        # extract Adobe custom properties\\n        try:\\n            adobe_transform = i8(s[1])\\n        except:\\n            pass\\n        else:\\n            self.info[\\"adobe_transform\\"] = adobe_transform\\n    elif marker == 0xFFE2 and s[:4] == b\\"MPF\\\\0\\":\\n        # extract MPO information\\n        self.info[\\"mp\\"] = s[4:]\\n        # offset is current location minus buffer size\\n        # plus constant header size\\n        self.info[\\"mpoffset\\"] = self.fp.tell() - n + 4\\n\\n\\ndef COM(self, marker):\\n    #\\n    # Comment marker.  Store these in the APP dictionary.\\n    n = i16(self.fp.read(2))-2\\n    s = ImageFile._safe_read(self.fp, n)\\n\\n    self.app[\\"COM\\"] = s  # compatibility\\n    self.applist.append((\\"COM\\", s))\\n\\n\\ndef SOF(self, marker):\\n    #\\n    # Start of frame marker.  Defines the size and mode of the\\n    # image.  JPEG is colour blind, so we use some simple\\n    # heuristics to map the number of layers to an appropriate\\n    # mode.  Note that this could be made a bit brighter, by\\n    # looking for JFIF and Adobe APP markers.\\n\\n    n = i16(self.fp.read(2))-2\\n    s = ImageFile._safe_read(self.fp, n)\\n    self.size = i16(s[3:]), i16(s[1:])\\n\\n    self.bits = i8(s[0])\\n    if self.bits != 8:\\n        raise SyntaxError(\\"cannot handle %d-bit layers\\" % self.bits)\\n\\n    self.layers = i8(s[5])\\n    if self.layers == 1:\\n        self.mode = \\"L\\"\\n    elif self.layers == 3:\\n        self.mode = \\"RGB\\"\\n    elif self.layers == 4:\\n        self.mode = \\"CMYK\\"\\n    else:\\n        raise SyntaxError(\\"cannot handle %d-layer images\\" % self.layers)\\n\\n    if marker in [0xFFC2, 0xFFC6, 0xFFCA, 0xFFCE]:\\n        self.info[\\"progressive\\"] = self.info[\\"progression\\"] = 1\\n\\n    if self.icclist:\\n        # fixup icc profile\\n        self.icclist.sort()  # sort by sequence number\\n        if i8(self.icclist[0][13]) == len(self.icclist):\\n            profile = []\\n            for p in self.icclist:\\n                profile.append(p[14:])\\n            icc_profile = b\\"\\".join(profile)\\n        else:\\n            icc_profile = None  # wrong number of fragments\\n        self.info[\\"icc_profile\\"] = icc_profile\\n        self.icclist = None\\n\\n    for i in range(6, len(s), 3):\\n        t = s[i:i+3]\\n        # 4-tuples: id, vsamp, hsamp, qtable\\n        self.layer.append((t[0], i8(t[1])//16, i8(t[1]) \\u0026 15, i8(t[2])))\\n\\n\\ndef DQT(self, marker):\\n    #\\n    # Define quantization table.  Support baseline 8-bit tables\\n    # only.  Note that there might be more than one table in\\n    # each marker.\\n\\n    # FIXME: The quantization tables can be used to estimate the\\n    # compression quality.\\n\\n    n = i16(self.fp.read(2))-2\\n    s = ImageFile._safe_read(self.fp, n)\\n    while len(s):\\n        if len(s) \\u003c 65:\\n            raise SyntaxError(\\"bad quantization table marker\\")\\n        v = i8(s[0])\\n        if v//16 == 0:\\n            self.quantization[v \\u0026 15] = array.array(\\"B\\", s[1:65])\\n            s = s[65:]\\n        else:\\n            return  # FIXME: add code to read 16-bit tables!\\n            # raise SyntaxError, \\"bad quantization table element size\\"\\n\\n\\n#\\n# JPEG marker table\\n\\nMARKER = {\\n    0xFFC0: (\\"SOF0\\", \\"Baseline DCT\\", SOF),\\n    0xFFC1: (\\"SOF1\\", \\"Extended Sequential DCT\\", SOF),\\n    0xFFC2: (\\"SOF2\\", \\"Progressive DCT\\", SOF),\\n    0xFFC3: (\\"SOF3\\", \\"Spatial lossless\\", SOF),\\n    0xFFC4: (\\"DHT\\", \\"Define Huffman table\\", Skip),\\n    0xFFC5: (\\"SOF5\\", \\"Differential sequential DCT\\", SOF),\\n    0xFFC6: (\\"SOF6\\", \\"Differential progressive DCT\\", SOF),\\n    0xFFC7: (\\"SOF7\\", \\"Differential spatial\\", SOF),\\n    0xFFC8: (\\"JPG\\", \\"Extension\\", None),\\n    0xFFC9: (\\"SOF9\\", \\"Extended sequential DCT (AC)\\", SOF),\\n    0xFFCA: (\\"SOF10\\", \\"Progressive DCT (AC)\\", SOF),\\n    0xFFCB: (\\"SOF11\\", \\"Spatial lossless DCT (AC)\\", SOF),\\n    0xFFCC: (\\"DAC\\", \\"Define arithmetic coding conditioning\\", Skip),\\n    0xFFCD: (\\"SOF13\\", \\"Differential sequential DCT (AC)\\", SOF),\\n    0xFFCE: (\\"SOF14\\", \\"Differential progressive DCT (AC)\\", SOF),\\n    0xFFCF: (\\"SOF15\\", \\"Differential spatial (AC)\\", SOF),\\n    0xFFD0: (\\"RST0\\", \\"Restart 0\\", None),\\n    0xFFD1: (\\"RST1\\", \\"Restart 1\\", None),\\n    0xFFD2: (\\"RST2\\", \\"Restart 2\\", None),\\n    0xFFD3: (\\"RST3\\", \\"Restart 3\\", None),\\n    0xFFD4: (\\"RST4\\", \\"Restart 4\\", None),\\n    0xFFD5: (\\"RST5\\", \\"Restart 5\\", None),\\n    0xFFD6: (\\"RST6\\", \\"Restart 6\\", None),\\n    0xFFD7: (\\"RST7\\", \\"Restart 7\\", None),\\n    0xFFD8: (\\"SOI\\", \\"Start of image\\", None),\\n    0xFFD9: (\\"EOI\\", \\"End of image\\", None),\\n    0xFFDA: (\\"SOS\\", \\"Start of scan\\", Skip),\\n    0xFFDB: (\\"DQT\\", \\"Define quantization table\\", DQT),\\n    0xFFDC: (\\"DNL\\", \\"Define number of lines\\", Skip),\\n    0xFFDD: (\\"DRI\\", \\"Define restart interval\\", Skip),\\n    0xFFDE: (\\"DHP\\", \\"Define hierarchical progression\\", SOF),\\n    0xFFDF: (\\"EXP\\", \\"Expand reference component\\", Skip),\\n    0xFFE0: (\\"APP0\\", \\"Application segment 0\\", APP),\\n    0xFFE1: (\\"APP1\\", \\"Application segment 1\\", APP),\\n    0xFFE2: (\\"APP2\\", \\"Application segment 2\\", APP),\\n    0xFFE3: (\\"APP3\\", \\"Application segment 3\\", APP),\\n    0xFFE4: (\\"APP4\\", \\"Application segment 4\\", APP),\\n    0xFFE5: (\\"APP5\\", \\"Application segment 5\\", APP),\\n    0xFFE6: (\\"APP6\\", \\"Application segment 6\\", APP),\\n    0xFFE7: (\\"APP7\\", \\"Application segment 7\\", APP),\\n    0xFFE8: (\\"APP8\\", \\"Application segment 8\\", APP),\\n    0xFFE9: (\\"APP9\\", \\"Application segment 9\\", APP),\\n    0xFFEA: (\\"APP10\\", \\"Application segment 10\\", APP),\\n    0xFFEB: (\\"APP11\\", \\"Application segment 11\\", APP),\\n    0xFFEC: (\\"APP12\\", \\"Application segment 12\\", APP),\\n    0xFFED: (\\"APP13\\", \\"Application segment 13\\", APP),\\n    0xFFEE: (\\"APP14\\", \\"Application segment 14\\", APP),\\n    0xFFEF: (\\"APP15\\", \\"Application segment 15\\", APP),\\n    0xFFF0: (\\"JPG0\\", \\"Extension 0\\", None),\\n    0xFFF1: (\\"JPG1\\", \\"Extension 1\\", None),\\n    0xFFF2: (\\"JPG2\\", \\"Extension 2\\", None),\\n    0xFFF3: (\\"JPG3\\", \\"Extension 3\\", None),\\n    0xFFF4: (\\"JPG4\\", \\"Extension 4\\", None),\\n    0xFFF5: (\\"JPG5\\", \\"Extension 5\\", None),\\n    0xFFF6: (\\"JPG6\\", \\"Extension 6\\", None),\\n    0xFFF7: (\\"JPG7\\", \\"Extension 7\\", None),\\n    0xFFF8: (\\"JPG8\\", \\"Extension 8\\", None),\\n    0xFFF9: (\\"JPG9\\", \\"Extension 9\\", None),\\n    0xFFFA: (\\"JPG10\\", \\"Extension 10\\", None),\\n    0xFFFB: (\\"JPG11\\", \\"Extension 11\\", None),\\n    0xFFFC: (\\"JPG12\\", \\"Extension 12\\", None),\\n    0xFFFD: (\\"JPG13\\", \\"Extension 13\\", None),\\n    0xFFFE: (\\"COM\\", \\"Comment\\", COM)\\n}\\n\\n\\ndef _accept(prefix):\\n    return prefix[0:1] == b\\"\\\\377\\"\\n\\n\\n##\\n# Image plugin for JPEG and JFIF images.\\n\\nclass JpegImageFile(ImageFile.ImageFile):\\n\\n    format = \\"JPEG\\"\\n    format_description = \\"JPEG (ISO 10918)\\"\\n\\n    def _open(self):\\n\\n        s = self.fp.read(1)\\n\\n        if i8(s) != 255:\\n            raise SyntaxError(\\"not a JPEG file\\")\\n\\n        # Create attributes\\n        self.bits = self.layers = 0\\n\\n        # JPEG specifics (internal)\\n        self.layer = []\\n        self.huffman_dc = {}\\n        self.huffman_ac = {}\\n        self.quantization = {}\\n        self.app = {}  # compatibility\\n        self.applist = []\\n        self.icclist = []\\n\\n        while True:\\n\\n            i = i8(s)\\n            if i == 0xFF:\\n                s = s + self.fp.read(1)\\n                i = i16(s)\\n            else:\\n                # Skip non-0xFF junk\\n                s = self.fp.read(1)\\n                continue\\n\\n            if i in MARKER:\\n                name, description, handler = MARKER[i]\\n                # print(hex(i), name, description)\\n                if handler is not None:\\n                    handler(self, i)\\n                if i == 0xFFDA:  # start of scan\\n                    rawmode = self.mode\\n                    if self.mode == \\"CMYK\\":\\n                        rawmode = \\"CMYK;I\\"  # assume adobe conventions\\n                    self.tile = [(\\"jpeg\\", (0, 0) + self.size, 0,\\n                                 (rawmode, \\"\\"))]\\n                    # self.__offset = self.fp.tell()\\n                    break\\n                s = self.fp.read(1)\\n            elif i == 0 or i == 0xFFFF:\\n                # padded marker or junk; move on\\n                s = b\\"\\\\xff\\"\\n            elif i == 0xFF00:  # Skip extraneous data (escaped 0xFF)\\n                s = self.fp.read(1)\\n            else:\\n                raise SyntaxError(\\"no marker found\\")\\n\\n    def draft(self, mode, size):\\n\\n        if len(self.tile) != 1:\\n            return\\n\\n        # Protect from second call\\n        if self.decoderconfig:\\n            return\\n\\n        d, e, o, a = self.tile[0]\\n        scale = 0\\n\\n        if a[0] == \\"RGB\\" and mode in [\\"L\\", \\"YCbCr\\"]:\\n            self.mode = mode\\n            a = mode, \\"\\"\\n\\n        if size:\\n            scale = min(self.size[0] // size[0], self.size[1] // size[1])\\n            for s in [8, 4, 2, 1]:\\n                if scale \\u003e= s:\\n                    break\\n            e = e[0], e[1], (e[2]-e[0]+s-1)//s+e[0], (e[3]-e[1]+s-1)//s+e[1]\\n            self.size = ((self.size[0]+s-1)//s, (self.size[1]+s-1)//s)\\n            scale = s\\n\\n        self.tile = [(d, e, o, a)]\\n        self.decoderconfig = (scale, 0)\\n\\n        return self\\n\\n    def load_djpeg(self):\\n\\n        # ALTERNATIVE: handle JPEGs via the IJG command line utilities\\n\\n        import subprocess\\n        import tempfile\\n        import os\\n        f, path = tempfile.mkstemp()\\n        os.close(f)\\n        if os.path.exists(self.filename):\\n            subprocess.check_call([\\"djpeg\\", \\"-outfile\\", path, self.filename])\\n        else:\\n            raise ValueError(\\"Invalid Filename\\")\\n\\n        try:\\n            _im = Image.open(path)\\n            _im.load()\\n            self.im = _im.im\\n        finally:\\n            try:\\n                os.unlink(path)\\n            except OSError:\\n                pass\\n\\n        self.mode = self.im.mode\\n        self.size = self.im.size\\n\\n        self.tile = []\\n\\n    def _getexif(self):\\n        return _getexif(self)\\n\\n    def _getmp(self):\\n        return _getmp(self)\\n\\n\\ndef _fixup_dict(src_dict):\\n    # Helper function for _getexif()\\n    # returns a dict with any single item tuples/lists as individual values\\n    def _fixup(value):\\n        try:\\n            if len(value) == 1 and not isinstance(value, dict):\\n                return value[0]\\n        except: pass\\n        return value\\n\\n    return {k: _fixup(v) for k, v in src_dict.items()}\\n\\n\\ndef _getexif(self):\\n    # Extract EXIF information.  This method is highly experimental,\\n    # and is likely to be replaced with something better in a future\\n    # version.\\n\\n    # The EXIF record consists of a TIFF file embedded in a JPEG\\n    # application marker (!).\\n    try:\\n        data = self.info[\\"exif\\"]\\n    except KeyError:\\n        return None\\n    file = io.BytesIO(data[6:])\\n    head = file.read(8)\\n    # process dictionary\\n    info = TiffImagePlugin.ImageFileDirectory_v1(head)\\n    info.load(file)\\n    exif = dict(_fixup_dict(info))\\n    # get exif extension\\n    try:\\n        # exif field 0x8769 is an offset pointer to the location\\n        # of the nested embedded exif ifd.\\n        # It should be a long, but may be corrupted.\\n        file.seek(exif[0x8769])\\n    except (KeyError, TypeError):\\n        pass\\n    else:\\n        info = TiffImagePlugin.ImageFileDirectory_v1(head)\\n        info.load(file)\\n        exif.update(_fixup_dict(info))\\n    # get gpsinfo extension\\n    try:\\n        # exif field 0x8825 is an offset pointer to the location\\n        # of the nested embedded gps exif ifd.\\n        # It should be a long, but may be corrupted.\\n        file.seek(exif[0x8825])\\n    except (KeyError, TypeError):\\n        pass\\n    else:\\n        info = TiffImagePlugin.ImageFileDirectory_v1(head)\\n        info.load(file)\\n        exif[0x8825] = _fixup_dict(info)\\n\\n    return exif\\n\\n\\ndef _getmp(self):\\n    # Extract MP information.  This method was inspired by the \\"highly\\n    # experimental\\" _getexif version that\'s been in use for years now,\\n    # itself based on the ImageFileDirectory class in the TIFF plug-in.\\n\\n    # The MP record essentially consists of a TIFF file embedded in a JPEG\\n    # application marker.\\n    try:\\n        data = self.info[\\"mp\\"]\\n    except KeyError:\\n        return None\\n    file_contents = io.BytesIO(data)\\n    head = file_contents.read(8)\\n    endianness = \'\\u003e\' if head[:4] == b\'\\\\x4d\\\\x4d\\\\x00\\\\x2a\' else \'\\u003c\'\\n    # process dictionary\\n    try:\\n        info = TiffImagePlugin.ImageFileDirectory_v2(head)\\n        info.load(file_contents)\\n        mp = dict(info)\\n    except:\\n        raise SyntaxError(\\"malformed MP Index (unreadable directory)\\")\\n    # it\'s an error not to have a number of images\\n    try:\\n        quant = mp[0xB001]\\n    except KeyError:\\n        raise SyntaxError(\\"malformed MP Index (no number of images)\\")\\n    # get MP entries\\n    mpentries = []\\n    try:\\n        rawmpentries = mp[0xB002]\\n        for entrynum in range(0, quant):\\n            unpackedentry = unpack_from(\\n                \'{}LLLHH\'.format(endianness), rawmpentries, entrynum * 16)\\n            labels = (\'Attribute\', \'Size\', \'DataOffset\', \'EntryNo1\',\\n                      \'EntryNo2\')\\n            mpentry = dict(zip(labels, unpackedentry))\\n            mpentryattr = {\\n                \'DependentParentImageFlag\': bool(mpentry[\'Attribute\'] \\u0026\\n                                                 (1 \\u003c\\u003c 31)),\\n                \'DependentChildImageFlag\': bool(mpentry[\'Attribute\'] \\u0026\\n                                                (1 \\u003c\\u003c 30)),\\n                \'RepresentativeImageFlag\': bool(mpentry[\'Attribute\'] \\u0026\\n                                                (1 \\u003c\\u003c 29)),\\n                \'Reserved\': (mpentry[\'Attribute\'] \\u0026 (3 \\u003c\\u003c 27)) \\u003e\\u003e 27,\\n                \'ImageDataFormat\': (mpentry[\'Attribute\'] \\u0026 (7 \\u003c\\u003c 24)) \\u003e\\u003e 24,\\n                \'MPType\': mpentry[\'Attribute\'] \\u0026 0x00FFFFFF\\n            }\\n            if mpentryattr[\'ImageDataFormat\'] == 0:\\n                mpentryattr[\'ImageDataFormat\'] = \'JPEG\'\\n            else:\\n                raise SyntaxError(\\"unsupported picture format in MPO\\")\\n            mptypemap = {\\n                0x000000: \'Undefined\',\\n                0x010001: \'Large Thumbnail (VGA Equivalent)\',\\n                0x010002: \'Large Thumbnail (Full HD Equivalent)\',\\n                0x020001: \'Multi-Frame Image (Panorama)\',\\n                0x020002: \'Multi-Frame Image: (Disparity)\',\\n                0x020003: \'Multi-Frame Image: (Multi-Angle)\',\\n                0x030000: \'Baseline MP Primary Image\'\\n            }\\n            mpentryattr[\'MPType\'] = mptypemap.get(mpentryattr[\'MPType\'],\\n                                                  \'Unknown\')\\n            mpentry[\'Attribute\'] = mpentryattr\\n            mpentries.append(mpentry)\\n        mp[0xB002] = mpentries\\n    except KeyError:\\n        raise SyntaxError(\\"malformed MP Index (bad MP Entry)\\")\\n    # Next we should try and parse the individual image unique ID list;\\n    # we don\'t because I\'ve never seen this actually used in a real MPO\\n    # file and so can\'t test it.\\n    return mp\\n\\n\\n# --------------------------------------------------------------------\\n# stuff to save JPEG files\\n\\nRAWMODE = {\\n    \\"1\\": \\"L\\",\\n    \\"L\\": \\"L\\",\\n    \\"RGB\\": \\"RGB\\",\\n    \\"RGBA\\": \\"RGB\\",\\n    \\"RGBX\\": \\"RGB\\",\\n    \\"CMYK\\": \\"CMYK;I\\",  # assume adobe conventions\\n    \\"YCbCr\\": \\"YCbCr\\",\\n}\\n\\nzigzag_index = (0,  1,  5,  6, 14, 15, 27, 28,\\n                2,  4,  7, 13, 16, 26, 29, 42,\\n                3,  8, 12, 17, 25, 30, 41, 43,\\n                9, 11, 18, 24, 31, 40, 44, 53,\\n               10, 19, 23, 32, 39, 45, 52, 54,\\n               20, 22, 33, 38, 46, 51, 55, 60,\\n               21, 34, 37, 47, 50, 56, 59, 61,\\n               35, 36, 48, 49, 57, 58, 62, 63)\\n\\nsamplings = {(1, 1, 1, 1, 1, 1): 0,\\n             (2, 1, 1, 1, 1, 1): 1,\\n             (2, 2, 1, 1, 1, 1): 2,\\n             }\\n\\n\\ndef convert_dict_qtables(qtables):\\n    qtables = [qtables[key] for key in range(len(qtables)) if key in qtables]\\n    for idx, table in enumerate(qtables):\\n        qtables[idx] = [table[i] for i in zigzag_index]\\n    return qtables\\n\\n\\ndef get_sampling(im):\\n    # There\'s no subsampling when image have only 1 layer\\n    # (grayscale images) or when they are CMYK (4 layers),\\n    # so set subsampling to default value.\\n    #\\n    # NOTE: currently Pillow can\'t encode JPEG to YCCK format.\\n    # If YCCK support is added in the future, subsampling code will have\\n    # to be updated (here and in JpegEncode.c) to deal with 4 layers.\\n    if not hasattr(im, \'layers\') or im.layers in (1, 4):\\n        return -1\\n    sampling = im.layer[0][1:3] + im.layer[1][1:3] + im.layer[2][1:3]\\n    return samplings.get(sampling, -1)\\n\\n\\ndef _save(im, fp, filename):\\n\\n    try:\\n        rawmode = RAWMODE[im.mode]\\n    except KeyError:\\n        raise IOError(\\"cannot write mode %s as JPEG\\" % im.mode)\\n\\n    if im.mode == \'RGBA\':\\n        warnings.warn(\\n            \'You are saving RGBA image as JPEG. The alpha channel will be \'\\n            \'discarded. This conversion is deprecated and will be disabled \'\\n            \'in Pillow 3.7. Please, convert the image to RGB explicitly.\',\\n            DeprecationWarning\\n        )\\n\\n    info = im.encoderinfo\\n\\n    dpi = [int(round(x)) for x in info.get(\\"dpi\\", (0, 0))]\\n\\n    quality = info.get(\\"quality\\", 0)\\n    subsampling = info.get(\\"subsampling\\", -1)\\n    qtables = info.get(\\"qtables\\")\\n\\n    if quality == \\"keep\\":\\n        quality = 0\\n        subsampling = \\"keep\\"\\n        qtables = \\"keep\\"\\n    elif quality in presets:\\n        preset = presets[quality]\\n        quality = 0\\n        subsampling = preset.get(\'subsampling\', -1)\\n        qtables = preset.get(\'quantization\')\\n    elif not isinstance(quality, int):\\n        raise ValueError(\\"Invalid quality setting\\")\\n    else:\\n        if subsampling in presets:\\n            subsampling = presets[subsampling].get(\'subsampling\', -1)\\n        if isStringType(qtables) and qtables in presets:\\n            qtables = presets[qtables].get(\'quantization\')\\n\\n    if subsampling == \\"4:4:4\\":\\n        subsampling = 0\\n    elif subsampling == \\"4:2:2\\":\\n        subsampling = 1\\n    elif subsampling == \\"4:1:1\\":\\n        subsampling = 2\\n    elif subsampling == \\"keep\\":\\n        if im.format != \\"JPEG\\":\\n            raise ValueError(\\n                \\"Cannot use \'keep\' when original image is not a JPEG\\")\\n        subsampling = get_sampling(im)\\n\\n    def validate_qtables(qtables):\\n        if qtables is None:\\n            return qtables\\n        if isStringType(qtables):\\n            try:\\n                lines = [int(num) for line in qtables.splitlines()\\n                         for num in line.split(\'#\', 1)[0].split()]\\n            except ValueError:\\n                raise ValueError(\\"Invalid quantization table\\")\\n            else:\\n                qtables = [lines[s:s+64] for s in range(0, len(lines), 64)]\\n        if isinstance(qtables, (tuple, list, dict)):\\n            if isinstance(qtables, dict):\\n                qtables = convert_dict_qtables(qtables)\\n            elif isinstance(qtables, tuple):\\n                qtables = list(qtables)\\n            if not (0 \\u003c len(qtables) \\u003c 5):\\n                raise ValueError(\\"None or too many quantization tables\\")\\n            for idx, table in enumerate(qtables):\\n                try:\\n                    if len(table) != 64:\\n                        raise\\n                    table = array.array(\'B\', table)\\n                except TypeError:\\n                    raise ValueError(\\"Invalid quantization table\\")\\n                else:\\n                    qtables[idx] = list(table)\\n            return qtables\\n\\n    if qtables == \\"keep\\":\\n        if im.format != \\"JPEG\\":\\n            raise ValueError(\\n                \\"Cannot use \'keep\' when original image is not a JPEG\\")\\n        qtables = getattr(im, \\"quantization\\", None)\\n    qtables = validate_qtables(qtables)\\n\\n    extra = b\\"\\"\\n\\n    icc_profile = info.get(\\"icc_profile\\")\\n    if icc_profile:\\n        ICC_OVERHEAD_LEN = 14\\n        MAX_BYTES_IN_MARKER = 65533\\n        MAX_DATA_BYTES_IN_MARKER = MAX_BYTES_IN_MARKER - ICC_OVERHEAD_LEN\\n        markers = []\\n        while icc_profile:\\n            markers.append(icc_profile[:MAX_DATA_BYTES_IN_MARKER])\\n            icc_profile = icc_profile[MAX_DATA_BYTES_IN_MARKER:]\\n        i = 1\\n        for marker in markers:\\n            size = struct.pack(\\"\\u003eH\\", 2 + ICC_OVERHEAD_LEN + len(marker))\\n            extra += (b\\"\\\\xFF\\\\xE2\\" + size + b\\"ICC_PROFILE\\\\0\\" + o8(i) +\\n                      o8(len(markers)) + marker)\\n            i += 1\\n\\n    # \\"progressive\\" is the official name, but older documentation\\n    # says \\"progression\\"\\n    # FIXME: issue a warning if the wrong form is used (post-1.1.7)\\n    progressive = info.get(\\"progressive\\", False) or\\\\\\n                  info.get(\\"progression\\", False)\\n\\n    optimize = info.get(\\"optimize\\", False)\\n\\n    # get keyword arguments\\n    im.encoderconfig = (\\n        quality,\\n        progressive,\\n        info.get(\\"smooth\\", 0),\\n        optimize,\\n        info.get(\\"streamtype\\", 0),\\n        dpi[0], dpi[1],\\n        subsampling,\\n        qtables,\\n        extra,\\n        info.get(\\"exif\\", b\\"\\")\\n        )\\n\\n    # if we optimize, libjpeg needs a buffer big enough to hold the whole image\\n    # in a shot. Guessing on the size, at im.size bytes. (raw pizel size is\\n    # channels*size, this is a value that\'s been used in a django patch.\\n    # https://github.com/matthewwithanm/django-imagekit/issues/50\\n    bufsize = 0\\n    if optimize or progressive:\\n        # CMYK can be bigger\\n        if im.mode == \'CMYK\':\\n            bufsize = 4 * im.size[0] * im.size[1]\\n        # keep sets quality to 0, but the actual value may be high.\\n        elif quality \\u003e= 95 or quality == 0:\\n            bufsize = 2 * im.size[0] * im.size[1]\\n        else:\\n            bufsize = im.size[0] * im.size[1]\\n\\n    # The exif info needs to be written as one block, + APP1, + one spare byte.\\n    # Ensure that our buffer is big enough\\n    bufsize = max(ImageFile.MAXBLOCK, bufsize, len(info.get(\\"exif\\", b\\"\\")) + 5)\\n\\n    ImageFile._save(im, fp, [(\\"jpeg\\", (0, 0)+im.size, 0, rawmode)], bufsize)\\n\\n\\ndef _save_cjpeg(im, fp, filename):\\n    # ALTERNATIVE: handle JPEGs via the IJG command line utilities.\\n    import os\\n    import subprocess\\n    tempfile = im._dump()\\n    subprocess.check_call([\\"cjpeg\\", \\"-outfile\\", filename, tempfile])\\n    try:\\n        os.unlink(tempfile)\\n    except OSError:\\n        pass\\n\\n\\n##\\n# Factory for making JPEG and MPO instances\\ndef jpeg_factory(fp=None, filename=None):\\n    im = JpegImageFile(fp, filename)\\n    try:\\n        mpheader = im._getmp()\\n        if mpheader[45057] \\u003e 1:\\n            # It\'s actually an MPO\\n            from .MpoImagePlugin import MpoImageFile\\n            im = MpoImageFile(fp, filename)\\n    except (TypeError, IndexError):\\n        # It is really a JPEG\\n        pass\\n    except SyntaxError:\\n        warnings.warn(\\"Image appears to be a malformed MPO file, it will be \\"\\n                      \\"interpreted as a base JPEG file\\")\\n    return im\\n\\n\\n# -------------------------------------------------------------------q-\\n# Registry stuff\\n\\nImage.register_open(JpegImageFile.format, jpeg_factory, _accept)\\nImage.register_save(JpegImageFile.format, _save)\\n\\nImage.register_extension(JpegImageFile.format, \\".jfif\\")\\nImage.register_extension(JpegImageFile.format, \\".jpe\\")\\nImage.register_extension(JpegImageFile.format, \\".jpg\\")\\nImage.register_extension(JpegImageFile.format, \\".jpeg\\")\\n\\nImage.register_mime(JpegImageFile.format, \\"image/jpeg\\")\\n"}\n'
line: b'{"repo_name":"Taketrung/betfair.py","ref":"refs/heads/master","path":"tests/fixtures.py","content":"# -*- coding: utf-8 -*-\\n\\nimport pytest\\n\\nimport os\\n\\nfrom betfair import betfair\\nfrom tests.utils import response_fixture_factory\\n\\n\\n@pytest.fixture\\ndef client():\\n    return betfair.Betfair(app_key=\'test\', cert_file=\'path/to/cert\')\\n\\n\\n@pytest.fixture\\ndef logged_in_client(client):\\n    client = betfair.Betfair(app_key=\'test\', cert_file=\'path/to/cert\')\\n    client.session_token = \'secret\'\\n    return client\\n\\nlogin_success = response_fixture_factory(\\n    os.path.join(betfair.IDENTITY_URLS[None], \'certlogin\'),\\n    {\\n        \'loginStatus\': \'SUCCESS\',\\n        \'sessionToken\': \'secret\',\\n    },\\n)\\n\\nlogin_failure = response_fixture_factory(\\n    os.path.join(betfair.IDENTITY_URLS[None], \'certlogin\'),\\n    {\'loginStatus\': \'INVALID_USERNAME_OR_PASSWORD\'},\\n)\\n\\nlogin_bad_code = response_fixture_factory(\\n    os.path.join(betfair.IDENTITY_URLS[None], \'certlogin\'),\\n    status=422,\\n)\\n\\nkeepalive_success = response_fixture_factory(\\n    os.path.join(betfair.IDENTITY_URLS[None], \'keepAlive\'),\\n    {\'status\': \'SUCCESS\'},\\n)\\n\\nkeepalive_failure = response_fixture_factory(\\n    os.path.join(betfair.IDENTITY_URLS[None], \'keepAlive\'),\\n    {\\n        \'status\': \'FAIL\',\\n        \'error\': \'NO_SESSION\',\\n    },\\n)\\n\\nlogout_success = response_fixture_factory(\\n    os.path.join(betfair.IDENTITY_URLS[None], \'logout\'),\\n    {\'status\': \'SUCCESS\'},\\n)\\n\\nlogout_failure = response_fixture_factory(\\n    os.path.join(betfair.IDENTITY_URLS[None], \'logout\'),\\n    {\\n        \'status\': \'FAIL\',\\n        \'error\': \'NO_SESSION\',\\n    },\\n)\\n\\nlogin_required_methods = [\\n    \'keep_alive\',\\n    \'logout\',\\n    \'list_event_types\',\\n    \'list_competitions\',\\n    \'list_time_ranges\',\\n    \'list_events\',\\n    \'list_market_types\',\\n    \'list_countries\',\\n    \'list_venues\',\\n    \'list_market_catalogue\',\\n    \'list_market_book\',\\n    \'list_market_profit_and_loss\',\\n    \'list_current_orders\',\\n    \'list_cleared_orders\',\\n    \'place_orders\',\\n    \'cancel_orders\',\\n    \'replace_orders\',\\n    \'update_orders\',\\n]\\n"}\n'
line: b'{"repo_name":"mjtamlyn/django","ref":"refs/heads/master","path":"tests/mail/tests.py","content":"import asyncore\\nimport base64\\nimport mimetypes\\nimport os\\nimport shutil\\nimport smtpd\\nimport socket\\nimport sys\\nimport tempfile\\nimport threading\\nfrom email import message_from_binary_file, message_from_bytes\\nfrom email.header import Header\\nfrom email.mime.text import MIMEText\\nfrom email.utils import parseaddr\\nfrom io import StringIO\\nfrom smtplib import SMTP, SMTPAuthenticationError, SMTPException\\nfrom ssl import SSLError\\n\\nfrom django.core import mail\\nfrom django.core.mail import (\\n    EmailMessage, EmailMultiAlternatives, mail_admins, mail_managers,\\n    send_mail, send_mass_mail,\\n)\\nfrom django.core.mail.backends import console, dummy, filebased, locmem, smtp\\nfrom django.core.mail.message import BadHeaderError, sanitize_address\\nfrom django.test import SimpleTestCase, override_settings\\nfrom django.test.utils import requires_tz_support\\nfrom django.utils.encoding import force_bytes, force_text\\nfrom django.utils.translation import gettext_lazy\\n\\n\\nclass HeadersCheckMixin:\\n\\n    def assertMessageHasHeaders(self, message, headers):\\n        \\"\\"\\"\\n        Asserts that the `message` has all `headers`.\\n\\n        message: can be an instance of an email.Message subclass or a string\\n                 with the contents of an email message.\\n        headers: should be a set of (header-name, header-value) tuples.\\n        \\"\\"\\"\\n        if isinstance(message, bytes):\\n            message = message_from_bytes(message)\\n        msg_headers = set(message.items())\\n        self.assertTrue(headers.issubset(msg_headers), msg=\'Message is missing \'\\n                        \'the following headers: %s\' % (headers - msg_headers),)\\n\\n\\nclass MailTests(HeadersCheckMixin, SimpleTestCase):\\n    \\"\\"\\"\\n    Non-backend specific tests.\\n    \\"\\"\\"\\n    def get_decoded_attachments(self, django_message):\\n        \\"\\"\\"\\n        Encode the specified django.core.mail.message.EmailMessage, then decode\\n        it using Python\'s email.parser module and, for each attachment of the\\n        message, return a list of tuples with (filename, content, mimetype).\\n        \\"\\"\\"\\n        msg_bytes = django_message.message().as_bytes()\\n        email_message = message_from_bytes(msg_bytes)\\n\\n        def iter_attachments():\\n            for i in email_message.walk():\\n                # Once support for Python\\u003c3.5 has been dropped, we can use\\n                # i.get_content_disposition() here instead.\\n                content_disposition = i.get(\'content-disposition\', \'\').split(\';\')[0].lower()\\n                if content_disposition == \'attachment\':\\n                    filename = i.get_filename()\\n                    content = i.get_payload(decode=True)\\n                    mimetype = i.get_content_type()\\n                    yield filename, content, mimetype\\n\\n        return list(iter_attachments())\\n\\n    def test_ascii(self):\\n        email = EmailMessage(\'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'])\\n        message = email.message()\\n        self.assertEqual(message[\'Subject\'], \'Subject\')\\n        self.assertEqual(message.get_payload(), \'Content\')\\n        self.assertEqual(message[\'From\'], \'from@example.com\')\\n        self.assertEqual(message[\'To\'], \'to@example.com\')\\n\\n    def test_multiple_recipients(self):\\n        email = EmailMessage(\'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\', \'other@example.com\'])\\n        message = email.message()\\n        self.assertEqual(message[\'Subject\'], \'Subject\')\\n        self.assertEqual(message.get_payload(), \'Content\')\\n        self.assertEqual(message[\'From\'], \'from@example.com\')\\n        self.assertEqual(message[\'To\'], \'to@example.com, other@example.com\')\\n\\n    def test_recipients_with_empty_strings(self):\\n        \\"\\"\\"\\n        Empty strings in various recipient arguments are always stripped\\n        off the final recipient list.\\n        \\"\\"\\"\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\', \'\'],\\n            cc=[\'cc@example.com\', \'\'],\\n            bcc=[\'\', \'bcc@example.com\'],\\n            reply_to=[\'\', None],\\n        )\\n        self.assertEqual(\\n            email.recipients(),\\n            [\'to@example.com\', \'cc@example.com\', \'bcc@example.com\']\\n        )\\n\\n    def test_cc(self):\\n        \\"\\"\\"Regression test for #7722\\"\\"\\"\\n        email = EmailMessage(\'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'], cc=[\'cc@example.com\'])\\n        message = email.message()\\n        self.assertEqual(message[\'Cc\'], \'cc@example.com\')\\n        self.assertEqual(email.recipients(), [\'to@example.com\', \'cc@example.com\'])\\n\\n        # Test multiple CC with multiple To\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\', \'other@example.com\'],\\n            cc=[\'cc@example.com\', \'cc.other@example.com\']\\n        )\\n        message = email.message()\\n        self.assertEqual(message[\'Cc\'], \'cc@example.com, cc.other@example.com\')\\n        self.assertEqual(\\n            email.recipients(),\\n            [\'to@example.com\', \'other@example.com\', \'cc@example.com\', \'cc.other@example.com\']\\n        )\\n\\n        # Testing with Bcc\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\', \'other@example.com\'],\\n            cc=[\'cc@example.com\', \'cc.other@example.com\'], bcc=[\'bcc@example.com\']\\n        )\\n        message = email.message()\\n        self.assertEqual(message[\'Cc\'], \'cc@example.com, cc.other@example.com\')\\n        self.assertEqual(\\n            email.recipients(),\\n            [\'to@example.com\', \'other@example.com\', \'cc@example.com\', \'cc.other@example.com\', \'bcc@example.com\']\\n        )\\n\\n    def test_reply_to(self):\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'],\\n            reply_to=[\'reply_to@example.com\'],\\n        )\\n        message = email.message()\\n        self.assertEqual(message[\'Reply-To\'], \'reply_to@example.com\')\\n\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'],\\n            reply_to=[\'reply_to1@example.com\', \'reply_to2@example.com\']\\n        )\\n        message = email.message()\\n        self.assertEqual(message[\'Reply-To\'], \'reply_to1@example.com, reply_to2@example.com\')\\n\\n    def test_recipients_as_tuple(self):\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'from@example.com\', (\'to@example.com\', \'other@example.com\'),\\n            cc=(\'cc@example.com\', \'cc.other@example.com\'), bcc=(\'bcc@example.com\',)\\n        )\\n        message = email.message()\\n        self.assertEqual(message[\'Cc\'], \'cc@example.com, cc.other@example.com\')\\n        self.assertEqual(\\n            email.recipients(),\\n            [\'to@example.com\', \'other@example.com\', \'cc@example.com\', \'cc.other@example.com\', \'bcc@example.com\']\\n        )\\n\\n    def test_recipients_as_string(self):\\n        with self.assertRaisesMessage(TypeError, \'\\"to\\" argument must be a list or tuple\'):\\n            EmailMessage(to=\'foo@example.com\')\\n        with self.assertRaisesMessage(TypeError, \'\\"cc\\" argument must be a list or tuple\'):\\n            EmailMessage(cc=\'foo@example.com\')\\n        with self.assertRaisesMessage(TypeError, \'\\"bcc\\" argument must be a list or tuple\'):\\n            EmailMessage(bcc=\'foo@example.com\')\\n        with self.assertRaisesMessage(TypeError, \'\\"reply_to\\" argument must be a list or tuple\'):\\n            EmailMessage(reply_to=\'reply_to@example.com\')\\n\\n    def test_header_injection(self):\\n        email = EmailMessage(\'Subject\\\\nInjection Test\', \'Content\', \'from@example.com\', [\'to@example.com\'])\\n        with self.assertRaises(BadHeaderError):\\n            email.message()\\n        email = EmailMessage(\\n            gettext_lazy(\'Subject\\\\nInjection Test\'), \'Content\', \'from@example.com\', [\'to@example.com\']\\n        )\\n        with self.assertRaises(BadHeaderError):\\n            email.message()\\n\\n    def test_space_continuation(self):\\n        \\"\\"\\"\\n        Test for space continuation character in long (ASCII) subject headers (#7747)\\n        \\"\\"\\"\\n        email = EmailMessage(\\n            \'Long subject lines that get wrapped should contain a space \'\\n            \'continuation character to get expected behavior in Outlook and Thunderbird\',\\n            \'Content\', \'from@example.com\', [\'to@example.com\']\\n        )\\n        message = email.message()\\n        self.assertEqual(\\n            message[\'Subject\'].encode(),\\n            b\'Long subject lines that get wrapped should contain a space continuation\\\\n\'\\n            b\' character to get expected behavior in Outlook and Thunderbird\'\\n        )\\n\\n    def test_message_header_overrides(self):\\n        \\"\\"\\"\\n        Specifying dates or message-ids in the extra headers overrides the\\n        default values (#9233)\\n        \\"\\"\\"\\n        headers = {\\"date\\": \\"Fri, 09 Nov 2001 01:08:47 -0000\\", \\"Message-ID\\": \\"foo\\"}\\n        email = EmailMessage(\'subject\', \'content\', \'from@example.com\', [\'to@example.com\'], headers=headers)\\n\\n        self.assertMessageHasHeaders(email.message(), {\\n            (\'Content-Transfer-Encoding\', \'7bit\'),\\n            (\'Content-Type\', \'text/plain; charset=\\"utf-8\\"\'),\\n            (\'From\', \'from@example.com\'),\\n            (\'MIME-Version\', \'1.0\'),\\n            (\'Message-ID\', \'foo\'),\\n            (\'Subject\', \'subject\'),\\n            (\'To\', \'to@example.com\'),\\n            (\'date\', \'Fri, 09 Nov 2001 01:08:47 -0000\'),\\n        })\\n\\n    def test_from_header(self):\\n        \\"\\"\\"\\n        Make sure we can manually set the From header (#9214)\\n        \\"\\"\\"\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        message = email.message()\\n        self.assertEqual(message[\'From\'], \'from@example.com\')\\n\\n    def test_to_header(self):\\n        \\"\\"\\"\\n        Make sure we can manually set the To header (#17444)\\n        \\"\\"\\"\\n        email = EmailMessage(\'Subject\', \'Content\', \'bounce@example.com\',\\n                             [\'list-subscriber@example.com\', \'list-subscriber2@example.com\'],\\n                             headers={\'To\': \'mailing-list@example.com\'})\\n        message = email.message()\\n        self.assertEqual(message[\'To\'], \'mailing-list@example.com\')\\n        self.assertEqual(email.to, [\'list-subscriber@example.com\', \'list-subscriber2@example.com\'])\\n\\n        # If we don\'t set the To header manually, it should default to the `to` argument to the constructor\\n        email = EmailMessage(\'Subject\', \'Content\', \'bounce@example.com\',\\n                             [\'list-subscriber@example.com\', \'list-subscriber2@example.com\'])\\n        message = email.message()\\n        self.assertEqual(message[\'To\'], \'list-subscriber@example.com, list-subscriber2@example.com\')\\n        self.assertEqual(email.to, [\'list-subscriber@example.com\', \'list-subscriber2@example.com\'])\\n\\n    def test_reply_to_header(self):\\n        \\"\\"\\"\\n        Specifying \'Reply-To\' in headers should override reply_to.\\n        \\"\\"\\"\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'bounce@example.com\', [\'to@example.com\'],\\n            reply_to=[\'foo@example.com\'], headers={\'Reply-To\': \'override@example.com\'},\\n        )\\n        message = email.message()\\n        self.assertEqual(message[\'Reply-To\'], \'override@example.com\')\\n\\n    def test_multiple_message_call(self):\\n        \\"\\"\\"\\n        Regression for #13259 - Make sure that headers are not changed when\\n        calling EmailMessage.message()\\n        \\"\\"\\"\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        message = email.message()\\n        self.assertEqual(message[\'From\'], \'from@example.com\')\\n        message = email.message()\\n        self.assertEqual(message[\'From\'], \'from@example.com\')\\n\\n    def test_unicode_address_header(self):\\n        \\"\\"\\"\\n        Regression for #11144 - When a to/from/cc header contains unicode,\\n        make sure the email addresses are parsed correctly (especially with\\n        regards to commas)\\n        \\"\\"\\"\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'from@example.com\',\\n            [\'\\"Firstname S\xc3\xbcrname\\" \\u003cto@example.com\\u003e\', \'other@example.com\'],\\n        )\\n        self.assertEqual(\\n            email.message()[\'To\'],\\n            \'=?utf-8?q?Firstname_S=C3=BCrname?= \\u003cto@example.com\\u003e, other@example.com\'\\n        )\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'from@example.com\',\\n            [\'\\"S\xc3\xbcrname, Firstname\\" \\u003cto@example.com\\u003e\', \'other@example.com\'],\\n        )\\n        self.assertEqual(\\n            email.message()[\'To\'],\\n            \'=?utf-8?q?S=C3=BCrname=2C_Firstname?= \\u003cto@example.com\\u003e, other@example.com\'\\n        )\\n\\n    def test_unicode_headers(self):\\n        email = EmailMessage(\\"G\xc5\xbceg\xc5\xbc\xc3\xb3\xc5\x82ka\\", \\"Content\\", \\"from@example.com\\", [\\"to@example.com\\"],\\n                             headers={\\"Sender\\": \'\\"Firstname S\xc3\xbcrname\\" \\u003csender@example.com\\u003e\',\\n                                      \\"Comments\\": \'My S\xc3\xbcrname is non-ASCII\'})\\n        message = email.message()\\n        self.assertEqual(message[\'Subject\'], \'=?utf-8?b?R8W8ZWfFvMOzxYJrYQ==?=\')\\n        self.assertEqual(message[\'Sender\'], \'=?utf-8?q?Firstname_S=C3=BCrname?= \\u003csender@example.com\\u003e\')\\n        self.assertEqual(message[\'Comments\'], \'=?utf-8?q?My_S=C3=BCrname_is_non-ASCII?=\')\\n\\n    def test_safe_mime_multipart(self):\\n        \\"\\"\\"\\n        Make sure headers can be set with a different encoding than utf-8 in\\n        SafeMIMEMultipart as well\\n        \\"\\"\\"\\n        headers = {\\"Date\\": \\"Fri, 09 Nov 2001 01:08:47 -0000\\", \\"Message-ID\\": \\"foo\\"}\\n        from_email, to = \'from@example.com\', \'\\"S\xc3\xbcrname, Firstname\\" \\u003cto@example.com\\u003e\'\\n        text_content = \'This is an important message.\'\\n        html_content = \'\\u003cp\\u003eThis is an \\u003cstrong\\u003eimportant\\u003c/strong\\u003e message.\\u003c/p\\u003e\'\\n        msg = EmailMultiAlternatives(\'Message from Firstname S\xc3\xbcrname\', text_content, from_email, [to], headers=headers)\\n        msg.attach_alternative(html_content, \\"text/html\\")\\n        msg.encoding = \'iso-8859-1\'\\n        self.assertEqual(msg.message()[\'To\'], \'=?iso-8859-1?q?S=FCrname=2C_Firstname?= \\u003cto@example.com\\u003e\')\\n        self.assertEqual(msg.message()[\'Subject\'], \'=?iso-8859-1?q?Message_from_Firstname_S=FCrname?=\')\\n\\n    def test_encoding(self):\\n        \\"\\"\\"\\n        Regression for #12791 - Encode body correctly with other encodings\\n        than utf-8\\n        \\"\\"\\"\\n        email = EmailMessage(\'Subject\', \'Firstname S\xc3\xbcrname is a great guy.\', \'from@example.com\', [\'other@example.com\'])\\n        email.encoding = \'iso-8859-1\'\\n        message = email.message()\\n        self.assertMessageHasHeaders(message, {\\n            (\'MIME-Version\', \'1.0\'),\\n            (\'Content-Type\', \'text/plain; charset=\\"iso-8859-1\\"\'),\\n            (\'Content-Transfer-Encoding\', \'quoted-printable\'),\\n            (\'Subject\', \'Subject\'),\\n            (\'From\', \'from@example.com\'),\\n            (\'To\', \'other@example.com\')})\\n        self.assertEqual(message.get_payload(), \'Firstname S=FCrname is a great guy.\')\\n\\n        # Make sure MIME attachments also works correctly with other encodings than utf-8\\n        text_content = \'Firstname S\xc3\xbcrname is a great guy.\'\\n        html_content = \'\\u003cp\\u003eFirstname S\xc3\xbcrname is a \\u003cstrong\\u003egreat\\u003c/strong\\u003e guy.\\u003c/p\\u003e\'\\n        msg = EmailMultiAlternatives(\'Subject\', text_content, \'from@example.com\', [\'to@example.com\'])\\n        msg.encoding = \'iso-8859-1\'\\n        msg.attach_alternative(html_content, \\"text/html\\")\\n        payload0 = msg.message().get_payload(0)\\n        self.assertMessageHasHeaders(payload0, {\\n            (\'MIME-Version\', \'1.0\'),\\n            (\'Content-Type\', \'text/plain; charset=\\"iso-8859-1\\"\'),\\n            (\'Content-Transfer-Encoding\', \'quoted-printable\')})\\n        self.assertTrue(payload0.as_bytes().endswith(b\'\\\\n\\\\nFirstname S=FCrname is a great guy.\'))\\n        payload1 = msg.message().get_payload(1)\\n        self.assertMessageHasHeaders(payload1, {\\n            (\'MIME-Version\', \'1.0\'),\\n            (\'Content-Type\', \'text/html; charset=\\"iso-8859-1\\"\'),\\n            (\'Content-Transfer-Encoding\', \'quoted-printable\')})\\n        self.assertTrue(\\n            payload1.as_bytes().endswith(b\'\\\\n\\\\n\\u003cp\\u003eFirstname S=FCrname is a \\u003cstrong\\u003egreat\\u003c/strong\\u003e guy.\\u003c/p\\u003e\')\\n        )\\n\\n    def test_attachments(self):\\n        \\"\\"\\"Regression test for #9367\\"\\"\\"\\n        headers = {\\"Date\\": \\"Fri, 09 Nov 2001 01:08:47 -0000\\", \\"Message-ID\\": \\"foo\\"}\\n        subject, from_email, to = \'hello\', \'from@example.com\', \'to@example.com\'\\n        text_content = \'This is an important message.\'\\n        html_content = \'\\u003cp\\u003eThis is an \\u003cstrong\\u003eimportant\\u003c/strong\\u003e message.\\u003c/p\\u003e\'\\n        msg = EmailMultiAlternatives(subject, text_content, from_email, [to], headers=headers)\\n        msg.attach_alternative(html_content, \\"text/html\\")\\n        msg.attach(\\"an attachment.pdf\\", b\\"%PDF-1.4.%...\\", mimetype=\\"application/pdf\\")\\n        msg_bytes = msg.message().as_bytes()\\n        message = message_from_bytes(msg_bytes)\\n        self.assertTrue(message.is_multipart())\\n        self.assertEqual(message.get_content_type(), \'multipart/mixed\')\\n        self.assertEqual(message.get_default_type(), \'text/plain\')\\n        payload = message.get_payload()\\n        self.assertEqual(payload[0].get_content_type(), \'multipart/alternative\')\\n        self.assertEqual(payload[1].get_content_type(), \'application/pdf\')\\n\\n    def test_non_ascii_attachment_filename(self):\\n        \\"\\"\\"Regression test for #14964\\"\\"\\"\\n        headers = {\\"Date\\": \\"Fri, 09 Nov 2001 01:08:47 -0000\\", \\"Message-ID\\": \\"foo\\"}\\n        subject, from_email, to = \'hello\', \'from@example.com\', \'to@example.com\'\\n        content = \'This is the message.\'\\n        msg = EmailMessage(subject, content, from_email, [to], headers=headers)\\n        # Unicode in file name\\n        msg.attach(\\"une pi\xc3\xa8ce jointe.pdf\\", b\\"%PDF-1.4.%...\\", mimetype=\\"application/pdf\\")\\n        msg_bytes = msg.message().as_bytes()\\n        message = message_from_bytes(msg_bytes)\\n        payload = message.get_payload()\\n        self.assertEqual(payload[1].get_filename(), \'une pi\xc3\xa8ce jointe.pdf\')\\n\\n    def test_attach_file(self):\\n        \\"\\"\\"\\n        Test attaching a file against different mimetypes and make sure that\\n        a file will be attached and sent properly even if an invalid mimetype\\n        is specified.\\n        \\"\\"\\"\\n        files = (\\n            # filename, actual mimetype\\n            (\'file.txt\', \'text/plain\'),\\n            (\'file.png\', \'image/png\'),\\n            (\'file_txt\', None),\\n            (\'file_png\', None),\\n            (\'file_txt.png\', \'image/png\'),\\n            (\'file_png.txt\', \'text/plain\'),\\n            (\'file.eml\', \'message/rfc822\'),\\n        )\\n        test_mimetypes = [\'text/plain\', \'image/png\', None]\\n\\n        for basename, real_mimetype in files:\\n            for mimetype in test_mimetypes:\\n                email = EmailMessage(\'subject\', \'body\', \'from@example.com\', [\'to@example.com\'])\\n                self.assertEqual(mimetypes.guess_type(basename)[0], real_mimetype)\\n                self.assertEqual(email.attachments, [])\\n                file_path = os.path.join(os.path.dirname(__file__), \'attachments\', basename)\\n                email.attach_file(file_path, mimetype=mimetype)\\n                self.assertEqual(len(email.attachments), 1)\\n                self.assertIn(basename, email.attachments[0])\\n                msgs_sent_num = email.send()\\n                self.assertEqual(msgs_sent_num, 1)\\n\\n    def test_attach_text_as_bytes(self):\\n        msg = EmailMessage(\'subject\', \'body\', \'from@example.com\', [\'to@example.com\'])\\n        msg.attach(\'file.txt\', b\'file content\')\\n        sent_num = msg.send()\\n        self.assertEqual(sent_num, 1)\\n        filename, content, mimetype = self.get_decoded_attachments(msg)[0]\\n        self.assertEqual(filename, \'file.txt\')\\n        self.assertEqual(content, b\'file content\')\\n        self.assertEqual(mimetype, \'text/plain\')\\n\\n    def test_attach_utf8_text_as_bytes(self):\\n        \\"\\"\\"\\n        Non-ASCII characters encoded as valid UTF-8 are correctly transported\\n        and decoded.\\n        \\"\\"\\"\\n        msg = EmailMessage(\'subject\', \'body\', \'from@example.com\', [\'to@example.com\'])\\n        msg.attach(\'file.txt\', b\'\\\\xc3\\\\xa4\')  # UTF-8 encoded a umlaut.\\n        filename, content, mimetype = self.get_decoded_attachments(msg)[0]\\n        self.assertEqual(filename, \'file.txt\')\\n        self.assertEqual(content, b\'\\\\xc3\\\\xa4\')\\n        self.assertEqual(mimetype, \'text/plain\')\\n\\n    def test_attach_non_utf8_text_as_bytes(self):\\n        \\"\\"\\"\\n        Binary data that can\'t be decoded as UTF-8 overrides the MIME type\\n        instead of decoding the data.\\n        \\"\\"\\"\\n        msg = EmailMessage(\'subject\', \'body\', \'from@example.com\', [\'to@example.com\'])\\n        msg.attach(\'file.txt\', b\'\\\\xff\')  # Invalid UTF-8.\\n        filename, content, mimetype = self.get_decoded_attachments(msg)[0]\\n        self.assertEqual(filename, \'file.txt\')\\n        # Content should be passed through unmodified.\\n        self.assertEqual(content, b\'\\\\xff\')\\n        self.assertEqual(mimetype, \'application/octet-stream\')\\n\\n    def test_dummy_backend(self):\\n        \\"\\"\\"\\n        Make sure that dummy backends returns correct number of sent messages\\n        \\"\\"\\"\\n        connection = dummy.EmailBackend()\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        self.assertEqual(connection.send_messages([email, email, email]), 3)\\n\\n    def test_arbitrary_keyword(self):\\n        \\"\\"\\"\\n        Make sure that get_connection() accepts arbitrary keyword that might be\\n        used with custom backends.\\n        \\"\\"\\"\\n        c = mail.get_connection(fail_silently=True, foo=\'bar\')\\n        self.assertTrue(c.fail_silently)\\n\\n    def test_custom_backend(self):\\n        \\"\\"\\"Test custom backend defined in this suite.\\"\\"\\"\\n        conn = mail.get_connection(\'mail.custombackend.EmailBackend\')\\n        self.assertTrue(hasattr(conn, \'test_outbox\'))\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        conn.send_messages([email])\\n        self.assertEqual(len(conn.test_outbox), 1)\\n\\n    def test_backend_arg(self):\\n        \\"\\"\\"Test backend argument of mail.get_connection()\\"\\"\\"\\n        self.assertIsInstance(mail.get_connection(\'django.core.mail.backends.smtp.EmailBackend\'), smtp.EmailBackend)\\n        self.assertIsInstance(\\n            mail.get_connection(\'django.core.mail.backends.locmem.EmailBackend\'),\\n            locmem.EmailBackend\\n        )\\n        self.assertIsInstance(mail.get_connection(\'django.core.mail.backends.dummy.EmailBackend\'), dummy.EmailBackend)\\n        self.assertIsInstance(\\n            mail.get_connection(\'django.core.mail.backends.console.EmailBackend\'),\\n            console.EmailBackend\\n        )\\n        with tempfile.TemporaryDirectory() as tmp_dir:\\n            self.assertIsInstance(\\n                mail.get_connection(\'django.core.mail.backends.filebased.EmailBackend\', file_path=tmp_dir),\\n                filebased.EmailBackend\\n            )\\n        self.assertIsInstance(mail.get_connection(), locmem.EmailBackend)\\n\\n    @override_settings(\\n        EMAIL_BACKEND=\'django.core.mail.backends.locmem.EmailBackend\',\\n        ADMINS=[(\'nobody\', \'nobody@example.com\')],\\n        MANAGERS=[(\'nobody\', \'nobody@example.com\')])\\n    def test_connection_arg(self):\\n        \\"\\"\\"Test connection argument to send_mail(), et. al.\\"\\"\\"\\n        mail.outbox = []\\n\\n        # Send using non-default connection\\n        connection = mail.get_connection(\'mail.custombackend.EmailBackend\')\\n        send_mail(\'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'], connection=connection)\\n        self.assertEqual(mail.outbox, [])\\n        self.assertEqual(len(connection.test_outbox), 1)\\n        self.assertEqual(connection.test_outbox[0].subject, \'Subject\')\\n\\n        connection = mail.get_connection(\'mail.custombackend.EmailBackend\')\\n        send_mass_mail([\\n            (\'Subject1\', \'Content1\', \'from1@example.com\', [\'to1@example.com\']),\\n            (\'Subject2\', \'Content2\', \'from2@example.com\', [\'to2@example.com\']),\\n        ], connection=connection)\\n        self.assertEqual(mail.outbox, [])\\n        self.assertEqual(len(connection.test_outbox), 2)\\n        self.assertEqual(connection.test_outbox[0].subject, \'Subject1\')\\n        self.assertEqual(connection.test_outbox[1].subject, \'Subject2\')\\n\\n        connection = mail.get_connection(\'mail.custombackend.EmailBackend\')\\n        mail_admins(\'Admin message\', \'Content\', connection=connection)\\n        self.assertEqual(mail.outbox, [])\\n        self.assertEqual(len(connection.test_outbox), 1)\\n        self.assertEqual(connection.test_outbox[0].subject, \'[Django] Admin message\')\\n\\n        connection = mail.get_connection(\'mail.custombackend.EmailBackend\')\\n        mail_managers(\'Manager message\', \'Content\', connection=connection)\\n        self.assertEqual(mail.outbox, [])\\n        self.assertEqual(len(connection.test_outbox), 1)\\n        self.assertEqual(connection.test_outbox[0].subject, \'[Django] Manager message\')\\n\\n    def test_dont_mangle_from_in_body(self):\\n        # Regression for #13433 - Make sure that EmailMessage doesn\'t mangle\\n        # \'From \' in message body.\\n        email = EmailMessage(\\n            \'Subject\', \'From the future\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        self.assertNotIn(b\'\\u003eFrom the future\', email.message().as_bytes())\\n\\n    def test_dont_base64_encode(self):\\n        # Ticket #3472\\n        # Shouldn\'t use Base64 encoding at all\\n        msg = EmailMessage(\\n            \'Subject\', \'UTF-8 encoded body\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        self.assertIn(b\'Content-Transfer-Encoding: 7bit\', msg.message().as_bytes())\\n\\n        # Ticket #11212\\n        # Shouldn\'t use quoted printable, should detect it can represent content with 7 bit data\\n        msg = EmailMessage(\\n            \'Subject\', \'Body with only ASCII characters.\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        s = msg.message().as_bytes()\\n        self.assertIn(b\'Content-Transfer-Encoding: 7bit\', s)\\n\\n        # Shouldn\'t use quoted printable, should detect it can represent content with 8 bit data\\n        msg = EmailMessage(\\n            \'Subject\', \'Body with latin characters: \xc3\xa0\xc3\xa1\xc3\xa4.\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        s = msg.message().as_bytes()\\n        self.assertIn(b\'Content-Transfer-Encoding: 8bit\', s)\\n        s = msg.message().as_string()\\n        self.assertIn(\'Content-Transfer-Encoding: 8bit\', s)\\n\\n        msg = EmailMessage(\\n            \'Subject\', \'Body with non latin characters: \xd0\x90 \xd0\x91 \xd0\x92 \xd0\x93 \xd0\x94 \xd0\x95 \xd0\x96 \xd0\x85 \xd0\x97 \xd0\x98 \xd0\x86 \xd0\x9a \xd0\x9b \xd0\x9c \xd0\x9d \xd0\x9e \xd0\x9f.\', \'bounce@example.com\',\\n            [\'to@example.com\'], headers={\'From\': \'from@example.com\'},\\n        )\\n        s = msg.message().as_bytes()\\n        self.assertIn(b\'Content-Transfer-Encoding: 8bit\', s)\\n        s = msg.message().as_string()\\n        self.assertIn(\'Content-Transfer-Encoding: 8bit\', s)\\n\\n    def test_dont_base64_encode_message_rfc822(self):\\n        # Ticket #18967\\n        # Shouldn\'t use base64 encoding for a child EmailMessage attachment.\\n        # Create a child message first\\n        child_msg = EmailMessage(\\n            \'Child Subject\', \'Some body of child message\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        child_s = child_msg.message().as_string()\\n\\n        # Now create a parent\\n        parent_msg = EmailMessage(\\n            \'Parent Subject\', \'Some parent body\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n\\n        # Attach to parent as a string\\n        parent_msg.attach(content=child_s, mimetype=\'message/rfc822\')\\n        parent_s = parent_msg.message().as_string()\\n\\n        # The child message header is not base64 encoded\\n        self.assertIn(\'Child Subject\', parent_s)\\n\\n        # Feature test: try attaching email.Message object directly to the mail.\\n        parent_msg = EmailMessage(\\n            \'Parent Subject\', \'Some parent body\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        parent_msg.attach(content=child_msg.message(), mimetype=\'message/rfc822\')\\n        parent_s = parent_msg.message().as_string()\\n\\n        # The child message header is not base64 encoded\\n        self.assertIn(\'Child Subject\', parent_s)\\n\\n        # Feature test: try attaching Django\'s EmailMessage object directly to the mail.\\n        parent_msg = EmailMessage(\\n            \'Parent Subject\', \'Some parent body\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        parent_msg.attach(content=child_msg, mimetype=\'message/rfc822\')\\n        parent_s = parent_msg.message().as_string()\\n\\n        # The child message header is not base64 encoded\\n        self.assertIn(\'Child Subject\', parent_s)\\n\\n    def test_sanitize_address(self):\\n        \\"\\"\\"\\n        Email addresses are properly sanitized.\\n        \\"\\"\\"\\n        # Simple ASCII address - string form\\n        self.assertEqual(sanitize_address(\'to@example.com\', \'ascii\'), \'to@example.com\')\\n        self.assertEqual(sanitize_address(\'to@example.com\', \'utf-8\'), \'to@example.com\')\\n\\n        # Simple ASCII address - tuple form\\n        self.assertEqual(\\n            sanitize_address((\'A name\', \'to@example.com\'), \'ascii\'),\\n            \'A name \\u003cto@example.com\\u003e\'\\n        )\\n        self.assertEqual(\\n            sanitize_address((\'A name\', \'to@example.com\'), \'utf-8\'),\\n            \'=?utf-8?q?A_name?= \\u003cto@example.com\\u003e\'\\n        )\\n\\n        # Unicode characters are are supported in RFC-6532.\\n        self.assertEqual(\\n            sanitize_address(\'t\xc3\xb3@example.com\', \'utf-8\'),\\n            \'=?utf-8?b?dMOz?=@example.com\'\\n        )\\n        self.assertEqual(\\n            sanitize_address((\'T\xc3\xb3 Example\', \'t\xc3\xb3@example.com\'), \'utf-8\'),\\n            \'=?utf-8?q?T=C3=B3_Example?= \\u003c=?utf-8?b?dMOz?=@example.com\\u003e\'\\n        )\\n\\n\\n@requires_tz_support\\nclass MailTimeZoneTests(SimpleTestCase):\\n\\n    @override_settings(EMAIL_USE_LOCALTIME=False, USE_TZ=True, TIME_ZONE=\'Africa/Algiers\')\\n    def test_date_header_utc(self):\\n        \\"\\"\\"\\n        EMAIL_USE_LOCALTIME=False creates a datetime in UTC.\\n        \\"\\"\\"\\n        email = EmailMessage(\'Subject\', \'Body\', \'bounce@example.com\', [\'to@example.com\'])\\n        self.assertTrue(email.message()[\'Date\'].endswith(\'-0000\'))\\n\\n    @override_settings(EMAIL_USE_LOCALTIME=True, USE_TZ=True, TIME_ZONE=\'Africa/Algiers\')\\n    def test_date_header_localtime(self):\\n        \\"\\"\\"\\n        EMAIL_USE_LOCALTIME=True creates a datetime in the local time zone.\\n        \\"\\"\\"\\n        email = EmailMessage(\'Subject\', \'Body\', \'bounce@example.com\', [\'to@example.com\'])\\n        self.assertTrue(email.message()[\'Date\'].endswith(\'+0100\'))  # Africa/Algiers is UTC+1\\n\\n\\nclass PythonGlobalState(SimpleTestCase):\\n    \\"\\"\\"\\n    Tests for #12422 -- Django smarts (#2472/#11212) with charset of utf-8 text\\n    parts shouldn\'t pollute global email Python package charset registry when\\n    django.mail.message is imported.\\n    \\"\\"\\"\\n\\n    def test_utf8(self):\\n        txt = MIMEText(\'UTF-8 encoded body\', \'plain\', \'utf-8\')\\n        self.assertIn(\'Content-Transfer-Encoding: base64\', txt.as_string())\\n\\n    def test_7bit(self):\\n        txt = MIMEText(\'Body with only ASCII characters.\', \'plain\', \'utf-8\')\\n        self.assertIn(\'Content-Transfer-Encoding: base64\', txt.as_string())\\n\\n    def test_8bit_latin(self):\\n        txt = MIMEText(\'Body with latin characters: \xc3\xa0\xc3\xa1\xc3\xa4.\', \'plain\', \'utf-8\')\\n        self.assertIn(\'Content-Transfer-Encoding: base64\', txt.as_string())\\n\\n    def test_8bit_non_latin(self):\\n        txt = MIMEText(\'Body with non latin characters: \xd0\x90 \xd0\x91 \xd0\x92 \xd0\x93 \xd0\x94 \xd0\x95 \xd0\x96 \xd0\x85 \xd0\x97 \xd0\x98 \xd0\x86 \xd0\x9a \xd0\x9b \xd0\x9c \xd0\x9d \xd0\x9e \xd0\x9f.\', \'plain\', \'utf-8\')\\n        self.assertIn(\'Content-Transfer-Encoding: base64\', txt.as_string())\\n\\n\\nclass BaseEmailBackendTests(HeadersCheckMixin):\\n    email_backend = None\\n\\n    def setUp(self):\\n        self.settings_override = override_settings(EMAIL_BACKEND=self.email_backend)\\n        self.settings_override.enable()\\n\\n    def tearDown(self):\\n        self.settings_override.disable()\\n\\n    def assertStartsWith(self, first, second):\\n        if not first.startswith(second):\\n            self.longMessage = True\\n            self.assertEqual(first[:len(second)], second, \\"First string doesn\'t start with the second.\\")\\n\\n    def get_mailbox_content(self):\\n        raise NotImplementedError(\'subclasses of BaseEmailBackendTests must provide a get_mailbox_content() method\')\\n\\n    def flush_mailbox(self):\\n        raise NotImplementedError(\'subclasses of BaseEmailBackendTests may require a flush_mailbox() method\')\\n\\n    def get_the_message(self):\\n        mailbox = self.get_mailbox_content()\\n        self.assertEqual(\\n            len(mailbox), 1,\\n            \\"Expected exactly one message, got %d.\\\\n%r\\" % (len(mailbox), [m.as_string() for m in mailbox])\\n        )\\n        return mailbox[0]\\n\\n    def test_send(self):\\n        email = EmailMessage(\'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'])\\n        num_sent = mail.get_connection().send_messages([email])\\n        self.assertEqual(num_sent, 1)\\n        message = self.get_the_message()\\n        self.assertEqual(message[\\"subject\\"], \\"Subject\\")\\n        self.assertEqual(message.get_payload(), \\"Content\\")\\n        self.assertEqual(message[\\"from\\"], \\"from@example.com\\")\\n        self.assertEqual(message.get_all(\\"to\\"), [\\"to@example.com\\"])\\n\\n    def test_send_unicode(self):\\n        email = EmailMessage(\'Ch\xc3\xa8re maman\', \'Je t\\\\\'aime tr\xc3\xa8s fort\', \'from@example.com\', [\'to@example.com\'])\\n        num_sent = mail.get_connection().send_messages([email])\\n        self.assertEqual(num_sent, 1)\\n        message = self.get_the_message()\\n        self.assertEqual(message[\\"subject\\"], \'=?utf-8?q?Ch=C3=A8re_maman?=\')\\n        self.assertEqual(force_text(message.get_payload(decode=True)), \'Je t\\\\\'aime tr\xc3\xa8s fort\')\\n\\n    def test_send_long_lines(self):\\n        \\"\\"\\"\\n        Email line length is limited to 998 chars by the RFC:\\n        https://tools.ietf.org/html/rfc5322#section-2.1.1\\n        Message body containing longer lines are converted to Quoted-Printable\\n        to avoid having to insert newlines, which could be hairy to do properly.\\n        \\"\\"\\"\\n        # Unencoded body length is \\u003c 998 (840) but \\u003e 998 when utf-8 encoded.\\n        email = EmailMessage(\'Subject\', \'\xd0\x92 \xd1\x8e\xd0\xb6\xd0\xbd\xd1\x8b\xd1\x85 \xd0\xbc\xd0\xbe\xd1\x80\xd1\x8f\xd1\x85 \' * 60, \'from@example.com\', [\'to@example.com\'])\\n        email.send()\\n        message = self.get_the_message()\\n        self.assertMessageHasHeaders(message, {\\n            (\'MIME-Version\', \'1.0\'),\\n            (\'Content-Type\', \'text/plain; charset=\\"utf-8\\"\'),\\n            (\'Content-Transfer-Encoding\', \'quoted-printable\'),\\n        })\\n\\n    def test_send_many(self):\\n        email1 = EmailMessage(\'Subject\', \'Content1\', \'from@example.com\', [\'to@example.com\'])\\n        email2 = EmailMessage(\'Subject\', \'Content2\', \'from@example.com\', [\'to@example.com\'])\\n        # send_messages() may take a list or a generator.\\n        emails_lists = ([email1, email2], (email for email in [email1, email2]))\\n        for emails_list in emails_lists:\\n            num_sent = mail.get_connection().send_messages(emails_list)\\n            self.assertEqual(num_sent, 2)\\n            messages = self.get_mailbox_content()\\n            self.assertEqual(len(messages), 2)\\n            self.assertEqual(messages[0].get_payload(), \'Content1\')\\n            self.assertEqual(messages[1].get_payload(), \'Content2\')\\n            self.flush_mailbox()\\n\\n    def test_send_verbose_name(self):\\n        email = EmailMessage(\\"Subject\\", \\"Content\\", \'\\"Firstname S\xc3\xbcrname\\" \\u003cfrom@example.com\\u003e\',\\n                             [\\"to@example.com\\"])\\n        email.send()\\n        message = self.get_the_message()\\n        self.assertEqual(message[\\"subject\\"], \\"Subject\\")\\n        self.assertEqual(message.get_payload(), \\"Content\\")\\n        self.assertEqual(message[\\"from\\"], \\"=?utf-8?q?Firstname_S=C3=BCrname?= \\u003cfrom@example.com\\u003e\\")\\n\\n    def test_plaintext_send_mail(self):\\n        \\"\\"\\"\\n        Test send_mail without the html_message\\n        regression test for adding html_message parameter to send_mail()\\n        \\"\\"\\"\\n        send_mail(\'Subject\', \'Content\', \'sender@example.com\', [\'nobody@example.com\'])\\n        message = self.get_the_message()\\n\\n        self.assertEqual(message.get(\'subject\'), \'Subject\')\\n        self.assertEqual(message.get_all(\'to\'), [\'nobody@example.com\'])\\n        self.assertFalse(message.is_multipart())\\n        self.assertEqual(message.get_payload(), \'Content\')\\n        self.assertEqual(message.get_content_type(), \'text/plain\')\\n\\n    def test_html_send_mail(self):\\n        \\"\\"\\"Test html_message argument to send_mail\\"\\"\\"\\n        send_mail(\'Subject\', \'Content\', \'sender@example.com\', [\'nobody@example.com\'], html_message=\'HTML Content\')\\n        message = self.get_the_message()\\n\\n        self.assertEqual(message.get(\'subject\'), \'Subject\')\\n        self.assertEqual(message.get_all(\'to\'), [\'nobody@example.com\'])\\n        self.assertTrue(message.is_multipart())\\n        self.assertEqual(len(message.get_payload()), 2)\\n        self.assertEqual(message.get_payload(0).get_payload(), \'Content\')\\n        self.assertEqual(message.get_payload(0).get_content_type(), \'text/plain\')\\n        self.assertEqual(message.get_payload(1).get_payload(), \'HTML Content\')\\n        self.assertEqual(message.get_payload(1).get_content_type(), \'text/html\')\\n\\n    @override_settings(MANAGERS=[(\'nobody\', \'nobody@example.com\')])\\n    def test_html_mail_managers(self):\\n        \\"\\"\\"Test html_message argument to mail_managers\\"\\"\\"\\n        mail_managers(\'Subject\', \'Content\', html_message=\'HTML Content\')\\n        message = self.get_the_message()\\n\\n        self.assertEqual(message.get(\'subject\'), \'[Django] Subject\')\\n        self.assertEqual(message.get_all(\'to\'), [\'nobody@example.com\'])\\n        self.assertTrue(message.is_multipart())\\n        self.assertEqual(len(message.get_payload()), 2)\\n        self.assertEqual(message.get_payload(0).get_payload(), \'Content\')\\n        self.assertEqual(message.get_payload(0).get_content_type(), \'text/plain\')\\n        self.assertEqual(message.get_payload(1).get_payload(), \'HTML Content\')\\n        self.assertEqual(message.get_payload(1).get_content_type(), \'text/html\')\\n\\n    @override_settings(ADMINS=[(\'nobody\', \'nobody@example.com\')])\\n    def test_html_mail_admins(self):\\n        \\"\\"\\"Test html_message argument to mail_admins \\"\\"\\"\\n        mail_admins(\'Subject\', \'Content\', html_message=\'HTML Content\')\\n        message = self.get_the_message()\\n\\n        self.assertEqual(message.get(\'subject\'), \'[Django] Subject\')\\n        self.assertEqual(message.get_all(\'to\'), [\'nobody@example.com\'])\\n        self.assertTrue(message.is_multipart())\\n        self.assertEqual(len(message.get_payload()), 2)\\n        self.assertEqual(message.get_payload(0).get_payload(), \'Content\')\\n        self.assertEqual(message.get_payload(0).get_content_type(), \'text/plain\')\\n        self.assertEqual(message.get_payload(1).get_payload(), \'HTML Content\')\\n        self.assertEqual(message.get_payload(1).get_content_type(), \'text/html\')\\n\\n    @override_settings(\\n        ADMINS=[(\'nobody\', \'nobody+admin@example.com\')],\\n        MANAGERS=[(\'nobody\', \'nobody+manager@example.com\')])\\n    def test_manager_and_admin_mail_prefix(self):\\n        \\"\\"\\"\\n        String prefix + lazy translated subject = bad output\\n        Regression for #13494\\n        \\"\\"\\"\\n        mail_managers(gettext_lazy(\'Subject\'), \'Content\')\\n        message = self.get_the_message()\\n        self.assertEqual(message.get(\'subject\'), \'[Django] Subject\')\\n\\n        self.flush_mailbox()\\n        mail_admins(gettext_lazy(\'Subject\'), \'Content\')\\n        message = self.get_the_message()\\n        self.assertEqual(message.get(\'subject\'), \'[Django] Subject\')\\n\\n    @override_settings(ADMINS=[], MANAGERS=[])\\n    def test_empty_admins(self):\\n        \\"\\"\\"\\n        mail_admins/mail_managers doesn\'t connect to the mail server\\n        if there are no recipients (#9383)\\n        \\"\\"\\"\\n        mail_admins(\'hi\', \'there\')\\n        self.assertEqual(self.get_mailbox_content(), [])\\n        mail_managers(\'hi\', \'there\')\\n        self.assertEqual(self.get_mailbox_content(), [])\\n\\n    def test_message_cc_header(self):\\n        \\"\\"\\"\\n        Regression test for #7722\\n        \\"\\"\\"\\n        email = EmailMessage(\'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'], cc=[\'cc@example.com\'])\\n        mail.get_connection().send_messages([email])\\n        message = self.get_the_message()\\n        self.assertMessageHasHeaders(message, {\\n            (\'MIME-Version\', \'1.0\'),\\n            (\'Content-Type\', \'text/plain; charset=\\"utf-8\\"\'),\\n            (\'Content-Transfer-Encoding\', \'7bit\'),\\n            (\'Subject\', \'Subject\'),\\n            (\'From\', \'from@example.com\'),\\n            (\'To\', \'to@example.com\'),\\n            (\'Cc\', \'cc@example.com\')})\\n        self.assertIn(\'\\\\nDate: \', message.as_string())\\n\\n    def test_idn_send(self):\\n        \\"\\"\\"\\n        Regression test for #14301\\n        \\"\\"\\"\\n        self.assertTrue(send_mail(\'Subject\', \'Content\', \'from@\xc3\xb6\xc3\xa4\xc3\xbc.com\', [\'to@\xc3\xb6\xc3\xa4\xc3\xbc.com\']))\\n        message = self.get_the_message()\\n        self.assertEqual(message.get(\'subject\'), \'Subject\')\\n        self.assertEqual(message.get(\'from\'), \'from@xn--4ca9at.com\')\\n        self.assertEqual(message.get(\'to\'), \'to@xn--4ca9at.com\')\\n\\n        self.flush_mailbox()\\n        m = EmailMessage(\'Subject\', \'Content\', \'from@\xc3\xb6\xc3\xa4\xc3\xbc.com\', [\'to@\xc3\xb6\xc3\xa4\xc3\xbc.com\'], cc=[\'cc@\xc3\xb6\xc3\xa4\xc3\xbc.com\'])\\n        m.send()\\n        message = self.get_the_message()\\n        self.assertEqual(message.get(\'subject\'), \'Subject\')\\n        self.assertEqual(message.get(\'from\'), \'from@xn--4ca9at.com\')\\n        self.assertEqual(message.get(\'to\'), \'to@xn--4ca9at.com\')\\n        self.assertEqual(message.get(\'cc\'), \'cc@xn--4ca9at.com\')\\n\\n    def test_recipient_without_domain(self):\\n        \\"\\"\\"\\n        Regression test for #15042\\n        \\"\\"\\"\\n        self.assertTrue(send_mail(\\"Subject\\", \\"Content\\", \\"tester\\", [\\"django\\"]))\\n        message = self.get_the_message()\\n        self.assertEqual(message.get(\'subject\'), \'Subject\')\\n        self.assertEqual(message.get(\'from\'), \\"tester\\")\\n        self.assertEqual(message.get(\'to\'), \\"django\\")\\n\\n    def test_lazy_addresses(self):\\n        \\"\\"\\"\\n        Email sending should support lazy email addresses (#24416).\\n        \\"\\"\\"\\n        _ = gettext_lazy\\n        self.assertTrue(send_mail(\'Subject\', \'Content\', _(\'tester\'), [_(\'django\')]))\\n        message = self.get_the_message()\\n        self.assertEqual(message.get(\'from\'), \'tester\')\\n        self.assertEqual(message.get(\'to\'), \'django\')\\n\\n        self.flush_mailbox()\\n        m = EmailMessage(\\n            \'Subject\', \'Content\', _(\'tester\'), [_(\'to1\'), _(\'to2\')],\\n            cc=[_(\'cc1\'), _(\'cc2\')],\\n            bcc=[_(\'bcc\')],\\n            reply_to=[_(\'reply\')],\\n        )\\n        self.assertEqual(m.recipients(), [\'to1\', \'to2\', \'cc1\', \'cc2\', \'bcc\'])\\n        m.send()\\n        message = self.get_the_message()\\n        self.assertEqual(message.get(\'from\'), \'tester\')\\n        self.assertEqual(message.get(\'to\'), \'to1, to2\')\\n        self.assertEqual(message.get(\'cc\'), \'cc1, cc2\')\\n        self.assertEqual(message.get(\'Reply-To\'), \'reply\')\\n\\n    def test_close_connection(self):\\n        \\"\\"\\"\\n        Connection can be closed (even when not explicitly opened)\\n        \\"\\"\\"\\n        conn = mail.get_connection(username=\'\', password=\'\')\\n        conn.close()\\n\\n    def test_use_as_contextmanager(self):\\n        \\"\\"\\"\\n        The connection can be used as a contextmanager.\\n        \\"\\"\\"\\n        opened = [False]\\n        closed = [False]\\n        conn = mail.get_connection(username=\'\', password=\'\')\\n\\n        def open():\\n            opened[0] = True\\n        conn.open = open\\n\\n        def close():\\n            closed[0] = True\\n        conn.close = close\\n        with conn as same_conn:\\n            self.assertTrue(opened[0])\\n            self.assertIs(same_conn, conn)\\n            self.assertFalse(closed[0])\\n        self.assertTrue(closed[0])\\n\\n\\nclass LocmemBackendTests(BaseEmailBackendTests, SimpleTestCase):\\n    email_backend = \'django.core.mail.backends.locmem.EmailBackend\'\\n\\n    def get_mailbox_content(self):\\n        return [m.message() for m in mail.outbox]\\n\\n    def flush_mailbox(self):\\n        mail.outbox = []\\n\\n    def tearDown(self):\\n        super().tearDown()\\n        mail.outbox = []\\n\\n    def test_locmem_shared_messages(self):\\n        \\"\\"\\"\\n        Make sure that the locmen backend populates the outbox.\\n        \\"\\"\\"\\n        connection = locmem.EmailBackend()\\n        connection2 = locmem.EmailBackend()\\n        email = EmailMessage(\\n            \'Subject\', \'Content\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        connection.send_messages([email])\\n        connection2.send_messages([email])\\n        self.assertEqual(len(mail.outbox), 2)\\n\\n    def test_validate_multiline_headers(self):\\n        # Ticket #18861 - Validate emails when using the locmem backend\\n        with self.assertRaises(BadHeaderError):\\n            send_mail(\'Subject\\\\nMultiline\', \'Content\', \'from@example.com\', [\'to@example.com\'])\\n\\n\\nclass FileBackendTests(BaseEmailBackendTests, SimpleTestCase):\\n    email_backend = \'django.core.mail.backends.filebased.EmailBackend\'\\n\\n    def setUp(self):\\n        super().setUp()\\n        self.tmp_dir = tempfile.mkdtemp()\\n        self.addCleanup(shutil.rmtree, self.tmp_dir)\\n        self._settings_override = override_settings(EMAIL_FILE_PATH=self.tmp_dir)\\n        self._settings_override.enable()\\n\\n    def tearDown(self):\\n        self._settings_override.disable()\\n        super().tearDown()\\n\\n    def flush_mailbox(self):\\n        for filename in os.listdir(self.tmp_dir):\\n            os.unlink(os.path.join(self.tmp_dir, filename))\\n\\n    def get_mailbox_content(self):\\n        messages = []\\n        for filename in os.listdir(self.tmp_dir):\\n            with open(os.path.join(self.tmp_dir, filename), \'rb\') as fp:\\n                session = fp.read().split(force_bytes(\'\\\\n\' + (\'-\' * 79) + \'\\\\n\', encoding=\'ascii\'))\\n            messages.extend(message_from_bytes(m) for m in session if m)\\n        return messages\\n\\n    def test_file_sessions(self):\\n        \\"\\"\\"Make sure opening a connection creates a new file\\"\\"\\"\\n        msg = EmailMessage(\\n            \'Subject\', \'Content\', \'bounce@example.com\', [\'to@example.com\'],\\n            headers={\'From\': \'from@example.com\'},\\n        )\\n        connection = mail.get_connection()\\n        connection.send_messages([msg])\\n\\n        self.assertEqual(len(os.listdir(self.tmp_dir)), 1)\\n        with open(os.path.join(self.tmp_dir, os.listdir(self.tmp_dir)[0]), \'rb\') as fp:\\n            message = message_from_binary_file(fp)\\n        self.assertEqual(message.get_content_type(), \'text/plain\')\\n        self.assertEqual(message.get(\'subject\'), \'Subject\')\\n        self.assertEqual(message.get(\'from\'), \'from@example.com\')\\n        self.assertEqual(message.get(\'to\'), \'to@example.com\')\\n\\n        connection2 = mail.get_connection()\\n        connection2.send_messages([msg])\\n        self.assertEqual(len(os.listdir(self.tmp_dir)), 2)\\n\\n        connection.send_messages([msg])\\n        self.assertEqual(len(os.listdir(self.tmp_dir)), 2)\\n\\n        msg.connection = mail.get_connection()\\n        self.assertTrue(connection.open())\\n        msg.send()\\n        self.assertEqual(len(os.listdir(self.tmp_dir)), 3)\\n        msg.send()\\n        self.assertEqual(len(os.listdir(self.tmp_dir)), 3)\\n\\n        connection.close()\\n\\n\\nclass ConsoleBackendTests(BaseEmailBackendTests, SimpleTestCase):\\n    email_backend = \'django.core.mail.backends.console.EmailBackend\'\\n\\n    def setUp(self):\\n        super().setUp()\\n        self.__stdout = sys.stdout\\n        self.stream = sys.stdout = StringIO()\\n\\n    def tearDown(self):\\n        del self.stream\\n        sys.stdout = self.__stdout\\n        del self.__stdout\\n        super().tearDown()\\n\\n    def flush_mailbox(self):\\n        self.stream = sys.stdout = StringIO()\\n\\n    def get_mailbox_content(self):\\n        messages = self.stream.getvalue().split(\'\\\\n\' + (\'-\' * 79) + \'\\\\n\')\\n        return [message_from_bytes(force_bytes(m)) for m in messages if m]\\n\\n    def test_console_stream_kwarg(self):\\n        \\"\\"\\"\\n        The console backend can be pointed at an arbitrary stream.\\n        \\"\\"\\"\\n        s = StringIO()\\n        connection = mail.get_connection(\'django.core.mail.backends.console.EmailBackend\', stream=s)\\n        send_mail(\'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'], connection=connection)\\n        message = force_bytes(s.getvalue().split(\'\\\\n\' + (\'-\' * 79) + \'\\\\n\')[0])\\n        self.assertMessageHasHeaders(message, {\\n            (\'MIME-Version\', \'1.0\'),\\n            (\'Content-Type\', \'text/plain; charset=\\"utf-8\\"\'),\\n            (\'Content-Transfer-Encoding\', \'7bit\'),\\n            (\'Subject\', \'Subject\'),\\n            (\'From\', \'from@example.com\'),\\n            (\'To\', \'to@example.com\')})\\n        self.assertIn(b\'\\\\nDate: \', message)\\n\\n\\nclass FakeSMTPChannel(smtpd.SMTPChannel):\\n\\n    def collect_incoming_data(self, data):\\n        try:\\n            smtpd.SMTPChannel.collect_incoming_data(self, data)\\n        except UnicodeDecodeError:\\n            # ignore decode error in SSL/TLS connection tests as we only care\\n            # whether the connection attempt was made\\n            pass\\n\\n    def smtp_AUTH(self, arg):\\n        if arg == \'CRAM-MD5\':\\n            # This is only the first part of the login process. But it\'s enough\\n            # for our tests.\\n            challenge = base64.b64encode(b\'somerandomstring13579\')\\n            self.push(\'334 %s\' % challenge.decode())\\n        else:\\n            self.push(\'502 Error: login \\"%s\\" not implemented\' % arg)\\n\\n\\nclass FakeSMTPServer(smtpd.SMTPServer, threading.Thread):\\n    \\"\\"\\"\\n    Asyncore SMTP server wrapped into a thread. Based on DummyFTPServer from:\\n    http://svn.python.org/view/python/branches/py3k/Lib/test/test_ftplib.py?revision=86061\\u0026view=markup\\n    \\"\\"\\"\\n    channel_class = FakeSMTPChannel\\n\\n    def __init__(self, *args, **kwargs):\\n        threading.Thread.__init__(self)\\n        # New kwarg added in Python 3.5; default switching to False in 3.6.\\n        if sys.version_info \\u003e= (3, 5):\\n            kwargs[\'decode_data\'] = True\\n        smtpd.SMTPServer.__init__(self, *args, **kwargs)\\n        self._sink = []\\n        self.active = False\\n        self.active_lock = threading.Lock()\\n        self.sink_lock = threading.Lock()\\n\\n    def process_message(self, peer, mailfrom, rcpttos, data):\\n        data = data.encode()\\n        m = message_from_bytes(data)\\n        maddr = parseaddr(m.get(\'from\'))[1]\\n\\n        if mailfrom != maddr:\\n            # According to the spec, mailfrom does not necessarily match the\\n            # From header - this is the case where the local part isn\'t\\n            # encoded, so try to correct that.\\n            lp, domain = mailfrom.split(\'@\', 1)\\n            lp = Header(lp, \'utf-8\').encode()\\n            mailfrom = \'@\'.join([lp, domain])\\n\\n        if mailfrom != maddr:\\n            return \\"553 \'%s\' != \'%s\'\\" % (mailfrom, maddr)\\n        with self.sink_lock:\\n            self._sink.append(m)\\n\\n    def get_sink(self):\\n        with self.sink_lock:\\n            return self._sink[:]\\n\\n    def flush_sink(self):\\n        with self.sink_lock:\\n            self._sink[:] = []\\n\\n    def start(self):\\n        assert not self.active\\n        self.__flag = threading.Event()\\n        threading.Thread.start(self)\\n        self.__flag.wait()\\n\\n    def run(self):\\n        self.active = True\\n        self.__flag.set()\\n        while self.active and asyncore.socket_map:\\n            with self.active_lock:\\n                asyncore.loop(timeout=0.1, count=1)\\n        asyncore.close_all()\\n\\n    def stop(self):\\n        if self.active:\\n            self.active = False\\n            self.join()\\n\\n\\nclass FakeAUTHSMTPConnection(SMTP):\\n    \\"\\"\\"\\n    A SMTP connection pretending support for the AUTH command. It does not, but\\n    at least this can allow testing the first part of the AUTH process.\\n    \\"\\"\\"\\n\\n    def ehlo(self, name=\'\'):\\n        response = SMTP.ehlo(self, name=name)\\n        self.esmtp_features.update({\\n            \'auth\': \'CRAM-MD5 PLAIN LOGIN\',\\n        })\\n        return response\\n\\n\\nclass SMTPBackendTestsBase(SimpleTestCase):\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        super().setUpClass()\\n        cls.server = FakeSMTPServer((\'127.0.0.1\', 0), None)\\n        cls._settings_override = override_settings(\\n            EMAIL_HOST=\\"127.0.0.1\\",\\n            EMAIL_PORT=cls.server.socket.getsockname()[1])\\n        cls._settings_override.enable()\\n        cls.server.start()\\n\\n    @classmethod\\n    def tearDownClass(cls):\\n        cls._settings_override.disable()\\n        cls.server.stop()\\n        super().tearDownClass()\\n\\n\\nclass SMTPBackendTests(BaseEmailBackendTests, SMTPBackendTestsBase):\\n    email_backend = \'django.core.mail.backends.smtp.EmailBackend\'\\n\\n    def setUp(self):\\n        super().setUp()\\n        self.server.flush_sink()\\n\\n    def tearDown(self):\\n        self.server.flush_sink()\\n        super().tearDown()\\n\\n    def flush_mailbox(self):\\n        self.server.flush_sink()\\n\\n    def get_mailbox_content(self):\\n        return self.server.get_sink()\\n\\n    @override_settings(\\n        EMAIL_HOST_USER=\\"not empty username\\",\\n        EMAIL_HOST_PASSWORD=\\"not empty password\\")\\n    def test_email_authentication_use_settings(self):\\n        backend = smtp.EmailBackend()\\n        self.assertEqual(backend.username, \'not empty username\')\\n        self.assertEqual(backend.password, \'not empty password\')\\n\\n    @override_settings(\\n        EMAIL_HOST_USER=\\"not empty username\\",\\n        EMAIL_HOST_PASSWORD=\\"not empty password\\")\\n    def test_email_authentication_override_settings(self):\\n        backend = smtp.EmailBackend(username=\'username\', password=\'password\')\\n        self.assertEqual(backend.username, \'username\')\\n        self.assertEqual(backend.password, \'password\')\\n\\n    @override_settings(\\n        EMAIL_HOST_USER=\\"not empty username\\",\\n        EMAIL_HOST_PASSWORD=\\"not empty password\\")\\n    def test_email_disabled_authentication(self):\\n        backend = smtp.EmailBackend(username=\'\', password=\'\')\\n        self.assertEqual(backend.username, \'\')\\n        self.assertEqual(backend.password, \'\')\\n\\n    def test_auth_attempted(self):\\n        \\"\\"\\"\\n        Opening the backend with non empty username/password tries\\n        to authenticate against the SMTP server.\\n        \\"\\"\\"\\n        backend = smtp.EmailBackend(\\n            username=\'not empty username\', password=\'not empty password\')\\n        with self.assertRaisesMessage(SMTPException, \'SMTP AUTH extension not supported by server.\'):\\n            with backend:\\n                pass\\n\\n    def test_server_open(self):\\n        \\"\\"\\"\\n        open() returns whether it opened a connection.\\n        \\"\\"\\"\\n        backend = smtp.EmailBackend(username=\'\', password=\'\')\\n        self.assertFalse(backend.connection)\\n        opened = backend.open()\\n        backend.close()\\n        self.assertTrue(opened)\\n\\n    def test_server_login(self):\\n        \\"\\"\\"\\n        Even if the Python SMTP server doesn\'t support authentication, the\\n        login process starts and the appropriate exception is raised.\\n        \\"\\"\\"\\n        class CustomEmailBackend(smtp.EmailBackend):\\n            connection_class = FakeAUTHSMTPConnection\\n\\n        backend = CustomEmailBackend(username=\'username\', password=\'password\')\\n        with self.assertRaises(SMTPAuthenticationError):\\n            with backend:\\n                pass\\n\\n    @override_settings(EMAIL_USE_TLS=True)\\n    def test_email_tls_use_settings(self):\\n        backend = smtp.EmailBackend()\\n        self.assertTrue(backend.use_tls)\\n\\n    @override_settings(EMAIL_USE_TLS=True)\\n    def test_email_tls_override_settings(self):\\n        backend = smtp.EmailBackend(use_tls=False)\\n        self.assertFalse(backend.use_tls)\\n\\n    def test_email_tls_default_disabled(self):\\n        backend = smtp.EmailBackend()\\n        self.assertFalse(backend.use_tls)\\n\\n    @override_settings(EMAIL_USE_SSL=True)\\n    def test_email_ssl_use_settings(self):\\n        backend = smtp.EmailBackend()\\n        self.assertTrue(backend.use_ssl)\\n\\n    @override_settings(EMAIL_USE_SSL=True)\\n    def test_email_ssl_override_settings(self):\\n        backend = smtp.EmailBackend(use_ssl=False)\\n        self.assertFalse(backend.use_ssl)\\n\\n    def test_email_ssl_default_disabled(self):\\n        backend = smtp.EmailBackend()\\n        self.assertFalse(backend.use_ssl)\\n\\n    @override_settings(EMAIL_SSL_CERTFILE=\'foo\')\\n    def test_email_ssl_certfile_use_settings(self):\\n        backend = smtp.EmailBackend()\\n        self.assertEqual(backend.ssl_certfile, \'foo\')\\n\\n    @override_settings(EMAIL_SSL_CERTFILE=\'foo\')\\n    def test_email_ssl_certfile_override_settings(self):\\n        backend = smtp.EmailBackend(ssl_certfile=\'bar\')\\n        self.assertEqual(backend.ssl_certfile, \'bar\')\\n\\n    def test_email_ssl_certfile_default_disabled(self):\\n        backend = smtp.EmailBackend()\\n        self.assertIsNone(backend.ssl_certfile)\\n\\n    @override_settings(EMAIL_SSL_KEYFILE=\'foo\')\\n    def test_email_ssl_keyfile_use_settings(self):\\n        backend = smtp.EmailBackend()\\n        self.assertEqual(backend.ssl_keyfile, \'foo\')\\n\\n    @override_settings(EMAIL_SSL_KEYFILE=\'foo\')\\n    def test_email_ssl_keyfile_override_settings(self):\\n        backend = smtp.EmailBackend(ssl_keyfile=\'bar\')\\n        self.assertEqual(backend.ssl_keyfile, \'bar\')\\n\\n    def test_email_ssl_keyfile_default_disabled(self):\\n        backend = smtp.EmailBackend()\\n        self.assertIsNone(backend.ssl_keyfile)\\n\\n    @override_settings(EMAIL_USE_TLS=True)\\n    def test_email_tls_attempts_starttls(self):\\n        backend = smtp.EmailBackend()\\n        self.assertTrue(backend.use_tls)\\n        with self.assertRaisesMessage(SMTPException, \'STARTTLS extension not supported by server.\'):\\n            with backend:\\n                pass\\n\\n    @override_settings(EMAIL_USE_SSL=True)\\n    def test_email_ssl_attempts_ssl_connection(self):\\n        backend = smtp.EmailBackend()\\n        self.assertTrue(backend.use_ssl)\\n        with self.assertRaises(SSLError):\\n            with backend:\\n                pass\\n\\n    def test_connection_timeout_default(self):\\n        \\"\\"\\"The connection\'s timeout value is None by default.\\"\\"\\"\\n        connection = mail.get_connection(\'django.core.mail.backends.smtp.EmailBackend\')\\n        self.assertIsNone(connection.timeout)\\n\\n    def test_connection_timeout_custom(self):\\n        \\"\\"\\"The timeout parameter can be customized.\\"\\"\\"\\n        class MyEmailBackend(smtp.EmailBackend):\\n            def __init__(self, *args, **kwargs):\\n                kwargs.setdefault(\'timeout\', 42)\\n                super().__init__(*args, **kwargs)\\n\\n        myemailbackend = MyEmailBackend()\\n        myemailbackend.open()\\n        self.assertEqual(myemailbackend.timeout, 42)\\n        self.assertEqual(myemailbackend.connection.timeout, 42)\\n        myemailbackend.close()\\n\\n    @override_settings(EMAIL_TIMEOUT=10)\\n    def test_email_timeout_override_settings(self):\\n        backend = smtp.EmailBackend()\\n        self.assertEqual(backend.timeout, 10)\\n\\n    def test_email_msg_uses_crlf(self):\\n        \\"\\"\\"#23063 -- RFC-compliant messages are sent over SMTP.\\"\\"\\"\\n        send = SMTP.send\\n        try:\\n            smtp_messages = []\\n\\n            def mock_send(self, s):\\n                smtp_messages.append(s)\\n                return send(self, s)\\n\\n            SMTP.send = mock_send\\n\\n            email = EmailMessage(\'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'])\\n            mail.get_connection().send_messages([email])\\n\\n            # Find the actual message\\n            msg = None\\n            for i, m in enumerate(smtp_messages):\\n                if m[:4] == \'data\':\\n                    msg = smtp_messages[i + 1]\\n                    break\\n\\n            self.assertTrue(msg)\\n\\n            msg = msg.decode()\\n            # The message only contains CRLF and not combinations of CRLF, LF, and CR.\\n            msg = msg.replace(\'\\\\r\\\\n\', \'\')\\n            self.assertNotIn(\'\\\\r\', msg)\\n            self.assertNotIn(\'\\\\n\', msg)\\n\\n        finally:\\n            SMTP.send = send\\n\\n    def test_send_messages_after_open_failed(self):\\n        \\"\\"\\"\\n        send_messages() shouldn\'t try to send messages if open() raises an\\n        exception after initializing the connection.\\n        \\"\\"\\"\\n        backend = smtp.EmailBackend()\\n        # Simulate connection initialization success and a subsequent\\n        # connection exception.\\n        backend.connection = True\\n        backend.open = lambda: None\\n        email = EmailMessage(\'Subject\', \'Content\', \'from@example.com\', [\'to@example.com\'])\\n        self.assertEqual(backend.send_messages([email]), None)\\n\\n\\nclass SMTPBackendStoppedServerTests(SMTPBackendTestsBase):\\n    \\"\\"\\"\\n    These tests require a separate class, because the FakeSMTPServer is shut\\n    down in setUpClass(), and it cannot be restarted (\\"RuntimeError: threads\\n    can only be started once\\").\\n    \\"\\"\\"\\n    @classmethod\\n    def setUpClass(cls):\\n        super().setUpClass()\\n        cls.backend = smtp.EmailBackend(username=\'\', password=\'\')\\n        cls.server.stop()\\n\\n    def test_server_stopped(self):\\n        \\"\\"\\"\\n        Closing the backend while the SMTP server is stopped doesn\'t raise an\\n        exception.\\n        \\"\\"\\"\\n        self.backend.close()\\n\\n    def test_fail_silently_on_connection_error(self):\\n        \\"\\"\\"\\n        A socket connection error is silenced with fail_silently=True.\\n        \\"\\"\\"\\n        with self.assertRaises(socket.error):\\n            self.backend.open()\\n        self.backend.fail_silently = True\\n        self.backend.open()\\n"}\n'
line: b'{"repo_name":"w1ll1am23/home-assistant","ref":"refs/heads/dev","path":"homeassistant/components/homematicip_cloud/weather.py","content":"\\n\\"\\"\\"Support for HomematicIP Cloud weather devices.\\"\\"\\"\\nimport logging\\n\\nfrom homematicip.aio.device import (\\n    AsyncWeatherSensor, AsyncWeatherSensorPlus, AsyncWeatherSensorPro)\\nfrom homematicip.aio.home import AsyncHome\\n\\nfrom homeassistant.components.weather import WeatherEntity\\nfrom homeassistant.config_entries import ConfigEntry\\nfrom homeassistant.const import TEMP_CELSIUS\\nfrom homeassistant.core import HomeAssistant\\n\\nfrom . import DOMAIN as HMIPC_DOMAIN, HMIPC_HAPID, HomematicipGenericDevice\\n\\n_LOGGER = logging.getLogger(__name__)\\n\\n\\nasync def async_setup_platform(\\n        hass, config, async_add_entities, discovery_info=None):\\n    \\"\\"\\"Set up the HomematicIP Cloud weather sensor.\\"\\"\\"\\n    pass\\n\\n\\nasync def async_setup_entry(hass: HomeAssistant, config_entry: ConfigEntry,\\n                            async_add_entities) -\\u003e None:\\n    \\"\\"\\"Set up the HomematicIP weather sensor from a config entry.\\"\\"\\"\\n    home = hass.data[HMIPC_DOMAIN][config_entry.data[HMIPC_HAPID]].home\\n    devices = []\\n    for device in home.devices:\\n        if isinstance(device, AsyncWeatherSensorPro):\\n            devices.append(HomematicipWeatherSensorPro(home, device))\\n        elif isinstance(device, (AsyncWeatherSensor, AsyncWeatherSensorPlus)):\\n            devices.append(HomematicipWeatherSensor(home, device))\\n\\n    if devices:\\n        async_add_entities(devices)\\n\\n\\nclass HomematicipWeatherSensor(HomematicipGenericDevice, WeatherEntity):\\n    \\"\\"\\"representation of a HomematicIP Cloud weather sensor plus \\u0026 basic.\\"\\"\\"\\n\\n    def __init__(self, home: AsyncHome, device) -\\u003e None:\\n        \\"\\"\\"Initialize the weather sensor.\\"\\"\\"\\n        super().__init__(home, device)\\n\\n    @property\\n    def name(self) -\\u003e str:\\n        \\"\\"\\"Return the name of the sensor.\\"\\"\\"\\n        return self._device.label\\n\\n    @property\\n    def temperature(self) -\\u003e float:\\n        \\"\\"\\"Return the platform temperature.\\"\\"\\"\\n        return self._device.actualTemperature\\n\\n    @property\\n    def temperature_unit(self) -\\u003e str:\\n        \\"\\"\\"Return the unit of measurement.\\"\\"\\"\\n        return TEMP_CELSIUS\\n\\n    @property\\n    def humidity(self) -\\u003e int:\\n        \\"\\"\\"Return the humidity.\\"\\"\\"\\n        return self._device.humidity\\n\\n    @property\\n    def wind_speed(self) -\\u003e float:\\n        \\"\\"\\"Return the wind speed.\\"\\"\\"\\n        return self._device.windSpeed\\n\\n    @property\\n    def attribution(self) -\\u003e str:\\n        \\"\\"\\"Return the attribution.\\"\\"\\"\\n        return \\"Powered by Homematic IP\\"\\n\\n    @property\\n    def condition(self) -\\u003e str:\\n        \\"\\"\\"Return the current condition.\\"\\"\\"\\n        if hasattr(self._device, \\"raining\\") and self._device.raining:\\n            return \'rainy\'\\n        if self._device.storm:\\n            return \'windy\'\\n        if self._device.sunshine:\\n            return \'sunny\'\\n        return \'\'\\n\\n\\nclass HomematicipWeatherSensorPro(HomematicipWeatherSensor):\\n    \\"\\"\\"representation of a HomematicIP weather sensor pro.\\"\\"\\"\\n\\n    @property\\n    def wind_bearing(self) -\\u003e float:\\n        \\"\\"\\"Return the wind bearing.\\"\\"\\"\\n        return self._device.windDirection\\n"}\n'
line: b'{"repo_name":"andybab/Impala","ref":"refs/heads/master","path":"tests/util/hdfs_util.py","content":"#!/usr/bin/env python\\n# Copyright (c) 2012 Cloudera, Inc. All rights reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n# http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n# Hdfs access utilities\\n\\nfrom xml.etree.ElementTree import parse\\nfrom pywebhdfs.webhdfs import PyWebHdfsClient, errors, _raise_pywebhdfs_exception\\nimport getpass\\nimport types\\nimport requests, httplib\\n\\nclass PyWebHdfsClientWithChmod(PyWebHdfsClient):\\n  def chmod(self, path, permission):\\n    \\"\\"\\"Set the permission of \'path\' to \'permission\' (specified as an octal string, e.g.\\n    \'775\'\\"\\"\\"\\n    uri = self._create_uri(path, \\"SETPERMISSION\\", permission=permission)\\n    response = requests.put(uri, allow_redirects=True)\\n    if not response.status_code == httplib.OK:\\n      _raise_pywebhdfs_exception(response.status_code, response.text)\\n\\n    return True\\n\\nclass HdfsConfig(object):\\n  \\"\\"\\"Reads an XML configuration file (produced by a mini-cluster) into a dictionary\\n  accessible via get()\\"\\"\\"\\n  def __init__(self, filename):\\n    self.conf = {}\\n    tree = parse(filename)\\n    for property in tree.getroot().getiterator(\'property\'):\\n      self.conf[property.find(\'name\').text] = property.find(\'value\').text\\n\\n  def get(self, key):\\n    return self.conf.get(key)\\n\\ndef get_hdfs_client_from_conf(conf):\\n  \\"\\"\\"Returns a new HTTP client for an HDFS cluster using an HdfsConfig object\\"\\"\\"\\n  hostport = conf.get(\'dfs.namenode.http-address\')\\n  if hostport is None:\\n    raise Exception(\\"dfs.namenode.http-address not found in config\\")\\n  host, port = hostport.split(\\":\\")\\n  return get_hdfs_client(host=host, port=port)\\n\\ndef __pyweb_hdfs_client_exists(self, path):\\n  \\"\\"\\"The PyWebHdfsClient doesn\'t provide an API to cleanly detect if a file or directory\\n  exists. This method is bound to each client that is created so tests can simply call\\n  hdfs_client.exists(\'path\') and get back a bool.\\n  \\"\\"\\"\\n  try:\\n    self.get_file_dir_status(path)\\n  except errors.FileNotFound:\\n    return False\\n  return True\\n\\ndef get_hdfs_client(host, port, user_name=getpass.getuser()):\\n  \\"\\"\\"Returns a new HTTP client for an HDFS cluster using an explict host:port pair\\"\\"\\"\\n  hdfs_client = PyWebHdfsClientWithChmod(host=host, port=port, user_name=user_name)\\n  # Bind our \\"exists\\" method to hdfs_client.exists\\n  hdfs_client.exists = types.MethodType(__pyweb_hdfs_client_exists, hdfs_client)\\n  return hdfs_client\\n"}\n'
line: b'{"repo_name":"doublebits/osf.io","ref":"refs/heads/develop","path":"admin_tests/factories.py","content":"import factory\\n\\nfrom admin.common_auth.models import MyUser\\n\\n\\nclass UserFactory(factory.Factory):\\n    class Meta:\\n        model = MyUser\\n\\n    id = 123\\n    email = \'cello@email.org\'\\n    first_name = \'Yo-yo\'\\n    last_name = \'Ma\'\\n    osf_id = \'abc12\'\\n\\n    @classmethod\\n    def is_in_group(cls, value):\\n        return True\\n"}\n'
line: b'{"repo_name":"sysadmind/ansible-modules-extras","ref":"refs/heads/devel","path":"monitoring/pagerduty.py","content":"#!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n#\\n# This file is part of Ansible\\n#\\n# Ansible is free software: you can redistribute it and/or modify\\n# it under the terms of the GNU General Public License as published by\\n# the Free Software Foundation, either version 3 of the License, or\\n# (at your option) any later version.\\n#\\n# Ansible is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU General Public License for more details.\\n#\\n# You should have received a copy of the GNU General Public License\\n# along with Ansible.  If not, see \\u003chttp://www.gnu.org/licenses/\\u003e.\\n\\nDOCUMENTATION = \'\'\'\\n\\nmodule: pagerduty\\nshort_description: Create PagerDuty maintenance windows\\ndescription:\\n    - This module will let you create PagerDuty maintenance windows\\nversion_added: \\"1.2\\"\\nauthor:\\n    - \\"Andrew Newdigate (@suprememoocow)\\"\\n    - \\"Dylan Silva (@thaumos)\\"\\n    - \\"Justin Johns\\"\\n    - \\"Bruce Pennypacker\\"\\nrequirements:\\n    - PagerDuty API access\\noptions:\\n    state:\\n        description:\\n            - Create a maintenance window or get a list of ongoing windows.\\n        required: true\\n        default: null\\n        choices: [ \\"running\\", \\"started\\", \\"ongoing\\", \\"absent\\" ]\\n        aliases: []\\n    name:\\n        description:\\n            - PagerDuty unique subdomain.\\n        required: true\\n        default: null\\n        choices: []\\n        aliases: []\\n    user:\\n        description:\\n            - PagerDuty user ID.\\n        required: true\\n        default: null\\n        choices: []\\n        aliases: []\\n    passwd:\\n        description:\\n            - PagerDuty user password.\\n        required: true\\n        default: null\\n        choices: []\\n        aliases: []\\n    token:\\n        description:\\n            - A pagerduty token, generated on the pagerduty site. Can be used instead of\\n              user/passwd combination.\\n        required: true\\n        default: null\\n        choices: []\\n        aliases: []\\n        version_added: \'1.8\'\\n    requester_id:\\n        description:\\n            - ID of user making the request. Only needed when using a token and creating a maintenance_window.\\n        required: true\\n        default: null\\n        choices: []\\n        aliases: []\\n        version_added: \'1.8\'\\n    service:\\n        description:\\n            - A comma separated list of PagerDuty service IDs.\\n        required: false\\n        default: null\\n        choices: []\\n        aliases: [ services ]\\n    hours:\\n        description:\\n            - Length of maintenance window in hours.\\n        required: false\\n        default: 1\\n        choices: []\\n        aliases: []\\n    minutes:\\n        description:\\n            - Maintenance window in minutes (this is added to the hours).\\n        required: false\\n        default: 0\\n        choices: []\\n        aliases: []\\n        version_added: \'1.8\'\\n    desc:\\n        description:\\n            - Short description of maintenance window.\\n        required: false\\n        default: Created by Ansible\\n        choices: []\\n        aliases: []\\n    validate_certs:\\n        description:\\n            - If C(no), SSL certificates will not be validated. This should only be used\\n              on personally controlled sites using self-signed certificates.\\n        required: false\\n        default: \'yes\'\\n        choices: [\'yes\', \'no\']\\n        version_added: 1.5.1\\n\'\'\'\\n\\nEXAMPLES=\'\'\'\\n# List ongoing maintenance windows using a user/passwd\\n- pagerduty: name=companyabc user=example@example.com passwd=password123 state=ongoing\\n\\n# List ongoing maintenance windows using a token\\n- pagerduty: name=companyabc token=xxxxxxxxxxxxxx state=ongoing\\n\\n# Create a 1 hour maintenance window for service FOO123, using a user/passwd\\n- pagerduty: name=companyabc\\n             user=example@example.com\\n             passwd=password123\\n             state=running\\n             service=FOO123\\n\\n# Create a 5 minute maintenance window for service FOO123, using a token\\n- pagerduty: name=companyabc\\n             token=xxxxxxxxxxxxxx\\n             hours=0\\n             minutes=5\\n             state=running\\n             service=FOO123\\n\\n\\n# Create a 4 hour maintenance window for service FOO123 with the description \\"deployment\\".\\n- pagerduty: name=companyabc\\n             user=example@example.com\\n             passwd=password123\\n             state=running\\n             service=FOO123\\n             hours=4\\n             desc=deployment\\n  register: pd_window\\n\\n# Delete the previous maintenance window\\n- pagerduty: name=companyabc\\n             user=example@example.com\\n             passwd=password123\\n             state=absent\\n             service={{ pd_window.result.maintenance_window.id }}\\n\'\'\'\\n\\nimport datetime\\nimport base64\\n\\ndef auth_header(user, passwd, token):\\n    if token:\\n        return \\"Token token=%s\\" % token\\n\\n    auth = base64.encodestring(\'%s:%s\' % (user, passwd)).replace(\'\\\\n\', \'\')\\n    return \\"Basic %s\\" % auth\\n\\ndef ongoing(module, name, user, passwd, token):\\n    url = \\"https://\\" + name + \\".pagerduty.com/api/v1/maintenance_windows/ongoing\\"\\n    headers = {\\"Authorization\\": auth_header(user, passwd, token)}\\n\\n    response, info = fetch_url(module, url, headers=headers)\\n    if info[\'status\'] != 200:\\n        module.fail_json(msg=\\"failed to lookup the ongoing window: %s\\" % info[\'msg\'])\\n\\n    try:\\n        json_out = json.loads(response.read())\\n    except:\\n        json_out = \\"\\"\\n\\n    return False, json_out, False\\n\\n\\ndef create(module, name, user, passwd, token, requester_id, service, hours, minutes, desc):\\n    now = datetime.datetime.utcnow()\\n    later = now + datetime.timedelta(hours=int(hours), minutes=int(minutes))\\n    start = now.strftime(\\"%Y-%m-%dT%H:%M:%SZ\\")\\n    end = later.strftime(\\"%Y-%m-%dT%H:%M:%SZ\\")\\n\\n    url = \\"https://\\" + name + \\".pagerduty.com/api/v1/maintenance_windows\\"\\n    headers = {\\n        \'Authorization\': auth_header(user, passwd, token),\\n        \'Content-Type\' : \'application/json\',\\n    }\\n    request_data = {\'maintenance_window\': {\'start_time\': start, \'end_time\': end, \'description\': desc, \'service_ids\': service}}\\n    \\n    if requester_id:\\n        request_data[\'requester_id\'] = requester_id\\n    else:\\n        if token:\\n            module.fail_json(msg=\\"requester_id is required when using a token\\")\\n\\n    data = json.dumps(request_data)\\n    response, info = fetch_url(module, url, data=data, headers=headers, method=\'POST\')\\n    if info[\'status\'] != 200:\\n        module.fail_json(msg=\\"failed to create the window: %s\\" % info[\'msg\'])\\n\\n    try:\\n        json_out = json.loads(response.read())\\n    except:\\n        json_out = \\"\\"\\n\\n    return False, json_out, True\\n\\ndef absent(module, name, user, passwd, token, requester_id, service):\\n    url = \\"https://\\" + name + \\".pagerduty.com/api/v1/maintenance_windows/\\" + service[0]\\n    headers = {\\n        \'Authorization\': auth_header(user, passwd, token),\\n        \'Content-Type\' : \'application/json\',\\n    }\\n    request_data = {}\\n    \\n    if requester_id:\\n        request_data[\'requester_id\'] = requester_id\\n    else:\\n        if token:\\n            module.fail_json(msg=\\"requester_id is required when using a token\\")\\n\\n    data = json.dumps(request_data)\\n    response, info = fetch_url(module, url, data=data, headers=headers, method=\'DELETE\')\\n    if info[\'status\'] != 200:\\n        module.fail_json(msg=\\"failed to delete the window: %s\\" % info[\'msg\'])\\n\\n    try:\\n        json_out = json.loads(response.read())\\n    except:\\n        json_out = \\"\\"\\n\\n    return False, json_out, True\\n\\n\\ndef main():\\n\\n    module = AnsibleModule(\\n        argument_spec=dict(\\n        state=dict(required=True, choices=[\'running\', \'started\', \'ongoing\', \'absent\']),\\n        name=dict(required=True),\\n        user=dict(required=False),\\n        passwd=dict(required=False),\\n        token=dict(required=False),\\n        service=dict(required=False, type=\'list\', aliases=[\\"services\\"]),\\n        requester_id=dict(required=False),\\n        hours=dict(default=\'1\', required=False),\\n        minutes=dict(default=\'0\', required=False),\\n        desc=dict(default=\'Created by Ansible\', required=False),\\n        validate_certs = dict(default=\'yes\', type=\'bool\'),\\n        )\\n    )\\n\\n    state = module.params[\'state\']\\n    name = module.params[\'name\']\\n    user = module.params[\'user\']\\n    passwd = module.params[\'passwd\']\\n    token = module.params[\'token\']\\n    service = module.params[\'service\']\\n    hours = module.params[\'hours\']\\n    minutes = module.params[\'minutes\']\\n    token = module.params[\'token\']\\n    desc = module.params[\'desc\']\\n    requester_id =  module.params[\'requester_id\']\\n\\n    if not token and not (user or passwd):\\n        module.fail_json(msg=\\"neither user and passwd nor token specified\\")\\n\\n    if state == \\"running\\" or state == \\"started\\":\\n        if not service:\\n            module.fail_json(msg=\\"service not specified\\")\\n        (rc, out, changed) = create(module, name, user, passwd, token, requester_id, service, hours, minutes, desc)\\n        if rc == 0:\\n            changed=True\\n\\n    if state == \\"ongoing\\":\\n        (rc, out, changed) = ongoing(module, name, user, passwd, token)\\n\\n    if state == \\"absent\\":\\n        (rc, out, changed) = absent(module, name, user, passwd, token, requester_id, service)\\n\\n    if rc != 0:\\n        module.fail_json(msg=\\"failed\\", result=out)\\n\\n\\n    module.exit_json(msg=\\"success\\", result=out, changed=changed)\\n\\n# import module snippets\\nfrom ansible.module_utils.basic import *\\nfrom ansible.module_utils.urls import *\\n\\nmain()\\n"}\n'
line: b'{"repo_name":"ftomassetti/intellij-community","ref":"refs/heads/master","path":"python/lib/Lib/site-packages/django/contrib/gis/geos/tests/test_geos.py","content":"import ctypes, random, unittest, sys\\nfrom django.contrib.gis.geos import *\\nfrom django.contrib.gis.geos.base import gdal, numpy, GEOSBase\\nfrom django.contrib.gis.geos.libgeos import GEOS_PREPARE\\nfrom django.contrib.gis.geometry.test_data import TestDataMixin\\n\\nclass GEOSTest(unittest.TestCase, TestDataMixin):\\n\\n    @property\\n    def null_srid(self):\\n        \\"\\"\\"\\n        Returns the proper null SRID depending on the GEOS version.\\n        See the comments in `test15_srid` for more details.\\n        \\"\\"\\"\\n        info = geos_version_info()\\n        if info[\'version\'] == \'3.0.0\' and info[\'release_candidate\']:\\n            return -1\\n        else:\\n            return None\\n\\n    def test00_base(self):\\n        \\"Tests out the GEOSBase class.\\"\\n        # Testing out GEOSBase class, which provides a `ptr` property\\n        # that abstracts out access to underlying C pointers.\\n        class FakeGeom1(GEOSBase):\\n            pass\\n\\n        # This one only accepts pointers to floats\\n        c_float_p = ctypes.POINTER(ctypes.c_float)\\n        class FakeGeom2(GEOSBase):\\n            ptr_type = c_float_p\\n\\n        # Default ptr_type is `c_void_p`.\\n        fg1 = FakeGeom1()\\n        # Default ptr_type is C float pointer\\n        fg2 = FakeGeom2()\\n\\n        # These assignments are OK -- None is allowed because\\n        # it\'s equivalent to the NULL pointer.\\n        fg1.ptr = ctypes.c_void_p()\\n        fg1.ptr = None\\n        fg2.ptr = c_float_p(ctypes.c_float(5.23))\\n        fg2.ptr = None\\n\\n        # Because pointers have been set to NULL, an exception should be\\n        # raised when we try to access it.  Raising an exception is\\n        # preferrable to a segmentation fault that commonly occurs when\\n        # a C method is given a NULL memory reference.\\n        for fg in (fg1, fg2):\\n            # Equivalent to `fg.ptr`\\n            self.assertRaises(GEOSException, fg._get_ptr)\\n\\n        # Anything that is either not None or the acceptable pointer type will\\n        # result in a TypeError when trying to assign it to the `ptr` property.\\n        # Thus, memmory addresses (integers) and pointers of the incorrect type\\n        # (in `bad_ptrs`) will not be allowed.\\n        bad_ptrs = (5, ctypes.c_char_p(\'foobar\'))\\n        for bad_ptr in bad_ptrs:\\n            # Equivalent to `fg.ptr = bad_ptr`\\n            self.assertRaises(TypeError, fg1._set_ptr, bad_ptr)\\n            self.assertRaises(TypeError, fg2._set_ptr, bad_ptr)\\n\\n    def test01a_wkt(self):\\n        \\"Testing WKT output.\\"\\n        for g in self.geometries.wkt_out:\\n            geom = fromstr(g.wkt)\\n            self.assertEqual(g.ewkt, geom.wkt)\\n\\n    def test01b_hex(self):\\n        \\"Testing HEX output.\\"\\n        for g in self.geometries.hex_wkt:\\n            geom = fromstr(g.wkt)\\n            self.assertEqual(g.hex, geom.hex)\\n\\n    def test01b_hexewkb(self):\\n        \\"Testing (HEX)EWKB output.\\"\\n        from binascii import a2b_hex\\n\\n        # For testing HEX(EWKB).\\n        ogc_hex = \'01010000000000000000000000000000000000F03F\'\\n        # `SELECT ST_AsHEXEWKB(ST_GeomFromText(\'POINT(0 1)\', 4326));`\\n        hexewkb_2d = \'0101000020E61000000000000000000000000000000000F03F\'\\n        # `SELECT ST_AsHEXEWKB(ST_GeomFromEWKT(\'SRID=4326;POINT(0 1 2)\'));`\\n        hexewkb_3d = \'01010000A0E61000000000000000000000000000000000F03F0000000000000040\'\\n\\n        pnt_2d = Point(0, 1, srid=4326)\\n        pnt_3d = Point(0, 1, 2, srid=4326)\\n\\n        # OGC-compliant HEX will not have SRID nor Z value.\\n        self.assertEqual(ogc_hex, pnt_2d.hex)\\n        self.assertEqual(ogc_hex, pnt_3d.hex)\\n\\n        # HEXEWKB should be appropriate for its dimension -- have to use an\\n        # a WKBWriter w/dimension set accordingly, else GEOS will insert\\n        # garbage into 3D coordinate if there is none.  Also, GEOS has a\\n        # a bug in versions prior to 3.1 that puts the X coordinate in\\n        # place of Z; an exception should be raised on those versions.\\n        self.assertEqual(hexewkb_2d, pnt_2d.hexewkb)\\n        if GEOS_PREPARE:\\n            self.assertEqual(hexewkb_3d, pnt_3d.hexewkb)\\n            self.assertEqual(True, GEOSGeometry(hexewkb_3d).hasz)\\n        else:\\n            try:\\n                hexewkb = pnt_3d.hexewkb\\n            except GEOSException:\\n                pass\\n            else:\\n                self.fail(\'Should have raised GEOSException.\')\\n\\n        # Same for EWKB.\\n        self.assertEqual(buffer(a2b_hex(hexewkb_2d)), pnt_2d.ewkb)\\n        if GEOS_PREPARE:\\n            self.assertEqual(buffer(a2b_hex(hexewkb_3d)), pnt_3d.ewkb)\\n        else:\\n            try:\\n                ewkb = pnt_3d.ewkb\\n            except GEOSException:\\n                pass\\n            else:\\n                self.fail(\'Should have raised GEOSException\')\\n\\n        # Redundant sanity check.\\n        self.assertEqual(4326, GEOSGeometry(hexewkb_2d).srid)\\n\\n    def test01c_kml(self):\\n        \\"Testing KML output.\\"\\n        for tg in self.geometries.wkt_out:\\n            geom = fromstr(tg.wkt)\\n            kml = getattr(tg, \'kml\', False)\\n            if kml: self.assertEqual(kml, geom.kml)\\n\\n    def test01d_errors(self):\\n        \\"Testing the Error handlers.\\"\\n        # string-based\\n        print \\"\\\\nBEGIN - expecting GEOS_ERROR; safe to ignore.\\\\n\\"\\n        for err in self.geometries.errors:\\n            try:\\n                g = fromstr(err.wkt)\\n            except (GEOSException, ValueError):\\n                pass\\n\\n        # Bad WKB\\n        self.assertRaises(GEOSException, GEOSGeometry, buffer(\'0\'))\\n\\n        print \\"\\\\nEND - expecting GEOS_ERROR; safe to ignore.\\\\n\\"\\n\\n        class NotAGeometry(object):\\n            pass\\n\\n        # Some other object\\n        self.assertRaises(TypeError, GEOSGeometry, NotAGeometry())\\n        # None\\n        self.assertRaises(TypeError, GEOSGeometry, None)\\n\\n    def test01e_wkb(self):\\n        \\"Testing WKB output.\\"\\n        from binascii import b2a_hex\\n        for g in self.geometries.hex_wkt:\\n            geom = fromstr(g.wkt)\\n            wkb = geom.wkb\\n            self.assertEqual(b2a_hex(wkb).upper(), g.hex)\\n\\n    def test01f_create_hex(self):\\n        \\"Testing creation from HEX.\\"\\n        for g in self.geometries.hex_wkt:\\n            geom_h = GEOSGeometry(g.hex)\\n            # we need to do this so decimal places get normalised\\n            geom_t = fromstr(g.wkt)\\n            self.assertEqual(geom_t.wkt, geom_h.wkt)\\n\\n    def test01g_create_wkb(self):\\n        \\"Testing creation from WKB.\\"\\n        from binascii import a2b_hex\\n        for g in self.geometries.hex_wkt:\\n            wkb = buffer(a2b_hex(g.hex))\\n            geom_h = GEOSGeometry(wkb)\\n            # we need to do this so decimal places get normalised\\n            geom_t = fromstr(g.wkt)\\n            self.assertEqual(geom_t.wkt, geom_h.wkt)\\n\\n    def test01h_ewkt(self):\\n        \\"Testing EWKT.\\"\\n        srid = 32140\\n        for p in self.geometries.polygons:\\n            ewkt = \'SRID=%d;%s\' % (srid, p.wkt)\\n            poly = fromstr(ewkt)\\n            self.assertEqual(srid, poly.srid)\\n            self.assertEqual(srid, poly.shell.srid)\\n            self.assertEqual(srid, fromstr(poly.ewkt).srid) # Checking export\\n\\n    def test01i_json(self):\\n        \\"Testing GeoJSON input/output (via GDAL).\\"\\n        if not gdal or not gdal.GEOJSON: return\\n        for g in self.geometries.json_geoms:\\n            geom = GEOSGeometry(g.wkt)\\n            if not hasattr(g, \'not_equal\'):\\n                self.assertEqual(g.json, geom.json)\\n                self.assertEqual(g.json, geom.geojson)\\n            self.assertEqual(GEOSGeometry(g.wkt), GEOSGeometry(geom.json))\\n\\n    def test01k_fromfile(self):\\n        \\"Testing the fromfile() factory.\\"\\n        from StringIO import StringIO\\n        ref_pnt = GEOSGeometry(\'POINT(5 23)\')\\n\\n        wkt_f = StringIO()\\n        wkt_f.write(ref_pnt.wkt)\\n        wkb_f = StringIO()\\n        wkb_f.write(str(ref_pnt.wkb))\\n\\n        # Other tests use `fromfile()` on string filenames so those\\n        # aren\'t tested here.\\n        for fh in (wkt_f, wkb_f):\\n            fh.seek(0)\\n            pnt = fromfile(fh)\\n            self.assertEqual(ref_pnt, pnt)\\n\\n    def test01k_eq(self):\\n        \\"Testing equivalence.\\"\\n        p = fromstr(\'POINT(5 23)\')\\n        self.assertEqual(p, p.wkt)\\n        self.assertNotEqual(p, \'foo\')\\n        ls = fromstr(\'LINESTRING(0 0, 1 1, 5 5)\')\\n        self.assertEqual(ls, ls.wkt)\\n        self.assertNotEqual(p, \'bar\')\\n        # Error shouldn\'t be raise on equivalence testing with\\n        # an invalid type.\\n        for g in (p, ls):\\n            self.assertNotEqual(g, None)\\n            self.assertNotEqual(g, {\'foo\' : \'bar\'})\\n            self.assertNotEqual(g, False)\\n\\n    def test02a_points(self):\\n        \\"Testing Point objects.\\"\\n        prev = fromstr(\'POINT(0 0)\')\\n        for p in self.geometries.points:\\n            # Creating the point from the WKT\\n            pnt = fromstr(p.wkt)\\n            self.assertEqual(pnt.geom_type, \'Point\')\\n            self.assertEqual(pnt.geom_typeid, 0)\\n            self.assertEqual(p.x, pnt.x)\\n            self.assertEqual(p.y, pnt.y)\\n            self.assertEqual(True, pnt == fromstr(p.wkt))\\n            self.assertEqual(False, pnt == prev)\\n\\n            # Making sure that the point\'s X, Y components are what we expect\\n            self.assertAlmostEqual(p.x, pnt.tuple[0], 9)\\n            self.assertAlmostEqual(p.y, pnt.tuple[1], 9)\\n\\n            # Testing the third dimension, and getting the tuple arguments\\n            if hasattr(p, \'z\'):\\n                self.assertEqual(True, pnt.hasz)\\n                self.assertEqual(p.z, pnt.z)\\n                self.assertEqual(p.z, pnt.tuple[2], 9)\\n                tup_args = (p.x, p.y, p.z)\\n                set_tup1 = (2.71, 3.14, 5.23)\\n                set_tup2 = (5.23, 2.71, 3.14)\\n            else:\\n                self.assertEqual(False, pnt.hasz)\\n                self.assertEqual(None, pnt.z)\\n                tup_args = (p.x, p.y)\\n                set_tup1 = (2.71, 3.14)\\n                set_tup2 = (3.14, 2.71)\\n\\n            # Centroid operation on point should be point itself\\n            self.assertEqual(p.centroid, pnt.centroid.tuple)\\n\\n            # Now testing the different constructors\\n            pnt2 = Point(tup_args)  # e.g., Point((1, 2))\\n            pnt3 = Point(*tup_args) # e.g., Point(1, 2)\\n            self.assertEqual(True, pnt == pnt2)\\n            self.assertEqual(True, pnt == pnt3)\\n\\n            # Now testing setting the x and y\\n            pnt.y = 3.14\\n            pnt.x = 2.71\\n            self.assertEqual(3.14, pnt.y)\\n            self.assertEqual(2.71, pnt.x)\\n\\n            # Setting via the tuple/coords property\\n            pnt.tuple = set_tup1\\n            self.assertEqual(set_tup1, pnt.tuple)\\n            pnt.coords = set_tup2\\n            self.assertEqual(set_tup2, pnt.coords)\\n\\n            prev = pnt # setting the previous geometry\\n\\n    def test02b_multipoints(self):\\n        \\"Testing MultiPoint objects.\\"\\n        for mp in self.geometries.multipoints:\\n            mpnt = fromstr(mp.wkt)\\n            self.assertEqual(mpnt.geom_type, \'MultiPoint\')\\n            self.assertEqual(mpnt.geom_typeid, 4)\\n\\n            self.assertAlmostEqual(mp.centroid[0], mpnt.centroid.tuple[0], 9)\\n            self.assertAlmostEqual(mp.centroid[1], mpnt.centroid.tuple[1], 9)\\n\\n            self.assertRaises(GEOSIndexError, mpnt.__getitem__, len(mpnt))\\n            self.assertEqual(mp.centroid, mpnt.centroid.tuple)\\n            self.assertEqual(mp.coords, tuple(m.tuple for m in mpnt))\\n            for p in mpnt:\\n                self.assertEqual(p.geom_type, \'Point\')\\n                self.assertEqual(p.geom_typeid, 0)\\n                self.assertEqual(p.empty, False)\\n                self.assertEqual(p.valid, True)\\n\\n    def test03a_linestring(self):\\n        \\"Testing LineString objects.\\"\\n        prev = fromstr(\'POINT(0 0)\')\\n        for l in self.geometries.linestrings:\\n            ls = fromstr(l.wkt)\\n            self.assertEqual(ls.geom_type, \'LineString\')\\n            self.assertEqual(ls.geom_typeid, 1)\\n            self.assertEqual(ls.empty, False)\\n            self.assertEqual(ls.ring, False)\\n            if hasattr(l, \'centroid\'):\\n                self.assertEqual(l.centroid, ls.centroid.tuple)\\n            if hasattr(l, \'tup\'):\\n                self.assertEqual(l.tup, ls.tuple)\\n\\n            self.assertEqual(True, ls == fromstr(l.wkt))\\n            self.assertEqual(False, ls == prev)\\n            self.assertRaises(GEOSIndexError, ls.__getitem__, len(ls))\\n            prev = ls\\n\\n            # Creating a LineString from a tuple, list, and numpy array\\n            self.assertEqual(ls, LineString(ls.tuple))  # tuple\\n            self.assertEqual(ls, LineString(*ls.tuple)) # as individual arguments\\n            self.assertEqual(ls, LineString([list(tup) for tup in ls.tuple])) # as list\\n            self.assertEqual(ls.wkt, LineString(*tuple(Point(tup) for tup in ls.tuple)).wkt) # Point individual arguments\\n            if numpy: self.assertEqual(ls, LineString(numpy.array(ls.tuple))) # as numpy array\\n\\n    def test03b_multilinestring(self):\\n        \\"Testing MultiLineString objects.\\"\\n        prev = fromstr(\'POINT(0 0)\')\\n        for l in self.geometries.multilinestrings:\\n            ml = fromstr(l.wkt)\\n            self.assertEqual(ml.geom_type, \'MultiLineString\')\\n            self.assertEqual(ml.geom_typeid, 5)\\n\\n            self.assertAlmostEqual(l.centroid[0], ml.centroid.x, 9)\\n            self.assertAlmostEqual(l.centroid[1], ml.centroid.y, 9)\\n\\n            self.assertEqual(True, ml == fromstr(l.wkt))\\n            self.assertEqual(False, ml == prev)\\n            prev = ml\\n\\n            for ls in ml:\\n                self.assertEqual(ls.geom_type, \'LineString\')\\n                self.assertEqual(ls.geom_typeid, 1)\\n                self.assertEqual(ls.empty, False)\\n\\n            self.assertRaises(GEOSIndexError, ml.__getitem__, len(ml))\\n            self.assertEqual(ml.wkt, MultiLineString(*tuple(s.clone() for s in ml)).wkt)\\n            self.assertEqual(ml, MultiLineString(*tuple(LineString(s.tuple) for s in ml)))\\n\\n    def test04_linearring(self):\\n        \\"Testing LinearRing objects.\\"\\n        for rr in self.geometries.linearrings:\\n            lr = fromstr(rr.wkt)\\n            self.assertEqual(lr.geom_type, \'LinearRing\')\\n            self.assertEqual(lr.geom_typeid, 2)\\n            self.assertEqual(rr.n_p, len(lr))\\n            self.assertEqual(True, lr.valid)\\n            self.assertEqual(False, lr.empty)\\n\\n            # Creating a LinearRing from a tuple, list, and numpy array\\n            self.assertEqual(lr, LinearRing(lr.tuple))\\n            self.assertEqual(lr, LinearRing(*lr.tuple))\\n            self.assertEqual(lr, LinearRing([list(tup) for tup in lr.tuple]))\\n            if numpy: self.assertEqual(lr, LinearRing(numpy.array(lr.tuple)))\\n\\n    def test05a_polygons(self):\\n        \\"Testing Polygon objects.\\"\\n\\n        # Testing `from_bbox` class method\\n        bbox = (-180, -90, 180, 90)\\n        p = Polygon.from_bbox( bbox )\\n        self.assertEqual(bbox, p.extent)\\n\\n        prev = fromstr(\'POINT(0 0)\')\\n        for p in self.geometries.polygons:\\n            # Creating the Polygon, testing its properties.\\n            poly = fromstr(p.wkt)\\n            self.assertEqual(poly.geom_type, \'Polygon\')\\n            self.assertEqual(poly.geom_typeid, 3)\\n            self.assertEqual(poly.empty, False)\\n            self.assertEqual(poly.ring, False)\\n            self.assertEqual(p.n_i, poly.num_interior_rings)\\n            self.assertEqual(p.n_i + 1, len(poly)) # Testing __len__\\n            self.assertEqual(p.n_p, poly.num_points)\\n\\n            # Area \\u0026 Centroid\\n            self.assertAlmostEqual(p.area, poly.area, 9)\\n            self.assertAlmostEqual(p.centroid[0], poly.centroid.tuple[0], 9)\\n            self.assertAlmostEqual(p.centroid[1], poly.centroid.tuple[1], 9)\\n\\n            # Testing the geometry equivalence\\n            self.assertEqual(True, poly == fromstr(p.wkt))\\n            self.assertEqual(False, poly == prev) # Should not be equal to previous geometry\\n            self.assertEqual(True, poly != prev)\\n\\n            # Testing the exterior ring\\n            ring = poly.exterior_ring\\n            self.assertEqual(ring.geom_type, \'LinearRing\')\\n            self.assertEqual(ring.geom_typeid, 2)\\n            if p.ext_ring_cs:\\n                self.assertEqual(p.ext_ring_cs, ring.tuple)\\n                self.assertEqual(p.ext_ring_cs, poly[0].tuple) # Testing __getitem__\\n\\n            # Testing __getitem__ and __setitem__ on invalid indices\\n            self.assertRaises(GEOSIndexError, poly.__getitem__, len(poly))\\n            self.assertRaises(GEOSIndexError, poly.__setitem__, len(poly), False)\\n            self.assertRaises(GEOSIndexError, poly.__getitem__, -1 * len(poly) - 1)\\n\\n            # Testing __iter__\\n            for r in poly:\\n                self.assertEqual(r.geom_type, \'LinearRing\')\\n                self.assertEqual(r.geom_typeid, 2)\\n\\n            # Testing polygon construction.\\n            self.assertRaises(TypeError, Polygon.__init__, 0, [1, 2, 3])\\n            self.assertRaises(TypeError, Polygon.__init__, \'foo\')\\n\\n            # Polygon(shell, (hole1, ... holeN))\\n            rings = tuple(r for r in poly)\\n            self.assertEqual(poly, Polygon(rings[0], rings[1:]))\\n\\n            # Polygon(shell_tuple, hole_tuple1, ... , hole_tupleN)\\n            ring_tuples = tuple(r.tuple for r in poly)\\n            self.assertEqual(poly, Polygon(*ring_tuples))\\n\\n            # Constructing with tuples of LinearRings.\\n            self.assertEqual(poly.wkt, Polygon(*tuple(r for r in poly)).wkt)\\n            self.assertEqual(poly.wkt, Polygon(*tuple(LinearRing(r.tuple) for r in poly)).wkt)\\n\\n    def test05b_multipolygons(self):\\n        \\"Testing MultiPolygon objects.\\"\\n        print \\"\\\\nBEGIN - expecting GEOS_NOTICE; safe to ignore.\\\\n\\"\\n        prev = fromstr(\'POINT (0 0)\')\\n        for mp in self.geometries.multipolygons:\\n            mpoly = fromstr(mp.wkt)\\n            self.assertEqual(mpoly.geom_type, \'MultiPolygon\')\\n            self.assertEqual(mpoly.geom_typeid, 6)\\n            self.assertEqual(mp.valid, mpoly.valid)\\n\\n            if mp.valid:\\n                self.assertEqual(mp.num_geom, mpoly.num_geom)\\n                self.assertEqual(mp.n_p, mpoly.num_coords)\\n                self.assertEqual(mp.num_geom, len(mpoly))\\n                self.assertRaises(GEOSIndexError, mpoly.__getitem__, len(mpoly))\\n                for p in mpoly:\\n                    self.assertEqual(p.geom_type, \'Polygon\')\\n                    self.assertEqual(p.geom_typeid, 3)\\n                    self.assertEqual(p.valid, True)\\n                self.assertEqual(mpoly.wkt, MultiPolygon(*tuple(poly.clone() for poly in mpoly)).wkt)\\n\\n        print \\"\\\\nEND - expecting GEOS_NOTICE; safe to ignore.\\\\n\\"\\n\\n    def test06a_memory_hijinks(self):\\n        \\"Testing Geometry __del__() on rings and polygons.\\"\\n        #### Memory issues with rings and polygons\\n\\n        # These tests are needed to ensure sanity with writable geometries.\\n\\n        # Getting a polygon with interior rings, and pulling out the interior rings\\n        poly = fromstr(self.geometries.polygons[1].wkt)\\n        ring1 = poly[0]\\n        ring2 = poly[1]\\n\\n        # These deletes should be \'harmless\' since they are done on child geometries\\n        del ring1\\n        del ring2\\n        ring1 = poly[0]\\n        ring2 = poly[1]\\n\\n        # Deleting the polygon\\n        del poly\\n\\n        # Access to these rings is OK since they are clones.\\n        s1, s2 = str(ring1), str(ring2)\\n\\n    def test08_coord_seq(self):\\n        \\"Testing Coordinate Sequence objects.\\"\\n        for p in self.geometries.polygons:\\n            if p.ext_ring_cs:\\n                # Constructing the polygon and getting the coordinate sequence\\n                poly = fromstr(p.wkt)\\n                cs = poly.exterior_ring.coord_seq\\n\\n                self.assertEqual(p.ext_ring_cs, cs.tuple) # done in the Polygon test too.\\n                self.assertEqual(len(p.ext_ring_cs), len(cs)) # Making sure __len__ works\\n\\n                # Checks __getitem__ and __setitem__\\n                for i in xrange(len(p.ext_ring_cs)):\\n                    c1 = p.ext_ring_cs[i] # Expected value\\n                    c2 = cs[i] # Value from coordseq\\n                    self.assertEqual(c1, c2)\\n\\n                    # Constructing the test value to set the coordinate sequence with\\n                    if len(c1) == 2: tset = (5, 23)\\n                    else: tset = (5, 23, 8)\\n                    cs[i] = tset\\n\\n                    # Making sure every set point matches what we expect\\n                    for j in range(len(tset)):\\n                        cs[i] = tset\\n                        self.assertEqual(tset[j], cs[i][j])\\n\\n    def test09_relate_pattern(self):\\n        \\"Testing relate() and relate_pattern().\\"\\n        g = fromstr(\'POINT (0 0)\')\\n        self.assertRaises(GEOSException, g.relate_pattern, 0, \'invalid pattern, yo\')\\n        for rg in self.geometries.relate_geoms:\\n            a = fromstr(rg.wkt_a)\\n            b = fromstr(rg.wkt_b)\\n            self.assertEqual(rg.result, a.relate_pattern(b, rg.pattern))\\n            self.assertEqual(rg.pattern, a.relate(b))\\n\\n    def test10_intersection(self):\\n        \\"Testing intersects() and intersection().\\"\\n        for i in xrange(len(self.geometries.topology_geoms)):\\n            a = fromstr(self.geometries.topology_geoms[i].wkt_a)\\n            b = fromstr(self.geometries.topology_geoms[i].wkt_b)\\n            i1 = fromstr(self.geometries.intersect_geoms[i].wkt)\\n            self.assertEqual(True, a.intersects(b))\\n            i2 = a.intersection(b)\\n            self.assertEqual(i1, i2)\\n            self.assertEqual(i1, a \\u0026 b) # __and__ is intersection operator\\n            a \\u0026= b # testing __iand__\\n            self.assertEqual(i1, a)\\n\\n    def test11_union(self):\\n        \\"Testing union().\\"\\n        for i in xrange(len(self.geometries.topology_geoms)):\\n            a = fromstr(self.geometries.topology_geoms[i].wkt_a)\\n            b = fromstr(self.geometries.topology_geoms[i].wkt_b)\\n            u1 = fromstr(self.geometries.union_geoms[i].wkt)\\n            u2 = a.union(b)\\n            self.assertEqual(u1, u2)\\n            self.assertEqual(u1, a | b) # __or__ is union operator\\n            a |= b # testing __ior__\\n            self.assertEqual(u1, a)\\n\\n    def test12_difference(self):\\n        \\"Testing difference().\\"\\n        for i in xrange(len(self.geometries.topology_geoms)):\\n            a = fromstr(self.geometries.topology_geoms[i].wkt_a)\\n            b = fromstr(self.geometries.topology_geoms[i].wkt_b)\\n            d1 = fromstr(self.geometries.diff_geoms[i].wkt)\\n            d2 = a.difference(b)\\n            self.assertEqual(d1, d2)\\n            self.assertEqual(d1, a - b) # __sub__ is difference operator\\n            a -= b # testing __isub__\\n            self.assertEqual(d1, a)\\n\\n    def test13_symdifference(self):\\n        \\"Testing sym_difference().\\"\\n        for i in xrange(len(self.geometries.topology_geoms)):\\n            a = fromstr(self.geometries.topology_geoms[i].wkt_a)\\n            b = fromstr(self.geometries.topology_geoms[i].wkt_b)\\n            d1 = fromstr(self.geometries.sdiff_geoms[i].wkt)\\n            d2 = a.sym_difference(b)\\n            self.assertEqual(d1, d2)\\n            self.assertEqual(d1, a ^ b) # __xor__ is symmetric difference operator\\n            a ^= b # testing __ixor__\\n            self.assertEqual(d1, a)\\n\\n    def test14_buffer(self):\\n        \\"Testing buffer().\\"\\n        for bg in self.geometries.buffer_geoms:\\n            g = fromstr(bg.wkt)\\n\\n            # The buffer we expect\\n            exp_buf = fromstr(bg.buffer_wkt)\\n            quadsegs = bg.quadsegs\\n            width = bg.width\\n\\n            # Can\'t use a floating-point for the number of quadsegs.\\n            self.assertRaises(ctypes.ArgumentError, g.buffer, width, float(quadsegs))\\n\\n            # Constructing our buffer\\n            buf = g.buffer(width, quadsegs)\\n            self.assertEqual(exp_buf.num_coords, buf.num_coords)\\n            self.assertEqual(len(exp_buf), len(buf))\\n\\n            # Now assuring that each point in the buffer is almost equal\\n            for j in xrange(len(exp_buf)):\\n                exp_ring = exp_buf[j]\\n                buf_ring = buf[j]\\n                self.assertEqual(len(exp_ring), len(buf_ring))\\n                for k in xrange(len(exp_ring)):\\n                    # Asserting the X, Y of each point are almost equal (due to floating point imprecision)\\n                    self.assertAlmostEqual(exp_ring[k][0], buf_ring[k][0], 9)\\n                    self.assertAlmostEqual(exp_ring[k][1], buf_ring[k][1], 9)\\n\\n    def test15_srid(self):\\n        \\"Testing the SRID property and keyword.\\"\\n        # Testing SRID keyword on Point\\n        pnt = Point(5, 23, srid=4326)\\n        self.assertEqual(4326, pnt.srid)\\n        pnt.srid = 3084\\n        self.assertEqual(3084, pnt.srid)\\n        self.assertRaises(ctypes.ArgumentError, pnt.set_srid, \'4326\')\\n\\n        # Testing SRID keyword on fromstr(), and on Polygon rings.\\n        poly = fromstr(self.geometries.polygons[1].wkt, srid=4269)\\n        self.assertEqual(4269, poly.srid)\\n        for ring in poly: self.assertEqual(4269, ring.srid)\\n        poly.srid = 4326\\n        self.assertEqual(4326, poly.shell.srid)\\n\\n        # Testing SRID keyword on GeometryCollection\\n        gc = GeometryCollection(Point(5, 23), LineString((0, 0), (1.5, 1.5), (3, 3)), srid=32021)\\n        self.assertEqual(32021, gc.srid)\\n        for i in range(len(gc)): self.assertEqual(32021, gc[i].srid)\\n\\n        # GEOS may get the SRID from HEXEWKB\\n        # \'POINT(5 23)\' at SRID=4326 in hex form -- obtained from PostGIS\\n        # using `SELECT GeomFromText(\'POINT (5 23)\', 4326);`.\\n        hex = \'0101000020E610000000000000000014400000000000003740\'\\n        p1 = fromstr(hex)\\n        self.assertEqual(4326, p1.srid)\\n\\n        # In GEOS 3.0.0rc1-4  when the EWKB and/or HEXEWKB is exported,\\n        # the SRID information is lost and set to -1 -- this is not a\\n        # problem on the 3.0.0 version (another reason to upgrade).\\n        exp_srid = self.null_srid\\n\\n        p2 = fromstr(p1.hex)\\n        self.assertEqual(exp_srid, p2.srid)\\n        p3 = fromstr(p1.hex, srid=-1) # -1 is intended.\\n        self.assertEqual(-1, p3.srid)\\n\\n    def test16_mutable_geometries(self):\\n        \\"Testing the mutability of Polygons and Geometry Collections.\\"\\n        ### Testing the mutability of Polygons ###\\n        for p in self.geometries.polygons:\\n            poly = fromstr(p.wkt)\\n\\n            # Should only be able to use __setitem__ with LinearRing geometries.\\n            self.assertRaises(TypeError, poly.__setitem__, 0, LineString((1, 1), (2, 2)))\\n\\n            # Constructing the new shell by adding 500 to every point in the old shell.\\n            shell_tup = poly.shell.tuple\\n            new_coords = []\\n            for point in shell_tup: new_coords.append((point[0] + 500., point[1] + 500.))\\n            new_shell = LinearRing(*tuple(new_coords))\\n\\n            # Assigning polygon\'s exterior ring w/the new shell\\n            poly.exterior_ring = new_shell\\n            s = str(new_shell) # new shell is still accessible\\n            self.assertEqual(poly.exterior_ring, new_shell)\\n            self.assertEqual(poly[0], new_shell)\\n\\n        ### Testing the mutability of Geometry Collections\\n        for tg in self.geometries.multipoints:\\n            mp = fromstr(tg.wkt)\\n            for i in range(len(mp)):\\n                # Creating a random point.\\n                pnt = mp[i]\\n                new = Point(random.randint(1, 100), random.randint(1, 100))\\n                # Testing the assignment\\n                mp[i] = new\\n                s = str(new) # what was used for the assignment is still accessible\\n                self.assertEqual(mp[i], new)\\n                self.assertEqual(mp[i].wkt, new.wkt)\\n                self.assertNotEqual(pnt, mp[i])\\n\\n        # MultiPolygons involve much more memory management because each\\n        # Polygon w/in the collection has its own rings.\\n        for tg in self.geometries.multipolygons:\\n            mpoly = fromstr(tg.wkt)\\n            for i in xrange(len(mpoly)):\\n                poly = mpoly[i]\\n                old_poly = mpoly[i]\\n                # Offsetting the each ring in the polygon by 500.\\n                for j in xrange(len(poly)):\\n                    r = poly[j]\\n                    for k in xrange(len(r)): r[k] = (r[k][0] + 500., r[k][1] + 500.)\\n                    poly[j] = r\\n\\n                self.assertNotEqual(mpoly[i], poly)\\n                # Testing the assignment\\n                mpoly[i] = poly\\n                s = str(poly) # Still accessible\\n                self.assertEqual(mpoly[i], poly)\\n                self.assertNotEqual(mpoly[i], old_poly)\\n\\n        # Extreme (!!) __setitem__ -- no longer works, have to detect\\n        # in the first object that __setitem__ is called in the subsequent\\n        # objects -- maybe mpoly[0, 0, 0] = (3.14, 2.71)?\\n        #mpoly[0][0][0] = (3.14, 2.71)\\n        #self.assertEqual((3.14, 2.71), mpoly[0][0][0])\\n        # Doing it more slowly..\\n        #self.assertEqual((3.14, 2.71), mpoly[0].shell[0])\\n        #del mpoly\\n\\n    def test17_threed(self):\\n        \\"Testing three-dimensional geometries.\\"\\n        # Testing a 3D Point\\n        pnt = Point(2, 3, 8)\\n        self.assertEqual((2.,3.,8.), pnt.coords)\\n        self.assertRaises(TypeError, pnt.set_coords, (1.,2.))\\n        pnt.coords = (1.,2.,3.)\\n        self.assertEqual((1.,2.,3.), pnt.coords)\\n\\n        # Testing a 3D LineString\\n        ls = LineString((2., 3., 8.), (50., 250., -117.))\\n        self.assertEqual(((2.,3.,8.), (50.,250.,-117.)), ls.tuple)\\n        self.assertRaises(TypeError, ls.__setitem__, 0, (1.,2.))\\n        ls[0] = (1.,2.,3.)\\n        self.assertEqual((1.,2.,3.), ls[0])\\n\\n    def test18_distance(self):\\n        \\"Testing the distance() function.\\"\\n        # Distance to self should be 0.\\n        pnt = Point(0, 0)\\n        self.assertEqual(0.0, pnt.distance(Point(0, 0)))\\n\\n        # Distance should be 1\\n        self.assertEqual(1.0, pnt.distance(Point(0, 1)))\\n\\n        # Distance should be ~ sqrt(2)\\n        self.assertAlmostEqual(1.41421356237, pnt.distance(Point(1, 1)), 11)\\n\\n        # Distances are from the closest vertex in each geometry --\\n        #  should be 3 (distance from (2, 2) to (5, 2)).\\n        ls1 = LineString((0, 0), (1, 1), (2, 2))\\n        ls2 = LineString((5, 2), (6, 1), (7, 0))\\n        self.assertEqual(3, ls1.distance(ls2))\\n\\n    def test19_length(self):\\n        \\"Testing the length property.\\"\\n        # Points have 0 length.\\n        pnt = Point(0, 0)\\n        self.assertEqual(0.0, pnt.length)\\n\\n        # Should be ~ sqrt(2)\\n        ls = LineString((0, 0), (1, 1))\\n        self.assertAlmostEqual(1.41421356237, ls.length, 11)\\n\\n        # Should be circumfrence of Polygon\\n        poly = Polygon(LinearRing((0, 0), (0, 1), (1, 1), (1, 0), (0, 0)))\\n        self.assertEqual(4.0, poly.length)\\n\\n        # Should be sum of each element\'s length in collection.\\n        mpoly = MultiPolygon(poly.clone(), poly)\\n        self.assertEqual(8.0, mpoly.length)\\n\\n    def test20a_emptyCollections(self):\\n        \\"Testing empty geometries and collections.\\"\\n        gc1 = GeometryCollection([])\\n        gc2 = fromstr(\'GEOMETRYCOLLECTION EMPTY\')\\n        pnt = fromstr(\'POINT EMPTY\')\\n        ls = fromstr(\'LINESTRING EMPTY\')\\n        poly = fromstr(\'POLYGON EMPTY\')\\n        mls = fromstr(\'MULTILINESTRING EMPTY\')\\n        mpoly1 = fromstr(\'MULTIPOLYGON EMPTY\')\\n        mpoly2 = MultiPolygon(())\\n\\n        for g in [gc1, gc2, pnt, ls, poly, mls, mpoly1, mpoly2]:\\n            self.assertEqual(True, g.empty)\\n\\n            # Testing len() and num_geom.\\n            if isinstance(g, Polygon):\\n                self.assertEqual(1, len(g)) # Has one empty linear ring\\n                self.assertEqual(1, g.num_geom)\\n                self.assertEqual(0, len(g[0]))\\n            elif isinstance(g, (Point, LineString)):\\n                self.assertEqual(1, g.num_geom)\\n                self.assertEqual(0, len(g))\\n            else:\\n                self.assertEqual(0, g.num_geom)\\n                self.assertEqual(0, len(g))\\n\\n            # Testing __getitem__ (doesn\'t work on Point or Polygon)\\n            if isinstance(g, Point):\\n                self.assertRaises(GEOSIndexError, g.get_x)\\n            elif isinstance(g, Polygon):\\n                lr = g.shell\\n                self.assertEqual(\'LINEARRING EMPTY\', lr.wkt)\\n                self.assertEqual(0, len(lr))\\n                self.assertEqual(True, lr.empty)\\n                self.assertRaises(GEOSIndexError, lr.__getitem__, 0)\\n            else:\\n                self.assertRaises(GEOSIndexError, g.__getitem__, 0)\\n\\n    def test20b_collections_of_collections(self):\\n        \\"Testing GeometryCollection handling of other collections.\\"\\n        # Creating a GeometryCollection WKT string composed of other\\n        # collections and polygons.\\n        coll = [mp.wkt for mp in self.geometries.multipolygons if mp.valid]\\n        coll.extend([mls.wkt for mls in self.geometries.multilinestrings])\\n        coll.extend([p.wkt for p in self.geometries.polygons])\\n        coll.extend([mp.wkt for mp in self.geometries.multipoints])\\n        gc_wkt = \'GEOMETRYCOLLECTION(%s)\' % \',\'.join(coll)\\n\\n        # Should construct ok from WKT\\n        gc1 = GEOSGeometry(gc_wkt)\\n\\n        # Should also construct ok from individual geometry arguments.\\n        gc2 = GeometryCollection(*tuple(g for g in gc1))\\n\\n        # And, they should be equal.\\n        self.assertEqual(gc1, gc2)\\n\\n    def test21_test_gdal(self):\\n        \\"Testing `ogr` and `srs` properties.\\"\\n        if not gdal.HAS_GDAL: return\\n        g1 = fromstr(\'POINT(5 23)\')\\n        self.assertEqual(True, isinstance(g1.ogr, gdal.OGRGeometry))\\n        self.assertEqual(g1.srs, None)\\n\\n        g2 = fromstr(\'LINESTRING(0 0, 5 5, 23 23)\', srid=4326)\\n        self.assertEqual(True, isinstance(g2.ogr, gdal.OGRGeometry))\\n        self.assertEqual(True, isinstance(g2.srs, gdal.SpatialReference))\\n        self.assertEqual(g2.hex, g2.ogr.hex)\\n        self.assertEqual(\'WGS 84\', g2.srs.name)\\n\\n    def test22_copy(self):\\n        \\"Testing use with the Python `copy` module.\\"\\n        import django.utils.copycompat as copy\\n        poly = GEOSGeometry(\'POLYGON((0 0, 0 23, 23 23, 23 0, 0 0), (5 5, 5 10, 10 10, 10 5, 5 5))\')\\n        cpy1 = copy.copy(poly)\\n        cpy2 = copy.deepcopy(poly)\\n        self.assertNotEqual(poly._ptr, cpy1._ptr)\\n        self.assertNotEqual(poly._ptr, cpy2._ptr)\\n\\n    def test23_transform(self):\\n        \\"Testing `transform` method.\\"\\n        if not gdal.HAS_GDAL: return\\n        orig = GEOSGeometry(\'POINT (-104.609 38.255)\', 4326)\\n        trans = GEOSGeometry(\'POINT (992385.4472045 481455.4944650)\', 2774)\\n\\n        # Using a srid, a SpatialReference object, and a CoordTransform object\\n        # for transformations.\\n        t1, t2, t3 = orig.clone(), orig.clone(), orig.clone()\\n        t1.transform(trans.srid)\\n        t2.transform(gdal.SpatialReference(\'EPSG:2774\'))\\n        ct = gdal.CoordTransform(gdal.SpatialReference(\'WGS84\'), gdal.SpatialReference(2774))\\n        t3.transform(ct)\\n\\n        # Testing use of the `clone` keyword.\\n        k1 = orig.clone()\\n        k2 = k1.transform(trans.srid, clone=True)\\n        self.assertEqual(k1, orig)\\n        self.assertNotEqual(k1, k2)\\n\\n        prec = 3\\n        for p in (t1, t2, t3, k2):\\n            self.assertAlmostEqual(trans.x, p.x, prec)\\n            self.assertAlmostEqual(trans.y, p.y, prec)\\n\\n    def test23_transform_noop(self):\\n        \\"\\"\\" Testing `transform` method (SRID match) \\"\\"\\"\\n        # transform() should no-op if source \\u0026 dest SRIDs match,\\n        # regardless of whether GDAL is available.\\n        if gdal.HAS_GDAL:\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', 4326)\\n            gt = g.tuple\\n            g.transform(4326)\\n            self.assertEqual(g.tuple, gt)\\n            self.assertEqual(g.srid, 4326)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', 4326)\\n            g1 = g.transform(4326, clone=True)\\n            self.assertEqual(g1.tuple, g.tuple)\\n            self.assertEqual(g1.srid, 4326)\\n            self.assert_(g1 is not g, \\"Clone didn\'t happen\\")\\n\\n        old_has_gdal = gdal.HAS_GDAL\\n        try:\\n            gdal.HAS_GDAL = False\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', 4326)\\n            gt = g.tuple\\n            g.transform(4326)\\n            self.assertEqual(g.tuple, gt)\\n            self.assertEqual(g.srid, 4326)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', 4326)\\n            g1 = g.transform(4326, clone=True)\\n            self.assertEqual(g1.tuple, g.tuple)\\n            self.assertEqual(g1.srid, 4326)\\n            self.assert_(g1 is not g, \\"Clone didn\'t happen\\")\\n        finally:\\n            gdal.HAS_GDAL = old_has_gdal\\n\\n    def test23_transform_nosrid(self):\\n        \\"\\"\\" Testing `transform` method (no SRID) \\"\\"\\"\\n        # raise a warning if SRID \\u003c0/None\\n        import warnings\\n        print \\"\\\\nBEGIN - expecting Warnings; safe to ignore.\\\\n\\"\\n\\n        # test for do-nothing behaviour.\\n        try:\\n            # Keeping line-noise down by only printing the relevant\\n            # warnings once.\\n            warnings.simplefilter(\'once\', UserWarning)\\n            warnings.simplefilter(\'once\', FutureWarning)    \\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', srid=None)\\n            g.transform(2774)\\n            self.assertEqual(g.tuple, (-104.609, 38.255))\\n            self.assertEqual(g.srid, None)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', srid=None)\\n            g1 = g.transform(2774, clone=True)\\n            self.assert_(g1 is None)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', srid=-1)\\n            g.transform(2774)\\n            self.assertEqual(g.tuple, (-104.609, 38.255))\\n            self.assertEqual(g.srid, -1)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', srid=-1)\\n            g1 = g.transform(2774, clone=True)\\n            self.assert_(g1 is None)\\n\\n        finally:\\n            warnings.simplefilter(\'default\', UserWarning)\\n            warnings.simplefilter(\'default\', FutureWarning)\\n\\n        print \\"\\\\nEND - expecting Warnings; safe to ignore.\\\\n\\"\\n\\n\\n        # test warning is raised\\n        try:\\n            warnings.simplefilter(\'error\', FutureWarning)\\n            warnings.simplefilter(\'ignore\', UserWarning)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', srid=None)\\n            self.assertRaises(FutureWarning, g.transform, 2774)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', srid=None)\\n            self.assertRaises(FutureWarning, g.transform, 2774, clone=True)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', srid=-1)\\n            self.assertRaises(FutureWarning, g.transform, 2774)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', srid=-1)\\n            self.assertRaises(FutureWarning, g.transform, 2774, clone=True)\\n        finally:\\n            warnings.simplefilter(\'default\', FutureWarning)\\n            warnings.simplefilter(\'default\', UserWarning)\\n\\n\\n    def test23_transform_nogdal(self):\\n        \\"\\"\\" Testing `transform` method (GDAL not available) \\"\\"\\"\\n        old_has_gdal = gdal.HAS_GDAL\\n        try:\\n            gdal.HAS_GDAL = False\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', 4326)\\n            self.assertRaises(GEOSException, g.transform, 2774)\\n\\n            g = GEOSGeometry(\'POINT (-104.609 38.255)\', 4326)\\n            self.assertRaises(GEOSException, g.transform, 2774, clone=True)\\n        finally:\\n            gdal.HAS_GDAL = old_has_gdal\\n\\n    def test24_extent(self):\\n        \\"Testing `extent` method.\\"\\n        # The xmin, ymin, xmax, ymax of the MultiPoint should be returned.\\n        mp = MultiPoint(Point(5, 23), Point(0, 0), Point(10, 50))\\n        self.assertEqual((0.0, 0.0, 10.0, 50.0), mp.extent)\\n        pnt = Point(5.23, 17.8)\\n        # Extent of points is just the point itself repeated.\\n        self.assertEqual((5.23, 17.8, 5.23, 17.8), pnt.extent)\\n        # Testing on the \'real world\' Polygon.\\n        poly = fromstr(self.geometries.polygons[3].wkt)\\n        ring = poly.shell\\n        x, y = ring.x, ring.y\\n        xmin, ymin = min(x), min(y)\\n        xmax, ymax = max(x), max(y)\\n        self.assertEqual((xmin, ymin, xmax, ymax), poly.extent)\\n\\n    def test25_pickle(self):\\n        \\"Testing pickling and unpickling support.\\"\\n        # Using both pickle and cPickle -- just \'cause.\\n        import pickle, cPickle\\n\\n        # Creating a list of test geometries for pickling,\\n        # and setting the SRID on some of them.\\n        def get_geoms(lst, srid=None):\\n            return [GEOSGeometry(tg.wkt, srid) for tg in lst]\\n        tgeoms = get_geoms(self.geometries.points)\\n        tgeoms.extend(get_geoms(self.geometries.multilinestrings, 4326))\\n        tgeoms.extend(get_geoms(self.geometries.polygons, 3084))\\n        tgeoms.extend(get_geoms(self.geometries.multipolygons, 900913))\\n\\n        # The SRID won\'t be exported in GEOS 3.0 release candidates.\\n        no_srid = self.null_srid == -1\\n        for geom in tgeoms:\\n            s1, s2 = cPickle.dumps(geom), pickle.dumps(geom)\\n            g1, g2 = cPickle.loads(s1), pickle.loads(s2)\\n            for tmpg in (g1, g2):\\n                self.assertEqual(geom, tmpg)\\n                if not no_srid: self.assertEqual(geom.srid, tmpg.srid)\\n\\n    def test26_prepared(self):\\n        \\"Testing PreparedGeometry support.\\"\\n        if not GEOS_PREPARE: return\\n        # Creating a simple multipolygon and getting a prepared version.\\n        mpoly = GEOSGeometry(\'MULTIPOLYGON(((0 0,0 5,5 5,5 0,0 0)),((5 5,5 10,10 10,10 5,5 5)))\')\\n        prep = mpoly.prepared\\n\\n        # A set of test points.\\n        pnts = [Point(5, 5), Point(7.5, 7.5), Point(2.5, 7.5)]\\n        covers = [True, True, False] # No `covers` op for regular GEOS geoms.\\n        for pnt, c in zip(pnts, covers):\\n            # Results should be the same (but faster)\\n            self.assertEqual(mpoly.contains(pnt), prep.contains(pnt))\\n            self.assertEqual(mpoly.intersects(pnt), prep.intersects(pnt))\\n            self.assertEqual(c, prep.covers(pnt))\\n\\n    def test26_line_merge(self):\\n        \\"Testing line merge support\\"\\n        ref_geoms = (fromstr(\'LINESTRING(1 1, 1 1, 3 3)\'),\\n                     fromstr(\'MULTILINESTRING((1 1, 3 3), (3 3, 4 2))\'),\\n                     )\\n        ref_merged = (fromstr(\'LINESTRING(1 1, 3 3)\'),\\n                      fromstr(\'LINESTRING (1 1, 3 3, 4 2)\'),\\n                      )\\n        for geom, merged in zip(ref_geoms, ref_merged):\\n            self.assertEqual(merged, geom.merged)\\n\\n    def test27_valid_reason(self):\\n        \\"Testing IsValidReason support\\"\\n        # Skipping tests if GEOS \\u003c v3.1.\\n        if not GEOS_PREPARE: return\\n\\n        g = GEOSGeometry(\\"POINT(0 0)\\")\\n        self.assert_(g.valid)\\n        self.assert_(isinstance(g.valid_reason, basestring))\\n        self.assertEqual(g.valid_reason, \\"Valid Geometry\\")\\n\\n        print \\"\\\\nBEGIN - expecting GEOS_NOTICE; safe to ignore.\\\\n\\"\\n\\n        g = GEOSGeometry(\\"LINESTRING(0 0, 0 0)\\")\\n\\n        self.assert_(not g.valid)\\n        self.assert_(isinstance(g.valid_reason, basestring))\\n        self.assert_(g.valid_reason.startswith(\\"Too few points in geometry component\\"))\\n\\n        print \\"\\\\nEND - expecting GEOS_NOTICE; safe to ignore.\\\\n\\"\\n\\ndef suite():\\n    s = unittest.TestSuite()\\n    s.addTest(unittest.makeSuite(GEOSTest))\\n    return s\\n\\ndef run(verbosity=2):\\n    unittest.TextTestRunner(verbosity=verbosity).run(suite())\\n"}\n'
line: b'{"repo_name":"arifgursel/pyglet","ref":"refs/heads/master","path":"pyglet/gl/lib.py","content":"# ----------------------------------------------------------------------------\\n# pyglet\\n# Copyright (c) 2006-2008 Alex Holkner\\n# All rights reserved.\\n# \\n# Redistribution and use in source and binary forms, with or without\\n# modification, are permitted provided that the following conditions \\n# are met:\\n#\\n#  * Redistributions of source code must retain the above copyright\\n#    notice, this list of conditions and the following disclaimer.\\n#  * Redistributions in binary form must reproduce the above copyright \\n#    notice, this list of conditions and the following disclaimer in\\n#    the documentation and/or other materials provided with the\\n#    distribution.\\n#  * Neither the name of pyglet nor the names of its\\n#    contributors may be used to endorse or promote products\\n#    derived from this software without specific prior written\\n#    permission.\\n#\\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\\n# \\"AS IS\\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS\\n# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE\\n# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\\n# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\\n# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\n# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN\\n# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\\n# POSSIBILITY OF SUCH DAMAGE.\\n# ----------------------------------------------------------------------------\\n\\n\'\'\'\\n\'\'\'\\n\\n__docformat__ = \'restructuredtext\'\\n__version__ = \'$Id$\'\\n\\nimport ctypes\\n\\nimport pyglet\\n\\n__all__ = [\'link_GL\', \'link_GLU\', \'link_AGL\', \'link_GLX\', \'link_WGL\']\\n\\n_debug_gl = pyglet.options[\'debug_gl\']\\n_debug_gl_trace = pyglet.options[\'debug_gl_trace\']\\n_debug_gl_trace_args = pyglet.options[\'debug_gl_trace_args\']\\n\\nclass MissingFunctionException(Exception):\\n    def __init__(self, name, requires=None, suggestions=None):\\n        msg = \'%s is not exported by the available OpenGL driver.\' % name\\n        if requires:\\n            msg += \'  %s is required for this functionality.\' % requires\\n        if suggestions:\\n            msg += \'  Consider alternative(s) %s.\' % \', \'.join(suggestions)\\n        Exception.__init__(self, msg)\\n\\ndef missing_function(name, requires=None, suggestions=None):\\n    def MissingFunction(*args, **kwargs):\\n        raise MissingFunctionException(name, requires, suggestions)\\n    return MissingFunction\\n\\n_int_types = (ctypes.c_int16, ctypes.c_int32)\\nif hasattr(ctypes, \'c_int64\'):\\n    # Some builds of ctypes apparently do not have c_int64\\n    # defined; it\'s a pretty good bet that these builds do not\\n    # have 64-bit pointers.\\n    _int_types += (ctypes.c_int64,)\\nfor t in _int_types:\\n    if ctypes.sizeof(t) == ctypes.sizeof(ctypes.c_size_t):\\n        c_ptrdiff_t = t\\n\\nclass c_void(ctypes.Structure):\\n    # c_void_p is a buggy return type, converting to int, so\\n    # POINTER(None) == c_void_p is actually written as\\n    # POINTER(c_void), so it can be treated as a real pointer.\\n    _fields_ = [(\'dummy\', ctypes.c_int)]\\n\\nclass GLException(Exception):\\n    pass\\n\\ndef errcheck(result, func, arguments):\\n    if _debug_gl_trace:\\n        try:\\n            name = func.__name__\\n        except AttributeError:\\n            name = repr(func)\\n        if _debug_gl_trace_args:\\n            trace_args = \', \'.join([repr(arg)[:20] for arg in arguments])\\n            print \'%s(%s)\' % (name, trace_args)\\n        else:\\n            print name\\n\\n    from pyglet import gl\\n    context = gl.current_context\\n    if not context:\\n        raise GLException(\'No GL context; create a Window first\')\\n    if not context._gl_begin:\\n        error = gl.glGetError()\\n        if error:\\n            msg = ctypes.cast(gl.gluErrorString(error), ctypes.c_char_p).value\\n            raise GLException(msg)\\n        return result\\n\\ndef errcheck_glbegin(result, func, arguments):\\n    from pyglet import gl\\n    context = gl.current_context\\n    if not context:\\n        raise GLException(\'No GL context; create a Window first\')\\n    context._gl_begin = True\\n    return result\\n\\ndef errcheck_glend(result, func, arguments):\\n    from pyglet import gl\\n    context = gl.current_context\\n    if not context:\\n        raise GLException(\'No GL context; create a Window first\')\\n    context._gl_begin = False\\n    return errcheck(result, func, arguments)\\n\\ndef decorate_function(func, name):\\n    if _debug_gl:\\n        if name == \'glBegin\':\\n            func.errcheck = errcheck_glbegin\\n        elif name == \'glEnd\':\\n            func.errcheck = errcheck_glend\\n        elif name not in (\'glGetError\', \'gluErrorString\') and \\\\\\n             name[:3] not in (\'glX\', \'agl\', \'wgl\'):\\n            func.errcheck = errcheck\\n\\nlink_AGL = None\\nlink_GLX = None\\nlink_WGL = None\\n\\nif pyglet.compat_platform in (\'win32\', \'cygwin\'):\\n    from pyglet.gl.lib_wgl import link_GL, link_GLU, link_WGL\\nelif pyglet.compat_platform == \'darwin\':\\n    from pyglet.gl.lib_agl import link_GL, link_GLU, link_AGL\\nelse:\\n    from pyglet.gl.lib_glx import link_GL, link_GLU, link_GLX\\n\\n"}\n'
line: b'{"repo_name":"tangyiyong/odoo","ref":"refs/heads/8.0","path":"addons/mrp/wizard/stock_move.py","content":"# -*- coding: utf-8 -*-\\n##############################################################################\\n#\\n#    OpenERP, Open Source Management Solution\\n#    Copyright (C) 2004-2010 Tiny SPRL (\\u003chttp://tiny.be\\u003e).\\n#\\n#    This program is free software: you can redistribute it and/or modify\\n#    it under the terms of the GNU Affero General Public License as\\n#    published by the Free Software Foundation, either version 3 of the\\n#    License, or (at your option) any later version.\\n#\\n#    This program is distributed in the hope that it will be useful,\\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#    GNU Affero General Public License for more details.\\n#\\n#    You should have received a copy of the GNU Affero General Public License\\n#    along with this program.  If not, see \\u003chttp://www.gnu.org/licenses/\\u003e.\\n#\\n##############################################################################\\n\\nfrom openerp.osv import fields, osv\\nfrom openerp.tools import float_compare\\nfrom openerp.tools.translate import _\\nimport openerp.addons.decimal_precision as dp\\n\\nclass stock_move_consume(osv.osv_memory):\\n    _name = \\"stock.move.consume\\"\\n    _description = \\"Consume Products\\"\\n\\n    _columns = {\\n        \'product_id\': fields.many2one(\'product.product\', \'Product\', required=True, select=True),\\n        \'product_qty\': fields.float(\'Quantity\', digits_compute=dp.get_precision(\'Product Unit of Measure\'), required=True),\\n        \'product_uom\': fields.many2one(\'product.uom\', \'Product Unit of Measure\', required=True),\\n        \'location_id\': fields.many2one(\'stock.location\', \'Location\', required=True),\\n        \'restrict_lot_id\': fields.many2one(\'stock.production.lot\', \'Lot\'),\\n    }\\n\\n    #TOFIX: product_uom should not have different category of default UOM of product. Qty should be convert into UOM of original move line before going in consume and scrap\\n    def default_get(self, cr, uid, fields, context=None):\\n        if context is None:\\n            context = {}\\n        res = super(stock_move_consume, self).default_get(cr, uid, fields, context=context)\\n        move = self.pool.get(\'stock.move\').browse(cr, uid, context[\'active_id\'], context=context)\\n        if \'product_id\' in fields:\\n            res.update({\'product_id\': move.product_id.id})\\n        if \'product_uom\' in fields:\\n            res.update({\'product_uom\': move.product_uom.id})\\n        if \'product_qty\' in fields:\\n            res.update({\'product_qty\': move.product_uom_qty})\\n        if \'location_id\' in fields:\\n            res.update({\'location_id\': move.location_id.id})\\n        return res\\n\\n\\n\\n    def do_move_consume(self, cr, uid, ids, context=None):\\n        if context is None:\\n            context = {}\\n        move_obj = self.pool.get(\'stock.move\')\\n        uom_obj = self.pool.get(\'product.uom\')\\n        production_obj = self.pool.get(\'mrp.production\')\\n        move_ids = context[\'active_ids\']\\n        move = move_obj.browse(cr, uid, move_ids[0], context=context)\\n        production_id = move.raw_material_production_id.id\\n        production = production_obj.browse(cr, uid, production_id, context=context)\\n        precision = self.pool[\'decimal.precision\'].precision_get(cr, uid, \'Product Unit of Measure\')\\n\\n        for data in self.browse(cr, uid, ids, context=context):\\n            qty = uom_obj._compute_qty(cr, uid, data[\'product_uom\'].id, data.product_qty, data.product_id.uom_id.id)\\n            remaining_qty = move.product_qty - qty\\n            #check for product quantity is less than previously planned\\n            if float_compare(remaining_qty, 0, precision_digits=precision) \\u003e= 0:\\n                move_obj.action_consume(cr, uid, move_ids, qty, data.location_id.id, restrict_lot_id=data.restrict_lot_id.id, context=context)\\n            else:\\n                consumed_qty = min(move.product_qty, qty)\\n                new_moves = move_obj.action_consume(cr, uid, move_ids, consumed_qty, data.location_id.id, restrict_lot_id=data.restrict_lot_id.id, context=context)\\n                #consumed more in wizard than previously planned\\n                extra_more_qty = qty - consumed_qty\\n                #create new line for a remaining qty of the product\\n                extra_move_id = production_obj._make_consume_line_from_data(cr, uid, production, data.product_id, data.product_id.uom_id.id, extra_more_qty, False, 0, context=context)\\n                move_obj.write(cr, uid, [extra_move_id], {\'restrict_lot_id\': data.restrict_lot_id.id}, context=context)\\n                move_obj.action_done(cr, uid, [extra_move_id], context=context)\\n\\n        return {\'type\': \'ir.actions.act_window_close\'}"}\n'
line: b'{"repo_name":"txemi/ansible","ref":"refs/heads/devel","path":"test/units/parsing/yaml/test_dumper.py","content":"# coding: utf-8\\n# This file is part of Ansible\\n#\\n# Ansible is free software: you can redistribute it and/or modify\\n# it under the terms of the GNU General Public License as published by\\n# the Free Software Foundation, either version 3 of the License, or\\n# (at your option) any later version.\\n#\\n# Ansible is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU General Public License for more details.\\n#\\n# You should have received a copy of the GNU General Public License\\n# along with Ansible.  If not, see \\u003chttp://www.gnu.org/licenses/\\u003e.\\n\\n# Make coding more python3-ish\\nfrom __future__ import (absolute_import, division, print_function)\\n__metaclass__ = type\\n\\nimport io\\nimport yaml\\n\\ntry:\\n    from _yaml import ParserError\\nexcept ImportError:\\n    from yaml.parser import ParserError\\n\\nfrom ansible.parsing.yaml import dumper\\nfrom ansible.parsing.yaml.loader import AnsibleLoader\\n\\nfrom ansible.compat.tests import unittest\\nfrom ansible.parsing.yaml import objects\\nfrom ansible.parsing import vault\\n\\nfrom units.mock.yaml_helper import YamlTestUtils\\n\\nclass TestAnsibleDumper(unittest.TestCase, YamlTestUtils):\\n    def setUp(self):\\n        self.vault_password = \\"hunter42\\"\\n        self.good_vault = vault.VaultLib(self.vault_password)\\n        self.vault = self.good_vault\\n        self.stream = self._build_stream()\\n        self.dumper = dumper.AnsibleDumper\\n\\n    def _build_stream(self,yaml_text=None):\\n        text = yaml_text or u\'\'\\n        stream = io.StringIO(text)\\n        return stream\\n\\n    def _loader(self, stream):\\n        return AnsibleLoader(stream, vault_password=self.vault_password)\\n\\n    def test(self):\\n        plaintext = \'This is a string we are going to encrypt.\'\\n        avu = objects.AnsibleVaultEncryptedUnicode.from_plaintext(plaintext, vault=self.vault)\\n\\n        yaml_out = self._dump_string(avu, dumper=self.dumper)\\n        stream = self._build_stream(yaml_out)\\n        loader = self._loader(stream)\\n\\n        data_from_yaml = loader.get_single_data()\\n\\n        self.assertEquals(plaintext, data_from_yaml.data)\\n"}\n'
line: b'{"repo_name":"rjschwei/azure-sdk-for-python","ref":"refs/heads/master","path":"azure-mgmt-logic/azure/mgmt/logic/models/generate_upgraded_definition_parameters.py","content":"# coding=utf-8\\n# --------------------------------------------------------------------------\\n# Copyright (c) Microsoft Corporation. All rights reserved.\\n# Licensed under the MIT License. See License.txt in the project root for\\n# license information.\\n#\\n# Code generated by Microsoft (R) AutoRest Code Generator.\\n# Changes may cause incorrect behavior and will be lost if the code is\\n# regenerated.\\n# --------------------------------------------------------------------------\\n\\nfrom msrest.serialization import Model\\n\\n\\nclass GenerateUpgradedDefinitionParameters(Model):\\n    \\"\\"\\"GenerateUpgradedDefinitionParameters.\\n\\n    :param target_schema_version: The target schema version.\\n    :type target_schema_version: str\\n    \\"\\"\\" \\n\\n    _attribute_map = {\\n        \'target_schema_version\': {\'key\': \'targetSchemaVersion\', \'type\': \'str\'},\\n    }\\n\\n    def __init__(self, target_schema_version=None):\\n        self.target_schema_version = target_schema_version\\n"}\n'
line: b'{"repo_name":"voidcc/PCTRL","ref":"refs/heads/master","path":"tests/unit/lib/mock_socket_test.py","content":"#!/usr/bin/env python\\n#\\n# Copyright 2011-2012 Andreas Wundsam\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at:\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\nimport unittest\\nimport sys\\nimport os.path\\nfrom copy import copy\\n\\nsys.path.append(os.path.dirname(__file__) + \\"/../../..\\")\\n\\nfrom pox.lib.mock_socket import MockSocket\\n\\nclass MockSocketTest(unittest.TestCase):\\n  def setUp(self):\\n    pass\\n\\n  def test_simple_send(self):\\n    (a, b) = MockSocket.pair()\\n    a.send(\\"Hallo\\")\\n    self.assertEquals(b.recv(), \\"Hallo\\")\\n    b.send(\\"Servus\\")\\n    self.assertEquals(a.recv(), \\"Servus\\")\\n\\n  def test_ready_to_recv(self):\\n    (a, b) = MockSocket.pair()\\n    a.send(\\"Hallo\\")\\n    self.assertFalse(a.ready_to_recv())\\n    self.assertTrue(b.ready_to_recv())\\n    self.assertEquals(b.recv(), \\"Hallo\\")\\n    self.assertFalse(b.ready_to_recv())\\n\\n    self.assertFalse(a.ready_to_recv())\\n    b.send(\\"Servus\\")\\n    self.assertTrue(a.ready_to_recv())\\n    self.assertEquals(a.recv(), \\"Servus\\")\\n    self.assertFalse(a.ready_to_recv())\\n\\n  def test_on_ready_to_recv(self):\\n    self.seen_size = -1\\n    self.called = 0\\n    def ready(socket, size):\\n      self.called += 1\\n      self.seen_size = size\\n\\n    (a, b) = MockSocket.pair()\\n    b.set_on_ready_to_recv(ready)\\n    self.assertEquals(self.called, 0)\\n    a.send(\\"Hallo\\")\\n    self.assertEquals(self.called, 1)\\n    self.assertEquals(self.seen_size, 5)\\n\\n    # check that it doesn\'t get called on the other sockets data\\n    b.send(\\"Huhu\\")\\n    self.assertEquals(self.called, 1)\\n\\n  def test_empty_recv(self):\\n    \\"\\"\\" test_empty_recv: Check that empty reads on socket return \\"\\"\\n       Note that this is actually non-sockety behavior and should probably be changed. This\\n       test documents it as intended for now, though\\n    \\"\\"\\"\\n    (a, b) = MockSocket.pair()\\n    self.assertEquals(a.recv(), \\"\\")\\n\\nif __name__ == \'__main__\':\\n  unittest.main()\\n"}\n'
line: b'{"repo_name":"mluo613/osf.io","ref":"refs/heads/develop","path":"scripts/update_comments.py","content":"\\"\\"\\"\\nUpdate User.comments_viewed_timestamp field.\\n\\"\\"\\"\\nimport logging\\nimport sys\\n\\nfrom modularodm import Q\\n\\nfrom framework.auth.core import User\\nfrom framework.transactions.context import TokuTransaction\\n\\nfrom website.models import Comment\\nfrom website.app import init_app\\nfrom scripts import utils as script_utils\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\ndef main():\\n    update_comments_viewed_timestamp()\\n\\n\\ndef update_comments_viewed_timestamp():\\n    users = User.find(Q(\'comments_viewed_timestamp\', \'ne\', None) \\u0026 Q(\'comments_viewed_timestamp\', \'ne\', {}))\\n    for user in users:\\n        if user.comments_viewed_timestamp:\\n            timestamps = {}\\n            for node_id in user.comments_viewed_timestamp:\\n                node_timestamps = user.comments_viewed_timestamp[node_id]\\n\\n                # node timestamp\\n                if node_timestamps.get(\'node\', None):\\n                    timestamps[node_id] = node_timestamps[\'node\']\\n\\n                # file timestamps\\n                file_timestamps = node_timestamps.get(\'files\', None)\\n                if file_timestamps:\\n                    for file_id in file_timestamps:\\n                        timestamps[file_id] = file_timestamps[file_id]\\n\\n            user.comments_viewed_timestamp = timestamps\\n            user.save()\\n            logger.info(\'Migrated timestamp for user {0}\'.format(user._id))\\n\\n\\nif __name__ == \'__main__\':\\n    dry = \'--dry\' in sys.argv\\n    if not dry:\\n        script_utils.add_file_logger(logger, __file__)\\n    init_app(routes=False, set_backends=True)\\n    with TokuTransaction():\\n        main()\\n        if dry:\\n            raise Exception(\'Dry Run -- Aborting Transaction\')\\n"}\n'
line: b'{"repo_name":"travisdesell/csg_boinc","ref":"refs/heads/master","path":"test/cgiserver.py","content":"#!/usr/bin/env python\\n\\n# $Id$\\n# cgi/php web server\\n\\nimport BaseHTTPServer, CGIHTTPServer\\nimport sys, os, urllib, select\\nimport random, time                     # XXX\\n\\nphp_path = None\\npossible_php_paths = [ \'/usr/lib/cgi-bin/php4\',\\n                       \'PROGRAM_PATH/fake_php.py\' ]\\ndef setup_php(program_path):\\n    global php_path\\n    for p in possible_php_paths:\\n        p = p.replace(\'PROGRAM_PATH\', program_path)\\n        if os.path.exists(p):\\n            php_path = p\\n            return\\n    raise Exception(\\"No php binary found - not even fake_php.py (program_path=%s) !\\"%program_path)\\n\\nclass PHPHTTPRequestHandler(CGIHTTPServer.CGIHTTPRequestHandler):\\n    def is_cgi(self):\\n        if os.path.split(self.path)[1] == \'\':\\n            index_php = os.path.join(self.path, \'index.php\')\\n            if os.path.exists(self.translate_path(index_php)):\\n                self.path = index_php\\n        if self.path.find(\'.php\') != -1:\\n            self.cgi_info = os.path.split(self.path)\\n            return True\\n\\n        for p in self.cgi_directories:\\n            p = os.path.join(p,\'\')\\n            if self.path.startswith(p):\\n                self.cgi_info = os.path.split(self.path)\\n                return True\\n        return False\\n\\n    def run_cgi(self):\\n        \\"\\"\\"Execute a CGI script.\\"\\"\\"\\n        dir, rest = self.cgi_info\\n        i = rest.rfind(\'?\')\\n        if i \\u003e= 0:\\n            rest, query = rest[:i], rest[i+1:]\\n        else:\\n            query = \'\'\\n        i = rest.find(\'/\')\\n        if i \\u003e= 0:\\n            script, rest = rest[:i], rest[i:]\\n        else:\\n            script, rest = rest, \'\'\\n        scriptname = dir + \'/\' + script\\n        is_php = script.endswith(\'.php\')\\n        # print \\"#### cgi_info=%s,dir=%s,rest=%s,script=%s,scriptname=%s,is_php=%s\\"%(self.cgi_info,dir,rest,script,scriptname,is_php)\\n        if is_php:\\n            if not php_path: raise Exception(\'php_path not set\')\\n            scriptfile = php_path\\n            sourcefile = self.translate_path(scriptname)\\n        else:\\n            scriptfile = self.translate_path(scriptname)\\n        if not os.path.exists(scriptfile):\\n            self.send_error(404, \\"No such CGI script (%s)\\" % `scriptname`)\\n            return\\n        if not os.path.isfile(scriptfile):\\n            self.send_error(403, \\"CGI script is not a plain file (%s)\\" %\\n                            `scriptname`)\\n            return\\n        ispy = self.is_python(scriptname)\\n        if not ispy:\\n            if not (self.have_fork or self.have_popen2 or self.have_popen3):\\n                self.send_error(403, \\"CGI script is not a Python script (%s)\\" %\\n                                `scriptname`)\\n                return\\n            if not self.is_executable(scriptfile):\\n                self.send_error(403, \\"CGI script is not executable (%s)\\" %\\n                                `scriptname`)\\n                return\\n\\n        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html\\n        # XXX Much of the following could be prepared ahead of time!\\n        env = {}\\n        env[\'DOCUMENT_ROOT\'] = os.getcwd()\\n        env[\'SERVER_SOFTWARE\'] = self.version_string()\\n        env[\'SERVER_NAME\'] = self.server.server_name\\n        env[\'GATEWAY_INTERFACE\'] = \'CGI/1.1\'\\n        env[\'SERVER_PROTOCOL\'] = self.protocol_version\\n        env[\'SERVER_PORT\'] = str(self.server.server_port)\\n        env[\'REQUEST_METHOD\'] = self.command\\n        uqrest = urllib.unquote(self.cgi_info[1])\\n        env[\'REQUEST_URI\'] = self.path\\n        # env[\'PATH_INFO\'] = uqrest\\n        # env[\'PATH_TRANSLATED\'] = self.translate_path(uqrest)\\n        env[\'SCRIPT_NAME\'] = scriptname\\n        env[\'SCRIPT_FILENAME\'] = self.translate_path(scriptname)\\n        if query:\\n            env[\'QUERY_STRING\'] = query\\n        host = self.address_string()\\n        if host != self.client_address[0]:\\n            env[\'REMOTE_HOST\'] = host\\n        env[\'REMOTE_ADDR\'] = self.client_address[0]\\n        env[\'REDIRECT_STATUS\'] = \'1\'      # for php\\n        # XXX AUTH_TYPE\\n        # XXX REMOTE_USER\\n        # XXX REMOTE_IDENT\\n        if self.headers.typeheader is None:\\n            env[\'CONTENT_TYPE\'] = self.headers.type\\n        else:\\n            env[\'CONTENT_TYPE\'] = self.headers.typeheader\\n        length = self.headers.getheader(\'content-length\')\\n        if length:\\n            env[\'CONTENT_LENGTH\'] = length\\n        accept = []\\n        for line in self.headers.getallmatchingheaders(\'accept\'):\\n            if line[:1] in \\"\\\\t\\\\n\\\\r \\":\\n                accept.append(line.strip())\\n            else:\\n                accept = accept + line[7:].split(\',\')\\n        env[\'HTTP_ACCEPT\'] = \',\'.join(accept)\\n        ua = self.headers.getheader(\'user-agent\')\\n        if ua:\\n            env[\'HTTP_USER_AGENT\'] = ua\\n        co = filter(None, self.headers.getheaders(\'cookie\'))\\n        if co:\\n            env[\'HTTP_COOKIE\'] = \', \'.join(co)\\n        # XXX Other HTTP_* headers\\n        if not self.have_fork:\\n            # Since we\'re setting the env in the parent, provide empty\\n            # values to override previously set values\\n            for k in (\'QUERY_STRING\', \'REMOTE_HOST\', \'CONTENT_LENGTH\',\\n                      \'HTTP_USER_AGENT\', \'HTTP_COOKIE\'):\\n                env.setdefault(k, \\"\\")\\n        os.environ.update(env)\\n\\n        self.send_response(200, \\"Script output follows\\")\\n\\n        decoded_query = query.replace(\'+\', \' \')\\n\\n        if self.have_fork:\\n            # Unix -- fork as we should\\n            if is_php:\\n                args = [php_path, sourcefile]\\n            else:\\n                args = [script]\\n            if \'=\' not in decoded_query:\\n                args.append(decoded_query)\\n            self.wfile.flush() # Always flush before forking\\n            pid = os.fork()\\n            if pid != 0:\\n                # Parent\\n                pid, sts = os.waitpid(pid, 0)\\n                # throw away additional data [see bug #427345]\\n                while select.select([self.rfile], [], [], 0)[0]:\\n                    try:\\n                        if not self.rfile.read(1):\\n                            break\\n                    except:\\n                        break\\n                if sts:\\n                    self.log_error(\\"CGI script exit status %#x\\", sts)\\n                return\\n            # Child\\n            try:\\n                if 0:\\n                    time.sleep(.1)\\n                    fn = \'/tmp/a%d\'%random.randint(1000,10000)\\n                    f = open(fn, \'w\')\\n                    s = \'\'\\n                    while select.select([self.rfile], [], [], 0)[0]:\\n                        try:\\n                            c = self.rfile.read(1)\\n                            if not c:\\n                                break\\n                            s += c\\n                        except:\\n                            break\\n                    print \'### input:\', repr(s)\\n                    print \\u003e\\u003ef, s\\n                    f.close()\\n                    self.rfile = open(fn, \'r\')\\n                os.dup2(self.rfile.fileno(), 0)\\n                os.dup2(self.wfile.fileno(), 1)\\n                os.chdir(self.translate_path(dir)) # KC\\n                os.execve(scriptfile, args, os.environ)\\n            except:\\n                self.server.handle_error(self.request, self.client_address)\\n                os._exit(127)\\n\\n        else:\\n            raise SystemExit(\'need fork()\')\\n\\ndef serve(bind=\'localhost\', port=8000, handler=PHPHTTPRequestHandler):\\n    httpd = BaseHTTPServer.HTTPServer((bind,port), handler)\\n    httpd.serve_forever()\\n\\nif __name__ == \'__main__\':\\n    setup_php(os.path.realpath(os.path.dirname(sys.argv[0])))\\n    serve()\\n"}\n'
input_path: /home/gcloud/TransCoder/data/test_dataset/python/python.004.json.gz
language: python
output_path: /home/gcloud/TransCoder/data/test_dataset/python/python.004.with_comments.tok
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/Decimal-to-binary.py","content":"def convert_to_binary(n): \\n\\n      if(n > 1):\\n        convert_to_binary(n//2)\\n  \\n      print(n % 2, end = \'\')\\n\\nconvert_to_binary(52)\\n"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/Factorial.py","content":"def factorial(n):\\n    factorial = 1\\n    # check is the number is negative, positive or zero\\n    if(n < 0):\\n        print(\'Sorry, factorial does not exist for negative numbers\')\\n    elif(n == 0):\\n        print(\'The factorial of 0 is 1\')\\n    else:\\n        for i in range(1,n):\\n            factorial = factorial * i\\n        print(\'The factorial of {} is {}\'.format(n,factorial))\\n\\nfactorial(4)\\n"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/Fibonacci.py","content":"def fibonacci(n): \\n    \\n    fibonacci_seq = [None for i in range(n)]\\n    fibonacci_seq[0] = fibonacci_seq[1] = 1\\n    for i in range(2,n):\\n        \\n           fibonacci_seq[i] = fibonacci_seq[i - 2] + fibonacci_seq[i - 1]\\n    print(\'First {} Fibonacci numbers:\'.format(n))\\n    print(fibonacci_seq)\\n\\n\\nfibonacci(10)\\n"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/Max-Min-of-Vector.py","content":"\\ndef max_min_vector(n):\\n\\n    print(\'Original vector:\')\\n    print(n)   \\n    print(\'Maximum value of the said vector:\',max(n))\\n    print(\'Minimum value of the said vector:\',min(n))\\n\\n\\nmax_min_vector([10, 20, 30, 40, 50, 60])\\n"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/Odd-or-Even.py","content":"\\ndef odd_or_even(n):\\n    num = n\\n    \\n    if((num % 2) == 0):\\n        print(\'{} is Even\'.format(num))\\n    else:\\n        print(\'{} is Odd\'.format(num))\\n\\n\\nodd_or_even(4)\\n"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/Prime-Numbers.py","content":"\\ndef prime_numbers(n):\\n    \\n     prime_nums = [] \\n     if (n >= 2):\\n        \\n        for i in range(2,n):\\n             for j in range(2,i):\\n                  \\n                  if i%j == 0:\\n                      break\\n             else:\\n                 prime_nums.append(i)\\n                \\n                \\n\\n        return(prime_nums)\\n     else: \\n           print(\'Input number should be at least 2.\')\\n"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/Read-csv-file.py.","content":"\\nimport pandas as pd\\n\\ndef read_csv_file():\\n    \\n    movie_data = pd.read_csv(\'movies.csv\')\\n    print(\'Content of the .csv file:\')\\n    print(movie_data)\\n\\n\\nread_csv_file()\\n"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/SumMeanProduct.py","content":"\\nimport numpy as np\\ndef sum(nums):\\n    sum = 0\\n    for x in nums:\\n        sum = sum + x\\n    print(sum)\\n\\ndef avg(nums):\\n   print(np.mean(nums))\\n\\ndef product(nums):\\n    print(np.prod(nums))\\n\\nlist1 = [3,4,5]\\n\\nsum(list1)\\navg(list1)\\nproduct(list1)"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/add1.py","content":"\\ndef add1(num):\\n    print(num + 1)\\n    \\nadd1(2)\\n"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/add2nums.py","content":"\\ndef add2nums(a, b):\\n    print (a+b)\\n\\nadd2nums(2, 3)\\n"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/areaSquare.py","content":"\\ndef areaSquare(side):\\n    if(side <= 0 ):\\n        print \'Invalid measurement\'\\n    else:\\n        print \'Area of the square is : \', side*side\\n\\nareaSquare(4)\\n"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/armstrong.py","content":"\\nnum = int(input(\'Enter a number: \'))\\n\\nsum = 0\\n\\ntemp = num\\nwhile temp > 0:\\n   digit = temp % 10\\n   sum += digit ** 3\\n   temp //= 10\\n\\nif num == sum:\\n   print(num,\'is an Armstrong number\')\\nelse:\\n   print(num,\'is not an Armstrong number\')\\n"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/describe.py","content":"\\nimport pandas as pd\\ndf = pd.read_csv(\'nba_2013.csv\')\\ndf.describe()\\n"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/divisibleby10.py","content":"\\ndef divisibleby10(n):\\n    if(n%10 == 0):\\n        print(\'True\')\\n    else:\\n        print(\'False\')\\n\\n\\ndivisibleby10(8)\\n"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/factors.py","content":"\\ndef print_factors(x):\\n   print(\'The factors of\',x,\'are:\')\\n   for i in range(1, x + 1):\\n       if x % i == 0:\\n           print(i)\\n\\nnum = 320\\n\\nprint_factors(num)\\n"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/integerType.py","content":"\\ndef integerType(num):\\n    if (num > 0):\\n        print(\'Number is Postive integer\')\\n    if(num < 0):\\n        print(\'Number is Negative integer\')\\n    if(num == 0):\\n        print(\'Number is Zero\')\\n"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/lcm.py","content":"\\ndef compute_lcm(x, y):\\n\\n   # choose the greater number\\n   if x > y:\\n       greater = x\\n   else:\\n       greater = y\\n\\n   while(True):\\n       if((greater % x == 0) and (greater % y == 0)):\\n           lcm = greater\\n           break\\n       greater += 1\\n\\n   return lcm\\n"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/lengthOfVector.py","content":"\\ndef lengthOfVector(nums):\\n    count = 0\\n    for x in nums:\\n        count = count+1\\n    \\n    print(count)\\n"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/palindrome.py","content":"\\ndef palindrome(num):\\n    temp = num\\n    rev = 0\\n    while(num > 0):\\n        dig = num % 10\\n        rev = rev * 10 + dig\\n        num = num // 10\\n    if(temp == rev):\\n        print(\'Number is palindrome\')\\n    else: \\n        print(\'Number is not palidrome\')\\n"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/perfect.py","content":"\\ndef perfect_number(n):\\n    sum = 0\\n    for x in range(1, n):\\n        if n % x == 0:\\n            sum += x\\n    return sum == n\\n"}\n'
line: b'{"repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/sortvector.py","content":"\\ndef sortVector(nums):\\n    nums.sort()\\n    print(nums)\\n"}\n'
input_path: /home/gcloud/TransCoder/data/test_dataset/python/python.003.json.gz
language: python
output_path: /home/gcloud/TransCoder/data/test_dataset/python/python.003.with_comments.tok
line: b'{ "repo_name": "skosukhin/spack", "ref": "refs/heads/esiwace", "path": "var/spack/repos/builtin/packages/perl-sub-install/package.py", "content": "##############################################################################\\n# Copyright (c) 2013-2017, Lawrence Livermore National Security, LLC.\\n# Produced at the Lawrence Livermore National Laboratory.\\n#\\n# This file is part of Spack.\\n# Created by Todd Gamblin, tgamblin@llnl.gov, All rights reserved.\\n# LLNL-CODE-647188\\n#\\n# For details, see https://github.com/spack/spack\\n# Please also see the NOTICE and LICENSE files for our notice and the LGPL.\\n#\\n# This program is free software; you can redistribute it and/or modify\\n# it under the terms of the GNU Lesser General Public License (as\\n# published by the Free Software Foundation) version 2.1, February 1999.\\n#\\n# This program is distributed in the hope that it will be useful, but\\n# WITHOUT ANY WARRANTY; without even the IMPLIED WARRANTY OF\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the terms and\\n# conditions of the GNU Lesser General Public License for more details.\\n#\\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this program; if not, write to the Free Software\\n# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA\\n##############################################################################\\nfrom spack import *\\n\\n\\nclass PerlSubInstall(PerlPackage):\\n    \\"\\"\\"Install subroutines into packages easily\\"\\"\\"\\n\\n    homepage = \\"http://search.cpan.org/~rjbs/Sub-Install-0.928/lib/Sub/Install.pm\\"\\n    url      = \\"http://search.cpan.org/CPAN/authors/id/R/RJ/RJBS/Sub-Install-0.928.tar.gz\\"\\n\\n    version(\'0.928\', \'e1ce4f9cb6b2f6b8778b036c31afa5ab\')\\n" } \n'
line: b'{ "repo_name": "maggienj/ActiveData", "ref": "refs/heads/es5", "path": "jx_elasticsearch/es09/setop.py", "content": "# encoding: utf-8\\n#\\n#\\n# This Source Code Form is subject to the terms of the Mozilla Public\\n# License, v. 2.0. If a copy of the MPL was not distributed with this file,\\n# You can obtain one at http:# mozilla.org/MPL/2.0/.\\n#\\n# Author: Kyle Lahnakoski (kyle@lahnakoski.com)\\n#\\nfrom __future__ import absolute_import\\nfrom __future__ import division\\nfrom __future__ import unicode_literals\\n\\nfrom collections import Mapping\\n\\nfrom jx_elasticsearch import es09\\nfrom jx_python import domains\\nfrom mo_dots import coalesce, split_field, Data, wrap\\nfrom mo_dots import listwrap, unwrap\\nfrom mo_logs import Log\\nfrom mo_math import AND, SUM, OR\\n\\nfrom jx_base.expressions import TRUE_FILTER, jx_expression, Variable\\nfrom jx_base.queries import is_variable_name\\nfrom jx_elasticsearch.es09.expressions import unpack_terms\\nfrom jx_elasticsearch.es09.util import aggregates\\nfrom jx_elasticsearch.es52.expressions import simplify_esfilter\\nfrom jx_python.containers.cube import Cube\\nfrom mo_collections.matrix import Matrix\\nfrom mo_dots.lists import FlatList\\n\\n\\ndef is_fieldop(query):\\n    # THESE SMOOTH EDGES REQUIRE ALL DATA (SETOP)\\n\\n    select = listwrap(query.select)\\n    if not query.edges:\\n        isDeep = len(split_field(query.frum.name)) > 1  # LOOKING INTO NESTED WILL REQUIRE A SCRIPT\\n        isSimple = AND(s.value != None and (s.value == \\"*\\" or is_variable_name(s.value)) for s in select)\\n        noAgg = AND(s.aggregate == \\"none\\" for s in select)\\n\\n        if not isDeep and isSimple and noAgg:\\n            return True\\n    else:\\n        isSmooth = AND((e.domain.type in domains.ALGEBRAIC and e.domain.interval == \\"none\\") for e in query.edges)\\n        if isSmooth:\\n            return True\\n\\n    return False\\n\\n\\ndef es_fieldop(es, query):\\n    FromES = es09.util.build_es_query(query)\\n    select = listwrap(query.select)\\n    FromES.query = {\\n        \\"bool\\": {\\n            \\"query\\": {\\n                \\"match_all\\": {}\\n          }\\n            \\"filter\\": simplify_esfilter(jx_expression(query.where).to_esfilter())\\n      }\\n  }\\n    FromES.size = coalesce(query.limit, 200000)\\n    FromES.fields = FlatList()\\n    for s in select.value:\\n        if s == \\"*\\":\\n            FromES.fields = None\\n        elif isinstance(s, list):\\n            FromES.fields.extend(s)\\n        elif isinstance(s, Mapping):\\n            FromES.fields.extend(s.values())\\n        else:\\n            FromES.fields.append(s)\\n    FromES.sort = [{s.field: \\"asc\\" if s.sort >= 0 else \\"desc\\"} for s in query.sort]\\n\\n    data = es09.util.post(es, FromES, query.limit)\\n\\n    T = data.hits.hits\\n    matricies = {}\\n    for s in select:\\n        if s.value == \\"*\\":\\n            matricies[s.name] = Matrix.wrap([t._source for t in T])\\n        elif isinstance(s.value, Mapping):\\n            # for k, v in s.value.items():\\n            #     matricies[join_field(split_field(s.name)+[k])] = Matrix.wrap([unwrap(t.fields)[v] for t in T])\\n            matricies[s.name] = Matrix.wrap([{k: unwrap(t.fields).get(v, None) for k, v in s.value.items()}for t in T])\\n        elif isinstance(s.value, list):\\n            matricies[s.name] = Matrix.wrap([tuple(unwrap(t.fields).get(ss, None) for ss in s.value) for t in T])\\n        elif not s.value:\\n            matricies[s.name] = Matrix.wrap([unwrap(t.fields).get(s.value, None) for t in T])\\n        else:\\n            try:\\n                matricies[s.name] = Matrix.wrap([unwrap(t.fields).get(s.value, None) for t in T])\\n            except Exception as e:\\n                Log.error(\\"\\", e)\\n\\n    cube = Cube(query.select, query.edges, matricies, frum=query)\\n    cube.frum = query\\n    return cube\\n\\n\\ndef is_setop(query):\\n    select = listwrap(query.select)\\n\\n    if not query.edges:\\n        isDeep = len(split_field(query.frum.name)) > 1  # LOOKING INTO NESTED WILL REQUIRE A SCRIPT\\n        simpleAgg = AND([s.aggregate in (\\"count\\", \\"none\\") for s in select])   # CONVERTING esfilter DEFINED PARTS WILL REQUIRE SCRIPT\\n\\n        # NO EDGES IMPLIES SIMPLER QUERIES: EITHER A SET OPERATION, OR RETURN SINGLE AGGREGATE\\n        if simpleAgg or isDeep:\\n            return True\\n    else:\\n        isSmooth = AND((e.domain.type in domains.ALGEBRAIC and e.domain.interval == \\"none\\") for e in query.edges)\\n        if isSmooth:\\n            return True\\n\\n    return False\\n\\n\\ndef es_setop(es, mvel, query):\\n    FromES = es09.util.build_es_query(query)\\n    select = listwrap(query.select)\\n\\n    isDeep = len(split_field(query.frum.name)) > 1  # LOOKING INTO NESTED WILL REQUIRE A SCRIPT\\n    isComplex = OR([s.value == None and s.aggregate not in (\\"count\\", \\"none\\") for s in select])   # CONVERTING esfilter DEFINED PARTS WILL REQUIRE SCRIPT\\n\\n    if not isDeep and not isComplex:\\n        if len(select) == 1 and not select[0].value or select[0].value == \\"*\\":\\n            FromES = wrap({\\n                \\"query\\": {\\"bool\\": {\\n                    \\"query\\": {\\"match_all\\": {}}\\n                    \\"filter\\": simplify_esfilter(jx_expression(query.where).to_esfilter())\\n              }}\\n                \\"sort\\": query.sort,\\n                \\"size\\": 0\\n          })\\n        elif all(isinstance(v, Variable) for v in select.value):\\n            FromES = wrap({\\n                \\"query\\": {\\"bool\\": {\\n                    \\"query\\": {\\"match_all\\": {}}\\n                    \\"filter\\": simplify_esfilter(query.where.to_esfilter())\\n              }}\\n                \\"fields\\": select.value,\\n                \\"sort\\": query.sort,\\n                \\"size\\": coalesce(query.limit, 200000)\\n          })\\n    elif not isDeep:\\n        simple_query = query.copy()\\n        simple_query.where = TRUE_FILTER  # THE FACET FILTER IS FASTER\\n        FromES.facets.mvel = {\\n            \\"terms\\": {\\n                \\"script_field\\": mvel.code(simple_query),\\n                \\"size\\": coalesce(simple_query.limit, 200000)\\n          }\\n            \\"facet_filter\\": simplify_esfilter(jx_expression(query.where).to_esfilter())\\n      }\\n    else:\\n        FromES.facets.mvel = {\\n            \\"terms\\": {\\n                \\"script_field\\": mvel.code(query),\\n                \\"size\\": coalesce(query.limit, 200000)\\n          }\\n            \\"facet_filter\\": simplify_esfilter(jx_expression(query.where).to_esfilter())\\n      }\\n\\n    data = es09.util.post(es, FromES, query.limit)\\n\\n    if len(select) == 1 and  not select[0].value or select[0].value == \\"*\\":\\n        # SPECIAL CASE FOR SINGLE COUNT\\n        cube = wrap(data).hits.hits._source\\n    elif isinstance(select[0].value, Variable):\\n        # SPECIAL CASE FOR SINGLE TERM\\n        cube = wrap(data).hits.hits.fields\\n    else:\\n        data_list = unpack_terms(data.facets.mvel, select)\\n        if not data_list:\\n            cube = Cube(select, [], {s.name: Matrix.wrap([]) for s in select})\\n        else:\\n            output = zip(*data_list)\\n            cube = Cube(select, [], {s.name: Matrix(list=output[i]) for i, s in enumerate(select)})\\n\\n    return Data(\\n        meta={\\"esquery\\": FromES}\\n        data=cube\\n    )\\n\\n\\ndef is_deep(query):\\n    select = listwrap(query.select)\\n    if len(select) > 1:\\n        return False\\n\\n    if aggregates[select[0].aggregate] not in (\\"none\\", \\"count\\"):\\n        return False\\n\\n    if len(query.edges)<=1:\\n        return False\\n\\n    isDeep = len(split_field(query[\\"from\\"].name)) > 1  # LOOKING INTO NESTED WILL REQUIRE A SCRIPT\\n    if not isDeep:\\n        return False   # BETTER TO USE TERM QUERY\\n\\n    return True\\n\\n\\ndef es_deepop(es, mvel, query):\\n    FromES = es09.util.build_es_query(query)\\n\\n    select = query.edges\\n\\n    temp_query = query.copy()\\n    temp_query.select = select\\n    temp_query.edges = FlatList()\\n    FromES.facets.mvel = {\\n        \\"terms\\": {\\n            \\"script_field\\": mvel.code(temp_query),\\n            \\"size\\": query.limit\\n      }\\n        \\"facet_filter\\": simplify_esfilter(jx_expression(query.where).to_esfilter())\\n  }\\n\\n    data = es09.util.post(es, FromES, query.limit)\\n\\n    rows = unpack_terms(data.facets.mvel, query.edges)\\n    terms = zip(*rows)\\n\\n    # NUMBER ALL EDGES FOR JSON EXPRESSION INDEXING\\n    edges = query.edges\\n    for f, e in enumerate(edges):\\n        for r in terms[f]:\\n            e.domain.getPartByKey(r)\\n\\n        e.index = f\\n        for p, part in enumerate(e.domain.partitions):\\n            part.dataIndex = p\\n        e.domain.NULL.dataIndex = len(e.domain.partitions)\\n\\n    # MAKE CUBE\\n    dims = [len(e.domain.partitions) for e in query.edges]\\n    output = Matrix(*dims)\\n\\n    # FILL CUBE\\n    for r in rows:\\n        term_coord = [e.domain.getPartByKey(r[i]).dataIndex for i, e in enumerate(edges)]\\n        output[term_coord] = SUM(output[term_coord], r[-1])\\n\\n    cube = Cube(query.select, query.edges, {query.select.name: output})\\n    cube.frum = query\\n    return cube\\n" }\n'
line: b'{ "repo_name": "SolaWing/ycmd", "ref": "refs/heads/mine", "path": "ycmd/tests/clang/signature_help_test.py", "content": "# encoding: utf-8\\n#\\n# Copyright (C) 2015-2018 ycmd contributors\\n#\\n# This file is part of ycmd.\\n#\\n# ycmd is free software: you can redistribute it and/or modify\\n# it under the terms of the GNU General Public License as published by\\n# the Free Software Foundation, either version 3 of the License, or\\n# (at your option) any later version.\\n#\\n# ycmd is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU General Public License for more details.\\n#\\n# You should have received a copy of the GNU General Public License\\n# along with ycmd.  If not, see <http://www.gnu.org/licenses/>.\\n\\nfrom __future__ import absolute_import\\nfrom __future__ import unicode_literals\\nfrom __future__ import print_function\\nfrom __future__ import division\\n# Not installing aliases from python-future; it\'s unreliable and slow.\\nfrom builtins import *  # noqa\\n\\n\\nfrom hamcrest import ( assert_that, empty, has_entries, has_items )\\n\\nfrom ycmd.utils import ReadFile\\nfrom ycmd.tests.clang import PathToTestFile, SharedYcmd\\nfrom ycmd.tests.test_utils import ( EMPTY_SIGNATURE_HELP,\\n                                    BuildRequest,\\n                                    CompletionEntryMatcher )\\n\\n\\n@SharedYcmd\\ndef SignatureHelp_NotImplemented_test( app ):\\n  app.post_json(\\n    \'/load_extra_conf_file\',\\n  { \'filepath\': PathToTestFile( \'.ycm_extra_conf.py\' ) } )\\n\\n  filepath = PathToTestFile( \'unity.cc\' )\\n  contents = ReadFile( filepath )\\n\\n  app.post_json( \'/event_notification\',\\n                 BuildRequest( filepath = filepath,\\n                               contents = contents,\\n                               filetype = \'cpp\',\\n                               event_name = \'FileReadyToParse\' ) )\\n\\n  # Doing a completion proves that we have semantic parsing working\\n  response_data = app.post_json( \'/completions\',\\n                                 BuildRequest( filepath = filepath,\\n                                               contents = contents,\\n                                               filetype = \'cpp\',\\n                                               line_num = 27,\\n                                               column_num = 11,\\n                                               force_semantic = True ) ).json\\n\\n  assert_that( response_data[ \'completions\' ],\\n               has_items( CompletionEntryMatcher( \'an_int\' ),\\n                          CompletionEntryMatcher( \'a_char\' ) ) )\\n\\n  # Signature help request always returns nothing\\n  # FIXME: A method to say \\"don\'t bother sending more signature help request\\"\\n  response_data = app.post_json( \'/signature_help\',\\n                                BuildRequest( filepath = filepath,\\n                                              contents = contents,\\n                                              filetype = \'cpp\',\\n                                              line_num = 24,\\n                                              column_num = 19 ) ).json\\n\\n  assert_that( response_data, has_entries( {\\n    \'errors\': empty(),\\n    \'signature_help\': EMPTY_SIGNATURE_HELP\\n} ) )\\n" }\n'
line: b'{ "repo_name": "CyBHFal/plugin.video.freplay", "ref": "refs/heads/cyb2", "path": "resources/lib/delete_catalog_cache.py", "content": "#-*- coding: utf-8 -*-\\nimport globalvar\\nimport shutil\\nimport xbmcgui\\nimport os\\n\\ndef delete_catalog_cache() :\\n    if os.path.exists(globalvar.CACHE_DIR) :\\n        shutil.rmtree(globalvar.CACHE_DIR)\\n    if not os.path.exists(globalvar.CACHE_DIR):\\n        os.makedirs(globalvar.CACHE_DIR, mode=0777)\\n    xbmcgui.Dialog().ok(globalvar.LANGUAGE(30000), globalvar.LANGUAGE(32000))\\n    \\nif ( __name__ == \\"__main__\\" ):\\n    if  globalvar.CACHE_DIR != \'\' :\\n        delete_catalog_cache()\\n" } \n'
line: b'{ "repo_name": "leeper/dataverse-1", "ref": "refs/heads/4.2", "path": "tests/test_create_test_account.py", "content": "from selenium import webdriver\\nimport time, unittest, config\\n\\ndef is_alert_present(wd):\\n    try:\\n        wd.switch_to_alert().text\\n        return True\\n    except:\\n        return False\\n\\nclass test_create_test_account(unittest.TestCase):\\n    def setUp(self):\\n        if (config.local):\\n            self.wd = webdriver.Firefox()\\n        else:\\n            desired_capabilities = webdriver.DesiredCapabilities.FIREFOX\\n            desired_capabilities[\'version\'] = \'24\'\\n            desired_capabilities[\'platform\'] = \'Linux\'\\n            desired_capabilities[\'name\'] = \'test_access\'\\n            self.wd = webdriver.Remote(\\n                desired_capabilities=desired_capabilities,\\n                command_executor=\\"http://esodvn:325caef9-81dd-47a5-8b74-433057ce888f@ondemand.saucelabs.com:80/wd/hub\\"\\n            )\\n \\n        self.wd.implicitly_wait(60)\\n    \\n    def test_test_create_test_account(self):\\n        success = True\\n        wd = self.wd\\n        wd.get(config.accessURL)\\n        wd.find_element_by_link_text(\\"Create Account\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:userName\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:userName\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:userName\\").send_keys(\\"tester\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:inputPassword\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:inputPassword\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:inputPassword\\").send_keys(\\"tester\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:retypePassword\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:retypePassword\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:retypePassword\\").send_keys(\\"tester\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:firstName\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:firstName\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:firstName\\").send_keys(\\"test\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:lastName\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:lastName\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:lastName\\").send_keys(\\"user\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:email\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:email\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:email\\").send_keys(\\"kcondon@hmdc.harvard.edu\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:institution\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:institution\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:institution\\").send_keys(\\"IQSS\\")\\n        wd.find_element_by_xpath(\\"//div[@id=\'dataverseUserForm:j_idt45\']/div[3]\\").click()\\n        wd.find_element_by_xpath(\\"//div[@class=\'ui-selectonemenu-items-wrapper\']//li[.=\'Staff\']\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:phone\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:phone\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:phone\\").send_keys(\\"1-222-333-4444\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:save\\").click()\\n        time.sleep(1)\\n        if (\\"This Username is already taken.\\" in wd.find_element_by_tag_name(\\"html\\").text):\\n            print(\\"Username exists. Exiting.\\")\\n            return   \\n        if not (\\"Log Out\\" in wd.find_element_by_tag_name(\\"html\\").text): \\n            success = false\\n            print(\\"User was not logged in after create account.\\")           \\n        self.assertTrue(success)\\n    \\n    def tearDown(self):\\n        if not (config.local):\\n            print(\\"Link to your job: https://saucelabs.com/jobs/%s\\" % self.wd.session_id)        \\n        self.wd.quit()\\n\\nif __name__ == \'__main__\':\\n    unittest.main()\\n" }\n'
line: b'{ "repo_name": "alanjw/GreenOpenERP-Win-X86", "ref": "refs/heads/7.0", "path": "python/Lib/sqlite3/test/hooks.py", "content": "#-*- coding: ISO-8859-1 -*-\\r\\n# pysqlite2/test/hooks.py: tests for various SQLite-specific hooks\\r\\n#\\r\\n# Copyright (C) 2006-2007 Gerhard H\xc3\xa4ring <gh@ghaering.de>\\r\\n#\\r\\n# This file is part of pysqlite.\\r\\n#\\r\\n# This software is provided \'as-is\', without any express or implied\\r\\n# warranty.  In no event will the authors be held liable for any damages\\r\\n# arising from the use of this software.\\r\\n#\\r\\n# Permission is granted to anyone to use this software for any purpose,\\r\\n# including commercial applications, and to alter it and redistribute it\\r\\n# freely, subject to the following restrictions:\\r\\n#\\r\\n# 1. The origin of this software must not be misrepresented; you must not\\r\\n#    claim that you wrote the original software. If you use this software\\r\\n#    in a product, an acknowledgment in the product documentation would be\\r\\n#    appreciated but is not required.\\r\\n# 2. Altered source versions must be plainly marked as such, and must not be\\r\\n#    misrepresented as being the original software.\\r\\n# 3. This notice may not be removed or altered from any source distribution.\\r\\n\\r\\nimport os, unittest\\r\\nimport sqlite3 as sqlite\\r\\n\\r\\nclass CollationTests(unittest.TestCase):\\r\\n    def setUp(self):\\r\\n        pass\\r\\n\\r\\n    def tearDown(self):\\r\\n        pass\\r\\n\\r\\n    def CheckCreateCollationNotCallable(self):\\r\\n        con = sqlite.connect(\\":memory:\\")\\r\\n        try:\\r\\n            con.create_collation(\\"X\\", 42)\\r\\n            self.fail(\\"should have raised a TypeError\\")\\r\\n        except TypeError, e:\\r\\n            self.assertEqual(e.args[0], \\"parameter must be callable\\")\\r\\n\\r\\n    def CheckCreateCollationNotAscii(self):\\r\\n        con = sqlite.connect(\\":memory:\\")\\r\\n        try:\\r\\n            con.create_collation(\\"coll\xc3\xa4\\", cmp)\\r\\n            self.fail(\\"should have raised a ProgrammingError\\")\\r\\n        except sqlite.ProgrammingError, e:\\r\\n            pass\\r\\n\\r\\n    def CheckCollationIsUsed(self):\\r\\n        if sqlite.version_info < (3, 2, 1):  # old SQLite versions crash on this test\\r\\n            return\\r\\n        def mycoll(x, y):\\r\\n            # reverse order\\r\\n            return -cmp(x, y)\\r\\n\\r\\n        con = sqlite.connect(\\":memory:\\")\\r\\n        con.create_collation(\\"mycoll\\", mycoll)\\r\\n        sql = \\"\\"\\"\\r\\n            select x from (\\r\\n            select \'a\' as x\\r\\n            union\\r\\n            select \'b\' as x\\r\\n            union\\r\\n            select \'c\' as x\\r\\n            ) order by x collate mycoll\\r\\n            \\"\\"\\"\\r\\n        result = con.execute(sql).fetchall()\\r\\n        if result[0][0] != \\"c\\" or result[1][0] != \\"b\\" or result[2][0] != \\"a\\":\\r\\n            self.fail(\\"the expected order was not returned\\")\\r\\n\\r\\n        con.create_collation(\\"mycoll\\", None)\\r\\n        try:\\r\\n            result = con.execute(sql).fetchall()\\r\\n            self.fail(\\"should have raised an OperationalError\\")\\r\\n        except sqlite.OperationalError, e:\\r\\n            self.assertEqual(e.args[0].lower(), \\"no such collation sequence: mycoll\\")\\r\\n\\r\\n    def CheckCollationRegisterTwice(self):\\r\\n        \\"\\"\\"\\r\\n        Register two different collation functions under the same name.\\r\\n        Verify that the last one is actually used.\\r\\n        \\"\\"\\"\\r\\n        con = sqlite.connect(\\":memory:\\")\\r\\n        con.create_collation(\\"mycoll\\", cmp)\\r\\n        con.create_collation(\\"mycoll\\", lambda x, y: -cmp(x, y))\\r\\n        result = con.execute(\\"\\"\\"\\r\\n            select x from (select \'a\' as x union select \'b\' as x) order by x collate mycoll\\r\\n            \\"\\"\\").fetchall()\\r\\n        if result[0][0] != \'b\' or result[1][0] != \'a\':\\r\\n            self.fail(\\"wrong collation function is used\\")\\r\\n\\r\\n    def CheckDeregisterCollation(self):\\r\\n        \\"\\"\\"\\r\\n        Register a collation, then deregister it. Make sure an error is raised if we try\\r\\n        to use it.\\r\\n        \\"\\"\\"\\r\\n        con = sqlite.connect(\\":memory:\\")\\r\\n        con.create_collation(\\"mycoll\\", cmp)\\r\\n        con.create_collation(\\"mycoll\\", None)\\r\\n        try:\\r\\n            con.execute(\\"select \'a\' as x union select \'b\' as x order by x collate mycoll\\")\\r\\n            self.fail(\\"should have raised an OperationalError\\")\\r\\n        except sqlite.OperationalError, e:\\r\\n            if not e.args[0].startswith(\\"no such collation sequence\\"):\\r\\n                self.fail(\\"wrong OperationalError raised\\")\\r\\n\\r\\nclass ProgressTests(unittest.TestCase):\\r\\n    def CheckProgressHandlerUsed(self):\\r\\n        \\"\\"\\"\\r\\n        Test that the progress handler is invoked once it is set.\\r\\n        \\"\\"\\"\\r\\n        con = sqlite.connect(\\":memory:\\")\\r\\n        progress_calls = []\\r\\n        def progress():\\r\\n            progress_calls.append(None)\\r\\n            return 0\\r\\n        con.set_progress_handler(progress, 1)\\r\\n        con.execute(\\"\\"\\"\\r\\n            create table foo(a, b)\\r\\n            \\"\\"\\")\\r\\n        self.assertTrue(progress_calls)\\r\\n\\r\\n\\r\\n    def CheckOpcodeCount(self):\\r\\n        \\"\\"\\"\\r\\n        Test that the opcode argument is respected.\\r\\n        \\"\\"\\"\\r\\n        con = sqlite.connect(\\":memory:\\")\\r\\n        progress_calls = []\\r\\n        def progress():\\r\\n            progress_calls.append(None)\\r\\n            return 0\\r\\n        con.set_progress_handler(progress, 1)\\r\\n        curs = con.cursor()\\r\\n        curs.execute(\\"\\"\\"\\r\\n            create table foo (a, b)\\r\\n            \\"\\"\\")\\r\\n        first_count = len(progress_calls)\\r\\n        progress_calls = []\\r\\n        con.set_progress_handler(progress, 2)\\r\\n        curs.execute(\\"\\"\\"\\r\\n            create table bar (a, b)\\r\\n            \\"\\"\\")\\r\\n        second_count = len(progress_calls)\\r\\n        self.assertTrue(first_count > second_count)\\r\\n\\r\\n    def CheckCancelOperation(self):\\r\\n        \\"\\"\\"\\r\\n        Test that returning a non-zero value stops the operation in progress.\\r\\n        \\"\\"\\"\\r\\n        con = sqlite.connect(\\":memory:\\")\\r\\n        progress_calls = []\\r\\n        def progress():\\r\\n            progress_calls.append(None)\\r\\n            return 1\\r\\n        con.set_progress_handler(progress, 1)\\r\\n        curs = con.cursor()\\r\\n        self.assertRaises(\\r\\n            sqlite.OperationalError,\\r\\n            curs.execute,\\r\\n            \\"create table bar (a, b)\\")\\r\\n\\r\\n    def CheckClearHandler(self):\\r\\n        \\"\\"\\"\\r\\n        Test that setting the progress handler to None clears the previously set handler.\\r\\n        \\"\\"\\"\\r\\n        con = sqlite.connect(\\":memory:\\")\\r\\n        action = []\\r\\n        def progress():\\r\\n            action.append(1)\\r\\n            return 0\\r\\n        con.set_progress_handler(progress, 1)\\r\\n        con.set_progress_handler(None, 1)\\r\\n        con.execute(\\"select 1 union select 2 union select 3\\").fetchall()\\r\\n        self.assertEqual(len(action), 0, \\"progress handler was not cleared\\")\\r\\n\\r\\ndef suite():\\r\\n    collation_suite = unittest.makeSuite(CollationTests, \\"Check\\")\\r\\n    progress_suite = unittest.makeSuite(ProgressTests, \\"Check\\")\\r\\n    return unittest.TestSuite((collation_suite, progress_suite))\\r\\n\\r\\ndef test():\\r\\n    runner = unittest.TextTestRunner()\\r\\n    runner.run(suite())\\r\\n\\r\\nif __name__ == \\"__main__\\":\\r\\n    test()\\r\\n" }\n'
line: b'{ "repo_name": "ekoi/DANS-DVN-4.6.1", "ref": "refs/heads/v4.6.1-merge", "path": "tests/test_create_test_account.py", "content": "from selenium import webdriver\\nimport time, unittest, config\\n\\ndef is_alert_present(wd):\\n    try:\\n        wd.switch_to_alert().text\\n        return True\\n    except:\\n        return False\\n\\nclass test_create_test_account(unittest.TestCase):\\n    def setUp(self):\\n        if (config.local):\\n            self.wd = webdriver.Firefox()\\n        else:\\n            desired_capabilities = webdriver.DesiredCapabilities.FIREFOX\\n            desired_capabilities[\'version\'] = \'24\'\\n            desired_capabilities[\'platform\'] = \'Linux\'\\n            desired_capabilities[\'name\'] = \'test_access\'\\n            self.wd = webdriver.Remote(\\n                desired_capabilities=desired_capabilities,\\n                command_executor=\\"http://esodvn:325caef9-81dd-47a5-8b74-433057ce888f@ondemand.saucelabs.com:80/wd/hub\\"\\n            )\\n \\n        self.wd.implicitly_wait(60)\\n    \\n    def test_test_create_test_account(self):\\n        success = True\\n        wd = self.wd\\n        wd.get(config.accessURL)\\n        wd.find_element_by_link_text(\\"Create Account\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:userName\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:userName\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:userName\\").send_keys(\\"tester\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:inputPassword\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:inputPassword\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:inputPassword\\").send_keys(\\"tester\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:retypePassword\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:retypePassword\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:retypePassword\\").send_keys(\\"tester\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:firstName\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:firstName\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:firstName\\").send_keys(\\"test\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:lastName\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:lastName\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:lastName\\").send_keys(\\"user\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:email\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:email\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:email\\").send_keys(\\"kcondon@hmdc.harvard.edu\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:institution\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:institution\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:institution\\").send_keys(\\"IQSS\\")\\n        wd.find_element_by_xpath(\\"//div[@id=\'dataverseUserForm:j_idt45\']/div[3]\\").click()\\n        wd.find_element_by_xpath(\\"//div[@class=\'ui-selectonemenu-items-wrapper\']//li[.=\'Staff\']\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:phone\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:phone\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:phone\\").send_keys(\\"1-222-333-4444\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:save\\").click()\\n        time.sleep(1)\\n        if (\\"This Username is already taken.\\" in wd.find_element_by_tag_name(\\"html\\").text):\\n            print(\\"Username exists. Exiting.\\")\\n            return   \\n        if not (\\"Log Out\\" in wd.find_element_by_tag_name(\\"html\\").text): \\n            success = false\\n            print(\\"User was not logged in after create account.\\")           \\n        self.assertTrue(success)\\n    \\n    def tearDown(self):\\n        if not (config.local):\\n            print(\\"Link to your job: https://saucelabs.com/jobs/%s\\" % self.wd.session_id)        \\n        self.wd.quit()\\n\\nif __name__ == \'__main__\':\\n    unittest.main()\\n" }\n'
line: b'{ "repo_name": "RonnyPfannschmidt/pytest", "ref": "refs/heads/features", "path": "testing/example_scripts/fixtures/fill_fixtures/test_extend_fixture_conftest_conftest/pkg/conftest.py", "content": "import pytest\\n\\n\\n@pytest.fixture\\ndef spam(spam):\\n    return spam * 2\\n" }\n'
line: b'{ "repo_name": "pytest-dev/pytest", "ref": "refs/heads/main", "path": "testing/example_scripts/fixtures/fill_fixtures/test_extend_fixture_conftest_conftest/pkg/conftest.py", "content": "import pytest\\n\\n\\n@pytest.fixture\\ndef spam(spam):\\n    return spam * 2\\n" }\n'
line: b'{ "repo_name": "rossburton/yocto-autobuilder", "ref": "refs/heads/ross", "path": "lib/python2.7/site-packages/buildbot-0.8.8-py2.7.egg/buildbot/test/unit/test_db_migrate_versions_021_fix_postgres_sequences.py", "content": "# This file is part of Buildbot.  Buildbot is free software: you can\\n# redistribute it and/or modify it under the terms of the GNU General Public\\n# License as published by the Free Software Foundation, version 2.\\n#\\n# This program is distributed in the hope that it will be useful, but WITHOUT\\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\\n# details.\\n#\\n# You should have received a copy of the GNU General Public License along with\\n# this program; if not, write to the Free Software Foundation, Inc., 51\\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\\n#\\n# Copyright Buildbot Team Members\\n\\nimport sqlalchemy as sa\\nfrom twisted.trial import unittest\\nfrom buildbot.test.util import migration\\n\\nclass Migration(migration.MigrateTestMixin, unittest.TestCase):\\n\\n    def setUp(self):\\n        return self.setUpMigrateTest()\\n\\n    def tearDown(self):\\n        return self.tearDownMigrateTest()\\n\\n    cols = [\\n        \'buildrequests.id\',\\n        \'builds.id\',\\n        \'buildsets.id\',\\n        \'changes.changeid\',\\n        \'patches.id\',\\n        \'sourcestampsets.id\',\\n        \'sourcestamps.id\',\\n        \'objects.id\',\\n        \'users.uid\',\\n    ]\\n\\n    # tests\\n\\n    def test_update(self):\\n        def setup_thd(conn):\\n            metadata = sa.MetaData()\\n            metadata.bind = conn\\n\\n            # insert a row into each table, giving an explicit id column so\\n            # that the sequence is not advanced correctly, but leave no rows in\\n            # one table to test that corner case\\n            for i, col in enumerate(self.cols):\\n                tbl_name, col_name = col.split(\'.\')\\n                tbl = sa.Table(tbl_name, metadata,\\n                        sa.Column(col_name, sa.Integer, primary_key=True))\\n                tbl.create()\\n                if i > 1:\\n                    conn.execute(tbl.insert(), { col_name : i })\\n\\n        def verify_thd(conn):\\n            metadata = sa.MetaData()\\n            metadata.bind = conn\\n\\n            # try inserting *without* an ID, and verify that the resulting ID\\n            # is as expected\\n            for i, col in enumerate(self.cols):\\n                tbl_name, col_name = col.split(\'.\')\\n                tbl = sa.Table(tbl_name, metadata,\\n                        sa.Column(col_name, sa.Integer, primary_key=True))\\n                r = conn.execute(tbl.insert(), {})\\n                if i > 1:\\n                    exp = i+1\\n                else:\\n                    exp = 1\\n                self.assertEqual(r.inserted_primary_key[0], exp)\\n\\n        return self.do_test_migration(20, 21, setup_thd, verify_thd)\\n" }\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/Decimal-to-binary.py","content":"def convert_to_binary(n): \\n\\n      if(n > 1):\\n        convert_to_binary(n//2)\\n  \\n      print(n % 2, end = \'\')\\n\\nconvert_to_binary(52)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/Factorial.py","content":"def factorial(n):\\n    factorial = 1\\n    # check is the number is negative, positive or zero\\n    if(n < 0):\\n        print(\'Sorry, factorial does not exist for negative numbers\')\\n    elif(n == 0):\\n        print(\'The factorial of 0 is 1\')\\n    else:\\n        for i in range(1,n):\\n            factorial = factorial * i\\n        print(\'The factorial of {} is {}\'.format(n,factorial))\\n\\nfactorial(4)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/Fibonacci.py","content":"def fibonacci(n): \\n    \\n    fibonacci_seq = [None for i in range(n)]\\n    fibonacci_seq[0] = fibonacci_seq[1] = 1\\n    for i in range(2,n):\\n        \\n           fibonacci_seq[i] = fibonacci_seq[i - 2] + fibonacci_seq[i - 1]\\n    print(\'First {} Fibonacci numbers:\'.format(n))\\n    print(fibonacci_seq)\\n\\n\\nfibonacci(10)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/Max-Min-of-Vector.py","content":"\\ndef max_min_vector(n):\\n\\n    print(\'Original vector:\')\\n    print(n)   \\n    print(\'Maximum value of the said vector:\',max(n))\\n    print(\'Minimum value of the said vector:\',min(n))\\n\\n\\nmax_min_vector([10, 20, 30, 40, 50, 60])\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/Odd-or-Even.py","content":"\\ndef odd_or_even(n):\\n    num = n\\n    \\n    if((num % 2) == 0):\\n        print(\'{} is Even\'.format(num))\\n    else:\\n        print(\'{} is Odd\'.format(num))\\n\\n\\nodd_or_even(4)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/Prime-Numbers.py","content":"\\ndef prime_numbers(n):\\n    \\n     prime_nums = [] \\n     if (n >= 2):\\n        \\n        for i in range(2,n):\\n             for j in range(2,i):\\n                  \\n                  if i%j == 0:\\n                      break\\n             else:\\n                 prime_nums.append(i)\\n                \\n                \\n\\n        return(prime_nums)\\n     else: \\n           print(\'Input number should be at least 2.\')\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/Read-csv-file.py.","content":"\\nimport pandas as pd\\n\\ndef read_csv_file():\\n    \\n    movie_data = pd.read_csv(\'movies.csv\')\\n    print(\'Content of the .csv file:\')\\n    print(movie_data)\\n\\n\\nread_csv_file()\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/SumMeanProduct.py","content":"\\nimport numpy as np\\ndef sum(nums):\\n    sum = 0\\n    for x in nums:\\n        sum = sum + x\\n    print(sum)\\n\\ndef avg(nums):\\n   print(np.mean(nums))\\n\\ndef product(nums):\\n    print(np.prod(nums))\\n\\nlist1 = [3,4,5]\\n\\nsum(list1)\\navg(list1)\\nproduct(list1)"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/add1.py","content":"\\ndef add1(num):\\n    print(num + 1)\\n    \\nadd1(2)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/add2nums.py","content":"\\ndef add2nums(a, b):\\n    print (a+b)\\n\\nadd2nums(2, 3)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/areaSquare.py","content":"\\ndef areaSquare(side):\\n    if(side <= 0 ):\\n        print \'Invalid measurement\'\\n    else:\\n        print \'Area of the square is : \', side*side\\n\\nareaSquare(4)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/armstrong.py","content":"\\nnum = int(input(\'Enter a number: \'))\\n\\nsum = 0\\n\\ntemp = num\\nwhile temp > 0:\\n   digit = temp % 10\\n   sum += digit ** 3\\n   temp //= 10\\n\\nif num == sum:\\n   print(num,\'is an Armstrong number\')\\nelse:\\n   print(num,\'is not an Armstrong number\')\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/describe.py","content":"\\nimport pandas as pd\\ndf = pd.read_csv(\'nba_2013.csv\')\\ndf.describe()\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/divisibleby10.py","content":"\\ndef divisibleby10(n):\\n    if(n%10 == 0):\\n        print(\'True\')\\n    else:\\n        print(\'False\')\\n\\n\\ndivisibleby10(8)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/factors.py","content":"\\ndef print_factors(x):\\n   print(\'The factors of\',x,\'are:\')\\n   for i in range(1, x + 1):\\n       if x % i == 0:\\n           print(i)\\n\\nnum = 320\\n\\nprint_factors(num)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/integerType.py","content":"\\ndef integerType(num):\\n    if (num > 0):\\n        print(\'Number is Postive integer\')\\n    if(num < 0):\\n        print(\'Number is Negative integer\')\\n    if(num == 0):\\n        print(\'Number is Zero\')\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/lcm.py","content":"\\ndef compute_lcm(x, y):\\n\\n   # choose the greater number\\n   if x > y:\\n       greater = x\\n   else:\\n       greater = y\\n\\n   while(True):\\n       if((greater % x == 0) and (greater % y == 0)):\\n           lcm = greater\\n           break\\n       greater += 1\\n\\n   return lcm\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/lengthOfVector.py","content":"\\ndef lengthOfVector(nums):\\n    count = 0\\n    for x in nums:\\n        count = count+1\\n    \\n    print(count)\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/palindrome.py","content":"\\ndef palindrome(num):\\n    temp = num\\n    rev = 0\\n    while(num > 0):\\n        dig = num % 10\\n        rev = rev * 10 + dig\\n        num = num // 10\\n    if(temp == rev):\\n        print(\'Number is palindrome\')\\n    else: \\n        print(\'Number is not palidrome\')\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/perfect.py","content":"\\ndef perfect_number(n):\\n    sum = 0\\n    for x in range(1, n):\\n        if n % x == 0:\\n            sum += x\\n    return sum == n\\n"}\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/Python/sortvector.py","content":"\\ndef sortVector(nums):\\n    nums.sort()\\n    print(nums)\\n"}\n'
line: b'{ "repo_name": "OCA/connector-telephony", "ref": "refs/heads/12.0", "path": "setup/_metapackage/setup.py", "content": "import setuptools\\n\\nwith open(\'VERSION.txt\', \'r\') as f:\\n    version = f.read().strip()\\n\\nsetuptools.setup(\\n    name=\\"odoo12-addons-oca-connector-telephony\\",\\n    description=\\"Meta package for oca-connector-telephony Odoo addons\\",\\n    version=version,\\n    install_requires=[\\n        \'odoo12-addon-asterisk_click2dial\',\\n        \'odoo12-addon-base_phone\',\\n        \'odoo12-addon-base_phone_popup\',\\n        \'odoo12-addon-connector_voicent\',\\n        \'odoo12-addon-crm_phone\',\\n        \'odoo12-addon-event_phone\',\\n        \'odoo12-addon-hr_phone\',\\n        \'odoo12-addon-hr_recruitment_phone\',\\n        \'odoo12-addon-sms_ovh_http\',\\n    ],\\n    classifiers=[\\n        \'Programming Language :: Python\',\\n        \'Framework :: Odoo\',\\n    ]\\n)\\n" }\n'
line: b'{ "repo_name": "JayanthyChengan/dataverse", "ref": "refs/heads/dataverse-contactform-afflist", "path": "tests/test_create_test_account.py", "content": "from selenium import webdriver\\nimport time, unittest, config\\n\\ndef is_alert_present(wd):\\n    try:\\n        wd.switch_to_alert().text\\n        return True\\n    except:\\n        return False\\n\\nclass test_create_test_account(unittest.TestCase):\\n    def setUp(self):\\n        if (config.local):\\n            self.wd = webdriver.Firefox()\\n        else:\\n            desired_capabilities = webdriver.DesiredCapabilities.FIREFOX\\n            desired_capabilities[\'version\'] = \'24\'\\n            desired_capabilities[\'platform\'] = \'Linux\'\\n            desired_capabilities[\'name\'] = \'test_access\'\\n            self.wd = webdriver.Remote(\\n                desired_capabilities=desired_capabilities,\\n                command_executor=\\"http://esodvn:325caef9-81dd-47a5-8b74-433057ce888f@ondemand.saucelabs.com:80/wd/hub\\"\\n            )\\n \\n        self.wd.implicitly_wait(60)\\n    \\n    def test_test_create_test_account(self):\\n        success = True\\n        wd = self.wd\\n        wd.get(config.accessURL)\\n        wd.find_element_by_link_text(\\"Create Account\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:userName\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:userName\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:userName\\").send_keys(\\"tester\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:inputPassword\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:inputPassword\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:inputPassword\\").send_keys(\\"tester\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:retypePassword\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:retypePassword\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:retypePassword\\").send_keys(\\"tester\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:firstName\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:firstName\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:firstName\\").send_keys(\\"test\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:lastName\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:lastName\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:lastName\\").send_keys(\\"user\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:email\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:email\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:email\\").send_keys(\\"kcondon@hmdc.harvard.edu\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:institution\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:institution\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:institution\\").send_keys(\\"IQSS\\")\\n        wd.find_element_by_xpath(\\"//div[@id=\'dataverseUserForm:j_idt45\']/div[3]\\").click()\\n        wd.find_element_by_xpath(\\"//div[@class=\'ui-selectonemenu-items-wrapper\']//li[.=\'Staff\']\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:phone\\").click()\\n        wd.find_element_by_id(\\"dataverseUserForm:phone\\").clear()\\n        wd.find_element_by_id(\\"dataverseUserForm:phone\\").send_keys(\\"1-222-333-4444\\")\\n        wd.find_element_by_id(\\"dataverseUserForm:save\\").click()\\n        time.sleep(1)\\n        if (\\"This Username is already taken.\\" in wd.find_element_by_tag_name(\\"html\\").text):\\n            print(\\"Username exists. Exiting.\\")\\n            return   \\n        if not (\\"Log Out\\" in wd.find_element_by_tag_name(\\"html\\").text): \\n            success = false\\n            print(\\"User was not logged in after create account.\\")           \\n        self.assertTrue(success)\\n    \\n    def tearDown(self):\\n        if not (config.local):\\n            print(\\"Link to your job: https://saucelabs.com/jobs/%s\\" % self.wd.session_id)        \\n        self.wd.quit()\\n\\nif __name__ == \'__main__\':\\n    unittest.main()\\n" }\n'
line: b'{ "repo_name": "molecular/electrum", "ref": "refs/heads/cash", "path": "gui/kivy/uix/dialogs/tx_dialog.py", "content": "from kivy.app import App\\nfrom kivy.factory import Factory\\nfrom kivy.properties import ObjectProperty\\nfrom kivy.lang import Builder\\nfrom kivy.clock import Clock\\nfrom kivy.uix.label import Label\\n\\nfrom electroncash_gui.kivy.i18n import _\\nfrom datetime import datetime\\nfrom electroncash.util import InvalidPassword\\n\\nBuilder.load_string(\'\'\'\\n\\n<TxDialog>\\n    id: popup\\n    title: _(\'Transaction\')\\n    is_mine: True\\n    can_sign: False\\n    can_broadcast: False\\n    fee_str: \'\'\\n    date_str: \'\'\\n    amount_str: \'\'\\n    tx_hash: \'\'\\n    status_str: \'\'\\n    description: \'\'\\n    outputs_str: \'\'\\n    BoxLayout:\\n        orientation: \'vertical\'\\n        ScrollView:\\n            GridLayout:\\n                height: self.minimum_height\\n                size_hint_y: None\\n                cols: 1\\n                spacing: \'10dp\'\\n                padding: \'10dp\'\\n                GridLayout:\\n                    height: self.minimum_height\\n                    size_hint_y: None\\n                    cols: 1\\n                    spacing: \'10dp\'\\n                    BoxLabel:\\n                        text: _(\'Status\')\\n                        value: root.status_str\\n                    BoxLabel:\\n                        text: _(\'Description\') if root.description else \'\'\\n                        value: root.description\\n                    BoxLabel:\\n                        text: _(\'Date\') if root.date_str else \'\'\\n                        value: root.date_str\\n                    BoxLabel:\\n                        text: _(\'Amount sent\') if root.is_mine else _(\'Amount received\')\\n                        value: root.amount_str\\n                    BoxLabel:\\n                        text: _(\'Transaction fee\') if root.fee_str else \'\'\\n                        value: root.fee_str\\n                TopLabel:\\n                    text: _(\'Outputs\') + \':\'\\n                OutputList:\\n                    height: self.minimum_height\\n                    size_hint: 1, None\\n                    id: output_list\\n                TopLabel:\\n                    text: _(\'Transaction ID\') + \':\' if root.tx_hash else \'\'\\n                TxHashLabel:\\n                    data: root.tx_hash\\n                    name: _(\'Transaction ID\')\\n        Widget:\\n            size_hint: 1, 0.1\\n\\n        BoxLayout:\\n            size_hint: 1, None\\n            height: \'48dp\'\\n            Button:\\n                size_hint: 0.5, None\\n                height: \'48dp\'\\n                text: _(\'Sign\') if root.can_sign else _(\'Broadcast\') if root.can_broadcast else \'\'\\n                disabled: not(root.can_sign or root.can_broadcast)\\n                opacity: 0 if self.disabled else 1\\n                on_release:\\n                    if root.can_sign: root.do_sign()\\n                    if root.can_broadcast: root.do_broadcast()\\n            IconButton:\\n                size_hint: 0.5, None\\n                height: \'48dp\'\\n                icon: \'atlas://gui/kivy/theming/light/qrcode\'\\n                on_release: root.show_qr()\\n            Button:\\n                size_hint: 0.5, None\\n                height: \'48dp\'\\n                text: _(\'Close\')\\n                on_release: root.dismiss()\\n\'\'\')\\n\\n\\nclass TxDialog(Factory.Popup):\\n\\n    def __init__(self, app, tx):\\n        Factory.Popup.__init__(self)\\n        self.app = app\\n        self.wallet = self.app.wallet\\n        self.tx = tx\\n\\n    def on_open(self):\\n        self.update()\\n\\n    def update(self):\\n        format_amount = self.app.format_amount_and_units\\n        tx_hash, self.status_str, self.description, self.can_broadcast, amount, fee, height, conf, timestamp, exp_n = self.wallet.get_tx_info(self.tx)\\n        self.tx_hash = tx_hash or \'\'\\n        if timestamp:\\n            self.date_str = datetime.fromtimestamp(timestamp).isoformat(\' \')[:-3]\\n        elif exp_n:\\n            self.date_str = _(\'Within %d blocks\') % exp_n if exp_n > 0 else _(\'unknown (low fee)\')\\n        else:\\n            self.date_str = \'\'\\n\\n        if amount is None:\\n            self.amount_str = _(\\"Transaction unrelated to your wallet\\")\\n        elif amount > 0:\\n            self.is_mine = False\\n            self.amount_str = format_amount(amount)\\n        else:\\n            self.is_mine = True\\n            self.amount_str = format_amount(-amount)\\n        self.fee_str = format_amount(fee) if fee is not None else _(\'unknown\')\\n        self.can_sign = self.wallet.can_sign(self.tx)\\n        self.ids.output_list.update(self.tx.outputs())\\n\\n    def do_sign(self):\\n        self.app.protected(_(\\"Enter your PIN code in order to sign this transaction\\"), self._do_sign, ())\\n\\n    def _do_sign(self, password):\\n        self.status_str = _(\'Signing\') + \'...\'\\n        Clock.schedule_once(lambda dt: self.__do_sign(password), 0.1)\\n\\n    def __do_sign(self, password):\\n        try:\\n            self.app.wallet.sign_transaction(self.tx, password)\\n        except InvalidPassword:\\n            self.app.show_error(_(\\"Invalid PIN\\"))\\n        self.update()\\n\\n    def do_broadcast(self):\\n        self.app.broadcast(self.tx)\\n\\n    def show_qr(self):\\n        from electroncash.bitcoin import base_encode\\n        text = str(self.tx).decode(\'hex\')\\n        text = base_encode(text, base=43)\\n        self.app.qr_dialog(_(\\"Raw Transaction\\"), text)\\n" }\n'
line: b'{ "repo_name": "S11001001/phantomjs", "ref": "refs/heads/pdf-patches", "path": "src/breakpad/src/tools/gyp/test/home_dot_gyp/gyptest-home-includes-regyp.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2009 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nVerifies inclusion of $HOME/.gyp/includes.gypi works properly with relocation\\nand with regeneration.\\n\\"\\"\\"\\n\\nimport os\\nimport TestGyp\\n\\n# Regenerating build files when a gyp file changes is currently only supported\\n# by the make generator.\\ntest = TestGyp.TestGyp(formats=[\'make\'])\\n\\nos.environ[\'HOME\'] = os.path.abspath(\'home\')\\n\\ntest.run_gyp(\'all.gyp\', chdir=\'src\')\\n\\n# After relocating, we should still be able to build (build file shouldn\'t\\n# contain relative reference to ~/.gyp/includes.gypi)\\ntest.relocate(\'src\', \'relocate/src\')\\n\\ntest.build(\'all.gyp\', test.ALL, chdir=\'relocate/src\')\\n\\ntest.run_built_executable(\'printfoo\',\\n                          chdir=\'relocate/src\',\\n                          stdout=\\"FOO is fromhome\\\\n\\");\\n\\n# Building should notice any changes to ~/.gyp/includes.gypi and regyp.\\ntest.sleep()\\n\\ntest.write(\'home/.gyp/include.gypi\', test.read(\'home2/.gyp/include.gypi\'))\\n\\ntest.build(\'all.gyp\', test.ALL, chdir=\'relocate/src\')\\n\\ntest.run_built_executable(\'printfoo\',\\n                          chdir=\'relocate/src\',\\n                          stdout=\\"FOO is fromhome2\\\\n\\");\\n\\ntest.pass_test()\\n" }\n'
line: b'{ "repo_name": "pigshell/nhnick", "ref": "refs/heads/vnc-websocket", "path": "src/breakpad/src/tools/gyp/test/home_dot_gyp/gyptest-home-includes-regyp.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2009 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nVerifies inclusion of $HOME/.gyp/includes.gypi works properly with relocation\\nand with regeneration.\\n\\"\\"\\"\\n\\nimport os\\nimport TestGyp\\n\\n# Regenerating build files when a gyp file changes is currently only supported\\n# by the make generator.\\ntest = TestGyp.TestGyp(formats=[\'make\'])\\n\\nos.environ[\'HOME\'] = os.path.abspath(\'home\')\\n\\ntest.run_gyp(\'all.gyp\', chdir=\'src\')\\n\\n# After relocating, we should still be able to build (build file shouldn\'t\\n# contain relative reference to ~/.gyp/includes.gypi)\\ntest.relocate(\'src\', \'relocate/src\')\\n\\ntest.build(\'all.gyp\', test.ALL, chdir=\'relocate/src\')\\n\\ntest.run_built_executable(\'printfoo\',\\n                          chdir=\'relocate/src\',\\n                          stdout=\\"FOO is fromhome\\\\n\\");\\n\\n# Building should notice any changes to ~/.gyp/includes.gypi and regyp.\\ntest.sleep()\\n\\ntest.write(\'home/.gyp/include.gypi\', test.read(\'home2/.gyp/include.gypi\'))\\n\\ntest.build(\'all.gyp\', test.ALL, chdir=\'relocate/src\')\\n\\ntest.run_built_executable(\'printfoo\',\\n                          chdir=\'relocate/src\',\\n                          stdout=\\"FOO is fromhome2\\\\n\\");\\n\\ntest.pass_test()\\n" }\n'
line: b'{ "repo_name": "dataxu/ansible", "ref": "refs/heads/dx-stable-2.5", "path": "lib/ansible/modules/remote_management/oneview/oneview_enclosure_facts.py", "content": "#!/usr/bin/python\\n\\n# Copyright: (c) 2016-2017, Hewlett Packard Enterprise Development LP\\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\\n\\nfrom __future__ import (absolute_import, division, print_function)\\n__metaclass__ = type\\n\\nANSIBLE_METADATA = {\'metadata_version\': \'1.1\',\\n                    \'status\': [\'preview\'],\\n                    \'supported_by\': \'community\'}\\n\\nDOCUMENTATION = \'\'\'\\n---\\nmodule: oneview_enclosure_facts\\nshort_description: Retrieve facts about one or more Enclosures\\ndescription:\\n    - Retrieve facts about one or more of the Enclosures from OneView.\\nversion_added: \\"2.5\\"\\nrequirements:\\n    - hpOneView >= 2.0.1\\nauthor:\\n    - Felipe Bulsoni (@fgbulsoni)\\n    - Thiago Miotto (@tmiotto)\\n    - Adriane Cardozo (@adriane-cardozo)\\noptions:\\n    name:\\n      description:\\n        - Enclosure name.\\n    options:\\n      description:\\n        - \\"List with options to gather additional facts about an Enclosure and related resources.\\n          Options allowed: C(script), C(environmentalConfiguration), and C(utilization). For the option C(utilization),\\n          you can provide specific parameters.\\"\\n\\nextends_documentation_fragment:\\n    - oneview\\n    - oneview.factsparams\\n\'\'\'\\n\\nEXAMPLES = \'\'\'\\n- name: Gather facts about all Enclosures\\n  oneview_enclosure_facts:\\n    hostname: 172.16.101.48\\n    username: administrator\\n    password: my_password\\n    api_version: 500\\n  no_log: true\\n  delegate_to: localhost\\n- debug: var=enclosures\\n\\n- name: Gather paginated, filtered and sorted facts about Enclosures\\n  oneview_enclosure_facts:\\n    params:\\n      start: 0\\n      count: 3\\n      sort: name:descending\\n      filter: status=OK\\n    hostname: 172.16.101.48\\n    username: administrator\\n    password: my_password\\n    api_version: 500\\n  no_log: true\\n  delegate_to: localhost\\n- debug: var=enclosures\\n\\n- name: Gather facts about an Enclosure by name\\n  oneview_enclosure_facts:\\n    name: Enclosure-Name\\n    hostname: 172.16.101.48\\n    username: administrator\\n    password: my_password\\n    api_version: 500\\n  no_log: true\\n  delegate_to: localhost\\n- debug: var=enclosures\\n\\n- name: Gather facts about an Enclosure by name with options\\n  oneview_enclosure_facts:\\n    name: Test-Enclosure\\n    options:\\n      - script                       # optional\\n      - environmentalConfiguration   # optional\\n      - utilization                  # optional\\n    hostname: 172.16.101.48\\n    username: administrator\\n    password: my_password\\n    api_version: 500\\n  no_log: true\\n  delegate_to: localhost\\n- debug: var=enclosures\\n- debug: var=enclosure_script\\n- debug: var=enclosure_environmental_configuration\\n- debug: var=enclosure_utilization\\n\\n- name: \\"Gather facts about an Enclosure with temperature data at a resolution of one sample per day, between two\\n         specified dates\\"\\n  oneview_enclosure_facts:\\n    name: Test-Enclosure\\n    options:\\n      - utilization:                   # optional\\n          fields: AmbientTemperature\\n          filter:\\n            - startDate=2016-07-01T14:29:42.000Z\\n            - endDate=2017-07-01T03:29:42.000Z\\n          view: day\\n          refresh: false\\n    hostname: 172.16.101.48\\n    username: administrator\\n    password: my_password\\n    api_version: 500\\n  no_log: true\\n  delegate_to: localhost\\n- debug: var=enclosures\\n- debug: var=enclosure_utilization\\n\'\'\'\\n\\nRETURN = \'\'\'\\nenclosures:\\n    description: Has all the OneView facts about the Enclosures.\\n    returned: Always, but can be null.\\n    type: dict\\n\\nenclosure_script:\\n    description: Has all the OneView facts about the script of an Enclosure.\\n    returned: When requested, but can be null.\\n    type: string\\n\\nenclosure_environmental_configuration:\\n    description: Has all the OneView facts about the environmental configuration of an Enclosure.\\n    returned: When requested, but can be null.\\n    type: dict\\n\\nenclosure_utilization:\\n    description: Has all the OneView facts about the utilization of an Enclosure.\\n    returned: When requested, but can be null.\\n    type: dict\\n\'\'\'\\n\\nfrom ansible.module_utils.oneview import OneViewModuleBase\\n\\n\\nclass EnclosureFactsModule(OneViewModuleBase):\\n    argument_spec = dict(name=dict(type=\'str\'), options=dict(type=\'list\'), params=dict(type=\'dict\'))\\n\\n    def __init__(self):\\n        super(EnclosureFactsModule, self).__init__(additional_arg_spec=self.argument_spec)\\n\\n    def execute_module(self):\\n\\n        ansible_facts = {}\\n\\n        if self.module.params[\'name\']:\\n            enclosures = self._get_by_name(self.module.params[\'name\'])\\n\\n            if self.options and enclosures:\\n                ansible_facts = self._gather_optional_facts(self.options, enclosures[0])\\n        else:\\n            enclosures = self.oneview_client.enclosures.get_all(**self.facts_params)\\n\\n        ansible_facts[\'enclosures\'] = enclosures\\n\\n        return dict(changed=False,\\n                    ansible_facts=ansible_facts)\\n\\n    def _gather_optional_facts(self, options, enclosure):\\n\\n        enclosure_client = self.oneview_client.enclosures\\n        ansible_facts = {}\\n\\n        if options.get(\'script\'):\\n            ansible_facts[\'enclosure_script\'] = enclosure_client.get_script(enclosure[\'uri\'])\\n        if options.get(\'environmentalConfiguration\'):\\n            env_config = enclosure_client.get_environmental_configuration(enclosure[\'uri\'])\\n            ansible_facts[\'enclosure_environmental_configuration\'] = env_config\\n        if options.get(\'utilization\'):\\n            ansible_facts[\'enclosure_utilization\'] = self._get_utilization(enclosure, options[\'utilization\'])\\n\\n        return ansible_facts\\n\\n    def _get_utilization(self, enclosure, params):\\n        fields = view = refresh = filter = \'\'\\n\\n        if isinstance(params, dict):\\n            fields = params.get(\'fields\')\\n            view = params.get(\'view\')\\n            refresh = params.get(\'refresh\')\\n            filter = params.get(\'filter\')\\n\\n        return self.oneview_client.enclosures.get_utilization(enclosure[\'uri\'],\\n                                                              fields=fields,\\n                                                              filter=filter,\\n                                                              refresh=refresh,\\n                                                              view=view)\\n\\n    def _get_by_name(self, name):\\n        return self.oneview_client.enclosures.get_by(\'name\', name)\\n\\n\\ndef main():\\n    EnclosureFactsModule().run()\\n\\n\\nif __name__ == \'__main__\':\\n    main()\\n" }\n'
line: b'{ "repo_name": "etiennekruger/phantomjs-qt5", "ref": "refs/heads/qt5", "path": "src/breakpad/src/tools/gyp/test/home_dot_gyp/gyptest-home-includes-regyp.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2009 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nVerifies inclusion of $HOME/.gyp/includes.gypi works properly with relocation\\nand with regeneration.\\n\\"\\"\\"\\n\\nimport os\\nimport TestGyp\\n\\n# Regenerating build files when a gyp file changes is currently only supported\\n# by the make generator.\\ntest = TestGyp.TestGyp(formats=[\'make\'])\\n\\nos.environ[\'HOME\'] = os.path.abspath(\'home\')\\n\\ntest.run_gyp(\'all.gyp\', chdir=\'src\')\\n\\n# After relocating, we should still be able to build (build file shouldn\'t\\n# contain relative reference to ~/.gyp/includes.gypi)\\ntest.relocate(\'src\', \'relocate/src\')\\n\\ntest.build(\'all.gyp\', test.ALL, chdir=\'relocate/src\')\\n\\ntest.run_built_executable(\'printfoo\',\\n                          chdir=\'relocate/src\',\\n                          stdout=\\"FOO is fromhome\\\\n\\");\\n\\n# Building should notice any changes to ~/.gyp/includes.gypi and regyp.\\ntest.sleep()\\n\\ntest.write(\'home/.gyp/include.gypi\', test.read(\'home2/.gyp/include.gypi\'))\\n\\ntest.build(\'all.gyp\', test.ALL, chdir=\'relocate/src\')\\n\\ntest.run_built_executable(\'printfoo\',\\n                          chdir=\'relocate/src\',\\n                          stdout=\\"FOO is fromhome2\\\\n\\");\\n\\ntest.pass_test()\\n" }\n'
line: b'{ "repo_name": "you21979/phantomjs", "ref": "refs/heads/2.0", "path": "src/breakpad/src/tools/gyp/test/home_dot_gyp/gyptest-home-includes-regyp.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2009 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nVerifies inclusion of $HOME/.gyp/includes.gypi works properly with relocation\\nand with regeneration.\\n\\"\\"\\"\\n\\nimport os\\nimport TestGyp\\n\\n# Regenerating build files when a gyp file changes is currently only supported\\n# by the make generator.\\ntest = TestGyp.TestGyp(formats=[\'make\'])\\n\\nos.environ[\'HOME\'] = os.path.abspath(\'home\')\\n\\ntest.run_gyp(\'all.gyp\', chdir=\'src\')\\n\\n# After relocating, we should still be able to build (build file shouldn\'t\\n# contain relative reference to ~/.gyp/includes.gypi)\\ntest.relocate(\'src\', \'relocate/src\')\\n\\ntest.build(\'all.gyp\', test.ALL, chdir=\'relocate/src\')\\n\\ntest.run_built_executable(\'printfoo\',\\n                          chdir=\'relocate/src\',\\n                          stdout=\\"FOO is fromhome\\\\n\\");\\n\\n# Building should notice any changes to ~/.gyp/includes.gypi and regyp.\\ntest.sleep()\\n\\ntest.write(\'home/.gyp/include.gypi\', test.read(\'home2/.gyp/include.gypi\'))\\n\\ntest.build(\'all.gyp\', test.ALL, chdir=\'relocate/src\')\\n\\ntest.run_built_executable(\'printfoo\',\\n                          chdir=\'relocate/src\',\\n                          stdout=\\"FOO is fromhome2\\\\n\\");\\n\\ntest.pass_test()\\n" }\n'
line: b'{ "repo_name": "miptliot/edx-platform", "ref": "refs/heads/ginkgo_openedu_docker", "path": "cms/envs/devstack.py", "content": "\\"\\"\\"\\nSpecific overrides to the base prod settings to make development easier.\\n\\"\\"\\"\\n\\nfrom os.path import abspath, dirname, join\\n\\nfrom .aws import *  # pylint: disable=wildcard-import, unused-wildcard-import\\n\\n# Don\'t use S3 in devstack, fall back to filesystem\\ndel DEFAULT_FILE_STORAGE\\nCOURSE_IMPORT_EXPORT_STORAGE = \'django.core.files.storage.FileSystemStorage\'\\nUSER_TASKS_ARTIFACT_STORAGE = COURSE_IMPORT_EXPORT_STORAGE\\n\\nDEBUG = True\\nUSE_I18N = True\\nDEFAULT_TEMPLATE_ENGINE[\'OPTIONS\'][\'debug\'] = DEBUG\\nHTTPS = \'off\'\\n\\n################################ LOGGERS ######################################\\n\\nimport logging\\n\\n# Disable noisy loggers\\nfor pkg_name in [\'track.contexts\', \'track.middleware\', \'dd.dogapi\']:\\n    logging.getLogger(pkg_name).setLevel(logging.CRITICAL)\\n\\n\\n################################ EMAIL ########################################\\n\\nEMAIL_BACKEND = \'django.core.mail.backends.console.EmailBackend\'\\n\\n################################# LMS INTEGRATION #############################\\n\\nLMS_BASE = \\"localhost:8000\\"\\nLMS_ROOT_URL = \\"http://{}\\".format(LMS_BASE)\\nFEATURES[\'PREVIEW_LMS_BASE\'] = \\"preview.\\" + LMS_BASE\\n\\n########################### PIPELINE #################################\\n\\n# Skip packaging and optimization in development\\nPIPELINE_ENABLED = False\\nSTATICFILES_STORAGE = \'openedx.core.storage.DevelopmentStorage\'\\n\\n# Revert to the default set of finders as we don\'t want the production pipeline\\nSTATICFILES_FINDERS = [\\n    \'openedx.core.djangoapps.theming.finders.ThemeFilesFinder\',\\n    \'django.contrib.staticfiles.finders.FileSystemFinder\',\\n    \'django.contrib.staticfiles.finders.AppDirectoriesFinder\',\\n]\\n\\n############################ PYFS XBLOCKS SERVICE #############################\\n# Set configuration for Django pyfilesystem\\n\\nDJFS = {\\n    \'type\': \'osfs\',\\n    \'directory_root\': \'cms/static/djpyfs\',\\n    \'url_root\': \'/static/djpyfs\',\\n}\\n\\n################################# CELERY ######################################\\n\\n# By default don\'t use a worker, execute tasks as if they were local functions\\nCELERY_ALWAYS_EAGER = True\\n\\n################################ DEBUG TOOLBAR ################################\\nINSTALLED_APPS += (\'debug_toolbar\', \'debug_toolbar_mongo\')\\nMIDDLEWARE_CLASSES += (\'debug_toolbar.middleware.DebugToolbarMiddleware\',)\\nINTERNAL_IPS = (\'127.0.0.1\',)\\n\\nDEBUG_TOOLBAR_PANELS = (\\n    \'debug_toolbar.panels.versions.VersionsPanel\',\\n    \'debug_toolbar.panels.timer.TimerPanel\',\\n    \'debug_toolbar.panels.settings.SettingsPanel\',\\n    \'debug_toolbar.panels.headers.HeadersPanel\',\\n    \'debug_toolbar.panels.request.RequestPanel\',\\n    \'debug_toolbar.panels.sql.SQLPanel\',\\n    \'debug_toolbar.panels.signals.SignalsPanel\',\\n    \'debug_toolbar.panels.logging.LoggingPanel\',\\n    \'debug_toolbar.panels.profiling.ProfilingPanel\',\\n)\\n\\nDEBUG_TOOLBAR_CONFIG = {\\n    # Profile panel is incompatible with wrapped views\\n    # See https://github.com/jazzband/django-debug-toolbar/issues/792\\n    \'DISABLE_PANELS\': (\\n        \'debug_toolbar.panels.profiling.ProfilingPanel\',\\n    ),\\n    \'SHOW_TOOLBAR_CALLBACK\': \'cms.envs.devstack.should_show_debug_toolbar\',\\n    \'JQUERY_URL\': None,\\n}\\n\\n\\ndef should_show_debug_toolbar(_):\\n    return True  # We always want the toolbar on devstack regardless of IP, auth, etc.\\n\\n\\n# To see stacktraces for MongoDB queries, set this to True.\\n# Stacktraces slow down page loads drastically (for pages with lots of queries).\\nDEBUG_TOOLBAR_MONGO_STACKTRACES = False\\n\\n\\n################################ MILESTONES ################################\\nFEATURES[\'MILESTONES_APP\'] = True\\n\\n\\n################################ ENTRANCE EXAMS ################################\\nFEATURES[\'ENTRANCE_EXAMS\'] = True\\n\\n################################ COURSE LICENSES ################################\\nFEATURES[\'LICENSING\'] = True\\n# Needed to enable licensing on video modules\\nXBLOCK_SETTINGS.update({\'VideoDescriptor\': {\'licensing_enabled\': True}})\\n\\n################################ SEARCH INDEX ################################\\nFEATURES[\'ENABLE_COURSEWARE_INDEX\'] = True\\nFEATURES[\'ENABLE_LIBRARY_INDEX\'] = True\\nSEARCH_ENGINE = \\"search.elastic.ElasticSearchEngine\\"\\n\\n########################## Certificates Web/HTML View #######################\\nFEATURES[\'CERTIFICATES_HTML_VIEW\'] = True\\n\\n########################## AUTHOR PERMISSION #######################\\nFEATURES[\'ENABLE_CREATOR_GROUP\'] = False\\n\\n################################# DJANGO-REQUIRE ###############################\\n\\n# Whether to run django-require in debug mode.\\nREQUIRE_DEBUG = DEBUG\\n\\n########################### OAUTH2 #################################\\nOAUTH_OIDC_ISSUER = \'http://127.0.0.1:8000/oauth2\'\\n\\nJWT_AUTH.update({\\n    \'JWT_SECRET_KEY\': \'lms-secret\',\\n    \'JWT_ISSUER\': \'http://127.0.0.1:8000/oauth2\',\\n    \'JWT_AUDIENCE\': \'lms-key\',\\n})\\n\\n###############################################################################\\n# See if the developer has any local overrides.\\nif os.path.isfile(join(dirname(abspath(__file__)), \'private.py\')):\\n    from .private import *  # pylint: disable=import-error,wildcard-import\\n\\n#####################################################################\\n# Lastly, run any migrations, if needed.\\nMODULESTORE = convert_module_store_setting_if_needed(MODULESTORE)\\n\\n# Dummy secret key for dev\\nSECRET_KEY = \'85920908f28904ed733fe576320db18cabd7b6cd\'\\n" }\n'
line: b'{ "repo_name": "klim-iv/phantomjs-qt5", "ref": "refs/heads/qt5", "path": "src/breakpad/src/tools/gyp/test/home_dot_gyp/gyptest-home-includes-regyp.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2009 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nVerifies inclusion of $HOME/.gyp/includes.gypi works properly with relocation\\nand with regeneration.\\n\\"\\"\\"\\n\\nimport os\\nimport TestGyp\\n\\n# Regenerating build files when a gyp file changes is currently only supported\\n# by the make generator.\\ntest = TestGyp.TestGyp(formats=[\'make\'])\\n\\nos.environ[\'HOME\'] = os.path.abspath(\'home\')\\n\\ntest.run_gyp(\'all.gyp\', chdir=\'src\')\\n\\n# After relocating, we should still be able to build (build file shouldn\'t\\n# contain relative reference to ~/.gyp/includes.gypi)\\ntest.relocate(\'src\', \'relocate/src\')\\n\\ntest.build(\'all.gyp\', test.ALL, chdir=\'relocate/src\')\\n\\ntest.run_built_executable(\'printfoo\',\\n                          chdir=\'relocate/src\',\\n                          stdout=\\"FOO is fromhome\\\\n\\");\\n\\n# Building should notice any changes to ~/.gyp/includes.gypi and regyp.\\ntest.sleep()\\n\\ntest.write(\'home/.gyp/include.gypi\', test.read(\'home2/.gyp/include.gypi\'))\\n\\ntest.build(\'all.gyp\', test.ALL, chdir=\'relocate/src\')\\n\\ntest.run_built_executable(\'printfoo\',\\n                          chdir=\'relocate/src\',\\n                          stdout=\\"FOO is fromhome2\\\\n\\");\\n\\ntest.pass_test()\\n" }\n'
line: b'{ "repo_name": "UrLab/DocHub", "ref": "refs/heads/main", "path": "telepathy/models.py", "content": "import json\\n\\nfrom django.conf import settings\\nfrom django.db import models\\nfrom django.urls import reverse\\n\\n\\nclass Thread(models.Model):\\n    # Possible placement options\\n    PLACEMENT_OPTS = {\'page-no\': int}\\n\\n    name = models.CharField(max_length=255)\\n\\n    user = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.CASCADE)\\n    created = models.DateTimeField(auto_now_add=True)\\n    edited = models.DateTimeField(auto_now=True)\\n    placement = models.TextField(default=\\"\\", blank=True)\\n\\n    course = models.ForeignKey(\'catalog.Course\', on_delete=models.CASCADE)\\n    document = models.ForeignKey(\'documents.Document\', null=True, on_delete=models.CASCADE)\\n\\n    def __str__(self):\\n        return self.name\\n\\n    def fullname(self):\\n        return self.__str__()\\n\\n    @property\\n    def page_no(self):\\n        if self.placement:\\n            placement = json.loads(self.placement)\\n            if \'page-no\' in placement:\\n                return placement[\'page-no\']\\n        return None\\n\\n    def get_absolute_url(self):\\n        return reverse(\'thread_show\', args=(self.id, ))\\n\\n    def write_perm(self, user, moderated_courses):\\n        if user.id == self.user_id:\\n            return True\\n\\n        if self.course_id in moderated_courses:\\n            return True\\n\\n        return False\\n\\n    class Meta:\\n        ordering = [\'-created\']\\n\\n\\nclass Message(models.Model):\\n    user = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.CASCADE)\\n    thread = models.ForeignKey(Thread, db_index=True, on_delete=models.CASCADE)\\n    text = models.TextField()\\n    created = models.DateTimeField(auto_now_add=True)\\n    edited = models.DateTimeField(auto_now=True)\\n\\n    def __str__(self):\\n        return self.text\\n\\n    def fullname(self):\\n        return \\"un message\\"\\n\\n    def get_absolute_url(self):\\n        return reverse(\'thread_show\', args=(self.thread_id, )) + f\\"#message-{self.id}\\"\\n\\n    def write_perm(self, user, moderated_courses):\\n        if user.id == self.user_id:\\n            return True\\n\\n        if self.thread.course_id in moderated_courses:\\n            return True\\n\\n        return False\\n" }\n'
line: b'{ "repo_name": "apanda/phantomjs-intercept", "ref": "refs/heads/2.0", "path": "src/breakpad/src/tools/gyp/test/home_dot_gyp/gyptest-home-includes-regyp.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2009 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nVerifies inclusion of $HOME/.gyp/includes.gypi works properly with relocation\\nand with regeneration.\\n\\"\\"\\"\\n\\nimport os\\nimport TestGyp\\n\\n# Regenerating build files when a gyp file changes is currently only supported\\n# by the make generator.\\ntest = TestGyp.TestGyp(formats=[\'make\'])\\n\\nos.environ[\'HOME\'] = os.path.abspath(\'home\')\\n\\ntest.run_gyp(\'all.gyp\', chdir=\'src\')\\n\\n# After relocating, we should still be able to build (build file shouldn\'t\\n# contain relative reference to ~/.gyp/includes.gypi)\\ntest.relocate(\'src\', \'relocate/src\')\\n\\ntest.build(\'all.gyp\', test.ALL, chdir=\'relocate/src\')\\n\\ntest.run_built_executable(\'printfoo\',\\n                          chdir=\'relocate/src\',\\n                          stdout=\\"FOO is fromhome\\\\n\\");\\n\\n# Building should notice any changes to ~/.gyp/includes.gypi and regyp.\\ntest.sleep()\\n\\ntest.write(\'home/.gyp/include.gypi\', test.read(\'home2/.gyp/include.gypi\'))\\n\\ntest.build(\'all.gyp\', test.ALL, chdir=\'relocate/src\')\\n\\ntest.run_built_executable(\'printfoo\',\\n                          chdir=\'relocate/src\',\\n                          stdout=\\"FOO is fromhome2\\\\n\\");\\n\\ntest.pass_test()\\n" }\n'
line: b'{ "repo_name": "danigonza/phantomjs", "ref": "refs/heads/webfonts", "path": "src/breakpad/src/tools/gyp/test/home_dot_gyp/gyptest-home-includes-regyp.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2009 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nVerifies inclusion of $HOME/.gyp/includes.gypi works properly with relocation\\nand with regeneration.\\n\\"\\"\\"\\n\\nimport os\\nimport TestGyp\\n\\n# Regenerating build files when a gyp file changes is currently only supported\\n# by the make generator.\\ntest = TestGyp.TestGyp(formats=[\'make\'])\\n\\nos.environ[\'HOME\'] = os.path.abspath(\'home\')\\n\\ntest.run_gyp(\'all.gyp\', chdir=\'src\')\\n\\n# After relocating, we should still be able to build (build file shouldn\'t\\n# contain relative reference to ~/.gyp/includes.gypi)\\ntest.relocate(\'src\', \'relocate/src\')\\n\\ntest.build(\'all.gyp\', test.ALL, chdir=\'relocate/src\')\\n\\ntest.run_built_executable(\'printfoo\',\\n                          chdir=\'relocate/src\',\\n                          stdout=\\"FOO is fromhome\\\\n\\");\\n\\n# Building should notice any changes to ~/.gyp/includes.gypi and regyp.\\ntest.sleep()\\n\\ntest.write(\'home/.gyp/include.gypi\', test.read(\'home2/.gyp/include.gypi\'))\\n\\ntest.build(\'all.gyp\', test.ALL, chdir=\'relocate/src\')\\n\\ntest.run_built_executable(\'printfoo\',\\n                          chdir=\'relocate/src\',\\n                          stdout=\\"FOO is fromhome2\\\\n\\");\\n\\ntest.pass_test()\\n" }\n'
line: b'{ "repo_name": "VinceZK/phantomjs", "ref": "refs/heads/decktape", "path": "src/breakpad/src/tools/gyp/test/home_dot_gyp/gyptest-home-includes-regyp.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2009 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nVerifies inclusion of $HOME/.gyp/includes.gypi works properly with relocation\\nand with regeneration.\\n\\"\\"\\"\\n\\nimport os\\nimport TestGyp\\n\\n# Regenerating build files when a gyp file changes is currently only supported\\n# by the make generator.\\ntest = TestGyp.TestGyp(formats=[\'make\'])\\n\\nos.environ[\'HOME\'] = os.path.abspath(\'home\')\\n\\ntest.run_gyp(\'all.gyp\', chdir=\'src\')\\n\\n# After relocating, we should still be able to build (build file shouldn\'t\\n# contain relative reference to ~/.gyp/includes.gypi)\\ntest.relocate(\'src\', \'relocate/src\')\\n\\ntest.build(\'all.gyp\', test.ALL, chdir=\'relocate/src\')\\n\\ntest.run_built_executable(\'printfoo\',\\n                          chdir=\'relocate/src\',\\n                          stdout=\\"FOO is fromhome\\\\n\\");\\n\\n# Building should notice any changes to ~/.gyp/includes.gypi and regyp.\\ntest.sleep()\\n\\ntest.write(\'home/.gyp/include.gypi\', test.read(\'home2/.gyp/include.gypi\'))\\n\\ntest.build(\'all.gyp\', test.ALL, chdir=\'relocate/src\')\\n\\ntest.run_built_executable(\'printfoo\',\\n                          chdir=\'relocate/src\',\\n                          stdout=\\"FOO is fromhome2\\\\n\\");\\n\\ntest.pass_test()\\n" }\n'
line: b'{ "repo_name": "raccoongang/edx-platform", "ref": "refs/heads/ginkgo-rg", "path": "cms/envs/devstack.py", "content": "\\"\\"\\"\\nSpecific overrides to the base prod settings to make development easier.\\n\\"\\"\\"\\n\\nfrom os.path import abspath, dirname, join\\n\\nfrom .aws import *  # pylint: disable=wildcard-import, unused-wildcard-import\\n\\n# Don\'t use S3 in devstack, fall back to filesystem\\ndel DEFAULT_FILE_STORAGE\\nCOURSE_IMPORT_EXPORT_STORAGE = \'django.core.files.storage.FileSystemStorage\'\\nUSER_TASKS_ARTIFACT_STORAGE = COURSE_IMPORT_EXPORT_STORAGE\\n\\nDEBUG = True\\nUSE_I18N = True\\nDEFAULT_TEMPLATE_ENGINE[\'OPTIONS\'][\'debug\'] = DEBUG\\nHTTPS = \'off\'\\n\\n################################ LOGGERS ######################################\\n\\nimport logging\\n\\n# Disable noisy loggers\\nfor pkg_name in [\'track.contexts\', \'track.middleware\', \'dd.dogapi\']:\\n    logging.getLogger(pkg_name).setLevel(logging.CRITICAL)\\n\\n\\n################################ EMAIL ########################################\\n\\nEMAIL_BACKEND = \'django.core.mail.backends.console.EmailBackend\'\\n\\n################################# LMS INTEGRATION #############################\\n\\nLMS_BASE = \\"localhost:8000\\"\\nLMS_ROOT_URL = \\"http://{}\\".format(LMS_BASE)\\nFEATURES[\'PREVIEW_LMS_BASE\'] = \\"preview.\\" + LMS_BASE\\n\\n########################### PIPELINE #################################\\n\\n# Skip packaging and optimization in development\\nPIPELINE_ENABLED = False\\nSTATICFILES_STORAGE = \'openedx.core.storage.DevelopmentStorage\'\\n\\n# Revert to the default set of finders as we don\'t want the production pipeline\\nSTATICFILES_FINDERS = [\\n    \'openedx.core.djangoapps.theming.finders.ThemeFilesFinder\',\\n    \'django.contrib.staticfiles.finders.FileSystemFinder\',\\n    \'django.contrib.staticfiles.finders.AppDirectoriesFinder\',\\n]\\n\\n############################ PYFS XBLOCKS SERVICE #############################\\n# Set configuration for Django pyfilesystem\\n\\nDJFS = {\\n    \'type\': \'osfs\',\\n    \'directory_root\': \'cms/static/djpyfs\',\\n    \'url_root\': \'/static/djpyfs\',\\n}\\n\\n################################# CELERY ######################################\\n\\n# By default don\'t use a worker, execute tasks as if they were local functions\\nCELERY_ALWAYS_EAGER = True\\n\\n################################ DEBUG TOOLBAR ################################\\nINSTALLED_APPS += (\'debug_toolbar\', \'debug_toolbar_mongo\')\\nMIDDLEWARE_CLASSES += (\'debug_toolbar.middleware.DebugToolbarMiddleware\',)\\nINTERNAL_IPS = (\'127.0.0.1\',)\\n\\nDEBUG_TOOLBAR_PANELS = (\\n    \'debug_toolbar.panels.versions.VersionsPanel\',\\n    \'debug_toolbar.panels.timer.TimerPanel\',\\n    \'debug_toolbar.panels.settings.SettingsPanel\',\\n    \'debug_toolbar.panels.headers.HeadersPanel\',\\n    \'debug_toolbar.panels.request.RequestPanel\',\\n    \'debug_toolbar.panels.sql.SQLPanel\',\\n    \'debug_toolbar.panels.signals.SignalsPanel\',\\n    \'debug_toolbar.panels.logging.LoggingPanel\',\\n    \'debug_toolbar.panels.profiling.ProfilingPanel\',\\n)\\n\\nDEBUG_TOOLBAR_CONFIG = {\\n    # Profile panel is incompatible with wrapped views\\n    # See https://github.com/jazzband/django-debug-toolbar/issues/792\\n    \'DISABLE_PANELS\': (\\n        \'debug_toolbar.panels.profiling.ProfilingPanel\',\\n    ),\\n    \'SHOW_TOOLBAR_CALLBACK\': \'cms.envs.devstack.should_show_debug_toolbar\',\\n    \'JQUERY_URL\': None,\\n}\\n\\n\\ndef should_show_debug_toolbar(_):\\n    return True  # We always want the toolbar on devstack regardless of IP, auth, etc.\\n\\n\\n# To see stacktraces for MongoDB queries, set this to True.\\n# Stacktraces slow down page loads drastically (for pages with lots of queries).\\nDEBUG_TOOLBAR_MONGO_STACKTRACES = False\\n\\n\\n################################ MILESTONES ################################\\nFEATURES[\'MILESTONES_APP\'] = True\\n\\n\\n################################ ENTRANCE EXAMS ################################\\nFEATURES[\'ENTRANCE_EXAMS\'] = True\\n\\n################################ COURSE LICENSES ################################\\nFEATURES[\'LICENSING\'] = True\\n# Needed to enable licensing on video modules\\nXBLOCK_SETTINGS.update({\'VideoDescriptor\': {\'licensing_enabled\': True}})\\n\\n################################ SEARCH INDEX ################################\\nFEATURES[\'ENABLE_COURSEWARE_INDEX\'] = True\\nFEATURES[\'ENABLE_LIBRARY_INDEX\'] = True\\nSEARCH_ENGINE = \\"search.elastic.ElasticSearchEngine\\"\\n\\n########################## Certificates Web/HTML View #######################\\nFEATURES[\'CERTIFICATES_HTML_VIEW\'] = True\\n\\n########################## AUTHOR PERMISSION #######################\\nFEATURES[\'ENABLE_CREATOR_GROUP\'] = False\\n\\n################################# DJANGO-REQUIRE ###############################\\n\\n# Whether to run django-require in debug mode.\\nREQUIRE_DEBUG = DEBUG\\n\\n########################### OAUTH2 #################################\\nOAUTH_OIDC_ISSUER = \'http://127.0.0.1:8000/oauth2\'\\n\\nJWT_AUTH.update({\\n    \'JWT_SECRET_KEY\': \'lms-secret\',\\n    \'JWT_ISSUER\': \'http://127.0.0.1:8000/oauth2\',\\n    \'JWT_AUDIENCE\': \'lms-key\',\\n})\\n\\n###############################################################################\\n# See if the developer has any local overrides.\\nif os.path.isfile(join(dirname(abspath(__file__)), \'private.py\')):\\n    from .private import *  # pylint: disable=import-error,wildcard-import\\n\\n#####################################################################\\n# Lastly, run any migrations, if needed.\\nMODULESTORE = convert_module_store_setting_if_needed(MODULESTORE)\\n\\n# Dummy secret key for dev\\nSECRET_KEY = \'85920908f28904ed733fe576320db18cabd7b6cd\'\\n" }\n'
line: b'{ "repo_name": "virtualopensystems/nova", "ref": "refs/heads/bp/vif-vhostuser", "path": "nova/virt/vmwareapi/constants.py", "content": "# Copyright (c) 2014 VMware, Inc.\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\n\\"\\"\\"\\nShared constants across the VMware driver\\n\\"\\"\\"\\n\\nfrom nova.network import model as network_model\\n\\nDISK_FORMAT_ISO = \'iso\'\\nDISK_FORMAT_VMDK = \'vmdk\'\\nDISK_FORMATS_ALL = [DISK_FORMAT_ISO, DISK_FORMAT_VMDK]\\n\\nDISK_TYPE_SPARSE = \'sparse\'\\nDISK_TYPE_PREALLOCATED = \'preallocated\'\\n\\nDEFAULT_VIF_MODEL = network_model.VIF_MODEL_E1000\\nDEFAULT_OS_TYPE = \\"otherGuest\\"\\nDEFAULT_ADAPTER_TYPE = \\"lsiLogic\\"\\nDEFAULT_DISK_TYPE = DISK_TYPE_PREALLOCATED\\nDEFAULT_DISK_FORMAT = DISK_FORMAT_VMDK\\n" }\n'
line: b'{ "repo_name": "sgerhart/ansible", "ref": "refs/heads/maintenance_group_node_module", "path": "lib/ansible/modules/remote_management/oneview/oneview_enclosure_facts.py", "content": "#!/usr/bin/python\\n\\n# Copyright: (c) 2016-2017, Hewlett Packard Enterprise Development LP\\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\\n\\nfrom __future__ import (absolute_import, division, print_function)\\n__metaclass__ = type\\n\\nANSIBLE_METADATA = {\'metadata_version\': \'1.1\',\\n                    \'status\': [\'preview\'],\\n                    \'supported_by\': \'community\'}\\n\\nDOCUMENTATION = \'\'\'\\n---\\nmodule: oneview_enclosure_facts\\nshort_description: Retrieve facts about one or more Enclosures\\ndescription:\\n    - Retrieve facts about one or more of the Enclosures from OneView.\\nversion_added: \\"2.5\\"\\nrequirements:\\n    - hpOneView >= 2.0.1\\nauthor:\\n    - Felipe Bulsoni (@fgbulsoni)\\n    - Thiago Miotto (@tmiotto)\\n    - Adriane Cardozo (@adriane-cardozo)\\noptions:\\n    name:\\n      description:\\n        - Enclosure name.\\n    options:\\n      description:\\n        - \\"List with options to gather additional facts about an Enclosure and related resources.\\n          Options allowed: C(script), C(environmentalConfiguration), and C(utilization). For the option C(utilization),\\n          you can provide specific parameters.\\"\\n\\nextends_documentation_fragment:\\n    - oneview\\n    - oneview.factsparams\\n\'\'\'\\n\\nEXAMPLES = \'\'\'\\n- name: Gather facts about all Enclosures\\n  oneview_enclosure_facts:\\n    hostname: 172.16.101.48\\n    username: administrator\\n    password: my_password\\n    api_version: 500\\n  no_log: true\\n  delegate_to: localhost\\n- debug: var=enclosures\\n\\n- name: Gather paginated, filtered and sorted facts about Enclosures\\n  oneview_enclosure_facts:\\n    params:\\n      start: 0\\n      count: 3\\n      sort: name:descending\\n      filter: status=OK\\n    hostname: 172.16.101.48\\n    username: administrator\\n    password: my_password\\n    api_version: 500\\n  no_log: true\\n  delegate_to: localhost\\n- debug: var=enclosures\\n\\n- name: Gather facts about an Enclosure by name\\n  oneview_enclosure_facts:\\n    name: Enclosure-Name\\n    hostname: 172.16.101.48\\n    username: administrator\\n    password: my_password\\n    api_version: 500\\n  no_log: true\\n  delegate_to: localhost\\n- debug: var=enclosures\\n\\n- name: Gather facts about an Enclosure by name with options\\n  oneview_enclosure_facts:\\n    name: Test-Enclosure\\n    options:\\n      - script                       # optional\\n      - environmentalConfiguration   # optional\\n      - utilization                  # optional\\n    hostname: 172.16.101.48\\n    username: administrator\\n    password: my_password\\n    api_version: 500\\n  no_log: true\\n  delegate_to: localhost\\n- debug: var=enclosures\\n- debug: var=enclosure_script\\n- debug: var=enclosure_environmental_configuration\\n- debug: var=enclosure_utilization\\n\\n- name: \\"Gather facts about an Enclosure with temperature data at a resolution of one sample per day, between two\\n         specified dates\\"\\n  oneview_enclosure_facts:\\n    name: Test-Enclosure\\n    options:\\n      - utilization:                   # optional\\n          fields: AmbientTemperature\\n          filter:\\n            - startDate=2016-07-01T14:29:42.000Z\\n            - endDate=2017-07-01T03:29:42.000Z\\n          view: day\\n          refresh: false\\n    hostname: 172.16.101.48\\n    username: administrator\\n    password: my_password\\n    api_version: 500\\n  no_log: true\\n  delegate_to: localhost\\n- debug: var=enclosures\\n- debug: var=enclosure_utilization\\n\'\'\'\\n\\nRETURN = \'\'\'\\nenclosures:\\n    description: Has all the OneView facts about the Enclosures.\\n    returned: Always, but can be null.\\n    type: dict\\n\\nenclosure_script:\\n    description: Has all the OneView facts about the script of an Enclosure.\\n    returned: When requested, but can be null.\\n    type: string\\n\\nenclosure_environmental_configuration:\\n    description: Has all the OneView facts about the environmental configuration of an Enclosure.\\n    returned: When requested, but can be null.\\n    type: dict\\n\\nenclosure_utilization:\\n    description: Has all the OneView facts about the utilization of an Enclosure.\\n    returned: When requested, but can be null.\\n    type: dict\\n\'\'\'\\n\\nfrom ansible.module_utils.oneview import OneViewModuleBase\\n\\n\\nclass EnclosureFactsModule(OneViewModuleBase):\\n    argument_spec = dict(name=dict(type=\'str\'), options=dict(type=\'list\'), params=dict(type=\'dict\'))\\n\\n    def __init__(self):\\n        super(EnclosureFactsModule, self).__init__(additional_arg_spec=self.argument_spec)\\n\\n    def execute_module(self):\\n\\n        ansible_facts = {}\\n\\n        if self.module.params[\'name\']:\\n            enclosures = self._get_by_name(self.module.params[\'name\'])\\n\\n            if self.options and enclosures:\\n                ansible_facts = self._gather_optional_facts(self.options, enclosures[0])\\n        else:\\n            enclosures = self.oneview_client.enclosures.get_all(**self.facts_params)\\n\\n        ansible_facts[\'enclosures\'] = enclosures\\n\\n        return dict(changed=False,\\n                    ansible_facts=ansible_facts)\\n\\n    def _gather_optional_facts(self, options, enclosure):\\n\\n        enclosure_client = self.oneview_client.enclosures\\n        ansible_facts = {}\\n\\n        if options.get(\'script\'):\\n            ansible_facts[\'enclosure_script\'] = enclosure_client.get_script(enclosure[\'uri\'])\\n        if options.get(\'environmentalConfiguration\'):\\n            env_config = enclosure_client.get_environmental_configuration(enclosure[\'uri\'])\\n            ansible_facts[\'enclosure_environmental_configuration\'] = env_config\\n        if options.get(\'utilization\'):\\n            ansible_facts[\'enclosure_utilization\'] = self._get_utilization(enclosure, options[\'utilization\'])\\n\\n        return ansible_facts\\n\\n    def _get_utilization(self, enclosure, params):\\n        fields = view = refresh = filter = \'\'\\n\\n        if isinstance(params, dict):\\n            fields = params.get(\'fields\')\\n            view = params.get(\'view\')\\n            refresh = params.get(\'refresh\')\\n            filter = params.get(\'filter\')\\n\\n        return self.oneview_client.enclosures.get_utilization(enclosure[\'uri\'],\\n                                                              fields=fields,\\n                                                              filter=filter,\\n                                                              refresh=refresh,\\n                                                              view=view)\\n\\n    def _get_by_name(self, name):\\n        return self.oneview_client.enclosures.get_by(\'name\', name)\\n\\n\\ndef main():\\n    EnclosureFactsModule().run()\\n\\n\\nif __name__ == \'__main__\':\\n    main()\\n" }\n'
line: b'{ "repo_name": "nawawi/poedit", "ref": "refs/heads/stable", "path": "deps/boost/libs/python/test/callbacks.py", "content": "# Copyright David Abrahams 2004. Distributed under the Boost\\n# Software License, Version 1.0. (See accompanying\\n# file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)\\n\'\'\'\\n>>> from callbacks_ext import *\\n\\n>>> def double(x):\\n...     return x + x\\n...\\n>>> apply_int_int(double, 42)\\n84\\n>>> apply_void_int(double, 42)\\n\\n>>> def identity(x):\\n...     return x\\n\\nOnce we have array conversion support, this test will fail. Er,\\nsucceed<wink>:\\n\\n>>> try: apply_to_string_literal(identity)\\n... except ReferenceError: pass # expected\\n... else: print(\'expected an exception!\')\\n\\n>>> try: apply_X_ref_handle(lambda ignored:X(42), None)\\n... except ReferenceError: pass # expected\\n... else: print(\'expected an exception!\')\\n\\n>>> x = X(42)\\n>>> x.y = X(7)\\n>>> apply_X_ref_handle(lambda z:z.y, x).value()\\n7\\n\\n>>> x = apply_X_X(identity, X(42))\\n>>> x.value()\\n42\\n>>> x_count()\\n1\\n>>> del x\\n>>> x_count()\\n0\\n\\n>>> def increment(x):\\n...     x.set(x.value() + 1)\\n...\\n>>> x = X(42)\\n>>> apply_void_X_ref(increment, x)\\n>>> x.value()\\n43\\n\\n>>> apply_void_X_cref(increment, x) \\n>>> x.value()  # const-ness is not respected, sorry!\\n44\\n\\n>>> last_x = 1\\n>>> def decrement(x):\\n...     global last_x\\n...     last_x = x\\n...     if x is not None:\\n...         x.set(x.value() - 1)\\n\\n>>> apply_void_X_ptr(decrement, x)\\n>>> x.value()\\n43\\n>>> last_x.value()\\n43\\n>>> increment(last_x)\\n>>> x.value()\\n44\\n>>> last_x.value()\\n44\\n\\n>>> apply_void_X_ptr(decrement, None)\\n>>> assert last_x is None\\n>>> x.value()\\n44\\n\\n>>> last_x = 1\\n>>> apply_void_X_deep_ptr(decrement, None)\\n>>> assert last_x is None\\n>>> x.value()\\n44\\n\\n>>> apply_void_X_deep_ptr(decrement, x)\\n>>> x.value()\\n44\\n>>> last_x.value()\\n43\\n\\n>>> y = apply_X_ref_handle(identity, x)\\n>>> assert y.value() == x.value()\\n>>> increment(x)\\n>>> assert y.value() == x.value()\\n\\n>>> y = apply_X_ptr_handle_cref(identity, x)\\n>>> assert y.value() == x.value()\\n>>> increment(x)\\n>>> assert y.value() == x.value()\\n\\n>>> y = apply_X_ptr_handle_cref(identity, None)\\n>>> y\\n\\n>>> def new_x(ignored):\\n...     return X(666)\\n...\\n>>> try: apply_X_ref_handle(new_x, 1)\\n... except ReferenceError: pass\\n... else: print(\'no error\')\\n\\n>>> try: apply_X_ptr_handle_cref(new_x, 1)\\n... except ReferenceError: pass\\n... else: print(\'no error\')\\n\\n>>> try: apply_cstring_cstring(identity, \'hello\')\\n... except ReferenceError: pass\\n... else: print(\'no error\')\\n\\n>>> apply_char_char(identity, \'x\')\\n\'x\'\\n\\n>>> apply_cstring_pyobject(identity, \'hello\')\\n\'hello\'\\n\\n>>> apply_cstring_pyobject(identity, None)\\n\\n\\n>>> apply_char_char(identity, \'x\')\\n\'x\'\\n\\n>>> assert apply_to_own_type(identity) is type(identity)\\n\\n>>> assert apply_object_object(identity, identity) is identity\\n\'\'\'\\n\\ndef run(args = None):\\n    import sys\\n    import doctest\\n\\n    if args is not None:\\n        sys.argv = args\\n    return doctest.testmod(sys.modules.get(__name__))\\n    \\nif __name__ == \'__main__\':\\n    print(\\"running...\\")\\n    import sys\\n    status = run()[0]\\n    if (status == 0): print(\\"Done.\\")\\n    sys.exit(status)\\n" }\n'
line: b'{ "repo_name": "baslr/ArangoDB", "ref": "refs/heads/3.1-silent", "path": "3rdParty/boost/1.62.0/libs/python/test/callbacks.py", "content": "# Copyright David Abrahams 2004. Distributed under the Boost\\n# Software License, Version 1.0. (See accompanying\\n# file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)\\n\'\'\'\\n>>> from callbacks_ext import *\\n\\n>>> def double(x):\\n...     return x + x\\n...\\n>>> apply_int_int(double, 42)\\n84\\n>>> apply_void_int(double, 42)\\n\\n>>> def identity(x):\\n...     return x\\n\\nOnce we have array conversion support, this test will fail. Er,\\nsucceed<wink>:\\n\\n>>> try: apply_to_string_literal(identity)\\n... except ReferenceError: pass # expected\\n... else: print(\'expected an exception!\')\\n\\n>>> try: apply_X_ref_handle(lambda ignored:X(42), None)\\n... except ReferenceError: pass # expected\\n... else: print(\'expected an exception!\')\\n\\n>>> x = X(42)\\n>>> x.y = X(7)\\n>>> apply_X_ref_handle(lambda z:z.y, x).value()\\n7\\n\\n>>> x = apply_X_X(identity, X(42))\\n>>> x.value()\\n42\\n>>> x_count()\\n1\\n>>> del x\\n>>> x_count()\\n0\\n\\n>>> def increment(x):\\n...     x.set(x.value() + 1)\\n...\\n>>> x = X(42)\\n>>> apply_void_X_ref(increment, x)\\n>>> x.value()\\n43\\n\\n>>> apply_void_X_cref(increment, x) \\n>>> x.value()  # const-ness is not respected, sorry!\\n44\\n\\n>>> last_x = 1\\n>>> def decrement(x):\\n...     global last_x\\n...     last_x = x\\n...     if x is not None:\\n...         x.set(x.value() - 1)\\n\\n>>> apply_void_X_ptr(decrement, x)\\n>>> x.value()\\n43\\n>>> last_x.value()\\n43\\n>>> increment(last_x)\\n>>> x.value()\\n44\\n>>> last_x.value()\\n44\\n\\n>>> apply_void_X_ptr(decrement, None)\\n>>> assert last_x is None\\n>>> x.value()\\n44\\n\\n>>> last_x = 1\\n>>> apply_void_X_deep_ptr(decrement, None)\\n>>> assert last_x is None\\n>>> x.value()\\n44\\n\\n>>> apply_void_X_deep_ptr(decrement, x)\\n>>> x.value()\\n44\\n>>> last_x.value()\\n43\\n\\n>>> y = apply_X_ref_handle(identity, x)\\n>>> assert y.value() == x.value()\\n>>> increment(x)\\n>>> assert y.value() == x.value()\\n\\n>>> y = apply_X_ptr_handle_cref(identity, x)\\n>>> assert y.value() == x.value()\\n>>> increment(x)\\n>>> assert y.value() == x.value()\\n\\n>>> y = apply_X_ptr_handle_cref(identity, None)\\n>>> y\\n\\n>>> def new_x(ignored):\\n...     return X(666)\\n...\\n>>> try: apply_X_ref_handle(new_x, 1)\\n... except ReferenceError: pass\\n... else: print(\'no error\')\\n\\n>>> try: apply_X_ptr_handle_cref(new_x, 1)\\n... except ReferenceError: pass\\n... else: print(\'no error\')\\n\\n>>> try: apply_cstring_cstring(identity, \'hello\')\\n... except ReferenceError: pass\\n... else: print(\'no error\')\\n\\n>>> apply_char_char(identity, \'x\')\\n\'x\'\\n\\n>>> apply_cstring_pyobject(identity, \'hello\')\\n\'hello\'\\n\\n>>> apply_cstring_pyobject(identity, None)\\n\\n\\n>>> apply_char_char(identity, \'x\')\\n\'x\'\\n\\n>>> assert apply_to_own_type(identity) is type(identity)\\n\\n>>> assert apply_object_object(identity, identity) is identity\\n\'\'\'\\n\\ndef run(args = None):\\n    import sys\\n    import doctest\\n\\n    if args is not None:\\n        sys.argv = args\\n    return doctest.testmod(sys.modules.get(__name__))\\n    \\nif __name__ == \'__main__\':\\n    print(\\"running...\\")\\n    import sys\\n    status = run()[0]\\n    if (status == 0): print(\\"Done.\\")\\n    sys.exit(status)\\n" }\n'
line: b'{ "repo_name": "jrossyra/adaptivemd", "ref": "refs/heads/rp_integration", "path": "adaptivemd/plan.py", "content": "import types\\n\\n\\nclass ExecutionPlan(object):\\n    \\"\\"\\"\\n    An wrap to turn python function into asynchronous execution\\n\\n    The function is executed on start and interrupted if you use\\n    ``yield {(list of )condition to continue}``\\n\\n    To make writing of asynchronous code easy you can use this wrapper class.\\n    Usually you start by opening a scheduler that you submit tasks to. Then\\n    submit a first task or yield a condition to wait for. Once this is met the\\n    code will continue to execute and you can submit more tasks until finally\\n    you will close the scheduler\\n\\n    \\"\\"\\"\\n    def __init__(self, generator):\\n        \\"\\"\\"\\n        Parameters\\n        ----------\\n        generator : function\\n            the function (generator) to be used\\n\\n        \\"\\"\\"\\n        super(ExecutionPlan, self).__init__()\\n\\n        if not isinstance(generator, types.GeneratorType):\\n            generator = generator()\\n\\n        assert isinstance(generator, types.GeneratorType)\\n\\n        self._generator = generator\\n        self._running = True\\n        self._finish_conditions = []\\n\\n    def _update_conditions(self):\\n        self._finish_conditions = [x for x in self._finish_conditions if not x()]\\n\\n    def __call__(self):\\n        if self._running:\\n            try:\\n                conditions = next(self._generator)\\n                if conditions is not None:\\n                    if isinstance(conditions, (tuple, list)):\\n                        self._finish_conditions.extend(conditions)\\n                    else:\\n                        self._finish_conditions.append(conditions)\\n                self._update_conditions()\\n            except StopIteration:\\n                self._running = False\\n\\n    def trigger(self):\\n        if self:\\n            self._update_conditions()\\n            while self._running and len(self._finish_conditions) == 0:\\n                self()\\n                self._update_conditions()\\n\\n    def __bool__(self):\\n        return self._running\\n\\n    def __nonzero__(self):\\n        return self.__bool__()\\n\\n    def __str__(self):\\n        return \'%s(%s)\' % (\\n            self.__class__.__name__,\\n            \'active\' if self else \'------\'\\n        )\\n\\n    @property\\n    def on_done(self):\\n        \\"\\"\\"\\n        Return a `Condition` that is True once the event is finished\\n\\n        Returns\\n        -------\\n\\n        \\"\\"\\"\\n        return lambda: not bool(self)\\n" }\n'
line: b'{ "repo_name": "alexryndin/ambari", "ref": "refs/heads/branch-adh-1.5", "path": "ambari-server/src/main/resources/common-services/FALCON/0.5.0.2.1/package/scripts/params_linux.py", "content": "\\"\\"\\"\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \\"AS IS\\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n\\"\\"\\"\\nimport status_params\\n\\nfrom resource_management.libraries.resources.hdfs_resource import HdfsResource\\nfrom resource_management.libraries.functions import stack_select\\nfrom resource_management.libraries.functions import format\\nfrom resource_management.libraries.functions.default import default\\nfrom resource_management.libraries.functions.get_not_managed_resources import get_not_managed_resources\\nfrom resource_management.libraries.functions import get_kinit_path\\nfrom resource_management.libraries.script.script import Script\\nimport os\\nfrom resource_management.libraries.functions.expect import expect\\nfrom resource_management.libraries.functions.stack_features import check_stack_feature\\nfrom resource_management.libraries.functions.version import format_stack_version\\nfrom resource_management.libraries.functions import StackFeature\\nfrom resource_management.libraries.functions.setup_atlas_hook import has_atlas_in_cluster\\n\\nconfig = Script.get_config()\\nstack_root = status_params.stack_root\\nstack_name = status_params.stack_name\\n\\nagent_stack_retry_on_unavailability = config[\'hostLevelParams\'][\'agent_stack_retry_on_unavailability\']\\nagent_stack_retry_count = expect(\\"/hostLevelParams/agent_stack_retry_count\\", int)\\n\\n# New Cluster Stack Version that is defined during the RESTART of a Rolling Upgrade\\nversion = default(\\"/commandParams/version\\", None)\\n\\nstack_version_unformatted = status_params.stack_version_unformatted\\nstack_version_formatted = status_params.stack_version_formatted\\nupgrade_direction = default(\\"/commandParams/upgrade_direction\\", None)\\njdk_location = config[\'hostLevelParams\'][\'jdk_location\']\\n\\n# current host stack version\\ncurrent_version = default(\\"/hostLevelParams/current_version\\", None)\\ncurrent_version_formatted = format_stack_version(current_version)\\n\\netc_prefix_dir = \\"/etc/falcon\\"\\n\\n# hadoop params\\nhadoop_home_dir = stack_select.get_hadoop_dir(\\"home\\")\\nhadoop_bin_dir = stack_select.get_hadoop_dir(\\"bin\\")\\n\\nif stack_version_formatted and check_stack_feature(StackFeature.ROLLING_UPGRADE, stack_version_formatted):\\n  # if this is a server action, then use the server binaries; smoke tests\\n  # use the client binaries\\n  server_role_dir_mapping = { \'FALCON_SERVER\' : \'falcon-server\',\\n    \'FALCON_SERVICE_CHECK\' : \'falcon-client\' }\\n\\n  command_role = default(\\"/role\\", \\"\\")\\n  if command_role not in server_role_dir_mapping:\\n    command_role = \'FALCON_SERVICE_CHECK\'\\n\\n  falcon_root = server_role_dir_mapping[command_role]\\n  falcon_webapp_dir = format(\'{stack_root}/current/{falcon_root}/webapp\')\\n  falcon_home = format(\'{stack_root}/current/{falcon_root}\')\\n\\n  # Extensions dir is only available in HDP 2.5 and higher\\n  falcon_extensions_source_dir = os.path.join(stack_root, \\"current\\", falcon_root, \\"extensions\\")\\n  # Dir in HDFS\\n  falcon_extensions_dest_dir = default(\\"/configurations/falcon-startup.properties/*.extension.store.uri\\", \\"/apps/falcon/extensions\\")\\nelse:\\n  falcon_webapp_dir = \'/var/lib/falcon/webapp\'\\n  falcon_home = \'/usr/lib/falcon\'\\n\\nfalcon_webinf_lib = falcon_home + \\"/server/webapp/falcon/WEB-INF/lib\\"\\n\\nhadoop_conf_dir = status_params.hadoop_conf_dir\\nfalcon_conf_dir = status_params.falcon_conf_dir\\noozie_user = config[\'configurations\'][\'oozie-env\'][\'oozie_user\']\\nfalcon_user = config[\'configurations\'][\'falcon-env\'][\'falcon_user\']\\nsmoke_user = config[\'configurations\'][\'cluster-env\'][\'smokeuser\']\\n\\nserver_pid_file = status_params.server_pid_file\\n\\nuser_group = config[\'configurations\'][\'cluster-env\'][\'user_group\']\\nproxyuser_group =  config[\'configurations\'][\'hadoop-env\'][\'proxyuser_group\']\\n\\njava_home = config[\'hostLevelParams\'][\'java_home\']\\nfalcon_local_dir = config[\'configurations\'][\'falcon-env\'][\'falcon_local_dir\']\\nfalcon_log_dir = config[\'configurations\'][\'falcon-env\'][\'falcon_log_dir\']\\n\\n# falcon-startup.properties\\nstore_uri = config[\'configurations\'][\'falcon-startup.properties\'][\'*.config.store.uri\']\\n# If these properties are present, the directories need to be created.\\nfalcon_graph_storage_directory = default(\\"/configurations/falcon-startup.properties/*.falcon.graph.storage.directory\\", None)  # explicitly set in HDP 2.2 and higher\\nfalcon_graph_serialize_path = default(\\"/configurations/falcon-startup.properties/*.falcon.graph.serialize.path\\", None)        # explicitly set in HDP 2.2 and higher\\n\\nfalcon_embeddedmq_data = config[\'configurations\'][\'falcon-env\'][\'falcon.embeddedmq.data\']\\nfalcon_embeddedmq_enabled = config[\'configurations\'][\'falcon-env\'][\'falcon.embeddedmq\']\\nfalcon_emeddedmq_port = config[\'configurations\'][\'falcon-env\'][\'falcon.emeddedmq.port\']\\n\\nfalcon_host = config[\'clusterHostInfo\'][\'falcon_server_hosts\'][0]\\nfalcon_port = config[\'configurations\'][\'falcon-env\'][\'falcon_port\']\\nfalcon_runtime_properties = config[\'configurations\'][\'falcon-runtime.properties\']\\nfalcon_startup_properties = config[\'configurations\'][\'falcon-startup.properties\']\\nfalcon_client_properties = config[\'configurations\'][\'falcon-client.properties\']\\nsmokeuser_keytab = config[\'configurations\'][\'cluster-env\'][\'smokeuser_keytab\']\\nfalcon_env_sh_template = config[\'configurations\'][\'falcon-env\'][\'content\']\\n\\n#Log4j properties\\nfalcon_log_maxfilesize = default(\'/configurations/falcon-log4j/falcon_log_maxfilesize\',256)\\nfalcon_log_maxbackupindex =  default(\'/configurations/falcon-log4j/falcon_log_maxbackupindex\',20)\\nfalcon_security_log_maxfilesize = default(\'/configurations/falcon-log4j/falcon_security_log_maxfilesize\',256)\\nfalcon_security_log_maxbackupindex = default(\'/configurations/falcon-log4j/falcon_security_log_maxbackupindex\',20)\\n\\nfalcon_log4j=config[\'configurations\'][\'falcon-log4j\'][\'content\']\\n\\nfalcon_apps_dir = config[\'configurations\'][\'falcon-env\'][\'falcon_apps_hdfs_dir\']\\n#for create_hdfs_directory\\nsecurity_enabled = config[\'configurations\'][\'cluster-env\'][\'security_enabled\']\\nhostname = config[\\"hostname\\"]\\nhdfs_user_keytab = config[\'configurations\'][\'hadoop-env\'][\'hdfs_user_keytab\']\\nhdfs_user = config[\'configurations\'][\'hadoop-env\'][\'hdfs_user\']\\nhdfs_principal_name = config[\'configurations\'][\'hadoop-env\'][\'hdfs_principal_name\']\\nsmokeuser_principal =  config[\'configurations\'][\'cluster-env\'][\'smokeuser_principal_name\']\\nkinit_path_local = get_kinit_path(default(\'/configurations/kerberos-env/executable_search_paths\', None))\\n\\nsupports_hive_dr = config[\'configurations\'][\'falcon-env\'][\'supports_hive_dr\']\\n# HDP 2.4 still supported the /usr/$STACK/$VERSION/falcon/data-mirroring folder, which had to be copied to HDFS\\n# In HDP 2.5, an empty data-mirroring folder has to be created, and the extensions folder has to be uploaded to HDFS.\\nsupports_data_mirroring = supports_hive_dr and (stack_version_formatted and not check_stack_feature(StackFeature.FALCON_EXTENSIONS, stack_version_formatted))\\n\\nlocal_data_mirroring_dir = format(\'{stack_root}/current/falcon-server/data-mirroring\')\\ndfs_data_mirroring_dir = \\"/apps/data-mirroring\\"\\n\\n\\n########################################################\\n############# Atlas related params #####################\\n########################################################\\n#region Atlas Hooks\\nfalcon_atlas_application_properties = default(\'/configurations/falcon-atlas-application.properties\', {})\\natlas_hook_filename = default(\'/configurations/atlas-env/metadata_conf_file\', \'atlas-application.properties\')\\nenable_atlas_hook = default(\'/configurations/falcon-env/falcon.atlas.hook\', False)\\n\\n# Calculate atlas_hook_cp to add to FALCON_EXTRA_CLASS_PATH\\nfalcon_atlas_support = False\\n\\n# Path to add to environment variable\\natlas_hook_cp = \\"\\"\\nif enable_atlas_hook:\\n\\n  # stack_version doesn\'t contain a minor number of the stack (only first two numbers: 2.3). Get it from current_version_formatted\\n  falcon_atlas_support = current_version_formatted and check_stack_feature(StackFeature.FALCON_ATLAS_SUPPORT_2_3, current_version_formatted) \\\\\\n      or check_stack_feature(StackFeature.FALCON_ATLAS_SUPPORT, stack_version_formatted)\\n\\n  if check_stack_feature(StackFeature.ATLAS_CONF_DIR_IN_PATH, stack_version_formatted):\\n    atlas_conf_dir = format(\'{stack_root}/current/atlas-server/conf\')\\n    atlas_home_dir = format(\'{stack_root}/current/atlas-server\')\\n    atlas_hook_cp = atlas_conf_dir + os.pathsep + os.path.join(atlas_home_dir, \\"hook\\", \\"falcon\\", \\"*\\") + os.pathsep\\n  elif check_stack_feature(StackFeature.ATLAS_UPGRADE_SUPPORT, stack_version_formatted):\\n    atlas_hook_cp = format(\'{stack_root}/current/atlas-client/hook/falcon/*\') + os.pathsep\\n\\natlas_application_class_addition = \\"\\"\\nif falcon_atlas_support:\\n  # Some stack versions do not support Atlas Falcon hook. See stack_features.json\\n  # Packaging was different in older versions.\\n  if current_version_formatted and check_stack_feature(StackFeature.FALCON_ATLAS_SUPPORT_2_3, current_version_formatted):\\n    atlas_application_class_addition = \\",\\\\\\\\\\\\norg.apache.falcon.atlas.service.AtlasService\\"\\n    atlas_plugin_package = \\"atlas-metadata*-falcon-plugin\\"\\n    atlas_ubuntu_plugin_package = \\"atlas-metadata.*-falcon-plugin\\"\\n  else:\\n    atlas_application_class_addition = \\",\\\\\\\\\\\\norg.apache.atlas.falcon.service.AtlasService\\"\\n    atlas_plugin_package = \\"atlas-metadata*-hive-plugin\\"\\n    atlas_ubuntu_plugin_package = \\"atlas-metadata.*-hive-plugin\\"\\n\\n#endregion\\n\\nhdfs_site = config[\'configurations\'][\'hdfs-site\']\\ndefault_fs = config[\'configurations\'][\'core-site\'][\'fs.defaultFS\']\\n\\ndfs_type = default(\\"/commandParams/dfs_type\\", \\"\\")\\n\\nbdb_jar_name = \\"je-5.0.73.jar\\"\\nbdb_resource_name = format(\\"{jdk_location}/{bdb_jar_name}\\")\\ntarget_jar_file = os.path.join(falcon_webinf_lib, bdb_jar_name)\\n\\n\\nimport functools\\n#create partial functions with common arguments for every HdfsResource call\\n#to create/delete hdfs directory/file/copyfromlocal we need to call params.HdfsResource in code\\nHdfsResource = functools.partial(\\n  HdfsResource,\\n  user=hdfs_user,\\n  hdfs_resource_ignore_file = \\"/var/lib/ambari-agent/data/.hdfs_resource_ignore\\",\\n  security_enabled = security_enabled,\\n  keytab = hdfs_user_keytab,\\n  kinit_path_local = kinit_path_local,\\n  hadoop_bin_dir = hadoop_bin_dir,\\n  hadoop_conf_dir = hadoop_conf_dir,\\n  principal_name = hdfs_principal_name,\\n  hdfs_site = hdfs_site,\\n  default_fs = default_fs,\\n  immutable_paths = get_not_managed_resources(),\\n  dfs_type = dfs_type\\n )\\n\\n" }\n'
line: b'{ "repo_name": "allenp/odoo", "ref": "refs/heads/9.0", "path": "addons/account_asset/account_asset_invoice.py", "content": "# -*- coding: utf-8 -*-\\n\\nfrom datetime import datetime\\nfrom dateutil.relativedelta import relativedelta\\nfrom openerp import api, fields, models\\nimport openerp.addons.decimal_precision as dp\\nfrom openerp.tools import DEFAULT_SERVER_DATE_FORMAT as DF\\n\\n\\nclass AccountInvoice(models.Model):\\n    _inherit = \'account.invoice\'\\n\\n    @api.multi\\n    def action_move_create(self):\\n        result = super(AccountInvoice, self).action_move_create()\\n        for inv in self:\\n            inv.invoice_line_ids.asset_create()\\n        return result\\n\\n\\nclass AccountInvoiceLine(models.Model):\\n    _inherit = \'account.invoice.line\'\\n\\n    asset_category_id = fields.Many2one(\'account.asset.category\', string=\'Asset Category\')\\n    asset_start_date = fields.Date(string=\'Asset End Date\', compute=\'_get_asset_date\', readonly=True, store=True)\\n    asset_end_date = fields.Date(string=\'Asset Start Date\', compute=\'_get_asset_date\', readonly=True, store=True)\\n    asset_mrr = fields.Float(string=\'Monthly Recurring Revenue\', compute=\'_get_asset_date\', readonly=True, digits=dp.get_precision(\'Account\'), store=True)\\n\\n    @api.one\\n    @api.depends(\'asset_category_id\', \'invoice_id.date_invoice\')\\n    def _get_asset_date(self):\\n        self.asset_mrr = 0\\n        self.asset_start_date = False\\n        self.asset_end_date = False\\n        cat = self.asset_category_id\\n        if cat:\\n            months = cat.method_number * cat.method_period\\n            if self.invoice_id.type in [\'out_invoice\', \'out_refund\']:\\n                self.asset_mrr = self.price_subtotal_signed / months\\n            if self.invoice_id.date_invoice:\\n                start_date = datetime.strptime(self.invoice_id.date_invoice, DF).replace(day=1)\\n                end_date = (start_date + relativedelta(months=months, days=-1))\\n                self.asset_start_date = start_date.strftime(DF)\\n                self.asset_end_date = end_date.strftime(DF)\\n\\n    @api.one\\n    def asset_create(self):\\n        if self.asset_category_id and self.asset_category_id.method_number > 1:\\n            vals = {\\n                \'name\': self.name,\\n                \'code\': self.invoice_id.number or False,\\n                \'category_id\': self.asset_category_id.id,\\n                \'value\': self.price_subtotal,\\n                \'partner_id\': self.invoice_id.partner_id.id,\\n                \'company_id\': self.invoice_id.company_id.id,\\n                \'currency_id\': self.invoice_id.currency_id.id,\\n                \'date\': self.asset_start_date or self.invoice_id.date_invoice,\\n                \'invoice_id\': self.invoice_id.id,\\n          }\\n            changed_vals = self.env[\'account.asset.asset\'].onchange_category_id_values(vals[\'category_id\'])\\n            vals.update(changed_vals[\'value\'])\\n            asset = self.env[\'account.asset.asset\'].create(vals)\\n            if self.asset_category_id.open_asset:\\n                asset.validate()\\n        return True\\n\\n    @api.onchange(\'product_id\')\\n    def onchange_product_id(self):\\n        if self.product_id:\\n            if self.invoice_id.type == \'out_invoice\':\\n                self.asset_category_id = self.product_id.product_tmpl_id.deferred_revenue_category_id\\n            elif self.invoice_id.type == \'in_invoice\':\\n                self.asset_category_id = self.product_id.product_tmpl_id.asset_category_id\\n\\n\\nclass ProductTemplate(models.Model):\\n    _inherit = \'product.template\'\\n    asset_category_id = fields.Many2one(\'account.asset.category\', string=\'Asset Type\', ondelete=\\"restrict\\")\\n    deferred_revenue_category_id = fields.Many2one(\'account.asset.category\', string=\'Deferred Revenue Type\', ondelete=\\"restrict\\")\\n" }\n'
line: b'{ "repo_name": "guorendong/iridium-browser-ubuntu", "ref": "refs/heads/ubuntu/precise", "path": "third_party/icu/source/test/depstest/depstest.py", "content": "#! /usr/bin/python\\n# -*- coding: utf-8 -*-\\n#\\n# Copyright (C) 2011-2014, International Business Machines\\n# Corporation and others. All Rights Reserved.\\n#\\n# file name: depstest.py\\n#\\n# created on: 2011may24\\n\\n\\"\\"\\"ICU dependency tester.\\n\\nThis probably works only on Linux.\\n\\nThe exit code is 0 if everything is fine, 1 for errors, 2 for only warnings.\\n\\nSample invocation:\\n  ~/svn.icu/trunk/src/source/test/depstest$ ./depstest.py ~/svn.icu/trunk/dbg\\n\\"\\"\\"\\n\\n__author__ = \\"Markus W. Scherer\\"\\n\\nimport glob\\nimport os.path\\nimport subprocess\\nimport sys\\n\\nimport dependencies\\n\\n_ignored_symbols = set()\\n_obj_files = {}\\n_symbols_to_files = {}\\n_return_value = 0\\n\\n# Classes with vtables (and thus virtual methods).\\n_virtual_classes = set()\\n# Classes with weakly defined destructors.\\n# nm shows a symbol class of \\"W\\" rather than \\"T\\".\\n_weak_destructors = set()\\n\\ndef _ReadObjFile(root_path, library_name, obj_name):\\n  global _ignored_symbols, _obj_files, _symbols_to_files\\n  global _virtual_classes, _weak_destructors\\n  lib_obj_name = library_name + \\"/\\" + obj_name\\n  if lib_obj_name in _obj_files:\\n    print \\"Warning: duplicate .o file \\" + lib_obj_name\\n    _return_value = 2\\n    return\\n\\n  path = os.path.join(root_path, library_name, obj_name)\\n  nm_result = subprocess.Popen([\\"nm\\", \\"--demangle\\", \\"--format=sysv\\",\\n                                \\"--extern-only\\", \\"--no-sort\\", path],\\n                               stdout=subprocess.PIPE).communicate()[0]\\n  obj_imports = set()\\n  obj_exports = set()\\n  for line in nm_result.splitlines():\\n    fields = line.split(\\"|\\")\\n    if len(fields) == 1: continue\\n    name = fields[0].strip()\\n    # Ignore symbols like \'__cxa_pure_virtual\',\\n    # \'vtable for __cxxabiv1::__si_class_type_info\' or\\n    # \'DW.ref.__gxx_personality_v0\'.\\n    if name.startswith(\\"__cxa\\") or \\"__cxxabi\\" in name or \\"__gxx\\" in name:\\n      _ignored_symbols.add(name)\\n      continue\\n    type = fields[2].strip()\\n    if type == \\"U\\":\\n      obj_imports.add(name)\\n    else:\\n      obj_exports.add(name)\\n      _symbols_to_files[name] = lib_obj_name\\n      # Is this a vtable? E.g., \\"vtable for icu_49::ByteSink\\".\\n      if name.startswith(\\"vtable for icu\\"):\\n        _virtual_classes.add(name[name.index(\\"::\\") + 2:])\\n      # Is this a destructor? E.g., \\"icu_49::ByteSink::~ByteSink()\\".\\n      index = name.find(\\"::~\\")\\n      if index >= 0 and type == \\"W\\":\\n        _weak_destructors.add(name[index + 3:name.index(\\"(\\", index)])\\n  _obj_files[lib_obj_name] = {\\"imports\\": obj_imports, \\"exports\\": obj_exports}\\n\\ndef _ReadLibrary(root_path, library_name):\\n  obj_paths = glob.glob(os.path.join(root_path, library_name, \\"*.o\\"))\\n  for path in obj_paths:\\n    _ReadObjFile(root_path, library_name, os.path.basename(path))\\n\\ndef _Resolve(name, parents):\\n  global _ignored_symbols, _obj_files, _symbols_to_files, _return_value\\n  item = dependencies.items[name]\\n  item_type = item[\\"type\\"]\\n  if name in parents:\\n    sys.exit(\\"Error: %s %s has a circular dependency on itself: %s\\" %\\n             (item_type, name, parents))\\n  # Check if already cached.\\n  exports = item.get(\\"exports\\")\\n  if exports != None: return item\\n  # Calculcate recursively.\\n  parents.append(name)\\n  imports = set()\\n  exports = set()\\n  system_symbols = item.get(\\"system_symbols\\")\\n  if system_symbols == None: system_symbols = item[\\"system_symbols\\"] = set()\\n  files = item.get(\\"files\\")\\n  if files:\\n    for file_name in files:\\n      obj_file = _obj_files[file_name]\\n      imports |= obj_file[\\"imports\\"]\\n      exports |= obj_file[\\"exports\\"]\\n  imports -= exports | _ignored_symbols\\n  deps = item.get(\\"deps\\")\\n  if deps:\\n    for dep in deps:\\n      dep_item = _Resolve(dep, parents)\\n      # Detect whether this item needs to depend on dep,\\n      # except when this item has no files, that is, when it is just\\n      # a deliberate umbrella group or library.\\n      dep_exports = dep_item[\\"exports\\"]\\n      dep_system_symbols = dep_item[\\"system_symbols\\"]\\n      if files and imports.isdisjoint(dep_exports) and imports.isdisjoint(dep_system_symbols):\\n        print \\"Info:  %s %s  does not need to depend on  %s\\\\n\\" % (item_type, name, dep)\\n      # We always include the dependency\'s exports, even if we do not need them\\n      # to satisfy local imports.\\n      exports |= dep_exports\\n      system_symbols |= dep_system_symbols\\n  item[\\"exports\\"] = exports\\n  item[\\"system_symbols\\"] = system_symbols\\n  imports -= exports | system_symbols\\n  for symbol in imports:\\n    for file_name in files:\\n      if symbol in _obj_files[file_name][\\"imports\\"]:\\n        neededFile = _symbols_to_files.get(symbol)\\n        if neededFile in dependencies.file_to_item:\\n          neededItem = \\"but %s does not depend on %s (for %s)\\" % (name, dependencies.file_to_item[neededFile], neededFile)\\n        else:\\n          neededItem = \\"- is this a new system symbol?\\"\\n        sys.stderr.write(\\"Error: in %s %s: %s imports %s %s\\\\n\\" %\\n                         (item_type, name, file_name, symbol, neededItem))\\n    _return_value = 1\\n  del parents[-1]\\n  return item\\n\\ndef Process(root_path):\\n  \\"\\"\\"Loads dependencies.txt, reads the libraries\' .o files, and processes them.\\n\\n  Modifies dependencies.items: Recursively builds each item\'s system_symbols and exports.\\n  \\"\\"\\"\\n  global _ignored_symbols, _obj_files, _return_value\\n  global _virtual_classes, _weak_destructors\\n  dependencies.Load()\\n  for name_and_item in dependencies.items.iteritems():\\n    name = name_and_item[0]\\n    item = name_and_item[1]\\n    system_symbols = item.get(\\"system_symbols\\")\\n    if system_symbols:\\n      for symbol in system_symbols:\\n        _symbols_to_files[symbol] = name\\n  for library_name in dependencies.libraries:\\n    _ReadLibrary(root_path, library_name)\\n  o_files_set = set(_obj_files.keys())\\n  files_missing_from_deps = o_files_set - dependencies.files\\n  files_missing_from_build = dependencies.files - o_files_set\\n  if files_missing_from_deps:\\n    sys.stderr.write(\\"Error: files missing from dependencies.txt:\\\\n%s\\\\n\\" %\\n                     sorted(files_missing_from_deps))\\n    _return_value = 1\\n  if files_missing_from_build:\\n    sys.stderr.write(\\"Error: files in dependencies.txt but not built:\\\\n%s\\\\n\\" %\\n                     sorted(files_missing_from_build))\\n    _return_value = 1\\n  if not _return_value:\\n    for library_name in dependencies.libraries:\\n      _Resolve(library_name, [])\\n  if not _return_value:\\n    virtual_classes_with_weak_destructors = _virtual_classes & _weak_destructors\\n    if virtual_classes_with_weak_destructors:\\n      sys.stderr.write(\\"Error: Some classes have virtual methods, and \\"\\n                       \\"an implicit or inline destructor \\"\\n                       \\"(see ICU ticket #8454 for details):\\\\n%s\\\\n\\" %\\n                       sorted(virtual_classes_with_weak_destructors))\\n      _return_value = 1\\n\\ndef main():\\n  global _return_value\\n  if len(sys.argv) <= 1:\\n    sys.exit((\\"Command line error: \\" +\\n             \\"need one argument with the root path to the built ICU libraries/*.o files.\\"))\\n  Process(sys.argv[1])\\n  if _ignored_symbols:\\n    print \\"Info: ignored symbols:\\\\n%s\\" % sorted(_ignored_symbols)\\n  if not _return_value:\\n    print \\"OK: Specified and actual dependencies match.\\"\\n  else:\\n    print \\"Error: There were errors, please fix them and re-run. Processing may have terminated abnormally.\\"\\n  return _return_value\\n\\nif __name__ == \\"__main__\\":\\n  sys.exit(main())\\n" }\n'
line: b'{ "repo_name": "cselis86/edx-platform", "ref": "refs/heads/installer", "path": "common/djangoapps/util/tests/test_disable_rate_limit.py", "content": "\\"\\"\\"Tests for disabling rate limiting. \\"\\"\\"\\nimport unittest\\nfrom django.test import TestCase\\nfrom django.core.cache import cache\\nfrom django.conf import settings\\nimport mock\\n\\nfrom rest_framework.views import APIView\\nfrom rest_framework.throttling import BaseThrottle\\nfrom rest_framework.exceptions import Throttled\\n\\nfrom util.disable_rate_limit import can_disable_rate_limit\\nfrom util.models import RateLimitConfiguration\\n\\n\\nclass FakeThrottle(BaseThrottle):\\n    def allow_request(self, request, view):\\n        return False\\n\\n\\n@can_disable_rate_limit\\nclass FakeApiView(APIView):\\n    authentication_classes = []\\n    permission_classes = []\\n    throttle_classes = [FakeThrottle]\\n\\n\\n@unittest.skipUnless(settings.ROOT_URLCONF == \'lms.urls\', \'Test only valid in lms\')\\nclass DisableRateLimitTest(TestCase):\\n    \\"\\"\\"Check that we can disable rate limiting for perf testing. \\"\\"\\"\\n\\n    def setUp(self):\\n        cache.clear()\\n        self.view = FakeApiView()\\n\\n    def test_enable_rate_limit(self):\\n        # Enable rate limiting using model-based config\\n        RateLimitConfiguration.objects.create(enabled=True)\\n\\n        # By default, should enforce rate limiting\\n        # Since our fake throttle always rejects requests,\\n        # we should expect the request to be rejected.\\n        request = mock.Mock()\\n        with self.assertRaises(Throttled):\\n            self.view.check_throttles(request)\\n\\n    def test_disable_rate_limit(self):\\n        # Disable rate limiting using model-based config\\n        RateLimitConfiguration.objects.create(enabled=False)\\n\\n        # With rate-limiting disabled, the request\\n        # should get through.  The `check_throttles()` call\\n        # should return without raising an exception.\\n        request = mock.Mock()\\n        self.view.check_throttles(request)\\n" }\n'
line: b'{ "repo_name": "ThinkOpen-Solutions/odoo", "ref": "refs/heads/stable", "path": "addons/product/__openerp__.py", "content": "# -*- coding: utf-8 -*-\\n##############################################################################\\n#\\n#    OpenERP, Open Source Management Solution\\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\\n#\\n#    This program is free software: you can redistribute it and/or modify\\n#    it under the terms of the GNU Affero General Public License as\\n#    published by the Free Software Foundation, either version 3 of the\\n#    License, or (at your option) any later version.\\n#\\n#    This program is distributed in the hope that it will be useful,\\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#    GNU Affero General Public License for more details.\\n#\\n#    You should have received a copy of the GNU Affero General Public License\\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\\n#\\n##############################################################################\\n\\n\\n{\\n    \'name\': \'Products & Pricelists\',\\n    \'version\': \'1.1\',\\n    \'author\': \'OpenERP SA\',\\n    \'category\': \'Sales Management\',\\n    \'depends\': [\'base\', \'decimal_precision\', \'mail\', \'report\'],\\n    \'demo\': [\\n        \'product_demo.xml\',\\n        \'product_image_demo.xml\',\\n    ],\\n    \'website\': \'https://www.odoo.com\',\\n    \'description\': \\"\\"\\"\\nThis is the base module for managing products and pricelists in OpenERP.\\n========================================================================\\n\\nProducts support variants, different pricing methods, suppliers information,\\nmake to stock/order, different unit of measures, packaging and properties.\\n\\nPricelists support:\\n-------------------\\n    * Multiple-level of discount (by product, category, quantities)\\n    * Compute price based on different criteria:\\n        * Other pricelist\\n        * Cost price\\n        * List price\\n        * Supplier price\\n\\nPricelists preferences by product and/or partners.\\n\\nPrint product labels with barcode.\\n    \\"\\"\\",\\n    \'data\': [\\n        \'security/product_security.xml\',\\n        \'security/ir.model.access.csv\',\\n        \'wizard/product_price_view.xml\',\\n        \'product_data.xml\',\\n        \'product_report.xml\',\\n        \'product_view.xml\',\\n        \'pricelist_view.xml\',\\n        \'partner_view.xml\',\\n        \'views/report_pricelist.xml\',\\n    ],\\n    \'test\': [\\n        \'product_pricelist_demo.yml\',\\n        \'test/product_pricelist.yml\',\\n    ],\\n    \'installable\': True,\\n    \'auto_install\': False,\\n    \'images\': [\'images/product_uom.jpeg\',\'images/product_pricelists.jpeg\',\'images/products_categories.jpeg\', \'images/products_form.jpeg\'],\\n}\\n\\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\\n" }\n'
line: b'{ "repo_name": "motion2015/a3", "ref": "refs/heads/a3", "path": "common/djangoapps/util/tests/test_disable_rate_limit.py", "content": "\\"\\"\\"Tests for disabling rate limiting. \\"\\"\\"\\nimport unittest\\nfrom django.test import TestCase\\nfrom django.core.cache import cache\\nfrom django.conf import settings\\nimport mock\\n\\nfrom rest_framework.views import APIView\\nfrom rest_framework.throttling import BaseThrottle\\nfrom rest_framework.exceptions import Throttled\\n\\nfrom util.disable_rate_limit import can_disable_rate_limit\\nfrom util.models import RateLimitConfiguration\\n\\n\\nclass FakeThrottle(BaseThrottle):\\n    def allow_request(self, request, view):\\n        return False\\n\\n\\n@can_disable_rate_limit\\nclass FakeApiView(APIView):\\n    authentication_classes = []\\n    permission_classes = []\\n    throttle_classes = [FakeThrottle]\\n\\n\\n@unittest.skipUnless(settings.ROOT_URLCONF == \'lms.urls\', \'Test only valid in lms\')\\nclass DisableRateLimitTest(TestCase):\\n    \\"\\"\\"Check that we can disable rate limiting for perf testing. \\"\\"\\"\\n\\n    def setUp(self):\\n        cache.clear()\\n        self.view = FakeApiView()\\n\\n    def test_enable_rate_limit(self):\\n        # Enable rate limiting using model-based config\\n        RateLimitConfiguration.objects.create(enabled=True)\\n\\n        # By default, should enforce rate limiting\\n        # Since our fake throttle always rejects requests,\\n        # we should expect the request to be rejected.\\n        request = mock.Mock()\\n        with self.assertRaises(Throttled):\\n            self.view.check_throttles(request)\\n\\n    def test_disable_rate_limit(self):\\n        # Disable rate limiting using model-based config\\n        RateLimitConfiguration.objects.create(enabled=False)\\n\\n        # With rate-limiting disabled, the request\\n        # should get through.  The `check_throttles()` call\\n        # should return without raising an exception.\\n        request = mock.Mock()\\n        self.view.check_throttles(request)\\n" }\n'
line: b'{ "repo_name": "Simran-B/arangodb", "ref": "refs/heads/docs_3.0", "path": "3rdParty/V8-4.3.61/third_party/icu/source/test/depstest/depstest.py", "content": "#! /usr/bin/python\\n# -*- coding: utf-8 -*-\\n#\\n# Copyright (C) 2011-2014, International Business Machines\\n# Corporation and others. All Rights Reserved.\\n#\\n# file name: depstest.py\\n#\\n# created on: 2011may24\\n\\n\\"\\"\\"ICU dependency tester.\\n\\nThis probably works only on Linux.\\n\\nThe exit code is 0 if everything is fine, 1 for errors, 2 for only warnings.\\n\\nSample invocation:\\n  ~/svn.icu/trunk/src/source/test/depstest$ ./depstest.py ~/svn.icu/trunk/dbg\\n\\"\\"\\"\\n\\n__author__ = \\"Markus W. Scherer\\"\\n\\nimport glob\\nimport os.path\\nimport subprocess\\nimport sys\\n\\nimport dependencies\\n\\n_ignored_symbols = set()\\n_obj_files = {}\\n_symbols_to_files = {}\\n_return_value = 0\\n\\n# Classes with vtables (and thus virtual methods).\\n_virtual_classes = set()\\n# Classes with weakly defined destructors.\\n# nm shows a symbol class of \\"W\\" rather than \\"T\\".\\n_weak_destructors = set()\\n\\ndef _ReadObjFile(root_path, library_name, obj_name):\\n  global _ignored_symbols, _obj_files, _symbols_to_files\\n  global _virtual_classes, _weak_destructors\\n  lib_obj_name = library_name + \\"/\\" + obj_name\\n  if lib_obj_name in _obj_files:\\n    print \\"Warning: duplicate .o file \\" + lib_obj_name\\n    _return_value = 2\\n    return\\n\\n  path = os.path.join(root_path, library_name, obj_name)\\n  nm_result = subprocess.Popen([\\"nm\\", \\"--demangle\\", \\"--format=sysv\\",\\n                                \\"--extern-only\\", \\"--no-sort\\", path],\\n                               stdout=subprocess.PIPE).communicate()[0]\\n  obj_imports = set()\\n  obj_exports = set()\\n  for line in nm_result.splitlines():\\n    fields = line.split(\\"|\\")\\n    if len(fields) == 1: continue\\n    name = fields[0].strip()\\n    # Ignore symbols like \'__cxa_pure_virtual\',\\n    # \'vtable for __cxxabiv1::__si_class_type_info\' or\\n    # \'DW.ref.__gxx_personality_v0\'.\\n    if name.startswith(\\"__cxa\\") or \\"__cxxabi\\" in name or \\"__gxx\\" in name:\\n      _ignored_symbols.add(name)\\n      continue\\n    type = fields[2].strip()\\n    if type == \\"U\\":\\n      obj_imports.add(name)\\n    else:\\n      obj_exports.add(name)\\n      _symbols_to_files[name] = lib_obj_name\\n      # Is this a vtable? E.g., \\"vtable for icu_49::ByteSink\\".\\n      if name.startswith(\\"vtable for icu\\"):\\n        _virtual_classes.add(name[name.index(\\"::\\") + 2:])\\n      # Is this a destructor? E.g., \\"icu_49::ByteSink::~ByteSink()\\".\\n      index = name.find(\\"::~\\")\\n      if index >= 0 and type == \\"W\\":\\n        _weak_destructors.add(name[index + 3:name.index(\\"(\\", index)])\\n  _obj_files[lib_obj_name] = {\\"imports\\": obj_imports, \\"exports\\": obj_exports}\\n\\ndef _ReadLibrary(root_path, library_name):\\n  obj_paths = glob.glob(os.path.join(root_path, library_name, \\"*.o\\"))\\n  for path in obj_paths:\\n    _ReadObjFile(root_path, library_name, os.path.basename(path))\\n\\ndef _Resolve(name, parents):\\n  global _ignored_symbols, _obj_files, _symbols_to_files, _return_value\\n  item = dependencies.items[name]\\n  item_type = item[\\"type\\"]\\n  if name in parents:\\n    sys.exit(\\"Error: %s %s has a circular dependency on itself: %s\\" %\\n             (item_type, name, parents))\\n  # Check if already cached.\\n  exports = item.get(\\"exports\\")\\n  if exports != None: return item\\n  # Calculcate recursively.\\n  parents.append(name)\\n  imports = set()\\n  exports = set()\\n  system_symbols = item.get(\\"system_symbols\\")\\n  if system_symbols == None: system_symbols = item[\\"system_symbols\\"] = set()\\n  files = item.get(\\"files\\")\\n  if files:\\n    for file_name in files:\\n      obj_file = _obj_files[file_name]\\n      imports |= obj_file[\\"imports\\"]\\n      exports |= obj_file[\\"exports\\"]\\n  imports -= exports | _ignored_symbols\\n  deps = item.get(\\"deps\\")\\n  if deps:\\n    for dep in deps:\\n      dep_item = _Resolve(dep, parents)\\n      # Detect whether this item needs to depend on dep,\\n      # except when this item has no files, that is, when it is just\\n      # a deliberate umbrella group or library.\\n      dep_exports = dep_item[\\"exports\\"]\\n      dep_system_symbols = dep_item[\\"system_symbols\\"]\\n      if files and imports.isdisjoint(dep_exports) and imports.isdisjoint(dep_system_symbols):\\n        print \\"Info:  %s %s  does not need to depend on  %s\\\\n\\" % (item_type, name, dep)\\n      # We always include the dependency\'s exports, even if we do not need them\\n      # to satisfy local imports.\\n      exports |= dep_exports\\n      system_symbols |= dep_system_symbols\\n  item[\\"exports\\"] = exports\\n  item[\\"system_symbols\\"] = system_symbols\\n  imports -= exports | system_symbols\\n  for symbol in imports:\\n    for file_name in files:\\n      if symbol in _obj_files[file_name][\\"imports\\"]:\\n        neededFile = _symbols_to_files.get(symbol)\\n        if neededFile in dependencies.file_to_item:\\n          neededItem = \\"but %s does not depend on %s (for %s)\\" % (name, dependencies.file_to_item[neededFile], neededFile)\\n        else:\\n          neededItem = \\"- is this a new system symbol?\\"\\n        sys.stderr.write(\\"Error: in %s %s: %s imports %s %s\\\\n\\" %\\n                         (item_type, name, file_name, symbol, neededItem))\\n    _return_value = 1\\n  del parents[-1]\\n  return item\\n\\ndef Process(root_path):\\n  \\"\\"\\"Loads dependencies.txt, reads the libraries\' .o files, and processes them.\\n\\n  Modifies dependencies.items: Recursively builds each item\'s system_symbols and exports.\\n  \\"\\"\\"\\n  global _ignored_symbols, _obj_files, _return_value\\n  global _virtual_classes, _weak_destructors\\n  dependencies.Load()\\n  for name_and_item in dependencies.items.iteritems():\\n    name = name_and_item[0]\\n    item = name_and_item[1]\\n    system_symbols = item.get(\\"system_symbols\\")\\n    if system_symbols:\\n      for symbol in system_symbols:\\n        _symbols_to_files[symbol] = name\\n  for library_name in dependencies.libraries:\\n    _ReadLibrary(root_path, library_name)\\n  o_files_set = set(_obj_files.keys())\\n  files_missing_from_deps = o_files_set - dependencies.files\\n  files_missing_from_build = dependencies.files - o_files_set\\n  if files_missing_from_deps:\\n    sys.stderr.write(\\"Error: files missing from dependencies.txt:\\\\n%s\\\\n\\" %\\n                     sorted(files_missing_from_deps))\\n    _return_value = 1\\n  if files_missing_from_build:\\n    sys.stderr.write(\\"Error: files in dependencies.txt but not built:\\\\n%s\\\\n\\" %\\n                     sorted(files_missing_from_build))\\n    _return_value = 1\\n  if not _return_value:\\n    for library_name in dependencies.libraries:\\n      _Resolve(library_name, [])\\n  if not _return_value:\\n    virtual_classes_with_weak_destructors = _virtual_classes & _weak_destructors\\n    if virtual_classes_with_weak_destructors:\\n      sys.stderr.write(\\"Error: Some classes have virtual methods, and \\"\\n                       \\"an implicit or inline destructor \\"\\n                       \\"(see ICU ticket #8454 for details):\\\\n%s\\\\n\\" %\\n                       sorted(virtual_classes_with_weak_destructors))\\n      _return_value = 1\\n\\ndef main():\\n  global _return_value\\n  if len(sys.argv) <= 1:\\n    sys.exit((\\"Command line error: \\" +\\n             \\"need one argument with the root path to the built ICU libraries/*.o files.\\"))\\n  Process(sys.argv[1])\\n  if _ignored_symbols:\\n    print \\"Info: ignored symbols:\\\\n%s\\" % sorted(_ignored_symbols)\\n  if not _return_value:\\n    print \\"OK: Specified and actual dependencies match.\\"\\n  else:\\n    print \\"Error: There were errors, please fix them and re-run. Processing may have terminated abnormally.\\"\\n  return _return_value\\n\\nif __name__ == \\"__main__\\":\\n  sys.exit(main())\\n" }\n'
line: b'{ "repo_name": "oracc/nammu", "ref": "refs/heads/development", "path": "python/nammu/controller/MenuController.py", "content": "\'\'\'\\nCopyright 2015 - 2018 University College London.\\n\\nThis file is part of Nammu.\\n\\nNammu is free software: you can redistribute it and/or modify\\nit under the terms of the GNU General Public License as published by\\nthe Free Software Foundation, either version 3 of the License, or\\n(at your option) any later version.\\n\\nNammu is distributed in the hope that it will be useful,\\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\nGNU General Public License for more details.\\n\\nYou should have received a copy of the GNU General Public License\\nalong with Nammu.  If not, see <http://www.gnu.org/licenses/>.\\n\'\'\'\\n\\nfrom ..view.MenuView import MenuView\\n\\n\\nclass MenuController(object):\\n    \'\'\'\\n    Creates the menu view and handles menu actions.\\n    \'\'\'\\n    def __init__(self, mainController):\\n\\n        # Needs delegating to parent presenter\\n        # Note: self.controller needs to be defined before creating the\\n        # ToolbarView, since the ToolbaView will delegate some actions to it.\\n        self.mainController = mainController\\n\\n        # Create view with a reference to its controller to handle events\\n        self.view = MenuView(self)\\n\\n    def enable_split_options(self, horizontal=True,\\n                             vertical=True, arabic=True):\\n        \\"\\"\\"\\n        Show split menu items as enabled or disabled.\\n        \\"\\"\\"\\n        self.view.enable_item(\\"Window\\", \\"Toggle Vertical Split Editor\\",\\n                              vertical)\\n        self.view.enable_item(\\"Window\\", \\"Toggle Horizontal Split Editor\\",\\n                              horizontal)\\n        self.view.enable_item(\\"Window\\", \\"Toggle Arabic Translation Editor\\",\\n                              arabic)\\n\\n    # Some actions need to be delegated to NammuController.\\n    # E.g. actions in menu that\'ll need modification of text area controlled\\n    # elsewhere and not accessible from this controller; as opposed to e.g.\\n    # showHelp that can be dealt with from MenuController.\\n\\n    # Whenever a MenuController\'s method is invoked, __getattr__ will search\\n    # for that given method name in this class. If it\'s not found, it\'ll\\n    # delegate the action with same name to NammuController\\n    def __getattr__(self, name):\\n        return getattr(self.mainController, name)\\n" }\n'
line: b'{ "repo_name": "blooparksystems/odoo", "ref": "refs/heads/9.0", "path": "addons/account_asset/account_asset_invoice.py", "content": "# -*- coding: utf-8 -*-\\n\\nfrom datetime import datetime\\nfrom dateutil.relativedelta import relativedelta\\nfrom openerp import api, fields, models\\nimport openerp.addons.decimal_precision as dp\\nfrom openerp.tools import DEFAULT_SERVER_DATE_FORMAT as DF\\n\\n\\nclass AccountInvoice(models.Model):\\n    _inherit = \'account.invoice\'\\n\\n    @api.multi\\n    def action_move_create(self):\\n        result = super(AccountInvoice, self).action_move_create()\\n        for inv in self:\\n            inv.invoice_line_ids.asset_create()\\n        return result\\n\\n\\nclass AccountInvoiceLine(models.Model):\\n    _inherit = \'account.invoice.line\'\\n\\n    asset_category_id = fields.Many2one(\'account.asset.category\', string=\'Asset Category\')\\n    asset_start_date = fields.Date(string=\'Asset End Date\', compute=\'_get_asset_date\', readonly=True, store=True)\\n    asset_end_date = fields.Date(string=\'Asset Start Date\', compute=\'_get_asset_date\', readonly=True, store=True)\\n    asset_mrr = fields.Float(string=\'Monthly Recurring Revenue\', compute=\'_get_asset_date\', readonly=True, digits=dp.get_precision(\'Account\'), store=True)\\n\\n    @api.one\\n    @api.depends(\'asset_category_id\', \'invoice_id.date_invoice\')\\n    def _get_asset_date(self):\\n        self.asset_mrr = 0\\n        self.asset_start_date = False\\n        self.asset_end_date = False\\n        cat = self.asset_category_id\\n        if cat:\\n            months = cat.method_number * cat.method_period\\n            if self.invoice_id.type in [\'out_invoice\', \'out_refund\']:\\n                self.asset_mrr = self.price_subtotal_signed / months\\n            if self.invoice_id.date_invoice:\\n                start_date = datetime.strptime(self.invoice_id.date_invoice, DF).replace(day=1)\\n                end_date = (start_date + relativedelta(months=months, days=-1))\\n                self.asset_start_date = start_date.strftime(DF)\\n                self.asset_end_date = end_date.strftime(DF)\\n\\n    @api.one\\n    def asset_create(self):\\n        if self.asset_category_id and self.asset_category_id.method_number > 1:\\n            vals = {\\n                \'name\': self.name,\\n                \'code\': self.invoice_id.number or False,\\n                \'category_id\': self.asset_category_id.id,\\n                \'value\': self.price_subtotal,\\n                \'partner_id\': self.invoice_id.partner_id.id,\\n                \'company_id\': self.invoice_id.company_id.id,\\n                \'currency_id\': self.invoice_id.currency_id.id,\\n                \'date\': self.asset_start_date or self.invoice_id.date_invoice,\\n                \'invoice_id\': self.invoice_id.id,\\n          }\\n            changed_vals = self.env[\'account.asset.asset\'].onchange_category_id_values(vals[\'category_id\'])\\n            vals.update(changed_vals[\'value\'])\\n            asset = self.env[\'account.asset.asset\'].create(vals)\\n            if self.asset_category_id.open_asset:\\n                asset.validate()\\n        return True\\n\\n    @api.onchange(\'product_id\')\\n    def onchange_product_id(self):\\n        if self.product_id:\\n            if self.invoice_id.type == \'out_invoice\':\\n                self.asset_category_id = self.product_id.product_tmpl_id.deferred_revenue_category_id\\n            elif self.invoice_id.type == \'in_invoice\':\\n                self.asset_category_id = self.product_id.product_tmpl_id.asset_category_id\\n\\n\\nclass ProductTemplate(models.Model):\\n    _inherit = \'product.template\'\\n    asset_category_id = fields.Many2one(\'account.asset.category\', string=\'Asset Type\', ondelete=\\"restrict\\")\\n    deferred_revenue_category_id = fields.Many2one(\'account.asset.category\', string=\'Deferred Revenue Type\', ondelete=\\"restrict\\")\\n" }\n'
line: b'{ "repo_name": "optima-ict/odoo", "ref": "refs/heads/9.0", "path": "addons/account_asset/account_asset_invoice.py", "content": "# -*- coding: utf-8 -*-\\n\\nfrom datetime import datetime\\nfrom dateutil.relativedelta import relativedelta\\nfrom openerp import api, fields, models\\nimport openerp.addons.decimal_precision as dp\\nfrom openerp.tools import DEFAULT_SERVER_DATE_FORMAT as DF\\n\\n\\nclass AccountInvoice(models.Model):\\n    _inherit = \'account.invoice\'\\n\\n    @api.multi\\n    def action_move_create(self):\\n        result = super(AccountInvoice, self).action_move_create()\\n        for inv in self:\\n            inv.invoice_line_ids.asset_create()\\n        return result\\n\\n\\nclass AccountInvoiceLine(models.Model):\\n    _inherit = \'account.invoice.line\'\\n\\n    asset_category_id = fields.Many2one(\'account.asset.category\', string=\'Asset Category\')\\n    asset_start_date = fields.Date(string=\'Asset End Date\', compute=\'_get_asset_date\', readonly=True, store=True)\\n    asset_end_date = fields.Date(string=\'Asset Start Date\', compute=\'_get_asset_date\', readonly=True, store=True)\\n    asset_mrr = fields.Float(string=\'Monthly Recurring Revenue\', compute=\'_get_asset_date\', readonly=True, digits=dp.get_precision(\'Account\'), store=True)\\n\\n    @api.one\\n    @api.depends(\'asset_category_id\', \'invoice_id.date_invoice\')\\n    def _get_asset_date(self):\\n        self.asset_mrr = 0\\n        self.asset_start_date = False\\n        self.asset_end_date = False\\n        cat = self.asset_category_id\\n        if cat:\\n            months = cat.method_number * cat.method_period\\n            if self.invoice_id.type in [\'out_invoice\', \'out_refund\']:\\n                self.asset_mrr = self.price_subtotal_signed / months\\n            if self.invoice_id.date_invoice:\\n                start_date = datetime.strptime(self.invoice_id.date_invoice, DF).replace(day=1)\\n                end_date = (start_date + relativedelta(months=months, days=-1))\\n                self.asset_start_date = start_date.strftime(DF)\\n                self.asset_end_date = end_date.strftime(DF)\\n\\n    @api.one\\n    def asset_create(self):\\n        if self.asset_category_id and self.asset_category_id.method_number > 1:\\n            vals = {\\n                \'name\': self.name,\\n                \'code\': self.invoice_id.number or False,\\n                \'category_id\': self.asset_category_id.id,\\n                \'value\': self.price_subtotal,\\n                \'partner_id\': self.invoice_id.partner_id.id,\\n                \'company_id\': self.invoice_id.company_id.id,\\n                \'currency_id\': self.invoice_id.currency_id.id,\\n                \'date\': self.asset_start_date or self.invoice_id.date_invoice,\\n                \'invoice_id\': self.invoice_id.id,\\n          }\\n            changed_vals = self.env[\'account.asset.asset\'].onchange_category_id_values(vals[\'category_id\'])\\n            vals.update(changed_vals[\'value\'])\\n            asset = self.env[\'account.asset.asset\'].create(vals)\\n            if self.asset_category_id.open_asset:\\n                asset.validate()\\n        return True\\n\\n    @api.onchange(\'product_id\')\\n    def onchange_product_id(self):\\n        if self.product_id:\\n            if self.invoice_id.type == \'out_invoice\':\\n                self.asset_category_id = self.product_id.product_tmpl_id.deferred_revenue_category_id\\n            elif self.invoice_id.type == \'in_invoice\':\\n                self.asset_category_id = self.product_id.product_tmpl_id.asset_category_id\\n\\n\\nclass ProductTemplate(models.Model):\\n    _inherit = \'product.template\'\\n    asset_category_id = fields.Many2one(\'account.asset.category\', string=\'Asset Type\', ondelete=\\"restrict\\")\\n    deferred_revenue_category_id = fields.Many2one(\'account.asset.category\', string=\'Deferred Revenue Type\', ondelete=\\"restrict\\")\\n" }\n'
line: b'{ "repo_name": "grupozeety/CDerpnext", "ref": "refs/heads/bk_master", "path": "erpnext/hr/doctype/job_opening/test_job_opening.py", "content": "# -*- coding: utf-8 -*-\\n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\\n# See license.txt\\nfrom __future__ import unicode_literals\\n\\nimport frappe\\nimport unittest\\n\\n# test_records = frappe.get_test_records(\'Job Opening\')\\n\\nclass TestJobOpening(unittest.TestCase):\\n\\tpass\\n" }\n'
line: b'{ "repo_name": "project-lovelace/lovelace-website", "ref": "refs/heads/main", "path": "src/static/code_stubs/python/scientific_temperatures.py", "content": "def fahrenheit_to_celsius(F):\\n    C = 0\\n\\n    # Your code goes here: calculate the temperature in Celsius,\\n    # store in a variable (we called it C), and return it.\\n\\n    return C\\n" }\n'
line: b'{ "repo_name": "bealdav/OCB", "ref": "refs/heads/patch-1", "path": "addons/product/__openerp__.py", "content": "# -*- coding: utf-8 -*-\\n##############################################################################\\n#\\n#    OpenERP, Open Source Management Solution\\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\\n#\\n#    This program is free software: you can redistribute it and/or modify\\n#    it under the terms of the GNU Affero General Public License as\\n#    published by the Free Software Foundation, either version 3 of the\\n#    License, or (at your option) any later version.\\n#\\n#    This program is distributed in the hope that it will be useful,\\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#    GNU Affero General Public License for more details.\\n#\\n#    You should have received a copy of the GNU Affero General Public License\\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\\n#\\n##############################################################################\\n\\n\\n{\\n    \'name\': \'Products & Pricelists\',\\n    \'version\': \'1.1\',\\n    \'author\': \'OpenERP SA\',\\n    \'category\': \'Sales Management\',\\n    \'depends\': [\'base\', \'decimal_precision\', \'mail\', \'report\'],\\n    \'demo\': [\\n        \'product_demo.xml\',\\n        \'product_image_demo.xml\',\\n    ],\\n    \'website\': \'https://www.odoo.com\',\\n    \'description\': \\"\\"\\"\\nThis is the base module for managing products and pricelists in OpenERP.\\n========================================================================\\n\\nProducts support variants, different pricing methods, suppliers information,\\nmake to stock/order, different unit of measures, packaging and properties.\\n\\nPricelists support:\\n-------------------\\n    * Multiple-level of discount (by product, category, quantities)\\n    * Compute price based on different criteria:\\n        * Other pricelist\\n        * Cost price\\n        * List price\\n        * Supplier price\\n\\nPricelists preferences by product and/or partners.\\n\\nPrint product labels with barcode.\\n    \\"\\"\\",\\n    \'data\': [\\n        \'security/product_security.xml\',\\n        \'security/ir.model.access.csv\',\\n        \'wizard/product_price_view.xml\',\\n        \'product_data.xml\',\\n        \'product_report.xml\',\\n        \'product_view.xml\',\\n        \'pricelist_view.xml\',\\n        \'partner_view.xml\',\\n        \'views/report_pricelist.xml\',\\n    ],\\n    \'test\': [\\n        \'product_pricelist_demo.yml\',\\n        \'test/product_pricelist.yml\',\\n    ],\\n    \'installable\': True,\\n    \'auto_install\': False,\\n    \'images\': [\'images/product_uom.jpeg\',\'images/product_pricelists.jpeg\',\'images/products_categories.jpeg\', \'images/products_form.jpeg\'],\\n}\\n\\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\\n" }\n'
line: b'{ "repo_name": "baslr/ArangoDB", "ref": "refs/heads/3.1-silent", "path": "3rdParty/V8/V8-5.0.71.39/third_party/icu/source/test/depstest/depstest.py", "content": "#! /usr/bin/python\\n# -*- coding: utf-8 -*-\\n#\\n# Copyright (C) 2011-2014, International Business Machines\\n# Corporation and others. All Rights Reserved.\\n#\\n# file name: depstest.py\\n#\\n# created on: 2011may24\\n\\n\\"\\"\\"ICU dependency tester.\\n\\nThis probably works only on Linux.\\n\\nThe exit code is 0 if everything is fine, 1 for errors, 2 for only warnings.\\n\\nSample invocation:\\n  ~/svn.icu/trunk/src/source/test/depstest$ ./depstest.py ~/svn.icu/trunk/dbg\\n\\"\\"\\"\\n\\n__author__ = \\"Markus W. Scherer\\"\\n\\nimport glob\\nimport os.path\\nimport subprocess\\nimport sys\\n\\nimport dependencies\\n\\n_ignored_symbols = set()\\n_obj_files = {}\\n_symbols_to_files = {}\\n_return_value = 0\\n\\n# Classes with vtables (and thus virtual methods).\\n_virtual_classes = set()\\n# Classes with weakly defined destructors.\\n# nm shows a symbol class of \\"W\\" rather than \\"T\\".\\n_weak_destructors = set()\\n\\ndef _ReadObjFile(root_path, library_name, obj_name):\\n  global _ignored_symbols, _obj_files, _symbols_to_files\\n  global _virtual_classes, _weak_destructors\\n  lib_obj_name = library_name + \\"/\\" + obj_name\\n  if lib_obj_name in _obj_files:\\n    print \\"Warning: duplicate .o file \\" + lib_obj_name\\n    _return_value = 2\\n    return\\n\\n  path = os.path.join(root_path, library_name, obj_name)\\n  nm_result = subprocess.Popen([\\"nm\\", \\"--demangle\\", \\"--format=sysv\\",\\n                                \\"--extern-only\\", \\"--no-sort\\", path],\\n                               stdout=subprocess.PIPE).communicate()[0]\\n  obj_imports = set()\\n  obj_exports = set()\\n  for line in nm_result.splitlines():\\n    fields = line.split(\\"|\\")\\n    if len(fields) == 1: continue\\n    name = fields[0].strip()\\n    # Ignore symbols like \'__cxa_pure_virtual\',\\n    # \'vtable for __cxxabiv1::__si_class_type_info\' or\\n    # \'DW.ref.__gxx_personality_v0\'.\\n    if name.startswith(\\"__cxa\\") or \\"__cxxabi\\" in name or \\"__gxx\\" in name:\\n      _ignored_symbols.add(name)\\n      continue\\n    type = fields[2].strip()\\n    if type == \\"U\\":\\n      obj_imports.add(name)\\n    else:\\n      obj_exports.add(name)\\n      _symbols_to_files[name] = lib_obj_name\\n      # Is this a vtable? E.g., \\"vtable for icu_49::ByteSink\\".\\n      if name.startswith(\\"vtable for icu\\"):\\n        _virtual_classes.add(name[name.index(\\"::\\") + 2:])\\n      # Is this a destructor? E.g., \\"icu_49::ByteSink::~ByteSink()\\".\\n      index = name.find(\\"::~\\")\\n      if index >= 0 and type == \\"W\\":\\n        _weak_destructors.add(name[index + 3:name.index(\\"(\\", index)])\\n  _obj_files[lib_obj_name] = {\\"imports\\": obj_imports, \\"exports\\": obj_exports}\\n\\ndef _ReadLibrary(root_path, library_name):\\n  obj_paths = glob.glob(os.path.join(root_path, library_name, \\"*.o\\"))\\n  for path in obj_paths:\\n    _ReadObjFile(root_path, library_name, os.path.basename(path))\\n\\ndef _Resolve(name, parents):\\n  global _ignored_symbols, _obj_files, _symbols_to_files, _return_value\\n  item = dependencies.items[name]\\n  item_type = item[\\"type\\"]\\n  if name in parents:\\n    sys.exit(\\"Error: %s %s has a circular dependency on itself: %s\\" %\\n             (item_type, name, parents))\\n  # Check if already cached.\\n  exports = item.get(\\"exports\\")\\n  if exports != None: return item\\n  # Calculcate recursively.\\n  parents.append(name)\\n  imports = set()\\n  exports = set()\\n  system_symbols = item.get(\\"system_symbols\\")\\n  if system_symbols == None: system_symbols = item[\\"system_symbols\\"] = set()\\n  files = item.get(\\"files\\")\\n  if files:\\n    for file_name in files:\\n      obj_file = _obj_files[file_name]\\n      imports |= obj_file[\\"imports\\"]\\n      exports |= obj_file[\\"exports\\"]\\n  imports -= exports | _ignored_symbols\\n  deps = item.get(\\"deps\\")\\n  if deps:\\n    for dep in deps:\\n      dep_item = _Resolve(dep, parents)\\n      # Detect whether this item needs to depend on dep,\\n      # except when this item has no files, that is, when it is just\\n      # a deliberate umbrella group or library.\\n      dep_exports = dep_item[\\"exports\\"]\\n      dep_system_symbols = dep_item[\\"system_symbols\\"]\\n      if files and imports.isdisjoint(dep_exports) and imports.isdisjoint(dep_system_symbols):\\n        print \\"Info:  %s %s  does not need to depend on  %s\\\\n\\" % (item_type, name, dep)\\n      # We always include the dependency\'s exports, even if we do not need them\\n      # to satisfy local imports.\\n      exports |= dep_exports\\n      system_symbols |= dep_system_symbols\\n  item[\\"exports\\"] = exports\\n  item[\\"system_symbols\\"] = system_symbols\\n  imports -= exports | system_symbols\\n  for symbol in imports:\\n    for file_name in files:\\n      if symbol in _obj_files[file_name][\\"imports\\"]:\\n        neededFile = _symbols_to_files.get(symbol)\\n        if neededFile in dependencies.file_to_item:\\n          neededItem = \\"but %s does not depend on %s (for %s)\\" % (name, dependencies.file_to_item[neededFile], neededFile)\\n        else:\\n          neededItem = \\"- is this a new system symbol?\\"\\n        sys.stderr.write(\\"Error: in %s %s: %s imports %s %s\\\\n\\" %\\n                         (item_type, name, file_name, symbol, neededItem))\\n    _return_value = 1\\n  del parents[-1]\\n  return item\\n\\ndef Process(root_path):\\n  \\"\\"\\"Loads dependencies.txt, reads the libraries\' .o files, and processes them.\\n\\n  Modifies dependencies.items: Recursively builds each item\'s system_symbols and exports.\\n  \\"\\"\\"\\n  global _ignored_symbols, _obj_files, _return_value\\n  global _virtual_classes, _weak_destructors\\n  dependencies.Load()\\n  for name_and_item in dependencies.items.iteritems():\\n    name = name_and_item[0]\\n    item = name_and_item[1]\\n    system_symbols = item.get(\\"system_symbols\\")\\n    if system_symbols:\\n      for symbol in system_symbols:\\n        _symbols_to_files[symbol] = name\\n  for library_name in dependencies.libraries:\\n    _ReadLibrary(root_path, library_name)\\n  o_files_set = set(_obj_files.keys())\\n  files_missing_from_deps = o_files_set - dependencies.files\\n  files_missing_from_build = dependencies.files - o_files_set\\n  if files_missing_from_deps:\\n    sys.stderr.write(\\"Error: files missing from dependencies.txt:\\\\n%s\\\\n\\" %\\n                     sorted(files_missing_from_deps))\\n    _return_value = 1\\n  if files_missing_from_build:\\n    sys.stderr.write(\\"Error: files in dependencies.txt but not built:\\\\n%s\\\\n\\" %\\n                     sorted(files_missing_from_build))\\n    _return_value = 1\\n  if not _return_value:\\n    for library_name in dependencies.libraries:\\n      _Resolve(library_name, [])\\n  if not _return_value:\\n    virtual_classes_with_weak_destructors = _virtual_classes & _weak_destructors\\n    if virtual_classes_with_weak_destructors:\\n      sys.stderr.write(\\"Error: Some classes have virtual methods, and \\"\\n                       \\"an implicit or inline destructor \\"\\n                       \\"(see ICU ticket #8454 for details):\\\\n%s\\\\n\\" %\\n                       sorted(virtual_classes_with_weak_destructors))\\n      _return_value = 1\\n\\ndef main():\\n  global _return_value\\n  if len(sys.argv) <= 1:\\n    sys.exit((\\"Command line error: \\" +\\n             \\"need one argument with the root path to the built ICU libraries/*.o files.\\"))\\n  Process(sys.argv[1])\\n  if _ignored_symbols:\\n    print \\"Info: ignored symbols:\\\\n%s\\" % sorted(_ignored_symbols)\\n  if not _return_value:\\n    print \\"OK: Specified and actual dependencies match.\\"\\n  else:\\n    print \\"Error: There were errors, please fix them and re-run. Processing may have terminated abnormally.\\"\\n  return _return_value\\n\\nif __name__ == \\"__main__\\":\\n  sys.exit(main())\\n" }\n'
line: b'{ "repo_name": "bobisme/odoo", "ref": "refs/heads/sp-8.0", "path": "addons/product/__openerp__.py", "content": "# -*- coding: utf-8 -*-\\n##############################################################################\\n#\\n#    OpenERP, Open Source Management Solution\\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\\n#\\n#    This program is free software: you can redistribute it and/or modify\\n#    it under the terms of the GNU Affero General Public License as\\n#    published by the Free Software Foundation, either version 3 of the\\n#    License, or (at your option) any later version.\\n#\\n#    This program is distributed in the hope that it will be useful,\\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#    GNU Affero General Public License for more details.\\n#\\n#    You should have received a copy of the GNU Affero General Public License\\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\\n#\\n##############################################################################\\n\\n\\n{\\n    \'name\': \'Products & Pricelists\',\\n    \'version\': \'1.1\',\\n    \'author\': \'OpenERP SA\',\\n    \'category\': \'Sales Management\',\\n    \'depends\': [\'base\', \'decimal_precision\', \'mail\', \'report\'],\\n    \'demo\': [\\n        \'product_demo.xml\',\\n        \'product_image_demo.xml\',\\n    ],\\n    \'website\': \'https://www.odoo.com\',\\n    \'description\': \\"\\"\\"\\nThis is the base module for managing products and pricelists in OpenERP.\\n========================================================================\\n\\nProducts support variants, different pricing methods, suppliers information,\\nmake to stock/order, different unit of measures, packaging and properties.\\n\\nPricelists support:\\n-------------------\\n    * Multiple-level of discount (by product, category, quantities)\\n    * Compute price based on different criteria:\\n        * Other pricelist\\n        * Cost price\\n        * List price\\n        * Supplier price\\n\\nPricelists preferences by product and/or partners.\\n\\nPrint product labels with barcode.\\n    \\"\\"\\",\\n    \'data\': [\\n        \'security/product_security.xml\',\\n        \'security/ir.model.access.csv\',\\n        \'wizard/product_price_view.xml\',\\n        \'product_data.xml\',\\n        \'product_report.xml\',\\n        \'product_view.xml\',\\n        \'pricelist_view.xml\',\\n        \'partner_view.xml\',\\n        \'views/report_pricelist.xml\',\\n    ],\\n    \'test\': [\\n        \'product_pricelist_demo.yml\',\\n        \'test/product_pricelist.yml\',\\n    ],\\n    \'installable\': True,\\n    \'auto_install\': False,\\n    \'images\': [\'images/product_uom.jpeg\',\'images/product_pricelists.jpeg\',\'images/products_categories.jpeg\', \'images/products_form.jpeg\'],\\n}\\n\\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\\n" }\n'
line: b'{ "repo_name": "libracore/erpnext", "ref": "refs/heads/v12", "path": "erpnext/hr/doctype/job_opening/test_job_opening.py", "content": "# -*- coding: utf-8 -*-\\n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\\n# See license.txt\\nfrom __future__ import unicode_literals\\n\\nimport frappe\\nimport unittest\\n\\n# test_records = frappe.get_test_records(\'Job Opening\')\\n\\nclass TestJobOpening(unittest.TestCase):\\n\\tpass\\n" }\n'
line: b'{ "repo_name": "alexdrenea/WinObjC", "ref": "refs/heads/tooling/toolsSln", "path": "deps/3rdparty/icu/icu/source/test/depstest/depstest.py", "content": "#! /usr/bin/python\\n# -*- coding: utf-8 -*-\\n#\\n# Copyright (C) 2011-2014, International Business Machines\\n# Corporation and others. All Rights Reserved.\\n#\\n# file name: depstest.py\\n#\\n# created on: 2011may24\\n\\n\\"\\"\\"ICU dependency tester.\\n\\nThis probably works only on Linux.\\n\\nThe exit code is 0 if everything is fine, 1 for errors, 2 for only warnings.\\n\\nSample invocation:\\n  ~/svn.icu/trunk/src/source/test/depstest$ ./depstest.py ~/svn.icu/trunk/dbg\\n\\"\\"\\"\\n\\n__author__ = \\"Markus W. Scherer\\"\\n\\nimport glob\\nimport os.path\\nimport subprocess\\nimport sys\\n\\nimport dependencies\\n\\n_ignored_symbols = set()\\n_obj_files = {}\\n_symbols_to_files = {}\\n_return_value = 0\\n\\n# Classes with vtables (and thus virtual methods).\\n_virtual_classes = set()\\n# Classes with weakly defined destructors.\\n# nm shows a symbol class of \\"W\\" rather than \\"T\\".\\n_weak_destructors = set()\\n\\ndef _ReadObjFile(root_path, library_name, obj_name):\\n  global _ignored_symbols, _obj_files, _symbols_to_files\\n  global _virtual_classes, _weak_destructors\\n  lib_obj_name = library_name + \\"/\\" + obj_name\\n  if lib_obj_name in _obj_files:\\n    print \\"Warning: duplicate .o file \\" + lib_obj_name\\n    _return_value = 2\\n    return\\n\\n  path = os.path.join(root_path, library_name, obj_name)\\n  nm_result = subprocess.Popen([\\"nm\\", \\"--demangle\\", \\"--format=sysv\\",\\n                                \\"--extern-only\\", \\"--no-sort\\", path],\\n                               stdout=subprocess.PIPE).communicate()[0]\\n  obj_imports = set()\\n  obj_exports = set()\\n  for line in nm_result.splitlines():\\n    fields = line.split(\\"|\\")\\n    if len(fields) == 1: continue\\n    name = fields[0].strip()\\n    # Ignore symbols like \'__cxa_pure_virtual\',\\n    # \'vtable for __cxxabiv1::__si_class_type_info\' or\\n    # \'DW.ref.__gxx_personality_v0\'.\\n    if name.startswith(\\"__cxa\\") or \\"__cxxabi\\" in name or \\"__gxx\\" in name:\\n      _ignored_symbols.add(name)\\n      continue\\n    type = fields[2].strip()\\n    if type == \\"U\\":\\n      obj_imports.add(name)\\n    else:\\n      obj_exports.add(name)\\n      _symbols_to_files[name] = lib_obj_name\\n      # Is this a vtable? E.g., \\"vtable for icu_49::ByteSink\\".\\n      if name.startswith(\\"vtable for icu\\"):\\n        _virtual_classes.add(name[name.index(\\"::\\") + 2:])\\n      # Is this a destructor? E.g., \\"icu_49::ByteSink::~ByteSink()\\".\\n      index = name.find(\\"::~\\")\\n      if index >= 0 and type == \\"W\\":\\n        _weak_destructors.add(name[index + 3:name.index(\\"(\\", index)])\\n  _obj_files[lib_obj_name] = {\\"imports\\": obj_imports, \\"exports\\": obj_exports}\\n\\ndef _ReadLibrary(root_path, library_name):\\n  obj_paths = glob.glob(os.path.join(root_path, library_name, \\"*.o\\"))\\n  for path in obj_paths:\\n    _ReadObjFile(root_path, library_name, os.path.basename(path))\\n\\ndef _Resolve(name, parents):\\n  global _ignored_symbols, _obj_files, _symbols_to_files, _return_value\\n  item = dependencies.items[name]\\n  item_type = item[\\"type\\"]\\n  if name in parents:\\n    sys.exit(\\"Error: %s %s has a circular dependency on itself: %s\\" %\\n             (item_type, name, parents))\\n  # Check if already cached.\\n  exports = item.get(\\"exports\\")\\n  if exports != None: return item\\n  # Calculcate recursively.\\n  parents.append(name)\\n  imports = set()\\n  exports = set()\\n  system_symbols = item.get(\\"system_symbols\\")\\n  if system_symbols == None: system_symbols = item[\\"system_symbols\\"] = set()\\n  files = item.get(\\"files\\")\\n  if files:\\n    for file_name in files:\\n      obj_file = _obj_files[file_name]\\n      imports |= obj_file[\\"imports\\"]\\n      exports |= obj_file[\\"exports\\"]\\n  imports -= exports | _ignored_symbols\\n  deps = item.get(\\"deps\\")\\n  if deps:\\n    for dep in deps:\\n      dep_item = _Resolve(dep, parents)\\n      # Detect whether this item needs to depend on dep,\\n      # except when this item has no files, that is, when it is just\\n      # a deliberate umbrella group or library.\\n      dep_exports = dep_item[\\"exports\\"]\\n      dep_system_symbols = dep_item[\\"system_symbols\\"]\\n      if files and imports.isdisjoint(dep_exports) and imports.isdisjoint(dep_system_symbols):\\n        print \\"Info:  %s %s  does not need to depend on  %s\\\\n\\" % (item_type, name, dep)\\n      # We always include the dependency\'s exports, even if we do not need them\\n      # to satisfy local imports.\\n      exports |= dep_exports\\n      system_symbols |= dep_system_symbols\\n  item[\\"exports\\"] = exports\\n  item[\\"system_symbols\\"] = system_symbols\\n  imports -= exports | system_symbols\\n  for symbol in imports:\\n    for file_name in files:\\n      if symbol in _obj_files[file_name][\\"imports\\"]:\\n        neededFile = _symbols_to_files.get(symbol)\\n        if neededFile in dependencies.file_to_item:\\n          neededItem = \\"but %s does not depend on %s (for %s)\\" % (name, dependencies.file_to_item[neededFile], neededFile)\\n        else:\\n          neededItem = \\"- is this a new system symbol?\\"\\n        sys.stderr.write(\\"Error: in %s %s: %s imports %s %s\\\\n\\" %\\n                         (item_type, name, file_name, symbol, neededItem))\\n    _return_value = 1\\n  del parents[-1]\\n  return item\\n\\ndef Process(root_path):\\n  \\"\\"\\"Loads dependencies.txt, reads the libraries\' .o files, and processes them.\\n\\n  Modifies dependencies.items: Recursively builds each item\'s system_symbols and exports.\\n  \\"\\"\\"\\n  global _ignored_symbols, _obj_files, _return_value\\n  global _virtual_classes, _weak_destructors\\n  dependencies.Load()\\n  for name_and_item in dependencies.items.iteritems():\\n    name = name_and_item[0]\\n    item = name_and_item[1]\\n    system_symbols = item.get(\\"system_symbols\\")\\n    if system_symbols:\\n      for symbol in system_symbols:\\n        _symbols_to_files[symbol] = name\\n  for library_name in dependencies.libraries:\\n    _ReadLibrary(root_path, library_name)\\n  o_files_set = set(_obj_files.keys())\\n  files_missing_from_deps = o_files_set - dependencies.files\\n  files_missing_from_build = dependencies.files - o_files_set\\n  if files_missing_from_deps:\\n    sys.stderr.write(\\"Error: files missing from dependencies.txt:\\\\n%s\\\\n\\" %\\n                     sorted(files_missing_from_deps))\\n    _return_value = 1\\n  if files_missing_from_build:\\n    sys.stderr.write(\\"Error: files in dependencies.txt but not built:\\\\n%s\\\\n\\" %\\n                     sorted(files_missing_from_build))\\n    _return_value = 1\\n  if not _return_value:\\n    for library_name in dependencies.libraries:\\n      _Resolve(library_name, [])\\n  if not _return_value:\\n    virtual_classes_with_weak_destructors = _virtual_classes & _weak_destructors\\n    if virtual_classes_with_weak_destructors:\\n      sys.stderr.write(\\"Error: Some classes have virtual methods, and \\"\\n                       \\"an implicit or inline destructor \\"\\n                       \\"(see ICU ticket #8454 for details):\\\\n%s\\\\n\\" %\\n                       sorted(virtual_classes_with_weak_destructors))\\n      _return_value = 1\\n\\ndef main():\\n  global _return_value\\n  if len(sys.argv) <= 1:\\n    sys.exit((\\"Command line error: \\" +\\n             \\"need one argument with the root path to the built ICU libraries/*.o files.\\"))\\n  Process(sys.argv[1])\\n  if _ignored_symbols:\\n    print \\"Info: ignored symbols:\\\\n%s\\" % sorted(_ignored_symbols)\\n  if not _return_value:\\n    print \\"OK: Specified and actual dependencies match.\\"\\n  else:\\n    print \\"Error: There were errors, please fix them and re-run. Processing may have terminated abnormally.\\"\\n  return _return_value\\n\\nif __name__ == \\"__main__\\":\\n  sys.exit(main())\\n" }\n'
line: b'{ "repo_name": "syci/OCB", "ref": "refs/heads/9.0", "path": "addons/account_asset/account_asset_invoice.py", "content": "# -*- coding: utf-8 -*-\\n\\nfrom datetime import datetime\\nfrom dateutil.relativedelta import relativedelta\\nfrom openerp import api, fields, models\\nimport openerp.addons.decimal_precision as dp\\nfrom openerp.tools import DEFAULT_SERVER_DATE_FORMAT as DF\\n\\n\\nclass AccountInvoice(models.Model):\\n    _inherit = \'account.invoice\'\\n\\n    @api.multi\\n    def action_move_create(self):\\n        result = super(AccountInvoice, self).action_move_create()\\n        for inv in self:\\n            inv.invoice_line_ids.asset_create()\\n        return result\\n\\n\\nclass AccountInvoiceLine(models.Model):\\n    _inherit = \'account.invoice.line\'\\n\\n    asset_category_id = fields.Many2one(\'account.asset.category\', string=\'Asset Category\')\\n    asset_start_date = fields.Date(string=\'Asset End Date\', compute=\'_get_asset_date\', readonly=True, store=True)\\n    asset_end_date = fields.Date(string=\'Asset Start Date\', compute=\'_get_asset_date\', readonly=True, store=True)\\n    asset_mrr = fields.Float(string=\'Monthly Recurring Revenue\', compute=\'_get_asset_date\', readonly=True, digits=dp.get_precision(\'Account\'), store=True)\\n\\n    @api.one\\n    @api.depends(\'asset_category_id\', \'invoice_id.date_invoice\')\\n    def _get_asset_date(self):\\n        self.asset_mrr = 0\\n        self.asset_start_date = False\\n        self.asset_end_date = False\\n        cat = self.asset_category_id\\n        if cat:\\n            months = cat.method_number * cat.method_period\\n            if self.invoice_id.type in [\'out_invoice\', \'out_refund\']:\\n                self.asset_mrr = self.price_subtotal_signed / months\\n            if self.invoice_id.date_invoice:\\n                start_date = datetime.strptime(self.invoice_id.date_invoice, DF).replace(day=1)\\n                end_date = (start_date + relativedelta(months=months, days=-1))\\n                self.asset_start_date = start_date.strftime(DF)\\n                self.asset_end_date = end_date.strftime(DF)\\n\\n    @api.one\\n    def asset_create(self):\\n        if self.asset_category_id and self.asset_category_id.method_number > 1:\\n            vals = {\\n                \'name\': self.name,\\n                \'code\': self.invoice_id.number or False,\\n                \'category_id\': self.asset_category_id.id,\\n                \'value\': self.price_subtotal,\\n                \'partner_id\': self.invoice_id.partner_id.id,\\n                \'company_id\': self.invoice_id.company_id.id,\\n                \'currency_id\': self.invoice_id.currency_id.id,\\n                \'date\': self.asset_start_date or self.invoice_id.date_invoice,\\n                \'invoice_id\': self.invoice_id.id,\\n          }\\n            changed_vals = self.env[\'account.asset.asset\'].onchange_category_id_values(vals[\'category_id\'])\\n            vals.update(changed_vals[\'value\'])\\n            asset = self.env[\'account.asset.asset\'].create(vals)\\n            if self.asset_category_id.open_asset:\\n                asset.validate()\\n        return True\\n\\n    @api.onchange(\'product_id\')\\n    def onchange_product_id(self):\\n        if self.product_id:\\n            if self.invoice_id.type == \'out_invoice\':\\n                self.asset_category_id = self.product_id.product_tmpl_id.deferred_revenue_category_id\\n            elif self.invoice_id.type == \'in_invoice\':\\n                self.asset_category_id = self.product_id.product_tmpl_id.asset_category_id\\n\\n\\nclass ProductTemplate(models.Model):\\n    _inherit = \'product.template\'\\n    asset_category_id = fields.Many2one(\'account.asset.category\', string=\'Asset Type\', ondelete=\\"restrict\\")\\n    deferred_revenue_category_id = fields.Many2one(\'account.asset.category\', string=\'Deferred Revenue Type\', ondelete=\\"restrict\\")\\n" }\n'
line: b'{ "repo_name": "Exgibichi/statusquo", "ref": "refs/heads/0.15", "path": "test/functional/zapwallettxes.py", "content": "#!/usr/bin/env python3\\n# Copyright (c) 2014-2016 The Bitcoin Core developers\\n# Distributed under the MIT software license, see the accompanying\\n# file COPYING or http://www.opensource.org/licenses/mit-license.php.\\n\\"\\"\\"Test the zapwallettxes functionality.\\n\\n- start two statusquod nodes\\n- create two transactions on node 0 - one is confirmed and one is unconfirmed.\\n- restart node 0 and verify that both the confirmed and the unconfirmed\\n  transactions are still available.\\n- restart node 0 with zapwallettxes and persistmempool, and verify that both\\n  the confirmed and the unconfirmed transactions are still available.\\n- restart node 0 with just zapwallettxed and verify that the confirmed\\n  transactions are still available, but that the unconfirmed transaction has\\n  been zapped.\\n\\"\\"\\"\\nfrom test_framework.test_framework import StatusquoTestFramework\\nfrom test_framework.util import (assert_equal,\\n                                 assert_raises_jsonrpc,\\n                                 )\\n\\nclass ZapWalletTXesTest (StatusquoTestFramework):\\n\\n    def __init__(self):\\n        super().__init__()\\n        self.setup_clean_chain = True\\n        self.num_nodes = 2\\n\\n    def run_test(self):\\n        self.log.info(\\"Mining blocks...\\")\\n        self.nodes[0].generate(1)\\n        self.sync_all()\\n        self.nodes[1].generate(100)\\n        self.sync_all()\\n\\n        # This transaction will be confirmed\\n        txid1 = self.nodes[0].sendtoaddress(self.nodes[1].getnewaddress(), 10)\\n\\n        self.nodes[0].generate(1)\\n        self.sync_all()\\n\\n        # This transaction will not be confirmed\\n        txid2 = self.nodes[0].sendtoaddress(self.nodes[1].getnewaddress(), 20)\\n\\n        # Confirmed and unconfirmed transactions are now in the wallet.\\n        assert_equal(self.nodes[0].gettransaction(txid1)[\'txid\'], txid1)\\n        assert_equal(self.nodes[0].gettransaction(txid2)[\'txid\'], txid2)\\n\\n        # Stop-start node0. Both confirmed and unconfirmed transactions remain in the wallet.\\n        self.stop_node(0)\\n        self.nodes[0] = self.start_node(0, self.options.tmpdir)\\n\\n        assert_equal(self.nodes[0].gettransaction(txid1)[\'txid\'], txid1)\\n        assert_equal(self.nodes[0].gettransaction(txid2)[\'txid\'], txid2)\\n\\n        # Stop node0 and restart with zapwallettxes and persistmempool. The unconfirmed\\n        # transaction is zapped from the wallet, but is re-added when the mempool is reloaded.\\n        self.stop_node(0)\\n        self.nodes[0] = self.start_node(0, self.options.tmpdir, [\\"-persistmempool=1\\", \\"-zapwallettxes=2\\"])\\n\\n        assert_equal(self.nodes[0].gettransaction(txid1)[\'txid\'], txid1)\\n        assert_equal(self.nodes[0].gettransaction(txid2)[\'txid\'], txid2)\\n\\n        # Stop node0 and restart with zapwallettxes, but not persistmempool.\\n        # The unconfirmed transaction is zapped and is no longer in the wallet.\\n        self.stop_node(0)\\n        self.nodes[0] = self.start_node(0, self.options.tmpdir, [\\"-zapwallettxes=2\\"])\\n\\n        # tx1 is still be available because it was confirmed\\n        assert_equal(self.nodes[0].gettransaction(txid1)[\'txid\'], txid1)\\n\\n        # This will raise an exception because the unconfirmed transaction has been zapped\\n        assert_raises_jsonrpc(-5, \'Invalid or non-wallet transaction id\', self.nodes[0].gettransaction, txid2)\\n\\nif __name__ == \'__main__\':\\n    ZapWalletTXesTest().main()\\n" }\n'
line: b'{ "repo_name": "schristakidis/p2ner", "ref": "refs/heads/development", "path": "p2ner/components/ui/gtkgui/gtkgui/options/remoteproducer.py", "content": "import os, sys\\n#   Copyright 2012 Loris Corazza, Sakis Christakidis\\n#\\n#   Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n#   you may not use this file except in compliance with the License.\\n#   You may obtain a copy of the License at\\n#\\n#       http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#   Unless required by applicable law or agreed to in writing, software\\n#   distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n#   See the License for the specific language governing permissions and\\n#   limitations under the License.\\n\\nfrom twisted.internet import gtk2reactor\\ntry:\\n    gtk2reactor.install()\\nexcept:\\n    pass\\nimport pygtk\\nfrom twisted.internet import reactor\\npygtk.require(\\"2.0\\")\\nimport gtk\\nimport gobject\\nfrom generic import genericFrame\\nfrom p2ner.util.utilities import get_user_data_dir\\nfrom pkg_resources import resource_string\\nfrom gtkgui.remotefilechooser import RemoteFileChooser\\n\\n\\nENCODINGS={\'Greek\':\'ISO-8859-7\',\\n                      \'Universal\':\'UTF-8\',\\n                      \'Western European\':\'Latin-9\'}\\n\\nclass remoteproducerFrame(genericFrame):\\n    def initUI(self):\\n        self.builder = gtk.Builder()\\n        self.builder.add_from_string(resource_string(__name__, \'optRemote.glade\'))\\n        self.builder.connect_signals(self)\\n        \\n        self.changeButton=self.builder.get_object(\'changeButton\')\\n        self.frame=self.builder.get_object(\'remoteProducerFrame\')\\n        self.checkButton=self.builder.get_object(\'checkRemote\')\\n        self.checkButton.connect(\'toggled\',self.on_check_toggled)\\n        self.passEntry=self.builder.get_object(\'passEntry\')\\n\\n        self.passEntry.connect(\'activate\',self.on_pass_edited)\\n        self.passEntry.connect(\'focus-out-event\',self.on_focus_out)\\n        self.dirEntry=self.builder.get_object(\'dirEntry\')\\n        self.dirEntry.set_sensitive(False)\\n        \\n    def refresh(self):\\n        self.passEntry.set_sensitive(False)\\n        self.changeButton.set_sensitive(True)\\n        remotePref=self.preferences.getRemotePreferences()\\n        self.checkButton.set_active(remotePref[\'enable\'])\\n        self.passEntry.set_text(remotePref[\'password\'])\\n        self.oldPass=remotePref[\'password\']\\n        self.dirEntry.set_text(remotePref[\'dir\'])\\n        \\n\\n    def on_check_toggled(self,widget):\\n        self.preferences.setEnableRemoteProducer(widget.get_active())\\n        \\n    def on_changeButton_clicked(self,widget):\\n        widget.set_sensitive(False)\\n        self.passEntry.set_sensitive(True)\\n        self.passEntry.set_text(\'\')\\n        self.passEntry.grab_focus()\\n        \\n    def on_pass_edited(self,widget):\\n        self.oldPass=widget.get_text()\\n        self.preferences.setRemotePassword(self.oldPass)\\n        widget.set_sensitive(False)\\n        self.changeButton.set_sensitive(True)\\n        \\n    def on_focus_out(self,widget,event):\\n        widget.set_text(self.oldPass)\\n        widget.set_sensitive(False)\\n        self.changeButton.set_sensitive(True)\\n        \\n    def on_openButton_clicked(self,widget):\\n        if self.remote:\\n            RemoteFileChooser(self.browseFinished,self.interface,onlyDir=True)\\n        else:\\n            self.browseLocally()\\n    \\n    def browseLocally(self): \\n        dialog = gtk.FileChooserDialog(\\"Open..\\",\\n                               None,\\n                               gtk.FILE_CHOOSER_ACTION_SELECT_FOLDER,\\n                               (gtk.STOCK_CANCEL, gtk.RESPONSE_CANCEL,\\n                                gtk.STOCK_OPEN, gtk.RESPONSE_OK))\\n        dialog.set_default_response(gtk.RESPONSE_OK)\\n\\n        \\n        response = dialog.run()\\n        if response == gtk.RESPONSE_OK:\\n            #print dialog.get_filename(), \'selected\'\\n            filename = dialog.get_filename()     \\n            self.browseFinished(filename)      \\n        elif response == gtk.RESPONSE_CANCEL:\\n            filename=None\\n\\n        \\n        dialog.destroy()\\n        \\n    def browseFinished(self,filename):\\n        if filename:\\n            self.dirEntry.set_text(filename)\\n            self.preferences.setRemoteProducerDir(filename)           \\n\\n                \\n" }\n'
line: b'{ "repo_name": "petrus-v/server-tools", "ref": "refs/heads/7.0", "path": "base_optional_quick_create/__init__.py", "content": "# -*- coding: utf-8 -*-\\n##############################################################################\\n#\\n#    Copyright (C) 2013 Agile Business Group sagl (<http://www.agilebg.com>)\\n#\\n#    This program is free software: you can redistribute it and/or modify\\n#    it under the terms of the GNU Affero General Public License as published\\n#    by the Free Software Foundation, either version 3 of the License, or\\n#    (at your option) any later version.\\n#\\n#    This program is distributed in the hope that it will be useful,\\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#    GNU Affero General Public License for more details.\\n#\\n#    You should have received a copy of the GNU Affero General Public License\\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\\n#\\n##############################################################################\\nfrom . import model\\n" }\n'
line: b'{ "repo_name": "credosemi/rt-thread-openrisc", "ref": "refs/heads/stable-v1.2.x-openrisc", "path": "bsp/sep6200/rtconfig.py", "content": "import os\\n\\n# toolchains options\\nARCH     = \'unicore32\'\\nCPU      = \'sep6200\'\\nTextBase = \'0x40000000\'\\n\\nCROSS_TOOL \\t= \'gcc\'\\n\\nif os.getenv(\'RTT_CC\'):\\n\\tCROSS_TOOL = os.getenv(\'RTT_CC\')\\n\\nif  CROSS_TOOL == \'gcc\':\\n\\tPLATFORM \\t= \'gcc\'\\n\\tEXEC_PATH \\t= \'/usr/unicore/gnu-toolchain-unicore/uc4-1.0-beta-hard-RHELAS5/bin/\'\\nelse :\\n    print \'================ERROR============================\'\\n    print \'Not support yet!\'\\n    print \'=================================================\'\\n    exit(0)\\n\\nif os.getenv(\'RTT_EXEC_PATH\'):\\n\\tEXEC_PATH = os.getenv(\'RTT_EXEC_PATH\')\\n\\n#BUILD = \'debug\'\\nBUILD = \'release\'\\n\\nif PLATFORM == \'gcc\':\\n    # toolchains\\n    PREFIX = \'unicore32-linux-\'\\n    CC = PREFIX + \'gcc\'\\n    AS = PREFIX + \'gcc\'\\n    AR = PREFIX + \'ar\'\\n    LINK = PREFIX + \'ld\'\\n    TARGET_EXT = \'elf\'\\n    SIZE = PREFIX + \'size\'\\n    OBJDUMP = PREFIX + \'objdump\'\\n    OBJCPY = PREFIX + \'objcopy\'\\n\\n    DEVICE = \' \'\\n    CFLAGS = DEVICE\\n    AFLAGS = \' -c\' + DEVICE + \' -x assembler-with-cpp\' + \' -DTEXT_BASE=\' + TextBase\\n    LFLAGS = DEVICE + \' -Bstatic --gc-sections -Map=rtthread_sep6200.map -cref -u _start -T sep6200.ld -L/usr/unicore/gnu-toolchain-unicore/uc4-1.0-beta-hard-RHELAS5/lib/gcc/unicore32-linux/4.4.2 -lgcc\' + \' -Ttext \' + TextBase\\n\\n    CPATH = \'\'\\n    LPATH = \'\'\\n\\n    if BUILD == \'debug\':\\n        CFLAGS += \' -O0 -gdwarf-2\'\\n        AFLAGS += \' -gdwarf-2\'\\n    else:\\n        CFLAGS += \' -O2\'\\n\\n    POST_ACTION = OBJCPY + \' -O binary $TARGET rtthread.bin\\\\n\' + SIZE + \' $TARGET \\\\n\'\\n" }\n'
line: b'{ "repo_name": "jwhui/openthread", "ref": "refs/heads/efr32", "path": "tests/toranj/test-002-form.py", "content": "#!/usr/bin/env python3\\n#\\n#  Copyright (c) 2018, The OpenThread Authors.\\n#  All rights reserved.\\n#\\n#  Redistribution and use in source and binary forms, with or without\\n#  modification, are permitted provided that the following conditions are met:\\n#  1. Redistributions of source code must retain the above copyright\\n#     notice, this list of conditions and the following disclaimer.\\n#  2. Redistributions in binary form must reproduce the above copyright\\n#     notice, this list of conditions and the following disclaimer in the\\n#     documentation and/or other materials provided with the distribution.\\n#  3. Neither the name of the copyright holder nor the\\n#     names of its contributors may be used to endorse or promote products\\n#     derived from this software without specific prior written permission.\\n#\\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \\"AS IS\\"\\n#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\n#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\\n#  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\\n#  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\\n#  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\\n#  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\\n#  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\\n#  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\\n#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\\n#  POSSIBILITY OF SUCH DAMAGE.\\n\\nfrom wpan import verify\\nimport wpan\\n\\n# -----------------------------------------------------------------------------------------------------------------------\\n# Test description: forming a Thread network\\n\\ntest_name = __file__[:-3] if __file__.endswith(\'.py\') else __file__\\nprint(\'-\' * 120)\\nprint(\'Starting \\\\\'{}\\\\\'\'.format(test_name))\\n\\n# -----------------------------------------------------------------------------------------------------------------------\\n# Creating `wpan.Nodes` instances\\n\\nspeedup = 4\\nwpan.Node.set_time_speedup_factor(speedup)\\n\\nnode = wpan.Node()\\n\\n# -----------------------------------------------------------------------------------------------------------------------\\n# Init all nodes\\n\\nwpan.Node.init_all_nodes()\\n\\n# -----------------------------------------------------------------------------------------------------------------------\\n# Test implementation\\n\\n# default values after reset\\nDEFAULT_NAME = \'\\"OpenThread\\"\'\\nDEFAULT_PANID = \'0xFFFF\'\\nDEFAULT_XPANID = \'0xDEAD00BEEF00CAFE\'\\n\\nverify(node.get(wpan.WPAN_STATE) == wpan.STATE_OFFLINE)\\nverify(node.get(wpan.WPAN_NAME) == DEFAULT_NAME)\\nverify(node.get(wpan.WPAN_PANID) == DEFAULT_PANID)\\nverify(node.get(wpan.WPAN_XPANID) == DEFAULT_XPANID)\\n\\n# Form a network\\n\\nnode.form(\'asha\')\\nverify(node.get(wpan.WPAN_STATE) == wpan.STATE_ASSOCIATED)\\nverify(node.get(wpan.WPAN_NODE_TYPE) == wpan.NODE_TYPE_LEADER)\\nverify(node.get(wpan.WPAN_NAME) == \'\\"asha\\"\')\\nverify(node.get(wpan.WPAN_PANID) != DEFAULT_PANID)\\nverify(node.get(wpan.WPAN_XPANID) != DEFAULT_XPANID)\\n\\nnode.leave()\\nverify(node.get(wpan.WPAN_STATE) == wpan.STATE_OFFLINE)\\n\\n# Form a network on a specific channel.\\n\\nnode.form(\'ahura\', channel=20)\\nverify(node.get(wpan.WPAN_STATE) == wpan.STATE_ASSOCIATED)\\nverify(node.get(wpan.WPAN_NAME) == \'\\"ahura\\"\')\\nverify(node.get(wpan.WPAN_CHANNEL) == \'20\')\\nverify(node.get(wpan.WPAN_PANID) != DEFAULT_PANID)\\nverify(node.get(wpan.WPAN_XPANID) != DEFAULT_XPANID)\\n\\nnode.leave()\\nverify(node.get(wpan.WPAN_STATE) == wpan.STATE_OFFLINE)\\n\\n# Form a network with a specific panid, xpanid and key specified separately\\n\\nnode.set(wpan.WPAN_PANID, \'0x1977\')\\nnode.set(wpan.WPAN_XPANID, \'1020031510006016\', binary_data=True)\\nnode.set(wpan.WPAN_KEY, \'0123456789abcdeffecdba9876543210\', binary_data=True)\\n\\nnode.form(\'mazda\', channel=12)\\nverify(node.get(wpan.WPAN_STATE) == wpan.STATE_ASSOCIATED)\\nverify(node.get(wpan.WPAN_NAME) == \'\\"mazda\\"\')\\nverify(node.get(wpan.WPAN_CHANNEL) == \'12\')\\nverify(node.get(wpan.WPAN_KEY) == \'[0123456789ABCDEFFECDBA9876543210]\')\\nverify(node.get(wpan.WPAN_PANID) == \'0x1977\')\\nverify(node.get(wpan.WPAN_XPANID) == \'0x1020031510006016\')\\n\\nnode.leave()\\nverify(node.get(wpan.WPAN_STATE) == wpan.STATE_OFFLINE)\\n\\n# Form a network with all parameters given as part of `form` command itself\\n\\nnode.form(\\n    \'vahman\',\\n    channel_mask=\'15,20-24\',\\n    panid=\'0x1977\',\\n    xpanid=\'1020031510006016\',\\n    key=\'0123456789abcdeffecdba9876543210\',\\n    key_index=\'1\',\\n    mesh_local_prefix=\'fd00:cafe::\',\\n)\\n\\nverify(node.get(wpan.WPAN_STATE) == wpan.STATE_ASSOCIATED)\\nverify(node.get(wpan.WPAN_NAME) == \'\\"vahman\\"\')\\nchannel = int(node.get(wpan.WPAN_CHANNEL), 0)\\nverify(channel == 15 or (20 <= channel <= 24))\\nverify(node.get(wpan.WPAN_KEY) == \'[0123456789ABCDEFFECDBA9876543210]\')\\nverify(node.get(wpan.WPAN_KEY_INDEX) == \'1\')\\nverify(node.get(wpan.WPAN_PANID) == \'0x1977\')\\nverify(node.get(wpan.WPAN_XPANID) == \'0x1020031510006016\')\\nverify(node.get(wpan.WPAN_IP6_MESH_LOCAL_PREFIX) == \'\\"fd00:cafe::/64\\"\')\\n\\n# Verify behavior when commands are issued immediately after a `reset`\\n\\nnode.reset()\\nnode.leave()\\n\\nnode.reset()\\nnode.form(\'net-after-reset\')\\n\\n# -----------------------------------------------------------------------------------------------------------------------\\n# Test finished\\n\\nwpan.Node.finalize_all_nodes()\\n\\nprint(\'\\\\\'{}\\\\\' passed.\'.format(test_name))\\n" }\n'
line: b'{ "repo_name": "StefanRijnhart/server-tools", "ref": "refs/heads/7.0", "path": "base_optional_quick_create/__init__.py", "content": "# -*- coding: utf-8 -*-\\n##############################################################################\\n#\\n#    Copyright (C) 2013 Agile Business Group sagl (<http://www.agilebg.com>)\\n#\\n#    This program is free software: you can redistribute it and/or modify\\n#    it under the terms of the GNU Affero General Public License as published\\n#    by the Free Software Foundation, either version 3 of the License, or\\n#    (at your option) any later version.\\n#\\n#    This program is distributed in the hope that it will be useful,\\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#    GNU Affero General Public License for more details.\\n#\\n#    You should have received a copy of the GNU Affero General Public License\\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\\n#\\n##############################################################################\\nfrom . import model\\n" }\n'
line: b'{ "repo_name": "openthread/openthread", "ref": "refs/heads/main", "path": "tests/toranj/test-002-form.py", "content": "#!/usr/bin/env python3\\n#\\n#  Copyright (c) 2018, The OpenThread Authors.\\n#  All rights reserved.\\n#\\n#  Redistribution and use in source and binary forms, with or without\\n#  modification, are permitted provided that the following conditions are met:\\n#  1. Redistributions of source code must retain the above copyright\\n#     notice, this list of conditions and the following disclaimer.\\n#  2. Redistributions in binary form must reproduce the above copyright\\n#     notice, this list of conditions and the following disclaimer in the\\n#     documentation and/or other materials provided with the distribution.\\n#  3. Neither the name of the copyright holder nor the\\n#     names of its contributors may be used to endorse or promote products\\n#     derived from this software without specific prior written permission.\\n#\\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \\"AS IS\\"\\n#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\n#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\\n#  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\\n#  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\\n#  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\\n#  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\\n#  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\\n#  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\\n#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\\n#  POSSIBILITY OF SUCH DAMAGE.\\n\\nfrom wpan import verify\\nimport wpan\\n\\n# -----------------------------------------------------------------------------------------------------------------------\\n# Test description: forming a Thread network\\n\\ntest_name = __file__[:-3] if __file__.endswith(\'.py\') else __file__\\nprint(\'-\' * 120)\\nprint(\'Starting \\\\\'{}\\\\\'\'.format(test_name))\\n\\n# -----------------------------------------------------------------------------------------------------------------------\\n# Creating `wpan.Nodes` instances\\n\\nspeedup = 4\\nwpan.Node.set_time_speedup_factor(speedup)\\n\\nnode = wpan.Node()\\n\\n# -----------------------------------------------------------------------------------------------------------------------\\n# Init all nodes\\n\\nwpan.Node.init_all_nodes()\\n\\n# -----------------------------------------------------------------------------------------------------------------------\\n# Test implementation\\n\\n# default values after reset\\nDEFAULT_NAME = \'\\"OpenThread\\"\'\\nDEFAULT_PANID = \'0xFFFF\'\\nDEFAULT_XPANID = \'0xDEAD00BEEF00CAFE\'\\n\\nverify(node.get(wpan.WPAN_STATE) == wpan.STATE_OFFLINE)\\nverify(node.get(wpan.WPAN_NAME) == DEFAULT_NAME)\\nverify(node.get(wpan.WPAN_PANID) == DEFAULT_PANID)\\nverify(node.get(wpan.WPAN_XPANID) == DEFAULT_XPANID)\\n\\n# Form a network\\n\\nnode.form(\'asha\')\\nverify(node.get(wpan.WPAN_STATE) == wpan.STATE_ASSOCIATED)\\nverify(node.get(wpan.WPAN_NODE_TYPE) == wpan.NODE_TYPE_LEADER)\\nverify(node.get(wpan.WPAN_NAME) == \'\\"asha\\"\')\\nverify(node.get(wpan.WPAN_PANID) != DEFAULT_PANID)\\nverify(node.get(wpan.WPAN_XPANID) != DEFAULT_XPANID)\\n\\nnode.leave()\\nverify(node.get(wpan.WPAN_STATE) == wpan.STATE_OFFLINE)\\n\\n# Form a network on a specific channel.\\n\\nnode.form(\'ahura\', channel=20)\\nverify(node.get(wpan.WPAN_STATE) == wpan.STATE_ASSOCIATED)\\nverify(node.get(wpan.WPAN_NAME) == \'\\"ahura\\"\')\\nverify(node.get(wpan.WPAN_CHANNEL) == \'20\')\\nverify(node.get(wpan.WPAN_PANID) != DEFAULT_PANID)\\nverify(node.get(wpan.WPAN_XPANID) != DEFAULT_XPANID)\\n\\nnode.leave()\\nverify(node.get(wpan.WPAN_STATE) == wpan.STATE_OFFLINE)\\n\\n# Form a network with a specific panid, xpanid and key specified separately\\n\\nnode.set(wpan.WPAN_PANID, \'0x1977\')\\nnode.set(wpan.WPAN_XPANID, \'1020031510006016\', binary_data=True)\\nnode.set(wpan.WPAN_KEY, \'0123456789abcdeffecdba9876543210\', binary_data=True)\\n\\nnode.form(\'mazda\', channel=12)\\nverify(node.get(wpan.WPAN_STATE) == wpan.STATE_ASSOCIATED)\\nverify(node.get(wpan.WPAN_NAME) == \'\\"mazda\\"\')\\nverify(node.get(wpan.WPAN_CHANNEL) == \'12\')\\nverify(node.get(wpan.WPAN_KEY) == \'[0123456789ABCDEFFECDBA9876543210]\')\\nverify(node.get(wpan.WPAN_PANID) == \'0x1977\')\\nverify(node.get(wpan.WPAN_XPANID) == \'0x1020031510006016\')\\n\\nnode.leave()\\nverify(node.get(wpan.WPAN_STATE) == wpan.STATE_OFFLINE)\\n\\n# Form a network with all parameters given as part of `form` command itself\\n\\nnode.form(\\n    \'vahman\',\\n    channel_mask=\'15,20-24\',\\n    panid=\'0x1977\',\\n    xpanid=\'1020031510006016\',\\n    key=\'0123456789abcdeffecdba9876543210\',\\n    key_index=\'1\',\\n    mesh_local_prefix=\'fd00:cafe::\',\\n)\\n\\nverify(node.get(wpan.WPAN_STATE) == wpan.STATE_ASSOCIATED)\\nverify(node.get(wpan.WPAN_NAME) == \'\\"vahman\\"\')\\nchannel = int(node.get(wpan.WPAN_CHANNEL), 0)\\nverify(channel == 15 or (20 <= channel <= 24))\\nverify(node.get(wpan.WPAN_KEY) == \'[0123456789ABCDEFFECDBA9876543210]\')\\nverify(node.get(wpan.WPAN_KEY_INDEX) == \'1\')\\nverify(node.get(wpan.WPAN_PANID) == \'0x1977\')\\nverify(node.get(wpan.WPAN_XPANID) == \'0x1020031510006016\')\\nverify(node.get(wpan.WPAN_IP6_MESH_LOCAL_PREFIX) == \'\\"fd00:cafe::/64\\"\')\\n\\n# Verify behavior when commands are issued immediately after a `reset`\\n\\nnode.reset()\\nnode.leave()\\n\\nnode.reset()\\nnode.form(\'net-after-reset\')\\n\\n# -----------------------------------------------------------------------------------------------------------------------\\n# Test finished\\n\\nwpan.Node.finalize_all_nodes()\\n\\nprint(\'\\\\\'{}\\\\\' passed.\'.format(test_name))\\n" }\n'
line: b'{ "repo_name": "defivelo/db", "ref": "refs/heads/dependabot/pip/requirements/django-debug-toolbar-3.2.1", "path": "apps/user/migrations/0040_auto_20170824_1109.py", "content": "# Generated by Django 1.11.4 on 2017-08-24 09:09\\nfrom __future__ import unicode_literals\\n\\nfrom django.db import migrations, models\\n\\n\\nclass Migration(migrations.Migration):\\n\\n    dependencies = [\\n        (\'user\', \'0039_rename_back\'),\\n    ]\\n\\n    operations = [\\n        migrations.AlterField(\\n            model_name=\'userprofile\',\\n            name=\'actor_for\',\\n            field=models.ManyToManyField(blank=True, related_name=\'actor_for\', to=\'challenge.QualificationActivity\', verbose_name=\'Intervenant\'),\\n        ),\\n    ]\\n" }\n'
line: b'{ "repo_name": "archetipo/server-tools", "ref": "refs/heads/7.0", "path": "base_optional_quick_create/__init__.py", "content": "# -*- coding: utf-8 -*-\\n##############################################################################\\n#\\n#    Copyright (C) 2013 Agile Business Group sagl (<http://www.agilebg.com>)\\n#\\n#    This program is free software: you can redistribute it and/or modify\\n#    it under the terms of the GNU Affero General Public License as published\\n#    by the Free Software Foundation, either version 3 of the License, or\\n#    (at your option) any later version.\\n#\\n#    This program is distributed in the hope that it will be useful,\\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#    GNU Affero General Public License for more details.\\n#\\n#    You should have received a copy of the GNU Affero General Public License\\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\\n#\\n##############################################################################\\nfrom . import model\\n" }\n'
line: b'{ "repo_name": "georgid/sms-tools", "ref": "refs/heads/georgid-withMelodia", "path": "lectures/4-STFT/plots-code/sine-spectrum.py", "content": "import matplotlib.pyplot as plt\\nimport numpy as np\\nfrom scipy.fftpack import fft, ifft\\n\\nN = 256\\nM = 63\\nf0 = 1000\\nfs = 10000\\nA0 = .8 \\nhN = N/2 \\nhM = (M+1)/2\\nfftbuffer = np.zeros(N)\\nX1 = np.zeros(N, dtype=\'complex\')\\nX2 = np.zeros(N, dtype=\'complex\')\\n\\nx = A0 * np.cos(2*np.pi*f0/fs*np.arange(-hM+1,hM))\\n\\nplt.figure(1, figsize=(9.5, 7))\\nw = np.hanning(M)\\nplt.subplot(2,3,1)\\nplt.title(\'w (hanning window)\')\\nplt.plot(np.arange(-hM+1, hM), w, \'b\', lw=1.5)\\nplt.axis([-hM+1, hM, 0, 1])\\n\\nfftbuffer[:hM] = w[hM-1:]\\nfftbuffer[N-hM+1:] = w[:hM-1]  \\nX = fft(fftbuffer)\\nX1[:hN] = X[hN:]\\nX1[N-hN:] = X[:hN]\\nmX = 20*np.log10(abs(X1))       \\n\\nplt.subplot(2,3,2)\\nplt.title(\'mW\')\\nplt.plot(np.arange(-hN, hN), mX, \'r\', lw=1.5)\\nplt.axis([-hN,hN,-40,max(mX)])\\n\\npX = np.angle(X1)\\nplt.subplot(2,3,3)\\nplt.title(\'pW\')\\nplt.plot(np.arange(-hN, hN), np.unwrap(pX), \'c\', lw=1.5)\\nplt.axis([-hN,hN,min(np.unwrap(pX)),max(np.unwrap(pX))])\\n\\nplt.subplot(2,3,4)\\nplt.title(\'xw (windowed sinewave)\')\\nxw = x*w\\nplt.plot(np.arange(-hM+1, hM), xw, \'b\', lw=1.5)\\nplt.axis([-hM+1, hM, -1, 1])\\n\\nfftbuffer = np.zeros(N)\\nfftbuffer[0:hM] = xw[hM-1:]\\nfftbuffer[N-hM+1:] = xw[:hM-1]\\nX = fft(fftbuffer)\\nX2[:hN] = X[hN:]\\nX2[N-hN:] = X[:hN]\\nmX2 = 20*np.log10(abs(X2))  \\n\\nplt.subplot(2,3,5)\\nplt.title(\'mXW\')\\nplt.plot(np.arange(-hN, hN), mX2, \'r\', lw=1.5)\\nplt.axis([-hN,hN,-40,max(mX)])\\n\\npX = np.angle(X2)\\nplt.subplot(2,3,6)\\nplt.title(\'pXW\')\\nplt.plot(np.arange(-hN, hN), np.unwrap(pX), \'c\', lw=1.5)\\nplt.axis([-hN,hN,min(np.unwrap(pX)),max(np.unwrap(pX))])\\n\\nplt.tight_layout()\\nplt.savefig(\'sine-spectrum.png\')\\nplt.show()\\n" }\n'
line: b'{ "repo_name": "klahnakoski/Bugzilla-ETL", "ref": "refs/heads/v2", "path": "vendor/jx_python/namespace/normal.py", "content": "# encoding: utf-8\\n#\\n#\\n# This Source Code Form is subject to the terms of the Mozilla Public\\n# License, v. 2.0. If a copy of the MPL was not distributed with this file,\\n# You can obtain one at http://mozilla.org/MPL/2.0/.\\n#\\n# Author: Kyle Lahnakoski (kyle@lahnakoski.com)\\n#\\nfrom __future__ import absolute_import\\nfrom __future__ import division\\nfrom __future__ import unicode_literals\\n\\nfrom collections import Mapping\\nfrom copy import copy\\n\\nfrom mo_dots import Data\\nfrom mo_dots import FlatList\\nfrom mo_dots import coalesce, Null\\nfrom mo_dots import wrap, listwrap\\nfrom mo_logs import Log\\nfrom mo_math import Math\\n\\nfrom jx_base.dimensions import Dimension\\nfrom jx_base.domains import Domain\\nfrom jx_python.containers import Container\\nfrom jx_python.expressions import TRUE\\nfrom jx_python.namespace import Namespace, convert_list\\nfrom jx_base.query import QueryOp, get_all_vars\\n\\nDEFAULT_LIMIT = 10\\n\\n\\nclass Normal(Namespace):\\n    \\"\\"\\"\\n    UNREMARKABLE NAMESPACE, SIMPLY FOR CONVERTING QUERY TO NORMAL FORM\\n    \\"\\"\\"\\n\\n    def convert(self, expr):\\n        if isinstance(expr, Mapping) and expr[\\"from\\"]:\\n            return self._convert_query(expr)\\n        return expr\\n\\n\\n    def _convert_query(self, query):\\n        # if not isinstance(query[\\"from\\"], Container):\\n        #     Log.error(\'Expecting from clause to be a Container\')\\n        query = wrap(query)\\n\\n        output = QueryOp(\\"from\\", None)\\n        output[\\"from\\"] = self._convert_from(query[\\"from\\"])\\n\\n        output.format = query.format\\n\\n        if query.select:\\n            output.select = convert_list(self._convert_select, query.select)\\n        else:\\n            if query.edges or query.groupby:\\n                output.select = {\\"name\\": \\"count\\", \\"value\\": \\".\\", \\"aggregate\\": \\"count\\", \\"default\\": 0}\\n            else:\\n                output.select = {\\"name\\": \\"__all__\\", \\"value\\": \\"*\\", \\"aggregate\\": \\"none\\"}\\n\\n        if query.groupby and query.edges:\\n            Log.error(\\"You can not use both the `groupby` and `edges` clauses in the same query!\\")\\n        elif query.edges:\\n            output.edges = convert_list(self._convert_edge, query.edges)\\n            output.groupby = None\\n        elif query.groupby:\\n            output.edges = None\\n            output.groupby = convert_list(self._convert_group, query.groupby)\\n        else:\\n            output.edges = []\\n            output.groupby = None\\n\\n        output.where = self.convert(query.where)\\n        output.window = convert_list(self._convert_window, query.window)\\n        output.sort = self._convert_sort(query.sort)\\n\\n        output.limit = coalesce(query.limit, DEFAULT_LIMIT)\\n        if not Math.is_integer(output.limit) or output.limit < 0:\\n            Log.error(\\"Expecting limit >= 0\\")\\n\\n        output.isLean = query.isLean\\n\\n        # DEPTH ANALYSIS - LOOK FOR COLUMN REFERENCES THAT MAY BE DEEPER THAN\\n        # THE from SOURCE IS.\\n        vars = get_all_vars(output, exclude_where=True)  # WE WILL EXCLUDE where VARIABLES\\n        for c in query.columns:\\n            if c.name in vars and len(c.nested_path) != 1:\\n                Log.error(\\"This query, with variable {{var_name}} is too deep\\", var_name=c.name)\\n\\n        output.having = convert_list(self._convert_having, query.having)\\n\\n        return output\\n\\n    def _convert_from(self, frum):\\n        if isinstance(frum, text_type):\\n            return Data(name=frum)\\n        elif isinstance(frum, (Container, QueryOp)):\\n            return frum\\n        else:\\n            Log.error(\\"Expecting from clause to be a name, or a container\\")\\n\\n    def _convert_select(self, select):\\n        if isinstance(select, text_type):\\n            return Data(\\n                name=select.rstrip(\\".\\"),  # TRAILING DOT INDICATES THE VALUE, BUT IS INVALID FOR THE NAME\\n                value=select,\\n                aggregate=\\"none\\"\\n            )\\n        else:\\n            select = wrap(select)\\n            output = copy(select)\\n            if not select.value or isinstance(select.value, text_type):\\n                if select.value == \\".\\":\\n                    output.name = coalesce(select.name, select.aggregate)\\n                else:\\n                    output.name = coalesce(select.name, select.value, select.aggregate)\\n            elif not output.name:\\n                Log.error(\\"Must give name to each column in select clause\\")\\n\\n            if not output.name:\\n                Log.error(\\"expecting select to have a name: {{select}}\\",  select=select)\\n\\n            output.aggregate = coalesce(canonical_aggregates.get(select.aggregate), select.aggregate, \\"none\\")\\n            return output\\n\\n    def _convert_edge(self, edge):\\n        if isinstance(edge, text_type):\\n            return Data(\\n                name=edge,\\n                value=edge,\\n                domain=self._convert_domain()\\n            )\\n        else:\\n            edge = wrap(edge)\\n            if not edge.name and not isinstance(edge.value, text_type):\\n                Log.error(\\"You must name compound edges: {{edge}}\\",  edge= edge)\\n\\n            if isinstance(edge.value, (Mapping, list)) and not edge.domain:\\n                # COMPLEX EDGE IS SHORT HAND\\n                domain =self._convert_domain()\\n                domain.dimension = Data(fields=edge.value)\\n\\n                return Data(\\n                    name=edge.name,\\n                    allowNulls=False if edge.allowNulls is False else True,\\n                    domain=domain\\n                )\\n\\n            domain = self._convert_domain(edge.domain)\\n            return Data(\\n                name=coalesce(edge.name, edge.value),\\n                value=edge.value,\\n                range=edge.range,\\n                allowNulls=False if edge.allowNulls is False else True,\\n                domain=domain\\n            )\\n\\n    def _convert_group(self, column):\\n        if isinstance(column, text_type):\\n            return wrap({\\n                \\"name\\": column,\\n                \\"value\\": column,\\n                \\"domain\\": {\\"type\\": \\"default\\"}\\n          })\\n        else:\\n            column = wrap(column)\\n            if (column.domain and column.domain.type != \\"default\\") or column.allowNulls != None:\\n                Log.error(\\"groupby does not accept complicated domains\\")\\n\\n            if not column.name and not isinstance(column.value, text_type):\\n                Log.error(\\"You must name compound edges: {{edge}}\\",  edge= column)\\n\\n            return wrap({\\n                \\"name\\": coalesce(column.name, column.value),\\n                \\"value\\": column.value,\\n                \\"domain\\": {\\"type\\": \\"default\\"}\\n          })\\n\\n\\n    def _convert_domain(self, domain=None):\\n        if not domain:\\n            return Domain(type=\\"default\\")\\n        elif isinstance(domain, Dimension):\\n            return domain.getDomain()\\n        elif isinstance(domain, Domain):\\n            return domain\\n\\n        if not domain.name:\\n            domain = domain.copy()\\n            domain.name = domain.type\\n\\n        if not isinstance(domain.partitions, list):\\n            domain.partitions = list(domain.partitions)\\n\\n        return Domain(**domain)\\n\\n    def _convert_range(self, range):\\n        if range == None:\\n            return None\\n\\n        return Data(\\n            min=range.min,\\n            max=range.max\\n        )\\n\\n    def _convert_where(self, where):\\n        if where == None:\\n            return TRUE\\n        return where\\n\\n\\n    def _convert_window(self, window):\\n        return Data(\\n            name=coalesce(window.name, window.value),\\n            value=window.value,\\n            edges=[self._convert_edge(e) for e in listwrap(window.edges)],\\n            sort=self._convert_sort(window.sort),\\n            aggregate=window.aggregate,\\n            range=self._convert_range(window.range),\\n            where=self._convert_where(window.where)\\n        )\\n\\n\\n    def _convert_sort(self, sort):\\n        return normalize_sort(sort)\\n\\n\\ndef normalize_sort(sort=None):\\n    \\"\\"\\"\\n    CONVERT SORT PARAMETERS TO A NORMAL FORM SO EASIER TO USE\\n    \\"\\"\\"\\n\\n    if not sort:\\n        return Null\\n\\n    output = FlatList()\\n    for s in listwrap(sort):\\n        if isinstance(s, text_type) or Math.is_integer(s):\\n            output.append({\\"value\\": s, \\"sort\\": 1})\\n        elif not s.field and not s.value and s.sort==None:\\n            #ASSUME {name: sort} FORM\\n            for n, v in s.items():\\n                output.append({\\"value\\": n, \\"sort\\": sort_direction[v]})\\n        else:\\n            output.append({\\"value\\": coalesce(s.field, s.value), \\"sort\\": coalesce(sort_direction[s.sort], 1)})\\n    return wrap(output)\\n\\n\\nsort_direction = {\\n    \\"asc\\": 1,\\n    \\"desc\\": -1,\\n    \\"none\\": 0,\\n    1: 1,\\n    0: 0,\\n    -1: -1,\\n    None: 1,\\n    Null: 1\\n}\\n\\ncanonical_aggregates = {\\n    \\"none\\": \\"none\\",\\n    \\"one\\": \\"one\\",\\n    \\"count\\": \\"count\\",\\n    \\"sum\\": \\"sum\\",\\n    \\"add\\": \\"sum\\",\\n    \\"mean\\": \\"average\\",\\n    \\"average\\": \\"average\\",\\n    \\"avg\\": \\"average\\",\\n    \\"min\\": \\"minimum\\",\\n    \\"minimum\\": \\"minimum\\",\\n    \\"max\\": \\"maximum\\",\\n    \\"maximum\\": \\"minimum\\",\\n    \\"X2\\": \\"sum_of_squares\\",\\n    \\"std\\": \\"std\\",\\n    \\"stddev\\": \\"std\\",\\n    \\"std_deviation\\": \\"std\\",\\n    \\"var\\": \\"variance\\",\\n    \\"variance\\": \\"variance\\",\\n    \\"stats\\": \\"stats\\"\\n}\\n\\n" }\n'
line: b'{ "repo_name": "joachimmetz/dfvfs", "ref": "refs/heads/main", "path": "tests/vfs/xfs_file_system.py", "content": "#!/usr/bin/env python\\n# -*- coding: utf-8 -*-\\n\\"\\"\\"Tests for the file system implementation using pyfsxfs.\\"\\"\\"\\n\\nimport unittest\\n\\nfrom dfvfs.lib import definitions\\nfrom dfvfs.path import factory as path_spec_factory\\nfrom dfvfs.resolver import context\\nfrom dfvfs.vfs import xfs_file_system\\n\\nfrom tests import test_lib as shared_test_lib\\n\\n\\nclass XFSFileSystemTest(shared_test_lib.BaseTestCase):\\n  \\"\\"\\"Tests the XFS file entry.\\"\\"\\"\\n\\n  _INODE_PASSWORD_TXT = 11077\\n\\n  def setUp(self):\\n    \\"\\"\\"Sets up the needed objects used throughout the test.\\"\\"\\"\\n    self._resolver_context = context.Context()\\n    test_path = self._GetTestFilePath([\'xfs.raw\'])\\n    self._SkipIfPathNotExists(test_path)\\n\\n    test_os_path_spec = path_spec_factory.Factory.NewPathSpec(\\n        definitions.TYPE_INDICATOR_OS, location=test_path)\\n    self._raw_path_spec = path_spec_factory.Factory.NewPathSpec(\\n        definitions.TYPE_INDICATOR_RAW, parent=test_os_path_spec)\\n    self._xfs_path_spec = path_spec_factory.Factory.NewPathSpec(\\n        definitions.TYPE_INDICATOR_XFS, location=\'/\',\\n        parent=self._raw_path_spec)\\n\\n  def tearDown(self):\\n    \\"\\"\\"Cleans up the needed objects used throughout the test.\\"\\"\\"\\n    self._resolver_context.Empty()\\n\\n  def testOpenAndClose(self):\\n    \\"\\"\\"Test the open and close functionality.\\"\\"\\"\\n    file_system = xfs_file_system.XFSFileSystem(\\n        self._resolver_context, self._xfs_path_spec)\\n    self.assertIsNotNone(file_system)\\n\\n    file_system.Open()\\n\\n  def testFileEntryExistsByPathSpec(self):\\n    \\"\\"\\"Test the file entry exists by path specification functionality.\\"\\"\\"\\n    file_system = xfs_file_system.XFSFileSystem(\\n        self._resolver_context, self._xfs_path_spec)\\n    self.assertIsNotNone(file_system)\\n\\n    file_system.Open()\\n\\n    path_spec = path_spec_factory.Factory.NewPathSpec(\\n        definitions.TYPE_INDICATOR_XFS, location=\'/passwords.txt\',\\n        inode=self._INODE_PASSWORD_TXT, parent=self._raw_path_spec)\\n    self.assertTrue(file_system.FileEntryExistsByPathSpec(path_spec))\\n\\n    path_spec = path_spec_factory.Factory.NewPathSpec(\\n        definitions.TYPE_INDICATOR_XFS, location=\'/bogus.txt\',\\n        parent=self._raw_path_spec)\\n    self.assertFalse(file_system.FileEntryExistsByPathSpec(path_spec))\\n\\n  def testGetFileEntryByPathSpec(self):\\n    \\"\\"\\"Tests the GetFileEntryByPathSpec function.\\"\\"\\"\\n    file_system = xfs_file_system.XFSFileSystem(\\n        self._resolver_context, self._xfs_path_spec)\\n    self.assertIsNotNone(file_system)\\n\\n    file_system.Open()\\n\\n    path_spec = path_spec_factory.Factory.NewPathSpec(\\n        definitions.TYPE_INDICATOR_XFS, inode=self._INODE_PASSWORD_TXT,\\n        parent=self._raw_path_spec)\\n\\n    file_entry = file_system.GetFileEntryByPathSpec(path_spec)\\n\\n    self.assertIsNotNone(file_entry)\\n    # There is no way to determine the file_entry.name without a location string\\n    # in the path_spec or retrieving the file_entry from its parent.\\n\\n    path_spec = path_spec_factory.Factory.NewPathSpec(\\n        definitions.TYPE_INDICATOR_XFS, location=\'/passwords.txt\',\\n        inode=self._INODE_PASSWORD_TXT, parent=self._raw_path_spec)\\n    file_entry = file_system.GetFileEntryByPathSpec(path_spec)\\n\\n    self.assertIsNotNone(file_entry)\\n    self.assertEqual(file_entry.name, \'passwords.txt\')\\n\\n    path_spec = path_spec_factory.Factory.NewPathSpec(\\n        definitions.TYPE_INDICATOR_XFS, location=\'/bogus.txt\',\\n        parent=self._raw_path_spec)\\n    file_entry = file_system.GetFileEntryByPathSpec(path_spec)\\n\\n    self.assertIsNone(file_entry)\\n\\n  # TODO: add tests for GetXFSFileEntryByPathSpec function.\\n\\n  def testGetRootFileEntry(self):\\n    \\"\\"\\"Test the get root file entry functionality.\\"\\"\\"\\n    file_system = xfs_file_system.XFSFileSystem(\\n        self._resolver_context, self._xfs_path_spec)\\n    self.assertIsNotNone(file_system)\\n\\n    file_system.Open()\\n\\n    file_entry = file_system.GetRootFileEntry()\\n\\n    self.assertIsNotNone(file_entry)\\n    self.assertEqual(file_entry.name, \'\')\\n\\n\\nif __name__ == \'__main__\':\\n  unittest.main()\\n" }\n'
line: b'{ "repo_name": "deathmetalland/IkaLog", "ref": "refs/heads/youtube_sample", "path": "ikalog/ui/panel/last_result.py", "content": "#!/usr/bin/env python3\\n# -*- coding: utf-8 -*-\\n#\\n#  IkaLog\\n#  ======\\n#  Copyright (C) 2015 Takeshi HASEGAWA\\n#\\n#  Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n#  you may not use this file except in compliance with the License.\\n#  You may obtain a copy of the License at\\n#\\n#      http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#  Unless required by applicable law or agreed to in writing, software\\n#  distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n#  See the License for the specific language governing permissions and\\n#  limitations under the License.\\n#\\n\\nimport wx\\nimport cv2\\n\\nfrom ikalog.utils import *\\n\\n_ = Localization.gettext_translation(\'IkaUI\', fallback=True).gettext\\n\\nclass ResultsGUI(object):\\n    def __init__(self, ikalog_gui):\\n        self.ikalog_gui = ikalog_gui\\n        self.frame = None\\n        self.result_image = None\\n        self.size = (640, 360)\\n        self._init_frame()\\n\\n    def _init_frame(self):\\n        if self.frame:\\n            return\\n\\n        self.frame = wx.Frame(\\n            self.ikalog_gui.frame, wx.ID_ANY, _(\\"Last Result\\"), size=self.size)\\n        self.draw_image()\\n\\n    def show(self):\\n        if not self.frame:\\n            self._init_frame()\\n        self.frame.Show()\\n        self.frame.Raise()\\n\\n    def draw_image(self):\\n        if not self.result_image or not self.frame:\\n            return\\n        wx.StaticBitmap(self.frame, wx.ID_ANY, self.result_image,\\n                        (0, 0), self.size)\\n\\n    def on_game_individual_result(self, context):\\n        # FIXME\\n        return\\n\\n        cv_frame = cv2.resize(context[\'engine\'][\'frame\'], self.size)\\n        img_frame_rgb = cv2.cvtColor(cv_frame, cv2.COLOR_BGR2RGB)\\n        height, width = img_frame_rgb.shape[0:2]\\n\\n        self.result_image = wx.BitmapFromBuffer(width, height, img_frame_rgb)\\n        self.draw_image()\\n" } \n'
line: b'{ "repo_name": "log2timeline/dfvfs", "ref": "refs/heads/main", "path": "tests/vfs/xfs_file_system.py", "content": "#!/usr/bin/env python\\n# -*- coding: utf-8 -*-\\n\\"\\"\\"Tests for the file system implementation using pyfsxfs.\\"\\"\\"\\n\\nimport unittest\\n\\nfrom dfvfs.lib import definitions\\nfrom dfvfs.path import factory as path_spec_factory\\nfrom dfvfs.resolver import context\\nfrom dfvfs.vfs import xfs_file_system\\n\\nfrom tests import test_lib as shared_test_lib\\n\\n\\nclass XFSFileSystemTest(shared_test_lib.BaseTestCase):\\n  \\"\\"\\"Tests the XFS file entry.\\"\\"\\"\\n\\n  _INODE_PASSWORD_TXT = 11077\\n\\n  def setUp(self):\\n    \\"\\"\\"Sets up the needed objects used throughout the test.\\"\\"\\"\\n    self._resolver_context = context.Context()\\n    test_path = self._GetTestFilePath([\'xfs.raw\'])\\n    self._SkipIfPathNotExists(test_path)\\n\\n    test_os_path_spec = path_spec_factory.Factory.NewPathSpec(\\n        definitions.TYPE_INDICATOR_OS, location=test_path)\\n    self._raw_path_spec = path_spec_factory.Factory.NewPathSpec(\\n        definitions.TYPE_INDICATOR_RAW, parent=test_os_path_spec)\\n    self._xfs_path_spec = path_spec_factory.Factory.NewPathSpec(\\n        definitions.TYPE_INDICATOR_XFS, location=\'/\',\\n        parent=self._raw_path_spec)\\n\\n  def tearDown(self):\\n    \\"\\"\\"Cleans up the needed objects used throughout the test.\\"\\"\\"\\n    self._resolver_context.Empty()\\n\\n  def testOpenAndClose(self):\\n    \\"\\"\\"Test the open and close functionality.\\"\\"\\"\\n    file_system = xfs_file_system.XFSFileSystem(\\n        self._resolver_context, self._xfs_path_spec)\\n    self.assertIsNotNone(file_system)\\n\\n    file_system.Open()\\n\\n  def testFileEntryExistsByPathSpec(self):\\n    \\"\\"\\"Test the file entry exists by path specification functionality.\\"\\"\\"\\n    file_system = xfs_file_system.XFSFileSystem(\\n        self._resolver_context, self._xfs_path_spec)\\n    self.assertIsNotNone(file_system)\\n\\n    file_system.Open()\\n\\n    path_spec = path_spec_factory.Factory.NewPathSpec(\\n        definitions.TYPE_INDICATOR_XFS, location=\'/passwords.txt\',\\n        inode=self._INODE_PASSWORD_TXT, parent=self._raw_path_spec)\\n    self.assertTrue(file_system.FileEntryExistsByPathSpec(path_spec))\\n\\n    path_spec = path_spec_factory.Factory.NewPathSpec(\\n        definitions.TYPE_INDICATOR_XFS, location=\'/bogus.txt\',\\n        parent=self._raw_path_spec)\\n    self.assertFalse(file_system.FileEntryExistsByPathSpec(path_spec))\\n\\n  def testGetFileEntryByPathSpec(self):\\n    \\"\\"\\"Tests the GetFileEntryByPathSpec function.\\"\\"\\"\\n    file_system = xfs_file_system.XFSFileSystem(\\n        self._resolver_context, self._xfs_path_spec)\\n    self.assertIsNotNone(file_system)\\n\\n    file_system.Open()\\n\\n    path_spec = path_spec_factory.Factory.NewPathSpec(\\n        definitions.TYPE_INDICATOR_XFS, inode=self._INODE_PASSWORD_TXT,\\n        parent=self._raw_path_spec)\\n\\n    file_entry = file_system.GetFileEntryByPathSpec(path_spec)\\n\\n    self.assertIsNotNone(file_entry)\\n    # There is no way to determine the file_entry.name without a location string\\n    # in the path_spec or retrieving the file_entry from its parent.\\n\\n    path_spec = path_spec_factory.Factory.NewPathSpec(\\n        definitions.TYPE_INDICATOR_XFS, location=\'/passwords.txt\',\\n        inode=self._INODE_PASSWORD_TXT, parent=self._raw_path_spec)\\n    file_entry = file_system.GetFileEntryByPathSpec(path_spec)\\n\\n    self.assertIsNotNone(file_entry)\\n    self.assertEqual(file_entry.name, \'passwords.txt\')\\n\\n    path_spec = path_spec_factory.Factory.NewPathSpec(\\n        definitions.TYPE_INDICATOR_XFS, location=\'/bogus.txt\',\\n        parent=self._raw_path_spec)\\n    file_entry = file_system.GetFileEntryByPathSpec(path_spec)\\n\\n    self.assertIsNone(file_entry)\\n\\n  # TODO: add tests for GetXFSFileEntryByPathSpec function.\\n\\n  def testGetRootFileEntry(self):\\n    \\"\\"\\"Test the get root file entry functionality.\\"\\"\\"\\n    file_system = xfs_file_system.XFSFileSystem(\\n        self._resolver_context, self._xfs_path_spec)\\n    self.assertIsNotNone(file_system)\\n\\n    file_system.Open()\\n\\n    file_entry = file_system.GetRootFileEntry()\\n\\n    self.assertIsNotNone(file_entry)\\n    self.assertEqual(file_entry.name, \'\')\\n\\n\\nif __name__ == \'__main__\':\\n  unittest.main()\\n" } \n'
line: b'{ "repo_name": "vishl/ctags", "ref": "refs/heads/deploy", "path": "Test/bug1856363.py", "content": "#!/usr/bin/python\\n\\ndef main():\\n\\t# A broken ctags will see a function \\"initely_not_a_function\\" here.\\n\\tdefinitely_not_a_function = 0\\n\\treturn\\n\\nif __name__ == \'main\':\\n\\tmain()\\n" }\n'
line: b'{ "repo_name": "fabioz/ctags", "ref": "refs/heads/deploy", "path": "Test/bug1856363.py", "content": "#!/usr/bin/python\\n\\ndef main():\\n\\t# A broken ctags will see a function \\"initely_not_a_function\\" here.\\n\\tdefinitely_not_a_function = 0\\n\\treturn\\n\\nif __name__ == \'main\':\\n\\tmain()\\n" }\n'
line: b'{ "repo_name": "gangadhar-kadam/verve_erp", "ref": "refs/heads/v5.0", "path": "erpnext/stock/doctype/landed_cost_item/landed_cost_item.py", "content": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\\n# License: GNU General Public License v3. See license.txt\\n\\nfrom __future__ import unicode_literals\\nimport frappe\\n\\nfrom frappe.model.document import Document\\n\\nclass LandedCostItem(Document):\\n\\tpass" }\n'
line: b'{ "repo_name": "mawww/ctags", "ref": "refs/heads/deploy", "path": "Test/bug1856363.py", "content": "#!/usr/bin/python\\n\\ndef main():\\n\\t# A broken ctags will see a function \\"initely_not_a_function\\" here.\\n\\tdefinitely_not_a_function = 0\\n\\treturn\\n\\nif __name__ == \'main\':\\n\\tmain()\\n" }\n'
line: b'{ "repo_name": "pradyunsg/pip", "ref": "refs/heads/revamp-ci-apr-2021-v2", "path": "src/pip/_vendor/cachecontrol/adapter.py", "content": "import types\\nimport functools\\nimport zlib\\n\\nfrom pip._vendor.requests.adapters import HTTPAdapter\\n\\nfrom .controller import CacheController\\nfrom .cache import DictCache\\nfrom .filewrapper import CallbackFileWrapper\\n\\n\\nclass CacheControlAdapter(HTTPAdapter):\\n    invalidating_methods = {\\"PUT\\", \\"DELETE\\"}\\n\\n    def __init__(\\n        self,\\n        cache=None,\\n        cache_etags=True,\\n        controller_class=None,\\n        serializer=None,\\n        heuristic=None,\\n        cacheable_methods=None,\\n        *args,\\n        **kw\\n    ):\\n        super(CacheControlAdapter, self).__init__(*args, **kw)\\n        self.cache = DictCache() if cache is None else cache\\n        self.heuristic = heuristic\\n        self.cacheable_methods = cacheable_methods or (\\"GET\\",)\\n\\n        controller_factory = controller_class or CacheController\\n        self.controller = controller_factory(\\n            self.cache, cache_etags=cache_etags, serializer=serializer\\n        )\\n\\n    def send(self, request, cacheable_methods=None, **kw):\\n        \\"\\"\\"\\n        Send a request. Use the request information to see if it\\n        exists in the cache and cache the response if we need to and can.\\n        \\"\\"\\"\\n        cacheable = cacheable_methods or self.cacheable_methods\\n        if request.method in cacheable:\\n            try:\\n                cached_response = self.controller.cached_request(request)\\n            except zlib.error:\\n                cached_response = None\\n            if cached_response:\\n                return self.build_response(request, cached_response, from_cache=True)\\n\\n            # check for etags and add headers if appropriate\\n            request.headers.update(self.controller.conditional_headers(request))\\n\\n        resp = super(CacheControlAdapter, self).send(request, **kw)\\n\\n        return resp\\n\\n    def build_response(\\n        self, request, response, from_cache=False, cacheable_methods=None\\n    ):\\n        \\"\\"\\"\\n        Build a response by making a request or using the cache.\\n\\n        This will end up calling send and returning a potentially\\n        cached response\\n        \\"\\"\\"\\n        cacheable = cacheable_methods or self.cacheable_methods\\n        if not from_cache and request.method in cacheable:\\n            # Check for any heuristics that might update headers\\n            # before trying to cache.\\n            if self.heuristic:\\n                response = self.heuristic.apply(response)\\n\\n            # apply any expiration heuristics\\n            if response.status == 304:\\n                # We must have sent an ETag request. This could mean\\n                # that we\'ve been expired already or that we simply\\n                # have an etag. In either case, we want to try and\\n                # update the cache if that is the case.\\n                cached_response = self.controller.update_cached_response(\\n                    request, response\\n                )\\n\\n                if cached_response is not response:\\n                    from_cache = True\\n\\n                # We are done with the server response, read a\\n                # possible response body (compliant servers will\\n                # not return one, but we cannot be 100% sure) and\\n                # release the connection back to the pool.\\n                response.read(decode_content=False)\\n                response.release_conn()\\n\\n                response = cached_response\\n\\n            # We always cache the 301 responses\\n            elif response.status == 301:\\n                self.controller.cache_response(request, response)\\n            else:\\n                # Wrap the response file with a wrapper that will cache the\\n                #   response when the stream has been consumed.\\n                response._fp = CallbackFileWrapper(\\n                    response._fp,\\n                    functools.partial(\\n                        self.controller.cache_response, request, response\\n                    ),\\n                )\\n                if response.chunked:\\n                    super_update_chunk_length = response._update_chunk_length\\n\\n                    def _update_chunk_length(self):\\n                        super_update_chunk_length()\\n                        if self.chunk_left == 0:\\n                            self._fp._close()\\n\\n                    response._update_chunk_length = types.MethodType(\\n                        _update_chunk_length, response\\n                    )\\n\\n        resp = super(CacheControlAdapter, self).build_response(request, response)\\n\\n        # See if we should invalidate the cache.\\n        if request.method in self.invalidating_methods and resp.ok:\\n            cache_url = self.controller.cache_url(request.url)\\n            self.cache.delete(cache_url)\\n\\n        # Give the request a from_cache attr to let people use it\\n        resp.from_cache = from_cache\\n\\n        return resp\\n\\n    def close(self):\\n        self.cache.close()\\n        super(CacheControlAdapter, self).close()\\n" }\n'
line: b'{ "repo_name": "RusticiSoftware/TinCanPython", "ref": "refs/heads/3.x", "path": "tincan/substatement.py", "content": "# Copyright 2014 Rustici Software\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n#    you may not use this file except in compliance with the License.\\n#    You may obtain a copy of the License at\\n#\\n#    http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n#    See the License for the specific language governing permissions and\\n#    limitations under the License.\\n\\nfrom tincan.statement_base import StatementBase\\nfrom tincan.agent import Agent\\nfrom tincan.group import Group\\nfrom tincan.activity import Activity\\n\\n\\nclass SubStatement(StatementBase):\\n    _props_req = [\\n        \'object_type\'\\n    ]\\n\\n    _props = []\\n\\n    _props.extend(StatementBase._props)\\n    _props.extend(_props_req)\\n\\n    def __init__(self, *args, **kwargs):\\n        self._object_type = None\\n\\n        super(SubStatement, self).__init__(*args, **kwargs)\\n\\n    @property\\n    def object(self):\\n        \\"\\"\\"Object for SubStatement\\n\\n        :setter: Setter for object\\n        :setter type: :class:`tincan.Agent` | :class:`tincan.Group` | :class:`tincan.Activity`\\n        :rtype: :class:`tincan.Agent` | :class:`tincan.Group` | :class:`tincan.Activity`\\n\\n        \\"\\"\\"\\n        return self._object\\n\\n    @object.setter\\n    def object(self, value):\\n        if value is not None and \\\\\\n                not isinstance(value, Agent) and \\\\\\n                not isinstance(value, Group) and \\\\\\n                not isinstance(value, Activity):\\n            if isinstance(value, dict):\\n                if \'object_type\' in value or \'objectType\' in value:\\n                    if \'objectType\' in value:\\n                        value[\'object_type\'] = value[\'objectType\']\\n                        value.pop(\'objectType\')\\n                    if value[\'object_type\'] == \'Agent\':\\n                        value = Agent(value)\\n                    elif value[\'object_type\'] == \'Activity\':\\n                        value = Activity(value)\\n                    elif value[\'object_type\'] == \'Group\':\\n                        value = Group(value)\\n                    else:\\n                        value = Activity(value)\\n                else:\\n                    value = Activity(value)\\n        self._object = value\\n\\n    @object.deleter\\n    def object(self):\\n        del self._object\\n\\n    @property\\n    def object_type(self):\\n        \\"\\"\\"Object Type for SubStatement. Will always be \\"SubStatement\\"\\n\\n        :setter: Tries to convert to unicode\\n        :setter type: unicode\\n        :rtype: unicode\\n\\n        \\"\\"\\"\\n        return self._object_type\\n\\n    @object_type.setter\\n    def object_type(self, _):\\n        self._object_type = \'SubStatement\'\\n" }\n'
line: b'{ "repo_name": "brain-tec/server-tools", "ref": "refs/heads/11.0", "path": "fetchmail_incoming_log/models/__init__.py", "content": "from . import mail_thread\\n" }\n'
line: b'{ "repo_name": "yamaya/ctags", "ref": "refs/heads/deploy", "path": "Test/bug1856363.py", "content": "#!/usr/bin/python\\n\\ndef main():\\n\\t# A broken ctags will see a function \\"initely_not_a_function\\" here.\\n\\tdefinitely_not_a_function = 0\\n\\treturn\\n\\nif __name__ == \'main\':\\n\\tmain()\\n" }\n'
line: b'{ "repo_name": "peterdocter/ctags", "ref": "refs/heads/deploy_for_android", "path": "Test/bug1856363.py", "content": "#!/usr/bin/python\\n\\ndef main():\\n\\t# A broken ctags will see a function \\"initely_not_a_function\\" here.\\n\\tdefinitely_not_a_function = 0\\n\\treturn\\n\\nif __name__ == \'main\':\\n\\tmain()\\n" }\n'
line: b'{ "repo_name": "pypa/pip", "ref": "refs/heads/main", "path": "src/pip/_vendor/cachecontrol/adapter.py", "content": "import types\\nimport functools\\nimport zlib\\n\\nfrom pip._vendor.requests.adapters import HTTPAdapter\\n\\nfrom .controller import CacheController\\nfrom .cache import DictCache\\nfrom .filewrapper import CallbackFileWrapper\\n\\n\\nclass CacheControlAdapter(HTTPAdapter):\\n    invalidating_methods = {\\"PUT\\", \\"DELETE\\"}\\n\\n    def __init__(\\n        self,\\n        cache=None,\\n        cache_etags=True,\\n        controller_class=None,\\n        serializer=None,\\n        heuristic=None,\\n        cacheable_methods=None,\\n        *args,\\n        **kw\\n    ):\\n        super(CacheControlAdapter, self).__init__(*args, **kw)\\n        self.cache = DictCache() if cache is None else cache\\n        self.heuristic = heuristic\\n        self.cacheable_methods = cacheable_methods or (\\"GET\\",)\\n\\n        controller_factory = controller_class or CacheController\\n        self.controller = controller_factory(\\n            self.cache, cache_etags=cache_etags, serializer=serializer\\n        )\\n\\n    def send(self, request, cacheable_methods=None, **kw):\\n        \\"\\"\\"\\n        Send a request. Use the request information to see if it\\n        exists in the cache and cache the response if we need to and can.\\n        \\"\\"\\"\\n        cacheable = cacheable_methods or self.cacheable_methods\\n        if request.method in cacheable:\\n            try:\\n                cached_response = self.controller.cached_request(request)\\n            except zlib.error:\\n                cached_response = None\\n            if cached_response:\\n                return self.build_response(request, cached_response, from_cache=True)\\n\\n            # check for etags and add headers if appropriate\\n            request.headers.update(self.controller.conditional_headers(request))\\n\\n        resp = super(CacheControlAdapter, self).send(request, **kw)\\n\\n        return resp\\n\\n    def build_response(\\n        self, request, response, from_cache=False, cacheable_methods=None\\n    ):\\n        \\"\\"\\"\\n        Build a response by making a request or using the cache.\\n\\n        This will end up calling send and returning a potentially\\n        cached response\\n        \\"\\"\\"\\n        cacheable = cacheable_methods or self.cacheable_methods\\n        if not from_cache and request.method in cacheable:\\n            # Check for any heuristics that might update headers\\n            # before trying to cache.\\n            if self.heuristic:\\n                response = self.heuristic.apply(response)\\n\\n            # apply any expiration heuristics\\n            if response.status == 304:\\n                # We must have sent an ETag request. This could mean\\n                # that we\'ve been expired already or that we simply\\n                # have an etag. In either case, we want to try and\\n                # update the cache if that is the case.\\n                cached_response = self.controller.update_cached_response(\\n                    request, response\\n                )\\n\\n                if cached_response is not response:\\n                    from_cache = True\\n\\n                # We are done with the server response, read a\\n                # possible response body (compliant servers will\\n                # not return one, but we cannot be 100% sure) and\\n                # release the connection back to the pool.\\n                response.read(decode_content=False)\\n                response.release_conn()\\n\\n                response = cached_response\\n\\n            # We always cache the 301 responses\\n            elif response.status == 301:\\n                self.controller.cache_response(request, response)\\n            else:\\n                # Wrap the response file with a wrapper that will cache the\\n                #   response when the stream has been consumed.\\n                response._fp = CallbackFileWrapper(\\n                    response._fp,\\n                    functools.partial(\\n                        self.controller.cache_response, request, response\\n                    ),\\n                )\\n                if response.chunked:\\n                    super_update_chunk_length = response._update_chunk_length\\n\\n                    def _update_chunk_length(self):\\n                        super_update_chunk_length()\\n                        if self.chunk_left == 0:\\n                            self._fp._close()\\n\\n                    response._update_chunk_length = types.MethodType(\\n                        _update_chunk_length, response\\n                    )\\n\\n        resp = super(CacheControlAdapter, self).build_response(request, response)\\n\\n        # See if we should invalidate the cache.\\n        if request.method in self.invalidating_methods and resp.ok:\\n            cache_url = self.controller.cache_url(request.url)\\n            self.cache.delete(cache_url)\\n\\n        # Give the request a from_cache attr to let people use it\\n        resp.from_cache = from_cache\\n\\n        return resp\\n\\n    def close(self):\\n        self.cache.close()\\n        super(CacheControlAdapter, self).close()\\n" }\n'
line: b'{ "repo_name": "pantsbuild/pex", "ref": "refs/heads/main", "path": "pex/vendor/_vendored/pip/pip/_vendor/cachecontrol/adapter.py", "content": "import types\\nimport functools\\nimport zlib\\n\\nfrom pip._vendor.requests.adapters import HTTPAdapter\\n\\nfrom .controller import CacheController\\nfrom .cache import DictCache\\nfrom .filewrapper import CallbackFileWrapper\\n\\n\\nclass CacheControlAdapter(HTTPAdapter):\\n    invalidating_methods = {\\"PUT\\", \\"DELETE\\"}\\n\\n    def __init__(\\n        self,\\n        cache=None,\\n        cache_etags=True,\\n        controller_class=None,\\n        serializer=None,\\n        heuristic=None,\\n        cacheable_methods=None,\\n        *args,\\n        **kw\\n    ):\\n        super(CacheControlAdapter, self).__init__(*args, **kw)\\n        self.cache = DictCache() if cache is None else cache\\n        self.heuristic = heuristic\\n        self.cacheable_methods = cacheable_methods or (\\"GET\\",)\\n\\n        controller_factory = controller_class or CacheController\\n        self.controller = controller_factory(\\n            self.cache, cache_etags=cache_etags, serializer=serializer\\n        )\\n\\n    def send(self, request, cacheable_methods=None, **kw):\\n        \\"\\"\\"\\n        Send a request. Use the request information to see if it\\n        exists in the cache and cache the response if we need to and can.\\n        \\"\\"\\"\\n        cacheable = cacheable_methods or self.cacheable_methods\\n        if request.method in cacheable:\\n            try:\\n                cached_response = self.controller.cached_request(request)\\n            except zlib.error:\\n                cached_response = None\\n            if cached_response:\\n                return self.build_response(request, cached_response, from_cache=True)\\n\\n            # check for etags and add headers if appropriate\\n            request.headers.update(self.controller.conditional_headers(request))\\n\\n        resp = super(CacheControlAdapter, self).send(request, **kw)\\n\\n        return resp\\n\\n    def build_response(\\n        self, request, response, from_cache=False, cacheable_methods=None\\n    ):\\n        \\"\\"\\"\\n        Build a response by making a request or using the cache.\\n\\n        This will end up calling send and returning a potentially\\n        cached response\\n        \\"\\"\\"\\n        cacheable = cacheable_methods or self.cacheable_methods\\n        if not from_cache and request.method in cacheable:\\n            # Check for any heuristics that might update headers\\n            # before trying to cache.\\n            if self.heuristic:\\n                response = self.heuristic.apply(response)\\n\\n            # apply any expiration heuristics\\n            if response.status == 304:\\n                # We must have sent an ETag request. This could mean\\n                # that we\'ve been expired already or that we simply\\n                # have an etag. In either case, we want to try and\\n                # update the cache if that is the case.\\n                cached_response = self.controller.update_cached_response(\\n                    request, response\\n                )\\n\\n                if cached_response is not response:\\n                    from_cache = True\\n\\n                # We are done with the server response, read a\\n                # possible response body (compliant servers will\\n                # not return one, but we cannot be 100% sure) and\\n                # release the connection back to the pool.\\n                response.read(decode_content=False)\\n                response.release_conn()\\n\\n                response = cached_response\\n\\n            # We always cache the 301 responses\\n            elif response.status == 301:\\n                self.controller.cache_response(request, response)\\n            else:\\n                # Wrap the response file with a wrapper that will cache the\\n                #   response when the stream has been consumed.\\n                response._fp = CallbackFileWrapper(\\n                    response._fp,\\n                    functools.partial(\\n                        self.controller.cache_response, request, response\\n                    ),\\n                )\\n                if response.chunked:\\n                    super_update_chunk_length = response._update_chunk_length\\n\\n                    def _update_chunk_length(self):\\n                        super_update_chunk_length()\\n                        if self.chunk_left == 0:\\n                            self._fp._close()\\n\\n                    response._update_chunk_length = types.MethodType(\\n                        _update_chunk_length, response\\n                    )\\n\\n        resp = super(CacheControlAdapter, self).build_response(request, response)\\n\\n        # See if we should invalidate the cache.\\n        if request.method in self.invalidating_methods and resp.ok:\\n            cache_url = self.controller.cache_url(request.url)\\n            self.cache.delete(cache_url)\\n\\n        # Give the request a from_cache attr to let people use it\\n        resp.from_cache = from_cache\\n\\n        return resp\\n\\n    def close(self):\\n        self.cache.close()\\n        super(CacheControlAdapter, self).close()\\n" }\n'
line: b'{ "repo_name": "bjandre/ctags-fortran", "ref": "refs/heads/modern-fortran", "path": "Test/bug1856363.py", "content": "#!/usr/bin/python\\n\\ndef main():\\n\\t# A broken ctags will see a function \\"initely_not_a_function\\" here.\\n\\tdefinitely_not_a_function = 0\\n\\treturn\\n\\nif __name__ == \'main\':\\n\\tmain()\\n" }\n'
line: b'{ "repo_name": "pfmoore/pip", "ref": "refs/heads/main", "path": "src/pip/_vendor/cachecontrol/adapter.py", "content": "import types\\nimport functools\\nimport zlib\\n\\nfrom pip._vendor.requests.adapters import HTTPAdapter\\n\\nfrom .controller import CacheController\\nfrom .cache import DictCache\\nfrom .filewrapper import CallbackFileWrapper\\n\\n\\nclass CacheControlAdapter(HTTPAdapter):\\n    invalidating_methods = {\\"PUT\\", \\"DELETE\\"}\\n\\n    def __init__(\\n        self,\\n        cache=None,\\n        cache_etags=True,\\n        controller_class=None,\\n        serializer=None,\\n        heuristic=None,\\n        cacheable_methods=None,\\n        *args,\\n        **kw\\n    ):\\n        super(CacheControlAdapter, self).__init__(*args, **kw)\\n        self.cache = DictCache() if cache is None else cache\\n        self.heuristic = heuristic\\n        self.cacheable_methods = cacheable_methods or (\\"GET\\",)\\n\\n        controller_factory = controller_class or CacheController\\n        self.controller = controller_factory(\\n            self.cache, cache_etags=cache_etags, serializer=serializer\\n        )\\n\\n    def send(self, request, cacheable_methods=None, **kw):\\n        \\"\\"\\"\\n        Send a request. Use the request information to see if it\\n        exists in the cache and cache the response if we need to and can.\\n        \\"\\"\\"\\n        cacheable = cacheable_methods or self.cacheable_methods\\n        if request.method in cacheable:\\n            try:\\n                cached_response = self.controller.cached_request(request)\\n            except zlib.error:\\n                cached_response = None\\n            if cached_response:\\n                return self.build_response(request, cached_response, from_cache=True)\\n\\n            # check for etags and add headers if appropriate\\n            request.headers.update(self.controller.conditional_headers(request))\\n\\n        resp = super(CacheControlAdapter, self).send(request, **kw)\\n\\n        return resp\\n\\n    def build_response(\\n        self, request, response, from_cache=False, cacheable_methods=None\\n    ):\\n        \\"\\"\\"\\n        Build a response by making a request or using the cache.\\n\\n        This will end up calling send and returning a potentially\\n        cached response\\n        \\"\\"\\"\\n        cacheable = cacheable_methods or self.cacheable_methods\\n        if not from_cache and request.method in cacheable:\\n            # Check for any heuristics that might update headers\\n            # before trying to cache.\\n            if self.heuristic:\\n                response = self.heuristic.apply(response)\\n\\n            # apply any expiration heuristics\\n            if response.status == 304:\\n                # We must have sent an ETag request. This could mean\\n                # that we\'ve been expired already or that we simply\\n                # have an etag. In either case, we want to try and\\n                # update the cache if that is the case.\\n                cached_response = self.controller.update_cached_response(\\n                    request, response\\n                )\\n\\n                if cached_response is not response:\\n                    from_cache = True\\n\\n                # We are done with the server response, read a\\n                # possible response body (compliant servers will\\n                # not return one, but we cannot be 100% sure) and\\n                # release the connection back to the pool.\\n                response.read(decode_content=False)\\n                response.release_conn()\\n\\n                response = cached_response\\n\\n            # We always cache the 301 responses\\n            elif response.status == 301:\\n                self.controller.cache_response(request, response)\\n            else:\\n                # Wrap the response file with a wrapper that will cache the\\n                #   response when the stream has been consumed.\\n                response._fp = CallbackFileWrapper(\\n                    response._fp,\\n                    functools.partial(\\n                        self.controller.cache_response, request, response\\n                    ),\\n                )\\n                if response.chunked:\\n                    super_update_chunk_length = response._update_chunk_length\\n\\n                    def _update_chunk_length(self):\\n                        super_update_chunk_length()\\n                        if self.chunk_left == 0:\\n                            self._fp._close()\\n\\n                    response._update_chunk_length = types.MethodType(\\n                        _update_chunk_length, response\\n                    )\\n\\n        resp = super(CacheControlAdapter, self).build_response(request, response)\\n\\n        # See if we should invalidate the cache.\\n        if request.method in self.invalidating_methods and resp.ok:\\n            cache_url = self.controller.cache_url(request.url)\\n            self.cache.delete(cache_url)\\n\\n        # Give the request a from_cache attr to let people use it\\n        resp.from_cache = from_cache\\n\\n        return resp\\n\\n    def close(self):\\n        self.cache.close()\\n        super(CacheControlAdapter, self).close()\\n" }\n'
line: b'{ "repo_name": "grupozeety/CDerpnext", "ref": "refs/heads/bk_master", "path": "erpnext/stock/doctype/landed_cost_item/landed_cost_item.py", "content": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\\n# License: GNU General Public License v3. See license.txt\\n\\nfrom __future__ import unicode_literals\\nimport frappe\\n\\nfrom frappe.model.document import Document\\n\\nclass LandedCostItem(Document):\\n\\tpass" }\n'
line: b'{ "repo_name": "m4734/mysql_pio", "ref": "refs/heads/5.7", "path": "boost_1_59_0/tools/build/test/example_gettext.py", "content": "#!/usr/bin/python\\n\\n# Copyright (C) Vladimir Prus 2006.\\n# Distributed under the Boost Software License, Version 1.0. (See\\n# accompanying file LICENSE_1_0.txt or copy at\\n# http://www.boost.org/LICENSE_1_0.txt)\\n\\n# Test the \'gettext\' example.\\n\\nimport BoostBuild\\nimport os\\nimport string\\n\\nt = BoostBuild.Tester()\\n\\nt.set_tree(\\"../example/gettext\\")\\n\\nt.run_build_system(stderr=None)\\n\\nt.expect_addition([\\"bin/$toolset/debug/main.exe\\",\\n                   \\"bin/$toolset/debug/russian.mo\\"])\\n\\nfile = t.adjust_names([\\"bin/$toolset/debug/main.exe\\"])[0]\\n\\ninput_fd = os.popen(file)\\ninput = input_fd.read();\\n\\nt.fail_test(string.find(input, \\"international hello\\") != 0)\\n\\nt.cleanup()\\n" }\n'
line: b'{ "repo_name": "NicholasHoCode/Galaxy", "ref": "refs/heads/gh-pages", "path": "assets/code/MK.py", "content": "\xc2\xa9 Nicholas Ieng Kit Ho, 2016. All rights reserved. Cannot be copied, re-used, or edited\\n\\n# SAMPLE CODE\\n\\ndef main(k, T, filename):\\n\\n\\twith open(filename) as f:\\n\\t\\ttext = f.read()\\n\\n\\tkgrams = dict()\\n\\tcirc_text = text + text[:k]\\n\\tfor i in xrange(len(text)):\\n\\t\\tkgram = circ_text[i:i+k]\\n\\t\\tnext_char = circ_text[i+k]\\n\\t\\tif kgram in kgrams:\\n\\t\\t\\tkgrams[kgram].append(next_char)\\n\\t\\telse:\\n\\t\\t\\tkgrams[kgram] = [next_char]\\n\\n\\tcurrent = text[:k]\\n\\tsys.stdout.write(current)\\n\\tfor i in range(T-k):\\n\\t\\tnew = random.choice(kgrams[current])\\n\\t\\tsys.stdout.write(new)\\n\\t\\tcurrent = current[1:]+new\\n\\tprint \'\'\\n\\t\\nif __name__ == \\"__main__\\":\\n\\tk = sys.argv[1]\\n\\tT = sys.argv[2]\\n\\tfilename = sys.argv[3]\\n\\n\\tk = int(k)\\n\\tT = int(T)\\n\\t\\n\\tmain(k,T,filename)\\n\\n" }\n'
line: b'{ "repo_name": "Vauxoo/server-tools", "ref": "refs/heads/12.0", "path": "fetchmail_incoming_log/models/__init__.py", "content": "from . import mail_thread\\n" }\n'
line: b'{ "repo_name": "faceleg/ctags", "ref": "refs/heads/deploy", "path": "Test/bug1856363.py", "content": "#!/usr/bin/python\\n\\ndef main():\\n\\t# A broken ctags will see a function \\"initely_not_a_function\\" here.\\n\\tdefinitely_not_a_function = 0\\n\\treturn\\n\\nif __name__ == \'main\':\\n\\tmain()\\n" }\n'
line: b'{ "repo_name": "h3biomed/ansible", "ref": "refs/heads/h3", "path": "lib/ansible/modules/cloud/openstack/os_network.py", "content": "#!/usr/bin/python\\n\\n# Copyright (c) 2014 Hewlett-Packard Development Company, L.P.\\n# Copyright (c) 2013, Benno Joy <benno@ansible.com>\\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\\n\\nfrom __future__ import absolute_import, division, print_function\\n__metaclass__ = type\\n\\n\\nANSIBLE_METADATA = {\'metadata_version\': \'1.1\',\\n                    \'status\': [\'preview\'],\\n                    \'supported_by\': \'community\'}\\n\\n\\nDOCUMENTATION = \'\'\'\\n---\\nmodule: os_network\\nshort_description: Creates/removes networks from OpenStack\\nextends_documentation_fragment: openstack\\nversion_added: \\"2.0\\"\\nauthor: \\"Monty Taylor (@emonty)\\"\\ndescription:\\n   - Add or remove network from OpenStack.\\noptions:\\n   name:\\n     description:\\n        - Name to be assigned to the network.\\n     required: true\\n   shared:\\n     description:\\n        - Whether this network is shared or not.\\n     type: bool\\n     default: \'no\'\\n   admin_state_up:\\n     description:\\n        - Whether the state should be marked as up or down.\\n     type: bool\\n     default: \'yes\'\\n   external:\\n     description:\\n        - Whether this network is externally accessible.\\n     type: bool\\n     default: \'no\'\\n   state:\\n     description:\\n        - Indicate desired state of the resource.\\n     choices: [\'present\', \'absent\']\\n     default: present\\n   provider_physical_network:\\n     description:\\n        - The physical network where this network object is implemented.\\n     version_added: \\"2.1\\"\\n   provider_network_type:\\n     description:\\n        - The type of physical network that maps to this network resource.\\n     version_added: \\"2.1\\"\\n   provider_segmentation_id:\\n     description:\\n        - An isolated segment on the physical network. The I(network_type)\\n          attribute defines the segmentation model. For example, if the\\n          I(network_type) value is vlan, this ID is a vlan identifier. If\\n          the I(network_type) value is gre, this ID is a gre key.\\n     version_added: \\"2.1\\"\\n   project:\\n     description:\\n        - Project name or ID containing the network (name admin-only)\\n     version_added: \\"2.1\\"\\n   availability_zone:\\n     description:\\n       - Ignored. Present for backwards compatibility\\n   port_security_enabled:\\n     description:\\n        -  Whether port security is enabled on the network or not.\\n           Network will use OpenStack defaults if this option is\\n           not utilised.\\n     type: bool\\n     version_added: \\"2.8\\"\\nrequirements:\\n     - \\"openstacksdk\\"\\n\'\'\'\\n\\nEXAMPLES = \'\'\'\\n# Create an externally accessible network named \'ext_network\'.\\n- os_network:\\n    cloud: mycloud\\n    state: present\\n    name: ext_network\\n    external: true\\n\'\'\'\\n\\nRETURN = \'\'\'\\nnetwork:\\n    description: Dictionary describing the network.\\n    returned: On success when I(state) is \'present\'.\\n    type: complex\\n    contains:\\n        id:\\n            description: Network ID.\\n            type: str\\n            sample: \\"4bb4f9a5-3bd2-4562-bf6a-d17a6341bb56\\"\\n        name:\\n            description: Network name.\\n            type: str\\n            sample: \\"ext_network\\"\\n        shared:\\n            description: Indicates whether this network is shared across all tenants.\\n            type: bool\\n            sample: false\\n        status:\\n            description: Network status.\\n            type: str\\n            sample: \\"ACTIVE\\"\\n        mtu:\\n            description: The MTU of a network resource.\\n            type: int\\n            sample: 0\\n        admin_state_up:\\n            description: The administrative state of the network.\\n            type: bool\\n            sample: true\\n        port_security_enabled:\\n            description: The port security status\\n            type: bool\\n            sample: true\\n        router:external:\\n            description: Indicates whether this network is externally accessible.\\n            type: bool\\n            sample: true\\n        tenant_id:\\n            description: The tenant ID.\\n            type: str\\n            sample: \\"06820f94b9f54b119636be2728d216fc\\"\\n        subnets:\\n            description: The associated subnets.\\n            type: list\\n            sample: []\\n        \\"provider:physical_network\\":\\n            description: The physical network where this network object is implemented.\\n            type: str\\n            sample: my_vlan_net\\n        \\"provider:network_type\\":\\n            description: The type of physical network that maps to this network resource.\\n            type: str\\n            sample: vlan\\n        \\"provider:segmentation_id\\":\\n            description: An isolated segment on the physical network.\\n            type: str\\n            sample: 101\\n\'\'\'\\n\\nfrom ansible.module_utils.basic import AnsibleModule\\nfrom ansible.module_utils.openstack import openstack_full_argument_spec, openstack_module_kwargs, openstack_cloud_from_module\\n\\n\\ndef main():\\n    argument_spec = openstack_full_argument_spec(\\n        name=dict(required=True),\\n        shared=dict(default=False, type=\'bool\'),\\n        admin_state_up=dict(default=True, type=\'bool\'),\\n        external=dict(default=False, type=\'bool\'),\\n        provider_physical_network=dict(required=False),\\n        provider_network_type=dict(required=False),\\n        provider_segmentation_id=dict(required=False, type=\'int\'),\\n        state=dict(default=\'present\', choices=[\'absent\', \'present\']),\\n        project=dict(default=None),\\n        port_security_enabled=dict(type=\'bool\')\\n    )\\n\\n    module_kwargs = openstack_module_kwargs()\\n    module = AnsibleModule(argument_spec, **module_kwargs)\\n\\n    state = module.params[\'state\']\\n    name = module.params[\'name\']\\n    shared = module.params[\'shared\']\\n    admin_state_up = module.params[\'admin_state_up\']\\n    external = module.params[\'external\']\\n    provider_physical_network = module.params[\'provider_physical_network\']\\n    provider_network_type = module.params[\'provider_network_type\']\\n    provider_segmentation_id = module.params[\'provider_segmentation_id\']\\n    project = module.params.get(\'project\')\\n    port_security_enabled = module.params.get(\'port_security_enabled\')\\n\\n    sdk, cloud = openstack_cloud_from_module(module)\\n    try:\\n        if project is not None:\\n            proj = cloud.get_project(project)\\n            if proj is None:\\n                module.fail_json(msg=\'Project %s could not be found\' % project)\\n            project_id = proj[\'id\']\\n            filters = {\'tenant_id\': project_id}\\n        else:\\n            project_id = None\\n            filters = None\\n        net = cloud.get_network(name, filters=filters)\\n\\n        if state == \'present\':\\n            if not net:\\n                provider = {}\\n                if provider_physical_network:\\n                    provider[\'physical_network\'] = provider_physical_network\\n                if provider_network_type:\\n                    provider[\'network_type\'] = provider_network_type\\n                if provider_segmentation_id:\\n                    provider[\'segmentation_id\'] = provider_segmentation_id\\n\\n                if project_id is not None:\\n                    net = cloud.create_network(name, shared, admin_state_up,\\n                                               external, provider, project_id,\\n                                               port_security_enabled=port_security_enabled)\\n                else:\\n                    net = cloud.create_network(name, shared, admin_state_up,\\n                                               external, provider,\\n                                               port_security_enabled=port_security_enabled)\\n                changed = True\\n            else:\\n                changed = False\\n            module.exit_json(changed=changed, network=net, id=net[\'id\'])\\n\\n        elif state == \'absent\':\\n            if not net:\\n                module.exit_json(changed=False)\\n            else:\\n                cloud.delete_network(name)\\n                module.exit_json(changed=True)\\n\\n    except sdk.exceptions.OpenStackCloudException as e:\\n        module.fail_json(msg=str(e))\\n\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n" }\n'
line: b'{ "repo_name": "sinojelly/ctags", "ref": "refs/heads/deploy_for_android", "path": "Test/bug1856363.py", "content": "#!/usr/bin/python\\n\\ndef main():\\n\\t# A broken ctags will see a function \\"initely_not_a_function\\" here.\\n\\tdefinitely_not_a_function = 0\\n\\treturn\\n\\nif __name__ == \'main\':\\n\\tmain()\\n" }\n'
line: b'{ "repo_name": "gknops/ctags", "ref": "refs/heads/deploy", "path": "Test/bug1856363.py", "content": "#!/usr/bin/python\\n\\ndef main():\\n\\t# A broken ctags will see a function \\"initely_not_a_function\\" here.\\n\\tdefinitely_not_a_function = 0\\n\\treturn\\n\\nif __name__ == \'main\':\\n\\tmain()\\n" }\n'
line: b'{ "repo_name": "talha131/ctags", "ref": "refs/heads/deploy", "path": "Test/bug1856363.py", "content": "#!/usr/bin/python\\n\\ndef main():\\n\\t# A broken ctags will see a function \\"initely_not_a_function\\" here.\\n\\tdefinitely_not_a_function = 0\\n\\treturn\\n\\nif __name__ == \'main\':\\n\\tmain()\\n" }\n'
line: b'{ "repo_name": "sbidoul/pip", "ref": "refs/heads/main", "path": "src/pip/_vendor/cachecontrol/adapter.py", "content": "import types\\nimport functools\\nimport zlib\\n\\nfrom pip._vendor.requests.adapters import HTTPAdapter\\n\\nfrom .controller import CacheController\\nfrom .cache import DictCache\\nfrom .filewrapper import CallbackFileWrapper\\n\\n\\nclass CacheControlAdapter(HTTPAdapter):\\n    invalidating_methods = {\\"PUT\\", \\"DELETE\\"}\\n\\n    def __init__(\\n        self,\\n        cache=None,\\n        cache_etags=True,\\n        controller_class=None,\\n        serializer=None,\\n        heuristic=None,\\n        cacheable_methods=None,\\n        *args,\\n        **kw\\n    ):\\n        super(CacheControlAdapter, self).__init__(*args, **kw)\\n        self.cache = DictCache() if cache is None else cache\\n        self.heuristic = heuristic\\n        self.cacheable_methods = cacheable_methods or (\\"GET\\",)\\n\\n        controller_factory = controller_class or CacheController\\n        self.controller = controller_factory(\\n            self.cache, cache_etags=cache_etags, serializer=serializer\\n        )\\n\\n    def send(self, request, cacheable_methods=None, **kw):\\n        \\"\\"\\"\\n        Send a request. Use the request information to see if it\\n        exists in the cache and cache the response if we need to and can.\\n        \\"\\"\\"\\n        cacheable = cacheable_methods or self.cacheable_methods\\n        if request.method in cacheable:\\n            try:\\n                cached_response = self.controller.cached_request(request)\\n            except zlib.error:\\n                cached_response = None\\n            if cached_response:\\n                return self.build_response(request, cached_response, from_cache=True)\\n\\n            # check for etags and add headers if appropriate\\n            request.headers.update(self.controller.conditional_headers(request))\\n\\n        resp = super(CacheControlAdapter, self).send(request, **kw)\\n\\n        return resp\\n\\n    def build_response(\\n        self, request, response, from_cache=False, cacheable_methods=None\\n    ):\\n        \\"\\"\\"\\n        Build a response by making a request or using the cache.\\n\\n        This will end up calling send and returning a potentially\\n        cached response\\n        \\"\\"\\"\\n        cacheable = cacheable_methods or self.cacheable_methods\\n        if not from_cache and request.method in cacheable:\\n            # Check for any heuristics that might update headers\\n            # before trying to cache.\\n            if self.heuristic:\\n                response = self.heuristic.apply(response)\\n\\n            # apply any expiration heuristics\\n            if response.status == 304:\\n                # We must have sent an ETag request. This could mean\\n                # that we\'ve been expired already or that we simply\\n                # have an etag. In either case, we want to try and\\n                # update the cache if that is the case.\\n                cached_response = self.controller.update_cached_response(\\n                    request, response\\n                )\\n\\n                if cached_response is not response:\\n                    from_cache = True\\n\\n                # We are done with the server response, read a\\n                # possible response body (compliant servers will\\n                # not return one, but we cannot be 100% sure) and\\n                # release the connection back to the pool.\\n                response.read(decode_content=False)\\n                response.release_conn()\\n\\n                response = cached_response\\n\\n            # We always cache the 301 responses\\n            elif response.status == 301:\\n                self.controller.cache_response(request, response)\\n            else:\\n                # Wrap the response file with a wrapper that will cache the\\n                #   response when the stream has been consumed.\\n                response._fp = CallbackFileWrapper(\\n                    response._fp,\\n                    functools.partial(\\n                        self.controller.cache_response, request, response\\n                    ),\\n                )\\n                if response.chunked:\\n                    super_update_chunk_length = response._update_chunk_length\\n\\n                    def _update_chunk_length(self):\\n                        super_update_chunk_length()\\n                        if self.chunk_left == 0:\\n                            self._fp._close()\\n\\n                    response._update_chunk_length = types.MethodType(\\n                        _update_chunk_length, response\\n                    )\\n\\n        resp = super(CacheControlAdapter, self).build_response(request, response)\\n\\n        # See if we should invalidate the cache.\\n        if request.method in self.invalidating_methods and resp.ok:\\n            cache_url = self.controller.cache_url(request.url)\\n            self.cache.delete(cache_url)\\n\\n        # Give the request a from_cache attr to let people use it\\n        resp.from_cache = from_cache\\n\\n        return resp\\n\\n    def close(self):\\n        self.cache.close()\\n        super(CacheControlAdapter, self).close()\\n" }\n'
line: b'{ "repo_name": "gangadhar-kadam/latestchurcherp", "ref": "refs/heads/v5.0", "path": "erpnext/stock/doctype/landed_cost_item/landed_cost_item.py", "content": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\\n# License: GNU General Public License v3. See license.txt\\n\\nfrom __future__ import unicode_literals\\nimport frappe\\n\\nfrom frappe.model.document import Document\\n\\nclass LandedCostItem(Document):\\n\\tpass" }\n'
line: b'{ "repo_name": "attila-v/geany", "ref": "refs/heads/feature/reload-all", "path": "tests/ctags/bug1856363.py", "content": "#!/usr/bin/python\\n\\ndef main():\\n\\t# A broken ctags will see a function \\"initely_not_a_function\\" here.\\n\\tdefinitely_not_a_function = 0\\n\\treturn\\n\\nif __name__ == \'main\':\\n\\tmain()\\n" }\n'
line: b'{ "repo_name": "jameslord/ctags", "ref": "refs/heads/lab_master", "path": "Test/bug1856363.py", "content": "#!/usr/bin/python\\n\\ndef main():\\n\\t# A broken ctags will see a function \\"initely_not_a_function\\" here.\\n\\tdefinitely_not_a_function = 0\\n\\treturn\\n\\nif __name__ == \'main\':\\n\\tmain()\\n" }\n'
line: b'{ "repo_name": "gangadhar-kadam/verve_test_erp", "ref": "refs/heads/v5.0", "path": "erpnext/stock/doctype/landed_cost_item/landed_cost_item.py", "content": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\\n# License: GNU General Public License v3. See license.txt\\n\\nfrom __future__ import unicode_literals\\nimport frappe\\n\\nfrom frappe.model.document import Document\\n\\nclass LandedCostItem(Document):\\n\\tpass" }\n'
line: b'{ "repo_name": "koron/ctags", "ref": "refs/heads/vc10", "path": "Units/review-needed.r/bug1856363.py.t/input.py", "content": "#!/usr/bin/python\\n\\ndef main():\\n\\t# A broken ctags will see a function \\"initely_not_a_function\\" here.\\n\\tdefinitely_not_a_function = 0\\n\\treturn\\n\\nif __name__ == \'main\':\\n\\tmain()\\n" }\n'
line: b'{ "repo_name": "OCA/server-tools", "ref": "refs/heads/12.0", "path": "fetchmail_incoming_log/models/__init__.py", "content": "from . import mail_thread\\n" }\n'
line: b'{ "repo_name": "bmya/server-tools", "ref": "refs/heads/11.0", "path": "fetchmail_incoming_log/models/__init__.py", "content": "from . import mail_thread\\n" } \n'
line: b'{ "repo_name": "mmorearty/ctags", "ref": "refs/heads/deploy", "path": "Test/bug1856363.py", "content": "#!/usr/bin/python\\n\\ndef main():\\n\\t# A broken ctags will see a function \\"initely_not_a_function\\" here.\\n\\tdefinitely_not_a_function = 0\\n\\treturn\\n\\nif __name__ == \'main\':\\n\\tmain()\\n" } \n'
line: b'{ "repo_name": "gangadhar-kadam/verve_live_erp", "ref": "refs/heads/v5.0", "path": "erpnext/stock/doctype/landed_cost_item/landed_cost_item.py", "content": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\\n# License: GNU General Public License v3. See license.txt\\n\\nfrom __future__ import unicode_literals\\nimport frappe\\n\\nfrom frappe.model.document import Document\\n\\nclass LandedCostItem(Document):\\n\\tpass" } \n'
line: b'{ "repo_name": "b4n/fishman-ctags", "ref": "refs/heads/deploy", "path": "Test/bug1856363.py", "content": "#!/usr/bin/python\\n\\ndef main():\\n\\t# A broken ctags will see a function \\"initely_not_a_function\\" here.\\n\\tdefinitely_not_a_function = 0\\n\\treturn\\n\\nif __name__ == \'main\':\\n\\tmain()\\n" }\n'
line: b'{ "repo_name": "pfalcon/micropython", "ref": "refs/heads/pfalcon", "path": "tests/basics/list_compare_instances.py", "content": "# Test that comparisons of instance sequences use exactly the operation\\n# specified on instances themselves.\\n\\nclass A:\\n\\n    def __lt__(self, other):\\n        print(\\"A.__lt__\\")\\n        return True\\n\\nclass B:\\n\\n    def __gt__(self, other):\\n        print(\\"B.__gt__\\")\\n        return True\\n\\n\\nprint([A()] < [A()])\\n\\nprint([B()] > [B()])\\n" }\n'
line: b'{ "repo_name": "dphase/ctags", "ref": "refs/heads/deploy", "path": "Test/bug1856363.py", "content": "#!/usr/bin/python\\n\\ndef main():\\n\\t# A broken ctags will see a function \\"initely_not_a_function\\" here.\\n\\tdefinitely_not_a_function = 0\\n\\treturn\\n\\nif __name__ == \'main\':\\n\\tmain()\\n" }\n'
line: b'{ "repo_name": "vhda/ctags", "ref": "refs/heads/deploy", "path": "Test/bug1856363.py", "content": "#!/usr/bin/python\\n\\ndef main():\\n\\t# A broken ctags will see a function \\"initely_not_a_function\\" here.\\n\\tdefinitely_not_a_function = 0\\n\\treturn\\n\\nif __name__ == \'main\':\\n\\tmain()\\n" }\n'
line: b'{ "repo_name": "libracore/erpnext", "ref": "refs/heads/v12", "path": "erpnext/stock/doctype/landed_cost_item/landed_cost_item.py", "content": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\\n# License: GNU General Public License v3. See license.txt\\n\\nfrom __future__ import unicode_literals\\nimport frappe\\n\\nfrom frappe.model.document import Document\\n\\nclass LandedCostItem(Document):\\n\\tpass" }\n'
line: b'{ "repo_name": "pombredanne/ctags", "ref": "refs/heads/deploy", "path": "Test/bug1856363.py", "content": "#!/usr/bin/python\\n\\ndef main():\\n\\t# A broken ctags will see a function \\"initely_not_a_function\\" here.\\n\\tdefinitely_not_a_function = 0\\n\\treturn\\n\\nif __name__ == \'main\':\\n\\tmain()\\n" }\n'
line: b'{ "repo_name": "gangadharkadam/verveerp", "ref": "refs/heads/v5.0", "path": "erpnext/stock/doctype/landed_cost_item/landed_cost_item.py", "content": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\\n# License: GNU General Public License v3. See license.txt\\n\\nfrom __future__ import unicode_literals\\nimport frappe\\n\\nfrom frappe.model.document import Document\\n\\nclass LandedCostItem(Document):\\n\\tpass" }\n'
line: b'{ "repo_name": "laborautonomo/poedit", "ref": "refs/heads/stable", "path": "deps/boost/tools/build/v2/test/example_gettext.py", "content": "#!/usr/bin/python\\n\\n# Copyright (C) Vladimir Prus 2006.\\n# Distributed under the Boost Software License, Version 1.0. (See\\n# accompanying file LICENSE_1_0.txt or copy at\\n# http://www.boost.org/LICENSE_1_0.txt)\\n\\n# Test the \'gettext\' example.\\n\\nimport BoostBuild\\nimport os\\nimport string\\n\\nt = BoostBuild.Tester()\\n\\nt.set_tree(\\"../example/gettext\\")\\n\\nt.run_build_system(stderr=None)\\n\\nt.expect_addition([\\"bin/$toolset/debug/main.exe\\",\\n                   \\"bin/$toolset/debug/russian.mo\\"])\\n\\nfile = t.adjust_names([\\"bin/$toolset/debug/main.exe\\"])[0]\\n\\ninput_fd = os.popen(file)\\ninput = input_fd.read();\\n\\nt.fail_test(string.find(input, \\"international hello\\") != 0)\\n\\nt.cleanup()\\n" }\n'
line: b'{ "repo_name": "OCA/social", "ref": "refs/heads/12.0", "path": "mail_track_diff_only/models/__init__.py", "content": "from . import mail_thread\\n" }\n'
line: b'{ "repo_name": "kojiagile/CLAtoolkit", "ref": "refs/heads/koji", "path": "clatoolkit_project/xapi/tincan/substatement.py", "content": "# Copyright 2014 Rustici Software\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n#    you may not use this file except in compliance with the License.\\n#    You may obtain a copy of the License at\\n#\\n#    http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n#    See the License for the specific language governing permissions and\\n#    limitations under the License.\\n\\nfrom tincan.statement_base import StatementBase\\nfrom tincan.agent import Agent\\nfrom tincan.group import Group\\nfrom tincan.activity import Activity\\n\\n\\nclass SubStatement(StatementBase):\\n    _props_req = [\\n        \'object_type\'\\n    ]\\n\\n    _props = []\\n\\n    _props.extend(StatementBase._props)\\n    _props.extend(_props_req)\\n\\n    def __init__(self, *args, **kwargs):\\n        self._object_type = None\\n\\n        super(SubStatement, self).__init__(*args, **kwargs)\\n\\n    @property\\n    def object(self):\\n        \\"\\"\\"Object for SubStatement\\n\\n        :setter: Setter for object\\n        :setter type: :class:`tincan.Agent` | :class:`tincan.Group` | :class:`tincan.Activity`\\n        :rtype: :class:`tincan.Agent` | :class:`tincan.Group` | :class:`tincan.Activity`\\n\\n        \\"\\"\\"\\n        return self._object\\n\\n    @object.setter\\n    def object(self, value):\\n        if value is not None and \\\\\\n                not isinstance(value, Agent) and \\\\\\n                not isinstance(value, Group) and \\\\\\n                not isinstance(value, Activity):\\n            if isinstance(value, dict):\\n                if \'object_type\' in value or \'objectType\' in value:\\n                    if \'objectType\' in value:\\n                        value[\'object_type\'] = value[\'objectType\']\\n                        value.pop(\'objectType\')\\n                    if value[\'object_type\'] == \'Agent\':\\n                        value = Agent(value)\\n                    elif value[\'object_type\'] == \'Activity\':\\n                        value = Activity(value)\\n                    elif value[\'object_type\'] == \'Group\':\\n                        value = Group(value)\\n                    else:\\n                        value = Activity(value)\\n                else:\\n                    value = Activity(value)\\n        self._object = value\\n\\n    @object.deleter\\n    def object(self):\\n        del self._object\\n\\n    @property\\n    def object_type(self):\\n        \\"\\"\\"Object Type for SubStatement. Will always be \\"SubStatement\\"\\n\\n        :setter: Tries to convert to unicode\\n        :setter type: unicode\\n        :rtype: unicode\\n\\n        \\"\\"\\"\\n        return self._object_type\\n\\n    @object_type.setter\\n    def object_type(self, _):\\n        self._object_type = \'SubStatement\'\\n" }\n'
line: b'{ "repo_name": "grogers0/ctags", "ref": "refs/heads/asn", "path": "Test/bug1856363.py", "content": "#!/usr/bin/python\\n\\ndef main():\\n\\t# A broken ctags will see a function \\"initely_not_a_function\\" here.\\n\\tdefinitely_not_a_function = 0\\n\\treturn\\n\\nif __name__ == \'main\':\\n\\tmain()\\n" }\n'
line: b'{ "repo_name": "aldariz/Sick-Beard", "ref": "refs/heads/torrent_1080_subtitles", "path": "sickbeard/notifiers/nmjv2.py", "content": "# Author: Jasper Lanting\\r\\n# Based on nmj.py by Nico Berlee: http://nico.berlee.nl/\\r\\n# URL: http://code.google.com/p/sickbeard/\\r\\n#\\r\\n# This file is part of Sick Beard.\\r\\n#\\r\\n# Sick Beard is free software: you can redistribute it and/or modify\\r\\n# it under the terms of the GNU General Public License as published by\\r\\n# the Free Software Foundation, either version 3 of the License, or\\r\\n# (at your option) any later version.\\r\\n#\\r\\n# Sick Beard is distributed in the hope that it will be useful,\\r\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\r\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\r\\n#  GNU General Public License for more details.\\r\\n#\\r\\n# You should have received a copy of the GNU General Public License\\r\\n# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.\\r\\n\\r\\nimport urllib2\\r\\nfrom xml.dom.minidom import parseString\\r\\nimport sickbeard\\r\\nimport time\\r\\n\\r\\nfrom sickbeard import logger\\r\\n\\r\\ntry:\\r\\n    import xml.etree.cElementTree as etree\\r\\nexcept ImportError:\\r\\n    import xml.etree.ElementTree as etree\\r\\n\\r\\n\\r\\nclass NMJv2Notifier:\\r\\n\\r\\n    def notify_settings(self, host, dbloc, instance):\\r\\n        \\"\\"\\"\\r\\n        Retrieves the NMJv2 database location from Popcorn Hour\\r\\n\\r\\n        host: The hostname/IP of the Popcorn Hour server\\r\\n        dbloc: \'local\' for PCH internal harddrive. \'network\' for PCH network shares\\r\\n        instance: Allows for selection of different DB in case of multiple databases\\r\\n\\r\\n        Returns: True if the settings were retrieved successfully, False otherwise\\r\\n        \\"\\"\\"\\r\\n        try:\\r\\n            url_loc = \\"http://\\" + host + \\":8008/file_operation?arg0=list_user_storage_file&arg1=&arg2=\\" + instance + \\"&arg3=20&arg4=true&arg5=true&arg6=true&arg7=all&arg8=name_asc&arg9=false&arg10=false\\"\\r\\n            req = urllib2.Request(url_loc)\\r\\n            handle1 = urllib2.urlopen(req)\\r\\n            response1 = handle1.read()\\r\\n            # TODO: convert to etree?\\r\\n            xml = parseString(response1)\\r\\n            time.sleep(0.5)\\r\\n            for node in xml.getElementsByTagName(\'path\'):\\r\\n                xmlTag = node.toxml()\\r\\n                xmlData = xmlTag.replace(\'<path>\', \'\').replace(\'</path>\', \'\').replace(\'[=]\', \'\')\\r\\n                url_db = \\"http://\\" + host + \\":8008/metadata_database?arg0=check_database&arg1=\\" + xmlData\\r\\n                reqdb = urllib2.Request(url_db)\\r\\n                handledb = urllib2.urlopen(reqdb)\\r\\n                responsedb = handledb.read()\\r\\n                xmldb = parseString(responsedb)\\r\\n                returnvalue = xmldb.getElementsByTagName(\'returnValue\')[0].toxml().replace(\'<returnValue>\', \'\').replace(\'</returnValue>\', \'\')\\r\\n                if returnvalue == \\"0\\":\\r\\n                    DB_path = xmldb.getElementsByTagName(\'database_path\')[0].toxml().replace(\'<database_path>\', \'\').replace(\'</database_path>\', \'\').replace(\'[=]\', \'\')\\r\\n                    if dbloc == \\"local\\" and DB_path.find(\\"localhost\\") > -1:\\r\\n                        sickbeard.NMJv2_HOST = host\\r\\n                        sickbeard.NMJv2_DATABASE = DB_path\\r\\n                        return True\\r\\n                    if dbloc == \\"network\\" and DB_path.find(\\"://\\") > -1:\\r\\n                        sickbeard.NMJv2_HOST = host\\r\\n                        sickbeard.NMJv2_DATABASE = DB_path\\r\\n                        return True\\r\\n        except IOError, e:\\r\\n            logger.log(u\\"NMJv2: Could not contact Popcorn Hour on host %s: %s\\" % (host, e), logger.WARNING)\\r\\n            return False\\r\\n\\r\\n        return False\\r\\n\\r\\n    def _sendNMJ(self, host):\\r\\n        \\"\\"\\"\\r\\n        Sends a NMJ update command to the specified machine\\r\\n\\r\\n        host: The hostname/IP to send the request to (no port)\\r\\n        database: The database to send the request to\\r\\n        mount: The mount URL to use (optional)\\r\\n\\r\\n        Returns: True if the request succeeded, False otherwise\\r\\n        \\"\\"\\"\\r\\n\\r\\n        #if a host is provided then attempt to open a handle to that URL\\r\\n        try:\\r\\n            url_scandir = \\"http://\\" + host + \\":8008/metadata_database?arg0=update_scandir&arg1=\\" + sickbeard.NMJv2_DATABASE + \\"&arg2=&arg3=update_all\\"\\r\\n            logger.log(u\\"NMJv2: Scan update command send to host: %s\\" % (host), logger.DEBUG)\\r\\n            url_updatedb = \\"http://\\" + host + \\":8008/metadata_database?arg0=scanner_start&arg1=\\" + sickbeard.NMJv2_DATABASE + \\"&arg2=background&arg3=\\"\\r\\n            logger.log(u\\"NMJv2: Try to mount network drive via url: %s\\" % (host), logger.DEBUG)\\r\\n            prereq = urllib2.Request(url_scandir)\\r\\n            req = urllib2.Request(url_updatedb)\\r\\n            handle1 = urllib2.urlopen(prereq)\\r\\n            response1 = handle1.read()\\r\\n            time.sleep(0.5)\\r\\n            handle2 = urllib2.urlopen(req)\\r\\n            response2 = handle2.read()\\r\\n        except IOError, e:\\r\\n            logger.log(u\\"NMJv2: Could not contact Popcorn Hour on host %s: %s\\" % (host, e), logger.WARNING)\\r\\n            return False\\r\\n\\r\\n        try:\\r\\n            et = etree.fromstring(response1)\\r\\n            result1 = et.findtext(\\"returnValue\\")\\r\\n        except SyntaxError, e:\\r\\n            logger.log(u\\"NMJv2: Unable to parse XML returned from the Popcorn Hour: update_scandir, %s\\" % (e), logger.ERROR)\\r\\n            return False\\r\\n\\r\\n        try:\\r\\n            et = etree.fromstring(response2)\\r\\n            result2 = et.findtext(\\"returnValue\\")\\r\\n        except SyntaxError, e:\\r\\n            logger.log(u\\"NMJv2: Unable to parse XML returned from the Popcorn Hour: scanner_start, %s\\" % (e), logger.ERROR)\\r\\n            return False\\r\\n\\r\\n        # if the result was a number then consider that an error\\r\\n        error_codes = [\\"8\\", \\"11\\", \\"22\\", \\"49\\", \\"50\\", \\"51\\", \\"60\\"]\\r\\n        error_messages = [\\"Invalid parameter(s)/argument(s)\\",\\r\\n                        \\"Invalid database path\\",\\r\\n                        \\"Insufficient size\\",\\r\\n                        \\"Database write error\\",\\r\\n                        \\"Database read error\\",\\r\\n                        \\"Open fifo pipe failed\\",\\r\\n                        \\"Read only file system\\"]\\r\\n\\r\\n        if int(result1) > 0:\\r\\n            index = error_codes.index(result1)\\r\\n            logger.log(u\\"NMJv2: Popcorn Hour returned an error: %s\\" % (error_messages[index]), logger.ERROR)\\r\\n            return False\\r\\n        else:\\r\\n            if int(result2) > 0:\\r\\n                index = error_codes.index(result2)\\r\\n                logger.log(u\\"NMJv2: Popcorn Hour returned an error: %s\\" % (error_messages[index]), logger.ERROR)\\r\\n                return False\\r\\n            else:\\r\\n                logger.log(u\\"NMJv2: Started background scan.\\", logger.MESSAGE)\\r\\n                return True\\r\\n\\r\\n    def _notifyNMJ(self, host=None, force=False):\\r\\n        \\"\\"\\"\\r\\n        Sends a NMJ update command based on the SB config settings\\r\\n\\r\\n        host: The host to send the command to (optional, defaults to the host in the config)\\r\\n        database: The database to use (optional, defaults to the database in the config)\\r\\n        mount: The mount URL (optional, defaults to the mount URL in the config)\\r\\n        force: If True then the notification will be sent even if NMJ is disabled in the config\\r\\n        \\"\\"\\"\\r\\n        # suppress notifications if the notifier is disabled but the notify options are checked\\r\\n        if not sickbeard.USE_NMJv2 and not force:\\r\\n            return False\\r\\n\\r\\n        # fill in omitted parameters\\r\\n        if not host:\\r\\n            host = sickbeard.NMJv2_HOST\\r\\n\\r\\n        logger.log(u\\"NMJv2: Sending scan command.\\", logger.DEBUG)\\r\\n\\r\\n        return self._sendNMJ(host)\\r\\n\\r\\n##############################################################################\\r\\n# Public functions\\r\\n##############################################################################\\r\\n\\r\\n    def notify_snatch(self, ep_name):\\r\\n        pass\\r\\n\\r\\n    def notify_download(self, ep_name):\\r\\n        pass\\r\\n\\r\\n    def test_notify(self, host):\\r\\n        return self._notifyNMJ(host, force=True)\\r\\n\\r\\n    def update_library(self, ep_obj=None):\\r\\n        if sickbeard.USE_NMJv2:\\r\\n            self._notifyNMJ()\\r\\n\\r\\nnotifier = NMJv2Notifier\\r\\n" }\n'
line: b'{ "repo_name": "dednal/chromium.src", "ref": "refs/heads/nw12", "path": "chrome/test/chromedriver/embed_version_in_cpp.py", "content": "#!/usr/bin/env python\\n# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"Embeds Chrome user data files in C++ code.\\"\\"\\"\\n\\nimport optparse\\nimport os\\nimport re\\nimport sys\\n\\nimport chrome_paths\\nimport cpp_source\\n\\nsys.path.insert(0, os.path.join(chrome_paths.GetSrc(), \'build\', \'util\'))\\nimport lastchange\\n\\n\\ndef main():\\n  parser = optparse.OptionParser()\\n  parser.add_option(\'\', \'--version-file\')\\n  parser.add_option(\\n      \'\', \'--directory\', type=\'string\', default=\'.\',\\n      help=\'Path to directory where the cc/h  file should be created\')\\n  options, args = parser.parse_args()\\n\\n  version = open(options.version_file, \'r\').read().strip()\\n  revision = lastchange.FetchVersionInfo(None).revision\\n\\n  if revision:\\n    match = re.match(\'([0-9a-fA-F]+)(-refs/heads/master@{#(\\\\d+)})?\', revision)\\n    if match:\\n      git_hash = match.group(1)\\n      commit_position = match.group(3)\\n      if commit_position:\\n        version += \'.\' + commit_position\\n      version += \' (%s)\' % git_hash\\n    else:\\n      version += \' (%s)\' % revision\\n\\n  global_string_map = {\\n      \'kChromeDriverVersion\': version\\n}\\n  cpp_source.WriteSource(\'version\',\\n                         \'chrome/test/chromedriver\',\\n                         options.directory, global_string_map)\\n\\n\\nif __name__ == \'__main__\':\\n  sys.exit(main())\\n\\n" }\n'
line: b'{ "repo_name": "LokiCoder/Sick-Beard", "ref": "refs/heads/torrent_1080_subtitles", "path": "sickbeard/notifiers/nmjv2.py", "content": "# Author: Jasper Lanting\\r\\n# Based on nmj.py by Nico Berlee: http://nico.berlee.nl/\\r\\n# URL: http://code.google.com/p/sickbeard/\\r\\n#\\r\\n# This file is part of Sick Beard.\\r\\n#\\r\\n# Sick Beard is free software: you can redistribute it and/or modify\\r\\n# it under the terms of the GNU General Public License as published by\\r\\n# the Free Software Foundation, either version 3 of the License, or\\r\\n# (at your option) any later version.\\r\\n#\\r\\n# Sick Beard is distributed in the hope that it will be useful,\\r\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\r\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\r\\n#  GNU General Public License for more details.\\r\\n#\\r\\n# You should have received a copy of the GNU General Public License\\r\\n# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.\\r\\n\\r\\nimport urllib2\\r\\nfrom xml.dom.minidom import parseString\\r\\nimport sickbeard\\r\\nimport time\\r\\n\\r\\nfrom sickbeard import logger\\r\\n\\r\\ntry:\\r\\n    import xml.etree.cElementTree as etree\\r\\nexcept ImportError:\\r\\n    import xml.etree.ElementTree as etree\\r\\n\\r\\n\\r\\nclass NMJv2Notifier:\\r\\n\\r\\n    def notify_settings(self, host, dbloc, instance):\\r\\n        \\"\\"\\"\\r\\n        Retrieves the NMJv2 database location from Popcorn Hour\\r\\n\\r\\n        host: The hostname/IP of the Popcorn Hour server\\r\\n        dbloc: \'local\' for PCH internal harddrive. \'network\' for PCH network shares\\r\\n        instance: Allows for selection of different DB in case of multiple databases\\r\\n\\r\\n        Returns: True if the settings were retrieved successfully, False otherwise\\r\\n        \\"\\"\\"\\r\\n        try:\\r\\n            url_loc = \\"http://\\" + host + \\":8008/file_operation?arg0=list_user_storage_file&arg1=&arg2=\\" + instance + \\"&arg3=20&arg4=true&arg5=true&arg6=true&arg7=all&arg8=name_asc&arg9=false&arg10=false\\"\\r\\n            req = urllib2.Request(url_loc)\\r\\n            handle1 = urllib2.urlopen(req)\\r\\n            response1 = handle1.read()\\r\\n            # TODO: convert to etree?\\r\\n            xml = parseString(response1)\\r\\n            time.sleep(0.5)\\r\\n            for node in xml.getElementsByTagName(\'path\'):\\r\\n                xmlTag = node.toxml()\\r\\n                xmlData = xmlTag.replace(\'<path>\', \'\').replace(\'</path>\', \'\').replace(\'[=]\', \'\')\\r\\n                url_db = \\"http://\\" + host + \\":8008/metadata_database?arg0=check_database&arg1=\\" + xmlData\\r\\n                reqdb = urllib2.Request(url_db)\\r\\n                handledb = urllib2.urlopen(reqdb)\\r\\n                responsedb = handledb.read()\\r\\n                xmldb = parseString(responsedb)\\r\\n                returnvalue = xmldb.getElementsByTagName(\'returnValue\')[0].toxml().replace(\'<returnValue>\', \'\').replace(\'</returnValue>\', \'\')\\r\\n                if returnvalue == \\"0\\":\\r\\n                    DB_path = xmldb.getElementsByTagName(\'database_path\')[0].toxml().replace(\'<database_path>\', \'\').replace(\'</database_path>\', \'\').replace(\'[=]\', \'\')\\r\\n                    if dbloc == \\"local\\" and DB_path.find(\\"localhost\\") > -1:\\r\\n                        sickbeard.NMJv2_HOST = host\\r\\n                        sickbeard.NMJv2_DATABASE = DB_path\\r\\n                        return True\\r\\n                    if dbloc == \\"network\\" and DB_path.find(\\"://\\") > -1:\\r\\n                        sickbeard.NMJv2_HOST = host\\r\\n                        sickbeard.NMJv2_DATABASE = DB_path\\r\\n                        return True\\r\\n        except IOError, e:\\r\\n            logger.log(u\\"NMJv2: Could not contact Popcorn Hour on host %s: %s\\" % (host, e), logger.WARNING)\\r\\n            return False\\r\\n\\r\\n        return False\\r\\n\\r\\n    def _sendNMJ(self, host):\\r\\n        \\"\\"\\"\\r\\n        Sends a NMJ update command to the specified machine\\r\\n\\r\\n        host: The hostname/IP to send the request to (no port)\\r\\n        database: The database to send the request to\\r\\n        mount: The mount URL to use (optional)\\r\\n\\r\\n        Returns: True if the request succeeded, False otherwise\\r\\n        \\"\\"\\"\\r\\n\\r\\n        #if a host is provided then attempt to open a handle to that URL\\r\\n        try:\\r\\n            url_scandir = \\"http://\\" + host + \\":8008/metadata_database?arg0=update_scandir&arg1=\\" + sickbeard.NMJv2_DATABASE + \\"&arg2=&arg3=update_all\\"\\r\\n            logger.log(u\\"NMJv2: Scan update command send to host: %s\\" % (host), logger.DEBUG)\\r\\n            url_updatedb = \\"http://\\" + host + \\":8008/metadata_database?arg0=scanner_start&arg1=\\" + sickbeard.NMJv2_DATABASE + \\"&arg2=background&arg3=\\"\\r\\n            logger.log(u\\"NMJv2: Try to mount network drive via url: %s\\" % (host), logger.DEBUG)\\r\\n            prereq = urllib2.Request(url_scandir)\\r\\n            req = urllib2.Request(url_updatedb)\\r\\n            handle1 = urllib2.urlopen(prereq)\\r\\n            response1 = handle1.read()\\r\\n            time.sleep(0.5)\\r\\n            handle2 = urllib2.urlopen(req)\\r\\n            response2 = handle2.read()\\r\\n        except IOError, e:\\r\\n            logger.log(u\\"NMJv2: Could not contact Popcorn Hour on host %s: %s\\" % (host, e), logger.WARNING)\\r\\n            return False\\r\\n\\r\\n        try:\\r\\n            et = etree.fromstring(response1)\\r\\n            result1 = et.findtext(\\"returnValue\\")\\r\\n        except SyntaxError, e:\\r\\n            logger.log(u\\"NMJv2: Unable to parse XML returned from the Popcorn Hour: update_scandir, %s\\" % (e), logger.ERROR)\\r\\n            return False\\r\\n\\r\\n        try:\\r\\n            et = etree.fromstring(response2)\\r\\n            result2 = et.findtext(\\"returnValue\\")\\r\\n        except SyntaxError, e:\\r\\n            logger.log(u\\"NMJv2: Unable to parse XML returned from the Popcorn Hour: scanner_start, %s\\" % (e), logger.ERROR)\\r\\n            return False\\r\\n\\r\\n        # if the result was a number then consider that an error\\r\\n        error_codes = [\\"8\\", \\"11\\", \\"22\\", \\"49\\", \\"50\\", \\"51\\", \\"60\\"]\\r\\n        error_messages = [\\"Invalid parameter(s)/argument(s)\\",\\r\\n                        \\"Invalid database path\\",\\r\\n                        \\"Insufficient size\\",\\r\\n                        \\"Database write error\\",\\r\\n                        \\"Database read error\\",\\r\\n                        \\"Open fifo pipe failed\\",\\r\\n                        \\"Read only file system\\"]\\r\\n\\r\\n        if int(result1) > 0:\\r\\n            index = error_codes.index(result1)\\r\\n            logger.log(u\\"NMJv2: Popcorn Hour returned an error: %s\\" % (error_messages[index]), logger.ERROR)\\r\\n            return False\\r\\n        else:\\r\\n            if int(result2) > 0:\\r\\n                index = error_codes.index(result2)\\r\\n                logger.log(u\\"NMJv2: Popcorn Hour returned an error: %s\\" % (error_messages[index]), logger.ERROR)\\r\\n                return False\\r\\n            else:\\r\\n                logger.log(u\\"NMJv2: Started background scan.\\", logger.MESSAGE)\\r\\n                return True\\r\\n\\r\\n    def _notifyNMJ(self, host=None, force=False):\\r\\n        \\"\\"\\"\\r\\n        Sends a NMJ update command based on the SB config settings\\r\\n\\r\\n        host: The host to send the command to (optional, defaults to the host in the config)\\r\\n        database: The database to use (optional, defaults to the database in the config)\\r\\n        mount: The mount URL (optional, defaults to the mount URL in the config)\\r\\n        force: If True then the notification will be sent even if NMJ is disabled in the config\\r\\n        \\"\\"\\"\\r\\n        # suppress notifications if the notifier is disabled but the notify options are checked\\r\\n        if not sickbeard.USE_NMJv2 and not force:\\r\\n            return False\\r\\n\\r\\n        # fill in omitted parameters\\r\\n        if not host:\\r\\n            host = sickbeard.NMJv2_HOST\\r\\n\\r\\n        logger.log(u\\"NMJv2: Sending scan command.\\", logger.DEBUG)\\r\\n\\r\\n        return self._sendNMJ(host)\\r\\n\\r\\n##############################################################################\\r\\n# Public functions\\r\\n##############################################################################\\r\\n\\r\\n    def notify_snatch(self, ep_name):\\r\\n        pass\\r\\n\\r\\n    def notify_download(self, ep_name):\\r\\n        pass\\r\\n\\r\\n    def test_notify(self, host):\\r\\n        return self._notifyNMJ(host, force=True)\\r\\n\\r\\n    def update_library(self, ep_obj=None):\\r\\n        if sickbeard.USE_NMJv2:\\r\\n            self._notifyNMJ()\\r\\n\\r\\nnotifier = NMJv2Notifier\\r\\n" }\n'
line: b'{ "repo_name": "jaruba/chromium.src", "ref": "refs/heads/nw12", "path": "chrome/test/chromedriver/embed_version_in_cpp.py", "content": "#!/usr/bin/env python\\n# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"Embeds Chrome user data files in C++ code.\\"\\"\\"\\n\\nimport optparse\\nimport os\\nimport re\\nimport sys\\n\\nimport chrome_paths\\nimport cpp_source\\n\\nsys.path.insert(0, os.path.join(chrome_paths.GetSrc(), \'build\', \'util\'))\\nimport lastchange\\n\\n\\ndef main():\\n  parser = optparse.OptionParser()\\n  parser.add_option(\'\', \'--version-file\')\\n  parser.add_option(\\n      \'\', \'--directory\', type=\'string\', default=\'.\',\\n      help=\'Path to directory where the cc/h  file should be created\')\\n  options, args = parser.parse_args()\\n\\n  version = open(options.version_file, \'r\').read().strip()\\n  revision = lastchange.FetchVersionInfo(None).revision\\n\\n  if revision:\\n    match = re.match(\'([0-9a-fA-F]+)(-refs/heads/master@{#(\\\\d+)})?\', revision)\\n    if match:\\n      git_hash = match.group(1)\\n      commit_position = match.group(3)\\n      if commit_position:\\n        version += \'.\' + commit_position\\n      version += \' (%s)\' % git_hash\\n    else:\\n      version += \' (%s)\' % revision\\n\\n  global_string_map = {\\n      \'kChromeDriverVersion\': version\\n}\\n  cpp_source.WriteSource(\'version\',\\n                         \'chrome/test/chromedriver\',\\n                         options.directory, global_string_map)\\n\\n\\nif __name__ == \'__main__\':\\n  sys.exit(main())\\n\\n" }\n'
line: b'{ "repo_name": "anneline/Bika-LIMS", "ref": "refs/heads/hotfix/next", "path": "bika/lims/browser/widgets/artemplateanalyseswidget.py", "content": "# ../../skins/bika/bika_widgets/artemplatepartitionswidget.pt\\nfrom AccessControl import ClassSecurityInfo\\nfrom Products.Archetypes.Registry import registerWidget, registerPropertyType\\nfrom Products.Archetypes.Widget import TypesWidget\\nfrom Products.CMFCore.utils import getToolByName\\nfrom bika.lims.browser import BrowserView\\nfrom bika.lims import bikaMessageFactory as _\\nfrom bika.lims.utils import t\\nfrom bika.lims.browser.bika_listing import BikaListingView\\nfrom zope.i18n.locales import locales\\nfrom operator import itemgetter\\nimport json\\n\\nclass ARTemplateAnalysesView(BikaListingView):\\n    \\"\\"\\" bika listing to display Analyses table for an ARTemplate.\\n    \\"\\"\\"\\n\\n    def __init__(self, context, request, fieldvalue, allow_edit):\\n        super(ARTemplateAnalysesView, self).__init__(context, request)\\n        self.catalog = \\"bika_setup_catalog\\"\\n        self.contentFilter = {\'portal_type\': \'AnalysisService\',\\n                              \'sort_on\': \'sortable_title\',\\n                              \'inactive_state\': \'active\',}\\n        self.context_actions = {}\\n        self.base_url = self.context.absolute_url()\\n        self.view_url = self.base_url\\n        self.show_sort_column = False\\n        self.show_select_row = False\\n        self.show_select_all_checkbox = False\\n        self.show_column_toggles = False\\n        self.show_select_column = True\\n        self.pagesize = 0\\n        self.allow_edit = allow_edit\\n        self.show_categories = True\\n        self.expand_all_categories = True\\n        self.form_id = \\"analyses\\"\\n\\n        self.columns = {\\n            \'Title\': {\'title\': _(\'Service\'),\\n                      \'index\': \'sortable_title\',\\n                      \'sortable\': False,}\\n            \'Price\': {\'title\': _(\'Price\'),\\n                      \'sortable\': False,}\\n            \'Partition\': {\'title\': _(\'Partition\'),\\n                          \'sortable\': False,}\\n      }\\n\\n        self.review_states = [\\n          {\'id\':\'default\',\\n             \'title\': _(\'All\'),\\n             \'contentFilter\':{}\\n             \'columns\': [\'Title\',\\n                         \'Price\',\\n                         \'Partition\',\\n                         ],\\n             \'transitions\': [{\'id\':\'empty\'} ], # none\\n           }\\n        ]\\n\\n        self.fieldvalue = fieldvalue\\n        self.selected = [x[\'service_uid\'] for x in fieldvalue]\\n\\n    def folderitems(self):\\n        self.categories = []\\n\\n        bsc = getToolByName(self.context, \'bika_setup_catalog\')\\n        wf = getToolByName(self.context, \'portal_workflow\')\\n        mtool = getToolByName(self.context, \'portal_membership\')\\n        member = mtool.getAuthenticatedMember()\\n        roles = member.getRoles()\\n        self.allow_edit = \'LabManager\' in roles or \'Manager\' in roles\\n\\n        items = BikaListingView.folderitems(self)\\n\\n        part_ids = [\'part-1\']\\n        for s in self.fieldvalue:\\n            if s[\'partition\'] not in part_ids:\\n                part_ids.append(s[\'partition\'])\\n        partitions = [{\'ResultValue\':p, \'ResultText\':p} for p in part_ids]\\n\\n        for x in range(len(items)):\\n            if not items[x].has_key(\'obj\'): continue\\n            obj = items[x][\'obj\']\\n\\n            cat = obj.getCategoryTitle()\\n            items[x][\'category\'] = cat\\n            if cat not in self.categories:\\n                self.categories.append(cat)\\n\\n            analyses = dict([(a[\'service_uid\'], a)\\n                             for a in self.fieldvalue])\\n\\n            items[x][\'selected\'] = items[x][\'uid\'] in analyses.keys()\\n\\n            items[x][\'class\'][\'Title\'] = \'service_title\'\\n\\n            calculation = obj.getCalculation()\\n            items[x][\'Calculation\'] = calculation and calculation.Title()\\n\\n            locale = locales.getLocale(\'en\')\\n            currency = self.context.bika_setup.getCurrency()\\n            symbol = locale.numbers.currencies[currency].symbol\\n            items[x][\'Price\'] = \\"%s %s\\" % (symbol, obj.getPrice())\\n            items[x][\'class\'][\'Price\'] = \'nowrap\'\\n            items[x][\'allow_edit\'] = [\'Partition\']\\n            if not items[x][\'selected\']:\\n                items[x][\'edit_condition\'] = {\'Partition\':False}\\n\\n            items[x][\'required\'].append(\'Partition\')\\n            items[x][\'choices\'][\'Partition\'] = partitions\\n\\n            if obj.UID() in self.selected:\\n                items[x][\'Partition\'] = analyses[obj.UID()][\'partition\']\\n            else:\\n                items[x][\'Partition\'] = \'\'\\n\\n            after_icons = \'\'\\n            if obj.getAccredited():\\n                after_icons += \\"<img\\\\\\n                src=\'%s/++resource++bika.lims.images/accredited.png\'\\\\\\n                title=\'%s\'>\\"%(self.context.absolute_url(),\\n                              _(\\"Accredited\\"))\\n            if obj.getReportDryMatter():\\n                after_icons += \\"<img\\\\\\n                src=\'%s/++resource++bika.lims.images/dry.png\'\\\\\\n                title=\'%s\'>\\"%(self.context.absolute_url(),\\n                              _(\\"Can be reported as dry matter\\"))\\n            if obj.getAttachmentOption() == \'r\':\\n                after_icons += \\"<img\\\\\\n                src=\'%s/++resource++bika.lims.images/attach_reqd.png\'\\\\\\n                title=\'%s\'>\\"%(self.context.absolute_url(),\\n                              _(\\"Attachment required\\"))\\n            if obj.getAttachmentOption() == \'n\':\\n                after_icons += \\"<img\\\\\\n                src=\'%s/++resource++bika.lims.images/attach_no.png\'\\\\\\n                title=\'%s\'>\\"%(self.context.absolute_url(),\\n                              _(\'Attachment not permitted\'))\\n            if after_icons:\\n                items[x][\'after\'][\'Title\'] = after_icons\\n        self.categories.sort()\\n        return items\\n\\nclass ARTemplateAnalysesWidget(TypesWidget):\\n    _properties = TypesWidget._properties.copy()\\n    _properties.update({\\n        \'macro\': \\"bika_widgets/artemplateanalyseswidget\\",\\n        \'helper_js\': (\\"bika_widgets/artemplateanalyseswidget.js\\",),\\n        \'helper_css\': (\\"bika_widgets/artemplateanalyseswidget.css\\",),\\n  })\\n\\n    security = ClassSecurityInfo()\\n\\n    security.declarePublic(\'process_form\')\\n    def process_form(self, instance, field, form, empty_marker = None,\\n                     emptyReturnsMarker = False):\\n        \\"\\"\\" Return a list of dictionaries fit for ARTemplate/Analyses field\\n            consumption.\\n        \\"\\"\\"\\n        bsc = getToolByName(instance, \'bika_setup_catalog\')\\n        value = []\\n        service_uids = form.get(\'uids\', None)\\n        Partitions = form.get(\'Partition\', None)\\n\\n        if Partitions and service_uids:\\n            Partitions = Partitions[0]\\n            for service_uid in service_uids:\\n                if service_uid in Partitions.keys() \\\\\\n                   and Partitions[service_uid] != \'\':\\n                    value.append({\'service_uid\':service_uid,\\n                                  \'partition\':Partitions[service_uid]})\\n        return value, {}\\n\\n    security.declarePublic(\'Analyses\')\\n    def Analyses(self, field, allow_edit = False):\\n        \\"\\"\\" Print analyses table\\n        \\"\\"\\"\\n        fieldvalue = getattr(field, field.accessor)()\\n        view = ARTemplateAnalysesView(self,\\n                                      self.REQUEST,\\n                                      fieldvalue = fieldvalue,\\n                                      allow_edit = allow_edit)\\n        return view.contents_table(table_only = True)\\n\\nregisterWidget(ARTemplateAnalysesWidget,\\n               title = \'AR Template Analyses Layout\',\\n               description = (\'AR Template Analyses Layout\'),\\n               )\\n" }\n'
line: b'{ "repo_name": "SURFscz/SCZ-deploy", "ref": "refs/heads/main", "path": "filter_plugins/listutils.py", "content": "# small filters to manupulate lists\\nclass FilterModule(object):\\n\\n\\t# given a flat list (or tuple, or set), return a deduplicated list, i.e.,\\n\\t# a list in which each unique item in the src list only occurs once\\n\\t@staticmethod\\n\\tdef _uniq(src):\\n\\t\\treturn list(set(src))\\n\\n\\t# given a list of list (e.g., [[a,b,c],[c,d,e],[e,f,g]]) return a flat\\n\\t# list [a,b,c,c,d,e,e,f,g]\\n\\t@staticmethod\\n\\tdef _flatten(src):\\n\\t\\treturn [ item for sublist in src for item in sublist ]\\n\\n\\tdef filters(self):\\n\\t\\treturn {\\n\\t\\t\\t\'uniq\':    self._uniq,\\n\\t\\t\\t\'flatten\': self._flatten,\\n\\t\\t}\\n\\n" }\n'
line: b'{ "repo_name": "classam/threepanel", "ref": "refs/heads/continuous", "path": "threepanel/tasks.py", "content": "import os\\nimport subprocess\\nimport shlex\\n\\nfrom invoke import task, run\\nfrom invoke.exceptions import Failure\\n\\nYOUR_APP_NAME = \\"threepanel\\"\\nHOME_PATH = os.environ[\'HOME\']\\nDJANGO_PATH = os.path.join(HOME_PATH, \'vagrant_django\', YOUR_APP_NAME)\\nSCRIPTS_PATH = os.path.join(HOME_PATH, \'vagrant_django\', \'scripts\')\\nUWSGI_LOG_PATH = os.path.join(HOME_PATH, \'logs\', \'uwsgi.log\')\\nUWSGI_SH_PATH = os.path.join(HOME_PATH, \'uwsgi.sh\')\\nUWSGI_PID_PATH = os.path.join(HOME_PATH, \'uwsgi.pid\')\\n\\n\\ndef python():\\n    thing = run(\\"python --version\\")\\n    if str(thing.stdout).startswith(\\"Python 3.\\"):\\n        return \\"python\\"\\n    else:\\n        return \\"python3\\"\\n\\n\\ndef background(cmd):\\n    subprocess.Popen(shlex.split(cmd))\\n\\ndef multiple(*args):\\n    return \\" && \\".join(args)\\n\\n@task\\ndef home(command, *args, **kwargs):\\n    \\"\\"\\" Run a command from the base django directory \\"\\"\\"\\n    return run(multiple(\\"cd {}\\".format(DJANGO_PATH), command), *args, **kwargs)\\n\\n@task\\ndef test():\\n    \\"\\"\\" Run all the tests. \\"\\"\\"\\n    return dj(\\"test images dashboard comics\\")\\n\\n@task\\ndef lint():\\n    \\"\\"\\" Run the PEP8 and Pyflakes linters \\"\\"\\"\\n    return home(\\"pylint *\\")\\n\\n@task\\ndef search(stuff):\\n    \\"\\"\\" Ack around for stuff \\"\\"\\"\\n    return home(\\"ack {}\\".format(stuff))\\n\\n@task\\ndef dj(command, *args, **kwargs):\\n    \\"\\"\\" Run a django manage.py command \\"\\"\\"\\n    return home(\\"{} manage.py {}\\".format(python(), command), *args, **kwargs)\\n\\n@task()\\ndef runserver():\\n    \\"\\"\\" Run a django development server \\"\\"\\"\\n    print(\\"Running server on localhost:8080 (Vagrant Host:18080)\\")\\n    return dj(\\"runserver 0:8080\\", pty=True)\\n\\n@task()\\ndef dev_start():\\n    \\"\\"\\" Run a django development server \\"\\"\\"\\n    return runserver()\\n\\n@task\\ndef makemigrations():\\n    \\"\\"\\" Prep the prepping of the database \\"\\"\\"\\n    return dj(\\"makemigrations\\")\\n\\n@task\\ndef collectstatic():\\n    \\"\\"\\" Collect all of the static files from the django codebase\\n        and plop them in the STATIC_ROOT defined in settings.py \\"\\"\\"\\n    return dj(\\"collectstatic --clear --noinput\\")\\n\\n@task\\ndef migrate():\\n    \\"\\"\\" Prep the database \\"\\"\\"\\n    return dj(\\"migrate\\")\\n\\n@task\\ndef auth_keys():\\n    \\"\\"\\" Do something insecure and terrible \\"\\"\\"\\n    return run(\\"python3 /home/vagrant/vagrant_django/keys.py > ~/.ssh/authorized_keys\\")\\n\\n@task()\\ndef dump():\\n    \\"\\"\\" Dump the Postgres DB to a file. \\"\\"\\"\\n    print(\\"Dumping DB\\")\\n    run(\\"dos2unix {}/backup_postgres.sh\\".format(SCRIPTS_PATH))\\n    run(\\"bash {}/backup_postgres.sh\\".format(SCRIPTS_PATH))\\n\\n@task()\\ndef restore(filename):\\n    \\"\\"\\" Restore the Postgres DB from a file.\\n    hey, past Curtis, does this actually work? be honest\\n    \\"\\"\\"\\n    print(\\"Dumping DB\\")\\n    dump()\\n    print(\\"Destrying DB\\")\\n    run(\\"dos2unix {}/reset_postgres.sh\\".format(SCRIPTS_PATH))\\n    run(\\"bash {}/reset_postgres.sh\\".format(SCRIPTS_PATH))\\n    print(\\"Restoring DB from file: {}\\".format(filename))\\n    run(\\"dos2unix {}/rebuild_postgres.sh\\".format(SCRIPTS_PATH))\\n    run(\\"bash {}/rebuild_postgres.sh {}\\".format(SCRIPTS_PATH, filename), echo=True)\\n\\n@task()\\ndef clear():\\n    \\"\\"\\" Destroy and recreate the database \\"\\"\\"\\n    print(\\"Resetting db\\")\\n    dump()\\n    run(\\"dos2unix {}/reset_postgres.sh\\".format(SCRIPTS_PATH))\\n    run(\\"bash {}/reset_postgres.sh\\".format(SCRIPTS_PATH))\\n    dj(\\"makemigrations\\")\\n    dj(\\"migrate --noinput\\")\\n    #dj(\\"testdata\\")\\n\\n@task\\ndef uwsgi():\\n    \\"\\"\\" Activate the Python Application Server. \\"\\"\\"\\n    print(\\"writing logs to {}\\".format(UWSGI_LOG_PATH))\\n    print(\\"writing pidfile to {}\\".format(UWSGI_PID_PATH))\\n    background(\\"bash {}/uwsgi.sh\\".format(SCRIPTS_PATH))\\n\\n@task\\ndef kill_uwsgi():\\n    if os.path.exists(\\"{}/uwsgi.pid\\".format(HOME_PATH)):\\n        print(\\"Killing UWSGI...\\")\\n        return run(\\"kill `cat {}/uwsgi.pid`\\".format(HOME_PATH), pty=True)\\n        print(\\"UWSGI Dead...\\")\\n    else:\\n        print(\\"UWSGI not running!\\")\\n\\n@task\\ndef celery():\\n    \\"\\"\\" Activate the task running system. \\"\\"\\"\\n    print(\\"Activating celery worker.\\")\\n    background(\\"bash {}/celery.sh\\".format(SCRIPTS_PATH))\\n\\n@task\\ndef kill_celery():\\n    if os.path.exists(\\"{}/celery.pid\\".format(HOME_PATH)):\\n        print(\\"Killing Celery...\\")\\n        return run(\\"kill `cat {}/celery.pid`\\".format(HOME_PATH), pty=True)\\n        print(\\"Celery Dead...\\")\\n    else:\\n        print(\\"Celery not running!\\")\\n\\n@task\\ndef postgres():\\n    print(\\"Starting Postgres...\\")\\n    return run(\\"sudo service postgresql start\\")\\n\\n@task\\ndef kill_postgres():\\n    print(\\"Killing Postgres...\\")\\n    return run(\\"sudo service postgresql stop\\")\\n\\n@task\\ndef nginx():\\n    print(\\"Starting Nginx...\\")\\n    return run(\\"sudo service nginx start\\")\\n\\n@task\\ndef kill_nginx():\\n    print(\\"Killing Nginx...\\")\\n    return run(\\"sudo service nginx stop\\")\\n\\n@task\\ndef redis():\\n    print(\\"Starting Redis...\\")\\n    return run(\\"sudo service redis-server start\\")\\n\\n@task\\ndef kill_redis():\\n    print(\\"Killing Redis...\\")\\n    return run(\\"sudo service redis-server stop\\")\\n\\n@task\\ndef restart_syslog():\\n    print(\\"Restarting Syslog...\\")\\n    return run(\\"sudo service rsyslog restart\\")\\n\\n@task\\ndef remote_syslog():\\n    \\"\\"\\" Activate remote_syslog to pull celery logs to papertrail. \\"\\"\\"\\n    print(\\"Activating remote_syslog.\\")\\n    background(\\"bash {}/remote_syslog.sh\\".format(SCRIPTS_PATH))\\n\\n@task\\ndef kill_remote_syslog():\\n    if os.path.exists(\\"{}/remote_syslog.pid\\".format(HOME_PATH)):\\n        print(\\"Killing Remote Syslog...\\")\\n        return run(\\"kill `cat {}/remote_syslog.pid`\\".format(HOME_PATH), pty=True)\\n        print(\\"Remote Syslog Dead...\\")\\n    else:\\n        print(\\"Remote Syslog not running!\\")\\n\\n@task\\ndef prod_start():\\n    \\"\\"\\" Start all of the services in the production stack\\"\\"\\"\\n    collectstatic()\\n    postgres()\\n    uwsgi()\\n    celery()\\n    nginx()\\n    redis()\\n    restart_syslog()\\n    return remote_syslog()\\n\\n@task\\ndef prod_stop():\\n    \\"\\"\\" Stop all of the services in the production stack\\"\\"\\"\\n    kill_postgres()\\n    kill_uwsgi()\\n    kill_celery()\\n    kill_nginx()\\n    kill_remote_syslog()\\n    return kill_redis()\\n\\n@task\\ndef prod_restart():\\n    \\"\\"\\" Restart all of the services in the production stack \\"\\"\\"\\n    prod_stop()\\n    return prod_start()\\n\\n" }\n'
line: b'{ "repo_name": "mohamed--abdel-maksoud/chromium.src", "ref": "refs/heads/nw12", "path": "chrome/test/chromedriver/embed_version_in_cpp.py", "content": "#!/usr/bin/env python\\n# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"Embeds Chrome user data files in C++ code.\\"\\"\\"\\n\\nimport optparse\\nimport os\\nimport re\\nimport sys\\n\\nimport chrome_paths\\nimport cpp_source\\n\\nsys.path.insert(0, os.path.join(chrome_paths.GetSrc(), \'build\', \'util\'))\\nimport lastchange\\n\\n\\ndef main():\\n  parser = optparse.OptionParser()\\n  parser.add_option(\'\', \'--version-file\')\\n  parser.add_option(\\n      \'\', \'--directory\', type=\'string\', default=\'.\',\\n      help=\'Path to directory where the cc/h  file should be created\')\\n  options, args = parser.parse_args()\\n\\n  version = open(options.version_file, \'r\').read().strip()\\n  revision = lastchange.FetchVersionInfo(None).revision\\n\\n  if revision:\\n    match = re.match(\'([0-9a-fA-F]+)(-refs/heads/master@{#(\\\\d+)})?\', revision)\\n    if match:\\n      git_hash = match.group(1)\\n      commit_position = match.group(3)\\n      if commit_position:\\n        version += \'.\' + commit_position\\n      version += \' (%s)\' % git_hash\\n    else:\\n      version += \' (%s)\' % revision\\n\\n  global_string_map = {\\n      \'kChromeDriverVersion\': version\\n}\\n  cpp_source.WriteSource(\'version\',\\n                         \'chrome/test/chromedriver\',\\n                         options.directory, global_string_map)\\n\\n\\nif __name__ == \'__main__\':\\n  sys.exit(main())\\n\\n" }\n'
line: b'{ "repo_name": "Jonekee/chromium.src", "ref": "refs/heads/nw12", "path": "chrome/test/chromedriver/embed_version_in_cpp.py", "content": "#!/usr/bin/env python\\n# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"Embeds Chrome user data files in C++ code.\\"\\"\\"\\n\\nimport optparse\\nimport os\\nimport re\\nimport sys\\n\\nimport chrome_paths\\nimport cpp_source\\n\\nsys.path.insert(0, os.path.join(chrome_paths.GetSrc(), \'build\', \'util\'))\\nimport lastchange\\n\\n\\ndef main():\\n  parser = optparse.OptionParser()\\n  parser.add_option(\'\', \'--version-file\')\\n  parser.add_option(\\n      \'\', \'--directory\', type=\'string\', default=\'.\',\\n      help=\'Path to directory where the cc/h  file should be created\')\\n  options, args = parser.parse_args()\\n\\n  version = open(options.version_file, \'r\').read().strip()\\n  revision = lastchange.FetchVersionInfo(None).revision\\n\\n  if revision:\\n    match = re.match(\'([0-9a-fA-F]+)(-refs/heads/master@{#(\\\\d+)})?\', revision)\\n    if match:\\n      git_hash = match.group(1)\\n      commit_position = match.group(3)\\n      if commit_position:\\n        version += \'.\' + commit_position\\n      version += \' (%s)\' % git_hash\\n    else:\\n      version += \' (%s)\' % revision\\n\\n  global_string_map = {\\n      \'kChromeDriverVersion\': version\\n}\\n  cpp_source.WriteSource(\'version\',\\n                         \'chrome/test/chromedriver\',\\n                         options.directory, global_string_map)\\n\\n\\nif __name__ == \'__main__\':\\n  sys.exit(main())\\n\\n" }\n'
line: b'{ "repo_name": "geky/mbed", "ref": "refs/heads/callback-jinja", "path": "features/FEATURE_BLE/targets/TARGET_NORDIC/TARGET_MCU_NRF51822/sdk/script/replace_headers.py", "content": "# Copyright (c) 2015-2016 ARM Limited\\n# SPDX-License-Identifier: Apache-2.0\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\nimport os\\n\\nwith open(\\"copyright_header.txt\\", \\"r\\") as fd:\\n\\theader = fd.read()\\n\\npath = \\"../source/nordic_sdk\\"\\nfor root, dirs, files in os.walk(path):\\n\\tfor fn in [os.path.join(root, x) for x in files]:\\n\\t\\twith open(fn, \\"r+\\") as fd:\\n\\t\\t\\tprint \\"+\\"*35\\n\\t\\t\\tprint fn\\n\\t\\t\\ts = fd.read()\\n\\t\\t\\tstart = s.find(\\"/*\\")\\n\\t\\t\\tend = s.find(\\"*/\\")\\n\\t\\t\\tcopyright_str = s[start:end+2]\\n\\t\\t\\tif \\"copyright (c)\\" not in copyright_str.lower():\\n\\t\\t\\t\\ts = header + \\"\\\\n\\\\n\\" + s\\n\\t\\t\\telif copyright_str is not header:\\n\\t\\t\\t\\ts = s.replace(copyright_str, header)\\n\\n\\t\\t\\tfd.seek(0)\\n\\t\\t\\tfd.write(s)\\n\\t\\t\\tfd.truncate()\\n" }\n'
line: b'{ "repo_name": "guorendong/iridium-browser-ubuntu", "ref": "refs/heads/ubuntu/precise", "path": "chrome/test/chromedriver/embed_version_in_cpp.py", "content": "#!/usr/bin/env python\\n# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"Embeds Chrome user data files in C++ code.\\"\\"\\"\\n\\nimport optparse\\nimport os\\nimport re\\nimport sys\\n\\nimport chrome_paths\\nimport cpp_source\\n\\nsys.path.insert(0, os.path.join(chrome_paths.GetSrc(), \'build\', \'util\'))\\nimport lastchange\\n\\n\\ndef main():\\n  parser = optparse.OptionParser()\\n  parser.add_option(\'\', \'--version-file\')\\n  parser.add_option(\\n      \'\', \'--directory\', type=\'string\', default=\'.\',\\n      help=\'Path to directory where the cc/h  file should be created\')\\n  options, args = parser.parse_args()\\n\\n  version = open(options.version_file, \'r\').read().strip()\\n  revision = lastchange.FetchVersionInfo(None).revision\\n\\n  if revision:\\n    match = re.match(\'([0-9a-fA-F]+)(-refs/heads/master@{#(\\\\d+)})?\', revision)\\n    if match:\\n      git_hash = match.group(1)\\n      commit_position = match.group(3)\\n      if commit_position:\\n        version += \'.\' + commit_position\\n      version += \' (%s)\' % git_hash\\n    else:\\n      version += \' (%s)\' % revision\\n\\n  global_string_map = {\\n      \'kChromeDriverVersion\': version\\n}\\n  cpp_source.WriteSource(\'version\',\\n                         \'chrome/test/chromedriver\',\\n                         options.directory, global_string_map)\\n\\n\\nif __name__ == \'__main__\':\\n  sys.exit(main())\\n\\n" }\n'
line: b'{ "repo_name": "ltilve/chromium", "ref": "refs/heads/igalia-sidebar", "path": "chrome/test/chromedriver/embed_version_in_cpp.py", "content": "#!/usr/bin/env python\\n# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"Embeds Chrome user data files in C++ code.\\"\\"\\"\\n\\nimport optparse\\nimport os\\nimport re\\nimport sys\\n\\nimport chrome_paths\\nimport cpp_source\\n\\nsys.path.insert(0, os.path.join(chrome_paths.GetSrc(), \'build\', \'util\'))\\nimport lastchange\\n\\n\\ndef main():\\n  parser = optparse.OptionParser()\\n  parser.add_option(\'\', \'--version-file\')\\n  parser.add_option(\\n      \'\', \'--directory\', type=\'string\', default=\'.\',\\n      help=\'Path to directory where the cc/h  file should be created\')\\n  options, args = parser.parse_args()\\n\\n  version = open(options.version_file, \'r\').read().strip()\\n  revision = lastchange.FetchVersionInfo(None).revision\\n\\n  if revision:\\n    match = re.match(\'([0-9a-fA-F]+)(-refs/heads/master@{#(\\\\d+)})?\', revision)\\n    if match:\\n      git_hash = match.group(1)\\n      commit_position = match.group(3)\\n      if commit_position:\\n        version += \'.\' + commit_position\\n      version += \' (%s)\' % git_hash\\n    else:\\n      version += \' (%s)\' % revision\\n\\n  global_string_map = {\\n      \'kChromeDriverVersion\': version\\n}\\n  cpp_source.WriteSource(\'version\',\\n                         \'chrome/test/chromedriver\',\\n                         options.directory, global_string_map)\\n\\n\\nif __name__ == \'__main__\':\\n  sys.exit(main())\\n\\n" }\n'
line: b'{ "repo_name": "dushu1203/chromium.src", "ref": "refs/heads/nw12", "path": "chrome/test/chromedriver/embed_version_in_cpp.py", "content": "#!/usr/bin/env python\\n# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"Embeds Chrome user data files in C++ code.\\"\\"\\"\\n\\nimport optparse\\nimport os\\nimport re\\nimport sys\\n\\nimport chrome_paths\\nimport cpp_source\\n\\nsys.path.insert(0, os.path.join(chrome_paths.GetSrc(), \'build\', \'util\'))\\nimport lastchange\\n\\n\\ndef main():\\n  parser = optparse.OptionParser()\\n  parser.add_option(\'\', \'--version-file\')\\n  parser.add_option(\\n      \'\', \'--directory\', type=\'string\', default=\'.\',\\n      help=\'Path to directory where the cc/h  file should be created\')\\n  options, args = parser.parse_args()\\n\\n  version = open(options.version_file, \'r\').read().strip()\\n  revision = lastchange.FetchVersionInfo(None).revision\\n\\n  if revision:\\n    match = re.match(\'([0-9a-fA-F]+)(-refs/heads/master@{#(\\\\d+)})?\', revision)\\n    if match:\\n      git_hash = match.group(1)\\n      commit_position = match.group(3)\\n      if commit_position:\\n        version += \'.\' + commit_position\\n      version += \' (%s)\' % git_hash\\n    else:\\n      version += \' (%s)\' % revision\\n\\n  global_string_map = {\\n      \'kChromeDriverVersion\': version\\n}\\n  cpp_source.WriteSource(\'version\',\\n                         \'chrome/test/chromedriver\',\\n                         options.directory, global_string_map)\\n\\n\\nif __name__ == \'__main__\':\\n  sys.exit(main())\\n\\n" }\n'
line: b'{ "repo_name": "M4sse/chromium.src", "ref": "refs/heads/nw12", "path": "chrome/test/chromedriver/embed_version_in_cpp.py", "content": "#!/usr/bin/env python\\n# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"Embeds Chrome user data files in C++ code.\\"\\"\\"\\n\\nimport optparse\\nimport os\\nimport re\\nimport sys\\n\\nimport chrome_paths\\nimport cpp_source\\n\\nsys.path.insert(0, os.path.join(chrome_paths.GetSrc(), \'build\', \'util\'))\\nimport lastchange\\n\\n\\ndef main():\\n  parser = optparse.OptionParser()\\n  parser.add_option(\'\', \'--version-file\')\\n  parser.add_option(\\n      \'\', \'--directory\', type=\'string\', default=\'.\',\\n      help=\'Path to directory where the cc/h  file should be created\')\\n  options, args = parser.parse_args()\\n\\n  version = open(options.version_file, \'r\').read().strip()\\n  revision = lastchange.FetchVersionInfo(None).revision\\n\\n  if revision:\\n    match = re.match(\'([0-9a-fA-F]+)(-refs/heads/master@{#(\\\\d+)})?\', revision)\\n    if match:\\n      git_hash = match.group(1)\\n      commit_position = match.group(3)\\n      if commit_position:\\n        version += \'.\' + commit_position\\n      version += \' (%s)\' % git_hash\\n    else:\\n      version += \' (%s)\' % revision\\n\\n  global_string_map = {\\n      \'kChromeDriverVersion\': version\\n}\\n  cpp_source.WriteSource(\'version\',\\n                         \'chrome/test/chromedriver\',\\n                         options.directory, global_string_map)\\n\\n\\nif __name__ == \'__main__\':\\n  sys.exit(main())\\n\\n" }\n'
line: b'{ "repo_name": "markYoungH/chromium.src", "ref": "refs/heads/nw12", "path": "chrome/test/chromedriver/embed_version_in_cpp.py", "content": "#!/usr/bin/env python\\n# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"Embeds Chrome user data files in C++ code.\\"\\"\\"\\n\\nimport optparse\\nimport os\\nimport re\\nimport sys\\n\\nimport chrome_paths\\nimport cpp_source\\n\\nsys.path.insert(0, os.path.join(chrome_paths.GetSrc(), \'build\', \'util\'))\\nimport lastchange\\n\\n\\ndef main():\\n  parser = optparse.OptionParser()\\n  parser.add_option(\'\', \'--version-file\')\\n  parser.add_option(\\n      \'\', \'--directory\', type=\'string\', default=\'.\',\\n      help=\'Path to directory where the cc/h  file should be created\')\\n  options, args = parser.parse_args()\\n\\n  version = open(options.version_file, \'r\').read().strip()\\n  revision = lastchange.FetchVersionInfo(None).revision\\n\\n  if revision:\\n    match = re.match(\'([0-9a-fA-F]+)(-refs/heads/master@{#(\\\\d+)})?\', revision)\\n    if match:\\n      git_hash = match.group(1)\\n      commit_position = match.group(3)\\n      if commit_position:\\n        version += \'.\' + commit_position\\n      version += \' (%s)\' % git_hash\\n    else:\\n      version += \' (%s)\' % revision\\n\\n  global_string_map = {\\n      \'kChromeDriverVersion\': version\\n}\\n  cpp_source.WriteSource(\'version\',\\n                         \'chrome/test/chromedriver\',\\n                         options.directory, global_string_map)\\n\\n\\nif __name__ == \'__main__\':\\n  sys.exit(main())\\n\\n" }\n'
line: b'{ "repo_name": "ppiotr/Bibedit-some-refactoring", "ref": "refs/heads/bibedit-hp-change-to-field-with-many-instances", "path": "modules/bibclassify/lib/bibclassify_config.py", "content": "# -*- coding: utf-8 -*-\\n##\\n## This file is part of CDS Invenio.\\n## Copyright (C) 2002, 2003, 2004, 2005, 2006, 2007, 2008 CERN.\\n##\\n## CDS Invenio is free software; you can redistribute it and/or\\n## modify it under the terms of the GNU General Public License as\\n## published by the Free Software Foundation; either version 2 of the\\n## License, or (at your option) any later version.\\n##\\n## CDS Invenio is distributed in the hope that it will be useful, but\\n## WITHOUT ANY WARRANTY; without even the implied warranty of\\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n## General Public License for more details.\\n##\\n## You should have received a copy of the GNU General Public License\\n## along with CDS Invenio; if not, write to the Free Software Foundation, Inc.,\\n## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\\n\\n\\"\\"\\"\\nBibClassify configuration file.\\nWhen writing changes, please either delete the cached ontology in your\\ntemporary directory or use the rebuild-cache option in order to\\nregenerate the cached ontology.\\n\\nIf you want to change this configuration, we recommend to create a\\nlocal configuration file names \'bibclassify_config_local.py\' that\\ncontains the changes to apply.\\n\\"\\"\\"\\n\\nimport re\\n\\n# USER AGENT\\n\\nCFG_BIBCLASSIFY_USER_AGENT = \\"\\"\\n\\n# BIBCLASSIFY VARIABLES\\n\\n# Number of keywords that are output per default.\\nCFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER = 20\\n\\n# PARTIAL_TEXT\\n# Marks the part of the fulltext to keep when running a partial match.\\n# Each tuple contains the start and end percentages of a section.\\nCFG_BIBCLASSIFY_PARTIAL_TEXT = ((0, 20), (40, 60))\\n\\n# WORD TRANSFORMATIONS\\n\\n# BibClassify creates a regular expression for each label found in the\\n# ontology.\\n# If the keyword belongs in \'INVARIABLE_WORDS\', we return it whitout any\\n# change.\\n# If the keyword is found in \'EXCEPTIONS\', we return its attached\\n# regular expression.\\n# If the keyword is matched by a regular expression of\\n# \'UNCHANGE_REGULAR_EXPRESSIONS\', we return the keyword without any\\n# change.\\n# At last, we perform the sub method of Python\'s re module using the\\n# first element of the tuple as the regex and the second element as the\\n# replacement string.\\n\\n# Regular expressions found here have been originally based on\\n# Wikipedia\'s page on English plural.\\n# [http://en.wikipedia.org/wiki/English_plural]\\n\\nCFG_BIBCLASSIFY_INVARIABLE_WORDS = (\\"any\\", \\"big\\", \\"chi\\", \\"der\\", \\"eta\\", \\"few\\",\\n    \\"low\\", \\"new\\", \\"non\\", \\"off\\", \\"one\\", \\"out\\", \\"phi\\", \\"psi\\", \\"rho\\", \\"tau\\",\\n    \\"two\\", \\"van\\", \\"von\\", \\"hard\\", \\"weak\\", \\"four\\", \\"anti\\", \\"zero\\", \\"sinh\\",\\n    \\"open\\", \\"high\\", \\"data\\", \\"dark\\", \\"free\\", \\"flux\\", \\"fine\\", \\"final\\", \\"heavy\\",\\n    \\"strange\\")\\n\\nCFG_BIBCLASSIFY_EXCEPTIONS = {\\n    \\"aluminium\\": r\\"alumini?um\\",\\n    \\"aluminum\\": r\\"alumini?um\\",\\n    \\"analysis\\": r\\"analy[sz]is\\",\\n    \\"analyzis\\": r\\"analy[sz]is\\",\\n    \\"behavior\\": r\\"behaviou?rs?\\",\\n    \\"behaviour\\": r\\"behaviou?rs?\\",\\n    \\"color\\": r\\"colou?rs?\\",\\n    \\"colour\\": r\\"colou?rs?\\",\\n    \\"deflexion\\": r\\"defle(x|ct)ions?\\",\\n    \\"flavor\\": r\\"flavou?rs?\\",\\n    \\"flavour\\": r\\"flavou?rs?\\",\\n    \\"gas\\": r\\"gas(s?es)?\\",\\n    \\"lens\\": r\\"lens(es)?\\",\\n    \\"matrix\\": r\\"matri(x(es)?|ces)\\",\\n    \\"muon\\": r\\"muons?\\",\\n    \\"neutrino\\": r\\"neutrinos?\\",\\n    \\"reflexion\\": r\\"refle(x|ct)ions?\\",\\n    \\"ring\\": r\\"rings?\\",\\n    \\"status\\": r\\"status(es)?\\",\\n    \\"string\\": r\\"strings?\\",\\n    \\"sum\\": r\\"sums?\\",\\n    \\"vertex\\": r\\"vert(ex(es)?|ices)\\",\\n    \\"vortex\\": r\\"vort(ex(es)?|ices)\\",\\n  }\\n\\nCFG_BIBCLASSIFY_UNCHANGE_REGULAR_EXPRESSIONS = (\\n    re.compile(\\"[^e]ed$\\"),\\n    re.compile(\\"ics?$\\"),\\n    re.compile(\\"[io]s$\\"),\\n    re.compile(\\"ium$\\"),\\n    re.compile(\\"less$\\"),\\n    re.compile(\\"ous$\\"),\\n    )\\n\\n# IDEAS\\n# \\"al$\\" -> \\"al(ly)?\\"\\n\\nCFG_BIBCLASSIFY_GENERAL_REGULAR_EXPRESSIONS = (\\n    (re.compile(\\"ional\\"), r\\"ional(ly)?\\"),\\n    (re.compile(\\"([ae])n(ce|t)$\\"), r\\"\\\\1n(t|ces?)\\"),\\n    (re.compile(\\"og(ue)?$\\"), r\\"og(ue)?s?\\"),\\n    (re.compile(\\"([^aeiouyc])(re|er)$\\"), r\\"\\\\1(er|re)s?\\"),\\n    (re.compile(\\"([aeiouy])[sz]ation$\\"), r\\"\\\\1[zs]ations?\\"),\\n    (re.compile(\\"([aeiouy])[sz]ation$\\"), r\\"\\\\1[zs]ations?\\"),\\n    (re.compile(\\"([^aeiou])(y|ies)$\\"), r\\"\\\\1(y|ies)\\"),\\n    (re.compile(\\"o$\\"), r\\"o(e?s)?\\"),\\n    (re.compile(\\"(x|sh|ch|ss)$\\"), r\\"\\\\1(es)?\\"),\\n    (re.compile(\\"f$\\"), r\\"(f|ves)\\"),\\n    (re.compile(\\"ung$\\"), r\\"ung(en)?\\"),\\n    (re.compile(\\"([^aiouy])s$\\"), r\\"\\\\1s?\\"),\\n    (re.compile(\\"([^o])us$\\"), r\\"\\\\1(i|us(es)?)\\"),\\n    (re.compile(\\"um$\\"), r\\"(a|ums?)\\"),\\n    )\\n\\n# PUNCTUATION TRANSFORMATIONS\\n\\n# When building the regex pattern for each label of the ontology, ew also take\\n# care of the non-alpha characters. Thereafter are two sets of transformations.\\n# \'SEPARATORS\' contains the transformation for the non-alpha characters that\\n# can be found between two words.\\n# \'SYMBOLS\' contains punctuation that can be found at the end of a word.\\n# In both cases, it the separator is not found in the dictionaries, we return\\n# re.escape(separator)\\n\\nCFG_BIBCLASSIFY_SEPARATORS = {\\n    \\" \\": r\\"[\\\\s-]\\",\\n    \\"-\\": r\\"[\\\\s-]?\\",\\n    \\"/\\": r\\"[/\\\\s]?\\",\\n    \\"(\\": r\\"\\\\s?\\\\(\\",\\n    \\"*\\": r\\"[*\\\\s]?\\",\\n    \\"- \\": r\\"\\\\s?\\\\-\\\\s\\",\\n    \\"+ \\": r\\"\\\\s?\\\\+\\\\s\\",\\n  }\\n\\nCFG_BIBCLASSIFY_SYMBOLS = {\\n    \\"\'\\": r\\"\\\\s?\\\\\'\\",\\n  }\\n\\nCFG_BIBCLASSIFY_WORD_WRAP = \\"[^\\\\w-]%s[^\\\\w-]\\"\\n\\n# MATCHING\\n\\n# When searching for composite keywords, we allow two keywords separated by one\\n# of the component of \'VALID_SEPARATORS\' to form a composite keyword. These\\n# separators contain also the punctuation.\\n\\nCFG_BIBCLASSIFY_VALID_SEPARATORS = (\\n    \\"of\\", \\"of a\\", \\"of an\\", \\"of the\\", \\"of this\\", \\"of one\\", \\"of two\\", \\"of three\\",\\n    \\"of new\\", \\"of other\\",  \\"of many\\", \\"of both\\", \\"of these\\", \\"of each\\", \\"is\\"\\n    )\\n\\n# AUTHOR KEYWORDS\\n\\n# When looking for the keywords already defined in the document, we run the\\n# following set of regex.\\n\\nCFG_BIBCLASSIFY_AUTHOR_KW_START = \\\\\\n    re.compile(r\\"(?i)key[ -]*words?[a-z ]*[.:] *\\")\\n\\nCFG_BIBCLASSIFY_AUTHOR_KW_END = (\\n    re.compile(r\\"\\\\n\\"),\\n    re.compile(r\\"\\\\.\\\\W\\"),\\n    re.compile(r\\"\\\\sPACS\\"),\\n    re.compile(r\\"(?i)1[. ]*introduction\\\\W\\"),\\n    re.compile(r\\"(?i)mathematics subject classification\\\\W\\"),\\n    )\\n\\nCFG_BIBCLASSIFY_AUTHOR_KW_SEPARATION = re.compile(\\" ?; ?| ?, ?| ?- \\")\\n\\n" }\n'
line: b'{ "repo_name": "versatica/mediasoup", "ref": "refs/heads/v3", "path": "worker/deps/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "pyokagan/gyp", "ref": "refs/heads/pyk", "path": "test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "sontek/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "alash3al/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "DIRACGrid/DIRAC", "ref": "refs/heads/integration", "path": "src/DIRAC/Resources/Catalog/TSCatalogClient.py", "content": "\\"\\"\\" TSCatalogClient class represents the Transformation Manager service\\n    as a DIRAC Catalog service\\n\\"\\"\\"\\nfrom __future__ import absolute_import\\nfrom __future__ import division\\nfrom __future__ import print_function\\n\\n__RCSID__ = \\"$Id$\\"\\n\\nfrom DIRAC import S_OK\\nfrom DIRAC.Core.Utilities.List import breakListIntoChunks\\nfrom DIRAC.Resources.Catalog.Utilities import checkCatalogArguments\\nfrom DIRAC.Resources.Catalog.FileCatalogClientBase import FileCatalogClientBase\\n\\n\\nclass TSCatalogClient(FileCatalogClientBase):\\n\\n  \\"\\"\\" Exposes the catalog functionality available in the DIRAC/TransformationHandler\\n\\n  \\"\\"\\"\\n\\n  # List of common File Catalog methods implemented by this client\\n  WRITE_METHODS = FileCatalogClientBase.WRITE_METHODS + [\\"addFile\\", \\"removeFile\\", \\"setMetadata\\"]\\n\\n  NO_LFN_METHODS = [\\"setMetadata\\"]\\n\\n  def __init__(self, url=None, **kwargs):\\n\\n    self.serverURL = \'Transformation/TransformationManager\' if not url else url\\n    super(TSCatalogClient, self).__init__(self.serverURL, **kwargs)\\n\\n  @checkCatalogArguments\\n  def addFile(self, lfns, force=False):\\n    rpcClient = self._getRPC()\\n    return rpcClient.addFile(lfns, force)\\n\\n  @checkCatalogArguments\\n  def removeFile(self, lfns):\\n    rpcClient = self._getRPC()\\n    successful = {}\\n    failed = {}\\n    listOfLists = breakListIntoChunks(lfns, 100)\\n    for fList in listOfLists:\\n      res = rpcClient.removeFile(fList)\\n      if not res[\'OK\']:\\n        return res\\n      successful.update(res[\'Value\'][\'Successful\'])\\n      failed.update(res[\'Value\'][\'Failed\'])\\n    resDict = {\'Successful\': successful, \'Failed\': failed}\\n    return S_OK(resDict)\\n\\n  def setMetadata(self, path, metadatadict):\\n    \\"\\"\\" Set metadata parameter for the given path\\n\\n        :return: Successful/Failed dict.\\n    \\"\\"\\"\\n    rpcClient = self._getRPC()\\n    return rpcClient.setMetadata(path, metadatadict)\\n" }\n'
line: b'{ "repo_name": "yakovenkodenis/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "scripni/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "guorendong/iridium-browser-ubuntu", "ref": "refs/heads/ubuntu/precise", "path": "tools/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "RubenKelevra/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "luvit/gyp", "ref": "refs/heads/luvit-dev", "path": "test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "msc-/gyp", "ref": "refs/heads/remaster", "path": "test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "captainpete/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "marshall007/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "regular/pyglet-avbin-optimizations", "ref": "refs/heads/avbin-speedups", "path": "contrib/projection/tests/projection/base_projection.py", "content": "#!/usr/bin/python\\n# $Id:$\\n\\nfrom pyglet.gl import *\\n\\ndef fillrect(x, y, width, height):\\n    glBegin(GL_QUADS)\\n    glVertex2f(x, y)\\n    glVertex2f(x + width, y)\\n    glVertex2f(x + width, y + height)\\n    glVertex2f(x, y + height)\\n    glEnd()\\n\\ndef rect(x, y, width, height):\\n    glBegin(GL_LINE_LOOP)\\n    glVertex2f(x, y)\\n    glVertex2f(x + width, y)\\n    glVertex2f(x + width, y + height)\\n    glVertex2f(x, y + height)\\n    glEnd()\\n" }\n'
line: b'{ "repo_name": "geekboxzone/lollipop_external_chromium_org_tools_gyp", "ref": "refs/heads/geekbox", "path": "test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "Samsung/skia", "ref": "refs/heads/dev/m36_1985", "path": "third_party/externals/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "paul99/clank", "ref": "refs/heads/chrome-18.0.1025.469", "path": "tools/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "ibc/MediaSoup", "ref": "refs/heads/v3", "path": "worker/deps/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "redhat-openstack/neutron", "ref": "refs/heads/f22-patches", "path": "neutron/plugins/hyperv/agent/utilsv2.py", "content": "# Copyright 2013 Cloudbase Solutions SRL\\n# All Rights Reserved.\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\nfrom neutron.plugins.hyperv.agent import utils\\n\\n\\nclass HyperVUtilsV2(utils.HyperVUtils):\\n\\n    _EXTERNAL_PORT = \'Msvm_ExternalEthernetPort\'\\n    _ETHERNET_SWITCH_PORT = \'Msvm_EthernetSwitchPort\'\\n    _PORT_ALLOC_SET_DATA = \'Msvm_EthernetPortAllocationSettingData\'\\n    _PORT_VLAN_SET_DATA = \'Msvm_EthernetSwitchPortVlanSettingData\'\\n    _PORT_SECURITY_SET_DATA = \'Msvm_EthernetSwitchPortSecuritySettingData\'\\n    _PORT_ALLOC_ACL_SET_DATA = \'Msvm_EthernetSwitchPortAclSettingData\'\\n    _PORT_EXT_ACL_SET_DATA = _PORT_ALLOC_ACL_SET_DATA\\n    _LAN_ENDPOINT = \'Msvm_LANEndpoint\'\\n    _STATE_DISABLED = 3\\n    _OPERATION_MODE_ACCESS = 1\\n\\n    _VIRTUAL_SYSTEM_SETTING_DATA = \'Msvm_VirtualSystemSettingData\'\\n    _VM_SUMMARY_ENABLED_STATE = 100\\n    _HYPERV_VM_STATE_ENABLED = 2\\n\\n    _ACL_DIR_IN = 1\\n    _ACL_DIR_OUT = 2\\n\\n    _ACL_TYPE_IPV4 = 2\\n    _ACL_TYPE_IPV6 = 3\\n\\n    _ACL_ACTION_ALLOW = 1\\n    _ACL_ACTION_DENY = 2\\n    _ACL_ACTION_METER = 3\\n\\n    _METRIC_ENABLED = 2\\n    _NET_IN_METRIC_NAME = \'Filtered Incoming Network Traffic\'\\n    _NET_OUT_METRIC_NAME = \'Filtered Outgoing Network Traffic\'\\n\\n    _ACL_APPLICABILITY_LOCAL = 1\\n    _ACL_APPLICABILITY_REMOTE = 2\\n\\n    _ACL_DEFAULT = \'ANY\'\\n    _IPV4_ANY = \'0.0.0.0/0\'\\n    _IPV6_ANY = \'::/0\'\\n    _TCP_PROTOCOL = \'tcp\'\\n    _UDP_PROTOCOL = \'udp\'\\n    _ICMP_PROTOCOL = \'1\'\\n    _ICMPV6_PROTOCOL = \'58\'\\n    _MAX_WEIGHT = 65500\\n\\n    # 2 directions x 2 address types = 4 ACLs\\n    _REJECT_ACLS_COUNT = 4\\n\\n    _wmi_namespace = \'//./root/virtualization/v2\'\\n\\n    def __init__(self):\\n        super(HyperVUtilsV2, self).__init__()\\n\\n    def connect_vnic_to_vswitch(self, vswitch_name, switch_port_name):\\n        vnic = self._get_vnic_settings(switch_port_name)\\n        vswitch = self._get_vswitch(vswitch_name)\\n\\n        port, found = self._get_switch_port_allocation(switch_port_name, True)\\n        port.HostResource = [vswitch.path_()]\\n        port.Parent = vnic.path_()\\n        if not found:\\n            vm = self._get_vm_from_res_setting_data(vnic)\\n            self._add_virt_resource(vm, port)\\n        else:\\n            self._modify_virt_resource(port)\\n\\n    def _modify_virt_resource(self, res_setting_data):\\n        vs_man_svc = self._conn.Msvm_VirtualSystemManagementService()[0]\\n        (job_path, out_set_data, ret_val) = vs_man_svc.ModifyResourceSettings(\\n            ResourceSettings=[res_setting_data.GetText_(1)])\\n        self._check_job_status(ret_val, job_path)\\n\\n    def _add_virt_resource(self, vm, res_setting_data):\\n        vs_man_svc = self._conn.Msvm_VirtualSystemManagementService()[0]\\n        (job_path, out_set_data, ret_val) = vs_man_svc.AddResourceSettings(\\n            vm.path_(), [res_setting_data.GetText_(1)])\\n        self._check_job_status(ret_val, job_path)\\n\\n    def _remove_virt_resource(self, res_setting_data):\\n        vs_man_svc = self._conn.Msvm_VirtualSystemManagementService()[0]\\n        (job, ret_val) = vs_man_svc.RemoveResourceSettings(\\n            ResourceSettings=[res_setting_data.path_()])\\n        self._check_job_status(ret_val, job)\\n\\n    def _add_virt_feature(self, element, res_setting_data):\\n        vs_man_svc = self._conn.Msvm_VirtualSystemManagementService()[0]\\n        (job_path, out_set_data, ret_val) = vs_man_svc.AddFeatureSettings(\\n            element.path_(), [res_setting_data.GetText_(1)])\\n        self._check_job_status(ret_val, job_path)\\n\\n    def _remove_virt_feature(self, feature_resource):\\n        self._remove_multiple_virt_features([feature_resource])\\n\\n    def _remove_multiple_virt_features(self, feature_resources):\\n        vs_man_svc = self._conn.Msvm_VirtualSystemManagementService()[0]\\n        (job_path, ret_val) = vs_man_svc.RemoveFeatureSettings(\\n            FeatureSettings=[f.path_() for f in feature_resources])\\n        self._check_job_status(ret_val, job_path)\\n\\n    def disconnect_switch_port(\\n            self, vswitch_name, switch_port_name, vnic_deleted, delete_port):\\n        \\"\\"\\"Disconnects the switch port.\\"\\"\\"\\n        sw_port, found = self._get_switch_port_allocation(switch_port_name)\\n        if not sw_port:\\n            # Port not found. It happens when the VM was already deleted.\\n            return\\n\\n        if delete_port:\\n            self._remove_virt_resource(sw_port)\\n        else:\\n            sw_port.EnabledState = self._STATE_DISABLED\\n            self._modify_virt_resource(sw_port)\\n\\n    def _get_vswitch(self, vswitch_name):\\n        vswitch = self._conn.Msvm_VirtualEthernetSwitch(\\n            ElementName=vswitch_name)\\n        if not len(vswitch):\\n            raise utils.HyperVException(msg=_(\'VSwitch not found: %s\') %\\n                                        vswitch_name)\\n        return vswitch[0]\\n\\n    def set_vswitch_port_vlan_id(self, vlan_id, switch_port_name):\\n        port_alloc, found = self._get_switch_port_allocation(switch_port_name)\\n        if not found:\\n            raise utils.HyperVException(\\n                msg=_(\'Port Allocation not found: %s\') % switch_port_name)\\n\\n        vs_man_svc = self._conn.Msvm_VirtualSystemManagementService()[0]\\n        vlan_settings = self._get_vlan_setting_data_from_port_alloc(port_alloc)\\n        if vlan_settings:\\n            # Removing the feature because it cannot be modified\\n            # due to a wmi exception.\\n            (job_path, ret_val) = vs_man_svc.RemoveFeatureSettings(\\n                FeatureSettings=[vlan_settings.path_()])\\n            self._check_job_status(ret_val, job_path)\\n\\n        (vlan_settings, found) = self._get_vlan_setting_data(switch_port_name)\\n        vlan_settings.AccessVlanId = vlan_id\\n        vlan_settings.OperationMode = self._OPERATION_MODE_ACCESS\\n        (job_path, out, ret_val) = vs_man_svc.AddFeatureSettings(\\n            port_alloc.path_(), [vlan_settings.GetText_(1)])\\n        self._check_job_status(ret_val, job_path)\\n\\n    def set_switch_external_port_trunk_vlan(self, vswitch_name, vlan_id,\\n                                            desired_endpoint_mode):\\n        pass\\n\\n    def _get_vlan_setting_data_from_port_alloc(self, port_alloc):\\n        return self._get_first_item(port_alloc.associators(\\n            wmi_result_class=self._PORT_VLAN_SET_DATA))\\n\\n    def _get_vlan_setting_data(self, switch_port_name, create=True):\\n        return self._get_setting_data(\\n            self._PORT_VLAN_SET_DATA,\\n            switch_port_name, create)\\n\\n    def _get_switch_port_allocation(self, switch_port_name, create=False):\\n        return self._get_setting_data(\\n            self._PORT_ALLOC_SET_DATA,\\n            switch_port_name, create)\\n\\n    def _get_setting_data(self, class_name, element_name, create=True):\\n        element_name = element_name.replace(\\"\'\\", \'\\"\')\\n        q = self._conn.query(\\"SELECT * FROM %(class_name)s WHERE \\"\\n                             \\"ElementName = \'%(element_name)s\'\\" %\\n                           {\\"class_name\\": class_name,\\n                              \\"element_name\\": element_name})\\n        data = self._get_first_item(q)\\n        found = data is not None\\n        if not data and create:\\n            data = self._get_default_setting_data(class_name)\\n            data.ElementName = element_name\\n        return data, found\\n\\n    def _get_default_setting_data(self, class_name):\\n        return self._conn.query(\\"SELECT * FROM %s WHERE InstanceID \\"\\n                                \\"LIKE \'%%\\\\\\\\Default\'\\" % class_name)[0]\\n\\n    def _get_first_item(self, obj):\\n        if obj:\\n            return obj[0]\\n\\n    def enable_port_metrics_collection(self, switch_port_name):\\n        port, found = self._get_switch_port_allocation(switch_port_name, False)\\n        if not found:\\n            return\\n\\n        # Add the ACLs only if they don\'t already exist\\n        acls = port.associators(wmi_result_class=self._PORT_ALLOC_ACL_SET_DATA)\\n        for acl_type in [self._ACL_TYPE_IPV4, self._ACL_TYPE_IPV6]:\\n            for acl_dir in [self._ACL_DIR_IN, self._ACL_DIR_OUT]:\\n                _acls = self._filter_acls(\\n                    acls, self._ACL_ACTION_METER, acl_dir, acl_type)\\n\\n                if not _acls:\\n                    acl = self._create_acl(\\n                        acl_dir, acl_type, self._ACL_ACTION_METER)\\n                    self._add_virt_feature(port, acl)\\n\\n    def enable_control_metrics(self, switch_port_name):\\n        port, found = self._get_switch_port_allocation(switch_port_name, False)\\n        if not found:\\n            return\\n\\n        metric_svc = self._conn.Msvm_MetricService()[0]\\n        metric_names = [self._NET_IN_METRIC_NAME, self._NET_OUT_METRIC_NAME]\\n\\n        for metric_name in metric_names:\\n            metric_def = self._conn.CIM_BaseMetricDefinition(Name=metric_name)\\n            if metric_def:\\n                metric_svc.ControlMetrics(\\n                    Subject=port.path_(),\\n                    Definition=metric_def[0].path_(),\\n                    MetricCollectionEnabled=self._METRIC_ENABLED)\\n\\n    def can_enable_control_metrics(self, switch_port_name):\\n        port, found = self._get_switch_port_allocation(switch_port_name, False)\\n        if not found:\\n            return False\\n\\n        if not self._is_port_vm_started(port):\\n            return False\\n\\n        # all 4 meter ACLs must be existent first. (2 x direction)\\n        acls = port.associators(wmi_result_class=self._PORT_ALLOC_ACL_SET_DATA)\\n        acls = [a for a in acls if a.Action == self._ACL_ACTION_METER]\\n        if len(acls) < 2:\\n            return False\\n        return True\\n\\n    def _is_port_vm_started(self, port):\\n        vs_man_svc = self._conn.Msvm_VirtualSystemManagementService()[0]\\n        vmsettings = port.associators(\\n            wmi_result_class=self._VIRTUAL_SYSTEM_SETTING_DATA)\\n        #See http://msdn.microsoft.com/en-us/library/cc160706%28VS.85%29.aspx\\n        (ret_val, summary_info) = vs_man_svc.GetSummaryInformation(\\n            [self._VM_SUMMARY_ENABLED_STATE],\\n            [v.path_() for v in vmsettings])\\n        if ret_val or not summary_info:\\n            raise utils.HyperVException(msg=_(\'Cannot get VM summary data \'\\n                                              \'for: %s\') % port.ElementName)\\n\\n        return summary_info[0].EnabledState is self._HYPERV_VM_STATE_ENABLED\\n\\n    def create_security_rule(self, switch_port_name, direction, acl_type,\\n                             local_port, protocol, remote_address):\\n        port, found = self._get_switch_port_allocation(switch_port_name, False)\\n        if not found:\\n            return\\n\\n        # Add the ACLs only if they don\'t already exist\\n        acls = port.associators(wmi_result_class=self._PORT_EXT_ACL_SET_DATA)\\n        weight = self._get_new_weight(acls)\\n        self._bind_security_rule(\\n            port, direction, acl_type, self._ACL_ACTION_ALLOW, local_port,\\n            protocol, remote_address, weight)\\n\\n    def remove_security_rule(self, switch_port_name, direction, acl_type,\\n                             local_port, protocol, remote_address):\\n        port, found = self._get_switch_port_allocation(switch_port_name, False)\\n        if not found:\\n            # Port not found. It happens when the VM was already deleted.\\n            return\\n\\n        acls = port.associators(wmi_result_class=self._PORT_EXT_ACL_SET_DATA)\\n        filtered_acls = self._filter_security_acls(\\n            acls, self._ACL_ACTION_ALLOW, direction, acl_type, local_port,\\n            protocol, remote_address)\\n\\n        for acl in filtered_acls:\\n            self._remove_virt_feature(acl)\\n\\n    def remove_all_security_rules(self, switch_port_name):\\n        port, found = self._get_switch_port_allocation(switch_port_name, False)\\n        if not found:\\n            # Port not found. It happens when the VM was already deleted.\\n            return\\n\\n        acls = port.associators(wmi_result_class=self._PORT_EXT_ACL_SET_DATA)\\n        filtered_acls = [a for a in acls if\\n                         a.Action is not self._ACL_ACTION_METER]\\n\\n        if filtered_acls:\\n            self._remove_multiple_virt_features(filtered_acls)\\n\\n    def create_default_reject_all_rules(self, switch_port_name):\\n        port, found = self._get_switch_port_allocation(switch_port_name, False)\\n        if not found:\\n            raise utils.HyperVException(\\n                msg=_(\'Port Allocation not found: %s\') % switch_port_name)\\n\\n        acls = port.associators(wmi_result_class=self._PORT_EXT_ACL_SET_DATA)\\n        filtered_acls = [v for v in acls if v.Action == self._ACL_ACTION_DENY]\\n\\n        if len(filtered_acls) >= self._REJECT_ACLS_COUNT:\\n            return\\n\\n        for acl in filtered_acls:\\n            self._remove_virt_feature(acl)\\n\\n        weight = 0\\n        ipv4_pair = (self._ACL_TYPE_IPV4, self._IPV4_ANY)\\n        ipv6_pair = (self._ACL_TYPE_IPV6, self._IPV6_ANY)\\n        for direction in [self._ACL_DIR_IN, self._ACL_DIR_OUT]:\\n            for acl_type, address in [ipv4_pair, ipv6_pair]:\\n                for protocol in [self._TCP_PROTOCOL, self._UDP_PROTOCOL,\\n                                 self._ICMP_PROTOCOL, self._ICMPV6_PROTOCOL]:\\n                    self._bind_security_rule(\\n                        port, direction, acl_type, self._ACL_ACTION_DENY,\\n                        self._ACL_DEFAULT, protocol, address, weight)\\n                    weight += 1\\n\\n    def _bind_security_rule(self, port, direction, acl_type, action,\\n                            local_port, protocol, remote_address, weight):\\n        acls = port.associators(wmi_result_class=self._PORT_EXT_ACL_SET_DATA)\\n        filtered_acls = self._filter_security_acls(\\n            acls, action, direction, acl_type, local_port, protocol,\\n            remote_address)\\n\\n        for acl in filtered_acls:\\n            self._remove_virt_feature(acl)\\n\\n        acl = self._create_security_acl(\\n            direction, acl_type, action, local_port, protocol, remote_address,\\n            weight)\\n\\n        self._add_virt_feature(port, acl)\\n\\n    def _create_acl(self, direction, acl_type, action):\\n        acl = self._get_default_setting_data(self._PORT_ALLOC_ACL_SET_DATA)\\n        acl.set(Direction=direction,\\n                AclType=acl_type,\\n                Action=action,\\n                Applicability=self._ACL_APPLICABILITY_LOCAL)\\n        return acl\\n\\n    def _create_security_acl(self, direction, acl_type, action, local_port,\\n                             protocol, remote_ip_address, weight):\\n        acl = self._create_acl(direction, acl_type, action)\\n        (remote_address, remote_prefix_length) = remote_ip_address.split(\'/\')\\n        acl.set(Applicability=self._ACL_APPLICABILITY_REMOTE,\\n                RemoteAddress=remote_address,\\n                RemoteAddressPrefixLength=remote_prefix_length)\\n        return acl\\n\\n    def _filter_acls(self, acls, action, direction, acl_type, remote_addr=\\"\\"):\\n        return [v for v in acls\\n                if v.Action == action and\\n                v.Direction == direction and\\n                v.AclType == acl_type and\\n                v.RemoteAddress == remote_addr]\\n\\n    def _filter_security_acls(self, acls, acl_action, direction, acl_type,\\n                              local_port, protocol, remote_addr=\\"\\"):\\n        (remote_address, remote_prefix_length) = remote_addr.split(\'/\')\\n        remote_prefix_length = int(remote_prefix_length)\\n\\n        return [v for v in acls\\n                if v.Direction == direction and\\n                v.Action in [self._ACL_ACTION_ALLOW, self._ACL_ACTION_DENY] and\\n                v.AclType == acl_type and\\n                v.RemoteAddress == remote_address and\\n                v.RemoteAddressPrefixLength == remote_prefix_length]\\n\\n    def _get_new_weight(self, acls):\\n        return 0\\n\\n\\nclass HyperVUtilsV2R2(HyperVUtilsV2):\\n    _PORT_EXT_ACL_SET_DATA = \'Msvm_EthernetSwitchPortExtendedAclSettingData\'\\n    _MAX_WEIGHT = 65500\\n\\n    # 2 directions x 2 address types x 4 protocols = 16 ACLs\\n    _REJECT_ACLS_COUNT = 16\\n\\n    def create_security_rule(self, switch_port_name, direction, acl_type,\\n                             local_port, protocol, remote_address):\\n        if protocol is self._ACL_DEFAULT:\\n            protocols = [self._ICMP_PROTOCOL, self._ICMPV6_PROTOCOL,\\n                         self._TCP_PROTOCOL, self._UDP_PROTOCOL]\\n        else:\\n            protocols = [protocol]\\n\\n        for protocol in protocols:\\n            super(HyperVUtilsV2R2, self).create_security_rule(\\n                switch_port_name, direction, acl_type, local_port, protocol,\\n                remote_address)\\n\\n    def remove_security_rule(self, switch_port_name, direction, acl_type,\\n                             local_port, protocol, remote_address):\\n        if protocol is self._ACL_DEFAULT:\\n            protocols = [self._ICMP_PROTOCOL, self._ICMPV6_PROTOCOL,\\n                         self._TCP_PROTOCOL, self._UDP_PROTOCOL]\\n        else:\\n            protocols = [protocol]\\n\\n        for protocol in protocols:\\n            super(HyperVUtilsV2R2, self).remove_security_rule(\\n                switch_port_name, direction, acl_type, local_port, protocol,\\n                remote_address)\\n\\n    def _create_security_acl(self, direction, acl_type, action, local_port,\\n                             protocol, remote_addr, weight):\\n        acl = self._get_default_setting_data(self._PORT_EXT_ACL_SET_DATA)\\n        is_icmp = protocol in [self._ICMP_PROTOCOL, self._ICMPV6_PROTOCOL]\\n        acl.set(Direction=direction,\\n                Action=action,\\n                LocalPort=str(local_port) if not is_icmp else \'\',\\n                Protocol=protocol,\\n                RemoteIPAddress=remote_addr,\\n                IdleSessionTimeout=0,\\n                Stateful=(not is_icmp and action is not self._ACL_ACTION_DENY),\\n                Weight=weight)\\n        return acl\\n\\n    def _filter_security_acls(self, acls, action, direction, acl_type,\\n                              local_port, protocol, remote_addr=\\"\\"):\\n        return [v for v in acls\\n                if v.Action == action and\\n                v.Direction == direction and\\n                v.LocalPort == ((str(local_port))\\n                                if not protocol ==\\n                                self._ICMP_PROTOCOL else \'\')\\n                and\\n                v.Protocol == protocol and\\n                v.RemoteIPAddress == remote_addr]\\n\\n    def _get_new_weight(self, acls):\\n        acls = [a for a in acls if a.Action is not self._ACL_ACTION_DENY]\\n        if not acls:\\n            return self._MAX_WEIGHT - 1\\n\\n        weights = [a.Weight for a in acls]\\n        min_weight = min(weights)\\n        for weight in range(min_weight, self._MAX_WEIGHT):\\n            if weight not in weights:\\n                return weight\\n\\n        return min_weight - 1\\n" }\n'
line: b'{ "repo_name": "stamhe/bitcoin", "ref": "refs/heads/master-study", "path": "contrib/devtools/optimize-pngs.py", "content": "#!/usr/bin/env python\\n# Copyright (c) 2014-2017 The Bitcoin Core developers\\n# Distributed under the MIT software license, see the accompanying\\n# file COPYING or http://www.opensource.org/licenses/mit-license.php.\\n\'\'\'\\nRun this script every time you change one of the png files. Using pngcrush, it will optimize the png files, remove various color profiles, remove ancillary chunks (alla) and text chunks (text).\\n#pngcrush -brute -ow -rem gAMA -rem cHRM -rem iCCP -rem sRGB -rem alla -rem text\\n\'\'\'\\nimport os\\nimport sys\\nimport subprocess\\nimport hashlib\\nfrom PIL import Image\\n\\ndef file_hash(filename):\\n    \'\'\'Return hash of raw file contents\'\'\'\\n    with open(filename, \'rb\') as f:\\n        return hashlib.sha256(f.read()).hexdigest()\\n\\ndef content_hash(filename):\\n    \'\'\'Return hash of RGBA contents of image\'\'\'\\n    i = Image.open(filename)\\n    i = i.convert(\'RGBA\')\\n    data = i.tobytes()\\n    return hashlib.sha256(data).hexdigest()\\n\\npngcrush = \'pngcrush\'\\ngit = \'git\'\\nfolders = [\\"src/qt/res/movies\\", \\"src/qt/res/icons\\", \\"share/pixmaps\\"]\\nbasePath = subprocess.check_output([git, \'rev-parse\', \'--show-toplevel\']).rstrip(\'\\\\n\')\\ntotalSaveBytes = 0\\nnoHashChange = True\\n\\noutputArray = []\\nfor folder in folders:\\n    absFolder=os.path.join(basePath, folder)\\n    for file in os.listdir(absFolder):\\n        extension = os.path.splitext(file)[1]\\n        if extension.lower() == \'.png\':\\n            print(\\"optimizing \\"+file+\\"...\\"),\\n            file_path = os.path.join(absFolder, file)\\n            fileMetaMap = {\'file\' : file, \'osize\': os.path.getsize(file_path), \'sha256Old\' : file_hash(file_path)}\\n            fileMetaMap[\'contentHashPre\'] = content_hash(file_path)\\n        \\n            pngCrushOutput = \\"\\"\\n            try:\\n                pngCrushOutput = subprocess.check_output(\\n                        [pngcrush, \\"-brute\\", \\"-ow\\", \\"-rem\\", \\"gAMA\\", \\"-rem\\", \\"cHRM\\", \\"-rem\\", \\"iCCP\\", \\"-rem\\", \\"sRGB\\", \\"-rem\\", \\"alla\\", \\"-rem\\", \\"text\\", file_path],\\n                        stderr=subprocess.STDOUT).rstrip(\'\\\\n\')\\n            except:\\n                print \\"pngcrush is not installed, aborting...\\"\\n                sys.exit(0)\\n        \\n            #verify\\n            if \\"Not a PNG file\\" in subprocess.check_output([pngcrush, \\"-n\\", \\"-v\\", file_path], stderr=subprocess.STDOUT):\\n                print \\"PNG file \\"+file+\\" is corrupted after crushing, check out pngcursh version\\"\\n                sys.exit(1)\\n            \\n            fileMetaMap[\'sha256New\'] = file_hash(file_path)\\n            fileMetaMap[\'contentHashPost\'] = content_hash(file_path)\\n\\n            if fileMetaMap[\'contentHashPre\'] != fileMetaMap[\'contentHashPost\']:\\n                print \\"Image contents of PNG file \\"+file+\\" before and after crushing don\'t match\\"\\n                sys.exit(1)\\n\\n            fileMetaMap[\'psize\'] = os.path.getsize(file_path)\\n            outputArray.append(fileMetaMap)\\n            print(\\"done\\\\n\\"),\\n\\nprint \\"summary:\\\\n+++++++++++++++++\\"\\nfor fileDict in outputArray:\\n    oldHash = fileDict[\'sha256Old\']\\n    newHash = fileDict[\'sha256New\']\\n    totalSaveBytes += fileDict[\'osize\'] - fileDict[\'psize\']\\n    noHashChange = noHashChange and (oldHash == newHash)\\n    print fileDict[\'file\']+\\"\\\\n  size diff from: \\"+str(fileDict[\'osize\'])+\\" to: \\"+str(fileDict[\'psize\'])+\\"\\\\n  old sha256: \\"+oldHash+\\"\\\\n  new sha256: \\"+newHash+\\"\\\\n\\"\\n    \\nprint \\"completed. Checksum stable: \\"+str(noHashChange)+\\". Total reduction: \\"+str(totalSaveBytes)+\\" bytes\\"\\n" }\n'
line: b'{ "repo_name": "spblightadv/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "Simran-B/arangodb", "ref": "refs/heads/docs_3.0", "path": "3rdParty/V8-4.3.61/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "losywee/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "Suwmlee/XX-Net", "ref": "refs/heads/python3", "path": "Python3/lib/ctypes/test/test_keeprefs.py", "content": "from ctypes import *\\r\\nimport unittest\\r\\n\\r\\nclass SimpleTestCase(unittest.TestCase):\\r\\n    def test_cint(self):\\r\\n        x = c_int()\\r\\n        self.assertEqual(x._objects, None)\\r\\n        x.value = 42\\r\\n        self.assertEqual(x._objects, None)\\r\\n        x = c_int(99)\\r\\n        self.assertEqual(x._objects, None)\\r\\n\\r\\n    def test_ccharp(self):\\r\\n        x = c_char_p()\\r\\n        self.assertEqual(x._objects, None)\\r\\n        x.value = b\\"abc\\"\\r\\n        self.assertEqual(x._objects, b\\"abc\\")\\r\\n        x = c_char_p(b\\"spam\\")\\r\\n        self.assertEqual(x._objects, b\\"spam\\")\\r\\n\\r\\nclass StructureTestCase(unittest.TestCase):\\r\\n    def test_cint_struct(self):\\r\\n        class X(Structure):\\r\\n            _fields_ = [(\\"a\\", c_int),\\r\\n                        (\\"b\\", c_int)]\\r\\n\\r\\n        x = X()\\r\\n        self.assertEqual(x._objects, None)\\r\\n        x.a = 42\\r\\n        x.b = 99\\r\\n        self.assertEqual(x._objects, None)\\r\\n\\r\\n    def test_ccharp_struct(self):\\r\\n        class X(Structure):\\r\\n            _fields_ = [(\\"a\\", c_char_p),\\r\\n                        (\\"b\\", c_char_p)]\\r\\n        x = X()\\r\\n        self.assertEqual(x._objects, None)\\r\\n\\r\\n        x.a = b\\"spam\\"\\r\\n        x.b = b\\"foo\\"\\r\\n        self.assertEqual(x._objects, {\\"0\\": b\\"spam\\", \\"1\\": b\\"foo\\"})\\r\\n\\r\\n    def test_struct_struct(self):\\r\\n        class POINT(Structure):\\r\\n            _fields_ = [(\\"x\\", c_int), (\\"y\\", c_int)]\\r\\n        class RECT(Structure):\\r\\n            _fields_ = [(\\"ul\\", POINT), (\\"lr\\", POINT)]\\r\\n\\r\\n        r = RECT()\\r\\n        r.ul.x = 0\\r\\n        r.ul.y = 1\\r\\n        r.lr.x = 2\\r\\n        r.lr.y = 3\\r\\n        self.assertEqual(r._objects, None)\\r\\n\\r\\n        r = RECT()\\r\\n        pt = POINT(1, 2)\\r\\n        r.ul = pt\\r\\n        self.assertEqual(r._objects, {\'0\': {}})\\r\\n        r.ul.x = 22\\r\\n        r.ul.y = 44\\r\\n        self.assertEqual(r._objects, {\'0\': {}})\\r\\n        r.lr = POINT()\\r\\n        self.assertEqual(r._objects, {\'0\': {} \'1\': {}})\\r\\n\\r\\nclass ArrayTestCase(unittest.TestCase):\\r\\n    def test_cint_array(self):\\r\\n        INTARR = c_int * 3\\r\\n\\r\\n        ia = INTARR()\\r\\n        self.assertEqual(ia._objects, None)\\r\\n        ia[0] = 1\\r\\n        ia[1] = 2\\r\\n        ia[2] = 3\\r\\n        self.assertEqual(ia._objects, None)\\r\\n\\r\\n        class X(Structure):\\r\\n            _fields_ = [(\\"x\\", c_int),\\r\\n                        (\\"a\\", INTARR)]\\r\\n\\r\\n        x = X()\\r\\n        x.x = 1000\\r\\n        x.a[0] = 42\\r\\n        x.a[1] = 96\\r\\n        self.assertEqual(x._objects, None)\\r\\n        x.a = ia\\r\\n        self.assertEqual(x._objects, {\'1\': {}})\\r\\n\\r\\nclass PointerTestCase(unittest.TestCase):\\r\\n    def test_p_cint(self):\\r\\n        i = c_int(42)\\r\\n        x = pointer(i)\\r\\n        self.assertEqual(x._objects, {\'1\': i})\\r\\n\\r\\nclass DeletePointerTestCase(unittest.TestCase):\\r\\n    @unittest.skip(\'test disabled\')\\r\\n    def test_X(self):\\r\\n        class X(Structure):\\r\\n            _fields_ = [(\\"p\\", POINTER(c_char_p))]\\r\\n        x = X()\\r\\n        i = c_char_p(\\"abc def\\")\\r\\n        from sys import getrefcount as grc\\r\\n        print(\\"2?\\", grc(i))\\r\\n        x.p = pointer(i)\\r\\n        print(\\"3?\\", grc(i))\\r\\n        for i in range(320):\\r\\n            c_int(99)\\r\\n            x.p[0]\\r\\n        print(x.p[0])\\r\\n##        del x\\r\\n##        print \\"2?\\", grc(i)\\r\\n##        del i\\r\\n        import gc\\r\\n        gc.collect()\\r\\n        for i in range(320):\\r\\n            c_int(99)\\r\\n            x.p[0]\\r\\n        print(x.p[0])\\r\\n        print(x.p.contents)\\r\\n##        print x._objects\\r\\n\\r\\n        x.p[0] = \\"spam spam\\"\\r\\n##        print x.p[0]\\r\\n        print(\\"+\\" * 42)\\r\\n        print(x._objects)\\r\\n\\r\\nclass PointerToStructure(unittest.TestCase):\\r\\n    def test(self):\\r\\n        class POINT(Structure):\\r\\n            _fields_ = [(\\"x\\", c_int), (\\"y\\", c_int)]\\r\\n        class RECT(Structure):\\r\\n            _fields_ = [(\\"a\\", POINTER(POINT)),\\r\\n                        (\\"b\\", POINTER(POINT))]\\r\\n        r = RECT()\\r\\n        p1 = POINT(1, 2)\\r\\n\\r\\n        r.a = pointer(p1)\\r\\n        r.b = pointer(p1)\\r\\n##        from pprint import pprint as pp\\r\\n##        pp(p1._objects)\\r\\n##        pp(r._objects)\\r\\n\\r\\n        r.a[0].x = 42\\r\\n        r.a[0].y = 99\\r\\n\\r\\n        # to avoid leaking when tests are run several times\\r\\n        # clean up the types left in the cache.\\r\\n        from ctypes import _pointer_type_cache\\r\\n        del _pointer_type_cache[POINT]\\r\\n\\r\\nif __name__ == \\"__main__\\":\\r\\n    unittest.main()\\r\\n" }\n'
line: b'{ "repo_name": "sebadiaz/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "mosaic-cloud/mosaic-distribution-dependencies", "ref": "refs/heads/development", "path": "dependencies/nodejs/0.8.22/deps/npm/node_modules/node-gyp/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "sssemil/cjdns", "ref": "refs/heads/socket", "path": "node_build/dependencies/libuv/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "greyhwndz/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "victorbriz/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "oktayacikalin/pyglet", "ref": "refs/heads/pyglet-1.2-maintenance", "path": "contrib/projection/tests/projection/base_projection.py", "content": "#!/usr/bin/python\\n# $Id:$\\n\\nfrom pyglet.gl import *\\n\\ndef fillrect(x, y, width, height):\\n    glBegin(GL_QUADS)\\n    glVertex2f(x, y)\\n    glVertex2f(x + width, y)\\n    glVertex2f(x + width, y + height)\\n    glVertex2f(x, y + height)\\n    glEnd()\\n\\ndef rect(x, y, width, height):\\n    glBegin(GL_LINE_LOOP)\\n    glVertex2f(x, y)\\n    glVertex2f(x + width, y)\\n    glVertex2f(x + width, y + height)\\n    glVertex2f(x, y + height)\\n    glEnd()\\n" }\n'
line: b'{ "repo_name": "bsa11b/mtasa-blue", "ref": "refs/heads/1.4.1", "path": "vendor/google-breakpad/src/tools/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "livecd-tools/livecd-tools", "ref": "refs/heads/aarch64-liveiso", "path": "imgcreate/dnfinst.py", "content": "#\\n# dnfinst.py : dnf utilities\\n#\\n# Copyright 2007, Red Hat  Inc.\\n# Copyright 2016, Kevin Kofler\\n# Copyright 2016, Neal Gompa\\n#\\n# Portions from Anaconda dnfpayload.py\\n# DNF/rpm software payload management.\\n#\\n# Copyright (C) 2013-2015  Red Hat, Inc.\\n#\\n# This program is free software; you can redistribute it and/or modify\\n# it under the terms of the GNU General Public License as published by\\n# the Free Software Foundation; version 2 of the License.\\n#\\n# This program is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU Library General Public License for more details.\\n#\\n# You should have received a copy of the GNU General Public License\\n# along with this program; if not, write to the Free Software\\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.\\n\\nfrom __future__ import print_function\\nimport glob\\nimport os\\nimport sys\\nimport logging\\nimport itertools\\n\\nimport dnf\\nimport dnf.rpm\\n# FIXME: Why are these hidden inside dnf.cli? Any text-mode app should be able\\n#        to make use of these.\\nfrom dnf.cli.progress import MultiFileProgressMeter as DownloadProgress\\nfrom dnf.cli.output import CliTransactionDisplay as TransactionProgress\\nimport hawkey\\nfrom pykickstart.constants import GROUP_DEFAULT, GROUP_REQUIRED, GROUP_ALL\\n\\nfrom imgcreate.errors import *\\n\\nclass DnfLiveCD(dnf.Base):\\n    def __init__(self, releasever=None, useplugins=False):\\n        \\"\\"\\"\\n        releasever = optional value to use in replacing $releasever in repos\\n        \\"\\"\\"\\n        dnf.Base.__init__(self)\\n        self.releasever = releasever\\n        self.useplugins = useplugins\\n\\n    def doFileLogSetup(self, uid, logfile):\\n        # don\'t do the file log for the livecd as it can lead to open fds\\n        # being left and an inability to clean up after ourself\\n        pass\\n\\n    def close(self):\\n        try:\\n            os.unlink(self.conf.installroot + \\"/dnf.conf\\")\\n        except:\\n            pass\\n        dnf.Base.close(self)\\n\\n    def __del__(self):\\n        pass\\n\\n    def _writeConf(self, confpath, installroot):\\n        conf  = \\"[main]\\\\n\\"\\n        conf += \\"installroot=%s\\\\n\\" % installroot\\n        conf += \\"cachedir=/var/cache/dnf\\\\n\\"\\n        if self.useplugins:\\n            conf += \\"plugins=1\\\\n\\"\\n        else:\\n            conf += \\"plugins=0\\\\n\\"\\n        conf += \\"reposdir=/dev/null\\\\n\\"\\n        conf += \\"failovermethod=priority\\\\n\\"\\n        conf += \\"keepcache=1\\\\n\\"\\n        conf += \\"obsoletes=1\\\\n\\"\\n        conf += \\"best=1\\\\n\\"\\n        conf += \\"tsflags=nocontexts\\\\n\\"\\n\\n        f = open(confpath, \\"w+\\")\\n        f.write(conf)\\n        f.close()\\n\\n        os.chmod(confpath, 0o644)\\n\\n    def _cleanupRpmdbLocks(self, installroot):\\n        # cleans up temporary files left by bdb so that differing\\n        # versions of rpm don\'t cause problems\\n        for f in glob.glob(installroot + \\"/var/lib/rpm/__db*\\"):\\n            os.unlink(f)\\n\\n    def setup(self, confpath, installroot, cacheonly=False, excludeWeakdeps=False):\\n        self._writeConf(confpath, installroot)\\n        self._cleanupRpmdbLocks(installroot)\\n        self.conf.read(confpath)\\n        self.conf.installroot = installroot\\n        self.conf.prepend_installroot(\\"cachedir\\")\\n        self.conf.prepend_installroot(\\"persistdir\\")\\n        self.conf.install_weak_deps = not excludeWeakdeps\\n        if cacheonly:\\n            dnf.repo.Repo.DEFAULT_SYNC = dnf.repo.SYNC_ONLY_CACHE\\n        else:\\n            dnf.repo.Repo.DEFAULT_SYNC = dnf.repo.SYNC_TRY_CACHE\\n\\n    def deselectPackage(self, pkg):\\n        \\"\\"\\"Deselect a given package.  Can be specified with name.arch or name*\\"\\"\\"\\n        subj = dnf.subject.Subject(pkg)\\n        pkgs = subj.get_best_query(self.sack)\\n        # The only way to get expected behavior is to declare it\\n        # as excluded from the installable set\\n        return self.sack.add_excludes(pkgs)\\n\\n    def selectPackage(self, pkg):\\n        \\"\\"\\"Select a given package.  Can be specified with name.arch or name*\\"\\"\\"\\n        return self.install(pkg)\\n        \\n    def selectGroup(self, group_id, exclude, include = GROUP_DEFAULT):\\n        grp = self.comps.group_by_pattern(group_id)\\n        if grp is None:\\n            raise dnf.exceptions.MarkingError(\'no such group\', \'@\' + group_id)\\n        # default to getting mandatory and default packages from a group\\n        # unless we have specific options from kickstart\\n        package_types = {\'mandatory\', \'default\'}\\n        if include == GROUP_REQUIRED:\\n            package_types.remove(\'default\')\\n        elif include == GROUP_ALL:\\n            package_types.add(\'optional\')\\n        try:\\n            self.group_install(grp.id, tuple(package_types), exclude=exclude)\\n        except dnf.exceptions.CompsError as e:\\n            # DNF raises this when it is already selected\\n            pass\\n\\n    def environmentGroups(self, environmentid, optional=True):\\n        env = self.comps.environment_by_pattern(environmentid)\\n        if env is None:\\n            dnf.exceptions.MarkingError(\'no such environment\', \'@^\' + environmentid)\\n        group_ids = (id_.name for id_ in env.group_ids)\\n        option_ids = (id_.name for id_ in env.option_ids)\\n        if optional:\\n            return list(itertools.chain(group_ids, option_ids))\\n        else:\\n            return list(group_ids)\\n\\n    def selectEnvironment(self, env_id, excluded, excludedPkgs):\\n        # dnf.base.environment_install excludes on packages instead of groups,\\n        # which is unhelpful. Instead, use group_install for each group in\\n        # the environment so we can skip the ones that are excluded.\\n        for groupid in set(self.environmentGroups(env_id, optional=False)) - set(excluded):\\n            self.selectGroup(groupid, excludedPkgs)\\n\\n    def addRepository(self, name, url = None, mirrorlist = None):\\n        def _varSubstitute(option):\\n            # takes a variable and substitutes like dnf configs do\\n            arch = hawkey.detect_arch()\\n            option = option.replace(\\"$basearch\\", dnf.rpm.basearch(arch))\\n            option = option.replace(\\"$arch\\", arch)\\n            # If the url includes $releasever substitute user\'s value or\\n            # current system\'s version.\\n            if option.find(\\"$releasever\\") > -1:\\n                if self.releasever:\\n                    option = option.replace(\\"$releasever\\", self.releasever)\\n                else:\\n                    try:\\n                        detected_releasever = dnf.rpm.detect_releasever(\\"/\\")\\n                    except dnf.exceptions.Error:\\n                        detected_releasever = None\\n                    if detected_releasever:\\n                        option = option.replace(\\"$releasever\\", detected_releasever)\\n                    else:\\n                        raise CreatorError(\\"$releasever in repo url, but no releasever set\\")\\n            return option\\n\\n        try:\\n            # dnf 2\\n            repo = dnf.repo.Repo(name, parent_conf = self.conf)\\n        except TypeError as e:\\n            # dnf 1\\n            repo = dnf.repo.Repo(name, cachedir = self.conf.cachedir)\\n        if url:\\n            # some overly clever trickery in dnf 3 prevents us just\\n            # using repo.baseurl.append here:\\n            # https://bugzilla.redhat.com/show_bug.cgi?id=1595917\\n            # with the change to representing it as a tuple in DNF 3.6\\n            # this \'+= (tuple)\' approach seems to work for DNF 2,\\n            # 3.0-3.5 *and* 3.6\\n            repo.baseurl += (_varSubstitute(url),)\\n        if mirrorlist:\\n            repo.mirrorlist = _varSubstitute(mirrorlist)\\n        repo.enable()\\n        repo.set_progress_bar(DownloadProgress())\\n        self.repos.add(repo)\\n        return repo\\n\\n    def runInstall(self):\\n        import dnf.exceptions\\n        os.environ[\\"HOME\\"] = \\"/\\"\\n        try:\\n            res = self.resolve()\\n        except dnf.exceptions.RepoError as e:\\n            raise CreatorError(\\"Unable to download from repo : %s\\" %(e,))\\n        except dnf.exceptions.Error as e:\\n            raise CreatorError(\\"Failed to build transaction : %s\\" %(e,))\\n        # Empty transactions are generally fine, we might be rebuilding an\\n        # existing image with no packages added\\n        if not res:\\n            return True\\n\\n        dlpkgs = self.transaction.install_set\\n        self.download_packages(dlpkgs, DownloadProgress())\\n        # FIXME: sigcheck?\\n\\n        ret = self.do_transaction(TransactionProgress())\\n        print(\\"\\")\\n        self._cleanupRpmdbLocks(self.conf.installroot)\\n        return ret\\n" }\n'
line: b'{ "repo_name": "gavioto/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "theblacklion/pyglet", "ref": "refs/heads/pyglet-1.2-maintenance", "path": "contrib/projection/tests/projection/base_projection.py", "content": "#!/usr/bin/python\\n# $Id:$\\n\\nfrom pyglet.gl import *\\n\\ndef fillrect(x, y, width, height):\\n    glBegin(GL_QUADS)\\n    glVertex2f(x, y)\\n    glVertex2f(x + width, y)\\n    glVertex2f(x + width, y + height)\\n    glVertex2f(x, y + height)\\n    glEnd()\\n\\ndef rect(x, y, width, height):\\n    glBegin(GL_LINE_LOOP)\\n    glVertex2f(x, y)\\n    glVertex2f(x + width, y)\\n    glVertex2f(x + width, y + height)\\n    glVertex2f(x, y + height)\\n    glEnd()\\n" }\n'
line: b'{ "repo_name": "jesseditson/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "baslr/ArangoDB", "ref": "refs/heads/3.1-silent", "path": "3rdParty/V8/V8-5.0.71.39/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "rrampage/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "AntouanK/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "wojons/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "StephenKing/summerschool-2015-ryu", "ref": "refs/heads/summerschool-step2-complete", "path": "ryu/services/protocols/bgp/signals/base.py", "content": "import logging\\nLOG = logging.getLogger(\'bgpspeaker.signals.base\')\\n\\n\\nclass SignalBus(object):\\n    def __init__(self):\\n        self._listeners = {}\\n\\n    def emit_signal(self, identifier, data):\\n        identifier = _to_tuple(identifier)\\n        LOG.debug(\'SIGNAL: %s emited with data: %s \', identifier, data)\\n        for func, filter_func in self._listeners.get(identifier, []):\\n            if not filter_func or filter_func(data):\\n                func(identifier, data)\\n\\n    def register_listener(self, identifier, func, filter_func=None):\\n        identifier = _to_tuple(identifier)\\n        substrings = (identifier[:i] for i in xrange(1, len(identifier) + 1))\\n        for partial_id in substrings:\\n            self._listeners.setdefault(\\n                partial_id,\\n                []\\n            ).append((func, filter_func))\\n\\n    def unregister_all(self):\\n        self._listeners = {}\\n\\n\\ndef _to_tuple(tuple_or_not):\\n    if not isinstance(tuple_or_not, tuple):\\n        return (tuple_or_not, )\\n    else:\\n        return tuple_or_not\\n" }\n'
line: b'{ "repo_name": "mquandalle/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "Omegaphora/external_chromium_org_tools_gyp", "ref": "refs/heads/lp5.1", "path": "test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "mcanthony/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "Wilbeibi/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "ayumilong/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "yaolinz/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "ajose01/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "MIPS/external-chromium_org-tools-gyp", "ref": "refs/heads/dev-mips-jb-kitkat", "path": "test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "l0b0/cds-invenio-vengmark", "ref": "refs/heads/install-from-source", "path": "modules/bibclassify/lib/bibclassify_config.py", "content": "# -*- coding: utf-8 -*-\\n##\\n## This file is part of CDS Invenio.\\n## Copyright (C) 2002, 2003, 2004, 2005, 2006, 2007, 2008 CERN.\\n##\\n## CDS Invenio is free software; you can redistribute it and/or\\n## modify it under the terms of the GNU General Public License as\\n## published by the Free Software Foundation; either version 2 of the\\n## License, or (at your option) any later version.\\n##\\n## CDS Invenio is distributed in the hope that it will be useful, but\\n## WITHOUT ANY WARRANTY; without even the implied warranty of\\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n## General Public License for more details.\\n##\\n## You should have received a copy of the GNU General Public License\\n## along with CDS Invenio; if not, write to the Free Software Foundation, Inc.,\\n## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\\n\\n\\"\\"\\"\\nBibClassify configuration file.\\nWhen writing changes, please either delete the cached ontology in your\\ntemporary directory or use the rebuild-cache option in order to\\nregenerate the cached ontology.\\n\\nIf you want to change this configuration, we recommend to create a\\nlocal configuration file names \'bibclassify_config_local.py\' that\\ncontains the changes to apply.\\n\\"\\"\\"\\n\\nimport re\\n\\n# USER AGENT\\n\\nCFG_BIBCLASSIFY_USER_AGENT = \\"\\"\\n\\n# BIBCLASSIFY VARIABLES\\n\\n# Number of keywords that are output per default.\\nCFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER = 20\\n\\n# PARTIAL_TEXT\\n# Marks the part of the fulltext to keep when running a partial match.\\n# Each tuple contains the start and end percentages of a section.\\nCFG_BIBCLASSIFY_PARTIAL_TEXT = ((0, 20), (40, 60))\\n\\n# WORD TRANSFORMATIONS\\n\\n# BibClassify creates a regular expression for each label found in the\\n# ontology.\\n# If the keyword belongs in \'INVARIABLE_WORDS\', we return it whitout any\\n# change.\\n# If the keyword is found in \'EXCEPTIONS\', we return its attached\\n# regular expression.\\n# If the keyword is matched by a regular expression of\\n# \'UNCHANGE_REGULAR_EXPRESSIONS\', we return the keyword without any\\n# change.\\n# At last, we perform the sub method of Python\'s re module using the\\n# first element of the tuple as the regex and the second element as the\\n# replacement string.\\n\\n# Regular expressions found here have been originally based on\\n# Wikipedia\'s page on English plural.\\n# [http://en.wikipedia.org/wiki/English_plural]\\n\\nCFG_BIBCLASSIFY_INVARIABLE_WORDS = (\\"any\\", \\"big\\", \\"chi\\", \\"der\\", \\"eta\\", \\"few\\",\\n    \\"low\\", \\"new\\", \\"non\\", \\"off\\", \\"one\\", \\"out\\", \\"phi\\", \\"psi\\", \\"rho\\", \\"tau\\",\\n    \\"two\\", \\"van\\", \\"von\\", \\"hard\\", \\"weak\\", \\"four\\", \\"anti\\", \\"zero\\", \\"sinh\\",\\n    \\"open\\", \\"high\\", \\"data\\", \\"dark\\", \\"free\\", \\"flux\\", \\"fine\\", \\"final\\", \\"heavy\\",\\n    \\"strange\\")\\n\\nCFG_BIBCLASSIFY_EXCEPTIONS = {\\n    \\"aluminium\\": r\\"alumini?um\\",\\n    \\"aluminum\\": r\\"alumini?um\\",\\n    \\"analysis\\": r\\"analy[sz]is\\",\\n    \\"analyzis\\": r\\"analy[sz]is\\",\\n    \\"behavior\\": r\\"behaviou?rs?\\",\\n    \\"behaviour\\": r\\"behaviou?rs?\\",\\n    \\"color\\": r\\"colou?rs?\\",\\n    \\"colour\\": r\\"colou?rs?\\",\\n    \\"deflexion\\": r\\"defle(x|ct)ions?\\",\\n    \\"flavor\\": r\\"flavou?rs?\\",\\n    \\"flavour\\": r\\"flavou?rs?\\",\\n    \\"gas\\": r\\"gas(s?es)?\\",\\n    \\"lens\\": r\\"lens(es)?\\",\\n    \\"matrix\\": r\\"matri(x(es)?|ces)\\",\\n    \\"muon\\": r\\"muons?\\",\\n    \\"neutrino\\": r\\"neutrinos?\\",\\n    \\"reflexion\\": r\\"refle(x|ct)ions?\\",\\n    \\"ring\\": r\\"rings?\\",\\n    \\"status\\": r\\"status(es)?\\",\\n    \\"string\\": r\\"strings?\\",\\n    \\"sum\\": r\\"sums?\\",\\n    \\"vertex\\": r\\"vert(ex(es)?|ices)\\",\\n    \\"vortex\\": r\\"vort(ex(es)?|ices)\\",\\n  }\\n\\nCFG_BIBCLASSIFY_UNCHANGE_REGULAR_EXPRESSIONS = (\\n    re.compile(\\"[^e]ed$\\"),\\n    re.compile(\\"ics?$\\"),\\n    re.compile(\\"[io]s$\\"),\\n    re.compile(\\"ium$\\"),\\n    re.compile(\\"less$\\"),\\n    re.compile(\\"ous$\\"),\\n    )\\n\\n# IDEAS\\n# \\"al$\\" -> \\"al(ly)?\\"\\n\\nCFG_BIBCLASSIFY_GENERAL_REGULAR_EXPRESSIONS = (\\n    (re.compile(\\"ional\\"), r\\"ional(ly)?\\"),\\n    (re.compile(\\"([ae])n(ce|t)$\\"), r\\"\\\\1n(t|ces?)\\"),\\n    (re.compile(\\"og(ue)?$\\"), r\\"og(ue)?s?\\"),\\n    (re.compile(\\"([^aeiouyc])(re|er)$\\"), r\\"\\\\1(er|re)s?\\"),\\n    (re.compile(\\"([aeiouy])[sz]ation$\\"), r\\"\\\\1[zs]ations?\\"),\\n    (re.compile(\\"([aeiouy])[sz]ation$\\"), r\\"\\\\1[zs]ations?\\"),\\n    (re.compile(\\"([^aeiou])(y|ies)$\\"), r\\"\\\\1(y|ies)\\"),\\n    (re.compile(\\"o$\\"), r\\"o(e?s)?\\"),\\n    (re.compile(\\"(x|sh|ch|ss)$\\"), r\\"\\\\1(es)?\\"),\\n    (re.compile(\\"f$\\"), r\\"(f|ves)\\"),\\n    (re.compile(\\"ung$\\"), r\\"ung(en)?\\"),\\n    (re.compile(\\"([^aiouy])s$\\"), r\\"\\\\1s?\\"),\\n    (re.compile(\\"([^o])us$\\"), r\\"\\\\1(i|us(es)?)\\"),\\n    (re.compile(\\"um$\\"), r\\"(a|ums?)\\"),\\n    )\\n\\n# PUNCTUATION TRANSFORMATIONS\\n\\n# When building the regex pattern for each label of the ontology, ew also take\\n# care of the non-alpha characters. Thereafter are two sets of transformations.\\n# \'SEPARATORS\' contains the transformation for the non-alpha characters that\\n# can be found between two words.\\n# \'SYMBOLS\' contains punctuation that can be found at the end of a word.\\n# In both cases, it the separator is not found in the dictionaries, we return\\n# re.escape(separator)\\n\\nCFG_BIBCLASSIFY_SEPARATORS = {\\n    \\" \\": r\\"[\\\\s-]\\",\\n    \\"-\\": r\\"[\\\\s-]?\\",\\n    \\"/\\": r\\"[/\\\\s]?\\",\\n    \\"(\\": r\\"\\\\s?\\\\(\\",\\n    \\"*\\": r\\"[*\\\\s]?\\",\\n    \\"- \\": r\\"\\\\s?\\\\-\\\\s\\",\\n    \\"+ \\": r\\"\\\\s?\\\\+\\\\s\\",\\n  }\\n\\nCFG_BIBCLASSIFY_SYMBOLS = {\\n    \\"\'\\": r\\"\\\\s?\\\\\'\\",\\n  }\\n\\nCFG_BIBCLASSIFY_WORD_WRAP = \\"[^\\\\w-]%s[^\\\\w-]\\"\\n\\n# MATCHING\\n\\n# When searching for composite keywords, we allow two keywords separated by one\\n# of the component of \'VALID_SEPARATORS\' to form a composite keyword. These\\n# separators contain also the punctuation.\\n\\nCFG_BIBCLASSIFY_VALID_SEPARATORS = (\\n    \\"of\\", \\"of a\\", \\"of an\\", \\"of the\\", \\"of this\\", \\"of one\\", \\"of two\\", \\"of three\\",\\n    \\"of new\\", \\"of other\\",  \\"of many\\", \\"of both\\", \\"of these\\", \\"of each\\", \\"is\\"\\n    )\\n\\n# AUTHOR KEYWORDS\\n\\n# When looking for the keywords already defined in the document, we run the\\n# following set of regex.\\n\\nCFG_BIBCLASSIFY_AUTHOR_KW_START = \\\\\\n    re.compile(r\\"(?i)key[ -]*words?[a-z ]*[.:] *\\")\\n\\nCFG_BIBCLASSIFY_AUTHOR_KW_END = (\\n    re.compile(r\\"\\\\n\\"),\\n    re.compile(r\\"\\\\.\\\\W\\"),\\n    re.compile(r\\"\\\\sPACS\\"),\\n    re.compile(r\\"(?i)1[. ]*introduction\\\\W\\"),\\n    re.compile(r\\"(?i)mathematics subject classification\\\\W\\"),\\n    )\\n\\nCFG_BIBCLASSIFY_AUTHOR_KW_SEPARATION = re.compile(\\" ?; ?| ?, ?| ?- \\")\\n\\n" }\n'
line: b'{ "repo_name": "urandu/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "elkingtonmcb/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "yxl/emscripten-calligra-mobile", "ref": "refs/heads/calligra/2.8", "path": "3rdparty/google-breakpad/src/tools/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "tempbottle/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "matthaywardwebdesign/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" } \n'
line: b'{ "repo_name": "lenstr/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "KSanthanam/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "wkennington/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "4talesa/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "OCA/l10n-italy", "ref": "refs/heads/12.0", "path": "l10n_it_ricevute_bancarie/wizard/wizard_unsolved.py", "content": "# Copyright (C) 2012 Andrea Cometa.\\n# Email: info@andreacometa.it\\n# Web site: http://www.andreacometa.it\\n# Copyright (C) 2012 Associazione OpenERP Italia\\n# (<http://www.odoo-italia.org>).\\n# Copyright (C) 2012-2017 Lorenzo Battistini - Agile Business Group\\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl).\\n\\nfrom odoo import fields, models, _, api\\nfrom odoo.exceptions import UserError\\n\\n\\nclass RibaUnsolved(models.TransientModel):\\n\\n    @api.model\\n    def _get_unsolved_journal_id(self):\\n        return self.env[\\n            \'riba.configuration\'\\n        ].get_default_value_by_list_line(\'unsolved_journal_id\')\\n\\n    @api.model\\n    def _get_effects_account_id(self):\\n        return self.env[\\n            \'riba.configuration\'\\n        ].get_default_value_by_list_line(\'acceptance_account_id\')\\n\\n    @api.model\\n    def _get_effects_amount(self):\\n        if not self.env.context.get(\'active_id\', False):\\n            return False\\n        return self.env[\\n            \'riba.distinta.line\'\\n        ].browse(self.env.context[\'active_id\']).amount\\n\\n    @api.model\\n    def _get_riba_bank_account_id(self):\\n        return self.env[\\n            \'riba.configuration\'\\n        ].get_default_value_by_list_line(\'accreditation_account_id\')\\n\\n    @api.model\\n    def _get_overdue_effects_account_id(self):\\n        return self.env[\\n            \'riba.configuration\'\\n        ].get_default_value_by_list_line(\'overdue_effects_account_id\')\\n\\n    @api.model\\n    def _get_bank_account_id(self):\\n        return self.env[\\n            \'riba.configuration\'\\n        ].get_default_value_by_list_line(\'bank_account_id\')\\n\\n    @api.model\\n    def _get_bank_expense_account_id(self):\\n        return self.env[\\n            \'riba.configuration\'\\n        ].get_default_value_by_list_line(\'protest_charge_account_id\')\\n\\n    _name = \\"riba.unsolved\\"\\n    _description = \\"Manage Past Due C/Os\\"\\n    unsolved_journal_id = fields.Many2one(\\n        \'account.journal\', \'Past Due Journal\', domain=[(\'type\', \'=\', \'bank\')],\\n        default=_get_unsolved_journal_id)\\n    effects_account_id = fields.Many2one(\\n        \'account.account\', \'Bills Account\',\\n        domain=[(\'internal_type\', \'=\', \'receivable\')],\\n        default=_get_effects_account_id)\\n    effects_amount = fields.Float(\\n        \'Bills Amount\', default=_get_effects_amount)\\n    riba_bank_account_id = fields.Many2one(\\n        \'account.account\', \'C/O Account\',\\n        default=_get_riba_bank_account_id)\\n    riba_bank_amount = fields.Float(\\n        \'C/O Amount\', default=_get_effects_amount)\\n    overdue_effects_account_id = fields.Many2one(\\n        \'account.account\', \'Past Due Bills Account\',\\n        domain=[(\'internal_type\', \'=\', \'receivable\')],\\n        default=_get_overdue_effects_account_id)\\n    overdue_effects_amount = fields.Float(\\n        \'Past Due Bills Amount\', default=_get_effects_amount)\\n    bank_account_id = fields.Many2one(\\n        \'account.account\', \'A/C Bank Account\', domain=[(\\n            \'internal_type\', \'=\', \'liquidity\')],\\n        default=_get_bank_account_id)\\n    bank_amount = fields.Float(\'Withdrawn Amount\')\\n    bank_expense_account_id = fields.Many2one(\\n        \'account.account\', \'Bank Fees Account\',\\n        default=_get_bank_expense_account_id)\\n    expense_amount = fields.Float(\'Fees Amount\')\\n\\n    def skip(self):\\n        active_id = self.env.context.get(\'active_id\')\\n        if not active_id:\\n            raise UserError(_(\'No active ID found.\'))\\n        line_model = self.env[\'riba.distinta.line\']\\n        line = line_model.browse(active_id)\\n        line.state = \'unsolved\'\\n        line.distinta_id.state = \'unsolved\'\\n        return {\'type\': \'ir.actions.act_window_close\'}\\n\\n    def create_move(self):\\n        active_id = self.env.context.get(\'active_id\', False)\\n        if not active_id:\\n            raise UserError(_(\'No active ID found.\'))\\n        move_model = self.env[\'account.move\']\\n        invoice_model = self.env[\'account.invoice\']\\n        move_line_model = self.env[\'account.move.line\']\\n        distinta_line = self.env[\'riba.distinta.line\'].browse(active_id)\\n        wizard = self\\n        if (\\n            not wizard.unsolved_journal_id or\\n            not wizard.effects_account_id or\\n            not wizard.riba_bank_account_id or\\n            not wizard.overdue_effects_account_id or\\n            not wizard.bank_account_id or\\n            not wizard.bank_expense_account_id\\n        ):\\n            raise UserError(_(\'Every account is mandatory.\'))\\n        move_vals = {\\n            \'ref\': _(\'Past Due C/O %s - Line %s\') % (\\n                distinta_line.distinta_id.name, distinta_line.sequence),\\n            \'journal_id\': wizard.unsolved_journal_id.id,\\n            \'line_ids\': [\\n                (0, 0, {\\n                    \'name\':  _(\'Bills\'),\\n                    \'account_id\': wizard.effects_account_id.id,\\n                    \'partner_id\': distinta_line.partner_id.id,\\n                    \'credit\': wizard.effects_amount,\\n                    \'debit\': 0.0,\\n              }),\\n                (0, 0, {\\n                    \'name\':  _(\'C/O\'),\\n                    \'account_id\': wizard.riba_bank_account_id.id,\\n                    \'debit\': wizard.riba_bank_amount,\\n                    \'credit\': 0.0,\\n              }),\\n                (0, 0, {\\n                    \'name\':  _(\'Past Due Bills\'),\\n                    \'account_id\': wizard.overdue_effects_account_id.id,\\n                    \'debit\': wizard.overdue_effects_amount,\\n                    \'credit\': 0.0,\\n                    \'partner_id\': distinta_line.partner_id.id,\\n                    \'date_maturity\': distinta_line.due_date,\\n              }),\\n                (0, 0, {\\n                    \'name\':  _(\'A/C Bank\'),\\n                    \'account_id\': wizard.bank_account_id.id,\\n                    \'credit\': wizard.bank_amount,\\n                    \'debit\': 0.0,\\n              }),\\n            ]\\n      }\\n\\n        if wizard.expense_amount:\\n            move_vals[\'line_ids\'].append(\\n                (0, 0, {\\n                    \'name\': _(\'Bank Fee\'),\\n                    \'account_id\': wizard.bank_expense_account_id.id,\\n                    \'debit\': wizard.expense_amount,\\n                    \'credit\': 0.0,\\n              }),)\\n\\n        move = move_model.create(move_vals)\\n        move.post()\\n\\n        to_be_reconciled = []\\n        for move_line in move.line_ids:\\n            if move_line.account_id.id == wizard.overdue_effects_account_id.id:\\n                for riba_move_line in distinta_line.move_line_ids:\\n                    invoice_ids = []\\n                    if riba_move_line.move_line_id.invoice_id:\\n                        invoice_ids = [\\n                            riba_move_line.move_line_id.invoice_id.id\\n                        ]\\n                    elif riba_move_line.move_line_id.unsolved_invoice_ids:\\n                        invoice_ids = [\\n                            i.id for i in\\n                            riba_move_line.move_line_id.unsolved_invoice_ids\\n                        ]\\n                    invoice_model.browse(invoice_ids).write({\\n                        \'unsolved_move_line_ids\': [(4, move_line.id)],\\n                  })\\n            if move_line.account_id.id == wizard.effects_account_id.id:\\n                to_be_reconciled.append(move_line.id)\\n        for acceptance_move_line in distinta_line.acceptance_move_id.line_ids:\\n            if (\\n                acceptance_move_line.account_id.id ==\\n                wizard.effects_account_id.id\\n            ):\\n                to_be_reconciled.append(acceptance_move_line.id)\\n\\n        distinta_line.write({\\n            \'unsolved_move_id\': move.id,\\n            \'state\': \'unsolved\',\\n      })\\n        to_be_reconciled_lines = move_line_model.with_context(\\n          {\'unsolved_reconciliation\': True}).browse(to_be_reconciled)\\n        to_be_reconciled_lines.reconcile()\\n        distinta_line.distinta_id.state = \'unsolved\'\\n        return {\\n            \'name\': _(\'Past Due Entry\'),\\n            \'view_type\': \'form\',\\n            \'view_mode\': \'form\',\\n            \'res_model\': \'account.move\',\\n            \'type\': \'ir.actions.act_window\',\\n            \'target\': \'current\',\\n            \'res_id\': move.id or False,\\n      }\\n" }\n'
line: b'{ "repo_name": "kostaspl/SpiderMonkey38", "ref": "refs/heads/tmpbr", "path": "media/webrtc/trunk/tools/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "AlericInglewood/3p-google-breakpad", "ref": "refs/heads/singularity", "path": "src/tools/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "sbusso/rethinkdb", "ref": "refs/heads/next", "path": "external/v8_3.30.33.16/build/gyp/test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "AOSPU/external_chromium_org_tools_gyp", "ref": "refs/heads/android-5.0/py3", "path": "test/mac/gyptest-debuginfo.py", "content": "#!/usr/bin/env python\\n\\n# Copyright (c) 2011 Google Inc. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\n\\"\\"\\"\\nTests things related to debug information generation.\\n\\"\\"\\"\\n\\nimport TestGyp\\n\\nimport sys\\n\\nif sys.platform == \'darwin\':\\n  test = TestGyp.TestGyp(formats=[\'ninja\', \'make\', \'xcode\'])\\n\\n  test.run_gyp(\'test.gyp\', chdir=\'debuginfo\')\\n\\n  test.build(\'test.gyp\', test.ALL, chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'libnonbundle_shared_library.dylib.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_loadable_module.so.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'nonbundle_executable.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.built_file_must_exist(\'bundle_shared_library.framework.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'bundle_loadable_module.bundle.dSYM\',\\n                             chdir=\'debuginfo\')\\n  test.built_file_must_exist(\'My App.app.dSYM\',\\n                             chdir=\'debuginfo\')\\n\\n  test.pass_test()\\n" }\n'
line: b'{ "repo_name": "DIRACGrid/DIRAC", "ref": "refs/heads/integration", "path": "docs/setup.py", "content": "\\"\\"\\"Basic setuptools script for DIRACDocs.\\"\\"\\"\\nfrom __future__ import absolute_import\\nfrom __future__ import division\\nfrom __future__ import print_function\\nimport os\\nimport glob\\n\\n# Actual setuptools\\nfrom setuptools import setup, find_packages\\n\\n# Find the base dir where the setup.py lies\\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \'diracdoctools\'))\\n\\n# Take all the packages but the scripts and tests\\nALL_PACKAGES = find_packages(where=BASE_DIR, exclude=[\'*test*\'])\\n\\nPACKAGE_DIR = dict((\\"%s\\" % p, os.path.join(BASE_DIR, p.replace(\'.\', \'/\'))) for p in ALL_PACKAGES)\\n\\n# We rename the packages so that they contain diracdoctools\\nALL_PACKAGES = [\'diracdoctools.%s\' % p for p in ALL_PACKAGES]\\nALL_PACKAGES.insert(0, \'diracdoctools\')\\n\\nPACKAGE_DIR[\'diracdoctools\'] = BASE_DIR\\n# The scripts to be distributed\\nSCRIPTS = glob.glob(\'%s/scripts/*.py\' % BASE_DIR)\\n\\nsetup(\\n    name=\'diracdoctools\',\\n    version=\'6.19.2\',\\n    url=\'https://github.com/DIRACGRID/DIRAC/docs\',\\n    license=\'GPLv3\',\\n    package_dir=PACKAGE_DIR,\\n    packages=ALL_PACKAGES,\\n    scripts=SCRIPTS,\\n)\\n" }\n'
line: b'{ "repo_name": "DIRACGrid/DIRAC", "ref": "refs/heads/integration", "path": "src/DIRAC/Interfaces/scripts/dirac_admin_set_site_protocols.py", "content": "#! /usr/bin/env python\\n########################################################################\\n# File :    dirac-admin-set-site-protocols\\n# Author :  Stuart Paterson\\n########################################################################\\n\\"\\"\\"\\nDefined protocols for each SE for a given site.\\n\\nUsage:\\n  dirac-admin-set-site-protocols [options] ... Protocol ...\\n\\nArguments:\\n  Protocol: SE access protocol (mandatory)\\n\\nExample:\\n  $ dirac-admin-set-site-protocols --Site=LCG.IN2P3.fr SRM2\\n\\"\\"\\"\\nfrom __future__ import print_function\\nfrom __future__ import absolute_import\\nfrom __future__ import division\\n\\n__RCSID__ = \\"$Id$\\"\\n\\nimport DIRAC\\nfrom DIRAC.Core.Base import Script\\nfrom DIRAC.Core.Utilities.DIRACScript import DIRACScript\\n\\n\\n@DIRACScript()\\ndef main():\\n  Script.registerSwitch(\\"\\", \\"Site=\\", \\"Site for which protocols are to be set (mandatory)\\")\\n  Script.parseCommandLine(ignoreErrors=True)\\n\\n  site = None\\n  for switch in Script.getUnprocessedSwitches():\\n    if switch[0].lower() == \\"site\\":\\n      site = switch[1]\\n\\n  args = Script.getPositionalArgs()\\n\\n  if not site or not args:\\n    Script.showHelp(exitCode=1)\\n\\n  from DIRAC.Interfaces.API.DiracAdmin import DiracAdmin\\n  diracAdmin = DiracAdmin()\\n  exitCode = 0\\n  result = diracAdmin.setSiteProtocols(site, args, printOutput=True)\\n  if not result[\'OK\']:\\n    print(\'ERROR: %s\' % result[\'Message\'])\\n    exitCode = 2\\n\\n  DIRAC.exit(exitCode)\\n\\n\\nif __name__ == \\"__main__\\":\\n  main()\\n" }\n'
line: b'{ "repo_name": "FeliciaLim/oss-fuzz", "ref": "refs/heads/opus", "path": "infra/base-images/base-msan-builder/packages/openssl.py", "content": "#!/usr/bin/env python\\n# Copyright 2017 Google Inc.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#      http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n################################################################################\\n\\nimport os\\nimport shutil\\n\\nimport package\\n\\n\\ndef AddNoAsmArg(config_path):\\n  \\"\\"\\"Add --no-asm to config scripts.\\"\\"\\"\\n  shutil.move(config_path, config_path + \'.real\')\\n  with open(config_path, \'w\') as f:\\n    f.write(\\n        \'#!/bin/sh\\\\n\'\\n        \'%s.real no-asm \\"$@\\"\\\\n\' % config_path)\\n  os.chmod(config_path, 0755)\\n\\n\\nclass Package(package.Package):\\n  \\"\\"\\"openssl package.\\"\\"\\"\\n\\n  def __init__(self, apt_version):\\n    super(Package, self).__init__(\'openssl\', apt_version)\\n\\n  def PreBuild(self, source_directory, env, custom_bin_dir):\\n    AddNoAsmArg(os.path.join(source_directory, \'Configure\'))\\n    AddNoAsmArg(os.path.join(source_directory, \'config\'))\\n" }\n'
line: b'{ "repo_name": "googlefonts/oss-fuzz", "ref": "refs/heads/main", "path": "infra/base-images/base-sanitizer-libs-builder/packages/openssl.py", "content": "#!/usr/bin/env python\\n# Copyright 2017 Google Inc.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#      http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n################################################################################\\n\\nimport os\\nimport shutil\\n\\nimport package\\n\\n\\ndef AddNoAsmArg(config_path):\\n  \\"\\"\\"Add --no-asm to config scripts.\\"\\"\\"\\n  shutil.move(config_path, config_path + \'.real\')\\n  with open(config_path, \'w\') as f:\\n    f.write(\\n        \'#!/bin/sh\\\\n\'\\n        \'%s.real no-asm \\"$@\\"\\\\n\' % config_path)\\n  os.chmod(config_path, 0755)\\n\\n\\nclass Package(package.Package):\\n  \\"\\"\\"openssl package.\\"\\"\\"\\n\\n  def __init__(self, apt_version):\\n    super(Package, self).__init__(\'openssl\', apt_version)\\n\\n  def PreBuild(self, source_directory, env, custom_bin_dir):\\n    AddNoAsmArg(os.path.join(source_directory, \'Configure\'))\\n    AddNoAsmArg(os.path.join(source_directory, \'config\'))\\n" }\n'
line: b'{ "repo_name": "DirectXMan12/nova-hacking", "ref": "refs/heads/feature_novnc_krb", "path": "nova/api/openstack/compute/contrib/used_limits.py", "content": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\\n\\n# Copyright 2012 OpenStack Foundation\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License\\n\\nfrom nova.api.openstack import extensions\\nfrom nova.api.openstack import wsgi\\nfrom nova.api.openstack import xmlutil\\nfrom nova import quota\\n\\n\\nQUOTAS = quota.QUOTAS\\n\\n\\nXMLNS = \\"http://docs.openstack.org/compute/ext/used_limits/api/v1.1\\"\\nALIAS = \\"os-used-limits\\"\\nauthorize = extensions.soft_extension_authorizer(\'compute\', \'used_limits\')\\nauthorize_for_admin = extensions.extension_authorizer(\'compute\',\\n                                                      \'used_limits_for_admin\')\\n\\n\\nclass UsedLimitsTemplate(xmlutil.TemplateBuilder):\\n    def construct(self):\\n        root = xmlutil.TemplateElement(\'limits\', selector=\'limits\')\\n        root.set(\'{%s}usedLimits\' % XMLNS, \'%s:usedLimits\' % ALIAS)\\n        return xmlutil.SlaveTemplate(root, 1, nsmap={ALIAS: XMLNS})\\n\\n\\nclass UsedLimitsController(wsgi.Controller):\\n\\n    def __init__(self, ext_mgr):\\n        self.ext_mgr = ext_mgr\\n\\n    @staticmethod\\n    def _reserved(req):\\n        try:\\n            return int(req.GET[\'reserved\'])\\n        except (ValueError, KeyError):\\n            return False\\n\\n    @wsgi.extends\\n    def index(self, req, resp_obj):\\n        resp_obj.attach(xml=UsedLimitsTemplate())\\n        context = req.environ[\'nova.context\']\\n        project_id = self._project_id(context, req)\\n        quotas = QUOTAS.get_project_quotas(context, project_id, usages=True)\\n        quota_map = {\\n            \'totalRAMUsed\': \'ram\',\\n            \'totalCoresUsed\': \'cores\',\\n            \'totalInstancesUsed\': \'instances\',\\n            \'totalFloatingIpsUsed\': \'floating_ips\',\\n            \'totalSecurityGroupsUsed\': \'security_groups\',\\n      }\\n        used_limits = {}\\n        for display_name, quota in quota_map.iteritems():\\n            if quota in quotas:\\n                reserved = (quotas[quota][\'reserved\']\\n                            if self._reserved(req) else 0)\\n                used_limits[display_name] = quotas[quota][\'in_use\'] + reserved\\n\\n        resp_obj.obj[\'limits\'][\'absolute\'].update(used_limits)\\n\\n    def _project_id(self, context, req):\\n        if self.ext_mgr.is_loaded(\'os-used-limits-for-admin\'):\\n            if \'tenant_id\' in req.GET:\\n                tenant_id = req.GET.get(\'tenant_id\')\\n                target = {\\n                    \'project_id\': tenant_id,\\n                    \'user_id\': context.user_id\\n                  }\\n                authorize_for_admin(context, target=target)\\n                return tenant_id\\n        return context.project_id\\n\\n\\nclass Used_limits(extensions.ExtensionDescriptor):\\n    \\"\\"\\"Provide data on limited resources that are being used.\\"\\"\\"\\n\\n    name = \\"UsedLimits\\"\\n    alias = ALIAS\\n    namespace = XMLNS\\n    updated = \\"2012-07-13T00:00:00+00:00\\"\\n\\n    def get_controller_extensions(self):\\n        controller = UsedLimitsController(self.ext_mgr)\\n        limits_ext = extensions.ControllerExtension(self, \'limits\',\\n                                                    controller=controller)\\n        return [limits_ext]\\n" }\n'
line: b'{ "repo_name": "koying/SPMC", "ref": "refs/heads/spmc-krypton", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "plexinc/plex-home-theater-public", "ref": "refs/heads/pht-frodo", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "inn1983/XbmcSunxi", "ref": "refs/heads/teropi-rebased", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "warped-rudi/xbmc", "ref": "refs/heads/Gotham-13.1-AE-fixes", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "theeternalsw0rd/xbmc", "ref": "refs/heads/delta", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "Kwiboo/plex-home-theater", "ref": "refs/heads/rockchip-krypton", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "RasPlex/plex-home-theatre", "ref": "refs/heads/rasplex-1.0", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "vidonme/xbmc", "ref": "refs/heads/isengard.vidon", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "pwong-mapr/private-hue", "ref": "refs/heads/HUE-1096-abe", "path": "desktop/core/ext-py/Django-1.4.5/django/utils/encoding.py", "content": "import types\\nimport urllib\\nimport locale\\nimport datetime\\nimport codecs\\nfrom decimal import Decimal\\n\\nfrom django.utils.functional import Promise\\n\\nclass DjangoUnicodeDecodeError(UnicodeDecodeError):\\n    def __init__(self, obj, *args):\\n        self.obj = obj\\n        UnicodeDecodeError.__init__(self, *args)\\n\\n    def __str__(self):\\n        original = UnicodeDecodeError.__str__(self)\\n        return \'%s. You passed in %r (%s)\' % (original, self.obj,\\n                type(self.obj))\\n\\nclass StrAndUnicode(object):\\n    \\"\\"\\"\\n    A class whose __str__ returns its __unicode__ as a UTF-8 bytestring.\\n\\n    Useful as a mix-in.\\n    \\"\\"\\"\\n    def __str__(self):\\n        return self.__unicode__().encode(\'utf-8\')\\n\\ndef smart_unicode(s, encoding=\'utf-8\', strings_only=False, errors=\'strict\'):\\n    \\"\\"\\"\\n    Returns a unicode object representing \'s\'. Treats bytestrings using the\\n    \'encoding\' codec.\\n\\n    If strings_only is True, don\'t convert (some) non-string-like objects.\\n    \\"\\"\\"\\n    if isinstance(s, Promise):\\n        # The input is the result of a gettext_lazy() call.\\n        return s\\n    return force_unicode(s, encoding, strings_only, errors)\\n\\ndef is_protected_type(obj):\\n    \\"\\"\\"Determine if the object instance is of a protected type.\\n\\n    Objects of protected types are preserved as-is when passed to\\n    force_unicode(strings_only=True).\\n    \\"\\"\\"\\n    return isinstance(obj, (\\n        types.NoneType,\\n        int, long,\\n        datetime.datetime, datetime.date, datetime.time,\\n        float, Decimal)\\n    )\\n\\ndef force_unicode(s, encoding=\'utf-8\', strings_only=False, errors=\'strict\'):\\n    \\"\\"\\"\\n    Similar to smart_unicode, except that lazy instances are resolved to\\n    strings, rather than kept as lazy objects.\\n\\n    If strings_only is True, don\'t convert (some) non-string-like objects.\\n    \\"\\"\\"\\n    # Handle the common case first, saves 30-40% in performance when s\\n    # is an instance of unicode. This function gets called often in that\\n    # setting.\\n    if isinstance(s, unicode):\\n        return s\\n    if strings_only and is_protected_type(s):\\n        return s\\n    try:\\n        if not isinstance(s, basestring,):\\n            if hasattr(s, \'__unicode__\'):\\n                s = unicode(s)\\n            else:\\n                try:\\n                    s = unicode(str(s), encoding, errors)\\n                except UnicodeEncodeError:\\n                    if not isinstance(s, Exception):\\n                        raise\\n                    # If we get to here, the caller has passed in an Exception\\n                    # subclass populated with non-ASCII data without special\\n                    # handling to display as a string. We need to handle this\\n                    # without raising a further exception. We do an\\n                    # approximation to what the Exception\'s standard str()\\n                    # output should be.\\n                    s = u\' \'.join([force_unicode(arg, encoding, strings_only,\\n                            errors) for arg in s])\\n        elif not isinstance(s, unicode):\\n            # Note: We use .decode() here, instead of unicode(s, encoding,\\n            # errors), so that if s is a SafeString, it ends up being a\\n            # SafeUnicode at the end.\\n            s = s.decode(encoding, errors)\\n    except UnicodeDecodeError, e:\\n        if not isinstance(s, Exception):\\n            raise DjangoUnicodeDecodeError(s, *e.args)\\n        else:\\n            # If we get to here, the caller has passed in an Exception\\n            # subclass populated with non-ASCII bytestring data without a\\n            # working unicode method. Try to handle this without raising a\\n            # further exception by individually forcing the exception args\\n            # to unicode.\\n            s = u\' \'.join([force_unicode(arg, encoding, strings_only,\\n                    errors) for arg in s])\\n    return s\\n\\ndef smart_str(s, encoding=\'utf-8\', strings_only=False, errors=\'strict\'):\\n    \\"\\"\\"\\n    Returns a bytestring version of \'s\', encoded as specified in \'encoding\'.\\n\\n    If strings_only is True, don\'t convert (some) non-string-like objects.\\n    \\"\\"\\"\\n    if strings_only and isinstance(s, (types.NoneType, int)):\\n        return s\\n    if isinstance(s, Promise):\\n        return unicode(s).encode(encoding, errors)\\n    elif not isinstance(s, basestring):\\n        try:\\n            return str(s)\\n        except UnicodeEncodeError:\\n            if isinstance(s, Exception):\\n                # An Exception subclass containing non-ASCII data that doesn\'t\\n                # know how to print itself properly. We shouldn\'t raise a\\n                # further exception.\\n                return \' \'.join([smart_str(arg, encoding, strings_only,\\n                        errors) for arg in s])\\n            return unicode(s).encode(encoding, errors)\\n    elif isinstance(s, unicode):\\n        return s.encode(encoding, errors)\\n    elif s and encoding != \'utf-8\':\\n        return s.decode(\'utf-8\', errors).encode(encoding, errors)\\n    else:\\n        return s\\n\\ndef iri_to_uri(iri):\\n    \\"\\"\\"\\n    Convert an Internationalized Resource Identifier (IRI) portion to a URI\\n    portion that is suitable for inclusion in a URL.\\n\\n    This is the algorithm from section 3.1 of RFC 3987.  However, since we are\\n    assuming input is either UTF-8 or unicode already, we can simplify things a\\n    little from the full method.\\n\\n    Returns an ASCII string containing the encoded result.\\n    \\"\\"\\"\\n    # The list of safe characters here is constructed from the \\"reserved\\" and\\n    # \\"unreserved\\" characters specified in sections 2.2 and 2.3 of RFC 3986:\\n    #     reserved    = gen-delims / sub-delims\\n    #     gen-delims  = \\":\\" / \\"/\\" / \\"?\\" / \\"#\\" / \\"[\\" / \\"]\\" / \\"@\\"\\n    #     sub-delims  = \\"!\\" / \\"$\\" / \\"&\\" / \\"\'\\" / \\"(\\" / \\")\\"\\n    #                   / \\"*\\" / \\"+\\" / \\",\\" / \\";\\" / \\"=\\"\\n    #     unreserved  = ALPHA / DIGIT / \\"-\\" / \\".\\" / \\"_\\" / \\"~\\"\\n    # Of the unreserved characters, urllib.quote already considers all but\\n    # the ~ safe.\\n    # The % character is also added to the list of safe characters here, as the\\n    # end of section 3.1 of RFC 3987 specifically mentions that % must not be\\n    # converted.\\n    if iri is None:\\n        return iri\\n    return urllib.quote(smart_str(iri), safe=\\"/#%[]=:;$&()+,!?*@\'~\\")\\n\\ndef filepath_to_uri(path):\\n    \\"\\"\\"Convert an file system path to a URI portion that is suitable for\\n    inclusion in a URL.\\n\\n    We are assuming input is either UTF-8 or unicode already.\\n\\n    This method will encode certain chars that would normally be recognized as\\n    special chars for URIs.  Note that this method does not encode the \'\\n    character, as it is a valid character within URIs.  See\\n    encodeURIComponent() JavaScript function for more details.\\n\\n    Returns an ASCII string containing the encoded result.\\n    \\"\\"\\"\\n    if path is None:\\n        return path\\n    # I know about `os.sep` and `os.altsep` but I want to leave\\n    # some flexibility for hardcoding separators.\\n    return urllib.quote(smart_str(path).replace(\\"\\\\\\\\\\", \\"/\\"), safe=\\"/~!*()\'\\")\\n\\n# The encoding of the default system locale but falls back to the\\n# given fallback encoding if the encoding is unsupported by python or could\\n# not be determined.  See tickets #10335 and #5846\\ntry:\\n    DEFAULT_LOCALE_ENCODING = locale.getdefaultlocale()[1] or \'ascii\'\\n    codecs.lookup(DEFAULT_LOCALE_ENCODING)\\nexcept:\\n    DEFAULT_LOCALE_ENCODING = \'ascii\'\\n\\n# Forwards compatibility with Django 1.5\\n\\ndef python_2_unicode_compatible(klass):\\n    # Always use the Python 2 branch of the decorator here in Django 1.4\\n    klass.__unicode__ = klass.__str__\\n    klass.__str__ = lambda self: self.__unicode__().encode(\'utf-8\')\\n    return klass\\n\\nsmart_text = smart_unicode\\nforce_text = force_unicode\\nsmart_bytes = smart_str\\n" }\n'
line: b'{ "repo_name": "chadoe/xbmc", "ref": "refs/heads/personalmod", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "OptimusGREEN/OGMC", "ref": "refs/heads/Krypton", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "kodibrasil/xbmc", "ref": "refs/heads/Krypton_dsplayer", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "rellla/xbmca10", "ref": "refs/heads/stage/Frodo", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "dhongu/l10n-romania", "ref": "refs/heads/11.0", "path": "account_storno/models/account_journal.py", "content": "# Copyright 2011- Slobodni programi d.o.o.\\n# Copyright 2018 Forest and Biomass Romania\\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl.html).\\n\\nfrom odoo import fields, models\\n\\n\\nclass AccountJournal(models.Model):\\n    _inherit = \\"account.journal\\"\\n\\n    posting_policy = fields.Selection([(\'contra\', \'Contra (Standard Odoo)\'), (\'storno\', \'Storno (-)\')],\\n                                      default=\'contra\', string=\'Storno or Contra\', required=True,\\n                                      help=\\"Storno allows minus postings, Refunds are posted on the \\"\\n                                           \\"same journal with negative sign.\\\\n\\"\\n                                           \\"Contra doesn\'t allow negative posting. \\"\\n                                           \\"Refunds are posted by swaping credit and debit side.\\")\\n" }\n'
line: b'{ "repo_name": "atupone/xbmc", "ref": "refs/heads/omxplayerNeeds", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "zidootech/zidoo-kodi-15.2", "ref": "refs/heads/zidoo-15.2", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "apavlenko/opencv", "ref": "refs/heads/copyright_fixes", "path": "samples/python2/edge.py", "content": "#!/usr/bin/env python\\n\\n\'\'\'\\nThis sample demonstrates Canny edge detection.\\n\\nUsage:\\n  edge.py [<video source>]\\n\\n  Trackbars control edge thresholds.\\n\\n\'\'\'\\n\\nimport cv2\\n\\n# relative module\\nimport video\\n\\n# built-in module\\nimport sys\\n\\n\\nif __name__ == \'__main__\':\\n    print __doc__\\n\\n    try:\\n        fn = sys.argv[1]\\n    except:\\n        fn = 0\\n\\n    def nothing(*arg):\\n        pass\\n\\n    cv2.namedWindow(\'edge\')\\n    cv2.createTrackbar(\'thrs1\', \'edge\', 2000, 5000, nothing)\\n    cv2.createTrackbar(\'thrs2\', \'edge\', 4000, 5000, nothing)\\n\\n    cap = video.create_capture(fn)\\n    while True:\\n        flag, img = cap.read()\\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\\n        thrs1 = cv2.getTrackbarPos(\'thrs1\', \'edge\')\\n        thrs2 = cv2.getTrackbarPos(\'thrs2\', \'edge\')\\n        edge = cv2.Canny(gray, thrs1, thrs2, apertureSize=5)\\n        vis = img.copy()\\n        vis /= 2\\n        vis[edge != 0] = (0, 255, 0)\\n        cv2.imshow(\'edge\', vis)\\n        ch = cv2.waitKey(5) & 0xFF\\n        if ch == 27:\\n            break\\n    cv2.destroyAllWindows()\\n" }\n'
line: b'{ "repo_name": "bkury/OpenPHT", "ref": "refs/heads/openpht-1.7", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "Distrotech/xbmc", "ref": "refs/heads/distrotech-xbmc", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "garbear/xbmc-retroplayer", "ref": "refs/heads/retroplayer-15alpha1", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "kingvuplus/xbmc", "ref": "refs/heads/Vuplus_Gotham_Dev", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "asavah/xbmc", "ref": "refs/heads/fix-cscriptrunner", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "dburner/zidoo-kodi-14.2", "ref": "refs/heads/kodi-rebased", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "kenvac/odoo-hr", "ref": "refs/heads/10.0", "path": "hr_holiday_officer/tests/__init__.py", "content": "# -*- coding: utf-8 -*-\\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl).\\n\\nfrom . import test_holiday_flow\\n" }\n'
line: b'{ "repo_name": "OCA/partner-contact", "ref": "refs/heads/13.0", "path": "partner_firstname/tests/test_create.py", "content": "# Copyright 2015 Grupo ESOC Ingenier\xc3\xada de Servicios, S.L. - Jairo Llopis.\\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl.html).\\n\\n\\"\\"\\"Test default values for models.\\"\\"\\"\\n\\nfrom odoo.tests.common import TransactionCase\\n\\nfrom .base import MailInstalled\\n\\n\\nclass PersonCase(TransactionCase):\\n    \\"\\"\\"Test ``res.partner`` when it is a person.\\"\\"\\"\\n\\n    context = {\\"default_is_company\\": False}\\n    model = \\"res.partner\\"\\n\\n    def setUp(self):\\n        super(PersonCase, self).setUp()\\n        self.good_values = {\\"firstname\\": \\"N\xc3\xba\xc3\xb1ez\\", \\"lastname\\": \\"Fern\xc3\xa1n\\"}\\n        self.good_values[\\"name\\"] = \\"{} {}\\".format(\\n            self.good_values[\\"firstname\\"], self.good_values[\\"lastname\\"]\\n        )\\n        if \\"default_is_company\\" in self.context:\\n            self.good_values[\\"is_company\\"] = self.context[\\"default_is_company\\"]\\n        self.values = self.good_values.copy()\\n\\n    def tearDown(self):\\n        self.record = (\\n            self.env[self.model].with_context(self.context).create(self.values)\\n        )\\n        for key, value in self.good_values.items():\\n            self.assertEqual(self.record[key], value, \\"Checking key %s\\" % key)\\n\\n        super(PersonCase, self).tearDown()\\n\\n    def test_no_name(self):\\n        \\"\\"\\"Name is calculated.\\"\\"\\"\\n        del self.values[\\"name\\"]\\n\\n    def test_wrong_name_value(self):\\n        \\"\\"\\"Wrong name value is ignored, name is calculated.\\"\\"\\"\\n        self.values[\\"name\\"] = \\"B\xc3\x84D\\"\\n\\n    def test_wrong_name_context(self):\\n        \\"\\"\\"Wrong name context is ignored, name is calculated.\\"\\"\\"\\n        del self.values[\\"name\\"]\\n        self.context[\\"default_name\\"] = \\"B\xc3\x84D\\"\\n\\n    def test_wrong_name_value_and_context(self):\\n        \\"\\"\\"Wrong name value and context is ignored, name is calculated.\\"\\"\\"\\n        self.values[\\"name\\"] = \\"B\xc3\x84D1\\"\\n        self.context[\\"default_name\\"] = \\"B\xc3\x84D2\\"\\n\\n\\nclass CompanyCase(PersonCase):\\n    \\"\\"\\"Test ``res.partner`` when it is a company.\\"\\"\\"\\n\\n    context = {\\"default_is_company\\": True}\\n\\n    def setUp(self):\\n        super(CompanyCase, self).setUp()\\n        self.good_values.update(lastname=self.values[\\"name\\"], firstname=False)\\n        self.values = self.good_values.copy()\\n\\n\\nclass UserCase(PersonCase, MailInstalled):\\n    \\"\\"\\"Test ``res.users``.\\"\\"\\"\\n\\n    model = \\"res.users\\"\\n    context = {\\"default_login\\": \\"user@example.com\\"}\\n\\n    def tearDown(self):\\n        # Cannot create users if ``mail`` is installed\\n        if self.mail_installed():\\n            # Skip tests\\n            super(PersonCase, self).tearDown()\\n        else:\\n            # Run tests\\n            super(UserCase, self).tearDown()\\n" }\n'
line: b'{ "repo_name": "kodi-game/xbmc", "ref": "refs/heads/retroplayer-15.1", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "PatrickOReilly/scikit-learn", "ref": "refs/heads/monotonic-trees", "path": "sklearn/externals/joblib/func_inspect.py", "content": "\\"\\"\\"\\nMy own variation on function-specific inspect-like features.\\n\\"\\"\\"\\n\\n# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\\n# Copyright (c) 2009 Gael Varoquaux\\n# License: BSD Style, 3 clauses.\\n\\nfrom itertools import islice\\nimport inspect\\nimport warnings\\nimport re\\nimport os\\n\\nfrom ._compat import _basestring\\nfrom .logger import pformat\\nfrom ._memory_helpers import open_py_source\\nfrom ._compat import PY3_OR_LATER\\n\\n\\ndef get_func_code(func):\\n    \\"\\"\\" Attempts to retrieve a reliable function code hash.\\n\\n        The reason we don\'t use inspect.getsource is that it caches the\\n        source, whereas we want this to be modified on the fly when the\\n        function is modified.\\n\\n        Returns\\n        -------\\n        func_code: string\\n            The function code\\n        source_file: string\\n            The path to the file in which the function is defined.\\n        first_line: int\\n            The first line of the code in the source file.\\n\\n        Notes\\n        ------\\n        This function does a bit more magic than inspect, and is thus\\n        more robust.\\n    \\"\\"\\"\\n    source_file = None\\n    try:\\n        code = func.__code__\\n        source_file = code.co_filename\\n        if not os.path.exists(source_file):\\n            # Use inspect for lambda functions and functions defined in an\\n            # interactive shell, or in doctests\\n            source_code = \'\'.join(inspect.getsourcelines(func)[0])\\n            line_no = 1\\n            if source_file.startswith(\'<doctest \'):\\n                source_file, line_no = re.match(\\n                    \'\\\\<doctest (.*\\\\.rst)\\\\[(.*)\\\\]\\\\>\', source_file).groups()\\n                line_no = int(line_no)\\n                source_file = \'<doctest %s>\' % source_file\\n            return source_code, source_file, line_no\\n        # Try to retrieve the source code.\\n        with open_py_source(source_file) as source_file_obj:\\n            first_line = code.co_firstlineno\\n            # All the lines after the function definition:\\n            source_lines = list(islice(source_file_obj, first_line - 1, None))\\n        return \'\'.join(inspect.getblock(source_lines)), source_file, first_line\\n    except:\\n        # If the source code fails, we use the hash. This is fragile and\\n        # might change from one session to another.\\n        if hasattr(func, \'__code__\'):\\n            # Python 3.X\\n            return str(func.__code__.__hash__()), source_file, -1\\n        else:\\n            # Weird objects like numpy ufunc don\'t have __code__\\n            # This is fragile, as quite often the id of the object is\\n            # in the repr, so it might not persist across sessions,\\n            # however it will work for ufuncs.\\n            return repr(func), source_file, -1\\n\\n\\ndef _clean_win_chars(string):\\n    \\"\\"\\"Windows cannot encode some characters in filename.\\"\\"\\"\\n    import urllib\\n    if hasattr(urllib, \'quote\'):\\n        quote = urllib.quote\\n    else:\\n        # In Python 3, quote is elsewhere\\n        import urllib.parse\\n        quote = urllib.parse.quote\\n    for char in (\'<\', \'>\', \'!\', \':\', \'\\\\\\\\\'):\\n        string = string.replace(char, quote(char))\\n    return string\\n\\n\\ndef get_func_name(func, resolv_alias=True, win_characters=True):\\n    \\"\\"\\" Return the function import path (as a list of module names), and\\n        a name for the function.\\n\\n        Parameters\\n        ----------\\n        func: callable\\n            The func to inspect\\n        resolv_alias: boolean, optional\\n            If true, possible local aliases are indicated.\\n        win_characters: boolean, optional\\n            If true, substitute special characters using urllib.quote\\n            This is useful in Windows, as it cannot encode some filenames\\n    \\"\\"\\"\\n    if hasattr(func, \'__module__\'):\\n        module = func.__module__\\n    else:\\n        try:\\n            module = inspect.getmodule(func)\\n        except TypeError:\\n            if hasattr(func, \'__class__\'):\\n                module = func.__class__.__module__\\n            else:\\n                module = \'unknown\'\\n    if module is None:\\n        # Happens in doctests, eg\\n        module = \'\'\\n    if module == \'__main__\':\\n        try:\\n            filename = os.path.abspath(inspect.getsourcefile(func))\\n        except:\\n            filename = None\\n        if filename is not None:\\n            # mangling of full path to filename\\n            parts = filename.split(os.sep)\\n            if parts[-1].startswith(\'<ipython-input\'):\\n                # function is defined in an IPython session. The filename\\n                # will change with every new kernel instance. This hack\\n                # always returns the same filename\\n                parts[-1] = \'__ipython-input__\'\\n            filename = \'-\'.join(parts)\\n            if filename.endswith(\'.py\'):\\n                filename = filename[:-3]\\n            module = module + \'-\' + filename\\n    module = module.split(\'.\')\\n    if hasattr(func, \'func_name\'):\\n        name = func.func_name\\n    elif hasattr(func, \'__name__\'):\\n        name = func.__name__\\n    else:\\n        name = \'unknown\'\\n    # Hack to detect functions not defined at the module-level\\n    if resolv_alias:\\n        # TODO: Maybe add a warning here?\\n        if hasattr(func, \'func_globals\') and name in func.func_globals:\\n            if not func.func_globals[name] is func:\\n                name = \'%s-alias\' % name\\n    if inspect.ismethod(func):\\n        # We need to add the name of the class\\n        if hasattr(func, \'im_class\'):\\n            klass = func.im_class\\n            module.append(klass.__name__)\\n    if os.name == \'nt\' and win_characters:\\n        # Stupid windows can\'t encode certain characters in filenames\\n        name = _clean_win_chars(name)\\n        module = [_clean_win_chars(s) for s in module]\\n    return module, name\\n\\n\\ndef getfullargspec(func):\\n    \\"\\"\\"Compatibility function to provide inspect.getfullargspec in Python 2\\n\\n    This should be rewritten using a backport of Python 3 signature\\n    once we drop support for Python 2.6. We went for a simpler\\n    approach at the time of writing because signature uses OrderedDict\\n    which is not available in Python 2.6.\\n    \\"\\"\\"\\n    try:\\n        return inspect.getfullargspec(func)\\n    except AttributeError:\\n        arg_spec = inspect.getargspec(func)\\n        import collections\\n        tuple_fields = (\'args varargs varkw defaults kwonlyargs \'\\n                        \'kwonlydefaults annotations\')\\n        tuple_type = collections.namedtuple(\'FullArgSpec\', tuple_fields)\\n\\n        return tuple_type(args=arg_spec.args,\\n                          varargs=arg_spec.varargs,\\n                          varkw=arg_spec.keywords,\\n                          defaults=arg_spec.defaults,\\n                          kwonlyargs=[],\\n                          kwonlydefaults=None,\\n                          annotations={})\\n\\n\\ndef _signature_str(function_name, arg_spec):\\n    \\"\\"\\"Helper function to output a function signature\\"\\"\\"\\n    # inspect.formatargspec can not deal with the same\\n    # number of arguments in python 2 and 3\\n    arg_spec_for_format = arg_spec[:7 if PY3_OR_LATER else 4]\\n\\n    arg_spec_str = inspect.formatargspec(*arg_spec_for_format)\\n    return \'{0}{1}\'.format(function_name, arg_spec_str)\\n\\n\\ndef _function_called_str(function_name, args, kwargs):\\n    \\"\\"\\"Helper function to output a function call\\"\\"\\"\\n    template_str = \'{0}({1} {2})\'\\n\\n    args_str = repr(args)[1:-1]\\n    kwargs_str = \', \'.join(\'%s=%s\' % (k, v)\\n                           for k, v in kwargs.items())\\n    return template_str.format(function_name, args_str,\\n                               kwargs_str)\\n\\n\\ndef filter_args(func, ignore_lst, args=(), kwargs=dict()):\\n    \\"\\"\\" Filters the given args and kwargs using a list of arguments to\\n        ignore, and a function specification.\\n\\n        Parameters\\n        ----------\\n        func: callable\\n            Function giving the argument specification\\n        ignore_lst: list of strings\\n            List of arguments to ignore (either a name of an argument\\n            in the function spec, or \'*\', or \'**\')\\n        *args: list\\n            Positional arguments passed to the function.\\n        **kwargs: dict\\n            Keyword arguments passed to the function\\n\\n        Returns\\n        -------\\n        filtered_args: list\\n            List of filtered positional and keyword arguments.\\n    \\"\\"\\"\\n    args = list(args)\\n    if isinstance(ignore_lst, _basestring):\\n        # Catch a common mistake\\n        raise ValueError(\\n            \'ignore_lst must be a list of parameters to ignore \'\\n            \'%s (type %s) was given\' % (ignore_lst, type(ignore_lst)))\\n    # Special case for functools.partial objects\\n    if (not inspect.ismethod(func) and not inspect.isfunction(func)):\\n        if ignore_lst:\\n            warnings.warn(\'Cannot inspect object %s, ignore list will \'\\n                          \'not work.\' % func, stacklevel=2)\\n        return {\'*\': args, \'**\': kwargs}\\n    arg_spec = getfullargspec(func)\\n    arg_names = arg_spec.args + arg_spec.kwonlyargs\\n    arg_defaults = arg_spec.defaults or ()\\n    arg_defaults = arg_defaults + tuple(arg_spec.kwonlydefaults[k]\\n                                        for k in arg_spec.kwonlyargs)\\n    arg_varargs = arg_spec.varargs\\n    arg_varkw = arg_spec.varkw\\n\\n    if inspect.ismethod(func):\\n        # First argument is \'self\', it has been removed by Python\\n        # we need to add it back:\\n        args = [func.__self__, ] + args\\n    # XXX: Maybe I need an inspect.isbuiltin to detect C-level methods, such\\n    # as on ndarrays.\\n\\n    _, name = get_func_name(func, resolv_alias=False)\\n    arg_dict = dict()\\n    arg_position = -1\\n    for arg_position, arg_name in enumerate(arg_names):\\n        if arg_position < len(args):\\n            # Positional argument or keyword argument given as positional\\n            if arg_name not in arg_spec.kwonlyargs:\\n                arg_dict[arg_name] = args[arg_position]\\n            else:\\n                raise ValueError(\\n                    \\"Keyword-only parameter \'%s\' was passed as \\"\\n                    \'positional parameter for %s:\\\\n\'\\n                    \'     %s was called.\'\\n                    % (arg_name,\\n                       _signature_str(name, arg_spec),\\n                       _function_called_str(name, args, kwargs))\\n                )\\n\\n        else:\\n            position = arg_position - len(arg_names)\\n            if arg_name in kwargs:\\n                arg_dict[arg_name] = kwargs.pop(arg_name)\\n            else:\\n                try:\\n                    arg_dict[arg_name] = arg_defaults[position]\\n                except (IndexError, KeyError):\\n                    # Missing argument\\n                    raise ValueError(\\n                        \'Wrong number of arguments for %s:\\\\n\'\\n                        \'     %s was called.\'\\n                        % (_signature_str(name, arg_spec),\\n                           _function_called_str(name, args, kwargs))\\n                    )\\n\\n    varkwargs = dict()\\n    for arg_name, arg_value in sorted(kwargs.items()):\\n        if arg_name in arg_dict:\\n            arg_dict[arg_name] = arg_value\\n        elif arg_varkw is not None:\\n            varkwargs[arg_name] = arg_value\\n        else:\\n            raise TypeError(\\"Ignore list for %s() contains an unexpected \\"\\n                            \\"keyword argument \'%s\'\\" % (name, arg_name))\\n\\n    if arg_varkw is not None:\\n        arg_dict[\'**\'] = varkwargs\\n    if arg_varargs is not None:\\n        varargs = args[arg_position + 1:]\\n        arg_dict[\'*\'] = varargs\\n\\n    # Now remove the arguments to be ignored\\n    for item in ignore_lst:\\n        if item in arg_dict:\\n            arg_dict.pop(item)\\n        else:\\n            raise ValueError(\\"Ignore list: argument \'%s\' is not defined for \\"\\n                             \\"function %s\\"\\n                             % (item,\\n                                _signature_str(name, arg_spec))\\n                             )\\n    # XXX: Return a sorted list of pairs?\\n    return arg_dict\\n\\n\\ndef format_signature(func, *args, **kwargs):\\n    # XXX: Should this use inspect.formatargvalues/formatargspec?\\n    module, name = get_func_name(func)\\n    module = [m for m in module if m]\\n    if module:\\n        module.append(name)\\n        module_path = \'.\'.join(module)\\n    else:\\n        module_path = name\\n    arg_str = list()\\n    previous_length = 0\\n    for arg in args:\\n        arg = pformat(arg, indent=2)\\n        if len(arg) > 1500:\\n            arg = \'%s...\' % arg[:700]\\n        if previous_length > 80:\\n            arg = \'\\\\n%s\' % arg\\n        previous_length = len(arg)\\n        arg_str.append(arg)\\n    arg_str.extend([\'%s=%s\' % (v, pformat(i)) for v, i in kwargs.items()])\\n    arg_str = \', \'.join(arg_str)\\n\\n    signature = \'%s(%s)\' % (name, arg_str)\\n    return module_path, signature\\n\\n\\ndef format_call(func, args, kwargs, object_name=\\"Memory\\"):\\n    \\"\\"\\" Returns a nicely formatted statement displaying the function\\n        call with the given arguments.\\n    \\"\\"\\"\\n    path, signature = format_signature(func, *args, **kwargs)\\n    msg = \'%s\\\\n[%s] Calling %s...\\\\n%s\' % (80 * \'_\', object_name,\\n                                          path, signature)\\n    return msg\\n    # XXX: Not using logging framework\\n    # self.debug(msg)\\n" }\n'
line: b'{ "repo_name": "RasPlex/OpenPHT", "ref": "refs/heads/openpht-1.9", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "holzingerm/xbmc", "ref": "refs/heads/master_work", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "aracnoz/xbmc", "ref": "refs/heads/Krypton_dsplayer", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "xhbl/Kodi_dualaudio", "ref": "refs/heads/Krypton-DA", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "onetechgenius/XBMCast2TV", "ref": "refs/heads/Helix", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "kassak/xbmc", "ref": "refs/heads/retroplayer-15.1", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "norbusan/plex-home-theater-public", "ref": "refs/heads/pht-frodo", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "ibazzi/xbmc", "ref": "refs/heads/spmc-jarvis", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "sergiocorato/partner-contact", "ref": "refs/heads/10.0", "path": "partner_contact_gender/tests/__init__.py", "content": "# -*- coding: utf-8 -*-\\n# \xc2\xa9 2016 Therp BV <http://therp.nl>\\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl.html).\\nfrom . import test_partner_contact_gender\\n" }\n'
line: b'{ "repo_name": "jmarcet/kodi", "ref": "refs/heads/Isengard-amlogic", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "RPGOne/Skynet", "ref": "refs/heads/Miho", "path": "scikit-learn-0.18.1/sklearn/externals/joblib/func_inspect.py", "content": "\\"\\"\\"\\nMy own variation on function-specific inspect-like features.\\n\\"\\"\\"\\n\\n# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\\n# Copyright (c) 2009 Gael Varoquaux\\n# License: BSD Style, 3 clauses.\\n\\nfrom itertools import islice\\nimport inspect\\nimport warnings\\nimport re\\nimport os\\n\\nfrom ._compat import _basestring\\nfrom .logger import pformat\\nfrom ._memory_helpers import open_py_source\\nfrom ._compat import PY3_OR_LATER\\n\\n\\ndef get_func_code(func):\\n    \\"\\"\\" Attempts to retrieve a reliable function code hash.\\n\\n        The reason we don\'t use inspect.getsource is that it caches the\\n        source, whereas we want this to be modified on the fly when the\\n        function is modified.\\n\\n        Returns\\n        -------\\n        func_code: string\\n            The function code\\n        source_file: string\\n            The path to the file in which the function is defined.\\n        first_line: int\\n            The first line of the code in the source file.\\n\\n        Notes\\n        ------\\n        This function does a bit more magic than inspect, and is thus\\n        more robust.\\n    \\"\\"\\"\\n    source_file = None\\n    try:\\n        code = func.__code__\\n        source_file = code.co_filename\\n        if not os.path.exists(source_file):\\n            # Use inspect for lambda functions and functions defined in an\\n            # interactive shell, or in doctests\\n            source_code = \'\'.join(inspect.getsourcelines(func)[0])\\n            line_no = 1\\n            if source_file.startswith(\'<doctest \'):\\n                source_file, line_no = re.match(\\n                    \'\\\\<doctest (.*\\\\.rst)\\\\[(.*)\\\\]\\\\>\', source_file).groups()\\n                line_no = int(line_no)\\n                source_file = \'<doctest %s>\' % source_file\\n            return source_code, source_file, line_no\\n        # Try to retrieve the source code.\\n        with open_py_source(source_file) as source_file_obj:\\n            first_line = code.co_firstlineno\\n            # All the lines after the function definition:\\n            source_lines = list(islice(source_file_obj, first_line - 1, None))\\n        return \'\'.join(inspect.getblock(source_lines)), source_file, first_line\\n    except:\\n        # If the source code fails, we use the hash. This is fragile and\\n        # might change from one session to another.\\n        if hasattr(func, \'__code__\'):\\n            # Python 3.X\\n            return str(func.__code__.__hash__()), source_file, -1\\n        else:\\n            # Weird objects like numpy ufunc don\'t have __code__\\n            # This is fragile, as quite often the id of the object is\\n            # in the repr, so it might not persist across sessions,\\n            # however it will work for ufuncs.\\n            return repr(func), source_file, -1\\n\\n\\ndef _clean_win_chars(string):\\n    \\"\\"\\"Windows cannot encode some characters in filename.\\"\\"\\"\\n    import urllib\\n    if hasattr(urllib, \'quote\'):\\n        quote = urllib.quote\\n    else:\\n        # In Python 3, quote is elsewhere\\n        import urllib.parse\\n        quote = urllib.parse.quote\\n    for char in (\'<\', \'>\', \'!\', \':\', \'\\\\\\\\\'):\\n        string = string.replace(char, quote(char))\\n    return string\\n\\n\\ndef get_func_name(func, resolv_alias=True, win_characters=True):\\n    \\"\\"\\" Return the function import path (as a list of module names), and\\n        a name for the function.\\n\\n        Parameters\\n        ----------\\n        func: callable\\n            The func to inspect\\n        resolv_alias: boolean, optional\\n            If true, possible local aliases are indicated.\\n        win_characters: boolean, optional\\n            If true, substitute special characters using urllib.quote\\n            This is useful in Windows, as it cannot encode some filenames\\n    \\"\\"\\"\\n    if hasattr(func, \'__module__\'):\\n        module = func.__module__\\n    else:\\n        try:\\n            module = inspect.getmodule(func)\\n        except TypeError:\\n            if hasattr(func, \'__class__\'):\\n                module = func.__class__.__module__\\n            else:\\n                module = \'unknown\'\\n    if module is None:\\n        # Happens in doctests, eg\\n        module = \'\'\\n    if module == \'__main__\':\\n        try:\\n            filename = os.path.abspath(inspect.getsourcefile(func))\\n        except:\\n            filename = None\\n        if filename is not None:\\n            # mangling of full path to filename\\n            parts = filename.split(os.sep)\\n            if parts[-1].startswith(\'<ipython-input\'):\\n                # function is defined in an IPython session. The filename\\n                # will change with every new kernel instance. This hack\\n                # always returns the same filename\\n                parts[-1] = \'__ipython-input__\'\\n            filename = \'-\'.join(parts)\\n            if filename.endswith(\'.py\'):\\n                filename = filename[:-3]\\n            module = module + \'-\' + filename\\n    module = module.split(\'.\')\\n    if hasattr(func, \'func_name\'):\\n        name = func.func_name\\n    elif hasattr(func, \'__name__\'):\\n        name = func.__name__\\n    else:\\n        name = \'unknown\'\\n    # Hack to detect functions not defined at the module-level\\n    if resolv_alias:\\n        # TODO: Maybe add a warning here?\\n        if hasattr(func, \'func_globals\') and name in func.func_globals:\\n            if not func.func_globals[name] is func:\\n                name = \'%s-alias\' % name\\n    if inspect.ismethod(func):\\n        # We need to add the name of the class\\n        if hasattr(func, \'im_class\'):\\n            klass = func.im_class\\n            module.append(klass.__name__)\\n    if os.name == \'nt\' and win_characters:\\n        # Stupid windows can\'t encode certain characters in filenames\\n        name = _clean_win_chars(name)\\n        module = [_clean_win_chars(s) for s in module]\\n    return module, name\\n\\n\\ndef getfullargspec(func):\\n    \\"\\"\\"Compatibility function to provide inspect.getfullargspec in Python 2\\n\\n    This should be rewritten using a backport of Python 3 signature\\n    once we drop support for Python 2.6. We went for a simpler\\n    approach at the time of writing because signature uses OrderedDict\\n    which is not available in Python 2.6.\\n    \\"\\"\\"\\n    try:\\n        return inspect.getfullargspec(func)\\n    except AttributeError:\\n        arg_spec = inspect.getargspec(func)\\n        import collections\\n        tuple_fields = (\'args varargs varkw defaults kwonlyargs \'\\n                        \'kwonlydefaults annotations\')\\n        tuple_type = collections.namedtuple(\'FullArgSpec\', tuple_fields)\\n\\n        return tuple_type(args=arg_spec.args,\\n                          varargs=arg_spec.varargs,\\n                          varkw=arg_spec.keywords,\\n                          defaults=arg_spec.defaults,\\n                          kwonlyargs=[],\\n                          kwonlydefaults=None,\\n                          annotations={})\\n\\n\\ndef _signature_str(function_name, arg_spec):\\n    \\"\\"\\"Helper function to output a function signature\\"\\"\\"\\n    # inspect.formatargspec can not deal with the same\\n    # number of arguments in python 2 and 3\\n    arg_spec_for_format = arg_spec[:7 if PY3_OR_LATER else 4]\\n\\n    arg_spec_str = inspect.formatargspec(*arg_spec_for_format)\\n    return \'{0}{1}\'.format(function_name, arg_spec_str)\\n\\n\\ndef _function_called_str(function_name, args, kwargs):\\n    \\"\\"\\"Helper function to output a function call\\"\\"\\"\\n    template_str = \'{0}({1} {2})\'\\n\\n    args_str = repr(args)[1:-1]\\n    kwargs_str = \', \'.join(\'%s=%s\' % (k, v)\\n                           for k, v in kwargs.items())\\n    return template_str.format(function_name, args_str,\\n                               kwargs_str)\\n\\n\\ndef filter_args(func, ignore_lst, args=(), kwargs=dict()):\\n    \\"\\"\\" Filters the given args and kwargs using a list of arguments to\\n        ignore, and a function specification.\\n\\n        Parameters\\n        ----------\\n        func: callable\\n            Function giving the argument specification\\n        ignore_lst: list of strings\\n            List of arguments to ignore (either a name of an argument\\n            in the function spec, or \'*\', or \'**\')\\n        *args: list\\n            Positional arguments passed to the function.\\n        **kwargs: dict\\n            Keyword arguments passed to the function\\n\\n        Returns\\n        -------\\n        filtered_args: list\\n            List of filtered positional and keyword arguments.\\n    \\"\\"\\"\\n    args = list(args)\\n    if isinstance(ignore_lst, _basestring):\\n        # Catch a common mistake\\n        raise ValueError(\\n            \'ignore_lst must be a list of parameters to ignore \'\\n            \'%s (type %s) was given\' % (ignore_lst, type(ignore_lst)))\\n    # Special case for functools.partial objects\\n    if (not inspect.ismethod(func) and not inspect.isfunction(func)):\\n        if ignore_lst:\\n            warnings.warn(\'Cannot inspect object %s, ignore list will \'\\n                          \'not work.\' % func, stacklevel=2)\\n        return {\'*\': args, \'**\': kwargs}\\n    arg_spec = getfullargspec(func)\\n    arg_names = arg_spec.args + arg_spec.kwonlyargs\\n    arg_defaults = arg_spec.defaults or ()\\n    arg_defaults = arg_defaults + tuple(arg_spec.kwonlydefaults[k]\\n                                        for k in arg_spec.kwonlyargs)\\n    arg_varargs = arg_spec.varargs\\n    arg_varkw = arg_spec.varkw\\n\\n    if inspect.ismethod(func):\\n        # First argument is \'self\', it has been removed by Python\\n        # we need to add it back:\\n        args = [func.__self__, ] + args\\n    # XXX: Maybe I need an inspect.isbuiltin to detect C-level methods, such\\n    # as on ndarrays.\\n\\n    _, name = get_func_name(func, resolv_alias=False)\\n    arg_dict = dict()\\n    arg_position = -1\\n    for arg_position, arg_name in enumerate(arg_names):\\n        if arg_position < len(args):\\n            # Positional argument or keyword argument given as positional\\n            if arg_name not in arg_spec.kwonlyargs:\\n                arg_dict[arg_name] = args[arg_position]\\n            else:\\n                raise ValueError(\\n                    \\"Keyword-only parameter \'%s\' was passed as \\"\\n                    \'positional parameter for %s:\\\\n\'\\n                    \'     %s was called.\'\\n                    % (arg_name,\\n                       _signature_str(name, arg_spec),\\n                       _function_called_str(name, args, kwargs))\\n                )\\n\\n        else:\\n            position = arg_position - len(arg_names)\\n            if arg_name in kwargs:\\n                arg_dict[arg_name] = kwargs.pop(arg_name)\\n            else:\\n                try:\\n                    arg_dict[arg_name] = arg_defaults[position]\\n                except (IndexError, KeyError):\\n                    # Missing argument\\n                    raise ValueError(\\n                        \'Wrong number of arguments for %s:\\\\n\'\\n                        \'     %s was called.\'\\n                        % (_signature_str(name, arg_spec),\\n                           _function_called_str(name, args, kwargs))\\n                    )\\n\\n    varkwargs = dict()\\n    for arg_name, arg_value in sorted(kwargs.items()):\\n        if arg_name in arg_dict:\\n            arg_dict[arg_name] = arg_value\\n        elif arg_varkw is not None:\\n            varkwargs[arg_name] = arg_value\\n        else:\\n            raise TypeError(\\"Ignore list for %s() contains an unexpected \\"\\n                            \\"keyword argument \'%s\'\\" % (name, arg_name))\\n\\n    if arg_varkw is not None:\\n        arg_dict[\'**\'] = varkwargs\\n    if arg_varargs is not None:\\n        varargs = args[arg_position + 1:]\\n        arg_dict[\'*\'] = varargs\\n\\n    # Now remove the arguments to be ignored\\n    for item in ignore_lst:\\n        if item in arg_dict:\\n            arg_dict.pop(item)\\n        else:\\n            raise ValueError(\\"Ignore list: argument \'%s\' is not defined for \\"\\n                             \\"function %s\\"\\n                             % (item,\\n                                _signature_str(name, arg_spec))\\n                             )\\n    # XXX: Return a sorted list of pairs?\\n    return arg_dict\\n\\n\\ndef format_signature(func, *args, **kwargs):\\n    # XXX: Should this use inspect.formatargvalues/formatargspec?\\n    module, name = get_func_name(func)\\n    module = [m for m in module if m]\\n    if module:\\n        module.append(name)\\n        module_path = \'.\'.join(module)\\n    else:\\n        module_path = name\\n    arg_str = list()\\n    previous_length = 0\\n    for arg in args:\\n        arg = pformat(arg, indent=2)\\n        if len(arg) > 1500:\\n            arg = \'%s...\' % arg[:700]\\n        if previous_length > 80:\\n            arg = \'\\\\n%s\' % arg\\n        previous_length = len(arg)\\n        arg_str.append(arg)\\n    arg_str.extend([\'%s=%s\' % (v, pformat(i)) for v, i in kwargs.items()])\\n    arg_str = \', \'.join(arg_str)\\n\\n    signature = \'%s(%s)\' % (name, arg_str)\\n    return module_path, signature\\n\\n\\ndef format_call(func, args, kwargs, object_name=\\"Memory\\"):\\n    \\"\\"\\" Returns a nicely formatted statement displaying the function\\n        call with the given arguments.\\n    \\"\\"\\"\\n    path, signature = format_signature(func, *args, **kwargs)\\n    msg = \'%s\\\\n[%s] Calling %s...\\\\n%s\' % (80 * \'_\', object_name,\\n                                          path, signature)\\n    return msg\\n    # XXX: Not using logging framework\\n    # self.debug(msg)\\n" }\n'
line: b'{ "repo_name": "Shine-/xbmc", "ref": "refs/heads/Krypton_alwaysontop", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "EmbER-Dev/Kodi", "ref": "refs/heads/kodi-17.6", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "PIPplware/xbmc", "ref": "refs/heads/leia_backports", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "Owersun/xbmc", "ref": "refs/heads/Krypton", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "be-cloud-be/horizon-addons", "ref": "refs/heads/9.0", "path": "partner-contact/partner_contact_gender/tests/__init__.py", "content": "# -*- coding: utf-8 -*-\\n# \xc2\xa9 2016 Therp BV <http://therp.nl>\\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl.html).\\nfrom . import test_partner_contact_gender\\n" }\n'
line: b'{ "repo_name": "Stane1983/xbmc-Gotham-aml", "ref": "refs/heads/13.2-Gotham", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "JamesLinEngineer/RKMC", "ref": "refs/heads/Jarvis", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "koying/xbmc", "ref": "refs/heads/master-krypton", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "gripped2/xbmc", "ref": "refs/heads/Isengard", "path": "lib/libUPnP/Neptune/Extras/Tools/Testing/MakeUrlList.py", "content": "import urllib2\\nimport simplejson\\n\\n# This example request includes an optional API key which you will need to\\nwords = open(\'/usr/share/dict/words\').readlines()\\nfor word in words:\\n\\tword = word.rstrip()\\n\\turl = (\'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&q=\'+word+\'+https&userip=67.169.84.240\')\\n\\t#print url\\n\\t#print word.rstrip()\\n\\trequest = urllib2.Request(url, None, {\'Referer\': \'http://test.com\'})\\n\\tresponse = urllib2.urlopen(request)\\n\\n\\tresults = simplejson.load(response)\\n\\tfor result in results[\'responseData\'][\'results\']:\\n\\t\\tprint result[\'unescapedUrl\']\\n" }\n'
line: b'{ "repo_name": "asomya/test", "ref": "refs/heads/quantum-integration", "path": "horizon/dashboards/nova/instances_and_volumes/instances/urls.py", "content": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\\n\\n# Copyright 2012 United States Government as represented by the\\n# Administrator of the National Aeronautics and Space Administration.\\n# All Rights Reserved.\\n#\\n# Copyright 2012 Nebula, Inc.\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\nfrom django.conf.urls.defaults import patterns, url\\n\\nfrom .views import UpdateView, DetailView\\n\\n\\nINSTANCES = r\'^(?P<instance_id>[^/]+)/%s$\'\\n\\n\\nurlpatterns = patterns(\\n    \'horizon.dashboards.nova.instances_and_volumes.instances.views\',\\n    url(INSTANCES % \'detail\', DetailView.as_view(), name=\'detail\'),\\n    url(INSTANCES % \'console\', \'console\', name=\'console\'),\\n    url(INSTANCES % \'vnc\', \'vnc\', name=\'vnc\'),\\n    url(INSTANCES % \'update\', UpdateView.as_view(), name=\'update\'),\\n)\\n" }\n'
line: b'{ "repo_name": "steveandroulakis/mytardis", "ref": "refs/heads/3.0", "path": "tardis/tardis_portal/models/license.py", "content": "from django.db import models\\n\\nfrom itertools import chain\\nimport logging\\nlogger = logging.getLogger(__name__)\\n\\nclass License(models.Model):\\n    \'\'\'\\n    Represents a licence for experiment content.\\n\\n    Instances should provide enough detail for both researchers to select the\\n    licence, and for the users of their data to divine correct usage of\\n    experiment content.\\n\\n    (Non-US developers: We\'re using US spelling in the code.)\\n    \'\'\'\\n\\n    class Meta:\\n        app_label = \'tardis_portal\'\\n\\n    name = models.CharField(max_length=400, unique=True, blank=False)\\n    url  = models.URLField(\\n        verify_exists=True,\\n        max_length=2000,\\n        blank=False,\\n        unique=True,\\n        help_text=\\"Link to document outlining licensing details.\\")\\n    internal_description = models.TextField(blank=False)\\n    image_url = models.URLField(verify_exists=True, max_length=2000, blank=True)\\n    allows_distribution = models.BooleanField(\\n        default=False,\\n        help_text=\\"Does this license provide distribution rights?\\")\\n    is_active = models.BooleanField(\\n        default=True,\\n        help_text=\\"Can experiments continue to select this license?\\")\\n\\n    def __unicode__(self):\\n        return self.name\\n\\n    @classmethod\\n    def get_suitable_licenses(cls, public_access_method = None):\\n        def with_none(seq):\\n            return chain([cls.get_none_option_license()], seq)\\n        # If no method specify, return all\\n        if public_access_method == None:\\n            return with_none(cls.objects.filter(is_active=True))\\n        # Otherwise, ask Experiment to put it in terms we understand\\n        from .experiment import Experiment\\n        if Experiment.public_access_implies_distribution(public_access_method):\\n            # Only licences which allow distribution\\n            return cls.objects.filter(is_active=True, allows_distribution=True)\\n        else:\\n            # Only licenses which don\'t allow distribution (including none)\\n            return with_none(cls.objects.filter(is_active=True,\\n                                                allows_distribution=False))\\n\\n    @classmethod\\n    def get_none_option_license(cls):\\n        url = \'http://en.wikipedia.org/wiki/Copyright#Exclusive_rights\'\\n        desc = \'\'\'\\n        No license is explicitly specified. You implicitly retain all rights\\n        under copyright.\\n        \'\'\'\\n        return License(id=\'\',\\n                       name=\'Unspecified License\',\\n                       internal_description=desc,\\n                       url=url,\\n                       allows_distribution=False)\\n\\n" }\n'
line: b'{ "repo_name": "jeffreylu9/django-cms", "ref": "refs/heads/wlsite", "path": "cms/utils/helpers.py", "content": "# -*- coding: utf-8 -*-\\nimport re\\n\\nfrom django.contrib.sites.models import SITE_CACHE, Site\\nfrom .compat.dj import is_installed\\n\\nSITE_VAR = \\"site__exact\\"\\n\\n\\n# modify reversions to match our needs if required...\\ndef reversion_register(model_class, fields=None, follow=(), format=\\"json\\", exclude_fields=None):\\n    \\"\\"\\"CMS interface to reversion api - helper function. Registers model for\\n    reversion only if reversion is available.\\n\\n    Auto excludes publisher fields.\\n\\n    \\"\\"\\"\\n\\n    # reversion\'s merely recommended, not required\\n    if not is_installed(\'reversion\'):\\n        return\\n\\n    if fields and exclude_fields:\\n        raise ValueError(\\"Just one of fields, exclude_fields arguments can be passed.\\")\\n\\n    opts = model_class._meta\\n    local_fields = opts.local_fields + opts.local_many_to_many\\n    if fields is None:\\n        fields = [field.name for field in local_fields]\\n\\n    exclude_fields = exclude_fields or []\\n\\n    fields = filter(lambda name: not name in exclude_fields, fields)\\n\\n    from cms.utils import reversion_hacks\\n    reversion_hacks.register_draft_only(model_class, fields, follow, format)\\n\\n\\ndef make_revision_with_plugins(obj, user=None, message=None):\\n    from cms.models.pluginmodel import CMSPlugin\\n    # we can safely import reversion - calls here always check for\\n    # reversion in installed_applications first\\n    import reversion\\n    if hasattr(reversion.models, \'VERSION_CHANGE\'):\\n        from reversion.models import VERSION_CHANGE\\n    \\"\\"\\"\\n    Only add to revision if it is a draft.\\n    \\"\\"\\"\\n    revision_manager = reversion.revision\\n    revision_context = reversion.revision_context_manager\\n\\n    cls = obj.__class__\\n    if hasattr(revision_manager, \'_registration_key_for_model\'):\\n        model_key = revision_manager._registration_key_for_model(cls)\\n    else:\\n        model_key = cls\\n\\n    if model_key in revision_manager._registered_models:\\n\\n        placeholder_relation = find_placeholder_relation(obj)\\n\\n        if revision_context.is_active():\\n            if user:\\n                revision_context.set_user(user)\\n            if message:\\n                revision_context.set_comment(message)\\n            # add toplevel object to the revision\\n            adapter = revision_manager.get_adapter(obj.__class__)\\n            if hasattr(reversion.models, \'VERSION_CHANGE\'):\\n                revision_context.add_to_context(revision_manager, obj, adapter.get_version_data(obj, VERSION_CHANGE))\\n            else:\\n                revision_context.add_to_context(revision_manager, obj, adapter.get_version_data(obj))\\n            # add placeholders to the revision\\n            for ph in obj.get_placeholders():\\n                phadapter = revision_manager.get_adapter(ph.__class__)\\n                if hasattr(reversion.models, \'VERSION_CHANGE\'):\\n                    revision_context.add_to_context(revision_manager, ph, phadapter.get_version_data(ph, VERSION_CHANGE))\\n                else:\\n                    revision_context.add_to_context(revision_manager, ph, phadapter.get_version_data(ph))\\n            # add plugins and subclasses to the revision\\n            filters = {\'placeholder__%s\' % placeholder_relation: obj}\\n            for plugin in CMSPlugin.objects.filter(**filters):\\n                plugin_instance, admin = plugin.get_plugin_instance()\\n                if plugin_instance:\\n                    padapter = revision_manager.get_adapter(plugin_instance.__class__)\\n                    if hasattr(reversion.models, \'VERSION_CHANGE\'):\\n                        revision_context.add_to_context(revision_manager, plugin_instance, padapter.get_version_data(plugin_instance, VERSION_CHANGE))\\n                    else:\\n                        revision_context.add_to_context(revision_manager, plugin_instance, padapter.get_version_data(plugin_instance))\\n                bpadapter = revision_manager.get_adapter(plugin.__class__)\\n                if hasattr(reversion.models, \'VERSION_CHANGE\'):\\n                    revision_context.add_to_context(revision_manager, plugin, bpadapter.get_version_data(plugin, VERSION_CHANGE))\\n                else:\\n                    revision_context.add_to_context(revision_manager, plugin, bpadapter.get_version_data(plugin))\\n\\n\\ndef find_placeholder_relation(obj):\\n    return \'page\'\\n\\n\\nclass classproperty(object):\\n    \\"\\"\\"Like @property, but for classes, not just instances.\\n\\n    Example usage:\\n\\n        >>> from cms.utils.helpers import classproperty\\n        >>> class A(object):\\n        ...     @classproperty\\n        ...     def x(cls):\\n        ...         return \'x\'\\n        ...     @property\\n        ...     def y(self):\\n        ...         return \'y\'\\n        ...\\n        >>> A.x\\n        \'x\'\\n        >>> A().x\\n        \'x\'\\n        >>> A.y\\n        <property object at 0x2939628>\\n        >>> A().y\\n        \'y\'\\n\\n    \\"\\"\\"\\n    def __init__(self, fget):\\n        self.fget = fget\\n\\n    def __get__(self, owner_self, owner_cls):\\n        return self.fget(owner_cls)\\n\\n\\ndef current_site(request):\\n    site_pk = request.GET.get(SITE_VAR, None) if request.GET.get(SITE_VAR, None) else request.POST.get(SITE_VAR, None)\\n    if not site_pk:\\n        site_pk = request.session.get(\'cms_admin_site\', None)\\n    if site_pk:\\n        try:\\n            site = SITE_CACHE.get(site_pk) or Site.objects.get(pk=site_pk)\\n            SITE_CACHE[site_pk] = site\\n            return site\\n        except Site.DoesNotExist:\\n            return None\\n    else:\\n        return Site.objects.get_current()\\n\\n\\ndef normalize_name(name):\\n    \\"\\"\\"\\n    Converts camel-case style names into underscore seperated words. Example::\\n\\n        >>> normalize_name(\'oneTwoThree\')\\n        \'one_two_three\'\\n        >>> normalize_name(\'FourFiveSix\')\\n        \'four_five_six\'\\n\\n    taken from django.contrib.formtools\\n    \\"\\"\\"\\n    new = re.sub(\'(((?<=[a-z])[A-Z])|([A-Z](?![A-Z]|$)))\', \'_\\\\\\\\1\', name)\\n    return new.lower().strip(\'_\')" }\n'
line: b'{ "repo_name": "libracore/erpnext", "ref": "refs/heads/v12", "path": "erpnext/non_profit/doctype/membership_type/test_membership_type.py", "content": "# -*- coding: utf-8 -*-\\n# Copyright (c) 2017, Frappe Technologies Pvt. Ltd. and Contributors\\n# See license.txt\\nfrom __future__ import unicode_literals\\n\\nimport unittest\\n\\nclass TestMembershipType(unittest.TestCase):\\n\\tpass\\n" }\n'
line: b'{ "repo_name": "Pexego/alimentacion", "ref": "refs/heads/7.0", "path": "stock_location_templates/wizard/__init__.py", "content": "# -*- coding: utf-8 -*-\\n##############################################################################\\n#\\n#    Copyright (C) 2004-2012 Pexego Sistemas Inform\xc3\xa1ticos All Rights Reserved\\n#    $Marta V\xc3\xa1zquez Rodr\xc3\xadguez$ <marta@pexego.es>\\n#\\n#    This program is free software: you can redistribute it and/or modify\\n#    it under the terms of the GNU Affero General Public License as published\\n#    by the Free Software Foundation, either version 3 of the License, or\\n#    (at your option) any later version.\\n#\\n#    This program is distributed in the hope that it will be useful,\\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#    GNU Affero General Public License for more details.\\n#\\n#    You should have received a copy of the GNU Affero General Public License\\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\\n#\\n##############################################################################\\nimport templates_product" }\n'
line: b'{ "repo_name": "lokeshh/stem", "ref": "refs/heads/run_tests.py_fix", "path": "test/integ/descriptor/__init__.py", "content": "\\"\\"\\"\\nIntegration tests for stem.descriptor.* contents.\\n\\"\\"\\"\\n\\n__all__ = [\\n  \'extrainfo_descriptor\',\\n  \'microdescriptor\',\\n  \'server_descriptor\',\\n  \'get_resource\',\\n  \'open_desc\',\\n]\\n" }\n'
line: b'{ "repo_name": "Comunitea/alimentacion", "ref": "refs/heads/7.0", "path": "stock_location_templates/wizard/__init__.py", "content": "# -*- coding: utf-8 -*-\\n##############################################################################\\n#\\n#    Copyright (C) 2004-2012 Pexego Sistemas Inform\xc3\xa1ticos All Rights Reserved\\n#    $Marta V\xc3\xa1zquez Rodr\xc3\xadguez$ <marta@pexego.es>\\n#\\n#    This program is free software: you can redistribute it and/or modify\\n#    it under the terms of the GNU Affero General Public License as published\\n#    by the Free Software Foundation, either version 3 of the License, or\\n#    (at your option) any later version.\\n#\\n#    This program is distributed in the hope that it will be useful,\\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#    GNU Affero General Public License for more details.\\n#\\n#    You should have received a copy of the GNU Affero General Public License\\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\\n#\\n##############################################################################\\nimport templates_product" }\n'
line: b'{ "repo_name": "xingyepei/edx-platform", "ref": "refs/heads/release", "path": "openedx/core/djangoapps/programs/models.py", "content": "\\"\\"\\"\\nModels providing Programs support for the LMS and Studio.\\n\\"\\"\\"\\n\\nfrom urlparse import urljoin\\n\\nfrom django.db.models import BooleanField, IntegerField, URLField\\nfrom django.utils.translation import ugettext_lazy as _\\n\\nfrom config_models.models import ConfigurationModel\\n\\n\\nclass ProgramsApiConfig(ConfigurationModel):\\n    \\"\\"\\"\\n    Manages configuration for connecting to the Programs service and using its\\n    API.\\n    \\"\\"\\"\\n\\n    internal_service_url = URLField(verbose_name=_(\\"Internal Service URL\\"))\\n    public_service_url = URLField(verbose_name=_(\\"Public Service URL\\"))\\n    api_version_number = IntegerField(verbose_name=_(\\"API Version\\"))\\n    enable_student_dashboard = BooleanField(verbose_name=_(\\"Enable Student Dashboard Displays\\"))\\n\\n    @property\\n    def internal_api_url(self):\\n        \\"\\"\\"\\n        Generate a URL based on internal service URL and api version number.\\n        \\"\\"\\"\\n        return urljoin(self.internal_service_url, \\"/api/v{}/\\".format(self.api_version_number))\\n\\n    @property\\n    def public_api_url(self):\\n        \\"\\"\\"\\n        Generate a URL based on public service URL and api version number.\\n        \\"\\"\\"\\n        return urljoin(self.public_service_url, \\"/api/v{}/\\".format(self.api_version_number))\\n\\n    @property\\n    def is_student_dashboard_enabled(self):\\n        \\"\\"\\"\\n        Indicate whether LMS dashboard functionality related to Programs should\\n        be enabled or not.\\n        \\"\\"\\"\\n        return self.enabled and self.enable_student_dashboard\\n" }\n'
line: b'{ "repo_name": "Foxboron/stem", "ref": "refs/heads/foxboron/python3/codebase", "path": "test/integ/descriptor/__init__.py", "content": "\\"\\"\\"\\nIntegration tests for stem.descriptor.* contents.\\n\\"\\"\\"\\n\\n__all__ = [\\n  \'extrainfo_descriptor\',\\n  \'microdescriptor\',\\n  \'server_descriptor\',\\n  \'get_resource\',\\n  \'open_desc\',\\n]\\n" }\n'
line: b'{ "repo_name": "yohn89/pythoner.net", "ref": "refs/heads/matt", "path": "pythoner/topic/migrations/0003_auto__add_field_topic_md_content.py", "content": "# -*- coding: utf-8 -*-\\nimport datetime\\nfrom south.db import db\\nfrom south.v2 import SchemaMigration\\nfrom django.db import models\\n\\n\\nclass Migration(SchemaMigration):\\n\\n    def forwards(self, orm):\\n        # Adding field \'Topic.md_content\'\\n        db.add_column(\'topic_topic\', \'md_content\',\\n                      self.gf(\'django.db.models.fields.TextField\')(default=\'\'),\\n                      keep_default=False)\\n\\n\\n    def backwards(self, orm):\\n        # Deleting field \'Topic.md_content\'\\n        db.delete_column(\'topic_topic\', \'md_content\')\\n\\n\\n    models = {\\n        \'auth.group\': {\\n            \'Meta\': {\'object_name\': \'Group\'}\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'unique\': \'True\', \'max_length\': \'80\'}),\\n            \'permissions\': (\'django.db.models.fields.related.ManyToManyField\', [], {\'to\': \\"orm[\'auth.Permission\']\\", \'symmetrical\': \'False\', \'blank\': \'True\'})\\n      }\\n        \'auth.permission\': {\\n            \'Meta\': {\'ordering\': \\"(\'content_type__app_label\', \'content_type__model\', \'codename\')\\", \'unique_together\': \\"((\'content_type\', \'codename\'),)\\", \'object_name\': \'Permission\'}\\n            \'codename\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'content_type\': (\'django.db.models.fields.related.ForeignKey\', [], {\'to\': \\"orm[\'contenttypes.ContentType\']\\"}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\'})\\n      }\\n        \'auth.user\': {\\n            \'Meta\': {\'object_name\': \'User\'}\\n            \'date_joined\': (\'django.db.models.fields.DateTimeField\', [], {\'default\': \'datetime.datetime.now\'}),\\n            \'email\': (\'django.db.models.fields.EmailField\', [], {\'max_length\': \'75\', \'blank\': \'True\'}),\\n            \'first_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'30\', \'blank\': \'True\'}),\\n            \'groups\': (\'django.db.models.fields.related.ManyToManyField\', [], {\'to\': \\"orm[\'auth.Group\']\\", \'symmetrical\': \'False\', \'blank\': \'True\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'is_active\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'is_staff\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'is_superuser\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'last_login\': (\'django.db.models.fields.DateTimeField\', [], {\'default\': \'datetime.datetime.now\'}),\\n            \'last_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'30\', \'blank\': \'True\'}),\\n            \'password\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'128\'}),\\n            \'user_permissions\': (\'django.db.models.fields.related.ManyToManyField\', [], {\'to\': \\"orm[\'auth.Permission\']\\", \'symmetrical\': \'False\', \'blank\': \'True\'}),\\n            \'username\': (\'django.db.models.fields.CharField\', [], {\'unique\': \'True\', \'max_length\': \'30\'})\\n      }\\n        \'contenttypes.contenttype\': {\\n            \'Meta\': {\'ordering\': \\"(\'name\',)\\", \'unique_together\': \\"((\'app_label\', \'model\'),)\\", \'object_name\': \'ContentType\', \'db_table\': \\"\'django_content_type\'\\"}\\n            \'app_label\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'model\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'})\\n      }\\n        \'topic.favorite\': {\\n            \'Meta\': {\'object_name\': \'Favorite\'}\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'topic\': (\'django.db.models.fields.related.ForeignKey\', [], {\'to\': \\"orm[\'topic.Topic\']\\"}),\\n            \'user\': (\'django.db.models.fields.related.ForeignKey\', [], {\'to\': \\"orm[\'auth.User\']\\"})\\n      }\\n        \'topic.tag\': {\\n            \'Meta\': {\'object_name\': \'Tag\'}\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'unique\': \'True\', \'max_length\': \'10\'}),\\n            \'remark\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'300\', \'null\': \'True\', \'blank\': \'True\'})\\n      }\\n        \'topic.topic\': {\\n            \'Meta\': {\'ordering\': \\"[\'deleted\', \'-top\', \'-latest_response\', \'-sub_time\', \'author\', \'-click_times\']\\", \'object_name\': \'Topic\'}\\n            \'author\': (\'django.db.models.fields.related.ForeignKey\', [], {\'related_name\': \\"\'author\'\\", \'to\': \\"orm[\'auth.User\']\\"}),\\n            \'click_times\': (\'django.db.models.fields.PositiveIntegerField\', [], {\'default\': \'0\', \'max_length\': \'10\'}),\\n            \'content\': (\'django.db.models.fields.TextField\', [], {}),\\n            \'deleted\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'ip\': (\'django.db.models.fields.IPAddressField\', [], {\'default\': \\"\'127.0.0.1\'\\", \'max_length\': \'15\'}),\\n            \'latest_response\': (\'django.db.models.fields.DateTimeField\', [], {\'null\': \'True\', \'blank\': \'True\'}),\\n            \'md_content\': (\'django.db.models.fields.TextField\', [], {\'default\': \\"\'\'\\"}),\\n            \'notice\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'sub_time\': (\'django.db.models.fields.DateTimeField\', [], {\'auto_now_add\': \'True\', \'blank\': \'True\'}),\\n            \'tag\': (\'django.db.models.fields.related.ManyToManyField\', [], {\'symmetrical\': \'False\', \'to\': \\"orm[\'topic.Tag\']\\", \'null\': \'True\', \'blank\': \'True\'}),\\n            \'title\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\'}),\\n            \'top\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'})\\n      }\\n  }\\n\\n    complete_apps = [\'topic\']" }\n'
line: b'{ "repo_name": "vmendez/DIRAC", "ref": "refs/heads/integration", "path": "Core/Utilities/TimeLeft/LSFTimeLeft.py", "content": "\\"\\"\\" The LSF TimeLeft utility interrogates the LSF batch system for the\\n    current CPU and Wallclock consumed, as well as their limits.\\n\\"\\"\\"\\n__RCSID__ = \\"$Id$\\"\\n\\nimport os\\nimport re\\nimport time\\n\\nfrom DIRAC import gLogger, S_OK, S_ERROR\\nfrom DIRAC.Core.Utilities.TimeLeft.TimeLeft import runCommand\\n\\nfrom DIRAC.Core.Utilities.Os import sourceEnv\\n\\n\\nclass LSFTimeLeft( object ):\\n\\n  #############################################################################\\n  def __init__( self ):\\n    \\"\\"\\" Standard constructor\\n    \\"\\"\\"\\n    self.log = gLogger.getSubLogger( \'LSFTimeLeft\' )\\n    self.jobID = os.environ.get( \'LSB_JOBID\' )\\n    self.queue = os.environ.get( \'LSB_QUEUE\' )\\n    self.bin = os.environ.get( \'LSF_BINDIR\' )\\n    self.host = os.environ.get( \'LSB_HOSTS\' )\\n    self.year = time.strftime( \'%Y\', time.gmtime() )\\n    self.log.verbose( \'LSB_JOBID=%s, LSB_QUEUE=%s, LSF_BINDIR=%s, LSB_HOSTS=%s\' % ( self.jobID,\\n                                                                                    self.queue,\\n                                                                                    self.bin,\\n                                                                                    self.host ) )\\n\\n    self.cpuLimit = None\\n    self.cpuRef = None\\n    self.normRef = None\\n    self.wallClockLimit = None\\n    self.hostNorm = None\\n\\n    cmd = \'%s/bqueues -l %s\' % ( self.bin, self.queue )\\n    result = runCommand( cmd )\\n    if not result[\'OK\']:\\n      return\\n\\n    lines = str( result[\'Value\'] ).split( \'\\\\n\' )\\n    self.log.debug( \'From %s\' % cmd, \'\\\\n\'.join( [line if len( line ) <= 128 else line[:128] + \' [...]\' for line in lines] ) )\\n    for i in xrange( len( lines ) ):\\n      if re.search( \'.*CPULIMIT.*\', lines[i] ):\\n        info = lines[i + 1].split()\\n        if len( info ) >= 4:\\n          self.cpuLimit = float( info[0] ) * 60\\n          self.cpuRef = info[3]\\n        else:\\n          self.log.warn( \'Problem parsing \\"%s\\" for CPU limit\' % lines[i + 1] )\\n          self.cpuLimit = -1\\n      elif re.search( \'.*RUNLIMIT.*\', lines[i] ):\\n        info = lines[i + 1].split()\\n        if len( info ) >= 1:\\n          self.wallClockLimit = float( info[0] ) * 60\\n        else:\\n          self.log.warn( \'Problem parsing \\"%s\\" for wall clock limit\' % lines[i + 1] )\\n          self.wallClockLimit = -1\\n\\n    modelMaxNorm = 0\\n    if self.cpuRef:\\n      # Now try to get the CPU_FACTOR for this reference CPU,\\n      # it must be either a Model, a Host or the largest Model\\n\\n      cmd = \'%s/lshosts -w %s\' % ( self.bin, self.cpuRef )\\n      result = runCommand( cmd )\\n      if result[\'OK\']:\\n        # At CERN this command will return an error since there is no host defined\\n        # with the name of the reference Host.\\n        lines = str( result[\'Value\'] ).split( \'\\\\n\' )\\n        l1 = lines[0].split()\\n        l2 = lines[1].split()\\n        if len( l1 ) > len( l2 ):\\n          self.log.error( \\"Failed lshost command\\", \\"%s:\\\\n %s\\\\n %s\\" % ( cmd, lines[0], lines[0] ) )\\n        else:\\n          for i in range( len( l1 ) ):\\n            if l1[i] == \'cpuf\':\\n              try:\\n                self.normRef = float( l2[i] )\\n                self.log.info( \'Reference Normalization taken from Host\', \'%s: %s\' % ( self.cpuRef, self.normRef ) )\\n              except ValueError as e:\\n                self.log.exception( \'Exception parsing lshosts output\', \'\', e )\\n\\n      if not self.normRef:\\n        # Try if there is a model define with the name of cpuRef\\n        cmd = \'%s/lsinfo -m\' % ( self.bin )\\n        result = runCommand( cmd )\\n        if result[\'OK\']:\\n          lines = str( result[\'Value\'] ).split( \'\\\\n\' )\\n          for line in lines[1:]:\\n            words = line.split()\\n            if len( words ) > 1:\\n              try:\\n                norm = float( words[1] )\\n                if norm > modelMaxNorm:\\n                  modelMaxNorm = norm\\n                if words[0].find( self.cpuRef ) > -1:\\n                  self.normRef = norm\\n                  self.log.info( \'Reference Normalization taken from Host Model\',\\n                                 \'%s: %s\' % ( self.cpuRef, self.normRef ) )\\n              except ValueError as e:\\n                self.log.exception( \'Exception parsing lsfinfo output\', \'\', e )\\n\\n      if not self.normRef:\\n        # Now parse LSF configuration files\\n        if not os.path.isfile( \'./lsf.sh\' ):\\n          os.symlink( os.path.join( os.environ[\'LSF_ENVDIR\'], \'lsf.conf\' ) , \'./lsf.sh\' )\\n        # As the variables are not exported, we must force it\\n        ret = sourceEnv( 10, [\'./lsf\', \'&& export LSF_CONFDIR\' ] )\\n        if ret[\'OK\']:\\n          lsfEnv = ret[\'outputEnv\']\\n          shared = None\\n          try:\\n            egoShared = os.path.join( lsfEnv[\'LSF_CONFDIR\'], \'ego.shared\' )\\n            lsfShared = os.path.join( lsfEnv[\'LSF_CONFDIR\'], \'lsf.shared\' )\\n            if os.path.exists( egoShared ):\\n              shared = egoShared\\n            elif os.path.exists( lsfShared ):\\n              shared = lsfShared\\n          except KeyError as e:\\n            self.log.exception( \'Exception getting LSF configuration\', \'\', e )\\n          if shared:\\n            f = open( shared )\\n            hostModelSection = False\\n            for line in f.readlines():\\n              if line.find( \'Begin HostModel\' ) == 0:\\n                hostModelSection = True\\n                continue\\n              if not hostModelSection:\\n                continue\\n              if line.find( \'End HostModel\' ) == 0:\\n                break\\n              line = line.strip()\\n              if line and line.split()[0] == self.cpuRef:\\n                try:\\n                  self.normRef = float( line.split()[1] )\\n                  self.log.info( \'Reference Normalization taken from Configuration File\',\\n                                 \'(%s) %s: %s\' % ( shared, self.cpuRef, self.normRef ) )\\n                except ValueError as e:\\n                  self.log.exception( \'Exception reading LSF configuration\', \'\', e )\\n          else:\\n            self.log.warn( \'Could not find LSF configuration\' )\\n        else:\\n          self.log.error( \'Cannot source the LSF environment\', ret[\'Message\'] )\\n    if not self.normRef:\\n      # If nothing worked, take the maximum defined for a Model\\n      if modelMaxNorm:\\n        self.normRef = modelMaxNorm\\n        self.log.info( \'Reference Normalization taken from Max Model:\', self.normRef )\\n\\n    # Now get the Normalization for the current Host\\n    if self.host:\\n      cmd = \'%s/lshosts -w %s\' % ( self.bin, self.host )\\n      result = runCommand( cmd )\\n      if result[\'OK\']:\\n        lines = str( result[\'Value\'] ).split( \'\\\\n\' )\\n        l1 = lines[0].split()\\n        l2 = lines[1].split()\\n        if len( l1 ) > len( l2 ):\\n          self.log.error( \\"Failed lshost command\\", \\"%s:\\\\n %s\\\\n %s\\" % ( cmd, lines[0], lines[0] ) )\\n        else:\\n          for i in range( len( l1 ) ):\\n            if l1[i] == \'cpuf\':\\n              try:\\n                self.hostNorm = float( l2[i] )\\n                self.log.info( \'Host Normalization\', \'%s: %s\' % ( self.host, self.hostNorm ) )\\n              except ValueError as e:\\n                self.log.exception( \'Exception parsing lshosts output\', l1, e )\\n              finally:\\n                break\\n\\n      if self.hostNorm and self.normRef:\\n        self.hostNorm /= self.normRef\\n        self.log.info( \'CPU power w.r.t. batch unit\', self.hostNorm )\\n\\n      if self.hostNorm:\\n        # Set the limits in real seconds\\n        self.cpuLimit /= self.hostNorm\\n        self.wallClockLimit /= self.hostNorm\\n\\n  #############################################################################\\n  def getResourceUsage( self ):\\n    \\"\\"\\"Returns a dictionary containing CPUConsumed, CPULimit, WallClockConsumed\\n       and WallClockLimit for current slot.  All values returned in seconds.\\n    \\"\\"\\"\\n    if not self.bin:\\n      return S_ERROR( \'Could not determine bin directory for LSF\' )\\n    if not self.hostNorm:\\n      return S_ERROR( \'Could not determine host Norm factor\' )\\n\\n\\n    cpu = None\\n    wallClock = None\\n\\n    cmd = \'%s/bjobs -W %s\' % ( self.bin, self.jobID )\\n    result = runCommand( cmd )\\n    if not result[\'OK\']:\\n      return result\\n    lines = str( result[\'Value\'] ).split( \'\\\\n\' )\\n    l1 = lines[0].split()\\n    l2 = lines[1].split()\\n    if len( l1 ) > len( l2 ):\\n      self.log.error( \\"Failed bjobs command\\", \\"%s:\\\\n %s\\\\n %s\\" % ( cmd, lines[0], lines[0] ) )\\n      return S_ERROR( \'Can not parse LSF output\' )\\n\\n    sCPU = None\\n    sStart = None\\n    for i in range( len( l1 ) ):\\n      if l1[i] == \'CPU_USED\':\\n        sCPU = l2[i]\\n        lCPU = sCPU.split( \':\' )\\n        try:\\n          cpu = float( lCPU[0] ) * 3600 + float( lCPU[1] ) * 60 + float( lCPU[2] )\\n        except ( ValueError, IndexError ) as _e:\\n          pass\\n      elif l1[i] == \'START_TIME\':\\n        sStart = l2[i]\\n        sStart = \'%s %s\' % ( sStart, self.year )\\n        try:\\n          timeTup = time.strptime( sStart, \'%m/%d-%H:%M:%S %Y\' )\\n          wallClock = time.mktime( time.localtime() ) - time.mktime( timeTup )\\n        except ValueError:\\n          pass\\n\\n    if cpu is None or wallClock is None:\\n      return S_ERROR( \'Failed to parse LSF output\' )\\n\\n    consumed = {\'CPU\':cpu, \'CPULimit\':self.cpuLimit, \'WallClock\':wallClock, \'WallClockLimit\':self.wallClockLimit}\\n    self.log.debug( consumed )\\n\\n    if None not in consumed.values():\\n      return S_OK( consumed )\\n    else:\\n      missed = [key for key, val in consumed.items() if val is None]\\n      msg = \'Could not determine some parameters\'\\n      self.log.info( msg, \': %s\\\\nThis is the stdout from the batch system call\\\\n%s\' % ( \',\'.join( missed ), result[\'Value\'] ) )\\n      return S_ERROR( msg )\\n\\n# EOF#EOF#EOF#EOF#EOF#EOF#EOF#EOF#EOF#EOF#EOF#EOF#EOF#EOF#EOF#EOF#EOF#EOF#EOF#\\n" }\n'
line: b'{ "repo_name": "dex4er/django", "ref": "refs/heads/1.6.x", "path": "django/views/generic/base.py", "content": "from __future__ import unicode_literals\\n\\nimport logging\\nfrom functools import update_wrapper\\n\\nfrom django import http\\nfrom django.core.exceptions import ImproperlyConfigured\\nfrom django.core.urlresolvers import reverse, NoReverseMatch\\nfrom django.template.response import TemplateResponse\\nfrom django.utils.decorators import classonlymethod\\nfrom django.utils import six\\n\\nlogger = logging.getLogger(\'django.request\')\\n\\n\\nclass ContextMixin(object):\\n    \\"\\"\\"\\n    A default context mixin that passes the keyword arguments received by\\n    get_context_data as the template context.\\n    \\"\\"\\"\\n\\n    def get_context_data(self, **kwargs):\\n        if \'view\' not in kwargs:\\n            kwargs[\'view\'] = self\\n        return kwargs\\n\\n\\nclass View(object):\\n    \\"\\"\\"\\n    Intentionally simple parent class for all views. Only implements\\n    dispatch-by-method and simple sanity checking.\\n    \\"\\"\\"\\n\\n    http_method_names = [\'get\', \'post\', \'put\', \'patch\', \'delete\', \'head\', \'options\', \'trace\']\\n\\n    def __init__(self, **kwargs):\\n        \\"\\"\\"\\n        Constructor. Called in the URLconf; can contain helpful extra\\n        keyword arguments, and other things.\\n        \\"\\"\\"\\n        # Go through keyword arguments, and either save their values to our\\n        # instance, or raise an error.\\n        for key, value in six.iteritems(kwargs):\\n            setattr(self, key, value)\\n\\n    @classonlymethod\\n    def as_view(cls, **initkwargs):\\n        \\"\\"\\"\\n        Main entry point for a request-response process.\\n        \\"\\"\\"\\n        # sanitize keyword arguments\\n        for key in initkwargs:\\n            if key in cls.http_method_names:\\n                raise TypeError(\\"You tried to pass in the %s method name as a \\"\\n                                \\"keyword argument to %s(). Don\'t do that.\\"\\n                                % (key, cls.__name__))\\n            if not hasattr(cls, key):\\n                raise TypeError(\\"%s() received an invalid keyword %r. as_view \\"\\n                                \\"only accepts arguments that are already \\"\\n                                \\"attributes of the class.\\" % (cls.__name__, key))\\n\\n        def view(request, *args, **kwargs):\\n            self = cls(**initkwargs)\\n            if hasattr(self, \'get\') and not hasattr(self, \'head\'):\\n                self.head = self.get\\n            self.request = request\\n            self.args = args\\n            self.kwargs = kwargs\\n            return self.dispatch(request, *args, **kwargs)\\n\\n        # take name and docstring from class\\n        update_wrapper(view, cls, updated=())\\n\\n        # and possible attributes set by decorators\\n        # like csrf_exempt from dispatch\\n        update_wrapper(view, cls.dispatch, assigned=())\\n        return view\\n\\n    def dispatch(self, request, *args, **kwargs):\\n        # Try to dispatch to the right method; if a method doesn\'t exist,\\n        # defer to the error handler. Also defer to the error handler if the\\n        # request method isn\'t on the approved list.\\n        if request.method.lower() in self.http_method_names:\\n            handler = getattr(self, request.method.lower(), self.http_method_not_allowed)\\n        else:\\n            handler = self.http_method_not_allowed\\n        return handler(request, *args, **kwargs)\\n\\n    def http_method_not_allowed(self, request, *args, **kwargs):\\n        logger.warning(\'Method Not Allowed (%s): %s\', request.method, request.path,\\n            extra={\\n                \'status_code\': 405,\\n                \'request\': self.request\\n          }\\n        )\\n        return http.HttpResponseNotAllowed(self._allowed_methods())\\n\\n    def options(self, request, *args, **kwargs):\\n        \\"\\"\\"\\n        Handles responding to requests for the OPTIONS HTTP verb.\\n        \\"\\"\\"\\n        response = http.HttpResponse()\\n        response[\'Allow\'] = \', \'.join(self._allowed_methods())\\n        response[\'Content-Length\'] = \'0\'\\n        return response\\n\\n    def _allowed_methods(self):\\n        return [m.upper() for m in self.http_method_names if hasattr(self, m)]\\n\\n\\nclass TemplateResponseMixin(object):\\n    \\"\\"\\"\\n    A mixin that can be used to render a template.\\n    \\"\\"\\"\\n    template_name = None\\n    response_class = TemplateResponse\\n    content_type = None\\n\\n    def render_to_response(self, context, **response_kwargs):\\n        \\"\\"\\"\\n        Returns a response, using the `response_class` for this\\n        view, with a template rendered with the given context.\\n\\n        If any keyword arguments are provided, they will be\\n        passed to the constructor of the response class.\\n        \\"\\"\\"\\n        response_kwargs.setdefault(\'content_type\', self.content_type)\\n        return self.response_class(\\n            request = self.request,\\n            template = self.get_template_names(),\\n            context = context,\\n            **response_kwargs\\n        )\\n\\n    def get_template_names(self):\\n        \\"\\"\\"\\n        Returns a list of template names to be used for the request. Must return\\n        a list. May not be called if render_to_response is overridden.\\n        \\"\\"\\"\\n        if self.template_name is None:\\n            raise ImproperlyConfigured(\\n                \\"TemplateResponseMixin requires either a definition of \\"\\n                \\"\'template_name\' or an implementation of \'get_template_names()\'\\")\\n        else:\\n            return [self.template_name]\\n\\n\\nclass TemplateView(TemplateResponseMixin, ContextMixin, View):\\n    \\"\\"\\"\\n    A view that renders a template.  This view will also pass into the context\\n    any keyword arguments passed by the url conf.\\n    \\"\\"\\"\\n    def get(self, request, *args, **kwargs):\\n        context = self.get_context_data(**kwargs)\\n        return self.render_to_response(context)\\n\\n\\nclass RedirectView(View):\\n    \\"\\"\\"\\n    A view that provides a redirect on any GET request.\\n    \\"\\"\\"\\n    permanent = True\\n    url = None\\n    pattern_name = None\\n    query_string = False\\n\\n    def get_redirect_url(self, *args, **kwargs):\\n        \\"\\"\\"\\n        Return the URL redirect to. Keyword arguments from the\\n        URL pattern match generating the redirect request\\n        are provided as kwargs to this method.\\n        \\"\\"\\"\\n        if self.url:\\n            url = self.url % kwargs\\n        elif self.pattern_name:\\n            try:\\n                url = reverse(self.pattern_name, args=args, kwargs=kwargs)\\n            except NoReverseMatch:\\n                return None\\n        else:\\n            return None\\n\\n        args = self.request.META.get(\'QUERY_STRING\', \'\')\\n        if args and self.query_string:\\n            url = \\"%s?%s\\" % (url, args)\\n        return url\\n\\n    def get(self, request, *args, **kwargs):\\n        url = self.get_redirect_url(*args, **kwargs)\\n        if url:\\n            if self.permanent:\\n                return http.HttpResponsePermanentRedirect(url)\\n            else:\\n                return http.HttpResponseRedirect(url)\\n        else:\\n            logger.warning(\'Gone: %s\', self.request.path,\\n                        extra={\\n                            \'status_code\': 410,\\n                            \'request\': self.request\\n                      })\\n            return http.HttpResponseGone()\\n\\n    def head(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def post(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def options(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def delete(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def put(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def patch(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n" }\n'
line: b'{ "repo_name": "knowsis/django", "ref": "refs/heads/nonrel-1.6", "path": "django/views/generic/base.py", "content": "from __future__ import unicode_literals\\n\\nimport logging\\nfrom functools import update_wrapper\\n\\nfrom django import http\\nfrom django.core.exceptions import ImproperlyConfigured\\nfrom django.core.urlresolvers import reverse, NoReverseMatch\\nfrom django.template.response import TemplateResponse\\nfrom django.utils.decorators import classonlymethod\\nfrom django.utils import six\\n\\nlogger = logging.getLogger(\'django.request\')\\n\\n\\nclass ContextMixin(object):\\n    \\"\\"\\"\\n    A default context mixin that passes the keyword arguments received by\\n    get_context_data as the template context.\\n    \\"\\"\\"\\n\\n    def get_context_data(self, **kwargs):\\n        if \'view\' not in kwargs:\\n            kwargs[\'view\'] = self\\n        return kwargs\\n\\n\\nclass View(object):\\n    \\"\\"\\"\\n    Intentionally simple parent class for all views. Only implements\\n    dispatch-by-method and simple sanity checking.\\n    \\"\\"\\"\\n\\n    http_method_names = [\'get\', \'post\', \'put\', \'patch\', \'delete\', \'head\', \'options\', \'trace\']\\n\\n    def __init__(self, **kwargs):\\n        \\"\\"\\"\\n        Constructor. Called in the URLconf; can contain helpful extra\\n        keyword arguments, and other things.\\n        \\"\\"\\"\\n        # Go through keyword arguments, and either save their values to our\\n        # instance, or raise an error.\\n        for key, value in six.iteritems(kwargs):\\n            setattr(self, key, value)\\n\\n    @classonlymethod\\n    def as_view(cls, **initkwargs):\\n        \\"\\"\\"\\n        Main entry point for a request-response process.\\n        \\"\\"\\"\\n        # sanitize keyword arguments\\n        for key in initkwargs:\\n            if key in cls.http_method_names:\\n                raise TypeError(\\"You tried to pass in the %s method name as a \\"\\n                                \\"keyword argument to %s(). Don\'t do that.\\"\\n                                % (key, cls.__name__))\\n            if not hasattr(cls, key):\\n                raise TypeError(\\"%s() received an invalid keyword %r. as_view \\"\\n                                \\"only accepts arguments that are already \\"\\n                                \\"attributes of the class.\\" % (cls.__name__, key))\\n\\n        def view(request, *args, **kwargs):\\n            self = cls(**initkwargs)\\n            if hasattr(self, \'get\') and not hasattr(self, \'head\'):\\n                self.head = self.get\\n            self.request = request\\n            self.args = args\\n            self.kwargs = kwargs\\n            return self.dispatch(request, *args, **kwargs)\\n\\n        # take name and docstring from class\\n        update_wrapper(view, cls, updated=())\\n\\n        # and possible attributes set by decorators\\n        # like csrf_exempt from dispatch\\n        update_wrapper(view, cls.dispatch, assigned=())\\n        return view\\n\\n    def dispatch(self, request, *args, **kwargs):\\n        # Try to dispatch to the right method; if a method doesn\'t exist,\\n        # defer to the error handler. Also defer to the error handler if the\\n        # request method isn\'t on the approved list.\\n        if request.method.lower() in self.http_method_names:\\n            handler = getattr(self, request.method.lower(), self.http_method_not_allowed)\\n        else:\\n            handler = self.http_method_not_allowed\\n        return handler(request, *args, **kwargs)\\n\\n    def http_method_not_allowed(self, request, *args, **kwargs):\\n        logger.warning(\'Method Not Allowed (%s): %s\', request.method, request.path,\\n            extra={\\n                \'status_code\': 405,\\n                \'request\': self.request\\n          }\\n        )\\n        return http.HttpResponseNotAllowed(self._allowed_methods())\\n\\n    def options(self, request, *args, **kwargs):\\n        \\"\\"\\"\\n        Handles responding to requests for the OPTIONS HTTP verb.\\n        \\"\\"\\"\\n        response = http.HttpResponse()\\n        response[\'Allow\'] = \', \'.join(self._allowed_methods())\\n        response[\'Content-Length\'] = \'0\'\\n        return response\\n\\n    def _allowed_methods(self):\\n        return [m.upper() for m in self.http_method_names if hasattr(self, m)]\\n\\n\\nclass TemplateResponseMixin(object):\\n    \\"\\"\\"\\n    A mixin that can be used to render a template.\\n    \\"\\"\\"\\n    template_name = None\\n    response_class = TemplateResponse\\n    content_type = None\\n\\n    def render_to_response(self, context, **response_kwargs):\\n        \\"\\"\\"\\n        Returns a response, using the `response_class` for this\\n        view, with a template rendered with the given context.\\n\\n        If any keyword arguments are provided, they will be\\n        passed to the constructor of the response class.\\n        \\"\\"\\"\\n        response_kwargs.setdefault(\'content_type\', self.content_type)\\n        return self.response_class(\\n            request = self.request,\\n            template = self.get_template_names(),\\n            context = context,\\n            **response_kwargs\\n        )\\n\\n    def get_template_names(self):\\n        \\"\\"\\"\\n        Returns a list of template names to be used for the request. Must return\\n        a list. May not be called if render_to_response is overridden.\\n        \\"\\"\\"\\n        if self.template_name is None:\\n            raise ImproperlyConfigured(\\n                \\"TemplateResponseMixin requires either a definition of \\"\\n                \\"\'template_name\' or an implementation of \'get_template_names()\'\\")\\n        else:\\n            return [self.template_name]\\n\\n\\nclass TemplateView(TemplateResponseMixin, ContextMixin, View):\\n    \\"\\"\\"\\n    A view that renders a template.  This view will also pass into the context\\n    any keyword arguments passed by the url conf.\\n    \\"\\"\\"\\n    def get(self, request, *args, **kwargs):\\n        context = self.get_context_data(**kwargs)\\n        return self.render_to_response(context)\\n\\n\\nclass RedirectView(View):\\n    \\"\\"\\"\\n    A view that provides a redirect on any GET request.\\n    \\"\\"\\"\\n    permanent = True\\n    url = None\\n    pattern_name = None\\n    query_string = False\\n\\n    def get_redirect_url(self, *args, **kwargs):\\n        \\"\\"\\"\\n        Return the URL redirect to. Keyword arguments from the\\n        URL pattern match generating the redirect request\\n        are provided as kwargs to this method.\\n        \\"\\"\\"\\n        if self.url:\\n            url = self.url % kwargs\\n        elif self.pattern_name:\\n            try:\\n                url = reverse(self.pattern_name, args=args, kwargs=kwargs)\\n            except NoReverseMatch:\\n                return None\\n        else:\\n            return None\\n\\n        args = self.request.META.get(\'QUERY_STRING\', \'\')\\n        if args and self.query_string:\\n            url = \\"%s?%s\\" % (url, args)\\n        return url\\n\\n    def get(self, request, *args, **kwargs):\\n        url = self.get_redirect_url(*args, **kwargs)\\n        if url:\\n            if self.permanent:\\n                return http.HttpResponsePermanentRedirect(url)\\n            else:\\n                return http.HttpResponseRedirect(url)\\n        else:\\n            logger.warning(\'Gone: %s\', self.request.path,\\n                        extra={\\n                            \'status_code\': 410,\\n                            \'request\': self.request\\n                      })\\n            return http.HttpResponseGone()\\n\\n    def head(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def post(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def options(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def delete(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def put(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def patch(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n" }\n'
line: b'{ "repo_name": "django-nonrel/django", "ref": "refs/heads/nonrel-1.6", "path": "django/views/generic/base.py", "content": "from __future__ import unicode_literals\\n\\nimport logging\\nfrom functools import update_wrapper\\n\\nfrom django import http\\nfrom django.core.exceptions import ImproperlyConfigured\\nfrom django.core.urlresolvers import reverse, NoReverseMatch\\nfrom django.template.response import TemplateResponse\\nfrom django.utils.decorators import classonlymethod\\nfrom django.utils import six\\n\\nlogger = logging.getLogger(\'django.request\')\\n\\n\\nclass ContextMixin(object):\\n    \\"\\"\\"\\n    A default context mixin that passes the keyword arguments received by\\n    get_context_data as the template context.\\n    \\"\\"\\"\\n\\n    def get_context_data(self, **kwargs):\\n        if \'view\' not in kwargs:\\n            kwargs[\'view\'] = self\\n        return kwargs\\n\\n\\nclass View(object):\\n    \\"\\"\\"\\n    Intentionally simple parent class for all views. Only implements\\n    dispatch-by-method and simple sanity checking.\\n    \\"\\"\\"\\n\\n    http_method_names = [\'get\', \'post\', \'put\', \'patch\', \'delete\', \'head\', \'options\', \'trace\']\\n\\n    def __init__(self, **kwargs):\\n        \\"\\"\\"\\n        Constructor. Called in the URLconf; can contain helpful extra\\n        keyword arguments, and other things.\\n        \\"\\"\\"\\n        # Go through keyword arguments, and either save their values to our\\n        # instance, or raise an error.\\n        for key, value in six.iteritems(kwargs):\\n            setattr(self, key, value)\\n\\n    @classonlymethod\\n    def as_view(cls, **initkwargs):\\n        \\"\\"\\"\\n        Main entry point for a request-response process.\\n        \\"\\"\\"\\n        # sanitize keyword arguments\\n        for key in initkwargs:\\n            if key in cls.http_method_names:\\n                raise TypeError(\\"You tried to pass in the %s method name as a \\"\\n                                \\"keyword argument to %s(). Don\'t do that.\\"\\n                                % (key, cls.__name__))\\n            if not hasattr(cls, key):\\n                raise TypeError(\\"%s() received an invalid keyword %r. as_view \\"\\n                                \\"only accepts arguments that are already \\"\\n                                \\"attributes of the class.\\" % (cls.__name__, key))\\n\\n        def view(request, *args, **kwargs):\\n            self = cls(**initkwargs)\\n            if hasattr(self, \'get\') and not hasattr(self, \'head\'):\\n                self.head = self.get\\n            self.request = request\\n            self.args = args\\n            self.kwargs = kwargs\\n            return self.dispatch(request, *args, **kwargs)\\n\\n        # take name and docstring from class\\n        update_wrapper(view, cls, updated=())\\n\\n        # and possible attributes set by decorators\\n        # like csrf_exempt from dispatch\\n        update_wrapper(view, cls.dispatch, assigned=())\\n        return view\\n\\n    def dispatch(self, request, *args, **kwargs):\\n        # Try to dispatch to the right method; if a method doesn\'t exist,\\n        # defer to the error handler. Also defer to the error handler if the\\n        # request method isn\'t on the approved list.\\n        if request.method.lower() in self.http_method_names:\\n            handler = getattr(self, request.method.lower(), self.http_method_not_allowed)\\n        else:\\n            handler = self.http_method_not_allowed\\n        return handler(request, *args, **kwargs)\\n\\n    def http_method_not_allowed(self, request, *args, **kwargs):\\n        logger.warning(\'Method Not Allowed (%s): %s\', request.method, request.path,\\n            extra={\\n                \'status_code\': 405,\\n                \'request\': self.request\\n          }\\n        )\\n        return http.HttpResponseNotAllowed(self._allowed_methods())\\n\\n    def options(self, request, *args, **kwargs):\\n        \\"\\"\\"\\n        Handles responding to requests for the OPTIONS HTTP verb.\\n        \\"\\"\\"\\n        response = http.HttpResponse()\\n        response[\'Allow\'] = \', \'.join(self._allowed_methods())\\n        response[\'Content-Length\'] = \'0\'\\n        return response\\n\\n    def _allowed_methods(self):\\n        return [m.upper() for m in self.http_method_names if hasattr(self, m)]\\n\\n\\nclass TemplateResponseMixin(object):\\n    \\"\\"\\"\\n    A mixin that can be used to render a template.\\n    \\"\\"\\"\\n    template_name = None\\n    response_class = TemplateResponse\\n    content_type = None\\n\\n    def render_to_response(self, context, **response_kwargs):\\n        \\"\\"\\"\\n        Returns a response, using the `response_class` for this\\n        view, with a template rendered with the given context.\\n\\n        If any keyword arguments are provided, they will be\\n        passed to the constructor of the response class.\\n        \\"\\"\\"\\n        response_kwargs.setdefault(\'content_type\', self.content_type)\\n        return self.response_class(\\n            request = self.request,\\n            template = self.get_template_names(),\\n            context = context,\\n            **response_kwargs\\n        )\\n\\n    def get_template_names(self):\\n        \\"\\"\\"\\n        Returns a list of template names to be used for the request. Must return\\n        a list. May not be called if render_to_response is overridden.\\n        \\"\\"\\"\\n        if self.template_name is None:\\n            raise ImproperlyConfigured(\\n                \\"TemplateResponseMixin requires either a definition of \\"\\n                \\"\'template_name\' or an implementation of \'get_template_names()\'\\")\\n        else:\\n            return [self.template_name]\\n\\n\\nclass TemplateView(TemplateResponseMixin, ContextMixin, View):\\n    \\"\\"\\"\\n    A view that renders a template.  This view will also pass into the context\\n    any keyword arguments passed by the url conf.\\n    \\"\\"\\"\\n    def get(self, request, *args, **kwargs):\\n        context = self.get_context_data(**kwargs)\\n        return self.render_to_response(context)\\n\\n\\nclass RedirectView(View):\\n    \\"\\"\\"\\n    A view that provides a redirect on any GET request.\\n    \\"\\"\\"\\n    permanent = True\\n    url = None\\n    pattern_name = None\\n    query_string = False\\n\\n    def get_redirect_url(self, *args, **kwargs):\\n        \\"\\"\\"\\n        Return the URL redirect to. Keyword arguments from the\\n        URL pattern match generating the redirect request\\n        are provided as kwargs to this method.\\n        \\"\\"\\"\\n        if self.url:\\n            url = self.url % kwargs\\n        elif self.pattern_name:\\n            try:\\n                url = reverse(self.pattern_name, args=args, kwargs=kwargs)\\n            except NoReverseMatch:\\n                return None\\n        else:\\n            return None\\n\\n        args = self.request.META.get(\'QUERY_STRING\', \'\')\\n        if args and self.query_string:\\n            url = \\"%s?%s\\" % (url, args)\\n        return url\\n\\n    def get(self, request, *args, **kwargs):\\n        url = self.get_redirect_url(*args, **kwargs)\\n        if url:\\n            if self.permanent:\\n                return http.HttpResponsePermanentRedirect(url)\\n            else:\\n                return http.HttpResponseRedirect(url)\\n        else:\\n            logger.warning(\'Gone: %s\', self.request.path,\\n                        extra={\\n                            \'status_code\': 410,\\n                            \'request\': self.request\\n                      })\\n            return http.HttpResponseGone()\\n\\n    def head(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def post(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def options(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def delete(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def put(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def patch(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n" }\n'
line: b'{ "repo_name": "nemesiscodex/JukyOS-sugar", "ref": "refs/heads/juky", "path": "src/jarabe/frame/frame.py", "content": "# Copyright (C) 2006-2007 Red Hat, Inc.\\n#\\n# This program is free software; you can redistribute it and/or modify\\n# it under the terms of the GNU General Public License as published by\\n# the Free Software Foundation; either version 2 of the License, or\\n# (at your option) any later version.\\n#\\n# This program is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU General Public License for more details.\\n#\\n# You should have received a copy of the GNU General Public License\\n# along with this program; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA\\n\\nimport logging\\n\\nimport gtk\\nimport gobject\\n\\nfrom sugar.graphics import animator\\nfrom sugar.graphics import style\\nfrom sugar.graphics import palettegroup\\nfrom sugar import profile\\n\\nfrom jarabe.frame.eventarea import EventArea\\nfrom jarabe.frame.activitiestray import ActivitiesTray\\nfrom jarabe.frame.zoomtoolbar import ZoomToolbar\\nfrom jarabe.frame.friendstray import FriendsTray\\nfrom jarabe.frame.devicestray import DevicesTray\\nfrom jarabe.frame.framewindow import FrameWindow\\nfrom jarabe.frame.clipboardpanelwindow import ClipboardPanelWindow\\nfrom jarabe.frame.notification import NotificationIcon, NotificationWindow\\nfrom jarabe.model import notifications\\n\\n\\nTOP_RIGHT = 0\\nTOP_LEFT = 1\\nBOTTOM_RIGHT = 2\\nBOTTOM_LEFT = 3\\n\\n_FRAME_HIDING_DELAY = 500\\n_NOTIFICATION_DURATION = 5000\\n\\n\\nclass _Animation(animator.Animation):\\n    def __init__(self, frame, end):\\n        start = frame.current_position\\n        animator.Animation.__init__(self, start, end)\\n        self._frame = frame\\n\\n    def next_frame(self, current):\\n        self._frame.move(current)\\n\\n\\nclass _MouseListener(object):\\n    def __init__(self, frame):\\n        self._frame = frame\\n        self._hide_sid = 0\\n\\n    def mouse_enter(self):\\n        self._show_frame()\\n\\n    def mouse_leave(self):\\n        if self._frame.mode == Frame.MODE_MOUSE:\\n            self._hide_frame()\\n\\n    def _show_frame(self):\\n        if self._hide_sid != 0:\\n            gobject.source_remove(self._hide_sid)\\n        self._frame.show(Frame.MODE_MOUSE)\\n\\n    def _hide_frame_timeout_cb(self):\\n        self._frame.hide()\\n        return False\\n\\n    def _hide_frame(self):\\n        if self._hide_sid != 0:\\n            gobject.source_remove(self._hide_sid)\\n        self._hide_sid = gobject.timeout_add(\\n                  _FRAME_HIDING_DELAY, self._hide_frame_timeout_cb)\\n\\n\\nclass _KeyListener(object):\\n    def __init__(self, frame):\\n        self._frame = frame\\n\\n    def key_press(self):\\n        if self._frame.visible:\\n            if self._frame.mode == Frame.MODE_KEYBOARD:\\n                self._frame.hide()\\n        else:\\n            self._frame.show(Frame.MODE_KEYBOARD)\\n\\n\\nclass Frame(object):\\n    MODE_MOUSE = 0\\n    MODE_KEYBOARD = 1\\n    MODE_NON_INTERACTIVE = 2\\n\\n    def __init__(self):\\n        logging.debug(\'STARTUP: Loading the frame\')\\n        self.mode = None\\n\\n        self._palette_group = palettegroup.get_group(\'frame\')\\n        self._palette_group.connect(\'popdown\', self._palette_group_popdown_cb)\\n\\n        self._left_panel = None\\n        self._right_panel = None\\n        self._top_panel = None\\n        self._bottom_panel = None\\n\\n        self.current_position = 0.0\\n        self._animator = None\\n\\n        self._event_area = EventArea()\\n        self._event_area.connect(\'enter\', self._enter_corner_cb)\\n        self._event_area.show()\\n\\n        self._top_panel = self._create_top_panel()\\n        self._bottom_panel = self._create_bottom_panel()\\n        self._left_panel = self._create_left_panel()\\n        self._right_panel = self._create_right_panel()\\n\\n        screen = gtk.gdk.screen_get_default()\\n        screen.connect(\'size-changed\', self._size_changed_cb)\\n\\n        self._key_listener = _KeyListener(self)\\n        self._mouse_listener = _MouseListener(self)\\n\\n        self._notif_by_icon = {}\\n\\n        notification_service = notifications.get_service()\\n        notification_service.notification_received.connect(\\n                self.__notification_received_cb)\\n        notification_service.notification_cancelled.connect(\\n                self.__notification_cancelled_cb)\\n\\n    def is_visible(self):\\n        return self.current_position != 0.0\\n\\n    visible = property(is_visible, None)\\n\\n    def hide(self):\\n        if self._animator:\\n            self._animator.stop()\\n\\n        self._animator = animator.Animator(0.5)\\n        self._animator.add(_Animation(self, 0.0))\\n        self._animator.start()\\n\\n        self.mode = None\\n\\n    def show(self, mode):\\n        if self.visible:\\n            return\\n        if self._animator:\\n            self._animator.stop()\\n\\n        self.mode = mode\\n\\n        self._animator = animator.Animator(0.5)\\n        self._animator.add(_Animation(self, 1.0))\\n        self._animator.start()\\n\\n    def move(self, pos):\\n        self.current_position = pos\\n        self._update_position()\\n\\n    def _is_hover(self):\\n        return (self._top_panel.hover or \\\\\\n                self._bottom_panel.hover or \\\\\\n                self._left_panel.hover or \\\\\\n                self._right_panel.hover)\\n\\n    def _create_top_panel(self):\\n        panel = self._create_panel(gtk.POS_TOP)\\n\\n        zoom_toolbar = ZoomToolbar()\\n        panel.append(zoom_toolbar, expand=False)\\n        zoom_toolbar.show()\\n\\n        activities_tray = ActivitiesTray()\\n        panel.append(activities_tray)\\n        activities_tray.show()\\n\\n        return panel\\n\\n    def _create_bottom_panel(self):\\n        panel = self._create_panel(gtk.POS_BOTTOM)\\n\\n        devices_tray = DevicesTray()\\n        panel.append(devices_tray)\\n        devices_tray.show()\\n\\n        return panel\\n\\n    def _create_right_panel(self):\\n        panel = self._create_panel(gtk.POS_RIGHT)\\n\\n        tray = FriendsTray()\\n        panel.append(tray)\\n        tray.show()\\n\\n        return panel\\n\\n    def _create_left_panel(self):\\n        panel = ClipboardPanelWindow(self, gtk.POS_LEFT)\\n\\n        self._connect_to_panel(panel)\\n        panel.connect(\'drag-motion\', self._drag_motion_cb)\\n        panel.connect(\'drag-leave\', self._drag_leave_cb)\\n\\n        return panel\\n\\n    def _create_panel(self, orientation):\\n        panel = FrameWindow(orientation)\\n        self._connect_to_panel(panel)\\n\\n        return panel\\n\\n    def _move_panel(self, panel, pos, x1, y1, x2, y2):\\n        x = (x2 - x1) * pos + x1\\n        y = (y2 - y1) * pos + y1\\n\\n        panel.move(int(x), int(y))\\n\\n        # FIXME we should hide and show as necessary to free memory\\n        if not panel.props.visible:\\n            panel.show()\\n\\n    def _connect_to_panel(self, panel):\\n        panel.connect(\'enter-notify-event\', self._enter_notify_cb)\\n        panel.connect(\'leave-notify-event\', self._leave_notify_cb)\\n\\n    def _update_position(self):\\n        screen_h = gtk.gdk.screen_height()\\n        screen_w = gtk.gdk.screen_width()\\n\\n        self._move_panel(self._top_panel, self.current_position,\\n                         0, - self._top_panel.size, 0, 0)\\n\\n        self._move_panel(self._bottom_panel, self.current_position,\\n                         0, screen_h, 0, screen_h - self._bottom_panel.size)\\n\\n        self._move_panel(self._left_panel, self.current_position,\\n                         - self._left_panel.size, 0, 0, 0)\\n\\n        self._move_panel(self._right_panel, self.current_position,\\n                         screen_w, 0, screen_w - self._right_panel.size, 0)\\n\\n    def _size_changed_cb(self, screen):\\n        self._update_position()\\n\\n    def _enter_notify_cb(self, window, event):\\n        if event.detail != gtk.gdk.NOTIFY_INFERIOR:\\n            self._mouse_listener.mouse_enter()\\n\\n    def _leave_notify_cb(self, window, event):\\n        if event.detail == gtk.gdk.NOTIFY_INFERIOR:\\n            return\\n\\n        if not self._is_hover() and not self._palette_group.is_up():\\n            self._mouse_listener.mouse_leave()\\n\\n    def _palette_group_popdown_cb(self, group):\\n        if not self._is_hover():\\n            self._mouse_listener.mouse_leave()\\n\\n    def _drag_motion_cb(self, window, context, x, y, time):\\n        self._mouse_listener.mouse_enter()\\n\\n    def _drag_leave_cb(self, window, drag_context, timestamp):\\n        self._mouse_listener.mouse_leave()\\n\\n    def _enter_corner_cb(self, event_area):\\n        self._mouse_listener.mouse_enter()\\n\\n    def notify_key_press(self):\\n        self._key_listener.key_press()\\n\\n    def add_notification(self, icon, corner=gtk.CORNER_TOP_LEFT,\\n                         duration=_NOTIFICATION_DURATION):\\n\\n        if not isinstance(icon, NotificationIcon):\\n            raise TypeError(\'icon must be a NotificationIcon.\')\\n\\n        window = NotificationWindow()\\n\\n        screen = gtk.gdk.screen_get_default()\\n        if corner == gtk.CORNER_TOP_LEFT:\\n            window.move(0, 0)\\n        elif corner == gtk.CORNER_TOP_RIGHT:\\n            window.move(screen.get_width() - style.GRID_CELL_SIZE, 0)\\n        elif corner == gtk.CORNER_BOTTOM_LEFT:\\n            window.move(0, screen.get_height() - style.GRID_CELL_SIZE)\\n        elif corner == gtk.CORNER_BOTTOM_RIGHT:\\n            window.move(screen.get_width() - style.GRID_CELL_SIZE,\\n                        screen.get_height() - style.GRID_CELL_SIZE)\\n        else:\\n            raise ValueError(\'Inalid corner: %r\' % corner)\\n\\n        window.add(icon)\\n        icon.show()\\n        window.show()\\n\\n        self._notif_by_icon[icon] = window\\n\\n        gobject.timeout_add(duration,\\n                        lambda: self.remove_notification(icon))\\n\\n    def remove_notification(self, icon):\\n        if icon not in self._notif_by_icon:\\n            logging.debug(\'icon %r not in list of notifications.\', icon)\\n            return\\n\\n        window = self._notif_by_icon[icon]\\n        window.destroy()\\n        del self._notif_by_icon[icon]\\n\\n    def __notification_received_cb(self, **kwargs):\\n        logging.debug(\'__notification_received_cb\')\\n        icon = NotificationIcon()\\n\\n        hints = kwargs[\'hints\']\\n\\n        icon_file_name = hints.get(\'x-sugar-icon-file-name\', \'\')\\n        if icon_file_name:\\n            icon.props.icon_filename = icon_file_name\\n        else:\\n            icon.props.icon_name = \'application-octet-stream\'\\n\\n        icon_colors = hints.get(\'x-sugar-icon-colors\', \'\')\\n        if not icon_colors:\\n            icon_colors = profile.get_color()\\n        icon.props.xo_color = icon_colors\\n\\n        duration = kwargs.get(\'expire_timeout\', -1)\\n        if duration == -1:\\n            duration = _NOTIFICATION_DURATION\\n\\n        self.add_notification(icon, gtk.CORNER_TOP_RIGHT, duration)\\n\\n    def __notification_cancelled_cb(self, **kwargs):\\n        # Do nothing for now. Our notification UI is so simple, there\'s no\\n        # point yet.\\n        pass\\n" }\n'
line: b'{ "repo_name": "lumig242/Hue-Integration-with-CDAP", "ref": "refs/heads/pull3", "path": "desktop/core/ext-py/Django-1.6.10/django/views/generic/base.py", "content": "from __future__ import unicode_literals\\n\\nimport logging\\nfrom functools import update_wrapper\\n\\nfrom django import http\\nfrom django.core.exceptions import ImproperlyConfigured\\nfrom django.core.urlresolvers import reverse, NoReverseMatch\\nfrom django.template.response import TemplateResponse\\nfrom django.utils.decorators import classonlymethod\\nfrom django.utils import six\\n\\nlogger = logging.getLogger(\'django.request\')\\n\\n\\nclass ContextMixin(object):\\n    \\"\\"\\"\\n    A default context mixin that passes the keyword arguments received by\\n    get_context_data as the template context.\\n    \\"\\"\\"\\n\\n    def get_context_data(self, **kwargs):\\n        if \'view\' not in kwargs:\\n            kwargs[\'view\'] = self\\n        return kwargs\\n\\n\\nclass View(object):\\n    \\"\\"\\"\\n    Intentionally simple parent class for all views. Only implements\\n    dispatch-by-method and simple sanity checking.\\n    \\"\\"\\"\\n\\n    http_method_names = [\'get\', \'post\', \'put\', \'patch\', \'delete\', \'head\', \'options\', \'trace\']\\n\\n    def __init__(self, **kwargs):\\n        \\"\\"\\"\\n        Constructor. Called in the URLconf; can contain helpful extra\\n        keyword arguments, and other things.\\n        \\"\\"\\"\\n        # Go through keyword arguments, and either save their values to our\\n        # instance, or raise an error.\\n        for key, value in six.iteritems(kwargs):\\n            setattr(self, key, value)\\n\\n    @classonlymethod\\n    def as_view(cls, **initkwargs):\\n        \\"\\"\\"\\n        Main entry point for a request-response process.\\n        \\"\\"\\"\\n        # sanitize keyword arguments\\n        for key in initkwargs:\\n            if key in cls.http_method_names:\\n                raise TypeError(\\"You tried to pass in the %s method name as a \\"\\n                                \\"keyword argument to %s(). Don\'t do that.\\"\\n                                % (key, cls.__name__))\\n            if not hasattr(cls, key):\\n                raise TypeError(\\"%s() received an invalid keyword %r. as_view \\"\\n                                \\"only accepts arguments that are already \\"\\n                                \\"attributes of the class.\\" % (cls.__name__, key))\\n\\n        def view(request, *args, **kwargs):\\n            self = cls(**initkwargs)\\n            if hasattr(self, \'get\') and not hasattr(self, \'head\'):\\n                self.head = self.get\\n            self.request = request\\n            self.args = args\\n            self.kwargs = kwargs\\n            return self.dispatch(request, *args, **kwargs)\\n\\n        # take name and docstring from class\\n        update_wrapper(view, cls, updated=())\\n\\n        # and possible attributes set by decorators\\n        # like csrf_exempt from dispatch\\n        update_wrapper(view, cls.dispatch, assigned=())\\n        return view\\n\\n    def dispatch(self, request, *args, **kwargs):\\n        # Try to dispatch to the right method; if a method doesn\'t exist,\\n        # defer to the error handler. Also defer to the error handler if the\\n        # request method isn\'t on the approved list.\\n        if request.method.lower() in self.http_method_names:\\n            handler = getattr(self, request.method.lower(), self.http_method_not_allowed)\\n        else:\\n            handler = self.http_method_not_allowed\\n        return handler(request, *args, **kwargs)\\n\\n    def http_method_not_allowed(self, request, *args, **kwargs):\\n        logger.warning(\'Method Not Allowed (%s): %s\', request.method, request.path,\\n            extra={\\n                \'status_code\': 405,\\n                \'request\': self.request\\n          }\\n        )\\n        return http.HttpResponseNotAllowed(self._allowed_methods())\\n\\n    def options(self, request, *args, **kwargs):\\n        \\"\\"\\"\\n        Handles responding to requests for the OPTIONS HTTP verb.\\n        \\"\\"\\"\\n        response = http.HttpResponse()\\n        response[\'Allow\'] = \', \'.join(self._allowed_methods())\\n        response[\'Content-Length\'] = \'0\'\\n        return response\\n\\n    def _allowed_methods(self):\\n        return [m.upper() for m in self.http_method_names if hasattr(self, m)]\\n\\n\\nclass TemplateResponseMixin(object):\\n    \\"\\"\\"\\n    A mixin that can be used to render a template.\\n    \\"\\"\\"\\n    template_name = None\\n    response_class = TemplateResponse\\n    content_type = None\\n\\n    def render_to_response(self, context, **response_kwargs):\\n        \\"\\"\\"\\n        Returns a response, using the `response_class` for this\\n        view, with a template rendered with the given context.\\n\\n        If any keyword arguments are provided, they will be\\n        passed to the constructor of the response class.\\n        \\"\\"\\"\\n        response_kwargs.setdefault(\'content_type\', self.content_type)\\n        return self.response_class(\\n            request = self.request,\\n            template = self.get_template_names(),\\n            context = context,\\n            **response_kwargs\\n        )\\n\\n    def get_template_names(self):\\n        \\"\\"\\"\\n        Returns a list of template names to be used for the request. Must return\\n        a list. May not be called if render_to_response is overridden.\\n        \\"\\"\\"\\n        if self.template_name is None:\\n            raise ImproperlyConfigured(\\n                \\"TemplateResponseMixin requires either a definition of \\"\\n                \\"\'template_name\' or an implementation of \'get_template_names()\'\\")\\n        else:\\n            return [self.template_name]\\n\\n\\nclass TemplateView(TemplateResponseMixin, ContextMixin, View):\\n    \\"\\"\\"\\n    A view that renders a template.  This view will also pass into the context\\n    any keyword arguments passed by the url conf.\\n    \\"\\"\\"\\n    def get(self, request, *args, **kwargs):\\n        context = self.get_context_data(**kwargs)\\n        return self.render_to_response(context)\\n\\n\\nclass RedirectView(View):\\n    \\"\\"\\"\\n    A view that provides a redirect on any GET request.\\n    \\"\\"\\"\\n    permanent = True\\n    url = None\\n    pattern_name = None\\n    query_string = False\\n\\n    def get_redirect_url(self, *args, **kwargs):\\n        \\"\\"\\"\\n        Return the URL redirect to. Keyword arguments from the\\n        URL pattern match generating the redirect request\\n        are provided as kwargs to this method.\\n        \\"\\"\\"\\n        if self.url:\\n            url = self.url % kwargs\\n        elif self.pattern_name:\\n            try:\\n                url = reverse(self.pattern_name, args=args, kwargs=kwargs)\\n            except NoReverseMatch:\\n                return None\\n        else:\\n            return None\\n\\n        args = self.request.META.get(\'QUERY_STRING\', \'\')\\n        if args and self.query_string:\\n            url = \\"%s?%s\\" % (url, args)\\n        return url\\n\\n    def get(self, request, *args, **kwargs):\\n        url = self.get_redirect_url(*args, **kwargs)\\n        if url:\\n            if self.permanent:\\n                return http.HttpResponsePermanentRedirect(url)\\n            else:\\n                return http.HttpResponseRedirect(url)\\n        else:\\n            logger.warning(\'Gone: %s\', self.request.path,\\n                        extra={\\n                            \'status_code\': 410,\\n                            \'request\': self.request\\n                      })\\n            return http.HttpResponseGone()\\n\\n    def head(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def post(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def options(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def delete(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def put(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def patch(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n" }\n'
line: b'{ "repo_name": "mapr/hue", "ref": "refs/heads/hue-3.9.0-mapr", "path": "desktop/core/ext-py/Django-1.6.10/django/views/generic/base.py", "content": "from __future__ import unicode_literals\\n\\nimport logging\\nfrom functools import update_wrapper\\n\\nfrom django import http\\nfrom django.core.exceptions import ImproperlyConfigured\\nfrom django.core.urlresolvers import reverse, NoReverseMatch\\nfrom django.template.response import TemplateResponse\\nfrom django.utils.decorators import classonlymethod\\nfrom django.utils import six\\n\\nlogger = logging.getLogger(\'django.request\')\\n\\n\\nclass ContextMixin(object):\\n    \\"\\"\\"\\n    A default context mixin that passes the keyword arguments received by\\n    get_context_data as the template context.\\n    \\"\\"\\"\\n\\n    def get_context_data(self, **kwargs):\\n        if \'view\' not in kwargs:\\n            kwargs[\'view\'] = self\\n        return kwargs\\n\\n\\nclass View(object):\\n    \\"\\"\\"\\n    Intentionally simple parent class for all views. Only implements\\n    dispatch-by-method and simple sanity checking.\\n    \\"\\"\\"\\n\\n    http_method_names = [\'get\', \'post\', \'put\', \'patch\', \'delete\', \'head\', \'options\', \'trace\']\\n\\n    def __init__(self, **kwargs):\\n        \\"\\"\\"\\n        Constructor. Called in the URLconf; can contain helpful extra\\n        keyword arguments, and other things.\\n        \\"\\"\\"\\n        # Go through keyword arguments, and either save their values to our\\n        # instance, or raise an error.\\n        for key, value in six.iteritems(kwargs):\\n            setattr(self, key, value)\\n\\n    @classonlymethod\\n    def as_view(cls, **initkwargs):\\n        \\"\\"\\"\\n        Main entry point for a request-response process.\\n        \\"\\"\\"\\n        # sanitize keyword arguments\\n        for key in initkwargs:\\n            if key in cls.http_method_names:\\n                raise TypeError(\\"You tried to pass in the %s method name as a \\"\\n                                \\"keyword argument to %s(). Don\'t do that.\\"\\n                                % (key, cls.__name__))\\n            if not hasattr(cls, key):\\n                raise TypeError(\\"%s() received an invalid keyword %r. as_view \\"\\n                                \\"only accepts arguments that are already \\"\\n                                \\"attributes of the class.\\" % (cls.__name__, key))\\n\\n        def view(request, *args, **kwargs):\\n            self = cls(**initkwargs)\\n            if hasattr(self, \'get\') and not hasattr(self, \'head\'):\\n                self.head = self.get\\n            self.request = request\\n            self.args = args\\n            self.kwargs = kwargs\\n            return self.dispatch(request, *args, **kwargs)\\n\\n        # take name and docstring from class\\n        update_wrapper(view, cls, updated=())\\n\\n        # and possible attributes set by decorators\\n        # like csrf_exempt from dispatch\\n        update_wrapper(view, cls.dispatch, assigned=())\\n        return view\\n\\n    def dispatch(self, request, *args, **kwargs):\\n        # Try to dispatch to the right method; if a method doesn\'t exist,\\n        # defer to the error handler. Also defer to the error handler if the\\n        # request method isn\'t on the approved list.\\n        if request.method.lower() in self.http_method_names:\\n            handler = getattr(self, request.method.lower(), self.http_method_not_allowed)\\n        else:\\n            handler = self.http_method_not_allowed\\n        return handler(request, *args, **kwargs)\\n\\n    def http_method_not_allowed(self, request, *args, **kwargs):\\n        logger.warning(\'Method Not Allowed (%s): %s\', request.method, request.path,\\n            extra={\\n                \'status_code\': 405,\\n                \'request\': self.request\\n          }\\n        )\\n        return http.HttpResponseNotAllowed(self._allowed_methods())\\n\\n    def options(self, request, *args, **kwargs):\\n        \\"\\"\\"\\n        Handles responding to requests for the OPTIONS HTTP verb.\\n        \\"\\"\\"\\n        response = http.HttpResponse()\\n        response[\'Allow\'] = \', \'.join(self._allowed_methods())\\n        response[\'Content-Length\'] = \'0\'\\n        return response\\n\\n    def _allowed_methods(self):\\n        return [m.upper() for m in self.http_method_names if hasattr(self, m)]\\n\\n\\nclass TemplateResponseMixin(object):\\n    \\"\\"\\"\\n    A mixin that can be used to render a template.\\n    \\"\\"\\"\\n    template_name = None\\n    response_class = TemplateResponse\\n    content_type = None\\n\\n    def render_to_response(self, context, **response_kwargs):\\n        \\"\\"\\"\\n        Returns a response, using the `response_class` for this\\n        view, with a template rendered with the given context.\\n\\n        If any keyword arguments are provided, they will be\\n        passed to the constructor of the response class.\\n        \\"\\"\\"\\n        response_kwargs.setdefault(\'content_type\', self.content_type)\\n        return self.response_class(\\n            request = self.request,\\n            template = self.get_template_names(),\\n            context = context,\\n            **response_kwargs\\n        )\\n\\n    def get_template_names(self):\\n        \\"\\"\\"\\n        Returns a list of template names to be used for the request. Must return\\n        a list. May not be called if render_to_response is overridden.\\n        \\"\\"\\"\\n        if self.template_name is None:\\n            raise ImproperlyConfigured(\\n                \\"TemplateResponseMixin requires either a definition of \\"\\n                \\"\'template_name\' or an implementation of \'get_template_names()\'\\")\\n        else:\\n            return [self.template_name]\\n\\n\\nclass TemplateView(TemplateResponseMixin, ContextMixin, View):\\n    \\"\\"\\"\\n    A view that renders a template.  This view will also pass into the context\\n    any keyword arguments passed by the url conf.\\n    \\"\\"\\"\\n    def get(self, request, *args, **kwargs):\\n        context = self.get_context_data(**kwargs)\\n        return self.render_to_response(context)\\n\\n\\nclass RedirectView(View):\\n    \\"\\"\\"\\n    A view that provides a redirect on any GET request.\\n    \\"\\"\\"\\n    permanent = True\\n    url = None\\n    pattern_name = None\\n    query_string = False\\n\\n    def get_redirect_url(self, *args, **kwargs):\\n        \\"\\"\\"\\n        Return the URL redirect to. Keyword arguments from the\\n        URL pattern match generating the redirect request\\n        are provided as kwargs to this method.\\n        \\"\\"\\"\\n        if self.url:\\n            url = self.url % kwargs\\n        elif self.pattern_name:\\n            try:\\n                url = reverse(self.pattern_name, args=args, kwargs=kwargs)\\n            except NoReverseMatch:\\n                return None\\n        else:\\n            return None\\n\\n        args = self.request.META.get(\'QUERY_STRING\', \'\')\\n        if args and self.query_string:\\n            url = \\"%s?%s\\" % (url, args)\\n        return url\\n\\n    def get(self, request, *args, **kwargs):\\n        url = self.get_redirect_url(*args, **kwargs)\\n        if url:\\n            if self.permanent:\\n                return http.HttpResponsePermanentRedirect(url)\\n            else:\\n                return http.HttpResponseRedirect(url)\\n        else:\\n            logger.warning(\'Gone: %s\', self.request.path,\\n                        extra={\\n                            \'status_code\': 410,\\n                            \'request\': self.request\\n                      })\\n            return http.HttpResponseGone()\\n\\n    def head(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def post(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def options(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def delete(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def put(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def patch(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n" }\n'
line: b'{ "repo_name": "felixjimenez/django", "ref": "refs/heads/nonrel-1.6", "path": "django/views/generic/base.py", "content": "from __future__ import unicode_literals\\n\\nimport logging\\nfrom functools import update_wrapper\\n\\nfrom django import http\\nfrom django.core.exceptions import ImproperlyConfigured\\nfrom django.core.urlresolvers import reverse, NoReverseMatch\\nfrom django.template.response import TemplateResponse\\nfrom django.utils.decorators import classonlymethod\\nfrom django.utils import six\\n\\nlogger = logging.getLogger(\'django.request\')\\n\\n\\nclass ContextMixin(object):\\n    \\"\\"\\"\\n    A default context mixin that passes the keyword arguments received by\\n    get_context_data as the template context.\\n    \\"\\"\\"\\n\\n    def get_context_data(self, **kwargs):\\n        if \'view\' not in kwargs:\\n            kwargs[\'view\'] = self\\n        return kwargs\\n\\n\\nclass View(object):\\n    \\"\\"\\"\\n    Intentionally simple parent class for all views. Only implements\\n    dispatch-by-method and simple sanity checking.\\n    \\"\\"\\"\\n\\n    http_method_names = [\'get\', \'post\', \'put\', \'patch\', \'delete\', \'head\', \'options\', \'trace\']\\n\\n    def __init__(self, **kwargs):\\n        \\"\\"\\"\\n        Constructor. Called in the URLconf; can contain helpful extra\\n        keyword arguments, and other things.\\n        \\"\\"\\"\\n        # Go through keyword arguments, and either save their values to our\\n        # instance, or raise an error.\\n        for key, value in six.iteritems(kwargs):\\n            setattr(self, key, value)\\n\\n    @classonlymethod\\n    def as_view(cls, **initkwargs):\\n        \\"\\"\\"\\n        Main entry point for a request-response process.\\n        \\"\\"\\"\\n        # sanitize keyword arguments\\n        for key in initkwargs:\\n            if key in cls.http_method_names:\\n                raise TypeError(\\"You tried to pass in the %s method name as a \\"\\n                                \\"keyword argument to %s(). Don\'t do that.\\"\\n                                % (key, cls.__name__))\\n            if not hasattr(cls, key):\\n                raise TypeError(\\"%s() received an invalid keyword %r. as_view \\"\\n                                \\"only accepts arguments that are already \\"\\n                                \\"attributes of the class.\\" % (cls.__name__, key))\\n\\n        def view(request, *args, **kwargs):\\n            self = cls(**initkwargs)\\n            if hasattr(self, \'get\') and not hasattr(self, \'head\'):\\n                self.head = self.get\\n            self.request = request\\n            self.args = args\\n            self.kwargs = kwargs\\n            return self.dispatch(request, *args, **kwargs)\\n\\n        # take name and docstring from class\\n        update_wrapper(view, cls, updated=())\\n\\n        # and possible attributes set by decorators\\n        # like csrf_exempt from dispatch\\n        update_wrapper(view, cls.dispatch, assigned=())\\n        return view\\n\\n    def dispatch(self, request, *args, **kwargs):\\n        # Try to dispatch to the right method; if a method doesn\'t exist,\\n        # defer to the error handler. Also defer to the error handler if the\\n        # request method isn\'t on the approved list.\\n        if request.method.lower() in self.http_method_names:\\n            handler = getattr(self, request.method.lower(), self.http_method_not_allowed)\\n        else:\\n            handler = self.http_method_not_allowed\\n        return handler(request, *args, **kwargs)\\n\\n    def http_method_not_allowed(self, request, *args, **kwargs):\\n        logger.warning(\'Method Not Allowed (%s): %s\', request.method, request.path,\\n            extra={\\n                \'status_code\': 405,\\n                \'request\': self.request\\n          }\\n        )\\n        return http.HttpResponseNotAllowed(self._allowed_methods())\\n\\n    def options(self, request, *args, **kwargs):\\n        \\"\\"\\"\\n        Handles responding to requests for the OPTIONS HTTP verb.\\n        \\"\\"\\"\\n        response = http.HttpResponse()\\n        response[\'Allow\'] = \', \'.join(self._allowed_methods())\\n        response[\'Content-Length\'] = \'0\'\\n        return response\\n\\n    def _allowed_methods(self):\\n        return [m.upper() for m in self.http_method_names if hasattr(self, m)]\\n\\n\\nclass TemplateResponseMixin(object):\\n    \\"\\"\\"\\n    A mixin that can be used to render a template.\\n    \\"\\"\\"\\n    template_name = None\\n    response_class = TemplateResponse\\n    content_type = None\\n\\n    def render_to_response(self, context, **response_kwargs):\\n        \\"\\"\\"\\n        Returns a response, using the `response_class` for this\\n        view, with a template rendered with the given context.\\n\\n        If any keyword arguments are provided, they will be\\n        passed to the constructor of the response class.\\n        \\"\\"\\"\\n        response_kwargs.setdefault(\'content_type\', self.content_type)\\n        return self.response_class(\\n            request = self.request,\\n            template = self.get_template_names(),\\n            context = context,\\n            **response_kwargs\\n        )\\n\\n    def get_template_names(self):\\n        \\"\\"\\"\\n        Returns a list of template names to be used for the request. Must return\\n        a list. May not be called if render_to_response is overridden.\\n        \\"\\"\\"\\n        if self.template_name is None:\\n            raise ImproperlyConfigured(\\n                \\"TemplateResponseMixin requires either a definition of \\"\\n                \\"\'template_name\' or an implementation of \'get_template_names()\'\\")\\n        else:\\n            return [self.template_name]\\n\\n\\nclass TemplateView(TemplateResponseMixin, ContextMixin, View):\\n    \\"\\"\\"\\n    A view that renders a template.  This view will also pass into the context\\n    any keyword arguments passed by the url conf.\\n    \\"\\"\\"\\n    def get(self, request, *args, **kwargs):\\n        context = self.get_context_data(**kwargs)\\n        return self.render_to_response(context)\\n\\n\\nclass RedirectView(View):\\n    \\"\\"\\"\\n    A view that provides a redirect on any GET request.\\n    \\"\\"\\"\\n    permanent = True\\n    url = None\\n    pattern_name = None\\n    query_string = False\\n\\n    def get_redirect_url(self, *args, **kwargs):\\n        \\"\\"\\"\\n        Return the URL redirect to. Keyword arguments from the\\n        URL pattern match generating the redirect request\\n        are provided as kwargs to this method.\\n        \\"\\"\\"\\n        if self.url:\\n            url = self.url % kwargs\\n        elif self.pattern_name:\\n            try:\\n                url = reverse(self.pattern_name, args=args, kwargs=kwargs)\\n            except NoReverseMatch:\\n                return None\\n        else:\\n            return None\\n\\n        args = self.request.META.get(\'QUERY_STRING\', \'\')\\n        if args and self.query_string:\\n            url = \\"%s?%s\\" % (url, args)\\n        return url\\n\\n    def get(self, request, *args, **kwargs):\\n        url = self.get_redirect_url(*args, **kwargs)\\n        if url:\\n            if self.permanent:\\n                return http.HttpResponsePermanentRedirect(url)\\n            else:\\n                return http.HttpResponseRedirect(url)\\n        else:\\n            logger.warning(\'Gone: %s\', self.request.path,\\n                        extra={\\n                            \'status_code\': 410,\\n                            \'request\': self.request\\n                      })\\n            return http.HttpResponseGone()\\n\\n    def head(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def post(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def options(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def delete(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def put(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def patch(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n" }\n'
line: b'{ "repo_name": "arviz-devs/arviz", "ref": "refs/heads/gh-pages", "path": "_downloads/f3778c5bef269387485b4113d05d6cd6/bokeh_plot_ppc.py", "content": "\\"\\"\\"\\nPosterior Predictive Check Plot\\n===============================\\n\\n_thumb: .6, .5\\n\\"\\"\\"\\nimport arviz as az\\n\\ndata = az.load_arviz_data(\\"non_centered_eight\\")\\nax = az.plot_ppc(data, alpha=0.03, figsize=(12, 6), backend=\\"bokeh\\")\\n" }\n'
line: b'{ "repo_name": "cisco-openstack/neutron", "ref": "refs/heads/staging/libertyplus", "path": "neutron/tests/unit/plugins/ml2/drivers/openvswitch/agent/openflow/ovs_ofctl/test_br_phys.py", "content": "# Copyright (C) 2014,2015 VA Linux Systems Japan K.K.\\n# Copyright (C) 2014,2015 YAMAMOTO Takashi <yamamoto at valinux co jp>\\n# All Rights Reserved.\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\nimport mock\\n\\nimport neutron.plugins.ml2.drivers.openvswitch.agent.common.constants \\\\\\n    as ovs_const\\nfrom neutron.tests.unit.plugins.ml2.drivers.openvswitch.agent.\\\\\\n    openflow.ovs_ofctl import ovs_bridge_test_base\\n\\n\\ncall = mock.call  # short hand\\n\\n\\nclass OVSPhysicalBridgeTest(ovs_bridge_test_base.OVSBridgeTestBase,\\n                            ovs_bridge_test_base.OVSDVRProcessTestMixin):\\n    dvr_process_table_id = ovs_const.DVR_PROCESS_VLAN\\n    dvr_process_next_table_id = ovs_const.LOCAL_VLAN_TRANSLATION\\n\\n    def setUp(self):\\n        super(OVSPhysicalBridgeTest, self).setUp()\\n        self.setup_bridge_mock(\'br-phys\', self.br_phys_cls)\\n\\n    def test_setup_default_table(self):\\n        self.br.setup_default_table()\\n        expected = [\\n            call.delete_flows(),\\n            call.add_flow(priority=0, table=0, actions=\'normal\'),\\n        ]\\n        self.assertEqual(expected, self.mock.mock_calls)\\n\\n    def test_provision_local_vlan(self):\\n        port = 999\\n        lvid = 888\\n        segmentation_id = 777\\n        distributed = False\\n        self.br.provision_local_vlan(port=port, lvid=lvid,\\n                                     segmentation_id=segmentation_id,\\n                                     distributed=distributed)\\n        expected = [\\n            call.add_flow(priority=4, table=0, dl_vlan=lvid, in_port=port,\\n                          actions=\'mod_vlan_vid:%s,normal\' % segmentation_id),\\n        ]\\n        self.assertEqual(expected, self.mock.mock_calls)\\n\\n    def test_provision_local_vlan_novlan(self):\\n        port = 999\\n        lvid = 888\\n        segmentation_id = None\\n        distributed = False\\n        self.br.provision_local_vlan(port=port, lvid=lvid,\\n                                     segmentation_id=segmentation_id,\\n                                     distributed=distributed)\\n        expected = [\\n            call.add_flow(priority=4, table=0, dl_vlan=lvid, in_port=port,\\n                          actions=\'strip_vlan,normal\')\\n        ]\\n        self.assertEqual(expected, self.mock.mock_calls)\\n\\n    def test_reclaim_local_vlan(self):\\n        port = 999\\n        lvid = 888\\n        self.br.reclaim_local_vlan(port=port, lvid=lvid)\\n        expected = [\\n            call.delete_flows(dl_vlan=lvid, in_port=port),\\n        ]\\n        self.assertEqual(expected, self.mock.mock_calls)\\n\\n    def test_add_dvr_mac_vlan(self):\\n        mac = \'00:02:b3:13:fe:3d\'\\n        port = 8888\\n        self.br.add_dvr_mac_vlan(mac=mac, port=port)\\n        expected = [\\n            call.add_flow(priority=2, table=3, dl_src=mac,\\n                          actions=\'output:%s\' % port),\\n        ]\\n        self.assertEqual(expected, self.mock.mock_calls)\\n\\n    def test_remove_dvr_mac_vlan(self):\\n        mac = \'00:02:b3:13:fe:3d\'\\n        self.br.remove_dvr_mac_vlan(mac=mac)\\n        expected = [\\n            call.delete_flows(eth_src=mac, table_id=3),\\n        ]\\n        self.assertEqual(expected, self.mock.mock_calls)\\n" }\n'
line: b'{ "repo_name": "dahfool/navigator", "ref": "refs/heads/release", "path": "markets/tests/__init__.py", "content": "import os\\nfrom markets.models import Market, Logo\\nfrom geography.models import Country\\nfrom products.models import Category\\n\\n\\nCURRENT_DIRECTORY = os.path.dirname(os.path.abspath(__file__))\\n\\n\\ndef create_market(**variable_data):\\n\\n    market_data = get_market_data(**variable_data)\\n\\n    if \'operating_countries\' not in variable_data:\\n        country = create_country(\'UK\')\\n        operating_countries = [country]\\n    else:\\n        operating_countries = market_data.pop(\'operating_countries\')\\n\\n    if \'product_categories\' not in variable_data:\\n        category = create_category(\'Toys\')\\n        product_categories = [category]\\n    else:\\n        product_categories = market_data.pop(\'product_categories\')\\n\\n    if \'name\' not in variable_data:\\n        variable_data[\'name\'] = \\"Amazon\\"\\n\\n    if \'logo\' not in variable_data:\\n        logo = create_logo()\\n    else:\\n        logo = market_data.pop(\'logo\')\\n\\n    market = Market(**market_data)\\n    market.save()\\n    market.operating_countries = operating_countries\\n    market.product_categories = product_categories\\n    market.logo = logo\\n    market.save()\\n    return market\\n\\n\\ndef get_market_data(**variable_data):\\n    \\"\\"\\"\\n    All basic fields that must be filled in to for a Market to be clean\\n    \\"\\"\\"\\n\\n    if \'name\' not in variable_data:\\n        variable_data[\'name\'] = \\"Amazon\\"\\n\\n    if \'e_marketplace_description\' not in variable_data:\\n        variable_data[\'e_marketplace_description\'] = \\"Lorem Ipsum\\"\\n\\n    if \'web_address\' not in variable_data:\\n        variable_data[\'web_address\'] = \\"example.com\\"\\n\\n    if \'one_off_registration_fee\' not in variable_data:\\n        variable_data[\'one_off_registration_fee\'] = 0\\n\\n    if \'membership_fees\' not in variable_data:\\n        variable_data[\'membership_fees\'] = 0\\n\\n    if \'fee_per_listing\' not in variable_data:\\n        variable_data[\'fee_per_listing\'] = True\\n\\n    if \'deposit\' not in variable_data:\\n        variable_data[\'deposit\'] = 0\\n\\n    return variable_data\\n\\n\\ndef create_logo(**variable_data):\\n    if \'name\' not in variable_data:\\n        variable_data[\'name\'] = \'logo\'\\n    if \'_encoded_data\' not in variable_data:\\n        variable_data[\'_encoded_data\'] = (\\n            \'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HA\'\\n            \'wCAAAAC0lEQVR4nGP6LwkAAiABG+faPgsAAAAASUVORK5CYII=\')\\n    logo = Logo(**variable_data)\\n    logo.save()\\n    return logo\\n\\n\\ndef create_country(name):\\n    country, created = Country.objects.get_or_create(name=name)\\n    return country\\n\\n\\ndef create_category(name):\\n    category, created = Category.objects.get_or_create(name=name)\\n    return category\\n\\n\\ndef load_sample_png():\\n    f = open(\\"{}/png/sample.png\\".format(CURRENT_DIRECTORY), \\"rb\\")\\n    return f\\n" }\n'
line: b'{ "repo_name": "skosukhin/spack", "ref": "refs/heads/esiwace", "path": "var/spack/repos/builtin/packages/py-ont-fast5-api/package.py", "content": "##############################################################################\\n# Copyright (c) 2013-2017, Lawrence Livermore National Security, LLC.\\n# Produced at the Lawrence Livermore National Laboratory.\\n#\\n# This file is part of Spack.\\n# Created by Todd Gamblin, tgamblin@llnl.gov, All rights reserved.\\n# LLNL-CODE-647188\\n#\\n# For details, see https://github.com/spack/spack\\n# Please also see the NOTICE and LICENSE files for our notice and the LGPL.\\n#\\n# This program is free software; you can redistribute it and/or modify\\n# it under the terms of the GNU Lesser General Public License (as\\n# published by the Free Software Foundation) version 2.1, February 1999.\\n#\\n# This program is distributed in the hope that it will be useful, but\\n# WITHOUT ANY WARRANTY; without even the IMPLIED WARRANTY OF\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the terms and\\n# conditions of the GNU Lesser General Public License for more details.\\n#\\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this program; if not, write to the Free Software\\n# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA\\n##############################################################################\\nfrom spack import *\\n\\n\\nclass PyOntFast5Api(PythonPackage):\\n    \\"\\"\\"This project provides classes and utility functions for working with\\n    read fast5 files. It provides an abstraction layer between the underlying\\n    h5py library and the various concepts central to read fast5 files, such as\\n    \\"reads\\", \\"analyses\\", \\"analysis summaries\\", and \\"analysis datasets\\".\\n    Ideally all interaction with a read fast5 file should be possible via this\\n    API, without having to directly invoke the h5py library.\\"\\"\\"\\n\\n    homepage = \\"https://github.com/nanoporetech/ont_fast5_api\\"\\n    url      = \\"https://pypi.io/packages/source/o/ont-fast5-api/ont-fast5-api-0.3.2.tar.gz\\"\\n\\n    version(\'0.3.2\', \'2ccfdbcd55239ffae712bb6e70ebfe8c\')\\n\\n    depends_on(\'py-setuptools\', type=\'build\')\\n    depends_on(\'py-h5py\', type=(\'build\', \'run\'))\\n    depends_on(\'py-numpy@1.8.1:\', type=(\'build\', \'run\'))\\n" }\n'
line: b'{ "repo_name": "jumpojoy/neutron", "ref": "refs/heads/generic_switch", "path": "neutron/tests/unit/plugins/ml2/drivers/openvswitch/agent/openflow/ovs_ofctl/test_br_phys.py", "content": "# Copyright (C) 2014,2015 VA Linux Systems Japan K.K.\\n# Copyright (C) 2014,2015 YAMAMOTO Takashi <yamamoto at valinux co jp>\\n# All Rights Reserved.\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\nimport mock\\n\\nimport neutron.plugins.ml2.drivers.openvswitch.agent.common.constants \\\\\\n    as ovs_const\\nfrom neutron.tests.unit.plugins.ml2.drivers.openvswitch.agent.\\\\\\n    openflow.ovs_ofctl import ovs_bridge_test_base\\n\\n\\ncall = mock.call  # short hand\\n\\n\\nclass OVSPhysicalBridgeTest(ovs_bridge_test_base.OVSBridgeTestBase,\\n                            ovs_bridge_test_base.OVSDVRProcessTestMixin):\\n    dvr_process_table_id = ovs_const.DVR_PROCESS_VLAN\\n    dvr_process_next_table_id = ovs_const.LOCAL_VLAN_TRANSLATION\\n\\n    def setUp(self):\\n        super(OVSPhysicalBridgeTest, self).setUp()\\n        self.setup_bridge_mock(\'br-phys\', self.br_phys_cls)\\n\\n    def test_setup_default_table(self):\\n        self.br.setup_default_table()\\n        expected = [\\n            call.delete_flows(),\\n            call.add_flow(priority=0, table=0, actions=\'normal\'),\\n        ]\\n        self.assertEqual(expected, self.mock.mock_calls)\\n\\n    def test_provision_local_vlan(self):\\n        port = 999\\n        lvid = 888\\n        segmentation_id = 777\\n        distributed = False\\n        self.br.provision_local_vlan(port=port, lvid=lvid,\\n                                     segmentation_id=segmentation_id,\\n                                     distributed=distributed)\\n        expected = [\\n            call.add_flow(priority=4, table=0, dl_vlan=lvid, in_port=port,\\n                          actions=\'mod_vlan_vid:%s,normal\' % segmentation_id),\\n        ]\\n        self.assertEqual(expected, self.mock.mock_calls)\\n\\n    def test_provision_local_vlan_novlan(self):\\n        port = 999\\n        lvid = 888\\n        segmentation_id = None\\n        distributed = False\\n        self.br.provision_local_vlan(port=port, lvid=lvid,\\n                                     segmentation_id=segmentation_id,\\n                                     distributed=distributed)\\n        expected = [\\n            call.add_flow(priority=4, table=0, dl_vlan=lvid, in_port=port,\\n                          actions=\'strip_vlan,normal\')\\n        ]\\n        self.assertEqual(expected, self.mock.mock_calls)\\n\\n    def test_reclaim_local_vlan(self):\\n        port = 999\\n        lvid = 888\\n        self.br.reclaim_local_vlan(port=port, lvid=lvid)\\n        expected = [\\n            call.delete_flows(dl_vlan=lvid, in_port=port),\\n        ]\\n        self.assertEqual(expected, self.mock.mock_calls)\\n\\n    def test_add_dvr_mac_vlan(self):\\n        mac = \'00:02:b3:13:fe:3d\'\\n        port = 8888\\n        self.br.add_dvr_mac_vlan(mac=mac, port=port)\\n        expected = [\\n            call.add_flow(priority=2, table=3, dl_src=mac,\\n                          actions=\'output:%s\' % port),\\n        ]\\n        self.assertEqual(expected, self.mock.mock_calls)\\n\\n    def test_remove_dvr_mac_vlan(self):\\n        mac = \'00:02:b3:13:fe:3d\'\\n        self.br.remove_dvr_mac_vlan(mac=mac)\\n        expected = [\\n            call.delete_flows(eth_src=mac, table_id=3),\\n        ]\\n        self.assertEqual(expected, self.mock.mock_calls)\\n" }\n'
line: b'{ "repo_name": "kaplun/ops", "ref": "refs/heads/prod", "path": "modules/bibformat/lib/elements/bfe_authors.py", "content": "# -*- coding: utf-8 -*-\\n##\\n## This file is part of Invenio.\\n## Copyright (C) 2006, 2007, 2008, 2009, 2010, 2011, 2013 CERN.\\n##\\n## Invenio is free software; you can redistribute it and/or\\n## modify it under the terms of the GNU General Public License as\\n## published by the Free Software Foundation; either version 2 of the\\n## License, or (at your option) any later version.\\n##\\n## Invenio is distributed in the hope that it will be useful, but\\n## WITHOUT ANY WARRANTY; without even the implied warranty of\\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n## General Public License for more details.\\n##\\n## You should have received a copy of the GNU General Public License\\n## along with Invenio; if not, write to the Free Software Foundation, Inc.,\\n## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\\n\\"\\"\\"BibFormat element - Prints authors\\n\\"\\"\\"\\n__revision__ = \\"$Id$\\"\\n\\nimport re\\nfrom urllib import quote\\nfrom cgi import escape\\nfrom invenio.config import CFG_BASE_URL, CFG_SITE_RECORD\\nfrom invenio.messages import gettext_set_language\\nfrom invenio.bibauthority_config import \\\\\\n    CFG_BIBAUTHORITY_AUTHORITY_COLLECTION_NAME, \\\\\\n    CFG_BIBAUTHORITY_TYPE_NAMES, \\\\\\n    CFG_BIBAUTHORITY_PREFIX_SEP\\nfrom invenio.bibauthority_engine import \\\\\\n    get_low_level_recIDs_from_control_no\\n\\ndef format_element(bfo, limit, separator=\' ; \',\\n           extension=\'[...]\',\\n           print_links=\\"yes\\",\\n           print_affiliations=\'no\',\\n           affiliation_prefix=\' (\',\\n           affiliation_suffix=\')\',\\n           interactive=\\"no\\",\\n           highlight=\\"no\\",\\n           link_author_pages=\\"no\\",\\n           link_mobile_pages=\\"no\\",\\n           relator_code_pattern=None):\\n    \\"\\"\\"\\n    Prints the list of authors of a record.\\n\\n    @param limit: the maximum number of authors to display\\n    @param separator: the separator between authors.\\n    @param extension: a text printed if more authors than \'limit\' exist\\n    @param print_links: if yes, prints the authors as HTML link to their publications\\n    @param print_affiliations: if yes, make each author name followed by its affiliation\\n    @param affiliation_prefix: prefix printed before each affiliation\\n    @param affiliation_suffix: suffix printed after each affiliation\\n    @param interactive: if yes, enable user to show/hide authors when there are too many (html + javascript)\\n    @param highlight: highlights authors corresponding to search query if set to \'yes\'\\n    @param link_author_pages: should we link to author pages if print_links in on?\\n    @param link_mobile_pages: should we link to mobile app pages if print_links in on?\\n    @param relator_code_pattern: a regular expression to filter authors based on subfield $4 (relator code)\\n    \\"\\"\\"\\n    _ = gettext_set_language(bfo.lang)    # load the right message language\\n\\n    authors = []\\n    authors_1 = bfo.fields(\'100__\', repeatable_subfields_p=True)\\n    authors_2 = bfo.fields(\'700__\', repeatable_subfields_p=True)\\n\\n    authors.extend(authors_1)\\n    authors.extend(authors_2)\\n\\n    # make unique string per key\\n    for author in authors:\\n        if \'a\' in author:\\n            author[\'a\'] = author[\'a\'][0]\\n        if \'u\' in author:\\n            author[\'u\'] = author[\'u\'][0]\\n        pattern = \'%s\' + CFG_BIBAUTHORITY_PREFIX_SEP + \\"(\\"\\n        for control_no in author.get(\'0\', []):\\n            if pattern % (CFG_BIBAUTHORITY_TYPE_NAMES[\\"INSTITUTION\\"]) in control_no:\\n                author[\'u0\'] = control_no # overwrite if multiples\\n            elif pattern % (CFG_BIBAUTHORITY_TYPE_NAMES[\\"AUTHOR\\"]) in control_no:\\n                author[\'a0\'] = control_no # overwrite if multiples\\n\\n\\n    if relator_code_pattern:\\n        p = re.compile(relator_code_pattern)\\n        authors = filter(lambda x: p.match(x.get(\'4\', \'\')), authors)\\n\\n    nb_authors = len(authors)\\n\\n    bibrec_id = bfo.control_field(\\"001\\")\\n\\n    # Process authors to add link, highlight and format affiliation\\n    for author in authors:\\n\\n        if author.has_key(\'a\'):\\n            if highlight == \'yes\':\\n                from invenio import bibformat_utils\\n                author[\'a\'] = bibformat_utils.highlight(author[\'a\'],\\n                                                        bfo.search_pattern)\\n\\n            if print_links.lower() == \\"yes\\":\\n                if link_author_pages == \\"yes\\":\\n                    author[\'a\'] = \'<a rel=\\"author\\" href=\\"\' + CFG_BASE_URL + \\\\\\n                                  \'/author/profile/\' + quote(author[\'a\']) + \\\\\\n                                  \'?recid=\' +  bibrec_id + \\\\\\n                                  \'&ln=\' + bfo.lang + \\\\\\n                                  \'\\">\' + escape(author[\'a\']) + \'</a>\'\\n                elif link_mobile_pages == \'yes\':\\n                    author[\'a\'] = \'<a rel=\\"external\\" href=\\"#page=search\' + \\\\\\n                                  \'&amp;f=author&amp;p=\' + quote(author[\'a\']) + \\\\\\n                                  \'\\">\' + escape(author[\'a\']) + \'</a>\'\\n                else:\\n                    auth_coll_param = \'\'\\n                    if \'a0\' in author:\\n                        recIDs = get_low_level_recIDs_from_control_no(author[\'a0\'])\\n                        if len(recIDs):\\n                            auth_coll_param = \'&amp;c=\' + \\\\\\n                                              CFG_BIBAUTHORITY_AUTHORITY_COLLECTION_NAME\\n                    author[\'a\'] = \'<a href=\\"\' + CFG_BASE_URL + \\\\\\n                                  \'/search?f=author&amp;p=\' + quote(author[\'a\']) + \\\\\\n                                   auth_coll_param + \\\\\\n                                  \'&amp;ln=\' + bfo.lang + \\\\\\n                                  \'\\">\' + escape(author[\'a\']) + \'</a>\'\\n\\n        if author.has_key(\'u\'):\\n            if print_affiliations == \\"yes\\":\\n                if \'u0\' in author:\\n                    recIDs = get_low_level_recIDs_from_control_no(author[\'u0\'])\\n                    # if there is more than 1 recID, clicking on link and\\n                    # thus displaying the authority record\'s page should\\n                    # contain a warning that there are multiple authority\\n                    # records with the same control number\\n                    if len(recIDs):\\n                        author[\'u\'] = \'<a href=\\"\' + CFG_BASE_URL + \'/\' + CFG_SITE_RECORD + \'/\' + \\\\\\n                                      str(recIDs[0]) + \\\\\\n                                      \'?ln=\' + bfo.lang + \\\\\\n                                      \'\\">\' + author[\'u\'] + \'</a>\'\\n                author[\'u\'] = affiliation_prefix + author[\'u\'] + \\\\\\n                              affiliation_suffix\\n\\n    # Flatten author instances\\n    if print_affiliations == \'yes\':\\n        authors = [author.get(\'a\', \'\') + author.get(\'u\', \'\')\\n                   for author in authors]\\n    else:\\n        authors = [author.get(\'a\', \'\')\\n                   for author in authors]\\n\\n    if limit.isdigit() and  nb_authors > int(limit) and interactive != \\"yes\\":\\n        return separator.join(authors[:int(limit)]) + extension\\n\\n    elif limit.isdigit() and nb_authors > int(limit) and interactive == \\"yes\\":\\n        out = \'<a name=\\"show_hide\\" />\'\\n        out += separator.join(authors[:int(limit)])\\n        out += \'<span id=\\"more_%s\\" style=\\"\\">\' % bibrec_id + separator + \\\\\\n               separator.join(authors[int(limit):]) + \'</span>\'\\n        out += \' <span id=\\"extension_%s\\"></span>\' % bibrec_id\\n        out += \' <small><i><a id=\\"link_%s\\" href=\\"#\\" style=\\"color:rgb(204,0,0);\\"></a></i></small>\' % bibrec_id\\n        out += \'\'\'\\n        <script type=\\"text/javascript\\">\\n        $(\'#link_%(recid)s\').click(function(event) {\\n            event.preventDefault();\\n            var more = document.getElementById(\'more_%(recid)s\');\\n            var link = document.getElementById(\'link_%(recid)s\');\\n            var extension = document.getElementById(\'extension_%(recid)s\');\\n            if (more.style.display==\'none\'){\\n                more.style.display = \'\';\\n                extension.style.display = \'none\';\\n                link.innerHTML = \\"%(show_less)s\\"\\n          } else {\\n                more.style.display = \'none\';\\n                extension.style.display = \'\';\\n                link.innerHTML = \\"%(show_more)s\\"\\n          }\\n            link.style.color = \\"rgb(204,0,0);\\"\\n      });\\n\\n        function set_up_%(recid)s(){\\n            var extension = document.getElementById(\'extension_%(recid)s\');\\n            extension.innerHTML = \\"%(extension)s\\";\\n            $(\'#link_%(recid)s\').click();\\n      }\\n\\n        </script>\\n        \'\'\' % {\'show_less\':_(\\"Hide\\"),\\n             \'show_more\':_(\\"Show all %i authors\\") % nb_authors,\\n             \'extension\':extension,\\n             \'recid\': bibrec_id}\\n        out += \'<script type=\\"text/javascript\\">set_up_%s()</script>\' % bibrec_id\\n\\n        return out\\n    elif nb_authors > 0:\\n        return separator.join(authors)\\n\\ndef escape_values(bfo):\\n    \\"\\"\\"\\n    Called by BibFormat in order to check if output of this element\\n    should be escaped.\\n    \\"\\"\\"\\n    return 0\\n" }\n'
line: b'{ "repo_name": "oasiswork/RatticWeb", "ref": "refs/heads/prod", "path": "cred/api.py", "content": "from django.core.exceptions import ObjectDoesNotExist, MultipleObjectsReturned\\nfrom django.core.files.base import File\\nfrom django.http import HttpResponse\\n\\nfrom tastypie import fields, http\\nfrom tastypie.authentication import SessionAuthentication, MultiAuthentication\\nfrom tastypie.validation import FormValidation\\nfrom tastypie.resources import ModelResource, ALL_WITH_RELATIONS\\nfrom tastypie.authorization import Authorization\\nfrom tastypie.exceptions import Unauthorized\\n\\nfrom account.authentication import MultiApiKeyAuthentication\\nfrom cred.models import Cred, Tag, CredAudit\\nfrom cred.forms import TagForm\\nfrom cred.ssh_key import SSHKey\\nfrom staff.api import GroupResource\\n\\nimport paramiko\\n\\n\\nclass CredAuthorization(Authorization):\\n    def read_list(self, object_list, bundle):\\n        # List views remove the deletes and historical credentials\\n        return object_list.filter(is_deleted=False, latest=None)\\n\\n    def read_detail(self, object_list, bundle):\\n        # Check user has perms\\n        if not bundle.obj.is_visible_by(bundle.request.user):\\n            return False\\n\\n        # This audit should go somewhere else,\\n        # is there a detail list function we can override?\\n        CredAudit(\\n            audittype=CredAudit.CREDPASSVIEW, cred=bundle.obj,\\n            user=bundle.request.user).save()\\n        return True\\n\\n    def create_list(self, object_list, bundle):\\n        # Assuming their auto-assigned to ``user``.\\n        raise Unauthorized(\\"Not yet implemented.\\")\\n\\n    def create_detail(self, object_list, bundle):\\n        return bundle.request.user.groups.filter(\\n            name=bundle.obj.group).exists()\\n\\n    def update_list(self, object_list, bundle):\\n        raise Unauthorized(\\"Not yet implemented.\\")\\n\\n    def update_detail(self, object_list, bundle):\\n        # Check user has perms\\n        if not bundle.obj.is_owned_by(bundle.request.user):\\n            return False\\n\\n        CredAudit(\\n            audittype=CredAudit.CREDCHANGE, cred=bundle.obj,\\n            user=bundle.request.user).save()\\n        return True\\n\\n    def delete_list(self, object_list, bundle):\\n        # Sorry user, no deletes for you!\\n        raise Unauthorized(\\"Not yet implemented.\\")\\n\\n    def delete_detail(self, object_list, bundle):\\n        # Check user has perms\\n        if not bundle.obj.is_owned_by(bundle.request.user):\\n            return False\\n\\n        CredAudit(\\n            audittype=CredAudit.CREDDELETE, cred=bundle.obj,\\n            user=bundle.request.user).save()\\n        return True\\n\\n\\nclass TagAuthorization(Authorization):\\n    def read_list(self, object_list, bundle):\\n        return object_list\\n\\n    def read_detail(self, object_list, bundle):\\n        return True\\n\\n    def create_list(self, object_list, bundle):\\n        # Assuming their auto-assigned to ``user``.\\n        raise Unauthorized(\\"Not yet implemented.\\")\\n\\n    def create_detail(self, object_list, bundle):\\n        return True\\n\\n    def update_list(self, object_list, bundle):\\n        raise Unauthorized(\\"Not yet implemented.\\")\\n\\n    def update_detail(self, object_list, bundle):\\n        raise Unauthorized(\\"Not yet implemented.\\")\\n\\n    def delete_list(self, object_list, bundle):\\n        # Sorry user, no deletes for you!\\n        raise Unauthorized(\\"Not yet implemented.\\")\\n\\n    def delete_detail(self, object_list, bundle):\\n        raise Unauthorized(\\"Not yet implemented.\\")\\n\\n\\nclass CredResource(ModelResource):\\n\\n    group = fields.ForeignKey(GroupResource, \'group\', full=True)\\n\\n    def get_object_list(self, request):\\n        # Only show latest, not deleted and accessible credentials\\n        return Cred.objects.visible(\\n            request.user, historical=True, deleted=True)\\n\\n    def dehydrate(self, bundle):\\n        # Add a value indicating if something is on the change queue\\n        bundle.data[\'on_changeq\'] = bundle.obj.on_changeq()\\n\\n        # Only display group name, not full object\\n        bundle.data[\'group\'] = bundle.obj.group\\n\\n        # Unless you are viewing the details for a cred, hide the password\\n        if self.get_resource_uri(bundle) != bundle.request.path:\\n            del bundle.data[\'password\']\\n\\n        # Expand the ssh key\\n        if bundle.obj.ssh_key:\\n            bundle.data[\'ssh_key\'] = bundle.obj.ssh_key.read()\\n        else:\\n            del bundle.data[\'ssh_key\']\\n\\n        return bundle\\n\\n    def obj_create(self, bundle, **kwargs):\\n        bundle = super(CredResource, self).obj_create(bundle, **kwargs)\\n\\n        CredAudit(\\n            audittype=CredAudit.CREDADD, cred=bundle.obj,\\n            user=bundle.request.user).save()\\n        return bundle\\n\\n    def post_detail(self, request, **kwargs):\\n        if \'ssh_key\' not in request.FILES:\\n            res = HttpResponse(\\"Please upload an ssh_key file\\")\\n            res.status_code = 500\\n            return res\\n\\n        basic_bundle = self.build_bundle(request=request)\\n\\n        try:\\n            obj = self.cached_obj_get(\\n                bundle=basic_bundle, **self.remove_api_resource_names(kwargs))\\n        except ObjectDoesNotExist:\\n            return http.HttpNotFound()\\n        except MultipleObjectsReturned:\\n            return http.HttpMultipleChoices(\\n                \\"More than one resource is found at this URI.\\")\\n\\n        ssh_key = request.FILES[\'ssh_key\']\\n        got = ssh_key.read()\\n        ssh_key.seek(0)\\n        try:\\n            SSHKey(got, obj.password).key_obj\\n        except paramiko.ssh_exception.SSHException as error:\\n            res = HttpResponse(error)\\n            res.status_code = 500\\n            return res\\n\\n        obj.ssh_key = File(ssh_key)\\n        obj.save()\\n\\n        if not self._meta.always_return_data:\\n            return http.HttpAccepted()\\n        else:\\n            bundle = self.build_bundle(obj=obj, request=request)\\n            bundle = self.full_dehydrate(bundle)\\n            bundle = self.alter_detail_data_to_serialize(request, bundle)\\n            return self.create_response(\\n                request, bundle, response_class=http.HttpAccepted)\\n\\n    class Meta:\\n        queryset = Cred.objects.filter(is_deleted=False, latest=None)\\n        always_return_data = True\\n        resource_name = \'cred\'\\n        excludes = [\'is_deleted\', \'attachment\']\\n        authentication = MultiAuthentication(\\n            SessionAuthentication(), MultiApiKeyAuthentication())\\n        authorization = CredAuthorization()\\n        filtering = {\\n            \'title\': (\'exact\', \'contains\', \'icontains\'),\\n            \'slug\': (\'exact\', \'contains\', \'icontains\', \'startswith\'),\\n            \'url\': (\'exact\', \'startswith\', ),\\n            \'group\': ALL_WITH_RELATIONS,\\n      }\\n\\n\\nclass TagResource(ModelResource):\\n    # When showing a tag, show all the creds under it,\\n    # that we are allowed to see\\n    creds = fields.ToManyField(\\n        CredResource,\\n        attribute=lambda bundle: Cred.objects.visible(\\n            bundle.request.user).filter(tags=bundle.obj),\\n        null=True,\\n    )\\n\\n    class Meta:\\n        queryset = Tag.objects.all()\\n        always_return_data = True\\n        filtering = {\\n            \'name\': (\\n                \'exact\', \'contains\', \'icontains\',\\n                \'startswith\', \'istartswith\'),\\n      }\\n        resource_name = \'tag\'\\n        authentication = MultiAuthentication(\\n            SessionAuthentication(), MultiApiKeyAuthentication())\\n        authorization = TagAuthorization()\\n        validation = FormValidation(form_class=TagForm)\\n" }\n'
line: b'{ "repo_name": "GRArmstrong/invenio-inspire-ops", "ref": "refs/heads/prod", "path": "modules/bibformat/lib/elements/bfe_authors.py", "content": "# -*- coding: utf-8 -*-\\n##\\n## This file is part of Invenio.\\n## Copyright (C) 2006, 2007, 2008, 2009, 2010, 2011, 2013 CERN.\\n##\\n## Invenio is free software; you can redistribute it and/or\\n## modify it under the terms of the GNU General Public License as\\n## published by the Free Software Foundation; either version 2 of the\\n## License, or (at your option) any later version.\\n##\\n## Invenio is distributed in the hope that it will be useful, but\\n## WITHOUT ANY WARRANTY; without even the implied warranty of\\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n## General Public License for more details.\\n##\\n## You should have received a copy of the GNU General Public License\\n## along with Invenio; if not, write to the Free Software Foundation, Inc.,\\n## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\\n\\"\\"\\"BibFormat element - Prints authors\\n\\"\\"\\"\\n__revision__ = \\"$Id$\\"\\n\\nimport re\\nfrom urllib import quote\\nfrom cgi import escape\\nfrom invenio.config import CFG_BASE_URL, CFG_SITE_RECORD\\nfrom invenio.messages import gettext_set_language\\nfrom invenio.bibauthority_config import \\\\\\n    CFG_BIBAUTHORITY_AUTHORITY_COLLECTION_NAME, \\\\\\n    CFG_BIBAUTHORITY_TYPE_NAMES, \\\\\\n    CFG_BIBAUTHORITY_PREFIX_SEP\\nfrom invenio.bibauthority_engine import \\\\\\n    get_low_level_recIDs_from_control_no\\n\\ndef format_element(bfo, limit, separator=\' ; \',\\n           extension=\'[...]\',\\n           print_links=\\"yes\\",\\n           print_affiliations=\'no\',\\n           affiliation_prefix=\' (\',\\n           affiliation_suffix=\')\',\\n           interactive=\\"no\\",\\n           highlight=\\"no\\",\\n           link_author_pages=\\"no\\",\\n           link_mobile_pages=\\"no\\",\\n           relator_code_pattern=None):\\n    \\"\\"\\"\\n    Prints the list of authors of a record.\\n\\n    @param limit: the maximum number of authors to display\\n    @param separator: the separator between authors.\\n    @param extension: a text printed if more authors than \'limit\' exist\\n    @param print_links: if yes, prints the authors as HTML link to their publications\\n    @param print_affiliations: if yes, make each author name followed by its affiliation\\n    @param affiliation_prefix: prefix printed before each affiliation\\n    @param affiliation_suffix: suffix printed after each affiliation\\n    @param interactive: if yes, enable user to show/hide authors when there are too many (html + javascript)\\n    @param highlight: highlights authors corresponding to search query if set to \'yes\'\\n    @param link_author_pages: should we link to author pages if print_links in on?\\n    @param link_mobile_pages: should we link to mobile app pages if print_links in on?\\n    @param relator_code_pattern: a regular expression to filter authors based on subfield $4 (relator code)\\n    \\"\\"\\"\\n    _ = gettext_set_language(bfo.lang)    # load the right message language\\n\\n    authors = []\\n    authors_1 = bfo.fields(\'100__\', repeatable_subfields_p=True)\\n    authors_2 = bfo.fields(\'700__\', repeatable_subfields_p=True)\\n\\n    authors.extend(authors_1)\\n    authors.extend(authors_2)\\n\\n    # make unique string per key\\n    for author in authors:\\n        if \'a\' in author:\\n            author[\'a\'] = author[\'a\'][0]\\n        if \'u\' in author:\\n            author[\'u\'] = author[\'u\'][0]\\n        pattern = \'%s\' + CFG_BIBAUTHORITY_PREFIX_SEP + \\"(\\"\\n        for control_no in author.get(\'0\', []):\\n            if pattern % (CFG_BIBAUTHORITY_TYPE_NAMES[\\"INSTITUTION\\"]) in control_no:\\n                author[\'u0\'] = control_no # overwrite if multiples\\n            elif pattern % (CFG_BIBAUTHORITY_TYPE_NAMES[\\"AUTHOR\\"]) in control_no:\\n                author[\'a0\'] = control_no # overwrite if multiples\\n\\n\\n    if relator_code_pattern:\\n        p = re.compile(relator_code_pattern)\\n        authors = filter(lambda x: p.match(x.get(\'4\', \'\')), authors)\\n\\n    nb_authors = len(authors)\\n\\n    bibrec_id = bfo.control_field(\\"001\\")\\n\\n    # Process authors to add link, highlight and format affiliation\\n    for author in authors:\\n\\n        if author.has_key(\'a\'):\\n            if highlight == \'yes\':\\n                from invenio import bibformat_utils\\n                author[\'a\'] = bibformat_utils.highlight(author[\'a\'],\\n                                                        bfo.search_pattern)\\n\\n            if print_links.lower() == \\"yes\\":\\n                if link_author_pages == \\"yes\\":\\n                    author[\'a\'] = \'<a rel=\\"author\\" href=\\"\' + CFG_BASE_URL + \\\\\\n                                  \'/author/profile/\' + quote(author[\'a\']) + \\\\\\n                                  \'?recid=\' +  bibrec_id + \\\\\\n                                  \'&ln=\' + bfo.lang + \\\\\\n                                  \'\\">\' + escape(author[\'a\']) + \'</a>\'\\n                elif link_mobile_pages == \'yes\':\\n                    author[\'a\'] = \'<a rel=\\"external\\" href=\\"#page=search\' + \\\\\\n                                  \'&amp;f=author&amp;p=\' + quote(author[\'a\']) + \\\\\\n                                  \'\\">\' + escape(author[\'a\']) + \'</a>\'\\n                else:\\n                    auth_coll_param = \'\'\\n                    if \'a0\' in author:\\n                        recIDs = get_low_level_recIDs_from_control_no(author[\'a0\'])\\n                        if len(recIDs):\\n                            auth_coll_param = \'&amp;c=\' + \\\\\\n                                              CFG_BIBAUTHORITY_AUTHORITY_COLLECTION_NAME\\n                    author[\'a\'] = \'<a href=\\"\' + CFG_BASE_URL + \\\\\\n                                  \'/search?f=author&amp;p=\' + quote(author[\'a\']) + \\\\\\n                                   auth_coll_param + \\\\\\n                                  \'&amp;ln=\' + bfo.lang + \\\\\\n                                  \'\\">\' + escape(author[\'a\']) + \'</a>\'\\n\\n        if author.has_key(\'u\'):\\n            if print_affiliations == \\"yes\\":\\n                if \'u0\' in author:\\n                    recIDs = get_low_level_recIDs_from_control_no(author[\'u0\'])\\n                    # if there is more than 1 recID, clicking on link and\\n                    # thus displaying the authority record\'s page should\\n                    # contain a warning that there are multiple authority\\n                    # records with the same control number\\n                    if len(recIDs):\\n                        author[\'u\'] = \'<a href=\\"\' + CFG_BASE_URL + \'/\' + CFG_SITE_RECORD + \'/\' + \\\\\\n                                      str(recIDs[0]) + \\\\\\n                                      \'?ln=\' + bfo.lang + \\\\\\n                                      \'\\">\' + author[\'u\'] + \'</a>\'\\n                author[\'u\'] = affiliation_prefix + author[\'u\'] + \\\\\\n                              affiliation_suffix\\n\\n    # Flatten author instances\\n    if print_affiliations == \'yes\':\\n        authors = [author.get(\'a\', \'\') + author.get(\'u\', \'\')\\n                   for author in authors]\\n    else:\\n        authors = [author.get(\'a\', \'\')\\n                   for author in authors]\\n\\n    if limit.isdigit() and  nb_authors > int(limit) and interactive != \\"yes\\":\\n        return separator.join(authors[:int(limit)]) + extension\\n\\n    elif limit.isdigit() and nb_authors > int(limit) and interactive == \\"yes\\":\\n        out = \'<a name=\\"show_hide\\" />\'\\n        out += separator.join(authors[:int(limit)])\\n        out += \'<span id=\\"more_%s\\" style=\\"\\">\' % bibrec_id + separator + \\\\\\n               separator.join(authors[int(limit):]) + \'</span>\'\\n        out += \' <span id=\\"extension_%s\\"></span>\' % bibrec_id\\n        out += \' <small><i><a id=\\"link_%s\\" href=\\"#\\" style=\\"color:rgb(204,0,0);\\"></a></i></small>\' % bibrec_id\\n        out += \'\'\'\\n        <script type=\\"text/javascript\\">\\n        $(\'#link_%(recid)s\').click(function(event) {\\n            event.preventDefault();\\n            var more = document.getElementById(\'more_%(recid)s\');\\n            var link = document.getElementById(\'link_%(recid)s\');\\n            var extension = document.getElementById(\'extension_%(recid)s\');\\n            if (more.style.display==\'none\'){\\n                more.style.display = \'\';\\n                extension.style.display = \'none\';\\n                link.innerHTML = \\"%(show_less)s\\"\\n          } else {\\n                more.style.display = \'none\';\\n                extension.style.display = \'\';\\n                link.innerHTML = \\"%(show_more)s\\"\\n          }\\n            link.style.color = \\"rgb(204,0,0);\\"\\n      });\\n\\n        function set_up_%(recid)s(){\\n            var extension = document.getElementById(\'extension_%(recid)s\');\\n            extension.innerHTML = \\"%(extension)s\\";\\n            $(\'#link_%(recid)s\').click();\\n      }\\n\\n        </script>\\n        \'\'\' % {\'show_less\':_(\\"Hide\\"),\\n             \'show_more\':_(\\"Show all %i authors\\") % nb_authors,\\n             \'extension\':extension,\\n             \'recid\': bibrec_id}\\n        out += \'<script type=\\"text/javascript\\">set_up_%s()</script>\' % bibrec_id\\n\\n        return out\\n    elif nb_authors > 0:\\n        return separator.join(authors)\\n\\ndef escape_values(bfo):\\n    \\"\\"\\"\\n    Called by BibFormat in order to check if output of this element\\n    should be escaped.\\n    \\"\\"\\"\\n    return 0\\n" }\n'
line: b'{ "repo_name": "Inspq/ansible", "ref": "refs/heads/inspq", "path": "lib/ansible/modules/cloud/ovirt/ovirt_snapshots.py", "content": "#!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n#\\n# Copyright (c) 2016 Red Hat, Inc.\\n#\\n# This file is part of Ansible\\n#\\n# Ansible is free software: you can redistribute it and/or modify\\n# it under the terms of the GNU General Public License as published by\\n# the Free Software Foundation, either version 3 of the License, or\\n# (at your option) any later version.\\n#\\n# Ansible is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU General Public License for more details.\\n#\\n# You should have received a copy of the GNU General Public License\\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\\n#\\n\\nANSIBLE_METADATA = {\'metadata_version\': \'1.0\',\\n                    \'status\': [\'preview\'],\\n                    \'supported_by\': \'community\'}\\n\\n\\nDOCUMENTATION = \'\'\'\\n---\\nmodule: ovirt_snapshots\\nshort_description: \\"Module to manage Virtual Machine Snapshots in oVirt\\"\\nversion_added: \\"2.3\\"\\nauthor: \\"Ondra Machacek (@machacekondra)\\"\\ndescription:\\n    - \\"Module to manage Virtual Machine Snapshots in oVirt\\"\\noptions:\\n    snapshot_id:\\n        description:\\n            - \\"ID of the snapshot to manage.\\"\\n    vm_name:\\n        description:\\n            - \\"Name of the Virtual Machine to manage.\\"\\n        required: true\\n    state:\\n        description:\\n            - \\"Should the Virtual Machine snapshot be restore/present/absent.\\"\\n        choices: [\'restore\', \'present\', \'absent\']\\n        default: present\\n    description:\\n        description:\\n            - \\"Description of the snapshot.\\"\\n    use_memory:\\n        description:\\n            - \\"If I(true) and C(state) is I(present) save memory of the Virtual\\n               Machine if it\'s running.\\"\\n            - \\"If I(true) and C(state) is I(restore) restore memory of the\\n               Virtual Machine.\\"\\n            - \\"Note that Virtual Machine will be paused while saving the memory.\\"\\nnotes:\\n    - \\"Note that without a guest agent the data on the created snapshot may be\\n       inconsistent.\\"\\n    - \\"Deleting a snapshot does not remove any information from the virtual\\n       machine - it simply removes a return-point. However, restoring a virtual\\n       machine from a snapshot deletes any content that was written to the\\n       virtual machine after the time the snapshot was taken.\\"\\nextends_documentation_fragment: ovirt\\n\'\'\'\\n\\n\\nEXAMPLES = \'\'\'\\n# Examples don\'t contain auth parameter for simplicity,\\n# look at ovirt_auth module to see how to reuse authentication:\\n\\n# Create snapshot:\\n- ovirt_snapshots:\\n    vm_name: rhel7\\n    description: MySnapshot\\n  register: snapshot\\n\\n# Create snapshot and save memory:\\n- ovirt_snapshots:\\n    vm_name: rhel7\\n    description: SnapWithMem\\n    use_memory: true\\n  register: snapshot\\n\\n# Restore snapshot:\\n- ovirt_snapshots:\\n    state: restore\\n    vm_name: rhel7\\n    snapshot_id: \\"{{ snapshot.id }}\\"\\n\\n# Remove snapshot:\\n- ovirt_snapshots:\\n    state: absent\\n    vm_name: rhel7\\n    snapshot_id: \\"{{ snapshot.id }}\\"\\n\'\'\'\\n\\n\\nRETURN = \'\'\'\\nid:\\n    description: ID of the snapshot which is managed\\n    returned: On success if snapshot is found.\\n    type: str\\n    sample: 7de90f31-222c-436c-a1ca-7e655bd5b60c\\nsnapshot:\\n    description: \\"Dictionary of all the snapshot attributes. Snapshot attributes can be found on your oVirt instance\\n                  at following url: https://ovirt.example.com/ovirt-engine/api/model#types/snapshot.\\"\\n    returned: On success if snapshot is found.\\n\'\'\'\\n\\n\\nimport traceback\\n\\ntry:\\n    import ovirtsdk4.types as otypes\\nexcept ImportError:\\n    pass\\n\\nfrom ansible.module_utils.basic import AnsibleModule\\nfrom ansible.module_utils.ovirt import (\\n    check_sdk,\\n    create_connection,\\n    get_dict_of_struct,\\n    get_entity,\\n    ovirt_full_argument_spec,\\n    search_by_name,\\n    wait,\\n)\\n\\n\\ndef create_snapshot(module, vm_service, snapshots_service):\\n    changed = False\\n    snapshot = get_entity(\\n        snapshots_service.snapshot_service(module.params[\'snapshot_id\'])\\n    )\\n    if snapshot is None:\\n        if not module.check_mode:\\n            snapshot = snapshots_service.add(\\n                otypes.Snapshot(\\n                    description=module.params.get(\'description\'),\\n                    persist_memorystate=module.params.get(\'use_memory\'),\\n                )\\n            )\\n        changed = True\\n        wait(\\n            service=snapshots_service.snapshot_service(snapshot.id),\\n            condition=lambda snap: snap.snapshot_status == otypes.SnapshotStatus.OK,\\n            wait=module.params[\'wait\'],\\n            timeout=module.params[\'timeout\'],\\n        )\\n    return {\\n        \'changed\': changed,\\n        \'id\': snapshot.id,\\n        \'snapshot\': get_dict_of_struct(snapshot),\\n  }\\n\\n\\ndef remove_snapshot(module, vm_service, snapshots_service):\\n    changed = False\\n    snapshot = get_entity(\\n        snapshots_service.snapshot_service(module.params[\'snapshot_id\'])\\n    )\\n\\n    if snapshot:\\n        snapshot_service = snapshots_service.snapshot_service(snapshot.id)\\n        if not module.check_mode:\\n            snapshot_service.remove()\\n        changed = True\\n        wait(\\n            service=snapshot_service,\\n            condition=lambda snapshot: snapshot is None,\\n            wait=module.params[\'wait\'],\\n            timeout=module.params[\'timeout\'],\\n        )\\n\\n    return {\\n        \'changed\': changed,\\n        \'id\': snapshot.id if snapshot else None,\\n        \'snapshot\': get_dict_of_struct(snapshot),\\n  }\\n\\n\\ndef restore_snapshot(module, vm_service, snapshots_service):\\n    changed = False\\n    snapshot_service = snapshots_service.snapshot_service(\\n        module.params[\'snapshot_id\']\\n    )\\n    snapshot = get_entity(snapshot_service)\\n    if snapshot is None:\\n        raise Exception(\\n            \\"Snapshot with id \'%s\' doesn\'t exist\\" % module.params[\'snapshot_id\']\\n        )\\n\\n    if snapshot.snapshot_status != otypes.SnapshotStatus.IN_PREVIEW:\\n        if not module.check_mode:\\n            snapshot_service.restore(\\n                restore_memory=module.params.get(\'use_memory\'),\\n            )\\n        changed = True\\n    else:\\n        if not module.check_mode:\\n            vm_service.commit_snapshot()\\n        changed = True\\n\\n    if changed:\\n        wait(\\n            service=snapshot_service,\\n            condition=lambda snap: snap.snapshot_status == otypes.SnapshotStatus.OK,\\n            wait=module.params[\'wait\'],\\n            timeout=module.params[\'timeout\'],\\n        )\\n    return {\\n        \'changed\': changed,\\n        \'id\': snapshot.id if snapshot else None,\\n        \'snapshot\': get_dict_of_struct(snapshot),\\n  }\\n\\n\\ndef main():\\n    argument_spec = ovirt_full_argument_spec(\\n        state=dict(\\n            choices=[\'restore\', \'present\', \'absent\'],\\n            default=\'present\',\\n        ),\\n        vm_name=dict(required=True),\\n        snapshot_id=dict(default=None),\\n        description=dict(default=None),\\n        use_memory=dict(\\n            default=None,\\n            type=\'bool\',\\n            aliases=[\'restore_memory\', \'save_memory\'],\\n        ),\\n    )\\n    module = AnsibleModule(\\n        argument_spec=argument_spec,\\n        supports_check_mode=True,\\n        required_if=[\\n            (\'state\', \'absent\', [\'snapshot_id\']),\\n            (\'state\', \'restore\', [\'snapshot_id\']),\\n        ]\\n    )\\n    check_sdk(module)\\n\\n    vm_name = module.params.get(\'vm_name\')\\n    auth = module.params.pop(\'auth\')\\n    connection = create_connection(auth)\\n    vms_service = connection.system_service().vms_service()\\n    vm = search_by_name(vms_service, vm_name)\\n    if not vm:\\n        module.fail_json(\\n            msg=\\"Vm \'{name}\' doesn\'t exist.\\".format(name=vm_name),\\n        )\\n\\n    vm_service = vms_service.vm_service(vm.id)\\n    snapshots_service = vms_service.vm_service(vm.id).snapshots_service()\\n    try:\\n        state = module.params[\'state\']\\n        if state == \'present\':\\n            ret = create_snapshot(module, vm_service, snapshots_service)\\n        elif state == \'restore\':\\n            ret = restore_snapshot(module, vm_service, snapshots_service)\\n        elif state == \'absent\':\\n            ret = remove_snapshot(module, vm_service, snapshots_service)\\n        module.exit_json(**ret)\\n    except Exception as e:\\n        module.fail_json(msg=str(e), exception=traceback.format_exc())\\n    finally:\\n        connection.close(logout=auth.get(\'token\') is None)\\n\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n" }\n'
line: b'{ "repo_name": "redhat-openstack/django", "ref": "refs/heads/epel7-patches", "path": "django/views/generic/base.py", "content": "from __future__ import unicode_literals\\n\\nimport logging\\nfrom functools import update_wrapper\\n\\nfrom django import http\\nfrom django.core.exceptions import ImproperlyConfigured\\nfrom django.core.urlresolvers import reverse, NoReverseMatch\\nfrom django.template.response import TemplateResponse\\nfrom django.utils.decorators import classonlymethod\\nfrom django.utils import six\\n\\nlogger = logging.getLogger(\'django.request\')\\n\\n\\nclass ContextMixin(object):\\n    \\"\\"\\"\\n    A default context mixin that passes the keyword arguments received by\\n    get_context_data as the template context.\\n    \\"\\"\\"\\n\\n    def get_context_data(self, **kwargs):\\n        if \'view\' not in kwargs:\\n            kwargs[\'view\'] = self\\n        return kwargs\\n\\n\\nclass View(object):\\n    \\"\\"\\"\\n    Intentionally simple parent class for all views. Only implements\\n    dispatch-by-method and simple sanity checking.\\n    \\"\\"\\"\\n\\n    http_method_names = [\'get\', \'post\', \'put\', \'patch\', \'delete\', \'head\', \'options\', \'trace\']\\n\\n    def __init__(self, **kwargs):\\n        \\"\\"\\"\\n        Constructor. Called in the URLconf; can contain helpful extra\\n        keyword arguments, and other things.\\n        \\"\\"\\"\\n        # Go through keyword arguments, and either save their values to our\\n        # instance, or raise an error.\\n        for key, value in six.iteritems(kwargs):\\n            setattr(self, key, value)\\n\\n    @classonlymethod\\n    def as_view(cls, **initkwargs):\\n        \\"\\"\\"\\n        Main entry point for a request-response process.\\n        \\"\\"\\"\\n        # sanitize keyword arguments\\n        for key in initkwargs:\\n            if key in cls.http_method_names:\\n                raise TypeError(\\"You tried to pass in the %s method name as a \\"\\n                                \\"keyword argument to %s(). Don\'t do that.\\"\\n                                % (key, cls.__name__))\\n            if not hasattr(cls, key):\\n                raise TypeError(\\"%s() received an invalid keyword %r. as_view \\"\\n                                \\"only accepts arguments that are already \\"\\n                                \\"attributes of the class.\\" % (cls.__name__, key))\\n\\n        def view(request, *args, **kwargs):\\n            self = cls(**initkwargs)\\n            if hasattr(self, \'get\') and not hasattr(self, \'head\'):\\n                self.head = self.get\\n            self.request = request\\n            self.args = args\\n            self.kwargs = kwargs\\n            return self.dispatch(request, *args, **kwargs)\\n\\n        # take name and docstring from class\\n        update_wrapper(view, cls, updated=())\\n\\n        # and possible attributes set by decorators\\n        # like csrf_exempt from dispatch\\n        update_wrapper(view, cls.dispatch, assigned=())\\n        return view\\n\\n    def dispatch(self, request, *args, **kwargs):\\n        # Try to dispatch to the right method; if a method doesn\'t exist,\\n        # defer to the error handler. Also defer to the error handler if the\\n        # request method isn\'t on the approved list.\\n        if request.method.lower() in self.http_method_names:\\n            handler = getattr(self, request.method.lower(), self.http_method_not_allowed)\\n        else:\\n            handler = self.http_method_not_allowed\\n        return handler(request, *args, **kwargs)\\n\\n    def http_method_not_allowed(self, request, *args, **kwargs):\\n        logger.warning(\'Method Not Allowed (%s): %s\', request.method, request.path,\\n            extra={\\n                \'status_code\': 405,\\n                \'request\': self.request\\n          }\\n        )\\n        return http.HttpResponseNotAllowed(self._allowed_methods())\\n\\n    def options(self, request, *args, **kwargs):\\n        \\"\\"\\"\\n        Handles responding to requests for the OPTIONS HTTP verb.\\n        \\"\\"\\"\\n        response = http.HttpResponse()\\n        response[\'Allow\'] = \', \'.join(self._allowed_methods())\\n        response[\'Content-Length\'] = \'0\'\\n        return response\\n\\n    def _allowed_methods(self):\\n        return [m.upper() for m in self.http_method_names if hasattr(self, m)]\\n\\n\\nclass TemplateResponseMixin(object):\\n    \\"\\"\\"\\n    A mixin that can be used to render a template.\\n    \\"\\"\\"\\n    template_name = None\\n    response_class = TemplateResponse\\n    content_type = None\\n\\n    def render_to_response(self, context, **response_kwargs):\\n        \\"\\"\\"\\n        Returns a response, using the `response_class` for this\\n        view, with a template rendered with the given context.\\n\\n        If any keyword arguments are provided, they will be\\n        passed to the constructor of the response class.\\n        \\"\\"\\"\\n        response_kwargs.setdefault(\'content_type\', self.content_type)\\n        return self.response_class(\\n            request = self.request,\\n            template = self.get_template_names(),\\n            context = context,\\n            **response_kwargs\\n        )\\n\\n    def get_template_names(self):\\n        \\"\\"\\"\\n        Returns a list of template names to be used for the request. Must return\\n        a list. May not be called if render_to_response is overridden.\\n        \\"\\"\\"\\n        if self.template_name is None:\\n            raise ImproperlyConfigured(\\n                \\"TemplateResponseMixin requires either a definition of \\"\\n                \\"\'template_name\' or an implementation of \'get_template_names()\'\\")\\n        else:\\n            return [self.template_name]\\n\\n\\nclass TemplateView(TemplateResponseMixin, ContextMixin, View):\\n    \\"\\"\\"\\n    A view that renders a template.  This view will also pass into the context\\n    any keyword arguments passed by the url conf.\\n    \\"\\"\\"\\n    def get(self, request, *args, **kwargs):\\n        context = self.get_context_data(**kwargs)\\n        return self.render_to_response(context)\\n\\n\\nclass RedirectView(View):\\n    \\"\\"\\"\\n    A view that provides a redirect on any GET request.\\n    \\"\\"\\"\\n    permanent = True\\n    url = None\\n    pattern_name = None\\n    query_string = False\\n\\n    def get_redirect_url(self, *args, **kwargs):\\n        \\"\\"\\"\\n        Return the URL redirect to. Keyword arguments from the\\n        URL pattern match generating the redirect request\\n        are provided as kwargs to this method.\\n        \\"\\"\\"\\n        if self.url:\\n            url = self.url % kwargs\\n        elif self.pattern_name:\\n            try:\\n                url = reverse(self.pattern_name, args=args, kwargs=kwargs)\\n            except NoReverseMatch:\\n                return None\\n        else:\\n            return None\\n\\n        args = self.request.META.get(\'QUERY_STRING\', \'\')\\n        if args and self.query_string:\\n            url = \\"%s?%s\\" % (url, args)\\n        return url\\n\\n    def get(self, request, *args, **kwargs):\\n        url = self.get_redirect_url(*args, **kwargs)\\n        if url:\\n            if self.permanent:\\n                return http.HttpResponsePermanentRedirect(url)\\n            else:\\n                return http.HttpResponseRedirect(url)\\n        else:\\n            logger.warning(\'Gone: %s\', self.request.path,\\n                        extra={\\n                            \'status_code\': 410,\\n                            \'request\': self.request\\n                      })\\n            return http.HttpResponseGone()\\n\\n    def head(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def post(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def options(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def delete(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def put(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n\\n    def patch(self, request, *args, **kwargs):\\n        return self.get(request, *args, **kwargs)\\n" }\n'
line: b'{ "repo_name": "wbond/subversion", "ref": "refs/heads/1.7.x", "path": "tools/hook-scripts/mailer/tests/mailer-tweak.py", "content": "#!/usr/bin/env python\\n#\\n#\\n# Licensed to the Apache Software Foundation (ASF) under one\\n# or more contributor license agreements.  See the NOTICE file\\n# distributed with this work for additional information\\n# regarding copyright ownership.  The ASF licenses this file\\n# to you under the Apache License, Version 2.0 (the\\n# \\"License\\"); you may not use this file except in compliance\\n# with the License.  You may obtain a copy of the License at\\n#\\n#   http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing,\\n# software distributed under the License is distributed on an\\n# \\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\n# KIND, either express or implied.  See the License for the\\n# specific language governing permissions and limitations\\n# under the License.\\n#\\n#\\n#\\n# mailer-tweak.py: tweak the svn:date and svn:author properties\\n#                  on all revisions\\n#\\n# We need constant dates and authors for the revisions so that we can\\n# consistently compare an output against a known quantity.\\n#\\n# USAGE: ./mailer-tweak.py REPOS\\n#\\n\\n\\nimport sys\\nimport os\\nimport getopt\\n\\nfrom svn import fs, core\\n\\nDATE_BASE = 1000000000\\nDATE_INCR = 10000\\n\\n\\ndef tweak_dates(pool, home=\'.\'):\\n  db_path = os.path.join(home, \'db\')\\n  if not os.path.exists(db_path):\\n    db_path = home\\n\\n  fsob = fs.new(None, pool)\\n  fs.open_berkeley(fsob, db_path)\\n\\n  for i in range(fs.youngest_rev(fsob, pool)):\\n    # convert secs into microseconds, then a string\\n    date = core.svn_time_to_cstring((DATE_BASE+i*DATE_INCR) * 1000000L, pool)\\n    #print date\\n    fs.change_rev_prop(fsob, i+1, core.SVN_PROP_REVISION_DATE, date, pool)\\n    fs.change_rev_prop(fsob, i+1, core.SVN_PROP_REVISION_AUTHOR, \'mailer test\', pool)\\n\\ndef main():\\n  if len(sys.argv) != 2:\\n    print(\'USAGE: %s REPOS\' % sys.argv[0])\\n    sys.exit(1)\\n\\n  core.run_app(tweak_dates, sys.argv[1])\\n\\nif __name__ == \'__main__\':\\n  main()\\n" }\n'
line: b'{ "repo_name": "pbs/cmsplugin-filer", "ref": "refs/heads/master_pbs", "path": "cmsplugin_filer_image/south_migrations/0010_auto__add_field_filerimage_target_blank.py", "content": "# -*- coding: utf-8 -*-\\nimport datetime\\nfrom south.db import db\\nfrom south.v2 import SchemaMigration\\nfrom django.db import models\\n\\n\\nclass Migration(SchemaMigration):\\n\\n    def forwards(self, orm):\\n        # Adding field \'FilerImage.target_blank\'\\n        db.add_column(\'cmsplugin_filerimage\', \'target_blank\',\\n                      self.gf(\'django.db.models.fields.BooleanField\')(default=False),\\n                      keep_default=False)\\n\\n\\n    def backwards(self, orm):\\n        # Deleting field \'FilerImage.target_blank\'\\n        db.delete_column(\'cmsplugin_filerimage\', \'target_blank\')\\n\\n\\n    models = {\\n        \'auth.group\': {\\n            \'Meta\': {\'object_name\': \'Group\'}\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'unique\': \'True\', \'max_length\': \'80\'}),\\n            \'permissions\': (\'django.db.models.fields.related.ManyToManyField\', [], {\'to\': \\"orm[\'auth.Permission\']\\", \'symmetrical\': \'False\', \'blank\': \'True\'})\\n      }\\n        \'auth.permission\': {\\n            \'Meta\': {\'ordering\': \\"(\'content_type__app_label\', \'content_type__model\', \'codename\')\\", \'unique_together\': \\"((\'content_type\', \'codename\'),)\\", \'object_name\': \'Permission\'}\\n            \'codename\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'content_type\': (\'django.db.models.fields.related.ForeignKey\', [], {\'to\': \\"orm[\'contenttypes.ContentType\']\\"}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\'})\\n      }\\n        \'auth.user\': {\\n            \'Meta\': {\'object_name\': \'User\'}\\n            \'date_joined\': (\'django.db.models.fields.DateTimeField\', [], {\'default\': \'datetime.datetime.now\'}),\\n            \'email\': (\'django.db.models.fields.EmailField\', [], {\'max_length\': \'75\', \'blank\': \'True\'}),\\n            \'first_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'30\', \'blank\': \'True\'}),\\n            \'groups\': (\'django.db.models.fields.related.ManyToManyField\', [], {\'to\': \\"orm[\'auth.Group\']\\", \'symmetrical\': \'False\', \'blank\': \'True\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'is_active\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'is_staff\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'is_superuser\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'last_login\': (\'django.db.models.fields.DateTimeField\', [], {\'default\': \'datetime.datetime.now\'}),\\n            \'last_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'30\', \'blank\': \'True\'}),\\n            \'password\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'128\'}),\\n            \'user_permissions\': (\'django.db.models.fields.related.ManyToManyField\', [], {\'to\': \\"orm[\'auth.Permission\']\\", \'symmetrical\': \'False\', \'blank\': \'True\'}),\\n            \'username\': (\'django.db.models.fields.CharField\', [], {\'unique\': \'True\', \'max_length\': \'30\'})\\n      }\\n        \'cms.cmsplugin\': {\\n            \'Meta\': {\'object_name\': \'CMSPlugin\'}\\n            \'changed_date\': (\'django.db.models.fields.DateTimeField\', [], {\'auto_now\': \'True\', \'blank\': \'True\'}),\\n            \'creation_date\': (\'django.db.models.fields.DateTimeField\', [], {\'default\': \'datetime.datetime(2012, 11, 29, 0, 0)\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'language\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'15\', \'db_index\': \'True\'}),\\n            \'level\': (\'django.db.models.fields.PositiveIntegerField\', [], {\'db_index\': \'True\'}),\\n            \'lft\': (\'django.db.models.fields.PositiveIntegerField\', [], {\'db_index\': \'True\'}),\\n            \'parent\': (\'django.db.models.fields.related.ForeignKey\', [], {\'to\': \\"orm[\'cms.CMSPlugin\']\\", \'null\': \'True\', \'blank\': \'True\'}),\\n            \'placeholder\': (\'django.db.models.fields.related.ForeignKey\', [], {\'to\': \\"orm[\'cms.Placeholder\']\\", \'null\': \'True\'}),\\n            \'plugin_type\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\', \'db_index\': \'True\'}),\\n            \'position\': (\'django.db.models.fields.PositiveSmallIntegerField\', [], {\'null\': \'True\', \'blank\': \'True\'}),\\n            \'rght\': (\'django.db.models.fields.PositiveIntegerField\', [], {\'db_index\': \'True\'}),\\n            \'tree_id\': (\'django.db.models.fields.PositiveIntegerField\', [], {\'db_index\': \'True\'})\\n      }\\n        \'cms.page\': {\\n            \'Meta\': {\'ordering\': \\"(\'site\', \'tree_id\', \'lft\')\\", \'object_name\': \'Page\'}\\n            \'changed_by\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'70\'}),\\n            \'changed_date\': (\'django.db.models.fields.DateTimeField\', [], {\'auto_now\': \'True\', \'blank\': \'True\'}),\\n            \'created_by\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'70\'}),\\n            \'creation_date\': (\'django.db.models.fields.DateTimeField\', [], {\'auto_now_add\': \'True\', \'blank\': \'True\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'in_navigation\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'True\', \'db_index\': \'True\'}),\\n            \'level\': (\'django.db.models.fields.PositiveIntegerField\', [], {\'db_index\': \'True\'}),\\n            \'lft\': (\'django.db.models.fields.PositiveIntegerField\', [], {\'db_index\': \'True\'}),\\n            \'limit_visibility_in_menu\': (\'django.db.models.fields.SmallIntegerField\', [], {\'default\': \'None\', \'null\': \'True\', \'db_index\': \'True\', \'blank\': \'True\'}),\\n            \'login_required\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'moderator_state\': (\'django.db.models.fields.SmallIntegerField\', [], {\'default\': \'1\', \'blank\': \'True\'}),\\n            \'navigation_extenders\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'80\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'parent\': (\'django.db.models.fields.related.ForeignKey\', [], {\'blank\': \'True\', \'related_name\': \\"\'children\'\\", \'null\': \'True\', \'to\': \\"orm[\'cms.Page\']\\"}),\\n            \'placeholders\': (\'django.db.models.fields.related.ManyToManyField\', [], {\'to\': \\"orm[\'cms.Placeholder\']\\", \'symmetrical\': \'False\'}),\\n            \'publication_date\': (\'django.db.models.fields.DateTimeField\', [], {\'db_index\': \'True\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'publication_end_date\': (\'django.db.models.fields.DateTimeField\', [], {\'db_index\': \'True\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'published\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'publisher_is_draft\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'True\', \'db_index\': \'True\'}),\\n            \'publisher_public\': (\'django.db.models.fields.related.OneToOneField\', [], {\'related_name\': \\"\'publisher_draft\'\\", \'unique\': \'True\', \'null\': \'True\', \'to\': \\"orm[\'cms.Page\']\\"}),\\n            \'publisher_state\': (\'django.db.models.fields.SmallIntegerField\', [], {\'default\': \'0\', \'db_index\': \'True\'}),\\n            \'reverse_id\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'40\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'rght\': (\'django.db.models.fields.PositiveIntegerField\', [], {\'db_index\': \'True\'}),\\n            \'site\': (\'django.db.models.fields.related.ForeignKey\', [], {\'to\': \\"orm[\'sites.Site\']\\"}),\\n            \'soft_root\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\', \'db_index\': \'True\'}),\\n            \'template\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'tree_id\': (\'django.db.models.fields.PositiveIntegerField\', [], {\'db_index\': \'True\'})\\n      }\\n        \'cms.placeholder\': {\\n            \'Meta\': {\'object_name\': \'Placeholder\'}\\n            \'default_width\': (\'django.db.models.fields.PositiveSmallIntegerField\', [], {\'null\': \'True\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'slot\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\', \'db_index\': \'True\'})\\n      }\\n        \'cmsplugin_filer_image.filerimage\': {\\n            \'Meta\': {\'object_name\': \'FilerImage\', \'db_table\': \\"\'cmsplugin_filerimage\'\\", \'_ormbases\': [\'cms.CMSPlugin\']}\\n            \'alignment\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'10\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'alt_text\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'255\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'caption_text\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'255\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'cmsplugin_ptr\': (\'django.db.models.fields.related.OneToOneField\', [], {\'to\': \\"orm[\'cms.CMSPlugin\']\\", \'unique\': \'True\', \'primary_key\': \'True\'}),\\n            \'crop\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'description\': (\'django.db.models.fields.TextField\', [], {\'null\': \'True\', \'blank\': \'True\'}),\\n            \'file_link\': (\'django.db.models.fields.related.ForeignKey\', [], {\'default\': \'None\', \'related_name\': \\"\'+\'\\", \'null\': \'True\', \'blank\': \'True\', \'to\': \\"orm[\'filer.File\']\\"}),\\n            \'free_link\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'255\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'height\': (\'django.db.models.fields.PositiveIntegerField\', [], {\'null\': \'True\', \'blank\': \'True\'}),\\n            \'image\': (\'django.db.models.fields.related.ForeignKey\', [], {\'default\': \'None\', \'to\': \\"orm[\'filer.Image\']\\", \'null\': \'True\', \'blank\': \'True\'}),\\n            \'image_url\': (\'django.db.models.fields.URLField\', [], {\'default\': \'None\', \'max_length\': \'200\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'original_link\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'page_link\': (\'django.db.models.fields.related.ForeignKey\', [], {\'to\': \\"orm[\'cms.Page\']\\", \'null\': \'True\', \'blank\': \'True\'}),\\n            \'target_blank\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'thumbnail_option\': (\'django.db.models.fields.related.ForeignKey\', [], {\'to\': \\"orm[\'cmsplugin_filer_image.ThumbnailOption\']\\", \'null\': \'True\', \'blank\': \'True\'}),\\n            \'upscale\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'use_autoscale\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'use_original_image\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'width\': (\'django.db.models.fields.PositiveIntegerField\', [], {\'null\': \'True\', \'blank\': \'True\'})\\n      }\\n        \'cmsplugin_filer_image.thumbnailoption\': {\\n            \'Meta\': {\'ordering\': \\"(\'width\', \'height\')\\", \'object_name\': \'ThumbnailOption\'}\\n            \'crop\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'height\': (\'django.db.models.fields.IntegerField\', [], {}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'upscale\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'width\': (\'django.db.models.fields.IntegerField\', [], {})\\n      }\\n        \'contenttypes.contenttype\': {\\n            \'Meta\': {\'ordering\': \\"(\'name\',)\\", \'unique_together\': \\"((\'app_label\', \'model\'),)\\", \'object_name\': \'ContentType\', \'db_table\': \\"\'django_content_type\'\\"}\\n            \'app_label\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'model\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'})\\n      }\\n        \'filer.file\': {\\n            \'Meta\': {\'object_name\': \'File\'}\\n            \'_file_size\': (\'django.db.models.fields.IntegerField\', [], {\'null\': \'True\', \'blank\': \'True\'}),\\n            \'description\': (\'django.db.models.fields.TextField\', [], {\'null\': \'True\', \'blank\': \'True\'}),\\n            \'file\': (\'django.db.models.fields.files.FileField\', [], {\'max_length\': \'255\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'folder\': (\'django.db.models.fields.related.ForeignKey\', [], {\'blank\': \'True\', \'related_name\': \\"\'all_files\'\\", \'null\': \'True\', \'to\': \\"orm[\'filer.Folder\']\\"}),\\n            \'has_all_mandatory_data\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'is_public\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'modified_at\': (\'django.db.models.fields.DateTimeField\', [], {\'auto_now\': \'True\', \'blank\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'default\': \\"\'\'\\", \'max_length\': \'255\', \'blank\': \'True\'}),\\n            \'original_filename\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'255\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'owner\': (\'django.db.models.fields.related.ForeignKey\', [], {\'blank\': \'True\', \'related_name\': \\"\'owned_files\'\\", \'null\': \'True\', \'to\': \\"orm[\'auth.User\']\\"}),\\n            \'polymorphic_ctype\': (\'django.db.models.fields.related.ForeignKey\', [], {\'related_name\': \\"\'polymorphic_filer.file_set\'\\", \'null\': \'True\', \'to\': \\"orm[\'contenttypes.ContentType\']\\"}),\\n            \'sha1\': (\'django.db.models.fields.CharField\', [], {\'default\': \\"\'\'\\", \'max_length\': \'40\', \'blank\': \'True\'}),\\n            \'uploaded_at\': (\'django.db.models.fields.DateTimeField\', [], {\'auto_now_add\': \'True\', \'blank\': \'True\'})\\n      }\\n        \'filer.folder\': {\\n            \'Meta\': {\'ordering\': \\"(\'name\',)\\", \'unique_together\': \\"((\'parent\', \'name\'),)\\", \'object_name\': \'Folder\'}\\n            \'created_at\': (\'django.db.models.fields.DateTimeField\', [], {\'auto_now_add\': \'True\', \'blank\': \'True\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'level\': (\'django.db.models.fields.PositiveIntegerField\', [], {\'db_index\': \'True\'}),\\n            \'lft\': (\'django.db.models.fields.PositiveIntegerField\', [], {\'db_index\': \'True\'}),\\n            \'modified_at\': (\'django.db.models.fields.DateTimeField\', [], {\'auto_now\': \'True\', \'blank\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'255\'}),\\n            \'owner\': (\'django.db.models.fields.related.ForeignKey\', [], {\'blank\': \'True\', \'related_name\': \\"\'filer_owned_folders\'\\", \'null\': \'True\', \'to\': \\"orm[\'auth.User\']\\"}),\\n            \'parent\': (\'django.db.models.fields.related.ForeignKey\', [], {\'blank\': \'True\', \'related_name\': \\"\'children\'\\", \'null\': \'True\', \'to\': \\"orm[\'filer.Folder\']\\"}),\\n            \'rght\': (\'django.db.models.fields.PositiveIntegerField\', [], {\'db_index\': \'True\'}),\\n            \'tree_id\': (\'django.db.models.fields.PositiveIntegerField\', [], {\'db_index\': \'True\'}),\\n            \'uploaded_at\': (\'django.db.models.fields.DateTimeField\', [], {\'auto_now_add\': \'True\', \'blank\': \'True\'})\\n      }\\n        \'filer.image\': {\\n            \'Meta\': {\'object_name\': \'Image\', \'_ormbases\': [\'filer.File\']}\\n            \'_height\': (\'django.db.models.fields.IntegerField\', [], {\'null\': \'True\', \'blank\': \'True\'}),\\n            \'_width\': (\'django.db.models.fields.IntegerField\', [], {\'null\': \'True\', \'blank\': \'True\'}),\\n            \'author\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'255\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'date_taken\': (\'django.db.models.fields.DateTimeField\', [], {\'null\': \'True\', \'blank\': \'True\'}),\\n            \'default_alt_text\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'255\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'default_caption\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'255\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'file_ptr\': (\'django.db.models.fields.related.OneToOneField\', [], {\'to\': \\"orm[\'filer.File\']\\", \'unique\': \'True\', \'primary_key\': \'True\'}),\\n            \'must_always_publish_author_credit\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'must_always_publish_copyright\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'subject_location\': (\'django.db.models.fields.CharField\', [], {\'default\': \'None\', \'max_length\': \'64\', \'null\': \'True\', \'blank\': \'True\'})\\n      }\\n        \'sites.site\': {\\n            \'Meta\': {\'ordering\': \\"(\'domain\',)\\", \'object_name\': \'Site\', \'db_table\': \\"\'django_site\'\\"}\\n            \'domain\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\'})\\n      }\\n  }\\n\\n    complete_apps = [\'cmsplugin_filer_image\']" }\n'
line: b'{ "repo_name": "balister/GNU-Radio", "ref": "refs/heads/adap", "path": "gnuradio-runtime/python/gnuradio/gr/__init__.py", "content": "#\\n# Copyright 2003-2012 Free Software Foundation, Inc.\\n#\\n# This file is part of GNU Radio\\n#\\n# GNU Radio is free software; you can redistribute it and/or modify\\n# it under the terms of the GNU General Public License as published by\\n# the Free Software Foundation; either version 3, or (at your option)\\n# any later version.\\n#\\n# GNU Radio is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU General Public License for more details.\\n#\\n# You should have received a copy of the GNU General Public License\\n# along with GNU Radio; see the file COPYING.  If not, write to\\n# the Free Software Foundation, Inc., 51 Franklin Street,\\n# Boston, MA 02110-1301, USA.\\n#\\n\\n# The presence of this file turns this directory into a Python package\\n\\n\\"\\"\\"\\nCore contents.\\n\\"\\"\\"\\n\\n# This is the main GNU Radio python module.\\n# We pull the swig output and the other modules into the gnuradio.gr namespace\\n\\n# If gnuradio is installed then the swig output will be in this directory.\\n# Otherwise it will reside in ../../../swig.\\n\\nimport os\\n\\ntry:\\n    from runtime_swig import *\\nexcept ImportError:\\n    dirname, filename = os.path.split(os.path.abspath(__file__))\\n    __path__.append(os.path.join(dirname, \\"..\\", \\"..\\", \\"..\\", \\"swig\\"))\\n    from runtime_swig import *\\n\\nfrom exceptions import *\\nfrom top_block import *\\nfrom hier_block2 import *\\nfrom tag_utils import *\\nfrom gateway import basic_block, sync_block, decim_block, interp_block\\n\\n# Force the preference database to be initialized\\nprefs = prefs.singleton\\n" }\n'
line: b'{ "repo_name": "yiqingj/work", "ref": "refs/heads/tn-vector", "path": "utils/pgsql2sqlite/build.py", "content": "#\\n# This file is part of Mapnik (c++ mapping toolkit)\\n#\\n# Copyright (C) 2009 Artem Pavlenko, Dane Springmeyer\\n#\\n# Mapnik is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n#\\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n#\\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA\\n#\\n# \\n\\nimport os\\nfrom copy import copy\\n\\nImport (\'env\')\\n\\nprefix = env[\'PREFIX\']\\n\\nprogram_env = env.Clone()\\n\\nsource = Split(\\n    \\"\\"\\"\\n    main.cpp\\n    sqlite.cpp\\n    \\"\\"\\"\\n    )\\n\\nprogram_env[\'CXXFLAGS\'] = copy(env[\'LIBMAPNIK_CXXFLAGS\'])\\nprogram_env.Append(CPPDEFINES = env[\'LIBMAPNIK_DEFINES\'])\\n\\nif env[\'HAS_CAIRO\']:\\n    program_env.PrependUnique(CPPPATH=env[\'CAIRO_CPPPATHS\'])\\n    program_env.Append(CPPDEFINES = \'-DHAVE_CAIRO\')\\n\\nprogram_env.PrependUnique(CPPPATH=[\'#plugins/input/postgis\'])\\n\\nlibraries = []\\nboost_program_options = \'boost_program_options%s\' % env[\'BOOST_APPEND\']\\nlibraries.extend([boost_program_options,\'sqlite3\',\'pq\',\'mapnik\',\'icuuc\'])\\n\\nif env.get(\'BOOST_LIB_VERSION_FROM_HEADER\'):\\n    boost_version_from_header = int(env[\'BOOST_LIB_VERSION_FROM_HEADER\'].split(\'_\')[1])\\n    if boost_version_from_header >= 50:\\n        boost_system = \'boost_system%s\' % env[\'BOOST_APPEND\']\\n        libraries.extend([boost_system])\\n\\nif env[\'SQLITE_LINKFLAGS\']:\\n    program_env.Append(LINKFLAGS=env[\'SQLITE_LINKFLAGS\'])\\n\\nif env[\'RUNTIME_LINK\'] == \'static\':\\n    if env[\'PLATFORM\'] == \'Darwin\':\\n        libraries.extend([\'ldap\', \'pam\', \'ssl\', \'crypto\', \'krb5\'])\\n    else:\\n        # TODO - parse back into libraries variable\\n        program_env.ParseConfig(\'pg_config --libs\')\\n        libraries.append(\'dl\')\\n\\npgsql2sqlite = program_env.Program(\'pgsql2sqlite\', source, LIBS=libraries)\\nDepends(pgsql2sqlite, env.subst(\'../../src/%s\' % env[\'MAPNIK_LIB_NAME\']))\\n\\nif \'uninstall\' not in COMMAND_LINE_TARGETS:\\n    env.Install(os.path.join(env[\'INSTALL_PREFIX\'],\'bin\'), pgsql2sqlite)\\n    env.Alias(\'install\', os.path.join(env[\'INSTALL_PREFIX\'],\'bin\'))\\n\\nenv[\'create_uninstall_target\'](env, os.path.join(env[\'INSTALL_PREFIX\'],\'bin\',\'pgsql2sqlite\'))\\n" }\n'
line: b'{ "repo_name": "cloudbase/nova-virtualbox", "ref": "refs/heads/virtualbox_driver", "path": "nova/db/sqlalchemy/migrate_repo/versions/274_update_instances_project_id_index.py", "content": "# Copyright 2014 Rackspace Hosting\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\n\\nfrom sqlalchemy import MetaData, Table, Index\\n\\n\\ndef upgrade(migrate_engine):\\n    \\"\\"\\"Change instances (project_id) index to cover (project_id, deleted).\\"\\"\\"\\n\\n    meta = MetaData(bind=migrate_engine)\\n\\n    # Indexes can\'t be changed, we need to create the new one and delete\\n    # the old one\\n\\n    instances = Table(\'instances\', meta, autoload=True)\\n\\n    index = Index(\'instances_project_id_deleted_idx\',\\n                  instances.c.project_id, instances.c.deleted)\\n    index.create()\\n\\n    for index in instances.indexes:\\n        if [c.name for c in index.columns] == [\'project_id\']:\\n            index.drop()\\n\\n\\ndef downgrade(migrate_engine):\\n    \\"\\"\\"Change instances (project_id, deleted) index to cover (project_id).\\"\\"\\"\\n\\n    meta = MetaData(bind=migrate_engine)\\n\\n    instances = Table(\'instances\', meta, autoload=True)\\n\\n    index = Index(\'project_id\', instances.c.project_id)\\n    index.create()\\n\\n    for index in instances.indexes:\\n        if [c.name for c in index.columns] == [\'project_id\', \'deleted\']:\\n            index.drop()\\n" }\n'
line: b'{ "repo_name": "jeffreylu9/django-cms", "ref": "refs/heads/wlsite", "path": "cms/admin/permissionadmin.py", "content": "# -*- coding: utf-8 -*-\\nfrom copy import deepcopy\\nfrom django.contrib import admin\\nfrom django.contrib.admin import site\\nfrom django.contrib.auth import get_user_model, get_permission_codename\\nfrom django.contrib.auth.admin import UserAdmin\\nfrom django.utils.translation import ugettext as _\\n\\nfrom cms.admin.forms import GlobalPagePermissionAdminForm, PagePermissionInlineAdminForm, ViewRestrictionInlineAdminForm\\nfrom cms.exceptions import NoPermissionsException\\nfrom cms.models import Page, PagePermission, GlobalPagePermission, PageUser\\nfrom cms.utils.conf import get_cms_setting\\nfrom cms.utils.helpers import classproperty\\nfrom cms.utils.permissions import get_user_permission_level\\n\\nfrom cms.admin import cmsadmin\\n\\nPERMISSION_ADMIN_INLINES = []\\n\\nuser_model = get_user_model()\\nadmin_class = UserAdmin\\nfor model, admin_instance in site._registry.items():\\n    if model == user_model:\\n        admin_class = admin_instance.__class__\\n\\nclass TabularInline(admin.TabularInline):\\n    pass\\n\\n\\nclass PagePermissionInlineAdmin(TabularInline):\\n    model = PagePermission\\n    # use special form, so we can override of user and group field\\n    form = PagePermissionInlineAdminForm\\n    classes = [\'collapse\', \'collapsed\']\\n    exclude = [\'can_view\']\\n    extra = 0  # edit page load time boost\\n\\n    @classproperty\\n    def raw_id_fields(cls):\\n        # Dynamically set raw_id_fields based on settings\\n        threshold = get_cms_setting(\'RAW_ID_USERS\')\\n        if threshold and get_user_model().objects.count() > threshold:\\n            return [\'user\']\\n        return []\\n\\n    def get_queryset(self, request):\\n        \\"\\"\\"\\n        Queryset change, so user with global change permissions can see\\n        all permissions. Otherwise can user see only permissions for\\n        peoples which are under him (he can\'t see his permissions, because\\n        this will lead to violation, when he can add more power to itself)\\n        \\"\\"\\"\\n        # can see only permissions for users which are under him in tree\\n\\n        # here an exception can be thrown\\n        try:\\n            qs = PagePermission.objects.subordinate_to_user(request.user)\\n            return qs.filter(can_view=False)\\n        except NoPermissionsException:\\n            return self.objects.get_empty_query_set()\\n\\n    def get_formset(self, request, obj=None, **kwargs):\\n        \\"\\"\\"\\n        Some fields may be excluded here. User can change only\\n        permissions which are available for him. E.g. if user does not haves\\n        can_publish flag, he can\'t change assign can_publish permissions.\\n        \\"\\"\\"\\n        exclude = self.exclude or []\\n        if obj:\\n            if not obj.has_add_permission(request):\\n                exclude.append(\'can_add\')\\n            if not obj.has_delete_permission(request):\\n                exclude.append(\'can_delete\')\\n            if not obj.has_publish_permission(request):\\n                exclude.append(\'can_publish\')\\n            if not obj.has_advanced_settings_permission(request):\\n                exclude.append(\'can_change_advanced_settings\')\\n            if not obj.has_move_page_permission(request):\\n                exclude.append(\'can_move_page\')\\n        formset_cls = super(PagePermissionInlineAdmin, self\\n        ).get_formset(request, obj=None, exclude=exclude, **kwargs)\\n        qs = self.get_queryset(request)\\n        if obj is not None:\\n            qs = qs.filter(page=obj)\\n        formset_cls._queryset = qs\\n        return formset_cls\\n\\n\\nclass ViewRestrictionInlineAdmin(PagePermissionInlineAdmin):\\n    extra = 0  # edit page load time boost\\n    form = ViewRestrictionInlineAdminForm\\n    verbose_name = _(\\"View restriction\\")\\n    verbose_name_plural = _(\\"View restrictions\\")\\n    exclude = [\\n        \'can_add\', \'can_change\', \'can_delete\', \'can_view\',\\n        \'can_publish\', \'can_change_advanced_settings\', \'can_move_page\',\\n        \'can_change_permissions\'\\n    ]\\n\\n    def get_formset(self, request, obj=None, **kwargs):\\n        \\"\\"\\"\\n        Some fields may be excluded here. User can change only permissions\\n        which are available for him. E.g. if user does not haves can_publish\\n        flag, he can\'t change assign can_publish permissions.\\n        \\"\\"\\"\\n        formset_cls = super(PagePermissionInlineAdmin, self).get_formset(request, obj, **kwargs)\\n        qs = self.get_queryset(request)\\n        if obj is not None:\\n            qs = qs.filter(page=obj)\\n        formset_cls._queryset = qs\\n        return formset_cls\\n\\n    def get_queryset(self, request):\\n        \\"\\"\\"\\n        Returns a QuerySet of all model instances that can be edited by the\\n        admin site. This is used by changelist_view.\\n        \\"\\"\\"\\n        qs = PagePermission.objects.subordinate_to_user(request.user)\\n        return qs.filter(can_view=True)\\n\\n\\nclass GlobalPagePermissionAdmin(admin.ModelAdmin):\\n    list_display = [\'user\', \'group\', \'can_change\', \'can_delete\', \'can_publish\', \'can_change_permissions\']\\n    list_filter = [\'user\', \'group\', \'can_change\', \'can_delete\', \'can_publish\', \'can_change_permissions\']\\n\\n    form = GlobalPagePermissionAdminForm\\n    search_fields = []\\n    for field in admin_class.search_fields:\\n        search_fields.append(\\"user__%s\\" % field)\\n    search_fields.append(\'group__name\')\\n\\n    exclude = []\\n\\n    list_display.append(\'can_change_advanced_settings\')\\n    list_filter.append(\'can_change_advanced_settings\')\\n\\n\\nclass GenericCmsPermissionAdmin(object):\\n    \\"\\"\\"\\n    Custom mixin for permission-enabled admin interfaces.\\n    \\"\\"\\"\\n\\n    def update_permission_fieldsets(self, request, obj=None):\\n        \\"\\"\\"\\n        Nobody can grant more than he haves, so check for user permissions\\n        to Page and User model and render fieldset depending on them.\\n        \\"\\"\\"\\n        fieldsets = deepcopy(self.fieldsets)\\n        perm_models = (\\n            (Page, _(\'Page permissions\')),\\n            (PageUser, _(\'User & Group permissions\')),\\n            (PagePermission, _(\'Page permissions management\')),\\n        )\\n        for i, perm_model in enumerate(perm_models):\\n            model, title = perm_model\\n            opts, fields = model._meta, []\\n            name = model.__name__.lower()\\n            for key in (\'add\', \'change\', \'delete\'):\\n                perm_code = \'%s.%s\' % (opts.app_label, get_permission_codename(key, opts))\\n                if request.user.has_perm(perm_code):\\n                    fields.append(\'can_%s_%s\' % (key, name))\\n            if fields:\\n                fieldsets.insert(2 + i, (title, {\'fields\': (fields,)}))\\n        return fieldsets\\n\\n    def _has_change_permissions_permission(self, request):\\n        \\"\\"\\"\\n        User is able to add/change objects only if he haves can change\\n        permission on some page.\\n        \\"\\"\\"\\n        try:\\n            get_user_permission_level(request.user)\\n        except NoPermissionsException:\\n            return False\\n        return True\\n\\n    def has_add_permission(self, request):\\n        return self._has_change_permissions_permission(request) and \\\\\\n               super(self.__class__, self).has_add_permission(request)\\n\\n    def has_change_permission(self, request, obj=None):\\n        return self._has_change_permissions_permission(request) and \\\\\\n               super(self.__class__, self).has_change_permission(request, obj)\\n\\n\\nif get_cms_setting(\'PERMISSION\'):\\n    admin.site.register(GlobalPagePermission, GlobalPagePermissionAdmin)\\n    PERMISSION_ADMIN_INLINES.extend([\\n        ViewRestrictionInlineAdmin,\\n        PagePermissionInlineAdmin,\\n    ])\\n\\n" }\n'
line: b'{ "repo_name": "ppiotr/Bibedit-some-refactoring", "ref": "refs/heads/bibedit-hp-change-to-field-with-many-instances", "path": "modules/bibcheck/web/admin/bibcheckadmin.py", "content": "## This file is part of CDS Invenio.\\n## Copyright (C) 2002, 2003, 2004, 2005, 2006, 2007, 2008 CERN.\\n##\\n## CDS Invenio is free software; you can redistribute it and/or\\n## modify it under the terms of the GNU General Public License as\\n## published by the Free Software Foundation; either version 2 of the\\n## License, or (at your option) any later version.\\n##\\n## CDS Invenio is distributed in the hope that it will be useful, but\\n## WITHOUT ANY WARRANTY; without even the implied warranty of\\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n## General Public License for more details.\\n##\\n## You should have received a copy of the GNU General Public License\\n## along with CDS Invenio; if not, write to the Free Software Foundation, Inc.,\\n## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\\n\\n\\"\\"\\"CDS Invenio BibCheck Administrator Interface.\\"\\"\\"\\n\\nimport cgi\\nimport os\\nimport os.path\\nimport MySQLdb\\nfrom invenio.bibrankadminlib import check_user\\nfrom invenio.webpage import page, create_error_box\\nfrom invenio.webuser import getUid, page_not_authorized\\nfrom invenio.messages import wash_language, gettext_set_language\\n#from invenio.urlutils import wash_url_argument, redirect_to_url\\nfrom invenio.config import CFG_SITE_LANG, CFG_SITE_URL, \\\\\\n                           CFG_SITE_NAME, CFG_ETCDIR, CFG_BINDIR\\n\\n__lastupdated__ = \\"\\"\\"$Date$\\"\\"\\"\\n\\n\\ndef is_admin(req):\\n    \\"\\"\\"checks if the user has the rights (etc)\\"\\"\\"\\n    # Check if user is authorized to administer\\n    uid = 0\\n    try:\\n        uid = getUid(req)\\n    except MySQLdb.Error:\\n        return error_page(req)\\n    (auth_code, auth_msg) = check_user(req, \'cfgbibformat\')\\n    if not auth_code:\\n        return (True, uid)\\n    else:\\n        return (False, uid)\\n\\n\\ndef index(req, search=\\"\\", ln=CFG_SITE_LANG):\\n    \\"\\"\\"\\n    Main BibCheck administration page.\\n    @param ln: language\\n    \\"\\"\\"\\n    ln = wash_language(ln)\\n    _ = gettext_set_language(ln)\\n    navtrail = \\"\\"\\"<a class=\\"navtrail\\" href=\\"%s/help/admin\\">%s</a>\\"\\"\\" % \\\\\\n               (CFG_SITE_URL, _(\\"Admin Area\\"))\\n    (admin_ok, uid) = is_admin(req)\\n    if admin_ok:\\n        return page(title=_(\\"BibCheck Admin\\"),\\n                body=_perform_request_index(ln, search),\\n                language=ln,\\n                uid=uid,\\n                navtrail = navtrail,\\n                lastupdated=__lastupdated__,\\n                req=req,\\n                warnings=[])\\n    else:\\n        #redirect to login\\n        return page_not_authorized(req=req, text=_(\\"Not authorized\\"), navtrail=navtrail)\\n\\n\\ndef _perform_request_index(ln, search=\\"\\"):\\n    \\"\\"\\" makes a listing of files that are found in etc/bibcheck.\\n        Include a delete button for each. \\"\\"\\"\\n    ln = wash_language(ln)\\n    _ = gettext_set_language(ln)\\n    mydir = CFG_ETCDIR+\\"/bibcheck\\"\\n    if not os.path.exists(mydir):\\n        return _(\\"ERROR\\")+\\" \\"+mydir+\\" \\"+_(\\"does not exist\\")\\n    if not os.path.isdir(mydir):\\n        return  _(\\"ERROR\\")+\\" \\"+mydir+\\" \\"+_(\\"is not a directory\\")\\n    if not os.access(mydir, os.W_OK):\\n        return  _(\\"ERROR\\")+\\" \\"+mydir+\\" \\"+_(\\"is not writable\\")\\n    myfiles = os.listdir(mydir)\\n    if search:\\n        #include only files that match\\n        matching = []\\n        for myfile in myfiles:\\n            if (myfile.count(search) > 0):\\n                matching.append(myfile)\\n            else: #see if the string is in the file\\n                mypath = CFG_ETCDIR+\\"/bibcheck/\\"+myfile\\n                infile = file(mypath, \'r\')\\n                filelines = infile.readlines()\\n                for line in filelines:\\n                    if line.count(search) > 0:\\n                        matching.append(myfile)\\n                        break\\n                infile.close()\\n        myfiles = matching\\n    lines = \\"\\"\\n    #add a search box\\n    lines += \\"\\"\\"\\n        <!--make a search box-->\\n        <table class=\\"admin_wvar\\" cellspacing=\\"0\\">\\n        <tr><td>\\n        <form action=\\"%(siteurl)s/admin/bibcheck/bibcheckadmin.py/index\\">\\n          %(searchforastr)s\\n          <input type=\\"text\\" name=\\"search\\" value=\\"%(search)s\\" />\\n          <input type=\\"hidden\\" name=\\"ln\\" value=\\"%(ln)s\\" />\\n          <input type=\\"submit\\" class=\\"adminbutton\\" value=\\"Search\\">\\n          </form>\\n          </td></tr></table> \\"\\"\\" % { \'siteurl\': CFG_SITE_URL,\\n                                     \'search\': search,\\n                                     \'ln\': ln,\\n                                      \'searchforastr\': _(\\"Limit to knowledge bases containing string:\\") }\\n    if myfiles:\\n        #create a table..\\n        oddstripestyle = \'style=\\"background-color: rgb(235, 247, 255);\\"\' #for every other line\\n        lines += \'<table class=\\"admin_wvar\\">\\\\n\'\\n        isodd = True\\n        myfiles.sort()\\n        for myfile in myfiles:\\n            mystyle = \\"\\"\\n            if isodd:\\n                mystyle = oddstripestyle\\n            isodd = not isodd\\n            lines += \\"<tr \\"+mystyle+\\">\\"\\n            line = \'<td>\' + cgi.escape(myfile) + \'</td>\'\\n            line += \'<td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\'\\n            line += \'<td><a href=\\"%s/admin/bibcheck/bibcheckadmin.py/edit?fname=\' % CFG_SITE_URL + \\\\\\n                    myfile+\'&ln=\'+ln+\'\\">Edit</a></td>\'\\n            line += \'<td>&nbsp;&nbsp;&nbsp;</td>\'\\n            reallydelq = _(\\"Really delete\\")+\\" \\"+myfile+\\"?\\"\\n            line += \'<td><a href=\\"%s/admin/bibcheck/bibcheckadmin.py/delete?fname=\' % CFG_SITE_URL + \\\\\\n                    myfile+\'&ln=\'+ln+\'\\" onclick=\\"return confirm(\\\\\'\'+reallydelq+\'\\\\\');\\">\'+_(\\"Delete\\")+\'</a></td>\'\\n            #verify syntax..\\n            line += \'<td>&nbsp;&nbsp;&nbsp;</td>\'\\n            line += \'<td><a href=\\"%s/admin/bibcheck/bibcheckadmin.py/verify?fname=\' % CFG_SITE_URL + \\\\\\n                    myfile+\'&ln=\'+ln+\'\\">\'+_(\\"Verify syntax\\")+\'</a></td>\'\\n            lines += line+\\"</tr>\\\\n\\"\\n        lines += \\"</table>\\\\n\\"\\n    myout = lines\\n    myout += \\"<br/><br/><a href=\\\\\\"%s/admin/bibcheck/bibcheckadmin.py/edit\\\\\\">\\" % CFG_SITE_URL + \\\\\\n             _(\\"Create new\\")+\\"</a>\\"\\n    return myout\\n\\ndef verify(req, fname, ln=CFG_SITE_LANG):\\n    \\"\\"\\"verify syntax by calling an external checking program\\"\\"\\"\\n    ln = wash_language(ln)\\n    _ = gettext_set_language(ln)\\n    (admin_ok, uid) = is_admin(req)\\n\\n    # sanity check for fname:\\n    fname = os.path.basename(fname)\\n\\n    navtrail = \\"\\"\\"<a class=\\"navtrail\\" href=\\"%s/help/admin\\">%s</a>\\"\\"\\" % \\\\\\n               (CFG_SITE_URL, _(\\"Admin Area\\"))\\n    navtrail += \\"\\"\\"&gt; <a class=\\"navtrail\\" href=\\"%s/admin/bibcheck/bibcheckadmin.py/\\">BibCheck Admin</a> \\"\\"\\" % CFG_SITE_URL\\n    errors = \\"\\"\\n    outstr = \\"\\"\\n    errstr = \\"\\"\\n    path_to_bibcheck_cli = CFG_BINDIR + os.sep + \'bibcheck\'\\n    if not os.path.exists(path_to_bibcheck_cli):\\n        errors = _(\\"File %s does not exist.\\") % path_to_bibcheck_cli\\n    if not errors:\\n        #first check where we have stderr now so that we can assign it back\\n        try:\\n            (handle, mystdout, mystderr) = os.popen3(path_to_bibcheck_cli + \\" --verify\\" + fname)\\n            outstr = str(mystdout.readlines())\\n            errstr = str(mystderr.readlines())\\n        except:\\n            #the call failed?\\n            errors = _(\\"Calling bibcheck -verify failed.\\")\\n    if not errors:\\n        if not errstr:\\n            return \\"OK\\"\\n        else:\\n            return errstr\\n    else:\\n        return page(title=_(\\"Verify BibCheck config file\\"),\\n                body= _(\\"Verify problem\\")+\\":<br/>\\"+errors,\\n                language= ln,\\n                uid=uid,\\n                navtrail = navtrail,\\n                lastupdated=__lastupdated__,\\n                req=req,\\n                warnings=[])\\n\\ndef edit(req, ln=CFG_SITE_LANG, fname=\\"\\"):\\n    \\"\\"\\" creates an editor for the file. This is called also when the user wants to\\n        create a new file. In the case fname is empty\\"\\"\\"\\n    ln = wash_language(ln)\\n    _ = gettext_set_language(ln)\\n\\n    # sanity check for fname:\\n    fname = os.path.basename(fname)\\n\\n    #check auth\\n    (admin_ok, uid) = is_admin(req)\\n    navtrail = \\"\\"\\"<a class=\\"navtrail\\" href=\\"%s/help/admin\\">%s</a>\\"\\"\\" % \\\\\\n               (CFG_SITE_URL, _(\\"Admin Area\\"))\\n    navtrail += \\"\\"\\"&gt; <a class=\\"navtrail\\" href=\\"%s/admin/bibcheck/bibcheckadmin.py/\\">BibCheck Admin</a> \\"\\"\\" % CFG_SITE_URL\\n    myout = _(\\"File\\")+\\" \\" + cgi.escape(fname) + \\"<br/>\\"\\n    if admin_ok:\\n        #add a javascript checker so that the user cannot save a form with empty\\n        #fname\\n        myout += \\"\\"\\"<script language=\\"JavaScript\\" type=\\"text/javascript\\">\\n                    <!--\\n                     function checkform ( form ) { if (form.fname.value == \\"\\") {\\n                              alert( \\"Missing filename.\\" ); form.fname.focus(); return false ;\\n                          }\\n                            return true ;\\n                    }\\n                     -->\\n                     </script>\\"\\"\\"\\n\\n\\n        #read the file if there is one\\n        filelines = []\\n        if fname:\\n            myfile = CFG_ETCDIR+\\"/bibcheck/\\"+fname\\n            infile = file(myfile, \'r\')\\n            filelines = infile.readlines()\\n            infile.close()\\n        myout += \'<form method=\\"post\\" action=\\"save\\" onsubmit=\\"return checkform(this);\\">\'\\n        #create a filename dialog box if there is no fname, otherwise it\'s hidden\\n        if fname:\\n            myout += \'<input type=\\"hidden\\" name=\\"fname\\" value=\\"\'+fname+\'\\">\'\\n        else:\\n            myout += \'<input name=\\"fname\\" value=\\"\'+fname+\'\\"><br/>\'\\n            myout += \'<input type=\\"hidden\\" name=\\"wasnew\\" value=\\"1\\">\'\\n        myout += \'<input type=\\"hidden\\" name=\\"ln\\" value=\\"\'+ln+\'\\">\'\\n        myout += \'<textarea name=\\"code\\" id=\\"code\\" rows=\\"25\\" style=\\"width:100%\\">\'\\n        for line in filelines:\\n            myout += line\\n        #create a save button\\n        myout += \'</textarea><br/><input type=\\"submit\\" name=\\"save\\" value=\\"\'+_(\\"Save Changes\\")+\'\\"></form>\'\\n        #create page\\n        return page(title=_(\\"Edit BibCheck config file\\"),\\n                body= myout,\\n                language= ln,\\n                uid=uid,\\n                navtrail = navtrail,\\n                lastupdated=__lastupdated__,\\n                req=req,\\n                warnings=[])\\n    else: #not admin\\n        return page_not_authorized(req=req, text=_(\\"Not authorized\\"), navtrail=navtrail)\\n\\ndef save(req, ln, fname, code, wasnew=0):\\n    \\"\\"\\"saves code into file fname. wasnew is 1 if this is a new file\\"\\"\\"\\n    ln = wash_language(ln)\\n    _ = gettext_set_language(ln)\\n\\n    # sanity check for fname:\\n    fname = os.path.basename(fname)\\n\\n    #check auth\\n    (admin_ok, uid) = is_admin(req)\\n    navtrail = \'\'\'<a class=\\"navtrail\\" href=\\"%s/help/admin\\">%s</a>\'\'\' % \\\\\\n               (CFG_SITE_URL, _(\\"Admin Area\\"))\\n    navtrail += \\"\\"\\"&gt; <a class=\\"navtrail\\" href=\\"%s/admin/bibcheck/bibcheckadmin.py/\\">BibCheck Admin</a> \\"\\"\\" % CFG_SITE_URL\\n    myout = _(\\"File\\")+\\" \\" + cgi.escape(fname) + \\" \\"\\n    if admin_ok:\\n        myfile = CFG_ETCDIR+\\"/bibcheck/\\"+fname\\n        #check if the file exists if this was new\\n        if wasnew and os.path.exists(myfile):\\n            msg = myout+\\" \\"+_(\\"already exists.\\")\\n        else:\\n            #write code into file\\n            msg = myout+_(\\"written OK.\\")\\n            try:\\n                outfile = file(myfile, \'w\')\\n                outfile.write(code)\\n                outfile.close()\\n            except IOError:\\n                msg = myout+_(\\"write failed.\\")\\n        #print message\\n        return page(title=_(\\"Save BibCheck config file\\"),\\n                body= msg,\\n                language= ln,\\n                uid=uid,\\n                navtrail = navtrail,\\n                lastupdated=__lastupdated__,\\n                req=req,\\n                warnings=[])\\n    else:\\n        return page_not_authorized(req=req, text=_(\\"Not authorized\\"), navtrail=navtrail)\\n\\ndef delete(req, ln, fname):\\n    \\"\\"\\"delete file fname\\"\\"\\"\\n    ln = wash_language(ln)\\n    _ = gettext_set_language(ln)\\n\\n    # sanity check for fname:\\n    fname = os.path.basename(fname)\\n\\n    #check auth\\n    (admin_ok, uid) = is_admin(req)\\n    navtrail = \\"\\"\\"<a class=\\"navtrail\\" href=\\"%s/help/admin\\">%s</a>\\"\\"\\" % \\\\\\n               (CFG_SITE_URL, _(\\"Admin Area\\"))\\n    navtrail += \\"\\"\\"&gt; <a class=\\"navtrail\\" href=\\"%s/admin/bibcheck/bibcheckadmin.py/\\">BibCheck Admin</a> \\"\\"\\" % CFG_SITE_URL\\n    myout = _(\\"File\\")+\\" \\"+fname+\\" \\"\\n    if admin_ok:\\n        msg = \\"\\"\\n        myfile = CFG_ETCDIR+\\"/bibcheck/\\"+fname\\n        success = 1\\n        try:\\n            os.remove(myfile)\\n        except:\\n            success = 0\\n        if success:\\n            msg = myout+_(\\"deleted\\")+\\".\\"\\n        else:\\n            msg = myout+_(\\"delete failed\\")+\\".\\"\\n        #print message\\n        return page(title=_(\\"Delete BibCheck config file\\"),\\n                body= msg,\\n                language= ln,\\n                uid=uid,\\n                navtrail = navtrail,\\n                lastupdated=__lastupdated__,\\n                req=req,\\n                warnings=[])\\n    else:\\n        return page_not_authorized(req=req, text=_(\\"Not authorized\\"), navtrail=navtrail)\\n\\ndef error_page(req, ln=CFG_SITE_LANG, verbose=1):\\n    \\"\\"\\"Generic error .. in case one cannot find anything more specific\\"\\"\\"\\n    _ = gettext_set_language(ln)\\n    return page(title=_(\\"Internal Error\\"),\\n                body = create_error_box(req, verbose=verbose, ln=ln),\\n                description=\\"%s - Internal Error\\" % CFG_SITE_NAME,\\n                keywords=\\"%s, Internal Error\\" % CFG_SITE_NAME,\\n                language=ln,\\n                req=req)\\n\\n\\n" }\n'
line: b'{ "repo_name": "Arakmar/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "sallaire/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "eptmp3/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "puzzlet/chardet", "ref": "refs/heads/MarkPilgrim", "path": "src-python2/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "brinbois/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "stonewell/learn-curve", "ref": "refs/heads/main", "path": "modules/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "murfz/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "fernandog/Sick-Beard", "ref": "refs/heads/ThePirateBay", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "LittleLama/Sick-Beard-BoxCar2", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "cstan11/Sick-Beard", "ref": "refs/heads/torrent_1080_subtitles", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "mano3m/CouchPotatoServer", "ref": "refs/heads/develop_mano3m", "path": "libs/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "bob123bob/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "tquizzle/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "flotre/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "alexpap/exareme", "ref": "refs/heads/mip", "path": "exareme-tools/madis/src/lib/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "rui-castro/Sick-Beard", "ref": "refs/heads/torrent_1080_subtitles", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "Taranys/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "Fafou/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "jymannob/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "Pistachitos/Sick-Beard", "ref": "refs/heads/Pistachitos", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "cyril51/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "icucinema/madcow", "ref": "refs/heads/icu-cinema-deployment", "path": "madcow/include/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "pepetreshere/odoo", "ref": "refs/heads/patch-2", "path": "addons/website_event_meet/models/__init__.py", "content": "# -*- coding: utf-8 -*-\\n# Part of Odoo. See LICENSE file for full copyright and licensing details.\\n\\nfrom . import event_event\\nfrom . import event_type\\nfrom . import event_meeting_room\\nfrom . import ir_autovacuum\\nfrom . import website_event_menu\\n" }\n'
line: b'{ "repo_name": "navycrow/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "kobolabs/calibre", "ref": "refs/heads/kobo", "path": "src/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "mozvip/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "MadCat34/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "gtko/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "nadley/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "sss/calibre-at-bzr", "ref": "refs/heads/upstream/master", "path": "src/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "l0b0/cds-invenio-vengmark", "ref": "refs/heads/install-from-source", "path": "modules/bibcheck/web/admin/bibcheckadmin.py", "content": "## This file is part of CDS Invenio.\\n## Copyright (C) 2002, 2003, 2004, 2005, 2006, 2007, 2008 CERN.\\n##\\n## CDS Invenio is free software; you can redistribute it and/or\\n## modify it under the terms of the GNU General Public License as\\n## published by the Free Software Foundation; either version 2 of the\\n## License, or (at your option) any later version.\\n##\\n## CDS Invenio is distributed in the hope that it will be useful, but\\n## WITHOUT ANY WARRANTY; without even the implied warranty of\\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n## General Public License for more details.\\n##\\n## You should have received a copy of the GNU General Public License\\n## along with CDS Invenio; if not, write to the Free Software Foundation, Inc.,\\n## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\\n\\n\\"\\"\\"CDS Invenio BibCheck Administrator Interface.\\"\\"\\"\\n\\nimport cgi\\nimport os\\nimport os.path\\nimport MySQLdb\\nfrom invenio.bibrankadminlib import check_user\\nfrom invenio.webpage import page, create_error_box\\nfrom invenio.webuser import getUid, page_not_authorized\\nfrom invenio.messages import wash_language, gettext_set_language\\n#from invenio.urlutils import wash_url_argument, redirect_to_url\\nfrom invenio.config import CFG_SITE_LANG, CFG_SITE_URL, \\\\\\n                           CFG_SITE_NAME, CFG_ETCDIR, CFG_BINDIR\\n\\n__lastupdated__ = \\"\\"\\"$Date$\\"\\"\\"\\n\\n\\ndef is_admin(req):\\n    \\"\\"\\"checks if the user has the rights (etc)\\"\\"\\"\\n    # Check if user is authorized to administer\\n    uid = 0\\n    try:\\n        uid = getUid(req)\\n    except MySQLdb.Error:\\n        return error_page(req)\\n    (auth_code, auth_msg) = check_user(req, \'cfgbibformat\')\\n    if not auth_code:\\n        return (True, uid)\\n    else:\\n        return (False, uid)\\n\\n\\ndef index(req, search=\\"\\", ln=CFG_SITE_LANG):\\n    \\"\\"\\"\\n    Main BibCheck administration page.\\n    @param ln: language\\n    \\"\\"\\"\\n    ln = wash_language(ln)\\n    _ = gettext_set_language(ln)\\n    navtrail = \\"\\"\\"<a class=\\"navtrail\\" href=\\"%s/help/admin\\">%s</a>\\"\\"\\" % \\\\\\n               (CFG_SITE_URL, _(\\"Admin Area\\"))\\n    (admin_ok, uid) = is_admin(req)\\n    if admin_ok:\\n        return page(title=_(\\"BibCheck Admin\\"),\\n                body=_perform_request_index(ln, search),\\n                language=ln,\\n                uid=uid,\\n                navtrail = navtrail,\\n                lastupdated=__lastupdated__,\\n                req=req,\\n                warnings=[])\\n    else:\\n        #redirect to login\\n        return page_not_authorized(req=req, text=_(\\"Not authorized\\"), navtrail=navtrail)\\n\\n\\ndef _perform_request_index(ln, search=\\"\\"):\\n    \\"\\"\\" makes a listing of files that are found in etc/bibcheck.\\n        Include a delete button for each. \\"\\"\\"\\n    ln = wash_language(ln)\\n    _ = gettext_set_language(ln)\\n    mydir = CFG_ETCDIR+\\"/bibcheck\\"\\n    if not os.path.exists(mydir):\\n        return _(\\"ERROR\\")+\\" \\"+mydir+\\" \\"+_(\\"does not exist\\")\\n    if not os.path.isdir(mydir):\\n        return  _(\\"ERROR\\")+\\" \\"+mydir+\\" \\"+_(\\"is not a directory\\")\\n    if not os.access(mydir, os.W_OK):\\n        return  _(\\"ERROR\\")+\\" \\"+mydir+\\" \\"+_(\\"is not writable\\")\\n    myfiles = os.listdir(mydir)\\n    if search:\\n        #include only files that match\\n        matching = []\\n        for myfile in myfiles:\\n            if (myfile.count(search) > 0):\\n                matching.append(myfile)\\n            else: #see if the string is in the file\\n                mypath = CFG_ETCDIR+\\"/bibcheck/\\"+myfile\\n                infile = file(mypath, \'r\')\\n                filelines = infile.readlines()\\n                for line in filelines:\\n                    if line.count(search) > 0:\\n                        matching.append(myfile)\\n                        break\\n                infile.close()\\n        myfiles = matching\\n    lines = \\"\\"\\n    #add a search box\\n    lines += \\"\\"\\"\\n        <!--make a search box-->\\n        <table class=\\"admin_wvar\\" cellspacing=\\"0\\">\\n        <tr><td>\\n        <form action=\\"%(siteurl)s/admin/bibcheck/bibcheckadmin.py/index\\">\\n          %(searchforastr)s\\n          <input type=\\"text\\" name=\\"search\\" value=\\"%(search)s\\" />\\n          <input type=\\"hidden\\" name=\\"ln\\" value=\\"%(ln)s\\" />\\n          <input type=\\"submit\\" class=\\"adminbutton\\" value=\\"Search\\">\\n          </form>\\n          </td></tr></table> \\"\\"\\" % { \'siteurl\': CFG_SITE_URL,\\n                                     \'search\': search,\\n                                     \'ln\': ln,\\n                                      \'searchforastr\': _(\\"Limit to knowledge bases containing string:\\") }\\n    if myfiles:\\n        #create a table..\\n        oddstripestyle = \'style=\\"background-color: rgb(235, 247, 255);\\"\' #for every other line\\n        lines += \'<table class=\\"admin_wvar\\">\\\\n\'\\n        isodd = True\\n        myfiles.sort()\\n        for myfile in myfiles:\\n            mystyle = \\"\\"\\n            if isodd:\\n                mystyle = oddstripestyle\\n            isodd = not isodd\\n            lines += \\"<tr \\"+mystyle+\\">\\"\\n            line = \'<td>\' + cgi.escape(myfile) + \'</td>\'\\n            line += \'<td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\'\\n            line += \'<td><a href=\\"%s/admin/bibcheck/bibcheckadmin.py/edit?fname=\' % CFG_SITE_URL + \\\\\\n                    myfile+\'&ln=\'+ln+\'\\">Edit</a></td>\'\\n            line += \'<td>&nbsp;&nbsp;&nbsp;</td>\'\\n            reallydelq = _(\\"Really delete\\")+\\" \\"+myfile+\\"?\\"\\n            line += \'<td><a href=\\"%s/admin/bibcheck/bibcheckadmin.py/delete?fname=\' % CFG_SITE_URL + \\\\\\n                    myfile+\'&ln=\'+ln+\'\\" onclick=\\"return confirm(\\\\\'\'+reallydelq+\'\\\\\');\\">\'+_(\\"Delete\\")+\'</a></td>\'\\n            #verify syntax..\\n            line += \'<td>&nbsp;&nbsp;&nbsp;</td>\'\\n            line += \'<td><a href=\\"%s/admin/bibcheck/bibcheckadmin.py/verify?fname=\' % CFG_SITE_URL + \\\\\\n                    myfile+\'&ln=\'+ln+\'\\">\'+_(\\"Verify syntax\\")+\'</a></td>\'\\n            lines += line+\\"</tr>\\\\n\\"\\n        lines += \\"</table>\\\\n\\"\\n    myout = lines\\n    myout += \\"<br/><br/><a href=\\\\\\"%s/admin/bibcheck/bibcheckadmin.py/edit\\\\\\">\\" % CFG_SITE_URL + \\\\\\n             _(\\"Create new\\")+\\"</a>\\"\\n    return myout\\n\\ndef verify(req, fname, ln=CFG_SITE_LANG):\\n    \\"\\"\\"verify syntax by calling an external checking program\\"\\"\\"\\n    ln = wash_language(ln)\\n    _ = gettext_set_language(ln)\\n    (admin_ok, uid) = is_admin(req)\\n\\n    # sanity check for fname:\\n    fname = os.path.basename(fname)\\n\\n    navtrail = \\"\\"\\"<a class=\\"navtrail\\" href=\\"%s/help/admin\\">%s</a>\\"\\"\\" % \\\\\\n               (CFG_SITE_URL, _(\\"Admin Area\\"))\\n    navtrail += \\"\\"\\"&gt; <a class=\\"navtrail\\" href=\\"%s/admin/bibcheck/bibcheckadmin.py/\\">BibCheck Admin</a> \\"\\"\\" % CFG_SITE_URL\\n    errors = \\"\\"\\n    outstr = \\"\\"\\n    errstr = \\"\\"\\n    path_to_bibcheck_cli = CFG_BINDIR + os.sep + \'bibcheck\'\\n    if not os.path.exists(path_to_bibcheck_cli):\\n        errors = _(\\"File %s does not exist.\\") % path_to_bibcheck_cli\\n    if not errors:\\n        #first check where we have stderr now so that we can assign it back\\n        try:\\n            (handle, mystdout, mystderr) = os.popen3(path_to_bibcheck_cli + \\" --verify\\" + fname)\\n            outstr = str(mystdout.readlines())\\n            errstr = str(mystderr.readlines())\\n        except:\\n            #the call failed?\\n            errors = _(\\"Calling bibcheck -verify failed.\\")\\n    if not errors:\\n        if not errstr:\\n            return \\"OK\\"\\n        else:\\n            return errstr\\n    else:\\n        return page(title=_(\\"Verify BibCheck config file\\"),\\n                body= _(\\"Verify problem\\")+\\":<br/>\\"+errors,\\n                language= ln,\\n                uid=uid,\\n                navtrail = navtrail,\\n                lastupdated=__lastupdated__,\\n                req=req,\\n                warnings=[])\\n\\ndef edit(req, ln=CFG_SITE_LANG, fname=\\"\\"):\\n    \\"\\"\\" creates an editor for the file. This is called also when the user wants to\\n        create a new file. In the case fname is empty\\"\\"\\"\\n    ln = wash_language(ln)\\n    _ = gettext_set_language(ln)\\n\\n    # sanity check for fname:\\n    fname = os.path.basename(fname)\\n\\n    #check auth\\n    (admin_ok, uid) = is_admin(req)\\n    navtrail = \\"\\"\\"<a class=\\"navtrail\\" href=\\"%s/help/admin\\">%s</a>\\"\\"\\" % \\\\\\n               (CFG_SITE_URL, _(\\"Admin Area\\"))\\n    navtrail += \\"\\"\\"&gt; <a class=\\"navtrail\\" href=\\"%s/admin/bibcheck/bibcheckadmin.py/\\">BibCheck Admin</a> \\"\\"\\" % CFG_SITE_URL\\n    myout = _(\\"File\\")+\\" \\" + cgi.escape(fname) + \\"<br/>\\"\\n    if admin_ok:\\n        #add a javascript checker so that the user cannot save a form with empty\\n        #fname\\n        myout += \\"\\"\\"<script language=\\"JavaScript\\" type=\\"text/javascript\\">\\n                    <!--\\n                     function checkform ( form ) { if (form.fname.value == \\"\\") {\\n                              alert( \\"Missing filename.\\" ); form.fname.focus(); return false ;\\n                          }\\n                            return true ;\\n                    }\\n                     -->\\n                     </script>\\"\\"\\"\\n\\n\\n        #read the file if there is one\\n        filelines = []\\n        if fname:\\n            myfile = CFG_ETCDIR+\\"/bibcheck/\\"+fname\\n            infile = file(myfile, \'r\')\\n            filelines = infile.readlines()\\n            infile.close()\\n        myout += \'<form method=\\"post\\" action=\\"save\\" onsubmit=\\"return checkform(this);\\">\'\\n        #create a filename dialog box if there is no fname, otherwise it\'s hidden\\n        if fname:\\n            myout += \'<input type=\\"hidden\\" name=\\"fname\\" value=\\"\'+fname+\'\\">\'\\n        else:\\n            myout += \'<input name=\\"fname\\" value=\\"\'+fname+\'\\"><br/>\'\\n            myout += \'<input type=\\"hidden\\" name=\\"wasnew\\" value=\\"1\\">\'\\n        myout += \'<input type=\\"hidden\\" name=\\"ln\\" value=\\"\'+ln+\'\\">\'\\n        myout += \'<textarea name=\\"code\\" id=\\"code\\" rows=\\"25\\" style=\\"width:100%\\">\'\\n        for line in filelines:\\n            myout += line\\n        #create a save button\\n        myout += \'</textarea><br/><input type=\\"submit\\" name=\\"save\\" value=\\"\'+_(\\"Save Changes\\")+\'\\"></form>\'\\n        #create page\\n        return page(title=_(\\"Edit BibCheck config file\\"),\\n                body= myout,\\n                language= ln,\\n                uid=uid,\\n                navtrail = navtrail,\\n                lastupdated=__lastupdated__,\\n                req=req,\\n                warnings=[])\\n    else: #not admin\\n        return page_not_authorized(req=req, text=_(\\"Not authorized\\"), navtrail=navtrail)\\n\\ndef save(req, ln, fname, code, wasnew=0):\\n    \\"\\"\\"saves code into file fname. wasnew is 1 if this is a new file\\"\\"\\"\\n    ln = wash_language(ln)\\n    _ = gettext_set_language(ln)\\n\\n    # sanity check for fname:\\n    fname = os.path.basename(fname)\\n\\n    #check auth\\n    (admin_ok, uid) = is_admin(req)\\n    navtrail = \'\'\'<a class=\\"navtrail\\" href=\\"%s/help/admin\\">%s</a>\'\'\' % \\\\\\n               (CFG_SITE_URL, _(\\"Admin Area\\"))\\n    navtrail += \\"\\"\\"&gt; <a class=\\"navtrail\\" href=\\"%s/admin/bibcheck/bibcheckadmin.py/\\">BibCheck Admin</a> \\"\\"\\" % CFG_SITE_URL\\n    myout = _(\\"File\\")+\\" \\" + cgi.escape(fname) + \\" \\"\\n    if admin_ok:\\n        myfile = CFG_ETCDIR+\\"/bibcheck/\\"+fname\\n        #check if the file exists if this was new\\n        if wasnew and os.path.exists(myfile):\\n            msg = myout+\\" \\"+_(\\"already exists.\\")\\n        else:\\n            #write code into file\\n            msg = myout+_(\\"written OK.\\")\\n            try:\\n                outfile = file(myfile, \'w\')\\n                outfile.write(code)\\n                outfile.close()\\n            except IOError:\\n                msg = myout+_(\\"write failed.\\")\\n        #print message\\n        return page(title=_(\\"Save BibCheck config file\\"),\\n                body= msg,\\n                language= ln,\\n                uid=uid,\\n                navtrail = navtrail,\\n                lastupdated=__lastupdated__,\\n                req=req,\\n                warnings=[])\\n    else:\\n        return page_not_authorized(req=req, text=_(\\"Not authorized\\"), navtrail=navtrail)\\n\\ndef delete(req, ln, fname):\\n    \\"\\"\\"delete file fname\\"\\"\\"\\n    ln = wash_language(ln)\\n    _ = gettext_set_language(ln)\\n\\n    # sanity check for fname:\\n    fname = os.path.basename(fname)\\n\\n    #check auth\\n    (admin_ok, uid) = is_admin(req)\\n    navtrail = \\"\\"\\"<a class=\\"navtrail\\" href=\\"%s/help/admin\\">%s</a>\\"\\"\\" % \\\\\\n               (CFG_SITE_URL, _(\\"Admin Area\\"))\\n    navtrail += \\"\\"\\"&gt; <a class=\\"navtrail\\" href=\\"%s/admin/bibcheck/bibcheckadmin.py/\\">BibCheck Admin</a> \\"\\"\\" % CFG_SITE_URL\\n    myout = _(\\"File\\")+\\" \\"+fname+\\" \\"\\n    if admin_ok:\\n        msg = \\"\\"\\n        myfile = CFG_ETCDIR+\\"/bibcheck/\\"+fname\\n        success = 1\\n        try:\\n            os.remove(myfile)\\n        except:\\n            success = 0\\n        if success:\\n            msg = myout+_(\\"deleted\\")+\\".\\"\\n        else:\\n            msg = myout+_(\\"delete failed\\")+\\".\\"\\n        #print message\\n        return page(title=_(\\"Delete BibCheck config file\\"),\\n                body= msg,\\n                language= ln,\\n                uid=uid,\\n                navtrail = navtrail,\\n                lastupdated=__lastupdated__,\\n                req=req,\\n                warnings=[])\\n    else:\\n        return page_not_authorized(req=req, text=_(\\"Not authorized\\"), navtrail=navtrail)\\n\\ndef error_page(req, ln=CFG_SITE_LANG, verbose=1):\\n    \\"\\"\\"Generic error .. in case one cannot find anything more specific\\"\\"\\"\\n    _ = gettext_set_language(ln)\\n    return page(title=_(\\"Internal Error\\"),\\n                body = create_error_box(req, verbose=verbose, ln=ln),\\n                description=\\"%s - Internal Error\\" % CFG_SITE_NAME,\\n                keywords=\\"%s, Internal Error\\" % CFG_SITE_NAME,\\n                language=ln,\\n                req=req)\\n\\n\\n" }\n'
line: b'{ "repo_name": "foufou55/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "gromez/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "Kiiv/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "sh4t/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "yannickcr/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "BaesFr/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "RAtechntukan/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "kinooo/Sick-Beard", "ref": "refs/heads/development", "path": "lib/requests/packages/chardet/codingstatemachine.py", "content": "######################## BEGIN LICENSE BLOCK ########################\\n# The Original Code is mozilla.org code.\\n#\\n# The Initial Developer of the Original Code is\\n# Netscape Communications Corporation.\\n# Portions created by the Initial Developer are Copyright (C) 1998\\n# the Initial Developer. All Rights Reserved.\\n#\\n# Contributor(s):\\n#   Mark Pilgrim - port to Python\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n# \\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n# \\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\\n# 02110-1301  USA\\n######################### END LICENSE BLOCK #########################\\n\\nfrom constants import eStart, eError, eItsMe\\n\\nclass CodingStateMachine:\\n    def __init__(self, sm):\\n        self._mModel = sm\\n        self._mCurrentBytePos = 0\\n        self._mCurrentCharLen = 0\\n        self.reset()\\n\\n    def reset(self):\\n        self._mCurrentState = eStart\\n\\n    def next_state(self, c):\\n        # for each byte we get its class\\n        # if it is first byte, we also get byte length\\n        byteCls = self._mModel[\'classTable\'][ord(c)]\\n        if self._mCurrentState == eStart:\\n            self._mCurrentBytePos = 0\\n            self._mCurrentCharLen = self._mModel[\'charLenTable\'][byteCls]\\n        # from byte\'s class and stateTable, we get its next state\\n        self._mCurrentState = self._mModel[\'stateTable\'][self._mCurrentState * self._mModel[\'classFactor\'] + byteCls]\\n        self._mCurrentBytePos += 1\\n        return self._mCurrentState\\n\\n    def get_current_charlen(self):\\n        return self._mCurrentCharLen\\n\\n    def get_coding_state_machine(self):\\n        return self._mModel[\'name\']\\n" }\n'
line: b'{ "repo_name": "nubark/odoo", "ref": "refs/heads/9.0", "path": "addons/account/tests/test_manual_reconciliation.py", "content": "from openerp.addons.account.tests.account_test_classes import AccountingTestCase\\n\\nclass TestManualReconciliation(AccountingTestCase):\\n\\n    def test_reconciliation_proposition(self):\\n        pass\\n\\n    def test_full_reconcile(self):\\n        pass\\n\\n    def test_partial_reconcile(self):\\n        pass\\n\\n    def test_reconcile_with_write_off(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "allenp/odoo", "ref": "refs/heads/9.0", "path": "addons/account/tests/test_manual_reconciliation.py", "content": "from openerp.addons.account.tests.account_test_classes import AccountingTestCase\\n\\nclass TestManualReconciliation(AccountingTestCase):\\n\\n    def test_reconciliation_proposition(self):\\n        pass\\n\\n    def test_full_reconcile(self):\\n        pass\\n\\n    def test_partial_reconcile(self):\\n        pass\\n\\n    def test_reconcile_with_write_off(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "odoobgorg/odoo", "ref": "refs/heads/9.0", "path": "addons/account/tests/test_manual_reconciliation.py", "content": "from openerp.addons.account.tests.account_test_classes import AccountingTestCase\\n\\nclass TestManualReconciliation(AccountingTestCase):\\n\\n    def test_reconciliation_proposition(self):\\n        pass\\n\\n    def test_full_reconcile(self):\\n        pass\\n\\n    def test_partial_reconcile(self):\\n        pass\\n\\n    def test_reconcile_with_write_off(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "Elico-Corp/odoo_OCB", "ref": "refs/heads/9.0", "path": "addons/account/tests/test_manual_reconciliation.py", "content": "from openerp.addons.account.tests.account_test_classes import AccountingTestCase\\n\\nclass TestManualReconciliation(AccountingTestCase):\\n\\n    def test_reconciliation_proposition(self):\\n        pass\\n\\n    def test_full_reconcile(self):\\n        pass\\n\\n    def test_partial_reconcile(self):\\n        pass\\n\\n    def test_reconcile_with_write_off(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "Vauxoo/server-tools", "ref": "refs/heads/12.0", "path": "company_country/migrations/12.0.1.0.2/pre-migration.py", "content": "import logging\\nfrom psycopg2.extensions import AsIs\\n\\nfrom odoo import tools\\n\\n\\n_logger = logging.getLogger(__name__)\\n\\n\\ndef migrate(cr, version):\\n    drop_table_model_company_country(cr)\\n\\n\\ndef drop_table_model_company_country(cr):\\n    tablename = \'company_country_config_settings\'\\n    if tools.table_exists(cr, tablename):\\n        _logger.info(\\"Dropping table %s\\", tablename)\\n        cr.execute(\\"DROP TABLE IF EXISTS %s;\\", (AsIs(tablename),))\\n" }\n'
line: b'{ "repo_name": "stephen144/odoo", "ref": "refs/heads/9.0", "path": "addons/account/tests/test_manual_reconciliation.py", "content": "from openerp.addons.account.tests.account_test_classes import AccountingTestCase\\n\\nclass TestManualReconciliation(AccountingTestCase):\\n\\n    def test_reconciliation_proposition(self):\\n        pass\\n\\n    def test_full_reconcile(self):\\n        pass\\n\\n    def test_partial_reconcile(self):\\n        pass\\n\\n    def test_reconcile_with_write_off(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "blooparksystems/odoo", "ref": "refs/heads/9.0", "path": "addons/account/tests/test_manual_reconciliation.py", "content": "from openerp.addons.account.tests.account_test_classes import AccountingTestCase\\n\\nclass TestManualReconciliation(AccountingTestCase):\\n\\n    def test_reconciliation_proposition(self):\\n        pass\\n\\n    def test_full_reconcile(self):\\n        pass\\n\\n    def test_partial_reconcile(self):\\n        pass\\n\\n    def test_reconcile_with_write_off(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "microcom/odoo", "ref": "refs/heads/9.0", "path": "addons/account/tests/test_manual_reconciliation.py", "content": "from openerp.addons.account.tests.account_test_classes import AccountingTestCase\\n\\nclass TestManualReconciliation(AccountingTestCase):\\n\\n    def test_reconciliation_proposition(self):\\n        pass\\n\\n    def test_full_reconcile(self):\\n        pass\\n\\n    def test_partial_reconcile(self):\\n        pass\\n\\n    def test_reconcile_with_write_off(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "optima-ict/odoo", "ref": "refs/heads/9.0", "path": "addons/account/tests/test_manual_reconciliation.py", "content": "from openerp.addons.account.tests.account_test_classes import AccountingTestCase\\n\\nclass TestManualReconciliation(AccountingTestCase):\\n\\n    def test_reconciliation_proposition(self):\\n        pass\\n\\n    def test_full_reconcile(self):\\n        pass\\n\\n    def test_partial_reconcile(self):\\n        pass\\n\\n    def test_reconcile_with_write_off(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "ludwiktrammer/odoo", "ref": "refs/heads/9.0", "path": "addons/account/tests/test_manual_reconciliation.py", "content": "from openerp.addons.account.tests.account_test_classes import AccountingTestCase\\n\\nclass TestManualReconciliation(AccountingTestCase):\\n\\n    def test_reconciliation_proposition(self):\\n        pass\\n\\n    def test_full_reconcile(self):\\n        pass\\n\\n    def test_partial_reconcile(self):\\n        pass\\n\\n    def test_reconcile_with_write_off(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "bplancher/odoo", "ref": "refs/heads/9.0", "path": "addons/account/tests/test_manual_reconciliation.py", "content": "from openerp.addons.account.tests.account_test_classes import AccountingTestCase\\n\\nclass TestManualReconciliation(AccountingTestCase):\\n\\n    def test_reconciliation_proposition(self):\\n        pass\\n\\n    def test_full_reconcile(self):\\n        pass\\n\\n    def test_partial_reconcile(self):\\n        pass\\n\\n    def test_reconcile_with_write_off(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "storm-computers/odoo", "ref": "refs/heads/9.0", "path": "addons/account/tests/test_manual_reconciliation.py", "content": "from openerp.addons.account.tests.account_test_classes import AccountingTestCase\\n\\nclass TestManualReconciliation(AccountingTestCase):\\n\\n    def test_reconciliation_proposition(self):\\n        pass\\n\\n    def test_full_reconcile(self):\\n        pass\\n\\n    def test_partial_reconcile(self):\\n        pass\\n\\n    def test_reconcile_with_write_off(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "sysadminmatmoz/OCB", "ref": "refs/heads/9.0", "path": "addons/account/tests/test_manual_reconciliation.py", "content": "from openerp.addons.account.tests.account_test_classes import AccountingTestCase\\n\\nclass TestManualReconciliation(AccountingTestCase):\\n\\n    def test_reconciliation_proposition(self):\\n        pass\\n\\n    def test_full_reconcile(self):\\n        pass\\n\\n    def test_partial_reconcile(self):\\n        pass\\n\\n    def test_reconcile_with_write_off(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "angelapper/odoo", "ref": "refs/heads/9.0", "path": "addons/account/tests/test_manual_reconciliation.py", "content": "from openerp.addons.account.tests.account_test_classes import AccountingTestCase\\n\\nclass TestManualReconciliation(AccountingTestCase):\\n\\n    def test_reconciliation_proposition(self):\\n        pass\\n\\n    def test_full_reconcile(self):\\n        pass\\n\\n    def test_partial_reconcile(self):\\n        pass\\n\\n    def test_reconcile_with_write_off(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "AyoubZahid/odoo", "ref": "refs/heads/9.0", "path": "addons/account/tests/test_manual_reconciliation.py", "content": "from openerp.addons.account.tests.account_test_classes import AccountingTestCase\\n\\nclass TestManualReconciliation(AccountingTestCase):\\n\\n    def test_reconciliation_proposition(self):\\n        pass\\n\\n    def test_full_reconcile(self):\\n        pass\\n\\n    def test_partial_reconcile(self):\\n        pass\\n\\n    def test_reconcile_with_write_off(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "laslabs/odoo", "ref": "refs/heads/9.0", "path": "addons/account/tests/test_manual_reconciliation.py", "content": "from openerp.addons.account.tests.account_test_classes import AccountingTestCase\\n\\nclass TestManualReconciliation(AccountingTestCase):\\n\\n    def test_reconciliation_proposition(self):\\n        pass\\n\\n    def test_full_reconcile(self):\\n        pass\\n\\n    def test_partial_reconcile(self):\\n        pass\\n\\n    def test_reconcile_with_write_off(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "OCA/server-tools", "ref": "refs/heads/12.0", "path": "company_country/migrations/12.0.1.0.2/pre-migration.py", "content": "import logging\\nfrom psycopg2.extensions import AsIs\\n\\nfrom odoo import tools\\n\\n\\n_logger = logging.getLogger(__name__)\\n\\n\\ndef migrate(cr, version):\\n    drop_table_model_company_country(cr)\\n\\n\\ndef drop_table_model_company_country(cr):\\n    tablename = \'company_country_config_settings\'\\n    if tools.table_exists(cr, tablename):\\n        _logger.info(\\"Dropping table %s\\", tablename)\\n        cr.execute(\\"DROP TABLE IF EXISTS %s;\\", (AsIs(tablename),))\\n" }\n'
line: b'{ "repo_name": "Fl0rianFischer/sme_odoo", "ref": "refs/heads/9.0", "path": "addons/account/tests/test_manual_reconciliation.py", "content": "from openerp.addons.account.tests.account_test_classes import AccountingTestCase\\n\\nclass TestManualReconciliation(AccountingTestCase):\\n\\n    def test_reconciliation_proposition(self):\\n        pass\\n\\n    def test_full_reconcile(self):\\n        pass\\n\\n    def test_partial_reconcile(self):\\n        pass\\n\\n    def test_reconcile_with_write_off(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "be-cloud-be/horizon-addons", "ref": "refs/heads/9.0", "path": "server/addons/account/tests/test_manual_reconciliation.py", "content": "from openerp.addons.account.tests.account_test_classes import AccountingTestCase\\n\\nclass TestManualReconciliation(AccountingTestCase):\\n\\n    def test_reconciliation_proposition(self):\\n        pass\\n\\n    def test_full_reconcile(self):\\n        pass\\n\\n    def test_partial_reconcile(self):\\n        pass\\n\\n    def test_reconcile_with_write_off(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "kmee/PySPED", "ref": "refs/heads/odoo", "path": "pysped/nfe/leiaute/soap_200.py", "content": "# -*- coding: utf-8 -*-\\n#\\n# PySPED - Python libraries to deal with Brazil\'s SPED Project\\n#\\n# Copyright (C) 2010-2012\\n# Copyright (C) Aristides Caldeira <aristides.caldeira at tauga.com.br>\\n#\\n# This program is free software: you can redistribute it and/or modify\\n# it under the terms of the GNU Library General Public License as\\n# published by the Free Software Foundation, either version 2.1 of the\\n# License, or (at your option) any later version.\\n#\\n# This program is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU Library General Public License for more details.\\n#\\n# You should have received a copy of the GNU Library General Public License\\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\\n#\\n# PySPED - Bibliotecas Python para o\\n#          SPED - Sistema P\xc3\xbablico de Escritura\xc3\xa7\xc3\xa3o Digital\\n#\\n# Copyright (C) 2010-2012\\n# Copyright (C) Aristides Caldeira <aristides.caldeira arroba tauga.com.br>\\n#\\n# Este programa \xc3\xa9 um software livre: voc\xc3\xaa pode redistribuir e/ou modificar\\n# este programa sob os termos da licen\xc3\xa7a GNU Library General Public License,\\n# publicada pela Free Software Foundation, em sua vers\xc3\xa3o 2.1 ou, de acordo\\n# com sua op\xc3\xa7\xc3\xa3o, qualquer vers\xc3\xa3o posterior.\\n#\\n# Este programa \xc3\xa9 distribuido na esperan\xc3\xa7a de que venha a ser \xc3\xbatil,\\n# por\xc3\xa9m SEM QUAISQUER GARANTIAS, nem mesmo a garantia impl\xc3\xadcita de\\n# COMERCIABILIDADE ou ADEQUA\xc3\x87\xc3\x83O A UMA FINALIDADE ESPEC\xc3\x8dFICA. Veja a\\n# GNU Library General Public License para mais detalhes.\\n#\\n# Voc\xc3\xaa deve ter recebido uma c\xc3\xb3pia da GNU Library General Public License\\n# juntamente com este programa. Caso esse n\xc3\xa3o seja o caso, acesse:\\n# <http://www.gnu.org/licenses/>\\n#\\n\\nfrom __future__ import division, print_function, unicode_literals\\n\\nfrom pysped.xml_sped import (ABERTURA, TagDecimal, TagInteiro, XMLNFe,\\n                             tira_abertura)\\nimport os\\n\\nDIRNAME = os.path.dirname(__file__)\\n\\n\\nclass NFeCabecMsg(XMLNFe):\\n    def __init__(self):\\n        super(NFeCabecMsg, self).__init__()\\n        self.webservice = \'\'\\n        self.cUF         = TagInteiro(nome=\'cUF\'        , codigo=\'\', raiz=\'//cabecMsg\', tamanho=[2, 2], valor=35)\\n        self.versaoDados = TagDecimal(nome=\'versaoDados\', codigo=\'\', raiz=\'//cabecMsg\', tamanho=[1, 4], valor=\'2.00\')\\n\\n    def get_xml(self):\\n        xml = XMLNFe.get_xml(self)\\n        xml += \'<nfeCabecMsg xmlns=\\"http://www.portalfiscal.inf.br/nfe/wsdl/\' + self.webservice + \'\\">\'\\n        xml += self.cUF.xml\\n        xml += self.versaoDados.xml\\n        xml += \'</nfeCabecMsg>\'\\n        return xml\\n\\n    def set_xml(self, arquivo):\\n        if self._le_xml(arquivo):\\n            self.cUF.xml         = arquivo\\n            self.versaoDados.xml = arquivo\\n\\n        return self.xml\\n\\n    xml = property(get_xml, set_xml)\\n\\n\\nclass NFeDadosMsg(XMLNFe):\\n    def __init__(self):\\n        super(NFeDadosMsg, self).__init__()\\n        self.webservice = \'\'\\n        self.dados = None\\n\\n    def get_xml(self):\\n        xml = XMLNFe.get_xml(self)\\n        xml += \'<nfeDadosMsg xmlns=\\"http://www.portalfiscal.inf.br/nfe/wsdl/\' + self.webservice + \'\\">\'\\n        xml += tira_abertura(self.dados.xml)\\n        xml += \'</nfeDadosMsg>\'\\n        return xml\\n\\n    def set_xml(self, arquivo):\\n        pass\\n\\n    xml = property(get_xml, set_xml)\\n\\n\\nclass SOAPEnvio(XMLNFe):\\n    def __init__(self):\\n        super(SOAPEnvio, self).__init__()\\n        self.webservice = \'\'\\n        self.metodo = \'\'\\n        self.cUF    = None\\n        self.envio  = None\\n        self.nfeCabecMsg = NFeCabecMsg()\\n        self.nfeDadosMsg = NFeDadosMsg()\\n        self._header = {b\'content-type\': b\'application/soap+xml; charset=utf-8\'}\\n        self.soap_action_webservice_e_metodo = False\\n\\n    def get_xml(self):\\n        self.nfeCabecMsg.webservice = self.webservice\\n        self.nfeCabecMsg.cUF.valor = self.cUF\\n        self.nfeCabecMsg.versaoDados.valor = self.envio.versao.valor\\n\\n        self.nfeDadosMsg.webservice = self.webservice\\n        self.nfeDadosMsg.dados = self.envio\\n\\n        if self.soap_action_webservice_e_metodo:\\n            self._header[b\'content-type\'] = b\'application/soap+xml; charset=utf-8; action=\\"http://www.portalfiscal.inf.br/nfe/wsdl/\' + self.webservice.encode(\'utf-8\') + b\'/\' + self.metodo.encode(\'utf-8\') + b\'\\"\'\\n        else:\\n            self._header[b\'content-type\'] = b\'application/soap+xml; charset=utf-8; action=\\"http://www.portalfiscal.inf.br/nfe/wsdl/\' + self.webservice.encode(\'utf-8\') + b\'\\"\'\\n\\n        xml = XMLNFe.get_xml(self)\\n        xml += ABERTURA\\n        xml += \'<soap:Envelope xmlns:soap=\\"http://www.w3.org/2003/05/soap-envelope\\">\'\\n        xml +=     \'<soap:Header>\'\\n        xml +=             self.nfeCabecMsg.xml\\n        xml +=     \'</soap:Header>\'\\n        xml +=     \'<soap:Body>\'\\n        xml +=             self.nfeDadosMsg.xml\\n        xml +=     \'</soap:Body>\'\\n        xml += \'</soap:Envelope>\'\\n        return xml\\n\\n    def set_xml(self):\\n        pass\\n\\n    xml = property(get_xml, set_xml)\\n\\n    def get_header(self):\\n        header = self._header\\n        return header\\n\\n    header = property(get_header)\\n\\n\\nclass SOAPRetorno(XMLNFe):\\n    def __init__(self):\\n        super(SOAPRetorno, self).__init__()\\n        self.webservice = \'\'\\n        self.metodo = \'\'\\n        self.nfeCabecMsg = NFeCabecMsg()\\n        self.resposta = None\\n\\n    def get_xml(self):\\n        xml = XMLNFe.get_xml(self)\\n        xml += ABERTURA\\n        xml += \'<soap:Envelope xmlns:soap=\\"http://www.w3.org/2003/05/soap-envelope\\">\'\\n        xml +=     \'<soap:Header>\'\\n        xml +=         \'<nfeCabecMsg xmlns=\\"http://www.portalfiscal.inf.br/nfe/wsdl/\' + self.webservice + \'\\">\'\\n        xml +=             self.nfeCabecMsg.xml\\n        xml +=         \'</nfeCabecMsg>\'\\n        xml +=     \'</soap:Header>\'\\n        xml +=     \'<soap:Body>\'\\n        xml +=         \'<\' + self.metodo + \'Result xmlns=\\"http://www.portalfiscal.inf.br/nfe/wsdl/\' + self.webservice + \'\\">\'\\n        xml +=             self.resposta.xml\\n        xml +=         \'</\' + self.metodo + \'Result>\'\\n        xml +=     \'</soap:Body>\'\\n        xml += \'</soap:Envelope>\'\\n        return xml\\n\\n    def set_xml(self, arquivo):\\n        if self._le_xml(arquivo):\\n            self.nfeCabecMsg.xml = arquivo\\n            self.resposta.xml = arquivo\\n\\n    xml = property(get_xml, set_xml)\\n" }\n'
line: b'{ "repo_name": "syci/OCB", "ref": "refs/heads/9.0", "path": "addons/account/tests/test_manual_reconciliation.py", "content": "from openerp.addons.account.tests.account_test_classes import AccountingTestCase\\n\\nclass TestManualReconciliation(AccountingTestCase):\\n\\n    def test_reconciliation_proposition(self):\\n        pass\\n\\n    def test_full_reconcile(self):\\n        pass\\n\\n    def test_partial_reconcile(self):\\n        pass\\n\\n    def test_reconcile_with_write_off(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "sourcefabric/airtime", "ref": "refs/heads/2.5.x", "path": "python_apps/media-monitor/mm2/tests/test_eventcontractor.py", "content": "import unittest\\nfrom media.monitor.eventcontractor import EventContractor\\n#from media.monitor.exceptions import BadSongFile\\nfrom media.monitor.events import FakePyinotify, NewFile, MoveFile, \\\\\\nDeleteFile\\n\\nclass TestMMP(unittest.TestCase):\\n    def test_event_registered(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = MoveFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        ev.register(e1)\\n        self.assertTrue( ev.event_registered(e2) )\\n\\n    def test_get_old_event(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = MoveFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        ev.register(e1)\\n        self.assertEqual( ev.get_old_event(e2), e1 )\\n\\n    def test_register(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = DeleteFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        self.assertTrue( ev.register(e1) )\\n\\n        self.assertFalse( ev.register(e2) )\\n\\n        self.assertEqual( len(ev.store.keys()), 1 )\\n\\n        delete_ev = e1.safe_pack()[0]\\n        self.assertEqual( delete_ev[\'mode\'], u\'delete\')\\n        self.assertEqual( len(ev.store.keys()), 0 )\\n\\n        e3 = DeleteFile( FakePyinotify(\'horse.mp3\') ).proxify()\\n        self.assertTrue( ev.register(e3) )\\n        self.assertTrue( ev.register(e2) )\\n\\n\\n    def test_register2(self):\\n        ev = EventContractor()\\n        p = \'bull.mp3\'\\n        events = [\\n                NewFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ),\\n                DeleteFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ), ]\\n        events = map(lambda x: x.proxify(), events)\\n        actual_events = []\\n        for e in events:\\n            if ev.register(e):\\n                actual_events.append(e)\\n        self.assertEqual( len(ev.store.keys()), 1 )\\n        #packed = [ x.safe_pack() for x in actual_events ]\\n\\nif __name__ == \'__main__\': unittest.main()\\n" }\n'
line: b'{ "repo_name": "radiorabe/airtime", "ref": "refs/heads/rabe", "path": "python_apps/media-monitor/mm2/tests/test_eventcontractor.py", "content": "import unittest\\nfrom media.monitor.eventcontractor import EventContractor\\n#from media.monitor.exceptions import BadSongFile\\nfrom media.monitor.events import FakePyinotify, NewFile, MoveFile, \\\\\\nDeleteFile\\n\\nclass TestMMP(unittest.TestCase):\\n    def test_event_registered(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = MoveFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        ev.register(e1)\\n        self.assertTrue( ev.event_registered(e2) )\\n\\n    def test_get_old_event(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = MoveFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        ev.register(e1)\\n        self.assertEqual( ev.get_old_event(e2), e1 )\\n\\n    def test_register(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = DeleteFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        self.assertTrue( ev.register(e1) )\\n\\n        self.assertFalse( ev.register(e2) )\\n\\n        self.assertEqual( len(ev.store.keys()), 1 )\\n\\n        delete_ev = e1.safe_pack()[0]\\n        self.assertEqual( delete_ev[\'mode\'], u\'delete\')\\n        self.assertEqual( len(ev.store.keys()), 0 )\\n\\n        e3 = DeleteFile( FakePyinotify(\'horse.mp3\') ).proxify()\\n        self.assertTrue( ev.register(e3) )\\n        self.assertTrue( ev.register(e2) )\\n\\n\\n    def test_register2(self):\\n        ev = EventContractor()\\n        p = \'bull.mp3\'\\n        events = [\\n                NewFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ),\\n                DeleteFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ), ]\\n        events = map(lambda x: x.proxify(), events)\\n        actual_events = []\\n        for e in events:\\n            if ev.register(e):\\n                actual_events.append(e)\\n        self.assertEqual( len(ev.store.keys()), 1 )\\n        #packed = [ x.safe_pack() for x in actual_events ]\\n\\nif __name__ == \'__main__\': unittest.main()\\n" }\n'
line: b'{ "repo_name": "vdeluca/tfi", "ref": "refs/heads/tfi", "path": "geonode/tasks/tests.py", "content": "from django.test import TestCase\\n\\n\\nclass TasksTests(TestCase):\\n    pass\\n" }\n'
line: b'{ "repo_name": "slabanja/ase", "ref": "refs/heads/lammps", "path": "doc/tutorials/spacegroup/spacegroup-sg2.py", "content": "print sg\\n" }\n'
line: b'{ "repo_name": "sourcefabric/Airtime", "ref": "refs/heads/2.5.x", "path": "python_apps/media-monitor/mm2/tests/test_eventcontractor.py", "content": "import unittest\\nfrom media.monitor.eventcontractor import EventContractor\\n#from media.monitor.exceptions import BadSongFile\\nfrom media.monitor.events import FakePyinotify, NewFile, MoveFile, \\\\\\nDeleteFile\\n\\nclass TestMMP(unittest.TestCase):\\n    def test_event_registered(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = MoveFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        ev.register(e1)\\n        self.assertTrue( ev.event_registered(e2) )\\n\\n    def test_get_old_event(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = MoveFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        ev.register(e1)\\n        self.assertEqual( ev.get_old_event(e2), e1 )\\n\\n    def test_register(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = DeleteFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        self.assertTrue( ev.register(e1) )\\n\\n        self.assertFalse( ev.register(e2) )\\n\\n        self.assertEqual( len(ev.store.keys()), 1 )\\n\\n        delete_ev = e1.safe_pack()[0]\\n        self.assertEqual( delete_ev[\'mode\'], u\'delete\')\\n        self.assertEqual( len(ev.store.keys()), 0 )\\n\\n        e3 = DeleteFile( FakePyinotify(\'horse.mp3\') ).proxify()\\n        self.assertTrue( ev.register(e3) )\\n        self.assertTrue( ev.register(e2) )\\n\\n\\n    def test_register2(self):\\n        ev = EventContractor()\\n        p = \'bull.mp3\'\\n        events = [\\n                NewFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ),\\n                DeleteFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ), ]\\n        events = map(lambda x: x.proxify(), events)\\n        actual_events = []\\n        for e in events:\\n            if ev.register(e):\\n                actual_events.append(e)\\n        self.assertEqual( len(ev.store.keys()), 1 )\\n        #packed = [ x.safe_pack() for x in actual_events ]\\n\\nif __name__ == \'__main__\': unittest.main()\\n" }\n'
line: b'{ "repo_name": "Ryex/airtime", "ref": "refs/heads/ktek-2.6.x", "path": "python_apps/media-monitor/mm2/tests/test_eventcontractor.py", "content": "import unittest\\nfrom media.monitor.eventcontractor import EventContractor\\n#from media.monitor.exceptions import BadSongFile\\nfrom media.monitor.events import FakePyinotify, NewFile, MoveFile, \\\\\\nDeleteFile\\n\\nclass TestMMP(unittest.TestCase):\\n    def test_event_registered(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = MoveFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        ev.register(e1)\\n        self.assertTrue( ev.event_registered(e2) )\\n\\n    def test_get_old_event(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = MoveFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        ev.register(e1)\\n        self.assertEqual( ev.get_old_event(e2), e1 )\\n\\n    def test_register(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = DeleteFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        self.assertTrue( ev.register(e1) )\\n\\n        self.assertFalse( ev.register(e2) )\\n\\n        self.assertEqual( len(ev.store.keys()), 1 )\\n\\n        delete_ev = e1.safe_pack()[0]\\n        self.assertEqual( delete_ev[\'mode\'], u\'delete\')\\n        self.assertEqual( len(ev.store.keys()), 0 )\\n\\n        e3 = DeleteFile( FakePyinotify(\'horse.mp3\') ).proxify()\\n        self.assertTrue( ev.register(e3) )\\n        self.assertTrue( ev.register(e2) )\\n\\n\\n    def test_register2(self):\\n        ev = EventContractor()\\n        p = \'bull.mp3\'\\n        events = [\\n                NewFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ),\\n                DeleteFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ), ]\\n        events = map(lambda x: x.proxify(), events)\\n        actual_events = []\\n        for e in events:\\n            if ev.register(e):\\n                actual_events.append(e)\\n        self.assertEqual( len(ev.store.keys()), 1 )\\n        #packed = [ x.safe_pack() for x in actual_events ]\\n\\nif __name__ == \'__main__\': unittest.main()\\n" }\n'
line: b'{ "repo_name": "habeanf/Open-Knesset", "ref": "refs/heads/upmaster", "path": "okhelptexts/migrations/0002_auto__add_field_helptext_moreinfo.py", "content": "# encoding: utf-8\\nimport datetime\\nfrom south.db import db\\nfrom south.v2 import SchemaMigration\\nfrom django.db import models\\n\\nclass Migration(SchemaMigration):\\n\\n    def forwards(self, orm):\\n        \\n        # Adding field \'Helptext.moreinfo\'\\n        db.add_column(\'okhelptexts_helptext\', \'moreinfo\', self.gf(\'django.db.models.fields.CharField\')(default=\'\', max_length=200, blank=True), keep_default=False)\\n\\n\\n    def backwards(self, orm):\\n        \\n        # Deleting field \'Helptext.moreinfo\'\\n        db.delete_column(\'okhelptexts_helptext\', \'moreinfo\')\\n\\n\\n    models = {\\n        \'okhelptexts.helptext\': {\\n            \'Meta\': {\'object_name\': \'Helptext\'}\\n            \'fulltext\': (\'django.db.models.fields.TextField\', [], {}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'moreinfo\': (\'django.db.models.fields.CharField\', [], {\'default\': \\"\'\'\\", \'max_length\': \'200\', \'blank\': \'True\'})\\n      }\\n        \'okhelptexts.keyword\': {\\n            \'Meta\': {\'object_name\': \'Keyword\'}\\n            \'helptext\': (\'django.db.models.fields.related.ForeignKey\', [], {\'to\': \\"orm[\'okhelptexts.Helptext\']\\"}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'kw_text\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'200\'})\\n      }\\n  }\\n\\n    complete_apps = [\'okhelptexts\']\\n" }\n'
line: b'{ "repo_name": "justvanbloom/airtime", "ref": "refs/heads/2.5.x", "path": "python_apps/media-monitor/mm2/tests/test_eventcontractor.py", "content": "import unittest\\nfrom media.monitor.eventcontractor import EventContractor\\n#from media.monitor.exceptions import BadSongFile\\nfrom media.monitor.events import FakePyinotify, NewFile, MoveFile, \\\\\\nDeleteFile\\n\\nclass TestMMP(unittest.TestCase):\\n    def test_event_registered(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = MoveFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        ev.register(e1)\\n        self.assertTrue( ev.event_registered(e2) )\\n\\n    def test_get_old_event(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = MoveFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        ev.register(e1)\\n        self.assertEqual( ev.get_old_event(e2), e1 )\\n\\n    def test_register(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = DeleteFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        self.assertTrue( ev.register(e1) )\\n\\n        self.assertFalse( ev.register(e2) )\\n\\n        self.assertEqual( len(ev.store.keys()), 1 )\\n\\n        delete_ev = e1.safe_pack()[0]\\n        self.assertEqual( delete_ev[\'mode\'], u\'delete\')\\n        self.assertEqual( len(ev.store.keys()), 0 )\\n\\n        e3 = DeleteFile( FakePyinotify(\'horse.mp3\') ).proxify()\\n        self.assertTrue( ev.register(e3) )\\n        self.assertTrue( ev.register(e2) )\\n\\n\\n    def test_register2(self):\\n        ev = EventContractor()\\n        p = \'bull.mp3\'\\n        events = [\\n                NewFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ),\\n                DeleteFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ), ]\\n        events = map(lambda x: x.proxify(), events)\\n        actual_events = []\\n        for e in events:\\n            if ev.register(e):\\n                actual_events.append(e)\\n        self.assertEqual( len(ev.store.keys()), 1 )\\n        #packed = [ x.safe_pack() for x in actual_events ]\\n\\nif __name__ == \'__main__\': unittest.main()\\n" }\n'
line: b'{ "repo_name": "ReganDryke/airtime", "ref": "refs/heads/2.5.x", "path": "python_apps/media-monitor/mm2/tests/test_eventcontractor.py", "content": "import unittest\\nfrom media.monitor.eventcontractor import EventContractor\\n#from media.monitor.exceptions import BadSongFile\\nfrom media.monitor.events import FakePyinotify, NewFile, MoveFile, \\\\\\nDeleteFile\\n\\nclass TestMMP(unittest.TestCase):\\n    def test_event_registered(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = MoveFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        ev.register(e1)\\n        self.assertTrue( ev.event_registered(e2) )\\n\\n    def test_get_old_event(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = MoveFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        ev.register(e1)\\n        self.assertEqual( ev.get_old_event(e2), e1 )\\n\\n    def test_register(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = DeleteFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        self.assertTrue( ev.register(e1) )\\n\\n        self.assertFalse( ev.register(e2) )\\n\\n        self.assertEqual( len(ev.store.keys()), 1 )\\n\\n        delete_ev = e1.safe_pack()[0]\\n        self.assertEqual( delete_ev[\'mode\'], u\'delete\')\\n        self.assertEqual( len(ev.store.keys()), 0 )\\n\\n        e3 = DeleteFile( FakePyinotify(\'horse.mp3\') ).proxify()\\n        self.assertTrue( ev.register(e3) )\\n        self.assertTrue( ev.register(e2) )\\n\\n\\n    def test_register2(self):\\n        ev = EventContractor()\\n        p = \'bull.mp3\'\\n        events = [\\n                NewFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ),\\n                DeleteFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ), ]\\n        events = map(lambda x: x.proxify(), events)\\n        actual_events = []\\n        for e in events:\\n            if ev.register(e):\\n                actual_events.append(e)\\n        self.assertEqual( len(ev.store.keys()), 1 )\\n        #packed = [ x.safe_pack() for x in actual_events ]\\n\\nif __name__ == \'__main__\': unittest.main()\\n" }\n'
line: b'{ "repo_name": "XKNX/xknx", "ref": "refs/heads/main", "path": "xknx/io/const.py", "content": "\\"\\"\\"KNX Constants used within io.\\"\\"\\"\\n\\nDEFAULT_MCAST_GRP = \\"224.0.23.12\\"\\nDEFAULT_MCAST_PORT = 3671\\n\\nCONNECTION_ALIVE_TIME = 120\\nCONNECTIONSTATE_REQUEST_TIMEOUT = 10\\nHEARTBEAT_RATE = CONNECTION_ALIVE_TIME - (CONNECTIONSTATE_REQUEST_TIMEOUT * 5)\\n" }\n'
line: b'{ "repo_name": "mkotsbak/ModemManager", "ref": "refs/heads/Samsung_LTE_support", "path": "test/disable.py", "content": "#!/usr/bin/python\\n# -*- Mode: python; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-\\n#\\n# This program is free software; you can redistribute it and/or modify\\n# it under the terms of the GNU General Public License as published by\\n# the Free Software Foundation; either version 2 of the License, or\\n# (at your option) any later version.\\n#\\n# This program is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU General Public License for more details:\\n#\\n# Copyright (C) 2009 - 2010 Red Hat, Inc.\\n#\\n\\nimport sys, dbus\\n\\nMM_DBUS_SERVICE=\'org.freedesktop.ModemManager\'\\nMM_DBUS_PATH=\'/org/freedesktop/ModemManager\'\\nMM_DBUS_INTERFACE_MODEM=\'org.freedesktop.ModemManager.Modem\'\\n\\nbus = dbus.SystemBus()\\nproxy = bus.get_object(MM_DBUS_SERVICE, sys.argv[1])\\nmodem = dbus.Interface(proxy, dbus_interface=MM_DBUS_INTERFACE_MODEM)\\nmodem.Enable (False)\\n\\n" }\n'
line: b'{ "repo_name": "thnkloud9/Airtime", "ref": "refs/heads/2.5.x", "path": "python_apps/media-monitor/mm2/tests/test_eventcontractor.py", "content": "import unittest\\nfrom media.monitor.eventcontractor import EventContractor\\n#from media.monitor.exceptions import BadSongFile\\nfrom media.monitor.events import FakePyinotify, NewFile, MoveFile, \\\\\\nDeleteFile\\n\\nclass TestMMP(unittest.TestCase):\\n    def test_event_registered(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = MoveFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        ev.register(e1)\\n        self.assertTrue( ev.event_registered(e2) )\\n\\n    def test_get_old_event(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = MoveFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        ev.register(e1)\\n        self.assertEqual( ev.get_old_event(e2), e1 )\\n\\n    def test_register(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = DeleteFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        self.assertTrue( ev.register(e1) )\\n\\n        self.assertFalse( ev.register(e2) )\\n\\n        self.assertEqual( len(ev.store.keys()), 1 )\\n\\n        delete_ev = e1.safe_pack()[0]\\n        self.assertEqual( delete_ev[\'mode\'], u\'delete\')\\n        self.assertEqual( len(ev.store.keys()), 0 )\\n\\n        e3 = DeleteFile( FakePyinotify(\'horse.mp3\') ).proxify()\\n        self.assertTrue( ev.register(e3) )\\n        self.assertTrue( ev.register(e2) )\\n\\n\\n    def test_register2(self):\\n        ev = EventContractor()\\n        p = \'bull.mp3\'\\n        events = [\\n                NewFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ),\\n                DeleteFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ), ]\\n        events = map(lambda x: x.proxify(), events)\\n        actual_events = []\\n        for e in events:\\n            if ev.register(e):\\n                actual_events.append(e)\\n        self.assertEqual( len(ev.store.keys()), 1 )\\n        #packed = [ x.safe_pack() for x in actual_events ]\\n\\nif __name__ == \'__main__\': unittest.main()\\n" }\n'
line: b'{ "repo_name": "comiconomenclaturist/Airtime", "ref": "refs/heads/2.5.x", "path": "python_apps/media-monitor/mm2/tests/test_eventcontractor.py", "content": "import unittest\\nfrom media.monitor.eventcontractor import EventContractor\\n#from media.monitor.exceptions import BadSongFile\\nfrom media.monitor.events import FakePyinotify, NewFile, MoveFile, \\\\\\nDeleteFile\\n\\nclass TestMMP(unittest.TestCase):\\n    def test_event_registered(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = MoveFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        ev.register(e1)\\n        self.assertTrue( ev.event_registered(e2) )\\n\\n    def test_get_old_event(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = MoveFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        ev.register(e1)\\n        self.assertEqual( ev.get_old_event(e2), e1 )\\n\\n    def test_register(self):\\n        ev = EventContractor()\\n        e1 = NewFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        e2 = DeleteFile( FakePyinotify(\'bull.mp3\') ).proxify()\\n        self.assertTrue( ev.register(e1) )\\n\\n        self.assertFalse( ev.register(e2) )\\n\\n        self.assertEqual( len(ev.store.keys()), 1 )\\n\\n        delete_ev = e1.safe_pack()[0]\\n        self.assertEqual( delete_ev[\'mode\'], u\'delete\')\\n        self.assertEqual( len(ev.store.keys()), 0 )\\n\\n        e3 = DeleteFile( FakePyinotify(\'horse.mp3\') ).proxify()\\n        self.assertTrue( ev.register(e3) )\\n        self.assertTrue( ev.register(e2) )\\n\\n\\n    def test_register2(self):\\n        ev = EventContractor()\\n        p = \'bull.mp3\'\\n        events = [\\n                NewFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ),\\n                DeleteFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ),\\n                NewFile( FakePyinotify(p) ), ]\\n        events = map(lambda x: x.proxify(), events)\\n        actual_events = []\\n        for e in events:\\n            if ev.register(e):\\n                actual_events.append(e)\\n        self.assertEqual( len(ev.store.keys()), 1 )\\n        #packed = [ x.safe_pack() for x in actual_events ]\\n\\nif __name__ == \'__main__\': unittest.main()\\n" }\n'
line: b'{ "repo_name": "named-data-ndnSIM/ns-3-dev", "ref": "refs/heads/ndnSIM-ns-3.29", "path": "src/lte/bindings/modulegen__gcc_ILP32.py", "content": null }\n'
line: b'{ "repo_name": "allenp/odoo", "ref": "refs/heads/9.0", "path": "addons/hw_proxy/controllers/main.py", "content": "# -*- coding: utf-8 -*-\\nimport logging\\nimport commands\\nimport json\\nimport os\\nimport os.path\\nimport openerp\\nimport time\\nimport random\\nimport subprocess\\nimport json\\nimport werkzeug\\nimport werkzeug.wrappers\\n_logger = logging.getLogger(__name__)\\n\\n\\nfrom openerp import http\\nfrom openerp.http import request\\n\\n# Those are the builtin raspberry pi USB modules, they should\\n# not appear in the list of connected devices.\\nBANNED_DEVICES = set([\\n\\t\\"0424:9514\\",\\t# Standard Microsystem Corp. Builtin Ethernet module\\n\\t\\"1d6b:0002\\",\\t# Linux Foundation 2.0 root hub\\n\\t\\"0424:ec00\\",\\t# Standard Microsystem Corp. Other Builtin Ethernet module\\n])\\n\\n\\n# drivers modules must add to drivers an object with a get_status() method \\n# so that \'status\' can return the status of all active drivers\\ndrivers = {}\\n\\nclass Proxy(http.Controller):\\n\\n    def get_status(self):\\n        statuses = {}\\n        for driver in drivers:\\n            statuses[driver] = drivers[driver].get_status()\\n        return statuses\\n\\n    @http.route(\'/hw_proxy/hello\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def hello(self):\\n        return \\"ping\\"\\n\\n    @http.route(\'/hw_proxy/handshake\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def handshake(self):\\n        return True\\n\\n    @http.route(\'/hw_proxy/status\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def status_http(self):\\n        resp = \\"\\"\\"\\n<!DOCTYPE HTML>\\n<html>\\n    <head>\\n        <title>Odoo\'s PosBox</title>\\n        <style>\\n        body {\\n            width: 480px;\\n            margin: 60px auto;\\n            font-family: sans-serif;\\n            text-align: justify;\\n            color: #6B6B6B;\\n      }\\n        .device {\\n            border-bottom: solid 1px rgb(216,216,216);\\n            padding: 9px;\\n      }\\n        .device:nth-child(2n) {\\n            background:rgb(240,240,240);\\n      }\\n        </style>\\n    </head>\\n    <body>\\n        <h1>Hardware Status</h1>\\n        <p>The list of enabled drivers and their status</p>\\n\\"\\"\\"\\n        statuses = self.get_status()\\n        for driver in statuses:\\n\\n            status = statuses[driver]\\n\\n            if status[\'status\'] == \'connecting\':\\n                color = \'black\'\\n            elif status[\'status\'] == \'connected\':\\n                color = \'green\'\\n            else:\\n                color = \'red\'\\n\\n            resp += \\"<h3 style=\'color:\\"+color+\\";\'>\\"+driver+\' : \'+status[\'status\']+\\"</h3>\\\\n\\"\\n            resp += \\"<ul>\\\\n\\"\\n            for msg in status[\'messages\']:\\n                resp += \'<li>\'+msg+\'</li>\\\\n\'\\n            resp += \\"</ul>\\\\n\\"\\n        resp += \\"\\"\\"\\n            <h2>Connected Devices</h2>\\n            <p>The list of connected USB devices as seen by the posbox</p>\\n        \\"\\"\\"\\n        devices = commands.getoutput(\\"lsusb\\").split(\'\\\\n\')\\n        count   = 0\\n        resp += \\"<div class=\'devices\'>\\\\n\\"\\n        for device in devices:\\n            device_name = device[device.find(\'ID\')+2:]\\n            device_id   = device_name.split()[0]\\n            if not (device_id in BANNED_DEVICES):\\n            \\tresp+= \\"<div class=\'device\' data-device=\'\\"+device+\\"\'>\\"+device_name+\\"</div>\\\\n\\"\\n                count += 1\\n        \\n        if count == 0:\\n            resp += \\"<div class=\'device\'>No USB Device Found</div>\\"\\n\\n        resp += \\"</div>\\\\n</body>\\\\n</html>\\\\n\\\\n\\"\\n\\n        return request.make_response(resp,{\\n            \'Cache-Control\': \'no-cache\', \\n            \'Content-Type\': \'text/html; charset=utf-8\',\\n            \'Access-Control-Allow-Origin\':  \'*\',\\n            \'Access-Control-Allow-Methods\': \'GET\',\\n          })\\n\\n    @http.route(\'/hw_proxy/status_json\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def status_json(self):\\n        return self.get_status()\\n\\n    @http.route(\'/hw_proxy/scan_item_success\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_success(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned with success\\n        \\"\\"\\"\\n        print \'scan_item_success: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/scan_item_error_unrecognized\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_error_unrecognized(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned without success\\n        \\"\\"\\"\\n        print \'scan_item_error_unrecognized: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/help_needed\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_needed(self):\\n        \\"\\"\\"\\n        The user wants an help (ex: light is on)\\n        \\"\\"\\"\\n        print \\"help_needed\\"\\n\\n    @http.route(\'/hw_proxy/help_canceled\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_canceled(self):\\n        \\"\\"\\"\\n        The user stops the help request\\n        \\"\\"\\"\\n        print \\"help_canceled\\"\\n\\n    @http.route(\'/hw_proxy/payment_request\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_request(self, price):\\n        \\"\\"\\"\\n        The PoS will activate the method payment \\n        \\"\\"\\"\\n        print \\"payment_request: price:\\"+str(price)\\n        return \'ok\'\\n\\n    @http.route(\'/hw_proxy/payment_status\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_status(self):\\n        print \\"payment_status\\"\\n        return { \'status\':\'waiting\' } \\n\\n    @http.route(\'/hw_proxy/payment_cancel\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_cancel(self):\\n        print \\"payment_cancel\\"\\n\\n    @http.route(\'/hw_proxy/transaction_start\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_start(self):\\n        print \'transaction_start\'\\n\\n    @http.route(\'/hw_proxy/transaction_end\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_end(self):\\n        print \'transaction_end\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_activated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_activated(self):\\n        print \'cashier_mode_activated\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_deactivated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_deactivated(self):\\n        print \'cashier_mode_deactivated\'\\n\\n    @http.route(\'/hw_proxy/open_cashbox\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def open_cashbox(self):\\n        print \'open_cashbox\'\\n\\n    @http.route(\'/hw_proxy/print_receipt\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_receipt(self, receipt):\\n        print \'print_receipt\' + str(receipt)\\n\\n    @http.route(\'/hw_proxy/is_scanner_connected\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def is_scanner_connected(self, receipt):\\n        print \'is_scanner_connected?\' \\n        return False\\n\\n    @http.route(\'/hw_proxy/scanner\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scanner(self, receipt):\\n        print \'scanner\' \\n        time.sleep(10)\\n        return \'\'\\n\\n    @http.route(\'/hw_proxy/log\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def log(self, arguments):\\n        _logger.info(\' \'.join(str(v) for v in arguments))\\n\\n    @http.route(\'/hw_proxy/print_pdf_invoice\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_pdf_invoice(self, pdfinvoice):\\n        print \'print_pdf_invoice\' + str(pdfinvoice)\\n" }\n'
line: b'{ "repo_name": "odoobgorg/odoo", "ref": "refs/heads/9.0", "path": "addons/hw_proxy/controllers/main.py", "content": "# -*- coding: utf-8 -*-\\nimport logging\\nimport commands\\nimport json\\nimport os\\nimport os.path\\nimport openerp\\nimport time\\nimport random\\nimport subprocess\\nimport json\\nimport werkzeug\\nimport werkzeug.wrappers\\n_logger = logging.getLogger(__name__)\\n\\n\\nfrom openerp import http\\nfrom openerp.http import request\\n\\n# Those are the builtin raspberry pi USB modules, they should\\n# not appear in the list of connected devices.\\nBANNED_DEVICES = set([\\n\\t\\"0424:9514\\",\\t# Standard Microsystem Corp. Builtin Ethernet module\\n\\t\\"1d6b:0002\\",\\t# Linux Foundation 2.0 root hub\\n\\t\\"0424:ec00\\",\\t# Standard Microsystem Corp. Other Builtin Ethernet module\\n])\\n\\n\\n# drivers modules must add to drivers an object with a get_status() method \\n# so that \'status\' can return the status of all active drivers\\ndrivers = {}\\n\\nclass Proxy(http.Controller):\\n\\n    def get_status(self):\\n        statuses = {}\\n        for driver in drivers:\\n            statuses[driver] = drivers[driver].get_status()\\n        return statuses\\n\\n    @http.route(\'/hw_proxy/hello\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def hello(self):\\n        return \\"ping\\"\\n\\n    @http.route(\'/hw_proxy/handshake\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def handshake(self):\\n        return True\\n\\n    @http.route(\'/hw_proxy/status\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def status_http(self):\\n        resp = \\"\\"\\"\\n<!DOCTYPE HTML>\\n<html>\\n    <head>\\n        <title>Odoo\'s PosBox</title>\\n        <style>\\n        body {\\n            width: 480px;\\n            margin: 60px auto;\\n            font-family: sans-serif;\\n            text-align: justify;\\n            color: #6B6B6B;\\n      }\\n        .device {\\n            border-bottom: solid 1px rgb(216,216,216);\\n            padding: 9px;\\n      }\\n        .device:nth-child(2n) {\\n            background:rgb(240,240,240);\\n      }\\n        </style>\\n    </head>\\n    <body>\\n        <h1>Hardware Status</h1>\\n        <p>The list of enabled drivers and their status</p>\\n\\"\\"\\"\\n        statuses = self.get_status()\\n        for driver in statuses:\\n\\n            status = statuses[driver]\\n\\n            if status[\'status\'] == \'connecting\':\\n                color = \'black\'\\n            elif status[\'status\'] == \'connected\':\\n                color = \'green\'\\n            else:\\n                color = \'red\'\\n\\n            resp += \\"<h3 style=\'color:\\"+color+\\";\'>\\"+driver+\' : \'+status[\'status\']+\\"</h3>\\\\n\\"\\n            resp += \\"<ul>\\\\n\\"\\n            for msg in status[\'messages\']:\\n                resp += \'<li>\'+msg+\'</li>\\\\n\'\\n            resp += \\"</ul>\\\\n\\"\\n        resp += \\"\\"\\"\\n            <h2>Connected Devices</h2>\\n            <p>The list of connected USB devices as seen by the posbox</p>\\n        \\"\\"\\"\\n        devices = commands.getoutput(\\"lsusb\\").split(\'\\\\n\')\\n        count   = 0\\n        resp += \\"<div class=\'devices\'>\\\\n\\"\\n        for device in devices:\\n            device_name = device[device.find(\'ID\')+2:]\\n            device_id   = device_name.split()[0]\\n            if not (device_id in BANNED_DEVICES):\\n            \\tresp+= \\"<div class=\'device\' data-device=\'\\"+device+\\"\'>\\"+device_name+\\"</div>\\\\n\\"\\n                count += 1\\n        \\n        if count == 0:\\n            resp += \\"<div class=\'device\'>No USB Device Found</div>\\"\\n\\n        resp += \\"</div>\\\\n</body>\\\\n</html>\\\\n\\\\n\\"\\n\\n        return request.make_response(resp,{\\n            \'Cache-Control\': \'no-cache\', \\n            \'Content-Type\': \'text/html; charset=utf-8\',\\n            \'Access-Control-Allow-Origin\':  \'*\',\\n            \'Access-Control-Allow-Methods\': \'GET\',\\n          })\\n\\n    @http.route(\'/hw_proxy/status_json\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def status_json(self):\\n        return self.get_status()\\n\\n    @http.route(\'/hw_proxy/scan_item_success\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_success(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned with success\\n        \\"\\"\\"\\n        print \'scan_item_success: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/scan_item_error_unrecognized\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_error_unrecognized(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned without success\\n        \\"\\"\\"\\n        print \'scan_item_error_unrecognized: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/help_needed\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_needed(self):\\n        \\"\\"\\"\\n        The user wants an help (ex: light is on)\\n        \\"\\"\\"\\n        print \\"help_needed\\"\\n\\n    @http.route(\'/hw_proxy/help_canceled\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_canceled(self):\\n        \\"\\"\\"\\n        The user stops the help request\\n        \\"\\"\\"\\n        print \\"help_canceled\\"\\n\\n    @http.route(\'/hw_proxy/payment_request\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_request(self, price):\\n        \\"\\"\\"\\n        The PoS will activate the method payment \\n        \\"\\"\\"\\n        print \\"payment_request: price:\\"+str(price)\\n        return \'ok\'\\n\\n    @http.route(\'/hw_proxy/payment_status\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_status(self):\\n        print \\"payment_status\\"\\n        return { \'status\':\'waiting\' } \\n\\n    @http.route(\'/hw_proxy/payment_cancel\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_cancel(self):\\n        print \\"payment_cancel\\"\\n\\n    @http.route(\'/hw_proxy/transaction_start\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_start(self):\\n        print \'transaction_start\'\\n\\n    @http.route(\'/hw_proxy/transaction_end\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_end(self):\\n        print \'transaction_end\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_activated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_activated(self):\\n        print \'cashier_mode_activated\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_deactivated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_deactivated(self):\\n        print \'cashier_mode_deactivated\'\\n\\n    @http.route(\'/hw_proxy/open_cashbox\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def open_cashbox(self):\\n        print \'open_cashbox\'\\n\\n    @http.route(\'/hw_proxy/print_receipt\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_receipt(self, receipt):\\n        print \'print_receipt\' + str(receipt)\\n\\n    @http.route(\'/hw_proxy/is_scanner_connected\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def is_scanner_connected(self, receipt):\\n        print \'is_scanner_connected?\' \\n        return False\\n\\n    @http.route(\'/hw_proxy/scanner\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scanner(self, receipt):\\n        print \'scanner\' \\n        time.sleep(10)\\n        return \'\'\\n\\n    @http.route(\'/hw_proxy/log\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def log(self, arguments):\\n        _logger.info(\' \'.join(str(v) for v in arguments))\\n\\n    @http.route(\'/hw_proxy/print_pdf_invoice\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_pdf_invoice(self, pdfinvoice):\\n        print \'print_pdf_invoice\' + str(pdfinvoice)\\n" }\n'
line: b'{ "repo_name": "stephen144/odoo", "ref": "refs/heads/9.0", "path": "addons/hw_proxy/controllers/main.py", "content": "# -*- coding: utf-8 -*-\\nimport logging\\nimport commands\\nimport json\\nimport os\\nimport os.path\\nimport openerp\\nimport time\\nimport random\\nimport subprocess\\nimport json\\nimport werkzeug\\nimport werkzeug.wrappers\\n_logger = logging.getLogger(__name__)\\n\\n\\nfrom openerp import http\\nfrom openerp.http import request\\n\\n# Those are the builtin raspberry pi USB modules, they should\\n# not appear in the list of connected devices.\\nBANNED_DEVICES = set([\\n\\t\\"0424:9514\\",\\t# Standard Microsystem Corp. Builtin Ethernet module\\n\\t\\"1d6b:0002\\",\\t# Linux Foundation 2.0 root hub\\n\\t\\"0424:ec00\\",\\t# Standard Microsystem Corp. Other Builtin Ethernet module\\n])\\n\\n\\n# drivers modules must add to drivers an object with a get_status() method \\n# so that \'status\' can return the status of all active drivers\\ndrivers = {}\\n\\nclass Proxy(http.Controller):\\n\\n    def get_status(self):\\n        statuses = {}\\n        for driver in drivers:\\n            statuses[driver] = drivers[driver].get_status()\\n        return statuses\\n\\n    @http.route(\'/hw_proxy/hello\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def hello(self):\\n        return \\"ping\\"\\n\\n    @http.route(\'/hw_proxy/handshake\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def handshake(self):\\n        return True\\n\\n    @http.route(\'/hw_proxy/status\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def status_http(self):\\n        resp = \\"\\"\\"\\n<!DOCTYPE HTML>\\n<html>\\n    <head>\\n        <title>Odoo\'s PosBox</title>\\n        <style>\\n        body {\\n            width: 480px;\\n            margin: 60px auto;\\n            font-family: sans-serif;\\n            text-align: justify;\\n            color: #6B6B6B;\\n      }\\n        .device {\\n            border-bottom: solid 1px rgb(216,216,216);\\n            padding: 9px;\\n      }\\n        .device:nth-child(2n) {\\n            background:rgb(240,240,240);\\n      }\\n        </style>\\n    </head>\\n    <body>\\n        <h1>Hardware Status</h1>\\n        <p>The list of enabled drivers and their status</p>\\n\\"\\"\\"\\n        statuses = self.get_status()\\n        for driver in statuses:\\n\\n            status = statuses[driver]\\n\\n            if status[\'status\'] == \'connecting\':\\n                color = \'black\'\\n            elif status[\'status\'] == \'connected\':\\n                color = \'green\'\\n            else:\\n                color = \'red\'\\n\\n            resp += \\"<h3 style=\'color:\\"+color+\\";\'>\\"+driver+\' : \'+status[\'status\']+\\"</h3>\\\\n\\"\\n            resp += \\"<ul>\\\\n\\"\\n            for msg in status[\'messages\']:\\n                resp += \'<li>\'+msg+\'</li>\\\\n\'\\n            resp += \\"</ul>\\\\n\\"\\n        resp += \\"\\"\\"\\n            <h2>Connected Devices</h2>\\n            <p>The list of connected USB devices as seen by the posbox</p>\\n        \\"\\"\\"\\n        devices = commands.getoutput(\\"lsusb\\").split(\'\\\\n\')\\n        count   = 0\\n        resp += \\"<div class=\'devices\'>\\\\n\\"\\n        for device in devices:\\n            device_name = device[device.find(\'ID\')+2:]\\n            device_id   = device_name.split()[0]\\n            if not (device_id in BANNED_DEVICES):\\n            \\tresp+= \\"<div class=\'device\' data-device=\'\\"+device+\\"\'>\\"+device_name+\\"</div>\\\\n\\"\\n                count += 1\\n        \\n        if count == 0:\\n            resp += \\"<div class=\'device\'>No USB Device Found</div>\\"\\n\\n        resp += \\"</div>\\\\n</body>\\\\n</html>\\\\n\\\\n\\"\\n\\n        return request.make_response(resp,{\\n            \'Cache-Control\': \'no-cache\', \\n            \'Content-Type\': \'text/html; charset=utf-8\',\\n            \'Access-Control-Allow-Origin\':  \'*\',\\n            \'Access-Control-Allow-Methods\': \'GET\',\\n          })\\n\\n    @http.route(\'/hw_proxy/status_json\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def status_json(self):\\n        return self.get_status()\\n\\n    @http.route(\'/hw_proxy/scan_item_success\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_success(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned with success\\n        \\"\\"\\"\\n        print \'scan_item_success: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/scan_item_error_unrecognized\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_error_unrecognized(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned without success\\n        \\"\\"\\"\\n        print \'scan_item_error_unrecognized: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/help_needed\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_needed(self):\\n        \\"\\"\\"\\n        The user wants an help (ex: light is on)\\n        \\"\\"\\"\\n        print \\"help_needed\\"\\n\\n    @http.route(\'/hw_proxy/help_canceled\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_canceled(self):\\n        \\"\\"\\"\\n        The user stops the help request\\n        \\"\\"\\"\\n        print \\"help_canceled\\"\\n\\n    @http.route(\'/hw_proxy/payment_request\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_request(self, price):\\n        \\"\\"\\"\\n        The PoS will activate the method payment \\n        \\"\\"\\"\\n        print \\"payment_request: price:\\"+str(price)\\n        return \'ok\'\\n\\n    @http.route(\'/hw_proxy/payment_status\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_status(self):\\n        print \\"payment_status\\"\\n        return { \'status\':\'waiting\' } \\n\\n    @http.route(\'/hw_proxy/payment_cancel\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_cancel(self):\\n        print \\"payment_cancel\\"\\n\\n    @http.route(\'/hw_proxy/transaction_start\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_start(self):\\n        print \'transaction_start\'\\n\\n    @http.route(\'/hw_proxy/transaction_end\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_end(self):\\n        print \'transaction_end\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_activated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_activated(self):\\n        print \'cashier_mode_activated\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_deactivated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_deactivated(self):\\n        print \'cashier_mode_deactivated\'\\n\\n    @http.route(\'/hw_proxy/open_cashbox\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def open_cashbox(self):\\n        print \'open_cashbox\'\\n\\n    @http.route(\'/hw_proxy/print_receipt\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_receipt(self, receipt):\\n        print \'print_receipt\' + str(receipt)\\n\\n    @http.route(\'/hw_proxy/is_scanner_connected\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def is_scanner_connected(self, receipt):\\n        print \'is_scanner_connected?\' \\n        return False\\n\\n    @http.route(\'/hw_proxy/scanner\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scanner(self, receipt):\\n        print \'scanner\' \\n        time.sleep(10)\\n        return \'\'\\n\\n    @http.route(\'/hw_proxy/log\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def log(self, arguments):\\n        _logger.info(\' \'.join(str(v) for v in arguments))\\n\\n    @http.route(\'/hw_proxy/print_pdf_invoice\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_pdf_invoice(self, pdfinvoice):\\n        print \'print_pdf_invoice\' + str(pdfinvoice)\\n" }\n'
line: b'{ "repo_name": "wbond/subversion", "ref": "refs/heads/1.7.x", "path": "contrib/client-side/svn_apply_autoprops.py", "content": "#!/usr/bin/env python\\n\\n# To do:\\n# 1) Switch to using the Subversion Python bindings.\\n#\\n# $HeadURL$\\n# $LastChangedRevision$\\n# $LastChangedDate$\\n# $LastChangedBy$\\n#\\n# Copyright (C) 2005,2006 Blair Zajac <blair@orcaware.com>\\n#\\n# This script is free software; you can redistribute it and/or modify\\n# it under the terms of the GNU General Public License as published by\\n# the Free Software Foundation; either version 2 of the License, or\\n# (at your option) any later version.\\n#\\n# This script is distributed in the hope that it will be useful, but\\n# WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# General Public License for more details.\\n#\\n# A copy of the GNU General Public License can be obtained by writing\\n# to the Free Software Foundation, Inc., 59 Temple Place, Suite 330,\\n# Boston, MA 02111-1307 USA.\\n\\nimport getopt\\nimport fnmatch\\nimport os\\nimport re\\nimport sys\\n\\n# The default path to the Subversion configuration file.\\nSVN_CONFIG_FILENAME = os.path.expandvars(\'$HOME/.subversion/config\')\\n\\n# The name of Subversion\'s private directory in working copies.\\nSVN_WC_ADM_DIR_NAME = \'.svn\'\\n\\n# The name this script was invoked as.\\nPROGNAME = os.path.basename(sys.argv[0])\\n\\ndef usage():\\n  print(\\"\\"\\"This script reads the auto-properties defined in the file\\n\'%s\'\\nand applies them recursively to all the files and directories in the\\ncurrent working copy.  It may behave differently than the Subversion\\ncommand line; where the subversion command line may only apply a single\\nmatching auto-property to a single pathname, this script will apply all\\nmatching lines to a single pathname.\\n\\nUsage:\\n  %s [options] [WC_PATH]\\nwhere WC_PATH is the path to a working copy.\\nIf WC_PATH is not specified, \'.\' is assumed.\\n\\nValid options are:\\n  --help, -h         : Print this help text.\\n  --config ARG       : Read the Subversion config file at path ARG\\n                       instead of \'%s\'.\\n\\"\\"\\" % (SVN_CONFIG_FILENAME, PROGNAME, SVN_CONFIG_FILENAME))\\n\\ndef get_autoprop_lines(fd):\\n  lines = []\\n  reading_autoprops = 0\\n\\n  re_start_autoprops = re.compile(\'^\\\\s*\\\\[auto-props\\\\]\\\\s*\')\\n  re_end_autoprops = re.compile(\'^\\\\s*\\\\[\\\\w+\\\\]\\\\s*\')\\n\\n  for line in fd.xreadlines():\\n    if reading_autoprops:\\n      if re_end_autoprops.match(line):\\n        reading_autoprops = 0\\n        continue\\n    else:\\n      if re_start_autoprops.match(line):\\n        reading_autoprops = 1\\n        continue\\n\\n    if reading_autoprops:\\n      lines += [line]\\n\\n  return lines\\n\\ndef process_autoprop_lines(lines):\\n  result = []\\n\\n  for line in lines:\\n    # Split the line on the = separating the fnmatch string from the\\n    # properties.\\n    try:\\n      (fnmatch, props) = line.split(\'=\', 1)\\n    except ValueError:\\n      continue\\n\\n    # Remove leading and trailing whitespace from the fnmatch and\\n    # properties.\\n    fnmatch = fnmatch.strip()\\n    props = props.strip()\\n\\n    # Create a list of property name and property values.  Remove all\\n    # leading and trailing whitespce from the propery names and\\n    # values.\\n    props_list = []\\n    for prop in props.split(\';\'):\\n      prop = prop.strip()\\n      if not len(prop):\\n        continue\\n      try:\\n        (prop_name, prop_value) = prop.split(\'=\', 1)\\n        prop_name = prop_name.strip()\\n        prop_value = prop_value.strip()\\n      except ValueError:\\n        prop_name = prop\\n        prop_value = \'*\'\\n      if len(prop_name):\\n        props_list += [(prop_name, prop_value)]\\n\\n    result += [(fnmatch, props_list)]\\n\\n  return result\\n\\ndef filter_walk(autoprop_lines, dirname, filenames):\\n  # Do not descend into a .svn directory.\\n  try:\\n    filenames.remove(SVN_WC_ADM_DIR_NAME)\\n  except ValueError:\\n    pass\\n\\n  filenames.sort()\\n\\n  # Find those filenames that match each fnmatch.\\n  for autoprops_line in autoprop_lines:\\n    fnmatch_str = autoprops_line[0]\\n    prop_list = autoprops_line[1]\\n\\n    matching_filenames = fnmatch.filter(filenames, fnmatch_str)\\n    matching_filenames = [f for f in matching_filenames \\\\\\n      if not os.path.islink(os.path.join(dirname, f))]\\n    if not matching_filenames:\\n      continue\\n\\n    for prop in prop_list:\\n      command = [\'svn\', \'propset\', prop[0], prop[1]]\\n      for f in matching_filenames:\\n        command += [\\"%s/%s\\" % (dirname, f)]\\n\\n      status = os.spawnvp(os.P_WAIT, \'svn\', command)\\n      if status:\\n        print(\'Command %s failed with exit status %s\' \\\\\\n              % (command, status))\\n\\ndef main():\\n  try:\\n    opts, args = getopt.getopt(sys.argv[1:], \'h\', [\'help\', \'config=\'])\\n  except getopt.GetoptError, e:\\n    usage()\\n    return 1\\n\\n  config_filename = None\\n  for (o, a) in opts:\\n    if o == \'-h\' or o == \'--help\':\\n      usage()\\n      return 0\\n    elif o == \'--config\':\\n      config_filename = os.path.abspath(a)\\n\\n  if not config_filename:\\n    config_filename = SVN_CONFIG_FILENAME\\n\\n  if len(args) == 0:\\n    wc_path = \'.\'\\n  elif len(args) == 1:\\n    wc_path = args[0]\\n  else:\\n    usage()\\n    print(\\"Too many arguments: %s\\" % \' \'.join(args))\\n    return 1\\n\\n  try:\\n    fd = file(config_filename)\\n  except IOError:\\n    print(\\"Cannot open svn configuration file \'%s\' for reading: %s\\" \\\\\\n          % (config_filename, sys.exc_value.strerror))\\n    return 1\\n\\n  autoprop_lines = get_autoprop_lines(fd)\\n\\n  fd.close()\\n\\n  autoprop_lines = process_autoprop_lines(autoprop_lines)\\n\\n  os.path.walk(wc_path, filter_walk, autoprop_lines)\\n\\nif __name__ == \'__main__\':\\n  sys.exit(main())\\n" }\n'
line: b'{ "repo_name": "microcom/odoo", "ref": "refs/heads/9.0", "path": "addons/hw_proxy/controllers/main.py", "content": "# -*- coding: utf-8 -*-\\nimport logging\\nimport commands\\nimport json\\nimport os\\nimport os.path\\nimport openerp\\nimport time\\nimport random\\nimport subprocess\\nimport json\\nimport werkzeug\\nimport werkzeug.wrappers\\n_logger = logging.getLogger(__name__)\\n\\n\\nfrom openerp import http\\nfrom openerp.http import request\\n\\n# Those are the builtin raspberry pi USB modules, they should\\n# not appear in the list of connected devices.\\nBANNED_DEVICES = set([\\n\\t\\"0424:9514\\",\\t# Standard Microsystem Corp. Builtin Ethernet module\\n\\t\\"1d6b:0002\\",\\t# Linux Foundation 2.0 root hub\\n\\t\\"0424:ec00\\",\\t# Standard Microsystem Corp. Other Builtin Ethernet module\\n])\\n\\n\\n# drivers modules must add to drivers an object with a get_status() method \\n# so that \'status\' can return the status of all active drivers\\ndrivers = {}\\n\\nclass Proxy(http.Controller):\\n\\n    def get_status(self):\\n        statuses = {}\\n        for driver in drivers:\\n            statuses[driver] = drivers[driver].get_status()\\n        return statuses\\n\\n    @http.route(\'/hw_proxy/hello\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def hello(self):\\n        return \\"ping\\"\\n\\n    @http.route(\'/hw_proxy/handshake\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def handshake(self):\\n        return True\\n\\n    @http.route(\'/hw_proxy/status\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def status_http(self):\\n        resp = \\"\\"\\"\\n<!DOCTYPE HTML>\\n<html>\\n    <head>\\n        <title>Odoo\'s PosBox</title>\\n        <style>\\n        body {\\n            width: 480px;\\n            margin: 60px auto;\\n            font-family: sans-serif;\\n            text-align: justify;\\n            color: #6B6B6B;\\n      }\\n        .device {\\n            border-bottom: solid 1px rgb(216,216,216);\\n            padding: 9px;\\n      }\\n        .device:nth-child(2n) {\\n            background:rgb(240,240,240);\\n      }\\n        </style>\\n    </head>\\n    <body>\\n        <h1>Hardware Status</h1>\\n        <p>The list of enabled drivers and their status</p>\\n\\"\\"\\"\\n        statuses = self.get_status()\\n        for driver in statuses:\\n\\n            status = statuses[driver]\\n\\n            if status[\'status\'] == \'connecting\':\\n                color = \'black\'\\n            elif status[\'status\'] == \'connected\':\\n                color = \'green\'\\n            else:\\n                color = \'red\'\\n\\n            resp += \\"<h3 style=\'color:\\"+color+\\";\'>\\"+driver+\' : \'+status[\'status\']+\\"</h3>\\\\n\\"\\n            resp += \\"<ul>\\\\n\\"\\n            for msg in status[\'messages\']:\\n                resp += \'<li>\'+msg+\'</li>\\\\n\'\\n            resp += \\"</ul>\\\\n\\"\\n        resp += \\"\\"\\"\\n            <h2>Connected Devices</h2>\\n            <p>The list of connected USB devices as seen by the posbox</p>\\n        \\"\\"\\"\\n        devices = commands.getoutput(\\"lsusb\\").split(\'\\\\n\')\\n        count   = 0\\n        resp += \\"<div class=\'devices\'>\\\\n\\"\\n        for device in devices:\\n            device_name = device[device.find(\'ID\')+2:]\\n            device_id   = device_name.split()[0]\\n            if not (device_id in BANNED_DEVICES):\\n            \\tresp+= \\"<div class=\'device\' data-device=\'\\"+device+\\"\'>\\"+device_name+\\"</div>\\\\n\\"\\n                count += 1\\n        \\n        if count == 0:\\n            resp += \\"<div class=\'device\'>No USB Device Found</div>\\"\\n\\n        resp += \\"</div>\\\\n</body>\\\\n</html>\\\\n\\\\n\\"\\n\\n        return request.make_response(resp,{\\n            \'Cache-Control\': \'no-cache\', \\n            \'Content-Type\': \'text/html; charset=utf-8\',\\n            \'Access-Control-Allow-Origin\':  \'*\',\\n            \'Access-Control-Allow-Methods\': \'GET\',\\n          })\\n\\n    @http.route(\'/hw_proxy/status_json\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def status_json(self):\\n        return self.get_status()\\n\\n    @http.route(\'/hw_proxy/scan_item_success\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_success(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned with success\\n        \\"\\"\\"\\n        print \'scan_item_success: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/scan_item_error_unrecognized\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_error_unrecognized(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned without success\\n        \\"\\"\\"\\n        print \'scan_item_error_unrecognized: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/help_needed\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_needed(self):\\n        \\"\\"\\"\\n        The user wants an help (ex: light is on)\\n        \\"\\"\\"\\n        print \\"help_needed\\"\\n\\n    @http.route(\'/hw_proxy/help_canceled\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_canceled(self):\\n        \\"\\"\\"\\n        The user stops the help request\\n        \\"\\"\\"\\n        print \\"help_canceled\\"\\n\\n    @http.route(\'/hw_proxy/payment_request\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_request(self, price):\\n        \\"\\"\\"\\n        The PoS will activate the method payment \\n        \\"\\"\\"\\n        print \\"payment_request: price:\\"+str(price)\\n        return \'ok\'\\n\\n    @http.route(\'/hw_proxy/payment_status\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_status(self):\\n        print \\"payment_status\\"\\n        return { \'status\':\'waiting\' } \\n\\n    @http.route(\'/hw_proxy/payment_cancel\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_cancel(self):\\n        print \\"payment_cancel\\"\\n\\n    @http.route(\'/hw_proxy/transaction_start\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_start(self):\\n        print \'transaction_start\'\\n\\n    @http.route(\'/hw_proxy/transaction_end\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_end(self):\\n        print \'transaction_end\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_activated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_activated(self):\\n        print \'cashier_mode_activated\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_deactivated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_deactivated(self):\\n        print \'cashier_mode_deactivated\'\\n\\n    @http.route(\'/hw_proxy/open_cashbox\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def open_cashbox(self):\\n        print \'open_cashbox\'\\n\\n    @http.route(\'/hw_proxy/print_receipt\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_receipt(self, receipt):\\n        print \'print_receipt\' + str(receipt)\\n\\n    @http.route(\'/hw_proxy/is_scanner_connected\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def is_scanner_connected(self, receipt):\\n        print \'is_scanner_connected?\' \\n        return False\\n\\n    @http.route(\'/hw_proxy/scanner\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scanner(self, receipt):\\n        print \'scanner\' \\n        time.sleep(10)\\n        return \'\'\\n\\n    @http.route(\'/hw_proxy/log\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def log(self, arguments):\\n        _logger.info(\' \'.join(str(v) for v in arguments))\\n\\n    @http.route(\'/hw_proxy/print_pdf_invoice\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_pdf_invoice(self, pdfinvoice):\\n        print \'print_pdf_invoice\' + str(pdfinvoice)\\n" }\n'
line: b'{ "repo_name": "optima-ict/odoo", "ref": "refs/heads/9.0", "path": "addons/hw_proxy/controllers/main.py", "content": "# -*- coding: utf-8 -*-\\nimport logging\\nimport commands\\nimport json\\nimport os\\nimport os.path\\nimport openerp\\nimport time\\nimport random\\nimport subprocess\\nimport json\\nimport werkzeug\\nimport werkzeug.wrappers\\n_logger = logging.getLogger(__name__)\\n\\n\\nfrom openerp import http\\nfrom openerp.http import request\\n\\n# Those are the builtin raspberry pi USB modules, they should\\n# not appear in the list of connected devices.\\nBANNED_DEVICES = set([\\n\\t\\"0424:9514\\",\\t# Standard Microsystem Corp. Builtin Ethernet module\\n\\t\\"1d6b:0002\\",\\t# Linux Foundation 2.0 root hub\\n\\t\\"0424:ec00\\",\\t# Standard Microsystem Corp. Other Builtin Ethernet module\\n])\\n\\n\\n# drivers modules must add to drivers an object with a get_status() method \\n# so that \'status\' can return the status of all active drivers\\ndrivers = {}\\n\\nclass Proxy(http.Controller):\\n\\n    def get_status(self):\\n        statuses = {}\\n        for driver in drivers:\\n            statuses[driver] = drivers[driver].get_status()\\n        return statuses\\n\\n    @http.route(\'/hw_proxy/hello\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def hello(self):\\n        return \\"ping\\"\\n\\n    @http.route(\'/hw_proxy/handshake\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def handshake(self):\\n        return True\\n\\n    @http.route(\'/hw_proxy/status\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def status_http(self):\\n        resp = \\"\\"\\"\\n<!DOCTYPE HTML>\\n<html>\\n    <head>\\n        <title>Odoo\'s PosBox</title>\\n        <style>\\n        body {\\n            width: 480px;\\n            margin: 60px auto;\\n            font-family: sans-serif;\\n            text-align: justify;\\n            color: #6B6B6B;\\n      }\\n        .device {\\n            border-bottom: solid 1px rgb(216,216,216);\\n            padding: 9px;\\n      }\\n        .device:nth-child(2n) {\\n            background:rgb(240,240,240);\\n      }\\n        </style>\\n    </head>\\n    <body>\\n        <h1>Hardware Status</h1>\\n        <p>The list of enabled drivers and their status</p>\\n\\"\\"\\"\\n        statuses = self.get_status()\\n        for driver in statuses:\\n\\n            status = statuses[driver]\\n\\n            if status[\'status\'] == \'connecting\':\\n                color = \'black\'\\n            elif status[\'status\'] == \'connected\':\\n                color = \'green\'\\n            else:\\n                color = \'red\'\\n\\n            resp += \\"<h3 style=\'color:\\"+color+\\";\'>\\"+driver+\' : \'+status[\'status\']+\\"</h3>\\\\n\\"\\n            resp += \\"<ul>\\\\n\\"\\n            for msg in status[\'messages\']:\\n                resp += \'<li>\'+msg+\'</li>\\\\n\'\\n            resp += \\"</ul>\\\\n\\"\\n        resp += \\"\\"\\"\\n            <h2>Connected Devices</h2>\\n            <p>The list of connected USB devices as seen by the posbox</p>\\n        \\"\\"\\"\\n        devices = commands.getoutput(\\"lsusb\\").split(\'\\\\n\')\\n        count   = 0\\n        resp += \\"<div class=\'devices\'>\\\\n\\"\\n        for device in devices:\\n            device_name = device[device.find(\'ID\')+2:]\\n            device_id   = device_name.split()[0]\\n            if not (device_id in BANNED_DEVICES):\\n            \\tresp+= \\"<div class=\'device\' data-device=\'\\"+device+\\"\'>\\"+device_name+\\"</div>\\\\n\\"\\n                count += 1\\n        \\n        if count == 0:\\n            resp += \\"<div class=\'device\'>No USB Device Found</div>\\"\\n\\n        resp += \\"</div>\\\\n</body>\\\\n</html>\\\\n\\\\n\\"\\n\\n        return request.make_response(resp,{\\n            \'Cache-Control\': \'no-cache\', \\n            \'Content-Type\': \'text/html; charset=utf-8\',\\n            \'Access-Control-Allow-Origin\':  \'*\',\\n            \'Access-Control-Allow-Methods\': \'GET\',\\n          })\\n\\n    @http.route(\'/hw_proxy/status_json\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def status_json(self):\\n        return self.get_status()\\n\\n    @http.route(\'/hw_proxy/scan_item_success\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_success(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned with success\\n        \\"\\"\\"\\n        print \'scan_item_success: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/scan_item_error_unrecognized\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_error_unrecognized(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned without success\\n        \\"\\"\\"\\n        print \'scan_item_error_unrecognized: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/help_needed\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_needed(self):\\n        \\"\\"\\"\\n        The user wants an help (ex: light is on)\\n        \\"\\"\\"\\n        print \\"help_needed\\"\\n\\n    @http.route(\'/hw_proxy/help_canceled\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_canceled(self):\\n        \\"\\"\\"\\n        The user stops the help request\\n        \\"\\"\\"\\n        print \\"help_canceled\\"\\n\\n    @http.route(\'/hw_proxy/payment_request\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_request(self, price):\\n        \\"\\"\\"\\n        The PoS will activate the method payment \\n        \\"\\"\\"\\n        print \\"payment_request: price:\\"+str(price)\\n        return \'ok\'\\n\\n    @http.route(\'/hw_proxy/payment_status\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_status(self):\\n        print \\"payment_status\\"\\n        return { \'status\':\'waiting\' } \\n\\n    @http.route(\'/hw_proxy/payment_cancel\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_cancel(self):\\n        print \\"payment_cancel\\"\\n\\n    @http.route(\'/hw_proxy/transaction_start\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_start(self):\\n        print \'transaction_start\'\\n\\n    @http.route(\'/hw_proxy/transaction_end\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_end(self):\\n        print \'transaction_end\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_activated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_activated(self):\\n        print \'cashier_mode_activated\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_deactivated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_deactivated(self):\\n        print \'cashier_mode_deactivated\'\\n\\n    @http.route(\'/hw_proxy/open_cashbox\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def open_cashbox(self):\\n        print \'open_cashbox\'\\n\\n    @http.route(\'/hw_proxy/print_receipt\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_receipt(self, receipt):\\n        print \'print_receipt\' + str(receipt)\\n\\n    @http.route(\'/hw_proxy/is_scanner_connected\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def is_scanner_connected(self, receipt):\\n        print \'is_scanner_connected?\' \\n        return False\\n\\n    @http.route(\'/hw_proxy/scanner\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scanner(self, receipt):\\n        print \'scanner\' \\n        time.sleep(10)\\n        return \'\'\\n\\n    @http.route(\'/hw_proxy/log\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def log(self, arguments):\\n        _logger.info(\' \'.join(str(v) for v in arguments))\\n\\n    @http.route(\'/hw_proxy/print_pdf_invoice\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_pdf_invoice(self, pdfinvoice):\\n        print \'print_pdf_invoice\' + str(pdfinvoice)\\n" }\n'
line: b'{ "repo_name": "ludwiktrammer/odoo", "ref": "refs/heads/9.0", "path": "addons/hw_proxy/controllers/main.py", "content": "# -*- coding: utf-8 -*-\\nimport logging\\nimport commands\\nimport json\\nimport os\\nimport os.path\\nimport openerp\\nimport time\\nimport random\\nimport subprocess\\nimport json\\nimport werkzeug\\nimport werkzeug.wrappers\\n_logger = logging.getLogger(__name__)\\n\\n\\nfrom openerp import http\\nfrom openerp.http import request\\n\\n# Those are the builtin raspberry pi USB modules, they should\\n# not appear in the list of connected devices.\\nBANNED_DEVICES = set([\\n\\t\\"0424:9514\\",\\t# Standard Microsystem Corp. Builtin Ethernet module\\n\\t\\"1d6b:0002\\",\\t# Linux Foundation 2.0 root hub\\n\\t\\"0424:ec00\\",\\t# Standard Microsystem Corp. Other Builtin Ethernet module\\n])\\n\\n\\n# drivers modules must add to drivers an object with a get_status() method \\n# so that \'status\' can return the status of all active drivers\\ndrivers = {}\\n\\nclass Proxy(http.Controller):\\n\\n    def get_status(self):\\n        statuses = {}\\n        for driver in drivers:\\n            statuses[driver] = drivers[driver].get_status()\\n        return statuses\\n\\n    @http.route(\'/hw_proxy/hello\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def hello(self):\\n        return \\"ping\\"\\n\\n    @http.route(\'/hw_proxy/handshake\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def handshake(self):\\n        return True\\n\\n    @http.route(\'/hw_proxy/status\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def status_http(self):\\n        resp = \\"\\"\\"\\n<!DOCTYPE HTML>\\n<html>\\n    <head>\\n        <title>Odoo\'s PosBox</title>\\n        <style>\\n        body {\\n            width: 480px;\\n            margin: 60px auto;\\n            font-family: sans-serif;\\n            text-align: justify;\\n            color: #6B6B6B;\\n      }\\n        .device {\\n            border-bottom: solid 1px rgb(216,216,216);\\n            padding: 9px;\\n      }\\n        .device:nth-child(2n) {\\n            background:rgb(240,240,240);\\n      }\\n        </style>\\n    </head>\\n    <body>\\n        <h1>Hardware Status</h1>\\n        <p>The list of enabled drivers and their status</p>\\n\\"\\"\\"\\n        statuses = self.get_status()\\n        for driver in statuses:\\n\\n            status = statuses[driver]\\n\\n            if status[\'status\'] == \'connecting\':\\n                color = \'black\'\\n            elif status[\'status\'] == \'connected\':\\n                color = \'green\'\\n            else:\\n                color = \'red\'\\n\\n            resp += \\"<h3 style=\'color:\\"+color+\\";\'>\\"+driver+\' : \'+status[\'status\']+\\"</h3>\\\\n\\"\\n            resp += \\"<ul>\\\\n\\"\\n            for msg in status[\'messages\']:\\n                resp += \'<li>\'+msg+\'</li>\\\\n\'\\n            resp += \\"</ul>\\\\n\\"\\n        resp += \\"\\"\\"\\n            <h2>Connected Devices</h2>\\n            <p>The list of connected USB devices as seen by the posbox</p>\\n        \\"\\"\\"\\n        devices = commands.getoutput(\\"lsusb\\").split(\'\\\\n\')\\n        count   = 0\\n        resp += \\"<div class=\'devices\'>\\\\n\\"\\n        for device in devices:\\n            device_name = device[device.find(\'ID\')+2:]\\n            device_id   = device_name.split()[0]\\n            if not (device_id in BANNED_DEVICES):\\n            \\tresp+= \\"<div class=\'device\' data-device=\'\\"+device+\\"\'>\\"+device_name+\\"</div>\\\\n\\"\\n                count += 1\\n        \\n        if count == 0:\\n            resp += \\"<div class=\'device\'>No USB Device Found</div>\\"\\n\\n        resp += \\"</div>\\\\n</body>\\\\n</html>\\\\n\\\\n\\"\\n\\n        return request.make_response(resp,{\\n            \'Cache-Control\': \'no-cache\', \\n            \'Content-Type\': \'text/html; charset=utf-8\',\\n            \'Access-Control-Allow-Origin\':  \'*\',\\n            \'Access-Control-Allow-Methods\': \'GET\',\\n          })\\n\\n    @http.route(\'/hw_proxy/status_json\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def status_json(self):\\n        return self.get_status()\\n\\n    @http.route(\'/hw_proxy/scan_item_success\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_success(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned with success\\n        \\"\\"\\"\\n        print \'scan_item_success: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/scan_item_error_unrecognized\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_error_unrecognized(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned without success\\n        \\"\\"\\"\\n        print \'scan_item_error_unrecognized: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/help_needed\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_needed(self):\\n        \\"\\"\\"\\n        The user wants an help (ex: light is on)\\n        \\"\\"\\"\\n        print \\"help_needed\\"\\n\\n    @http.route(\'/hw_proxy/help_canceled\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_canceled(self):\\n        \\"\\"\\"\\n        The user stops the help request\\n        \\"\\"\\"\\n        print \\"help_canceled\\"\\n\\n    @http.route(\'/hw_proxy/payment_request\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_request(self, price):\\n        \\"\\"\\"\\n        The PoS will activate the method payment \\n        \\"\\"\\"\\n        print \\"payment_request: price:\\"+str(price)\\n        return \'ok\'\\n\\n    @http.route(\'/hw_proxy/payment_status\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_status(self):\\n        print \\"payment_status\\"\\n        return { \'status\':\'waiting\' } \\n\\n    @http.route(\'/hw_proxy/payment_cancel\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_cancel(self):\\n        print \\"payment_cancel\\"\\n\\n    @http.route(\'/hw_proxy/transaction_start\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_start(self):\\n        print \'transaction_start\'\\n\\n    @http.route(\'/hw_proxy/transaction_end\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_end(self):\\n        print \'transaction_end\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_activated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_activated(self):\\n        print \'cashier_mode_activated\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_deactivated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_deactivated(self):\\n        print \'cashier_mode_deactivated\'\\n\\n    @http.route(\'/hw_proxy/open_cashbox\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def open_cashbox(self):\\n        print \'open_cashbox\'\\n\\n    @http.route(\'/hw_proxy/print_receipt\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_receipt(self, receipt):\\n        print \'print_receipt\' + str(receipt)\\n\\n    @http.route(\'/hw_proxy/is_scanner_connected\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def is_scanner_connected(self, receipt):\\n        print \'is_scanner_connected?\' \\n        return False\\n\\n    @http.route(\'/hw_proxy/scanner\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scanner(self, receipt):\\n        print \'scanner\' \\n        time.sleep(10)\\n        return \'\'\\n\\n    @http.route(\'/hw_proxy/log\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def log(self, arguments):\\n        _logger.info(\' \'.join(str(v) for v in arguments))\\n\\n    @http.route(\'/hw_proxy/print_pdf_invoice\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_pdf_invoice(self, pdfinvoice):\\n        print \'print_pdf_invoice\' + str(pdfinvoice)\\n" }\n'
line: b'{ "repo_name": "angelapper/odoo", "ref": "refs/heads/9.0", "path": "addons/hw_proxy/controllers/main.py", "content": "# -*- coding: utf-8 -*-\\nimport logging\\nimport commands\\nimport json\\nimport os\\nimport os.path\\nimport openerp\\nimport time\\nimport random\\nimport subprocess\\nimport json\\nimport werkzeug\\nimport werkzeug.wrappers\\n_logger = logging.getLogger(__name__)\\n\\n\\nfrom openerp import http\\nfrom openerp.http import request\\n\\n# Those are the builtin raspberry pi USB modules, they should\\n# not appear in the list of connected devices.\\nBANNED_DEVICES = set([\\n\\t\\"0424:9514\\",\\t# Standard Microsystem Corp. Builtin Ethernet module\\n\\t\\"1d6b:0002\\",\\t# Linux Foundation 2.0 root hub\\n\\t\\"0424:ec00\\",\\t# Standard Microsystem Corp. Other Builtin Ethernet module\\n])\\n\\n\\n# drivers modules must add to drivers an object with a get_status() method \\n# so that \'status\' can return the status of all active drivers\\ndrivers = {}\\n\\nclass Proxy(http.Controller):\\n\\n    def get_status(self):\\n        statuses = {}\\n        for driver in drivers:\\n            statuses[driver] = drivers[driver].get_status()\\n        return statuses\\n\\n    @http.route(\'/hw_proxy/hello\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def hello(self):\\n        return \\"ping\\"\\n\\n    @http.route(\'/hw_proxy/handshake\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def handshake(self):\\n        return True\\n\\n    @http.route(\'/hw_proxy/status\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def status_http(self):\\n        resp = \\"\\"\\"\\n<!DOCTYPE HTML>\\n<html>\\n    <head>\\n        <title>Odoo\'s PosBox</title>\\n        <style>\\n        body {\\n            width: 480px;\\n            margin: 60px auto;\\n            font-family: sans-serif;\\n            text-align: justify;\\n            color: #6B6B6B;\\n      }\\n        .device {\\n            border-bottom: solid 1px rgb(216,216,216);\\n            padding: 9px;\\n      }\\n        .device:nth-child(2n) {\\n            background:rgb(240,240,240);\\n      }\\n        </style>\\n    </head>\\n    <body>\\n        <h1>Hardware Status</h1>\\n        <p>The list of enabled drivers and their status</p>\\n\\"\\"\\"\\n        statuses = self.get_status()\\n        for driver in statuses:\\n\\n            status = statuses[driver]\\n\\n            if status[\'status\'] == \'connecting\':\\n                color = \'black\'\\n            elif status[\'status\'] == \'connected\':\\n                color = \'green\'\\n            else:\\n                color = \'red\'\\n\\n            resp += \\"<h3 style=\'color:\\"+color+\\";\'>\\"+driver+\' : \'+status[\'status\']+\\"</h3>\\\\n\\"\\n            resp += \\"<ul>\\\\n\\"\\n            for msg in status[\'messages\']:\\n                resp += \'<li>\'+msg+\'</li>\\\\n\'\\n            resp += \\"</ul>\\\\n\\"\\n        resp += \\"\\"\\"\\n            <h2>Connected Devices</h2>\\n            <p>The list of connected USB devices as seen by the posbox</p>\\n        \\"\\"\\"\\n        devices = commands.getoutput(\\"lsusb\\").split(\'\\\\n\')\\n        count   = 0\\n        resp += \\"<div class=\'devices\'>\\\\n\\"\\n        for device in devices:\\n            device_name = device[device.find(\'ID\')+2:]\\n            device_id   = device_name.split()[0]\\n            if not (device_id in BANNED_DEVICES):\\n            \\tresp+= \\"<div class=\'device\' data-device=\'\\"+device+\\"\'>\\"+device_name+\\"</div>\\\\n\\"\\n                count += 1\\n        \\n        if count == 0:\\n            resp += \\"<div class=\'device\'>No USB Device Found</div>\\"\\n\\n        resp += \\"</div>\\\\n</body>\\\\n</html>\\\\n\\\\n\\"\\n\\n        return request.make_response(resp,{\\n            \'Cache-Control\': \'no-cache\', \\n            \'Content-Type\': \'text/html; charset=utf-8\',\\n            \'Access-Control-Allow-Origin\':  \'*\',\\n            \'Access-Control-Allow-Methods\': \'GET\',\\n          })\\n\\n    @http.route(\'/hw_proxy/status_json\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def status_json(self):\\n        return self.get_status()\\n\\n    @http.route(\'/hw_proxy/scan_item_success\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_success(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned with success\\n        \\"\\"\\"\\n        print \'scan_item_success: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/scan_item_error_unrecognized\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_error_unrecognized(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned without success\\n        \\"\\"\\"\\n        print \'scan_item_error_unrecognized: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/help_needed\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_needed(self):\\n        \\"\\"\\"\\n        The user wants an help (ex: light is on)\\n        \\"\\"\\"\\n        print \\"help_needed\\"\\n\\n    @http.route(\'/hw_proxy/help_canceled\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_canceled(self):\\n        \\"\\"\\"\\n        The user stops the help request\\n        \\"\\"\\"\\n        print \\"help_canceled\\"\\n\\n    @http.route(\'/hw_proxy/payment_request\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_request(self, price):\\n        \\"\\"\\"\\n        The PoS will activate the method payment \\n        \\"\\"\\"\\n        print \\"payment_request: price:\\"+str(price)\\n        return \'ok\'\\n\\n    @http.route(\'/hw_proxy/payment_status\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_status(self):\\n        print \\"payment_status\\"\\n        return { \'status\':\'waiting\' } \\n\\n    @http.route(\'/hw_proxy/payment_cancel\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_cancel(self):\\n        print \\"payment_cancel\\"\\n\\n    @http.route(\'/hw_proxy/transaction_start\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_start(self):\\n        print \'transaction_start\'\\n\\n    @http.route(\'/hw_proxy/transaction_end\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_end(self):\\n        print \'transaction_end\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_activated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_activated(self):\\n        print \'cashier_mode_activated\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_deactivated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_deactivated(self):\\n        print \'cashier_mode_deactivated\'\\n\\n    @http.route(\'/hw_proxy/open_cashbox\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def open_cashbox(self):\\n        print \'open_cashbox\'\\n\\n    @http.route(\'/hw_proxy/print_receipt\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_receipt(self, receipt):\\n        print \'print_receipt\' + str(receipt)\\n\\n    @http.route(\'/hw_proxy/is_scanner_connected\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def is_scanner_connected(self, receipt):\\n        print \'is_scanner_connected?\' \\n        return False\\n\\n    @http.route(\'/hw_proxy/scanner\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scanner(self, receipt):\\n        print \'scanner\' \\n        time.sleep(10)\\n        return \'\'\\n\\n    @http.route(\'/hw_proxy/log\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def log(self, arguments):\\n        _logger.info(\' \'.join(str(v) for v in arguments))\\n\\n    @http.route(\'/hw_proxy/print_pdf_invoice\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_pdf_invoice(self, pdfinvoice):\\n        print \'print_pdf_invoice\' + str(pdfinvoice)\\n" }\n'
line: b'{ "repo_name": "swvist/Debexpo", "ref": "refs/heads/gsoc", "path": "debexpo/tests/functional/test_ppa.py", "content": "from debexpo.tests import *\\n\\nclass TestPpaController(TestController):\\n\\n    def test_index(self):\\n        response = self.app.get(url(controller=\'ppa\', action=\'index\'))\\n        # Test response...\\n" }\n'
line: b'{ "repo_name": "OCA/account-financial-tools", "ref": "refs/heads/13.0", "path": "account_journal_lock_date/wizards/__init__.py", "content": "# License AGPL-3.0 or later (https://www.gnu.org/licenses/agpl).\\n\\nfrom . import update_journal_lock_dates\\n" }\n'
line: b'{ "repo_name": "andrewcbennett/iris", "ref": "refs/heads/placeholder", "path": "lib/iris/tests/integration/test_regridding.py", "content": "# (C) British Crown Copyright 2013 - 2015, Met Office\\n#\\n# This file is part of Iris.\\n#\\n# Iris is free software: you can redistribute it and/or modify it under\\n# the terms of the GNU Lesser General Public License as published by the\\n# Free Software Foundation, either version 3 of the License, or\\n# (at your option) any later version.\\n#\\n# Iris is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU Lesser General Public License for more details.\\n#\\n# You should have received a copy of the GNU Lesser General Public License\\n# along with Iris.  If not, see <http://www.gnu.org/licenses/>.\\n\\"\\"\\"Integration tests for regridding.\\"\\"\\"\\n\\nfrom __future__ import (absolute_import, division, print_function)\\nfrom six.moves import (filter, input, map, range, zip)  # noqa\\n\\n# Import iris.tests first so that some things can be initialised before\\n# importing anything else.\\nimport iris.tests as tests\\n\\nimport numpy as np\\n\\nimport iris\\nfrom iris.analysis._regrid import RectilinearRegridder as Regridder\\nfrom iris.coord_systems import GeogCS\\nfrom iris.coords import DimCoord\\nfrom iris.cube import Cube\\nfrom iris.tests.stock import global_pp\\n\\n# Run tests in no graphics mode if matplotlib is not available.\\nif tests.MPL_AVAILABLE:\\n    import iris.quickplot as qplt\\n\\n\\n@tests.skip_data\\n@tests.skip_plot\\nclass TestOSGBToLatLon(tests.GraphicsTest):\\n    def setUp(self):\\n        path = tests.get_data_path(\\n            (\'NIMROD\', \'uk2km\', \'WO0000000003452\',\\n             \'201007020900_u1096_ng_ey00_visibility0180_screen_2km\'))\\n        self.src = iris.load_cube(path)[0]\\n        self.src.data = self.src.data.astype(np.float32)\\n        self.grid = Cube(np.empty((73, 96)))\\n        cs = GeogCS(6370000)\\n        lat = DimCoord(np.linspace(46, 65, 73), \'latitude\', units=\'degrees\',\\n                       coord_system=cs)\\n        lon = DimCoord(np.linspace(-14, 8, 96), \'longitude\', units=\'degrees\',\\n                       coord_system=cs)\\n        self.grid.add_dim_coord(lat, 0)\\n        self.grid.add_dim_coord(lon, 1)\\n\\n    def _regrid(self, method):\\n        regridder = Regridder(self.src, self.grid, method, \'mask\')\\n        result = regridder(self.src)\\n        qplt.pcolor(result, antialiased=False)\\n        qplt.plt.gca().coastlines()\\n\\n    def test_linear(self):\\n        self._regrid(\'linear\')\\n        self.check_graphic()\\n\\n    def test_nearest(self):\\n        self._regrid(\'nearest\')\\n        self.check_graphic()\\n\\n\\n@tests.skip_data\\n@tests.skip_plot\\nclass TestGlobalSubsample(tests.GraphicsTest):\\n    def setUp(self):\\n        self.src = global_pp()\\n        # Subsample and shift the target grid so that we can see a visual\\n        # difference between regridding scheme methods.\\n        grid = self.src[1::2, 1::3]\\n        grid.coord(\'latitude\').points = grid.coord(\'latitude\').points + 1\\n        grid.coord(\'longitude\').points = grid.coord(\'longitude\').points + 1\\n        self.grid = grid\\n\\n    def _regrid(self, method):\\n        regridder = Regridder(self.src, self.grid, method, \'mask\')\\n        result = regridder(self.src)\\n        qplt.pcolormesh(result)\\n        qplt.plt.gca().coastlines()\\n\\n    def test_linear(self):\\n        self._regrid(\'linear\')\\n        self.check_graphic()\\n\\n    def test_nearest(self):\\n        self._regrid(\'nearest\')\\n        self.check_graphic()\\n\\n\\nif __name__ == \\"__main__\\":\\n    tests.main()\\n" }\n'
line: b'{ "repo_name": "jkettleb/iris", "ref": "refs/heads/placeholder", "path": "lib/iris/tests/integration/test_regridding.py", "content": "# (C) British Crown Copyright 2013 - 2015, Met Office\\n#\\n# This file is part of Iris.\\n#\\n# Iris is free software: you can redistribute it and/or modify it under\\n# the terms of the GNU Lesser General Public License as published by the\\n# Free Software Foundation, either version 3 of the License, or\\n# (at your option) any later version.\\n#\\n# Iris is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU Lesser General Public License for more details.\\n#\\n# You should have received a copy of the GNU Lesser General Public License\\n# along with Iris.  If not, see <http://www.gnu.org/licenses/>.\\n\\"\\"\\"Integration tests for regridding.\\"\\"\\"\\n\\nfrom __future__ import (absolute_import, division, print_function)\\nfrom six.moves import (filter, input, map, range, zip)  # noqa\\n\\n# Import iris.tests first so that some things can be initialised before\\n# importing anything else.\\nimport iris.tests as tests\\n\\nimport numpy as np\\n\\nimport iris\\nfrom iris.analysis._regrid import RectilinearRegridder as Regridder\\nfrom iris.coord_systems import GeogCS\\nfrom iris.coords import DimCoord\\nfrom iris.cube import Cube\\nfrom iris.tests.stock import global_pp\\n\\n# Run tests in no graphics mode if matplotlib is not available.\\nif tests.MPL_AVAILABLE:\\n    import iris.quickplot as qplt\\n\\n\\n@tests.skip_data\\n@tests.skip_plot\\nclass TestOSGBToLatLon(tests.GraphicsTest):\\n    def setUp(self):\\n        path = tests.get_data_path(\\n            (\'NIMROD\', \'uk2km\', \'WO0000000003452\',\\n             \'201007020900_u1096_ng_ey00_visibility0180_screen_2km\'))\\n        self.src = iris.load_cube(path)[0]\\n        self.src.data = self.src.data.astype(np.float32)\\n        self.grid = Cube(np.empty((73, 96)))\\n        cs = GeogCS(6370000)\\n        lat = DimCoord(np.linspace(46, 65, 73), \'latitude\', units=\'degrees\',\\n                       coord_system=cs)\\n        lon = DimCoord(np.linspace(-14, 8, 96), \'longitude\', units=\'degrees\',\\n                       coord_system=cs)\\n        self.grid.add_dim_coord(lat, 0)\\n        self.grid.add_dim_coord(lon, 1)\\n\\n    def _regrid(self, method):\\n        regridder = Regridder(self.src, self.grid, method, \'mask\')\\n        result = regridder(self.src)\\n        qplt.pcolor(result, antialiased=False)\\n        qplt.plt.gca().coastlines()\\n\\n    def test_linear(self):\\n        self._regrid(\'linear\')\\n        self.check_graphic()\\n\\n    def test_nearest(self):\\n        self._regrid(\'nearest\')\\n        self.check_graphic()\\n\\n\\n@tests.skip_data\\n@tests.skip_plot\\nclass TestGlobalSubsample(tests.GraphicsTest):\\n    def setUp(self):\\n        self.src = global_pp()\\n        # Subsample and shift the target grid so that we can see a visual\\n        # difference between regridding scheme methods.\\n        grid = self.src[1::2, 1::3]\\n        grid.coord(\'latitude\').points = grid.coord(\'latitude\').points + 1\\n        grid.coord(\'longitude\').points = grid.coord(\'longitude\').points + 1\\n        self.grid = grid\\n\\n    def _regrid(self, method):\\n        regridder = Regridder(self.src, self.grid, method, \'mask\')\\n        result = regridder(self.src)\\n        qplt.pcolormesh(result)\\n        qplt.plt.gca().coastlines()\\n\\n    def test_linear(self):\\n        self._regrid(\'linear\')\\n        self.check_graphic()\\n\\n    def test_nearest(self):\\n        self._regrid(\'nearest\')\\n        self.check_graphic()\\n\\n\\nif __name__ == \\"__main__\\":\\n    tests.main()\\n" }\n'
line: b'{ "repo_name": "sysadminmatmoz/OCB", "ref": "refs/heads/9.0", "path": "addons/hw_proxy/controllers/main.py", "content": "# -*- coding: utf-8 -*-\\nimport logging\\nimport commands\\nimport json\\nimport os\\nimport os.path\\nimport openerp\\nimport time\\nimport random\\nimport subprocess\\nimport json\\nimport werkzeug\\nimport werkzeug.wrappers\\n_logger = logging.getLogger(__name__)\\n\\n\\nfrom openerp import http\\nfrom openerp.http import request\\n\\n# Those are the builtin raspberry pi USB modules, they should\\n# not appear in the list of connected devices.\\nBANNED_DEVICES = set([\\n\\t\\"0424:9514\\",\\t# Standard Microsystem Corp. Builtin Ethernet module\\n\\t\\"1d6b:0002\\",\\t# Linux Foundation 2.0 root hub\\n\\t\\"0424:ec00\\",\\t# Standard Microsystem Corp. Other Builtin Ethernet module\\n])\\n\\n\\n# drivers modules must add to drivers an object with a get_status() method \\n# so that \'status\' can return the status of all active drivers\\ndrivers = {}\\n\\nclass Proxy(http.Controller):\\n\\n    def get_status(self):\\n        statuses = {}\\n        for driver in drivers:\\n            statuses[driver] = drivers[driver].get_status()\\n        return statuses\\n\\n    @http.route(\'/hw_proxy/hello\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def hello(self):\\n        return \\"ping\\"\\n\\n    @http.route(\'/hw_proxy/handshake\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def handshake(self):\\n        return True\\n\\n    @http.route(\'/hw_proxy/status\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def status_http(self):\\n        resp = \\"\\"\\"\\n<!DOCTYPE HTML>\\n<html>\\n    <head>\\n        <title>Odoo\'s PosBox</title>\\n        <style>\\n        body {\\n            width: 480px;\\n            margin: 60px auto;\\n            font-family: sans-serif;\\n            text-align: justify;\\n            color: #6B6B6B;\\n      }\\n        .device {\\n            border-bottom: solid 1px rgb(216,216,216);\\n            padding: 9px;\\n      }\\n        .device:nth-child(2n) {\\n            background:rgb(240,240,240);\\n      }\\n        </style>\\n    </head>\\n    <body>\\n        <h1>Hardware Status</h1>\\n        <p>The list of enabled drivers and their status</p>\\n\\"\\"\\"\\n        statuses = self.get_status()\\n        for driver in statuses:\\n\\n            status = statuses[driver]\\n\\n            if status[\'status\'] == \'connecting\':\\n                color = \'black\'\\n            elif status[\'status\'] == \'connected\':\\n                color = \'green\'\\n            else:\\n                color = \'red\'\\n\\n            resp += \\"<h3 style=\'color:\\"+color+\\";\'>\\"+driver+\' : \'+status[\'status\']+\\"</h3>\\\\n\\"\\n            resp += \\"<ul>\\\\n\\"\\n            for msg in status[\'messages\']:\\n                resp += \'<li>\'+msg+\'</li>\\\\n\'\\n            resp += \\"</ul>\\\\n\\"\\n        resp += \\"\\"\\"\\n            <h2>Connected Devices</h2>\\n            <p>The list of connected USB devices as seen by the posbox</p>\\n        \\"\\"\\"\\n        devices = commands.getoutput(\\"lsusb\\").split(\'\\\\n\')\\n        count   = 0\\n        resp += \\"<div class=\'devices\'>\\\\n\\"\\n        for device in devices:\\n            device_name = device[device.find(\'ID\')+2:]\\n            device_id   = device_name.split()[0]\\n            if not (device_id in BANNED_DEVICES):\\n            \\tresp+= \\"<div class=\'device\' data-device=\'\\"+device+\\"\'>\\"+device_name+\\"</div>\\\\n\\"\\n                count += 1\\n        \\n        if count == 0:\\n            resp += \\"<div class=\'device\'>No USB Device Found</div>\\"\\n\\n        resp += \\"</div>\\\\n</body>\\\\n</html>\\\\n\\\\n\\"\\n\\n        return request.make_response(resp,{\\n            \'Cache-Control\': \'no-cache\', \\n            \'Content-Type\': \'text/html; charset=utf-8\',\\n            \'Access-Control-Allow-Origin\':  \'*\',\\n            \'Access-Control-Allow-Methods\': \'GET\',\\n          })\\n\\n    @http.route(\'/hw_proxy/status_json\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def status_json(self):\\n        return self.get_status()\\n\\n    @http.route(\'/hw_proxy/scan_item_success\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_success(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned with success\\n        \\"\\"\\"\\n        print \'scan_item_success: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/scan_item_error_unrecognized\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_error_unrecognized(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned without success\\n        \\"\\"\\"\\n        print \'scan_item_error_unrecognized: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/help_needed\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_needed(self):\\n        \\"\\"\\"\\n        The user wants an help (ex: light is on)\\n        \\"\\"\\"\\n        print \\"help_needed\\"\\n\\n    @http.route(\'/hw_proxy/help_canceled\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_canceled(self):\\n        \\"\\"\\"\\n        The user stops the help request\\n        \\"\\"\\"\\n        print \\"help_canceled\\"\\n\\n    @http.route(\'/hw_proxy/payment_request\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_request(self, price):\\n        \\"\\"\\"\\n        The PoS will activate the method payment \\n        \\"\\"\\"\\n        print \\"payment_request: price:\\"+str(price)\\n        return \'ok\'\\n\\n    @http.route(\'/hw_proxy/payment_status\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_status(self):\\n        print \\"payment_status\\"\\n        return { \'status\':\'waiting\' } \\n\\n    @http.route(\'/hw_proxy/payment_cancel\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_cancel(self):\\n        print \\"payment_cancel\\"\\n\\n    @http.route(\'/hw_proxy/transaction_start\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_start(self):\\n        print \'transaction_start\'\\n\\n    @http.route(\'/hw_proxy/transaction_end\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_end(self):\\n        print \'transaction_end\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_activated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_activated(self):\\n        print \'cashier_mode_activated\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_deactivated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_deactivated(self):\\n        print \'cashier_mode_deactivated\'\\n\\n    @http.route(\'/hw_proxy/open_cashbox\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def open_cashbox(self):\\n        print \'open_cashbox\'\\n\\n    @http.route(\'/hw_proxy/print_receipt\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_receipt(self, receipt):\\n        print \'print_receipt\' + str(receipt)\\n\\n    @http.route(\'/hw_proxy/is_scanner_connected\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def is_scanner_connected(self, receipt):\\n        print \'is_scanner_connected?\' \\n        return False\\n\\n    @http.route(\'/hw_proxy/scanner\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scanner(self, receipt):\\n        print \'scanner\' \\n        time.sleep(10)\\n        return \'\'\\n\\n    @http.route(\'/hw_proxy/log\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def log(self, arguments):\\n        _logger.info(\' \'.join(str(v) for v in arguments))\\n\\n    @http.route(\'/hw_proxy/print_pdf_invoice\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_pdf_invoice(self, pdfinvoice):\\n        print \'print_pdf_invoice\' + str(pdfinvoice)\\n" }\n'
line: b'{ "repo_name": "AyoubZahid/odoo", "ref": "refs/heads/9.0", "path": "addons/hw_proxy/controllers/main.py", "content": "# -*- coding: utf-8 -*-\\nimport logging\\nimport commands\\nimport json\\nimport os\\nimport os.path\\nimport openerp\\nimport time\\nimport random\\nimport subprocess\\nimport json\\nimport werkzeug\\nimport werkzeug.wrappers\\n_logger = logging.getLogger(__name__)\\n\\n\\nfrom openerp import http\\nfrom openerp.http import request\\n\\n# Those are the builtin raspberry pi USB modules, they should\\n# not appear in the list of connected devices.\\nBANNED_DEVICES = set([\\n\\t\\"0424:9514\\",\\t# Standard Microsystem Corp. Builtin Ethernet module\\n\\t\\"1d6b:0002\\",\\t# Linux Foundation 2.0 root hub\\n\\t\\"0424:ec00\\",\\t# Standard Microsystem Corp. Other Builtin Ethernet module\\n])\\n\\n\\n# drivers modules must add to drivers an object with a get_status() method \\n# so that \'status\' can return the status of all active drivers\\ndrivers = {}\\n\\nclass Proxy(http.Controller):\\n\\n    def get_status(self):\\n        statuses = {}\\n        for driver in drivers:\\n            statuses[driver] = drivers[driver].get_status()\\n        return statuses\\n\\n    @http.route(\'/hw_proxy/hello\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def hello(self):\\n        return \\"ping\\"\\n\\n    @http.route(\'/hw_proxy/handshake\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def handshake(self):\\n        return True\\n\\n    @http.route(\'/hw_proxy/status\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def status_http(self):\\n        resp = \\"\\"\\"\\n<!DOCTYPE HTML>\\n<html>\\n    <head>\\n        <title>Odoo\'s PosBox</title>\\n        <style>\\n        body {\\n            width: 480px;\\n            margin: 60px auto;\\n            font-family: sans-serif;\\n            text-align: justify;\\n            color: #6B6B6B;\\n      }\\n        .device {\\n            border-bottom: solid 1px rgb(216,216,216);\\n            padding: 9px;\\n      }\\n        .device:nth-child(2n) {\\n            background:rgb(240,240,240);\\n      }\\n        </style>\\n    </head>\\n    <body>\\n        <h1>Hardware Status</h1>\\n        <p>The list of enabled drivers and their status</p>\\n\\"\\"\\"\\n        statuses = self.get_status()\\n        for driver in statuses:\\n\\n            status = statuses[driver]\\n\\n            if status[\'status\'] == \'connecting\':\\n                color = \'black\'\\n            elif status[\'status\'] == \'connected\':\\n                color = \'green\'\\n            else:\\n                color = \'red\'\\n\\n            resp += \\"<h3 style=\'color:\\"+color+\\";\'>\\"+driver+\' : \'+status[\'status\']+\\"</h3>\\\\n\\"\\n            resp += \\"<ul>\\\\n\\"\\n            for msg in status[\'messages\']:\\n                resp += \'<li>\'+msg+\'</li>\\\\n\'\\n            resp += \\"</ul>\\\\n\\"\\n        resp += \\"\\"\\"\\n            <h2>Connected Devices</h2>\\n            <p>The list of connected USB devices as seen by the posbox</p>\\n        \\"\\"\\"\\n        devices = commands.getoutput(\\"lsusb\\").split(\'\\\\n\')\\n        count   = 0\\n        resp += \\"<div class=\'devices\'>\\\\n\\"\\n        for device in devices:\\n            device_name = device[device.find(\'ID\')+2:]\\n            device_id   = device_name.split()[0]\\n            if not (device_id in BANNED_DEVICES):\\n            \\tresp+= \\"<div class=\'device\' data-device=\'\\"+device+\\"\'>\\"+device_name+\\"</div>\\\\n\\"\\n                count += 1\\n        \\n        if count == 0:\\n            resp += \\"<div class=\'device\'>No USB Device Found</div>\\"\\n\\n        resp += \\"</div>\\\\n</body>\\\\n</html>\\\\n\\\\n\\"\\n\\n        return request.make_response(resp,{\\n            \'Cache-Control\': \'no-cache\', \\n            \'Content-Type\': \'text/html; charset=utf-8\',\\n            \'Access-Control-Allow-Origin\':  \'*\',\\n            \'Access-Control-Allow-Methods\': \'GET\',\\n          })\\n\\n    @http.route(\'/hw_proxy/status_json\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def status_json(self):\\n        return self.get_status()\\n\\n    @http.route(\'/hw_proxy/scan_item_success\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_success(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned with success\\n        \\"\\"\\"\\n        print \'scan_item_success: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/scan_item_error_unrecognized\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_error_unrecognized(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned without success\\n        \\"\\"\\"\\n        print \'scan_item_error_unrecognized: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/help_needed\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_needed(self):\\n        \\"\\"\\"\\n        The user wants an help (ex: light is on)\\n        \\"\\"\\"\\n        print \\"help_needed\\"\\n\\n    @http.route(\'/hw_proxy/help_canceled\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_canceled(self):\\n        \\"\\"\\"\\n        The user stops the help request\\n        \\"\\"\\"\\n        print \\"help_canceled\\"\\n\\n    @http.route(\'/hw_proxy/payment_request\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_request(self, price):\\n        \\"\\"\\"\\n        The PoS will activate the method payment \\n        \\"\\"\\"\\n        print \\"payment_request: price:\\"+str(price)\\n        return \'ok\'\\n\\n    @http.route(\'/hw_proxy/payment_status\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_status(self):\\n        print \\"payment_status\\"\\n        return { \'status\':\'waiting\' } \\n\\n    @http.route(\'/hw_proxy/payment_cancel\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_cancel(self):\\n        print \\"payment_cancel\\"\\n\\n    @http.route(\'/hw_proxy/transaction_start\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_start(self):\\n        print \'transaction_start\'\\n\\n    @http.route(\'/hw_proxy/transaction_end\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_end(self):\\n        print \'transaction_end\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_activated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_activated(self):\\n        print \'cashier_mode_activated\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_deactivated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_deactivated(self):\\n        print \'cashier_mode_deactivated\'\\n\\n    @http.route(\'/hw_proxy/open_cashbox\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def open_cashbox(self):\\n        print \'open_cashbox\'\\n\\n    @http.route(\'/hw_proxy/print_receipt\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_receipt(self, receipt):\\n        print \'print_receipt\' + str(receipt)\\n\\n    @http.route(\'/hw_proxy/is_scanner_connected\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def is_scanner_connected(self, receipt):\\n        print \'is_scanner_connected?\' \\n        return False\\n\\n    @http.route(\'/hw_proxy/scanner\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scanner(self, receipt):\\n        print \'scanner\' \\n        time.sleep(10)\\n        return \'\'\\n\\n    @http.route(\'/hw_proxy/log\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def log(self, arguments):\\n        _logger.info(\' \'.join(str(v) for v in arguments))\\n\\n    @http.route(\'/hw_proxy/print_pdf_invoice\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_pdf_invoice(self, pdfinvoice):\\n        print \'print_pdf_invoice\' + str(pdfinvoice)\\n" }\n'
line: b'{ "repo_name": "Fl0rianFischer/sme_odoo", "ref": "refs/heads/9.0", "path": "addons/hw_proxy/controllers/main.py", "content": "# -*- coding: utf-8 -*-\\nimport logging\\nimport commands\\nimport json\\nimport os\\nimport os.path\\nimport openerp\\nimport time\\nimport random\\nimport subprocess\\nimport json\\nimport werkzeug\\nimport werkzeug.wrappers\\n_logger = logging.getLogger(__name__)\\n\\n\\nfrom openerp import http\\nfrom openerp.http import request\\n\\n# Those are the builtin raspberry pi USB modules, they should\\n# not appear in the list of connected devices.\\nBANNED_DEVICES = set([\\n\\t\\"0424:9514\\",\\t# Standard Microsystem Corp. Builtin Ethernet module\\n\\t\\"1d6b:0002\\",\\t# Linux Foundation 2.0 root hub\\n\\t\\"0424:ec00\\",\\t# Standard Microsystem Corp. Other Builtin Ethernet module\\n])\\n\\n\\n# drivers modules must add to drivers an object with a get_status() method \\n# so that \'status\' can return the status of all active drivers\\ndrivers = {}\\n\\nclass Proxy(http.Controller):\\n\\n    def get_status(self):\\n        statuses = {}\\n        for driver in drivers:\\n            statuses[driver] = drivers[driver].get_status()\\n        return statuses\\n\\n    @http.route(\'/hw_proxy/hello\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def hello(self):\\n        return \\"ping\\"\\n\\n    @http.route(\'/hw_proxy/handshake\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def handshake(self):\\n        return True\\n\\n    @http.route(\'/hw_proxy/status\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def status_http(self):\\n        resp = \\"\\"\\"\\n<!DOCTYPE HTML>\\n<html>\\n    <head>\\n        <title>Odoo\'s PosBox</title>\\n        <style>\\n        body {\\n            width: 480px;\\n            margin: 60px auto;\\n            font-family: sans-serif;\\n            text-align: justify;\\n            color: #6B6B6B;\\n      }\\n        .device {\\n            border-bottom: solid 1px rgb(216,216,216);\\n            padding: 9px;\\n      }\\n        .device:nth-child(2n) {\\n            background:rgb(240,240,240);\\n      }\\n        </style>\\n    </head>\\n    <body>\\n        <h1>Hardware Status</h1>\\n        <p>The list of enabled drivers and their status</p>\\n\\"\\"\\"\\n        statuses = self.get_status()\\n        for driver in statuses:\\n\\n            status = statuses[driver]\\n\\n            if status[\'status\'] == \'connecting\':\\n                color = \'black\'\\n            elif status[\'status\'] == \'connected\':\\n                color = \'green\'\\n            else:\\n                color = \'red\'\\n\\n            resp += \\"<h3 style=\'color:\\"+color+\\";\'>\\"+driver+\' : \'+status[\'status\']+\\"</h3>\\\\n\\"\\n            resp += \\"<ul>\\\\n\\"\\n            for msg in status[\'messages\']:\\n                resp += \'<li>\'+msg+\'</li>\\\\n\'\\n            resp += \\"</ul>\\\\n\\"\\n        resp += \\"\\"\\"\\n            <h2>Connected Devices</h2>\\n            <p>The list of connected USB devices as seen by the posbox</p>\\n        \\"\\"\\"\\n        devices = commands.getoutput(\\"lsusb\\").split(\'\\\\n\')\\n        count   = 0\\n        resp += \\"<div class=\'devices\'>\\\\n\\"\\n        for device in devices:\\n            device_name = device[device.find(\'ID\')+2:]\\n            device_id   = device_name.split()[0]\\n            if not (device_id in BANNED_DEVICES):\\n            \\tresp+= \\"<div class=\'device\' data-device=\'\\"+device+\\"\'>\\"+device_name+\\"</div>\\\\n\\"\\n                count += 1\\n        \\n        if count == 0:\\n            resp += \\"<div class=\'device\'>No USB Device Found</div>\\"\\n\\n        resp += \\"</div>\\\\n</body>\\\\n</html>\\\\n\\\\n\\"\\n\\n        return request.make_response(resp,{\\n            \'Cache-Control\': \'no-cache\', \\n            \'Content-Type\': \'text/html; charset=utf-8\',\\n            \'Access-Control-Allow-Origin\':  \'*\',\\n            \'Access-Control-Allow-Methods\': \'GET\',\\n          })\\n\\n    @http.route(\'/hw_proxy/status_json\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def status_json(self):\\n        return self.get_status()\\n\\n    @http.route(\'/hw_proxy/scan_item_success\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_success(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned with success\\n        \\"\\"\\"\\n        print \'scan_item_success: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/scan_item_error_unrecognized\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_error_unrecognized(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned without success\\n        \\"\\"\\"\\n        print \'scan_item_error_unrecognized: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/help_needed\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_needed(self):\\n        \\"\\"\\"\\n        The user wants an help (ex: light is on)\\n        \\"\\"\\"\\n        print \\"help_needed\\"\\n\\n    @http.route(\'/hw_proxy/help_canceled\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_canceled(self):\\n        \\"\\"\\"\\n        The user stops the help request\\n        \\"\\"\\"\\n        print \\"help_canceled\\"\\n\\n    @http.route(\'/hw_proxy/payment_request\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_request(self, price):\\n        \\"\\"\\"\\n        The PoS will activate the method payment \\n        \\"\\"\\"\\n        print \\"payment_request: price:\\"+str(price)\\n        return \'ok\'\\n\\n    @http.route(\'/hw_proxy/payment_status\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_status(self):\\n        print \\"payment_status\\"\\n        return { \'status\':\'waiting\' } \\n\\n    @http.route(\'/hw_proxy/payment_cancel\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_cancel(self):\\n        print \\"payment_cancel\\"\\n\\n    @http.route(\'/hw_proxy/transaction_start\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_start(self):\\n        print \'transaction_start\'\\n\\n    @http.route(\'/hw_proxy/transaction_end\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_end(self):\\n        print \'transaction_end\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_activated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_activated(self):\\n        print \'cashier_mode_activated\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_deactivated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_deactivated(self):\\n        print \'cashier_mode_deactivated\'\\n\\n    @http.route(\'/hw_proxy/open_cashbox\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def open_cashbox(self):\\n        print \'open_cashbox\'\\n\\n    @http.route(\'/hw_proxy/print_receipt\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_receipt(self, receipt):\\n        print \'print_receipt\' + str(receipt)\\n\\n    @http.route(\'/hw_proxy/is_scanner_connected\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def is_scanner_connected(self, receipt):\\n        print \'is_scanner_connected?\' \\n        return False\\n\\n    @http.route(\'/hw_proxy/scanner\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scanner(self, receipt):\\n        print \'scanner\' \\n        time.sleep(10)\\n        return \'\'\\n\\n    @http.route(\'/hw_proxy/log\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def log(self, arguments):\\n        _logger.info(\' \'.join(str(v) for v in arguments))\\n\\n    @http.route(\'/hw_proxy/print_pdf_invoice\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_pdf_invoice(self, pdfinvoice):\\n        print \'print_pdf_invoice\' + str(pdfinvoice)\\n" }\n'
line: b'{ "repo_name": "rcomer/iris", "ref": "refs/heads/placeholder", "path": "lib/iris/tests/integration/test_regridding.py", "content": "# (C) British Crown Copyright 2013 - 2015, Met Office\\n#\\n# This file is part of Iris.\\n#\\n# Iris is free software: you can redistribute it and/or modify it under\\n# the terms of the GNU Lesser General Public License as published by the\\n# Free Software Foundation, either version 3 of the License, or\\n# (at your option) any later version.\\n#\\n# Iris is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU Lesser General Public License for more details.\\n#\\n# You should have received a copy of the GNU Lesser General Public License\\n# along with Iris.  If not, see <http://www.gnu.org/licenses/>.\\n\\"\\"\\"Integration tests for regridding.\\"\\"\\"\\n\\nfrom __future__ import (absolute_import, division, print_function)\\nfrom six.moves import (filter, input, map, range, zip)  # noqa\\n\\n# Import iris.tests first so that some things can be initialised before\\n# importing anything else.\\nimport iris.tests as tests\\n\\nimport numpy as np\\n\\nimport iris\\nfrom iris.analysis._regrid import RectilinearRegridder as Regridder\\nfrom iris.coord_systems import GeogCS\\nfrom iris.coords import DimCoord\\nfrom iris.cube import Cube\\nfrom iris.tests.stock import global_pp\\n\\n# Run tests in no graphics mode if matplotlib is not available.\\nif tests.MPL_AVAILABLE:\\n    import iris.quickplot as qplt\\n\\n\\n@tests.skip_data\\n@tests.skip_plot\\nclass TestOSGBToLatLon(tests.GraphicsTest):\\n    def setUp(self):\\n        path = tests.get_data_path(\\n            (\'NIMROD\', \'uk2km\', \'WO0000000003452\',\\n             \'201007020900_u1096_ng_ey00_visibility0180_screen_2km\'))\\n        self.src = iris.load_cube(path)[0]\\n        self.src.data = self.src.data.astype(np.float32)\\n        self.grid = Cube(np.empty((73, 96)))\\n        cs = GeogCS(6370000)\\n        lat = DimCoord(np.linspace(46, 65, 73), \'latitude\', units=\'degrees\',\\n                       coord_system=cs)\\n        lon = DimCoord(np.linspace(-14, 8, 96), \'longitude\', units=\'degrees\',\\n                       coord_system=cs)\\n        self.grid.add_dim_coord(lat, 0)\\n        self.grid.add_dim_coord(lon, 1)\\n\\n    def _regrid(self, method):\\n        regridder = Regridder(self.src, self.grid, method, \'mask\')\\n        result = regridder(self.src)\\n        qplt.pcolor(result, antialiased=False)\\n        qplt.plt.gca().coastlines()\\n\\n    def test_linear(self):\\n        self._regrid(\'linear\')\\n        self.check_graphic()\\n\\n    def test_nearest(self):\\n        self._regrid(\'nearest\')\\n        self.check_graphic()\\n\\n\\n@tests.skip_data\\n@tests.skip_plot\\nclass TestGlobalSubsample(tests.GraphicsTest):\\n    def setUp(self):\\n        self.src = global_pp()\\n        # Subsample and shift the target grid so that we can see a visual\\n        # difference between regridding scheme methods.\\n        grid = self.src[1::2, 1::3]\\n        grid.coord(\'latitude\').points = grid.coord(\'latitude\').points + 1\\n        grid.coord(\'longitude\').points = grid.coord(\'longitude\').points + 1\\n        self.grid = grid\\n\\n    def _regrid(self, method):\\n        regridder = Regridder(self.src, self.grid, method, \'mask\')\\n        result = regridder(self.src)\\n        qplt.pcolormesh(result)\\n        qplt.plt.gca().coastlines()\\n\\n    def test_linear(self):\\n        self._regrid(\'linear\')\\n        self.check_graphic()\\n\\n    def test_nearest(self):\\n        self._regrid(\'nearest\')\\n        self.check_graphic()\\n\\n\\nif __name__ == \\"__main__\\":\\n    tests.main()\\n" }\n'
line: b'{ "repo_name": "libracore/erpnext", "ref": "refs/heads/v12", "path": "erpnext/healthcare/doctype/healthcare_settings/healthcare_settings.py", "content": "# -*- coding: utf-8 -*-\\n# Copyright (c) 2017, Frappe Technologies Pvt. Ltd. and contributors\\n# For license information, please see license.txt\\n\\nfrom __future__ import unicode_literals\\nimport frappe\\nfrom frappe import _\\nfrom frappe.model.document import Document\\nfrom frappe.core.doctype.sms_settings.sms_settings import send_sms\\nimport json\\n\\nclass HealthcareSettings(Document):\\n\\tdef validate(self):\\n\\t\\tfor key in [\\"collect_registration_fee\\",\\"manage_customer\\",\\"patient_master_name\\",\\n\\t\\t\\"require_test_result_approval\\",\\"require_sample_collection\\", \\"default_medical_code_standard\\"]:\\n\\t\\t\\tfrappe.db.set_default(key, self.get(key, \\"\\"))\\n\\t\\tif(self.collect_registration_fee):\\n\\t\\t\\tif self.registration_fee <= 0 :\\n\\t\\t\\t\\tfrappe.throw(_(\\"Registration fee can not be Zero\\"))\\n\\t\\tif self.inpatient_visit_charge_item:\\n\\t\\t\\tvalidate_service_item(self.inpatient_visit_charge_item, \\"Configure a service Item for Inpatient Visit Charge Item\\")\\n\\t\\tif self.op_consulting_charge_item:\\n\\t\\t\\tvalidate_service_item(self.op_consulting_charge_item, \\"Configure a service Item for Out Patient Consulting Charge Item\\")\\n\\t\\tif self.clinical_procedure_consumable_item:\\n\\t\\t\\tvalidate_service_item(self.clinical_procedure_consumable_item, \\"Configure a service Item for Clinical Procedure Consumable Item\\")\\n\\n@frappe.whitelist()\\ndef get_sms_text(doc):\\n    sms_text = {}\\n    doc = frappe.get_doc(\\"Lab Test\\",doc)\\n    #doc = json.loads(doc)\\n    context = {\\"doc\\": doc, \\"alert\\": doc, \\"comments\\": None}\\n    emailed = frappe.db.get_value(\\"Healthcare Settings\\", None, \\"sms_emailed\\")\\n    sms_text[\'emailed\'] = frappe.render_template(emailed, context)\\n    printed = frappe.db.get_value(\\"Healthcare Settings\\", None, \\"sms_printed\\")\\n    sms_text[\'printed\'] = frappe.render_template(printed, context)\\n    return sms_text\\n\\ndef send_registration_sms(doc):\\n    if (frappe.db.get_value(\\"Healthcare Settings\\", None, \\"reg_sms\\")==\'1\'):\\n        if doc.mobile:\\n            context = {\\"doc\\": doc, \\"alert\\": doc, \\"comments\\": None}\\n            if doc.get(\\"_comments\\"):\\n                context[\\"comments\\"] = json.loads(doc.get(\\"_comments\\"))\\n            messages = frappe.db.get_value(\\"Healthcare Settings\\", None, \\"reg_msg\\")\\n            messages = frappe.render_template(messages, context)\\n            number = [doc.mobile]\\n            send_sms(number,messages)\\n        else:\\n            frappe.msgprint(doc.name + \\" Has no mobile number to send registration SMS\\", alert=True)\\n\\n\\ndef get_receivable_account(company):\\n    receivable_account = get_account(None, \\"receivable_account\\", \\"Healthcare Settings\\", company)\\n    if receivable_account:\\n        return receivable_account\\n    return frappe.get_cached_value(\'Company\',  company,  \\"default_receivable_account\\")\\n\\ndef get_income_account(practitioner, company):\\n    if(practitioner):\\n        income_account = get_account(\\"Healthcare Practitioner\\", None, practitioner, company)\\n        if income_account:\\n            return income_account\\n    income_account = get_account(None, \\"income_account\\", \\"Healthcare Settings\\", company)\\n    if income_account:\\n        return income_account\\n    return frappe.get_cached_value(\'Company\',  company,  \\"default_income_account\\")\\n\\ndef get_account(parent_type, parent_field, parent, company):\\n    if(parent_type):\\n        return frappe.db.get_value(\\"Party Account\\",\\n          {\\"parenttype\\": parent_type, \\"parent\\": parent, \\"company\\": company} \\"account\\")\\n    if(parent_field):\\n        return frappe.db.get_value(\\"Party Account\\",\\n          {\\"parentfield\\": parent_field, \\"parent\\": parent, \\"company\\": company} \\"account\\")\\n\\ndef validate_service_item(item, msg):\\n\\tif frappe.db.get_value(\\"Item\\", item, \\"is_stock_item\\") == 1:\\n\\t\\tfrappe.throw(_(msg))\\n" }\n'
line: b'{ "repo_name": "syci/OCB", "ref": "refs/heads/9.0", "path": "addons/hw_proxy/controllers/main.py", "content": "# -*- coding: utf-8 -*-\\nimport logging\\nimport commands\\nimport json\\nimport os\\nimport os.path\\nimport openerp\\nimport time\\nimport random\\nimport subprocess\\nimport json\\nimport werkzeug\\nimport werkzeug.wrappers\\n_logger = logging.getLogger(__name__)\\n\\n\\nfrom openerp import http\\nfrom openerp.http import request\\n\\n# Those are the builtin raspberry pi USB modules, they should\\n# not appear in the list of connected devices.\\nBANNED_DEVICES = set([\\n\\t\\"0424:9514\\",\\t# Standard Microsystem Corp. Builtin Ethernet module\\n\\t\\"1d6b:0002\\",\\t# Linux Foundation 2.0 root hub\\n\\t\\"0424:ec00\\",\\t# Standard Microsystem Corp. Other Builtin Ethernet module\\n])\\n\\n\\n# drivers modules must add to drivers an object with a get_status() method \\n# so that \'status\' can return the status of all active drivers\\ndrivers = {}\\n\\nclass Proxy(http.Controller):\\n\\n    def get_status(self):\\n        statuses = {}\\n        for driver in drivers:\\n            statuses[driver] = drivers[driver].get_status()\\n        return statuses\\n\\n    @http.route(\'/hw_proxy/hello\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def hello(self):\\n        return \\"ping\\"\\n\\n    @http.route(\'/hw_proxy/handshake\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def handshake(self):\\n        return True\\n\\n    @http.route(\'/hw_proxy/status\', type=\'http\', auth=\'none\', cors=\'*\')\\n    def status_http(self):\\n        resp = \\"\\"\\"\\n<!DOCTYPE HTML>\\n<html>\\n    <head>\\n        <title>Odoo\'s PosBox</title>\\n        <style>\\n        body {\\n            width: 480px;\\n            margin: 60px auto;\\n            font-family: sans-serif;\\n            text-align: justify;\\n            color: #6B6B6B;\\n      }\\n        .device {\\n            border-bottom: solid 1px rgb(216,216,216);\\n            padding: 9px;\\n      }\\n        .device:nth-child(2n) {\\n            background:rgb(240,240,240);\\n      }\\n        </style>\\n    </head>\\n    <body>\\n        <h1>Hardware Status</h1>\\n        <p>The list of enabled drivers and their status</p>\\n\\"\\"\\"\\n        statuses = self.get_status()\\n        for driver in statuses:\\n\\n            status = statuses[driver]\\n\\n            if status[\'status\'] == \'connecting\':\\n                color = \'black\'\\n            elif status[\'status\'] == \'connected\':\\n                color = \'green\'\\n            else:\\n                color = \'red\'\\n\\n            resp += \\"<h3 style=\'color:\\"+color+\\";\'>\\"+driver+\' : \'+status[\'status\']+\\"</h3>\\\\n\\"\\n            resp += \\"<ul>\\\\n\\"\\n            for msg in status[\'messages\']:\\n                resp += \'<li>\'+msg+\'</li>\\\\n\'\\n            resp += \\"</ul>\\\\n\\"\\n        resp += \\"\\"\\"\\n            <h2>Connected Devices</h2>\\n            <p>The list of connected USB devices as seen by the posbox</p>\\n        \\"\\"\\"\\n        devices = commands.getoutput(\\"lsusb\\").split(\'\\\\n\')\\n        count   = 0\\n        resp += \\"<div class=\'devices\'>\\\\n\\"\\n        for device in devices:\\n            device_name = device[device.find(\'ID\')+2:]\\n            device_id   = device_name.split()[0]\\n            if not (device_id in BANNED_DEVICES):\\n            \\tresp+= \\"<div class=\'device\' data-device=\'\\"+device+\\"\'>\\"+device_name+\\"</div>\\\\n\\"\\n                count += 1\\n        \\n        if count == 0:\\n            resp += \\"<div class=\'device\'>No USB Device Found</div>\\"\\n\\n        resp += \\"</div>\\\\n</body>\\\\n</html>\\\\n\\\\n\\"\\n\\n        return request.make_response(resp,{\\n            \'Cache-Control\': \'no-cache\', \\n            \'Content-Type\': \'text/html; charset=utf-8\',\\n            \'Access-Control-Allow-Origin\':  \'*\',\\n            \'Access-Control-Allow-Methods\': \'GET\',\\n          })\\n\\n    @http.route(\'/hw_proxy/status_json\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def status_json(self):\\n        return self.get_status()\\n\\n    @http.route(\'/hw_proxy/scan_item_success\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_success(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned with success\\n        \\"\\"\\"\\n        print \'scan_item_success: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/scan_item_error_unrecognized\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scan_item_error_unrecognized(self, ean):\\n        \\"\\"\\"\\n        A product has been scanned without success\\n        \\"\\"\\"\\n        print \'scan_item_error_unrecognized: \' + str(ean)\\n\\n    @http.route(\'/hw_proxy/help_needed\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_needed(self):\\n        \\"\\"\\"\\n        The user wants an help (ex: light is on)\\n        \\"\\"\\"\\n        print \\"help_needed\\"\\n\\n    @http.route(\'/hw_proxy/help_canceled\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def help_canceled(self):\\n        \\"\\"\\"\\n        The user stops the help request\\n        \\"\\"\\"\\n        print \\"help_canceled\\"\\n\\n    @http.route(\'/hw_proxy/payment_request\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_request(self, price):\\n        \\"\\"\\"\\n        The PoS will activate the method payment \\n        \\"\\"\\"\\n        print \\"payment_request: price:\\"+str(price)\\n        return \'ok\'\\n\\n    @http.route(\'/hw_proxy/payment_status\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_status(self):\\n        print \\"payment_status\\"\\n        return { \'status\':\'waiting\' } \\n\\n    @http.route(\'/hw_proxy/payment_cancel\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def payment_cancel(self):\\n        print \\"payment_cancel\\"\\n\\n    @http.route(\'/hw_proxy/transaction_start\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_start(self):\\n        print \'transaction_start\'\\n\\n    @http.route(\'/hw_proxy/transaction_end\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def transaction_end(self):\\n        print \'transaction_end\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_activated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_activated(self):\\n        print \'cashier_mode_activated\'\\n\\n    @http.route(\'/hw_proxy/cashier_mode_deactivated\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def cashier_mode_deactivated(self):\\n        print \'cashier_mode_deactivated\'\\n\\n    @http.route(\'/hw_proxy/open_cashbox\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def open_cashbox(self):\\n        print \'open_cashbox\'\\n\\n    @http.route(\'/hw_proxy/print_receipt\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_receipt(self, receipt):\\n        print \'print_receipt\' + str(receipt)\\n\\n    @http.route(\'/hw_proxy/is_scanner_connected\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def is_scanner_connected(self, receipt):\\n        print \'is_scanner_connected?\' \\n        return False\\n\\n    @http.route(\'/hw_proxy/scanner\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def scanner(self, receipt):\\n        print \'scanner\' \\n        time.sleep(10)\\n        return \'\'\\n\\n    @http.route(\'/hw_proxy/log\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def log(self, arguments):\\n        _logger.info(\' \'.join(str(v) for v in arguments))\\n\\n    @http.route(\'/hw_proxy/print_pdf_invoice\', type=\'json\', auth=\'none\', cors=\'*\')\\n    def print_pdf_invoice(self, pdfinvoice):\\n        print \'print_pdf_invoice\' + str(pdfinvoice)\\n" }\n'
line: b'{ "repo_name": "ClearCorp/odoo-costa-rica", "ref": "refs/heads/9.0", "path": "TODO-7.0/l10n_cr_account_financial_report_webkit/__init__.py", "content": "# -*- coding: utf-8 -*-\\n##############################################################################\\n#\\n#    OpenERP, Open Source Management Solution\\n#    Addons modules by CLEARCORP S.A.\\n#    Copyright (C) 2009-TODAY CLEARCORP S.A. (<http://clearcorp.co.cr>).\\n#\\n#    This program is free software: you can redistribute it and/or modify\\n#    it under the terms of the GNU Affero General Public License as\\n#    published by the Free Software Foundation, either version 3 of the\\n#    License, or (at your option) any later version.\\n#\\n#    This program is distributed in the hope that it will be useful,\\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n#    GNU Affero General Public License for more details.\\n#\\n#    You should have received a copy of the GNU Affero General Public License\\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\\n#\\n##############################################################################\\n\\nimport account\\nfrom . import wizard\\nfrom . import report\\nfrom . import account_account\\n" }\n'
line: b'{ "repo_name": "vaisaghvt/django-bot-server-tutorial", "ref": "refs/heads/websockets", "path": "chatbot_tutorial/urls.py", "content": "\\"\\"\\"chatbot_tutorial URL Configuration\\n\\nThe `urlpatterns` list routes URLs to views. For more information please see:\\n    https://docs.djangoproject.com/en/1.10/topics/http/urls/\\nExamples:\\nFunction views\\n    1. Add an import:  from my_app import views\\n    2. Add a URL to urlpatterns:  url(r\'^$\', views.home, name=\'home\')\\nClass-based views\\n    1. Add an import:  from other_app.views import Home\\n    2. Add a URL to urlpatterns:  url(r\'^$\', Home.as_view(), name=\'home\')\\nIncluding another URLconf\\n    1. Import the include() function: from django.conf.urls import url, include\\n    2. Add a URL to urlpatterns:  url(r\'^blog/\', include(\'blog.urls\'))\\n\\"\\"\\"\\nfrom django.conf.urls import url\\nfrom django.contrib import admin\\nfrom .views import chat\\n\\nurlpatterns = [\\n\\turl(r\'^chat/$\', chat, name=\'chat\'),\\n    url(r\'^admin/\', admin.site.urls)\\n]\\n" }\n'
line: b'{ "repo_name": "michael-dev2rights/ansible", "ref": "refs/heads/ansible-d2r", "path": "lib/ansible/modules/cloud/vmware/vca_fw.py", "content": "#!/usr/bin/python\\n\\n# Copyright (c) 2015 VMware, Inc. All Rights Reserved.\\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\\n\\nfrom __future__ import absolute_import, division, print_function\\n__metaclass__ = type\\n\\n\\nANSIBLE_METADATA = {\'metadata_version\': \'1.1\',\\n                    \'status\': [\'preview\'],\\n                    \'supported_by\': \'community\'}\\n\\n\\nDOCUMENTATION = \'\'\'\\n---\\nmodule: vca_fw\\nshort_description: add remove firewall rules in a gateway  in a vca\\ndescription:\\n  - Adds or removes firewall rules from a gateway in a vca environment\\nversion_added: \\"2.0\\"\\nauthor: Peter Sprygada (@privateip)\\noptions:\\n    fw_rules:\\n      description:\\n        - A list of firewall rules to be added to the gateway, Please see examples on valid entries\\n      required: True\\n      default: false\\nextends_documentation_fragment: vca.documentation\\n\'\'\'\\n\\nEXAMPLES = \'\'\'\\n\\n#Add a set of firewall rules\\n\\n- hosts: localhost\\n  connection: local\\n  tasks:\\n   - vca_fw:\\n       instance_id: \'b15ff1e5-1024-4f55-889f-ea0209726282\'\\n       vdc_name: \'benz_ansible\'\\n       state: \'absent\'\\n       fw_rules:\\n         - description: \\"ben testing\\"\\n           source_ip: \\"Any\\"\\n           dest_ip: 192.0.2.23\\n         - description: \\"ben testing 2\\"\\n           source_ip: 192.0.2.50\\n           source_port: \\"Any\\"\\n           dest_port: \\"22\\"\\n           dest_ip: 192.0.2.101\\n           is_enable: \\"true\\"\\n           enable_logging: \\"false\\"\\n           protocol: \\"Tcp\\"\\n           policy: \\"allow\\"\\n\\n\'\'\'\\n\\ntry:\\n    from pyvcloud.schema.vcd.v1_5.schemas.vcloud.networkType import FirewallRuleType\\n    from pyvcloud.schema.vcd.v1_5.schemas.vcloud.networkType import ProtocolsType\\nexcept ImportError:\\n    # normally set a flag here but it will be caught when testing for\\n    # the existence of pyvcloud (see module_utils/vca.py).  This just\\n    # protects against generating an exception at runtime\\n    pass\\n\\nfrom ansible.module_utils.basic import AnsibleModule\\nfrom ansible.module_utils.vca import VcaError, vca_argument_spec, vca_login\\n\\n\\nVALID_PROTO = [\'Tcp\', \'Udp\', \'Icmp\', \'Other\', \'Any\']\\nVALID_RULE_KEYS = [\'policy\', \'is_enable\', \'enable_logging\', \'description\',\\n                   \'dest_ip\', \'dest_port\', \'source_ip\', \'source_port\',\\n                   \'protocol\']\\n\\n\\ndef protocol_to_tuple(protocol):\\n    return (protocol.get_Tcp(),\\n            protocol.get_Udp(),\\n            protocol.get_Icmp(),\\n            protocol.get_Other(),\\n            protocol.get_Any())\\n\\ndef protocol_to_string(protocol):\\n    protocol = protocol_to_tuple(protocol)\\n    if protocol[0] is True:\\n        return \'Tcp\'\\n    elif protocol[1] is True:\\n        return \'Udp\'\\n    elif protocol[2] is True:\\n        return \'Icmp\'\\n    elif protocol[3] is True:\\n        return \'Other\'\\n    elif protocol[4] is True:\\n        return \'Any\'\\n\\ndef protocol_to_type(protocol):\\n    try:\\n        protocols = ProtocolsType()\\n        setattr(protocols, protocol, True)\\n        return protocols\\n    except AttributeError:\\n        raise VcaError(\\"The value in protocol is not valid\\")\\n\\ndef validate_fw_rules(fw_rules):\\n    for rule in fw_rules:\\n        for k in rule.keys():\\n            if k not in VALID_RULE_KEYS:\\n                raise VcaError(\\"%s is not a valid key in fw rules, please \\"\\n                               \\"check above..\\" % k, valid_keys=VALID_RULE_KEYS)\\n\\n        rule[\'dest_port\'] = str(rule.get(\'dest_port\', \'Any\')).lower()\\n        rule[\'dest_ip\'] = rule.get(\'dest_ip\', \'Any\').lower()\\n        rule[\'source_port\'] = str(rule.get(\'source_port\', \'Any\')).lower()\\n        rule[\'source_ip\'] = rule.get(\'source_ip\', \'Any\').lower()\\n        rule[\'protocol\'] = rule.get(\'protocol\', \'Any\').lower()\\n        rule[\'policy\'] = rule.get(\'policy\', \'allow\').lower()\\n        rule[\'is_enable\'] = rule.get(\'is_enable\', True)\\n        rule[\'enable_logging\'] = rule.get(\'enable_logging\', False)\\n        rule[\'description\'] = rule.get(\'description\', \'rule added by Ansible\')\\n\\n    return fw_rules\\n\\ndef fw_rules_to_dict(rules):\\n    fw_rules = list()\\n    for rule in rules:\\n        fw_rules.append(\\n            dict(\\n                dest_port=rule.get_DestinationPortRange().lower(),\\n                dest_ip=rule.get_DestinationIp().lower().lower(),\\n                source_port=rule.get_SourcePortRange().lower(),\\n                source_ip=rule.get_SourceIp().lower(),\\n                protocol=protocol_to_string(rule.get_Protocols()).lower(),\\n                policy=rule.get_Policy().lower(),\\n                is_enable=rule.get_IsEnabled(),\\n                enable_logging=rule.get_EnableLogging(),\\n                description=rule.get_Description()\\n            )\\n        )\\n    return fw_rules\\n\\ndef create_fw_rule(is_enable, description, policy, protocol, dest_port,\\n                   dest_ip, source_port, source_ip, enable_logging):\\n\\n    return FirewallRuleType(IsEnabled=is_enable,\\n                            Description=description,\\n                            Policy=policy,\\n                            Protocols=protocol_to_type(protocol),\\n                            DestinationPortRange=dest_port,\\n                            DestinationIp=dest_ip,\\n                            SourcePortRange=source_port,\\n                            SourceIp=source_ip,\\n                            EnableLogging=enable_logging)\\n\\ndef main():\\n    argument_spec = vca_argument_spec()\\n    argument_spec.update(\\n        dict(\\n            fw_rules = dict(required=True, type=\'list\'),\\n            gateway_name = dict(default=\'gateway\'),\\n            state = dict(default=\'present\', choices=[\'present\', \'absent\'])\\n        )\\n    )\\n\\n    module = AnsibleModule(argument_spec, supports_check_mode=True)\\n\\n    fw_rules = module.params.get(\'fw_rules\')\\n    gateway_name = module.params.get(\'gateway_name\')\\n    vdc_name = module.params[\'vdc_name\']\\n\\n    vca = vca_login(module)\\n\\n    gateway = vca.get_gateway(vdc_name, gateway_name)\\n    if not gateway:\\n        module.fail_json(msg=\\"Not able to find the gateway %s, please check \\"\\n                             \\"the gateway_name param\\" % gateway_name)\\n\\n    fwservice = gateway._getFirewallService()\\n\\n    rules = gateway.get_fw_rules()\\n    current_rules = fw_rules_to_dict(rules)\\n\\n    try:\\n        desired_rules = validate_fw_rules(fw_rules)\\n    except VcaError as e:\\n        module.fail_json(msg=e.message)\\n\\n    result = dict(changed=False)\\n    result[\'current_rules\'] = current_rules\\n    result[\'desired_rules\'] = desired_rules\\n\\n    updates = list()\\n    additions = list()\\n    deletions = list()\\n\\n    for (index, rule) in enumerate(desired_rules):\\n        try:\\n            if rule != current_rules[index]:\\n                updates.append((index, rule))\\n        except IndexError:\\n            additions.append(rule)\\n\\n    eol = len(current_rules) - len(desired_rules)\\n    if eol > 0:\\n        for rule in current_rules[eol:]:\\n            deletions.append(rule)\\n\\n    for rule in additions:\\n        if not module.check_mode:\\n            rule[\'protocol\'] = rule[\'protocol\'].capitalize()\\n            gateway.add_fw_rule(**rule)\\n        result[\'changed\'] = True\\n\\n    for index, rule in updates:\\n        if not module.check_mode:\\n            rule = create_fw_rule(**rule)\\n            fwservice.replace_FirewallRule_at(index, rule)\\n        result[\'changed\'] = True\\n\\n    keys = [\'protocol\', \'dest_port\', \'dest_ip\', \'source_port\', \'source_ip\']\\n    for rule in deletions:\\n        if not module.check_mode:\\n            kwargs = dict([(k, v) for k, v in rule.items() if k in keys])\\n            kwargs[\'protocol\'] = protocol_to_string(kwargs[\'protocol\'])\\n            gateway.delete_fw_rule(**kwargs)\\n        result[\'changed\'] = True\\n\\n    if not module.check_mode and result[\'changed\'] is True:\\n        task = gateway.save_services_configuration()\\n        if task:\\n            vca.block_until_completed(task)\\n\\n    result[\'rules_updated\'] = len(updates)\\n    result[\'rules_added\'] = len(additions)\\n    result[\'rules_deleted\'] = len(deletions)\\n\\n    return module.exit_json(**result)\\n\\n\\nif __name__ == \'__main__\':\\n    main()\\n" }\n'
line: b'{ "repo_name": "esikachev/scenario", "ref": "refs/heads/test_cases", "path": "sahara/plugins/mapr/services/hbase/hbase.py", "content": "# Copyright (c) 2015, MapR Technologies\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n# not use this file except in compliance with the License. You may obtain\\n# a copy of the License at\\n#\\n#       http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n# License for the specific language governing permissions and limitations\\n# under the License.\\n\\n\\nimport six\\n\\nimport sahara.plugins.mapr.domain.configuration_file as bcf\\nimport sahara.plugins.mapr.domain.node_process as np\\nimport sahara.plugins.mapr.domain.service as s\\nimport sahara.plugins.mapr.util.validation_utils as vu\\n\\n\\nHBASE_MASTER = np.NodeProcess(\\n    name=\'hbmaster\',\\n    ui_name=\'HBase-Master\',\\n    package=\'mapr-hbase-master\',\\n    open_ports=[60000, 60010]\\n)\\nHBASE_REGION_SERVER = np.NodeProcess(\\n    name=\'hbregionserver\',\\n    ui_name=\'HBase-RegionServer\',\\n    package=\'mapr-hbase-regionserver\',\\n    open_ports=[60020]\\n)\\nHBASE_THRIFT = np.NodeProcess(\\n    name=\'hbasethrift\',\\n    ui_name=\'HBase-Thrift\',\\n    package=\'mapr-hbasethrift\',\\n    open_ports=[9090]\\n)\\n\\n\\nclass HBase(s.Service):\\n    def __init__(self):\\n        super(HBase, self).__init__()\\n        self._name = \'hbase\'\\n        self._ui_name = \'HBase\'\\n        self._node_processes = [\\n            HBASE_MASTER,\\n            HBASE_REGION_SERVER,\\n            HBASE_THRIFT,\\n        ]\\n        self._cluster_defaults = [\'hbase-default.json\']\\n        self._validation_rules = [\\n            vu.at_least(1, HBASE_MASTER),\\n            vu.at_least(1, HBASE_REGION_SERVER),\\n        ]\\n\\n    def get_config_files(self, cluster_context, configs, instance=None):\\n        hbase_site = bcf.HadoopXML(\\"hbase-site.xml\\")\\n        hbase_site.remote_path = self.conf_dir(cluster_context)\\n        if instance:\\n            hbase_site.fetch(instance)\\n        hbase_site.load_properties(configs)\\n        return [hbase_site]\\n\\n\\n@six.add_metaclass(s.Single)\\nclass HBaseV094(HBase):\\n    def __init__(self):\\n        super(HBaseV094, self).__init__()\\n        self._version = \'0.94.24\'\\n        self._dependencies = [(\'mapr-hbase\', self.version)]\\n\\n\\n@six.add_metaclass(s.Single)\\nclass HBaseV098(HBase):\\n    def __init__(self):\\n        super(HBaseV098, self).__init__()\\n        self._version = \'0.98.7\'\\n        self._dependencies = [(\'mapr-hbase\', self.version)]\\n" }\n'
line: b'{ "repo_name": "lovelysystems/pyjamas", "ref": "refs/heads/ls-production", "path": "examples/showcase/src/demos_widgets/fileUpload.py", "content": "\\"\\"\\"\\nThe ``ui.FileUpload`` class implements a file uploader widget.\\n\\nThe FileUpload widget must be inside a ``ui.FormPanel`` which is used to submit\\nthe HTML form to the server.  Note that you must set the form encoding and\\nmethod like this:\\n\\n        self.form.setEncoding(FormPanel.ENCODING_MULTIPART)\\n        self.form.setMethod(FormPanel.METHOD_POST)\\n\\nThis will ensure that the form is submitted in a way that allows files to be\\nuploaded.\\n\\nThe example below doesn\'t really work, as there is no suitable server at\\n``nonexistent.com``.  However, it does show how a file upload widget could be\\nused within a FormPanel.\\n\\"\\"\\"\\nfrom pyjamas.ui.SimplePanel import SimplePanel\\nfrom pyjamas.ui.FormPanel import FormPanel\\nfrom pyjamas.ui.VerticalPanel import VerticalPanel\\nfrom pyjamas.ui.HorizontalPanel import HorizontalPanel\\nfrom pyjamas.ui.FileUpload import FileUpload\\nfrom pyjamas.ui.Label import Label\\nfrom pyjamas.ui.Button import Button\\n\\nclass FileUploadDemo(SimplePanel):\\n    def __init__(self):\\n        SimplePanel.__init__(self)\\n\\n        self.form = FormPanel()\\n        self.form.setEncoding(FormPanel.ENCODING_MULTIPART)\\n        self.form.setMethod(FormPanel.METHOD_POST)\\n        self.form.setAction(\\"http://nonexistent.com\\")\\n        self.form.setTarget(\\"results\\")\\n\\n        vPanel = VerticalPanel()\\n\\n        hPanel = HorizontalPanel()\\n        hPanel.setSpacing(5)\\n        hPanel.add(Label(\\"Upload file:\\"))\\n\\n        self.field = FileUpload()\\n        self.field.setName(\\"file\\")\\n        hPanel.add(self.field)\\n\\n        hPanel.add(Button(\\"Submit\\", getattr(self, \\"onBtnClick\\")))\\n\\n        vPanel.add(hPanel)\\n\\n        results = NamedFrame(\\"results\\")\\n        vPanel.add(results)\\n\\n        self.form.add(vPanel)\\n        self.add(self.form)\\n\\n\\n    def onBtnClick(self, event):\\n        self.form.submit()\\n\\n" }\n'
line: b'{ "repo_name": "redhat-openstack/sahara", "ref": "refs/heads/master-patches", "path": "sahara/plugins/mapr/services/hbase/hbase.py", "content": "# Copyright (c) 2015, MapR Technologies\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n# not use this file except in compliance with the License. You may obtain\\n# a copy of the License at\\n#\\n#       http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n# License for the specific language governing permissions and limitations\\n# under the License.\\n\\n\\nimport six\\n\\nimport sahara.plugins.mapr.domain.configuration_file as bcf\\nimport sahara.plugins.mapr.domain.node_process as np\\nimport sahara.plugins.mapr.domain.service as s\\nimport sahara.plugins.mapr.util.validation_utils as vu\\n\\n\\nHBASE_MASTER = np.NodeProcess(\\n    name=\'hbmaster\',\\n    ui_name=\'HBase-Master\',\\n    package=\'mapr-hbase-master\',\\n    open_ports=[60000, 60010]\\n)\\nHBASE_REGION_SERVER = np.NodeProcess(\\n    name=\'hbregionserver\',\\n    ui_name=\'HBase-RegionServer\',\\n    package=\'mapr-hbase-regionserver\',\\n    open_ports=[60020]\\n)\\nHBASE_THRIFT = np.NodeProcess(\\n    name=\'hbasethrift\',\\n    ui_name=\'HBase-Thrift\',\\n    package=\'mapr-hbasethrift\',\\n    open_ports=[9090]\\n)\\n\\n\\nclass HBase(s.Service):\\n    def __init__(self):\\n        super(HBase, self).__init__()\\n        self._name = \'hbase\'\\n        self._ui_name = \'HBase\'\\n        self._node_processes = [\\n            HBASE_MASTER,\\n            HBASE_REGION_SERVER,\\n            HBASE_THRIFT,\\n        ]\\n        self._cluster_defaults = [\'hbase-default.json\']\\n        self._validation_rules = [\\n            vu.at_least(1, HBASE_MASTER),\\n            vu.at_least(1, HBASE_REGION_SERVER),\\n        ]\\n\\n    def get_config_files(self, cluster_context, configs, instance=None):\\n        hbase_site = bcf.HadoopXML(\\"hbase-site.xml\\")\\n        hbase_site.remote_path = self.conf_dir(cluster_context)\\n        if instance:\\n            hbase_site.fetch(instance)\\n        hbase_site.load_properties(configs)\\n        return [hbase_site]\\n\\n\\n@six.add_metaclass(s.Single)\\nclass HBaseV094(HBase):\\n    def __init__(self):\\n        super(HBaseV094, self).__init__()\\n        self._version = \'0.94.24\'\\n        self._dependencies = [(\'mapr-hbase\', self.version)]\\n\\n\\n@six.add_metaclass(s.Single)\\nclass HBaseV098(HBase):\\n    def __init__(self):\\n        super(HBaseV098, self).__init__()\\n        self._version = \'0.98.7\'\\n        self._dependencies = [(\'mapr-hbase\', self.version)]\\n" }\n'
line: b'{ "repo_name": "Smile-SA/odoo_addons", "ref": "refs/heads/12.0", "path": "smile_anonymization/models/res_partner_title.py", "content": "# -*- coding: utf-8 -*-\\n# (C) 2019 Smile (<http://www.smile.fr>)\\n# License AGPL-3.0 or later (https://www.gnu.org/licenses/agpl).\\n\\nfrom odoo import fields, models\\n\\n\\nclass ResPartnerTitle(models.Model):\\n    _inherit = \'res.partner.title\'\\n\\n    name = fields.Char(data_mask=\\"\'title_\' || id::text\\")\\n    shortcut = fields.Char(data_mask=\\"NULL\\")\\n" }\n'
line: b'{ "repo_name": "bigfootproject/sahara", "ref": "refs/heads/spark-plugin", "path": "sahara/plugins/mapr/services/hbase/hbase.py", "content": "# Copyright (c) 2015, MapR Technologies\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n# not use this file except in compliance with the License. You may obtain\\n# a copy of the License at\\n#\\n#       http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n# License for the specific language governing permissions and limitations\\n# under the License.\\n\\n\\nimport six\\n\\nimport sahara.plugins.mapr.domain.configuration_file as bcf\\nimport sahara.plugins.mapr.domain.node_process as np\\nimport sahara.plugins.mapr.domain.service as s\\nimport sahara.plugins.mapr.util.validation_utils as vu\\n\\n\\nHBASE_MASTER = np.NodeProcess(\\n    name=\'hbmaster\',\\n    ui_name=\'HBase-Master\',\\n    package=\'mapr-hbase-master\',\\n    open_ports=[60000, 60010]\\n)\\nHBASE_REGION_SERVER = np.NodeProcess(\\n    name=\'hbregionserver\',\\n    ui_name=\'HBase-RegionServer\',\\n    package=\'mapr-hbase-regionserver\',\\n    open_ports=[60020]\\n)\\nHBASE_THRIFT = np.NodeProcess(\\n    name=\'hbasethrift\',\\n    ui_name=\'HBase-Thrift\',\\n    package=\'mapr-hbasethrift\',\\n    open_ports=[9090]\\n)\\n\\n\\nclass HBase(s.Service):\\n    def __init__(self):\\n        super(HBase, self).__init__()\\n        self._name = \'hbase\'\\n        self._ui_name = \'HBase\'\\n        self._node_processes = [\\n            HBASE_MASTER,\\n            HBASE_REGION_SERVER,\\n            HBASE_THRIFT,\\n        ]\\n        self._cluster_defaults = [\'hbase-default.json\']\\n        self._validation_rules = [\\n            vu.at_least(1, HBASE_MASTER),\\n            vu.at_least(1, HBASE_REGION_SERVER),\\n        ]\\n\\n    def get_config_files(self, cluster_context, configs, instance=None):\\n        hbase_site = bcf.HadoopXML(\\"hbase-site.xml\\")\\n        hbase_site.remote_path = self.conf_dir(cluster_context)\\n        if instance:\\n            hbase_site.fetch(instance)\\n        hbase_site.load_properties(configs)\\n        return [hbase_site]\\n\\n\\n@six.add_metaclass(s.Single)\\nclass HBaseV094(HBase):\\n    def __init__(self):\\n        super(HBaseV094, self).__init__()\\n        self._version = \'0.94.24\'\\n        self._dependencies = [(\'mapr-hbase\', self.version)]\\n\\n\\n@six.add_metaclass(s.Single)\\nclass HBaseV098(HBase):\\n    def __init__(self):\\n        super(HBaseV098, self).__init__()\\n        self._version = \'0.98.7\'\\n        self._dependencies = [(\'mapr-hbase\', self.version)]\\n" }\n'
line: b'{ "repo_name": "nubark/odoo", "ref": "refs/heads/9.0", "path": "openerp/addons/test_limits/__openerp__.py", "content": "# -*- coding: utf-8 -*-\\n{\\n    \'name\': \'test-limits\',\\n    \'version\': \'0.1\',\\n    \'category\': \'Tests\',\\n    \'description\': \\"\\"\\"A module with dummy methods.\\"\\"\\",\\n    \'depends\': [\'base\'],\\n    \'data\': [\'ir.model.access.csv\'],\\n    \'installable\': True,\\n    \'auto_install\': False,\\n}\\n" }\n'
line: b'{ "repo_name": "odoobgorg/odoo", "ref": "refs/heads/9.0", "path": "openerp/addons/test_limits/__openerp__.py", "content": "# -*- coding: utf-8 -*-\\n{\\n    \'name\': \'test-limits\',\\n    \'version\': \'0.1\',\\n    \'category\': \'Tests\',\\n    \'description\': \\"\\"\\"A module with dummy methods.\\"\\"\\",\\n    \'depends\': [\'base\'],\\n    \'data\': [\'ir.model.access.csv\'],\\n    \'installable\': True,\\n    \'auto_install\': False,\\n}\\n" }\n'
line: b'{ "repo_name": "Elico-Corp/odoo_OCB", "ref": "refs/heads/9.0", "path": "openerp/addons/test_limits/__openerp__.py", "content": "# -*- coding: utf-8 -*-\\n{\\n    \'name\': \'test-limits\',\\n    \'version\': \'0.1\',\\n    \'category\': \'Tests\',\\n    \'description\': \\"\\"\\"A module with dummy methods.\\"\\"\\",\\n    \'depends\': [\'base\'],\\n    \'data\': [\'ir.model.access.csv\'],\\n    \'installable\': True,\\n    \'auto_install\': False,\\n}\\n" }\n'
line: b'{ "repo_name": "kvar/ansible", "ref": "refs/heads/seas_master_2.9.5", "path": "test/units/modules/storage/netapp/test_na_ontap_vserver_cifs_security.py", "content": "# (c) 2019, NetApp, Inc\\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\\n\\n\'\'\' unit test template for ONTAP Ansible module \'\'\'\\n\\nfrom __future__ import (absolute_import, division, print_function)\\n__metaclass__ = type\\nimport json\\nimport pytest\\n\\nfrom units.compat import unittest\\nfrom units.compat.mock import patch, Mock\\nfrom ansible.module_utils import basic\\nfrom ansible.module_utils._text import to_bytes\\nimport ansible.module_utils.netapp as netapp_utils\\n\\nfrom ansible.modules.storage.netapp.na_ontap_vserver_cifs_security \\\\\\n    import NetAppONTAPCifsSecurity as cifs_security_module  # module under test\\n\\nif not netapp_utils.has_netapp_lib():\\n    pytestmark = pytest.mark.skip(\'skipping as missing required netapp_lib\')\\n\\n\\ndef set_module_args(args):\\n    \\"\\"\\"prepare arguments so that they will be picked up during module creation\\"\\"\\"\\n    args = json.dumps({\'ANSIBLE_MODULE_ARGS\': args})\\n    basic._ANSIBLE_ARGS = to_bytes(args)  # pylint: disable=protected-access\\n\\n\\nclass AnsibleExitJson(Exception):\\n    \\"\\"\\"Exception class to be raised by module.exit_json and caught by the test case\\"\\"\\"\\n    pass\\n\\n\\nclass AnsibleFailJson(Exception):\\n    \\"\\"\\"Exception class to be raised by module.fail_json and caught by the test case\\"\\"\\"\\n    pass\\n\\n\\ndef exit_json(*args, **kwargs):  # pylint: disable=unused-argument\\n    \\"\\"\\"function to patch over exit_json; package return data into an exception\\"\\"\\"\\n    if \'changed\' not in kwargs:\\n        kwargs[\'changed\'] = False\\n    raise AnsibleExitJson(kwargs)\\n\\n\\ndef fail_json(*args, **kwargs):  # pylint: disable=unused-argument\\n    \\"\\"\\"function to patch over fail_json; package return data into an exception\\"\\"\\"\\n    kwargs[\'failed\'] = True\\n    raise AnsibleFailJson(kwargs)\\n\\n\\nclass MockONTAPConnection(object):\\n    \'\'\' mock server connection to ONTAP host \'\'\'\\n\\n    def __init__(self, kind=None, data=None):\\n        \'\'\' save arguments \'\'\'\\n        self.type = kind\\n        self.data = data\\n        self.xml_in = None\\n        self.xml_out = None\\n\\n    def invoke_successfully(self, xml, enable_tunneling):  # pylint: disable=unused-argument\\n        \'\'\' mock invoke_successfully returning xml data \'\'\'\\n        self.xml_in = xml\\n        if self.type == \'cifs_security\':\\n            xml = self.build_port_info(self.data)\\n        if self.type == \'error\':\\n            error = netapp_utils.zapi.NaApiError(\'test\', \'error\')\\n            raise error\\n        self.xml_out = xml\\n        return xml\\n\\n    @staticmethod\\n    def build_port_info(cifs_security_details):\\n        \'\'\' build xml data for cifs-security \'\'\'\\n        xml = netapp_utils.zapi.NaElement(\'xml\')\\n        attributes = {\\n            \'num-records\': 1,\\n            \'attributes-list\': {\\n                \'cifs-security\': {\\n                    \'is_aes_encryption_enabled\': cifs_security_details[\'is_aes_encryption_enabled\'],\\n                    \'lm_compatibility_level\': cifs_security_details[\'lm_compatibility_level\']\\n              }\\n          }\\n      }\\n        xml.translate_struct(attributes)\\n        return xml\\n\\n\\nclass TestMyModule(unittest.TestCase):\\n    \'\'\' a group of related Unit Tests \'\'\'\\n\\n    def setUp(self):\\n        self.mock_module_helper = patch.multiple(basic.AnsibleModule,\\n                                                 exit_json=exit_json,\\n                                                 fail_json=fail_json)\\n        self.mock_module_helper.start()\\n        self.addCleanup(self.mock_module_helper.stop)\\n        self.mock_cifs_security = {\\n            \'is_aes_encryption_enabled\': \'true\',\\n            \'lm_compatibility_level\': \'krb\'\\n      }\\n\\n    def mock_args(self):\\n        return {\\n            \'is_aes_encryption_enabled\': self.mock_cifs_security[\'is_aes_encryption_enabled\'],\\n            \'lm_compatibility_level\': self.mock_cifs_security[\'lm_compatibility_level\'],\\n            \'vserver\': \'ansible\',\\n            \'hostname\': \'test\',\\n            \'username\': \'test_user\',\\n            \'password\': \'test_pass!\',\\n            \'https\': \'False\'\\n      }\\n\\n    def get_cifs_security_mock_object(self, kind=None):\\n        \\"\\"\\"\\n        Helper method to return an na_ontap_vserver_cifs_security object\\n        :param kind: passes this param to MockONTAPConnection()\\n        :return: na_ontap_vserver_cifs_security object\\n        \\"\\"\\"\\n        obj = cifs_security_module()\\n        obj.asup_log_for_cserver = Mock(return_value=None)\\n        obj.server = Mock()\\n        obj.server.invoke_successfully = Mock()\\n        if kind is None:\\n            obj.server = MockONTAPConnection()\\n        else:\\n            obj.server = MockONTAPConnection(kind=kind, data=self.mock_cifs_security)\\n        return obj\\n\\n    @patch(\'ansible.modules.storage.netapp.na_ontap_vserver_cifs_security.NetAppONTAPCifsSecurity.cifs_security_get_iter\')\\n    def test_successful_modify(self, get_cifs_security):\\n        \'\'\' Test successful modify max throughput \'\'\'\\n        data = self.mock_args()\\n        set_module_args(data)\\n        current = {\\n            \'is_aes_encryption_enabled\': False,\\n            \'lm_compatibility_level\': \'lm_ntlm_ntlmv2_krb\'\\n      }\\n        get_cifs_security.side_effect = [\\n            current\\n        ]\\n        with pytest.raises(AnsibleExitJson) as exc:\\n            self.get_cifs_security_mock_object(\'cifs_security\').apply()\\n        assert exc.value.args[0][\'changed\']\\n\\n    @patch(\'ansible.modules.storage.netapp.na_ontap_vserver_cifs_security.NetAppONTAPCifsSecurity.cifs_security_get_iter\')\\n    def test_modify_error(self, get_cifs_security):\\n        \'\'\' Test create idempotency \'\'\'\\n        data = self.mock_args()\\n        set_module_args(data)\\n        current = {\\n            \'is_aes_encryption_enabled\': False\\n      }\\n        get_cifs_security.side_effect = [\\n            current\\n        ]\\n        with pytest.raises(AnsibleFailJson) as exc:\\n            self.get_cifs_security_mock_object(\'error\').apply()\\n        assert exc.value.args[0][\'msg\'] == \'Error modifying cifs security on ansible: NetApp API failed. Reason - test:error\'\\n" } \n'
line: b'{ "repo_name": "microcom/odoo", "ref": "refs/heads/9.0", "path": "openerp/addons/test_limits/__openerp__.py", "content": "# -*- coding: utf-8 -*-\\n{\\n    \'name\': \'test-limits\',\\n    \'version\': \'0.1\',\\n    \'category\': \'Tests\',\\n    \'description\': \\"\\"\\"A module with dummy methods.\\"\\"\\",\\n    \'depends\': [\'base\'],\\n    \'data\': [\'ir.model.access.csv\'],\\n    \'installable\': True,\\n    \'auto_install\': False,\\n}\\n" }\n'
line: b'{ "repo_name": "bplancher/odoo", "ref": "refs/heads/9.0", "path": "openerp/addons/test_limits/__openerp__.py", "content": "# -*- coding: utf-8 -*-\\n{\\n    \'name\': \'test-limits\',\\n    \'version\': \'0.1\',\\n    \'category\': \'Tests\',\\n    \'description\': \\"\\"\\"A module with dummy methods.\\"\\"\\",\\n    \'depends\': [\'base\'],\\n    \'data\': [\'ir.model.access.csv\'],\\n    \'installable\': True,\\n    \'auto_install\': False,\\n}\\n" }\n'
line: b'{ "repo_name": "storm-computers/odoo", "ref": "refs/heads/9.0", "path": "openerp/addons/test_limits/__openerp__.py", "content": "# -*- coding: utf-8 -*-\\n{\\n    \'name\': \'test-limits\',\\n    \'version\': \'0.1\',\\n    \'category\': \'Tests\',\\n    \'description\': \\"\\"\\"A module with dummy methods.\\"\\"\\",\\n    \'depends\': [\'base\'],\\n    \'data\': [\'ir.model.access.csv\'],\\n    \'installable\': True,\\n    \'auto_install\': False,\\n}\\n" }\n'
line: b'{ "repo_name": "AyoubZahid/odoo", "ref": "refs/heads/9.0", "path": "openerp/addons/test_limits/__openerp__.py", "content": "# -*- coding: utf-8 -*-\\n{\\n    \'name\': \'test-limits\',\\n    \'version\': \'0.1\',\\n    \'category\': \'Tests\',\\n    \'description\': \\"\\"\\"A module with dummy methods.\\"\\"\\",\\n    \'depends\': [\'base\'],\\n    \'data\': [\'ir.model.access.csv\'],\\n    \'installable\': True,\\n    \'auto_install\': False,\\n}\\n" }\n'
line: b'{ "repo_name": "b12io/orchestra", "ref": "refs/heads/main", "path": "orchestra/utils/json_schema.py", "content": "\\"\\"\\"\\nValidation function for json schemas\\n\\"\\"\\"\\nfrom jsonschema import Draft4Validator\\nfrom jsonschema import validators\\n\\n\\ndef extend_with_default(validator_class):\\n    \\"\\"\\"\\n    Extends json schema validator so that it fills in default values.\\n\\n    NOTE(aditya): Copied code from\\n    https://github.com/b12io/crowdsurfing/blob/master/product/common/json_schema.py\\n    \\"\\"\\"\\n    validate_properties = validator_class.VALIDATORS[\'properties\']\\n\\n    def set_defaults(validator, properties, instance, schema):\\n        for property, subschema in properties.items():\\n            if \'default\' in subschema:\\n                instance.setdefault(property, subschema[\'default\'])\\n\\n        for error in validate_properties(\\n                validator, properties, instance, schema):\\n            yield error\\n\\n    return validators.extend(validator_class, {\'properties\': set_defaults})\\n\\n\\nDefaultValidatingDraft4Validator = extend_with_default(Draft4Validator)\\n" }\n'
line: b'{ "repo_name": "thaim/ansible", "ref": "refs/heads/fix-broken-link", "path": "test/units/modules/storage/netapp/test_na_ontap_vserver_cifs_security.py", "content": "# (c) 2019, NetApp, Inc\\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\\n\\n\'\'\' unit test template for ONTAP Ansible module \'\'\'\\n\\nfrom __future__ import (absolute_import, division, print_function)\\n__metaclass__ = type\\nimport json\\nimport pytest\\n\\nfrom units.compat import unittest\\nfrom units.compat.mock import patch, Mock\\nfrom ansible.module_utils import basic\\nfrom ansible.module_utils._text import to_bytes\\nimport ansible.module_utils.netapp as netapp_utils\\n\\nfrom ansible.modules.storage.netapp.na_ontap_vserver_cifs_security \\\\\\n    import NetAppONTAPCifsSecurity as cifs_security_module  # module under test\\n\\nif not netapp_utils.has_netapp_lib():\\n    pytestmark = pytest.mark.skip(\'skipping as missing required netapp_lib\')\\n\\n\\ndef set_module_args(args):\\n    \\"\\"\\"prepare arguments so that they will be picked up during module creation\\"\\"\\"\\n    args = json.dumps({\'ANSIBLE_MODULE_ARGS\': args})\\n    basic._ANSIBLE_ARGS = to_bytes(args)  # pylint: disable=protected-access\\n\\n\\nclass AnsibleExitJson(Exception):\\n    \\"\\"\\"Exception class to be raised by module.exit_json and caught by the test case\\"\\"\\"\\n    pass\\n\\n\\nclass AnsibleFailJson(Exception):\\n    \\"\\"\\"Exception class to be raised by module.fail_json and caught by the test case\\"\\"\\"\\n    pass\\n\\n\\ndef exit_json(*args, **kwargs):  # pylint: disable=unused-argument\\n    \\"\\"\\"function to patch over exit_json; package return data into an exception\\"\\"\\"\\n    if \'changed\' not in kwargs:\\n        kwargs[\'changed\'] = False\\n    raise AnsibleExitJson(kwargs)\\n\\n\\ndef fail_json(*args, **kwargs):  # pylint: disable=unused-argument\\n    \\"\\"\\"function to patch over fail_json; package return data into an exception\\"\\"\\"\\n    kwargs[\'failed\'] = True\\n    raise AnsibleFailJson(kwargs)\\n\\n\\nclass MockONTAPConnection(object):\\n    \'\'\' mock server connection to ONTAP host \'\'\'\\n\\n    def __init__(self, kind=None, data=None):\\n        \'\'\' save arguments \'\'\'\\n        self.type = kind\\n        self.data = data\\n        self.xml_in = None\\n        self.xml_out = None\\n\\n    def invoke_successfully(self, xml, enable_tunneling):  # pylint: disable=unused-argument\\n        \'\'\' mock invoke_successfully returning xml data \'\'\'\\n        self.xml_in = xml\\n        if self.type == \'cifs_security\':\\n            xml = self.build_port_info(self.data)\\n        if self.type == \'error\':\\n            error = netapp_utils.zapi.NaApiError(\'test\', \'error\')\\n            raise error\\n        self.xml_out = xml\\n        return xml\\n\\n    @staticmethod\\n    def build_port_info(cifs_security_details):\\n        \'\'\' build xml data for cifs-security \'\'\'\\n        xml = netapp_utils.zapi.NaElement(\'xml\')\\n        attributes = {\\n            \'num-records\': 1,\\n            \'attributes-list\': {\\n                \'cifs-security\': {\\n                    \'is_aes_encryption_enabled\': cifs_security_details[\'is_aes_encryption_enabled\'],\\n                    \'lm_compatibility_level\': cifs_security_details[\'lm_compatibility_level\']\\n              }\\n          }\\n      }\\n        xml.translate_struct(attributes)\\n        return xml\\n\\n\\nclass TestMyModule(unittest.TestCase):\\n    \'\'\' a group of related Unit Tests \'\'\'\\n\\n    def setUp(self):\\n        self.mock_module_helper = patch.multiple(basic.AnsibleModule,\\n                                                 exit_json=exit_json,\\n                                                 fail_json=fail_json)\\n        self.mock_module_helper.start()\\n        self.addCleanup(self.mock_module_helper.stop)\\n        self.mock_cifs_security = {\\n            \'is_aes_encryption_enabled\': \'true\',\\n            \'lm_compatibility_level\': \'krb\'\\n      }\\n\\n    def mock_args(self):\\n        return {\\n            \'is_aes_encryption_enabled\': self.mock_cifs_security[\'is_aes_encryption_enabled\'],\\n            \'lm_compatibility_level\': self.mock_cifs_security[\'lm_compatibility_level\'],\\n            \'vserver\': \'ansible\',\\n            \'hostname\': \'test\',\\n            \'username\': \'test_user\',\\n            \'password\': \'test_pass!\',\\n            \'https\': \'False\'\\n      }\\n\\n    def get_cifs_security_mock_object(self, kind=None):\\n        \\"\\"\\"\\n        Helper method to return an na_ontap_vserver_cifs_security object\\n        :param kind: passes this param to MockONTAPConnection()\\n        :return: na_ontap_vserver_cifs_security object\\n        \\"\\"\\"\\n        obj = cifs_security_module()\\n        obj.asup_log_for_cserver = Mock(return_value=None)\\n        obj.server = Mock()\\n        obj.server.invoke_successfully = Mock()\\n        if kind is None:\\n            obj.server = MockONTAPConnection()\\n        else:\\n            obj.server = MockONTAPConnection(kind=kind, data=self.mock_cifs_security)\\n        return obj\\n\\n    @patch(\'ansible.modules.storage.netapp.na_ontap_vserver_cifs_security.NetAppONTAPCifsSecurity.cifs_security_get_iter\')\\n    def test_successful_modify(self, get_cifs_security):\\n        \'\'\' Test successful modify max throughput \'\'\'\\n        data = self.mock_args()\\n        set_module_args(data)\\n        current = {\\n            \'is_aes_encryption_enabled\': False,\\n            \'lm_compatibility_level\': \'lm_ntlm_ntlmv2_krb\'\\n      }\\n        get_cifs_security.side_effect = [\\n            current\\n        ]\\n        with pytest.raises(AnsibleExitJson) as exc:\\n            self.get_cifs_security_mock_object(\'cifs_security\').apply()\\n        assert exc.value.args[0][\'changed\']\\n\\n    @patch(\'ansible.modules.storage.netapp.na_ontap_vserver_cifs_security.NetAppONTAPCifsSecurity.cifs_security_get_iter\')\\n    def test_modify_error(self, get_cifs_security):\\n        \'\'\' Test create idempotency \'\'\'\\n        data = self.mock_args()\\n        set_module_args(data)\\n        current = {\\n            \'is_aes_encryption_enabled\': False\\n      }\\n        get_cifs_security.side_effect = [\\n            current\\n        ]\\n        with pytest.raises(AnsibleFailJson) as exc:\\n            self.get_cifs_security_mock_object(\'error\').apply()\\n        assert exc.value.args[0][\'msg\'] == \'Error modifying cifs security on ansible: NetApp API failed. Reason - test:error\'\\n" }\n'
line: b'{ "repo_name": "roscopecoltran/scraper", "ref": "refs/heads/sniperkit", "path": ".staging/meta-engines/xlinkBook/update/update_stanford_cs.py", "content": "#!/usr/bin/env python\\n    \\n#author: wowdd1\\n#mail: developergf@gmail.com\\n#data: 2014.12.07\\n    \\nfrom spider import *\\nfrom update_stanford import StanfordSpider\\nfrom update_stanford_online import StanfordOnlineSpider\\nsys.path.append(\\"..\\")\\nfrom record import CourseRecord\\n\\nclass StanfordCSSpider(Spider):\\n    course_num_list = []\\n\\n    def __init__(self):\\n        Spider.__init__(self)\\n        self.school = \\"stanford\\"\\n        self.subject = \\"eecs\\"\\n        stanfordSpider = StanfordSpider()\\n        stanfordOnlineSpider = StanfordOnlineSpider()\\n\\n        self.description_dict = stanfordSpider.getDescriptionDict(\'Computer Science\')\\n        self.course_name_dict = stanfordOnlineSpider.getCourseNameDict()\\n    \\n    def isInCourseNumList(self, course_num):\\n        for item in self.course_num_list:\\n            if item == course_num:\\n                return True\\n        self.course_num_list.append(course_num)\\n        return False\\n   \\n    def formatCourseTitle(self, title):\\n        if title.find(\'(\') != -1:\\n            title = title[0 : title.find(\'(\')]\\n        return title.strip()\\n \\n    def processStanfordDate(self, f, url, course_dict):\\n        print \'processing \' + url\\n        r = requests.get(url)\\n        soup = BeautifulSoup(r.text)\\n        th_set = soup.find_all(\\"th\\")\\n        td_set_all = soup.find_all(\\"td\\")\\n        td_set = []\\n        td_set_2 = []\\n        del th_set[0:5]\\n        i = 0\\n        for td in td_set_all:\\n            i = i + 1\\n            if i == 1:\\n                td_set.append(td.string)\\n            if i == 2:\\n                td_set_2.append(td.string)\\n            if i == 4:\\n                i = 0\\n    \\n        for index in range(0,len(th_set)):\\n            link = th_set[index].prettify()\\n            link = link[link.find(\\"http\\"):link.find(\\"EDU\\") + 3]\\n            if self.isInCourseNumList(th_set[index].string):\\n                continue\\n            course_id = th_set[index].string.upper()\\n            description = \'\'\\n            description += \'instructors:\' + td_set_2[index] + \' \'\\n            if self.course_name_dict.get(self.formatCourseTitle(td_set[index]), \'\') != \'\':\\n                if self.course_name_dict.get(self.formatCourseTitle(td_set[index]), \'\') != \'\':\\n                    description += \'videourl:\' + self.course_name_dict[self.formatCourseTitle(td_set[index])] + \' \'\\n\\n            if self.description_dict.get(course_id, \'\') != \'\':\\n                description += \'description:\' + self.description_dict[course_id] + \' \'\\n            course_dict[th_set[index].string.upper()] = CourseRecord(self.get_storage_format(th_set[index].string.upper(), td_set[index], link, description))\\n\\n   \\n    def getCsCourseLinks(self):\\n        links = []\\n        r = requests.get(\'http://cs.stanford.edu/academics/courses\')\\n        soup = BeautifulSoup(r.text)\\n        for a in soup.find_all(\'a\'):\\n            if a.attrs.has_key(\'href\') and a[\'href\'].find(\'http://cs.stanford.edu/courses/schedules/\') != -1:\\n                links.append(a[\'href\'])\\n\\n        return links\\n\\n    def doWork(self):\\n        #stanford\\n        #\\"\\"\\"\\n        print \\"downloading stanford course info\\"\\n\\n        file_name = self.get_file_name(\\"eecs/\\" + \\"cs\\", self.school)\\n        file_lines = self.countFileLineNum(file_name)\\n        f = self.open_db(file_name + \\".tmp\\")\\n        self.count = 0\\n \\n        print \\"processing html and write data to file...\\"\\n        course_dict = {}\\n        for url in self.getCsCourseLinks():\\n            self.processStanfordDate(f, url, course_dict)\\n\\n        for k, record in [(k,course_dict[k]) for k in sorted(course_dict.keys())]:\\n            self.count += 1\\n            self.write_db(f, k, record.get_title().strip(), record.get_url().strip(), record.get_describe().strip()) \\n    \\n        self.close_db(f)\\n        if file_lines != self.count and self.count > 0:\\n            self.do_upgrade_db(file_name)\\n            print \\"before lines: \\" + str(file_lines) + \\" after update: \\" + str(self.count) + \\" \\\\n\\\\n\\"\\n        else:\\n            self.cancel_upgrade(file_name)\\n            print \\"no need upgrade\\\\n\\"\\n        #\\"\\"\\"\\n    \\ndef main(argv):\\n    start = StanfordCSSpider()\\n    start.doWork()\\n\\nif __name__ == \'__main__\':\\n    main(sys.argv)\\n\\n" }\n'
line: b'{ "repo_name": "unlimitedlabs/orchestra", "ref": "refs/heads/dependabot-npm_and_yarn-lodash-4.17.21", "path": "orchestra/utils/json_schema.py", "content": "\\"\\"\\"\\nValidation function for json schemas\\n\\"\\"\\"\\nfrom jsonschema import Draft4Validator\\nfrom jsonschema import validators\\n\\n\\ndef extend_with_default(validator_class):\\n    \\"\\"\\"\\n    Extends json schema validator so that it fills in default values.\\n\\n    NOTE(aditya): Copied code from\\n    https://github.com/b12io/crowdsurfing/blob/master/product/common/json_schema.py\\n    \\"\\"\\"\\n    validate_properties = validator_class.VALIDATORS[\'properties\']\\n\\n    def set_defaults(validator, properties, instance, schema):\\n        for property, subschema in properties.items():\\n            if \'default\' in subschema:\\n                instance.setdefault(property, subschema[\'default\'])\\n\\n        for error in validate_properties(\\n                validator, properties, instance, schema):\\n            yield error\\n\\n    return validators.extend(validator_class, {\'properties\': set_defaults})\\n\\n\\nDefaultValidatingDraft4Validator = extend_with_default(Draft4Validator)\\n" }\n'
line: b'{ "repo_name": "laslabs/odoo", "ref": "refs/heads/9.0", "path": "openerp/addons/test_limits/__openerp__.py", "content": "# -*- coding: utf-8 -*-\\n{\\n    \'name\': \'test-limits\',\\n    \'version\': \'0.1\',\\n    \'category\': \'Tests\',\\n    \'description\': \\"\\"\\"A module with dummy methods.\\"\\"\\",\\n    \'depends\': [\'base\'],\\n    \'data\': [\'ir.model.access.csv\'],\\n    \'installable\': True,\\n    \'auto_install\': False,\\n}\\n" }\n'
line: b'{ "repo_name": "pfalcon/micropython", "ref": "refs/heads/pfalcon", "path": "tests/basics/python36.py", "content": "# tests for things that only Python 3.6 supports\\n\\n# underscores in numeric literals\\nprint(100_000)\\nprint(0b1010_0101)\\nprint(0xff_ff)\\n\\n# underscore supported by int constructor\\nprint(int(\'1_2_3\'))\\nprint(int(\'0o1_2_3\', 8))\\n" }\n'
line: b'{ "repo_name": "be-cloud-be/horizon-addons", "ref": "refs/heads/9.0", "path": "server/openerp/addons/test_limits/__openerp__.py", "content": "# -*- coding: utf-8 -*-\\n{\\n    \'name\': \'test-limits\',\\n    \'version\': \'0.1\',\\n    \'category\': \'Tests\',\\n    \'description\': \\"\\"\\"A module with dummy methods.\\"\\"\\",\\n    \'depends\': [\'base\'],\\n    \'data\': [\'ir.model.access.csv\'],\\n    \'installable\': True,\\n    \'auto_install\': False,\\n}\\n" }\n'
line: b'{ "repo_name": "pozetroninc/micropython", "ref": "refs/heads/stable", "path": "tests/basics/python36.py", "content": "# tests for things that only Python 3.6 supports\\n\\n# underscores in numeric literals\\nprint(100_000)\\nprint(0b1010_0101)\\nprint(0xff_ff)\\n\\n# underscore supported by int constructor\\nprint(int(\'1_2_3\'))\\nprint(int(\'0o1_2_3\', 8))\\n" }\n'
line: b'{ "repo_name": "ngageoint/voxel-globe", "ref": "refs/heads/nga_p2_release", "path": "voxel_globe/ingest/serializers.py", "content": "from rest_framework import serializers\\n\\nimport voxel_globe.ingest.models\\n\\nclass UploadSessionSerializer(serializers.ModelSerializer):\\n  #directory = serializers.RelatedField(many=True, read_only=True)\\n  class Meta(object):\\n    model = voxel_globe.ingest.models.UploadSession\\n    fields = (\'file\',)\\n    read_only_fields = (\'file\',)\\n#Add all the fields\\nfn = map(lambda x:x.name, voxel_globe.ingest.models.UploadSession._meta.fields)\\n#Except owner\\nfn.remove(\'owner\')\\n#Add them to the existing list\\nUploadSessionSerializer.Meta.fields = UploadSessionSerializer.Meta.fields + \\\\\\n                                      tuple(fn)\\ndel fn #clean up\\n\\n# class DirectorySerializer(serializers.ModelSerializer):\\n#   class Meta(object):\\n#     model = voxel_globe.ingest.models.Directory\\n#     fields = (\'id\', \'name\', \'file\', \'session\')\\n#     read_only_fields = (\'file\',)\\n\\nclass FileSerializer(serializers.ModelSerializer):\\n  class Meta:\\n    model = voxel_globe.ingest.models.File\\n    fields = (\'id\', \'name\', \'session\', \'completed\')\\n\\n#Huge Security. Exposes Owner which exposes the password hash\\n#def NestFactory(serializer):\\n#  return type(\'Nested\', (serializer,), \\n#            {\'Meta\': type(\'Nested_Meta\', (serializer.Meta,), {\'depth\':1})})" }\n'
line: b'{ "repo_name": "trezor/micropython", "ref": "refs/heads/trezor-v1.12", "path": "tests/basics/python36.py", "content": "# tests for things that only Python 3.6 supports\\n\\n# underscores in numeric literals\\nprint(100_000)\\nprint(0b1010_0101)\\nprint(0xff_ff)\\n\\n# underscore supported by int constructor\\nprint(int(\'1_2_3\'))\\nprint(int(\'0o1_2_3\', 8))\\n" }\n'
line: b'{ "repo_name": "Southpaw-TACTIC/TACTIC", "ref": "refs/heads/4.7", "path": "src/context/client/tactic-api-python-4.0.api04/Lib/genericpath.py", "content": "\\"\\"\\"\\r\\nPath operations common to more than one OS\\r\\nDo not use directly.  The OS specific modules import the appropriate\\r\\nfunctions from this module themselves.\\r\\n\\"\\"\\"\\r\\nimport os\\r\\nimport stat\\r\\n\\r\\n__all__ = [\'commonprefix\', \'exists\', \'getatime\', \'getctime\', \'getmtime\',\\r\\n           \'getsize\', \'isdir\', \'isfile\']\\r\\n\\r\\n\\r\\n# Does a path exist?\\r\\n# This is false for dangling symbolic links on systems that support them.\\r\\ndef exists(path):\\r\\n    \\"\\"\\"Test whether a path exists.  Returns False for broken symbolic links\\"\\"\\"\\r\\n    try:\\r\\n        st = os.stat(path)\\r\\n    except os.error:\\r\\n        return False\\r\\n    return True\\r\\n\\r\\n\\r\\n# This follows symbolic links, so both islink() and isdir() can be true\\r\\n# for the same path ono systems that support symlinks\\r\\ndef isfile(path):\\r\\n    \\"\\"\\"Test whether a path is a regular file\\"\\"\\"\\r\\n    try:\\r\\n        st = os.stat(path)\\r\\n    except os.error:\\r\\n        return False\\r\\n    return stat.S_ISREG(st.st_mode)\\r\\n\\r\\n\\r\\n# Is a path a directory?\\r\\n# This follows symbolic links, so both islink() and isdir()\\r\\n# can be true for the same path on systems that support symlinks\\r\\ndef isdir(s):\\r\\n    \\"\\"\\"Return true if the pathname refers to an existing directory.\\"\\"\\"\\r\\n    try:\\r\\n        st = os.stat(s)\\r\\n    except os.error:\\r\\n        return False\\r\\n    return stat.S_ISDIR(st.st_mode)\\r\\n\\r\\n\\r\\ndef getsize(filename):\\r\\n    \\"\\"\\"Return the size of a file, reported by os.stat().\\"\\"\\"\\r\\n    return os.stat(filename).st_size\\r\\n\\r\\n\\r\\ndef getmtime(filename):\\r\\n    \\"\\"\\"Return the last modification time of a file, reported by os.stat().\\"\\"\\"\\r\\n    return os.stat(filename).st_mtime\\r\\n\\r\\n\\r\\ndef getatime(filename):\\r\\n    \\"\\"\\"Return the last access time of a file, reported by os.stat().\\"\\"\\"\\r\\n    return os.stat(filename).st_atime\\r\\n\\r\\n\\r\\ndef getctime(filename):\\r\\n    \\"\\"\\"Return the metadata change time of a file, reported by os.stat().\\"\\"\\"\\r\\n    return os.stat(filename).st_ctime\\r\\n\\r\\n\\r\\n# Return the longest prefix of all list elements.\\r\\ndef commonprefix(m):\\r\\n    \\"Given a list of pathnames, returns the longest common leading component\\"\\r\\n    if not m: return \'\'\\r\\n    s1 = min(m)\\r\\n    s2 = max(m)\\r\\n    for i, c in enumerate(s1):\\r\\n        if c != s2[i]:\\r\\n            return s1[:i]\\r\\n    return s1\\r\\n\\r\\n# Split a path in root and extension.\\r\\n# The extension is everything starting at the last dot in the last\\r\\n# pathname component; the root is everything before that.\\r\\n# It is always true that root + ext == p.\\r\\n\\r\\n# Generic implementation of splitext, to be parametrized with\\r\\n# the separators\\r\\ndef _splitext(p, sep, altsep, extsep):\\r\\n    \\"\\"\\"Split the extension from a pathname.\\r\\n\\r\\n    Extension is everything from the last dot to the end, ignoring\\r\\n    leading dots.  Returns \\"(root, ext)\\"; ext may be empty.\\"\\"\\"\\r\\n\\r\\n    sepIndex = p.rfind(sep)\\r\\n    if altsep:\\r\\n        altsepIndex = p.rfind(altsep)\\r\\n        sepIndex = max(sepIndex, altsepIndex)\\r\\n\\r\\n    dotIndex = p.rfind(extsep)\\r\\n    if dotIndex > sepIndex:\\r\\n        # skip all leading dots\\r\\n        filenameIndex = sepIndex + 1\\r\\n        while filenameIndex < dotIndex:\\r\\n            if p[filenameIndex] != extsep:\\r\\n                return p[:dotIndex], p[dotIndex:]\\r\\n            filenameIndex += 1\\r\\n\\r\\n    return p, \'\'\\r\\n" }\n'
line: b'{ "repo_name": "dwavesystems/dimod", "ref": "refs/heads/main", "path": "dimod/higherorder/__init__.py", "content": "# Copyright 2019 D-Wave Systems Inc.\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n#    you may not use this file except in compliance with the License.\\n#    You may obtain a copy of the License at\\n#\\n#        http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n#    See the License for the specific language governing permissions and\\n#    limitations under the License.\\n#\\n# ============================================================================\\n\\nfrom dimod.higherorder.polynomial import BinaryPolynomial\\nfrom dimod.higherorder.utils import make_quadratic, poly_energy, poly_energies\\n" }\n'
line: b'{ "repo_name": "sajuptpm/neutron-ipam", "ref": "refs/heads/stable/icehouse", "path": "neutron/plugins/metaplugin/meta_neutron_plugin.py", "content": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\\n\\n# Copyright 2012, Nachi Ueno, NTT MCL, Inc.\\n# All Rights Reserved.\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\nfrom oslo.config import cfg\\n\\nfrom neutron.common import exceptions as exc\\nfrom neutron.common import topics\\nfrom neutron import context as neutron_context\\nfrom neutron.db import api as db\\nfrom neutron.db import db_base_plugin_v2\\nfrom neutron.db import external_net_db\\nfrom neutron.db import extraroute_db\\nfrom neutron.db import l3_db\\nfrom neutron.db import models_v2\\nfrom neutron.extensions.flavor import (FLAVOR_NETWORK, FLAVOR_ROUTER)\\nfrom neutron.openstack.common import importutils\\nfrom neutron.openstack.common import log as logging\\nfrom neutron.plugins.metaplugin.common import config  # noqa\\nfrom neutron.plugins.metaplugin import meta_db_v2\\nfrom neutron.plugins.metaplugin.meta_models_v2 import (NetworkFlavor,\\n                                                       RouterFlavor)\\n\\n\\nLOG = logging.getLogger(__name__)\\n\\n\\n# Hooks used to select records which belong a target plugin.\\ndef _meta_network_model_hook(context, original_model, query):\\n    return query.outerjoin(NetworkFlavor,\\n                           NetworkFlavor.network_id == models_v2.Network.id)\\n\\n\\ndef _meta_port_model_hook(context, original_model, query):\\n    return query.join(NetworkFlavor,\\n                      NetworkFlavor.network_id == models_v2.Port.network_id)\\n\\n\\ndef _meta_flavor_filter_hook(query, filters):\\n    if FLAVOR_NETWORK in filters:\\n        return query.filter(NetworkFlavor.flavor ==\\n                            filters[FLAVOR_NETWORK][0])\\n    return query\\n\\n\\n# Metaplugin  Exceptions\\nclass FlavorNotFound(exc.NotFound):\\n    message = _(\\"Flavor %(flavor)s could not be found\\")\\n\\n\\nclass FaildToAddFlavorBinding(exc.NeutronException):\\n    message = _(\\"Failed to add flavor binding\\")\\n\\n\\nclass MetaPluginV2(db_base_plugin_v2.NeutronDbPluginV2,\\n                   external_net_db.External_net_db_mixin,\\n                   extraroute_db.ExtraRoute_db_mixin):\\n\\n    def __init__(self, configfile=None):\\n        super(MetaPluginV2, self).__init__()\\n        LOG.debug(_(\\"Start initializing metaplugin\\"))\\n        self.supported_extension_aliases = [\'flavor\', \'external-net\']\\n        if cfg.CONF.META.supported_extension_aliases:\\n            cfg_aliases = cfg.CONF.META.supported_extension_aliases.split(\',\')\\n            self.supported_extension_aliases += cfg_aliases\\n\\n        # Ignore config option overapping\\n        def _is_opt_registered(opts, opt):\\n            if opt.dest in opts:\\n                return True\\n            else:\\n                return False\\n\\n        cfg._is_opt_registered = _is_opt_registered\\n\\n        # Keep existing tables if multiple plugin use same table name.\\n        db.model_base.NeutronBase.__table_args__ = {\'keep_existing\': True}\\n\\n        self.plugins = {}\\n\\n        plugin_list = [plugin_set.split(\':\')\\n                       for plugin_set\\n                       in cfg.CONF.META.plugin_list.split(\',\')]\\n        self.rpc_flavor = cfg.CONF.META.rpc_flavor\\n        topic_save = topics.PLUGIN\\n        topic_fake = topic_save + \'-metaplugin\'\\n        for flavor, plugin_provider in plugin_list:\\n            # Rename topic used by a plugin other than rpc_flavor during\\n            # loading the plugin instance if rpc_flavor is specified.\\n            # This enforces the plugin specified by rpc_flavor is only\\n            # consumer of \'q-plugin\'. It is a bit tricky but there is no\\n            # bad effect.\\n            if self.rpc_flavor and self.rpc_flavor != flavor:\\n                topics.PLUGIN = topic_fake\\n            self.plugins[flavor] = self._load_plugin(plugin_provider)\\n            topics.PLUGIN = topic_save\\n\\n        self.l3_plugins = {}\\n        if cfg.CONF.META.l3_plugin_list:\\n            l3_plugin_list = [plugin_set.split(\':\')\\n                              for plugin_set\\n                              in cfg.CONF.META.l3_plugin_list.split(\',\')]\\n            for flavor, plugin_provider in l3_plugin_list:\\n                if flavor in self.plugins:\\n                    self.l3_plugins[flavor] = self.plugins[flavor]\\n                else:\\n                    # For l3 only plugin\\n                    self.l3_plugins[flavor] = self._load_plugin(\\n                        plugin_provider)\\n\\n        self.default_flavor = cfg.CONF.META.default_flavor\\n        if self.default_flavor not in self.plugins:\\n            raise exc.Invalid(_(\'default_flavor %s is not plugin list\') %\\n                              self.default_flavor)\\n\\n        if self.l3_plugins:\\n            self.default_l3_flavor = cfg.CONF.META.default_l3_flavor\\n            if self.default_l3_flavor not in self.l3_plugins:\\n                raise exc.Invalid(_(\'default_l3_flavor %s is not plugin list\')\\n                                  % self.default_l3_flavor)\\n            self.supported_extension_aliases += [\'router\', \'ext-gw-mode\',\\n                                                 \'extraroute\']\\n\\n        if self.rpc_flavor and self.rpc_flavor not in self.plugins:\\n            raise exc.Invalid(_(\'rpc_flavor %s is not plugin list\') %\\n                              self.rpc_flavor)\\n\\n        self.extension_map = {}\\n        if not cfg.CONF.META.extension_map == \'\':\\n            extension_list = [method_set.split(\':\')\\n                              for method_set\\n                              in cfg.CONF.META.extension_map.split(\',\')]\\n            for method_name, flavor in extension_list:\\n                self.extension_map[method_name] = flavor\\n\\n        # Register hooks.\\n        # The hooks are applied for each target plugin instance when\\n        # calling the base class to get networks/ports so that only records\\n        # which belong to the plugin are selected.\\n        #NOTE: Doing registration here (within __init__()) is to avoid\\n        # registration when merely importing this file. This is only\\n        # for running whole unit tests.\\n        db_base_plugin_v2.NeutronDbPluginV2.register_model_query_hook(\\n            models_v2.Network,\\n            \'metaplugin_net\',\\n            _meta_network_model_hook,\\n            None,\\n            _meta_flavor_filter_hook)\\n        db_base_plugin_v2.NeutronDbPluginV2.register_model_query_hook(\\n            models_v2.Port,\\n            \'metaplugin_port\',\\n            _meta_port_model_hook,\\n            None,\\n            _meta_flavor_filter_hook)\\n\\n    def _load_plugin(self, plugin_provider):\\n        LOG.debug(_(\\"Plugin location: %s\\"), plugin_provider)\\n        plugin_klass = importutils.import_class(plugin_provider)\\n        return plugin_klass()\\n\\n    def _get_plugin(self, flavor):\\n        if flavor not in self.plugins:\\n            raise FlavorNotFound(flavor=flavor)\\n        return self.plugins[flavor]\\n\\n    def _get_l3_plugin(self, flavor):\\n        if flavor not in self.l3_plugins:\\n            raise FlavorNotFound(flavor=flavor)\\n        return self.l3_plugins[flavor]\\n\\n    def __getattr__(self, key):\\n        # At first,  try to pickup extension command from extension_map\\n\\n        if key in self.extension_map:\\n            flavor = self.extension_map[key]\\n            plugin = self._get_plugin(flavor)\\n            if plugin and hasattr(plugin, key):\\n                return getattr(plugin, key)\\n\\n        # Second, try to match extension method in order of plugin list\\n        for flavor, plugin in self.plugins.items():\\n            if hasattr(plugin, key):\\n                return getattr(plugin, key)\\n\\n        # if no plugin support the method, then raise\\n        raise AttributeError\\n\\n    def _extend_network_dict(self, context, network):\\n        flavor = self._get_flavor_by_network_id(context, network[\'id\'])\\n        network[FLAVOR_NETWORK] = flavor\\n\\n    def start_rpc_listener(self):\\n        return self.plugins[self.rpc_flavor].start_rpc_listener()\\n\\n    def rpc_workers_supported(self):\\n        #NOTE: If a plugin which supports multiple RPC workers is desired\\n        # to handle RPC, rpc_flavor must be specified.\\n        return (self.rpc_flavor and\\n                self.plugins[self.rpc_flavor].rpc_workers_supported())\\n\\n    def create_network(self, context, network):\\n        n = network[\'network\']\\n        flavor = n.get(FLAVOR_NETWORK)\\n        if str(flavor) not in self.plugins:\\n            flavor = self.default_flavor\\n        plugin = self._get_plugin(flavor)\\n        net = plugin.create_network(context, network)\\n        LOG.debug(_(\\"Created network: %(net_id)s with flavor \\"\\n                    \\"%(flavor)s\\"), {\'net_id\': net[\'id\'], \'flavor\': flavor})\\n        try:\\n            meta_db_v2.add_network_flavor_binding(context.session,\\n                                                  flavor, str(net[\'id\']))\\n        except Exception:\\n            LOG.exception(_(\'Failed to add flavor bindings\'))\\n            plugin.delete_network(context, net[\'id\'])\\n            raise FaildToAddFlavorBinding()\\n\\n        LOG.debug(_(\\"Created network: %s\\"), net[\'id\'])\\n        self._extend_network_dict(context, net)\\n        return net\\n\\n    def update_network(self, context, id, network):\\n        flavor = meta_db_v2.get_flavor_by_network(context.session, id)\\n        plugin = self._get_plugin(flavor)\\n        return plugin.update_network(context, id, network)\\n\\n    def delete_network(self, context, id):\\n        flavor = meta_db_v2.get_flavor_by_network(context.session, id)\\n        plugin = self._get_plugin(flavor)\\n        return plugin.delete_network(context, id)\\n\\n    def get_network(self, context, id, fields=None):\\n        flavor = meta_db_v2.get_flavor_by_network(context.session, id)\\n        plugin = self._get_plugin(flavor)\\n        net = plugin.get_network(context, id, fields)\\n        net[\'id\'] = id\\n        if not fields or FLAVOR_NETWORK in fields:\\n            self._extend_network_dict(context, net)\\n        if fields and \'id\' not in fields:\\n            del net[\'id\']\\n        return net\\n\\n    def get_networks(self, context, filters=None, fields=None):\\n        nets = []\\n        for flavor, plugin in self.plugins.items():\\n            if (filters and FLAVOR_NETWORK in filters and\\n                    not flavor in filters[FLAVOR_NETWORK]):\\n                continue\\n            if filters:\\n                #NOTE: copy each time since a target plugin may modify\\n                # plugin_filters.\\n                plugin_filters = filters.copy()\\n            else:\\n                plugin_filters = {}\\n            plugin_filters[FLAVOR_NETWORK] = [flavor]\\n            plugin_nets = plugin.get_networks(context, plugin_filters, fields)\\n            for net in plugin_nets:\\n                if not fields or FLAVOR_NETWORK in fields:\\n                    net[FLAVOR_NETWORK] = flavor\\n                nets.append(net)\\n        return nets\\n\\n    def _get_flavor_by_network_id(self, context, network_id):\\n        return meta_db_v2.get_flavor_by_network(context.session, network_id)\\n\\n    def _get_flavor_by_router_id(self, context, router_id):\\n        return meta_db_v2.get_flavor_by_router(context.session, router_id)\\n\\n    def _get_plugin_by_network_id(self, context, network_id):\\n        flavor = self._get_flavor_by_network_id(context, network_id)\\n        return self._get_plugin(flavor)\\n\\n    def create_port(self, context, port):\\n        p = port[\'port\']\\n        if \'network_id\' not in p:\\n            raise exc.NotFound\\n        plugin = self._get_plugin_by_network_id(context, p[\'network_id\'])\\n        return plugin.create_port(context, port)\\n\\n    def update_port(self, context, id, port):\\n        port_in_db = self._get_port(context, id)\\n        plugin = self._get_plugin_by_network_id(context,\\n                                                port_in_db[\'network_id\'])\\n        return plugin.update_port(context, id, port)\\n\\n    def delete_port(self, context, id, l3_port_check=True):\\n        port_in_db = self._get_port(context, id)\\n        plugin = self._get_plugin_by_network_id(context,\\n                                                port_in_db[\'network_id\'])\\n        return plugin.delete_port(context, id, l3_port_check)\\n\\n    # This is necessary since there is a case that\\n    # NeutronManager.get_plugin()._make_port_dict is called.\\n    def _make_port_dict(self, port):\\n        context = neutron_context.get_admin_context()\\n        plugin = self._get_plugin_by_network_id(context,\\n                                                port[\'network_id\'])\\n        return plugin._make_port_dict(port)\\n\\n    def get_port(self, context, id, fields=None):\\n        port_in_db = self._get_port(context, id)\\n        plugin = self._get_plugin_by_network_id(context,\\n                                                port_in_db[\'network_id\'])\\n        return plugin.get_port(context, id, fields)\\n\\n    def get_ports(self, context, filters=None, fields=None):\\n        all_ports = []\\n        for flavor, plugin in self.plugins.items():\\n            if filters:\\n                #NOTE: copy each time since a target plugin may modify\\n                # plugin_filters.\\n                plugin_filters = filters.copy()\\n            else:\\n                plugin_filters = {}\\n            plugin_filters[FLAVOR_NETWORK] = [flavor]\\n            ports = plugin.get_ports(context, plugin_filters, fields)\\n            all_ports += ports\\n        return all_ports\\n\\n    def create_subnet(self, context, subnet):\\n        s = subnet[\'subnet\']\\n        if \'network_id\' not in s:\\n            raise exc.NotFound\\n        plugin = self._get_plugin_by_network_id(context,\\n                                                s[\'network_id\'])\\n        return plugin.create_subnet(context, subnet)\\n\\n    def update_subnet(self, context, id, subnet):\\n        s = self.get_subnet(context, id)\\n        plugin = self._get_plugin_by_network_id(context,\\n                                                s[\'network_id\'])\\n        return plugin.update_subnet(context, id, subnet)\\n\\n    def delete_subnet(self, context, id):\\n        s = self.get_subnet(context, id)\\n        plugin = self._get_plugin_by_network_id(context,\\n                                                s[\'network_id\'])\\n        return plugin.delete_subnet(context, id)\\n\\n    def _extend_router_dict(self, context, router):\\n        flavor = self._get_flavor_by_router_id(context, router[\'id\'])\\n        router[FLAVOR_ROUTER] = flavor\\n\\n    def create_router(self, context, router):\\n        r = router[\'router\']\\n        flavor = r.get(FLAVOR_ROUTER)\\n        if str(flavor) not in self.l3_plugins:\\n            flavor = self.default_l3_flavor\\n        plugin = self._get_l3_plugin(flavor)\\n        r_in_db = plugin.create_router(context, router)\\n        LOG.debug(_(\\"Created router: %(router_id)s with flavor \\"\\n                    \\"%(flavor)s\\"),\\n                {\'router_id\': r_in_db[\'id\'], \'flavor\': flavor})\\n        try:\\n            meta_db_v2.add_router_flavor_binding(context.session,\\n                                                 flavor, str(r_in_db[\'id\']))\\n        except Exception:\\n            LOG.exception(_(\'Failed to add flavor bindings\'))\\n            plugin.delete_router(context, r_in_db[\'id\'])\\n            raise FaildToAddFlavorBinding()\\n\\n        LOG.debug(_(\\"Created router: %s\\"), r_in_db[\'id\'])\\n        self._extend_router_dict(context, r_in_db)\\n        return r_in_db\\n\\n    def update_router(self, context, id, router):\\n        flavor = meta_db_v2.get_flavor_by_router(context.session, id)\\n        plugin = self._get_l3_plugin(flavor)\\n        return plugin.update_router(context, id, router)\\n\\n    def delete_router(self, context, id):\\n        flavor = meta_db_v2.get_flavor_by_router(context.session, id)\\n        plugin = self._get_l3_plugin(flavor)\\n        return plugin.delete_router(context, id)\\n\\n    def get_router(self, context, id, fields=None):\\n        flavor = meta_db_v2.get_flavor_by_router(context.session, id)\\n        plugin = self._get_l3_plugin(flavor)\\n        router = plugin.get_router(context, id, fields)\\n        if not fields or FLAVOR_ROUTER in fields:\\n            self._extend_router_dict(context, router)\\n        return router\\n\\n    def get_routers_with_flavor(self, context, filters=None,\\n                                fields=None):\\n        collection = self._model_query(context, l3_db.Router)\\n        r_model = RouterFlavor\\n        collection = collection.join(r_model,\\n                                     l3_db.Router.id == r_model.router_id)\\n        if filters:\\n            for key, value in filters.iteritems():\\n                if key == FLAVOR_ROUTER:\\n                    column = RouterFlavor.flavor\\n                else:\\n                    column = getattr(l3_db.Router, key, None)\\n                if column:\\n                    collection = collection.filter(column.in_(value))\\n        return [self._make_router_dict(c, fields) for c in collection]\\n\\n    def get_routers(self, context, filters=None, fields=None):\\n        routers = self.get_routers_with_flavor(context, filters,\\n                                               None)\\n        return [self.get_router(context, router[\'id\'],\\n                                fields)\\n                for router in routers]\\n" }\n'
line: b'{ "repo_name": "libracore/erpnext", "ref": "refs/heads/v12", "path": "erpnext/setup/doctype/item_group/item_group.py", "content": "# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\\n# License: GNU General Public License v3. See license.txt\\n\\nfrom __future__ import unicode_literals\\nimport frappe\\nimport copy\\nfrom frappe import _\\nfrom frappe.utils import nowdate, cint, cstr\\nfrom frappe.utils.nestedset import NestedSet\\nfrom frappe.website.website_generator import WebsiteGenerator\\nfrom frappe.website.render import clear_cache\\nfrom frappe.website.doctype.website_slideshow.website_slideshow import get_slideshow\\nfrom erpnext.shopping_cart.product_info import set_product_info_for_website\\nfrom erpnext.utilities.product import get_qty_in_stock\\nfrom six.moves.urllib.parse import quote\\n\\nclass ItemGroup(NestedSet, WebsiteGenerator):\\n\\tnsm_parent_field = \'parent_item_group\'\\n\\twebsite = frappe._dict(\\n\\t\\tcondition_field = \\"show_in_website\\",\\n\\t\\ttemplate = \\"templates/generators/item_group.html\\",\\n\\t\\tno_cache = 1\\n\\t)\\n\\n\\tdef autoname(self):\\n\\t\\tself.name = self.item_group_name\\n\\n\\tdef validate(self):\\n\\t\\tsuper(ItemGroup, self).validate()\\n\\n\\t\\tif not self.parent_item_group and not frappe.flags.in_test:\\n\\t\\t\\tif frappe.db.exists(\\"Item Group\\", _(\'All Item Groups\')):\\n\\t\\t\\t\\tself.parent_item_group = _(\'All Item Groups\')\\n\\n\\t\\tself.make_route()\\n\\n\\tdef on_update(self):\\n\\t\\tNestedSet.on_update(self)\\n\\t\\tinvalidate_cache_for(self)\\n\\t\\tself.validate_name_with_item()\\n\\t\\tself.validate_one_root()\\n\\n\\tdef make_route(self):\\n\\t\\t\'\'\'Make website route\'\'\'\\n\\t\\tif not self.route:\\n\\t\\t\\tself.route = \'\'\\n\\t\\t\\tif self.parent_item_group:\\n\\t\\t\\t\\tparent_item_group = frappe.get_doc(\'Item Group\', self.parent_item_group)\\n\\n\\t\\t\\t\\t# make parent route only if not root\\n\\t\\t\\t\\tif parent_item_group.parent_item_group and parent_item_group.route:\\n\\t\\t\\t\\t\\tself.route = parent_item_group.route + \'/\'\\n\\n\\t\\t\\tself.route += self.scrub(self.item_group_name)\\n\\n\\t\\t\\treturn self.route\\n\\n\\tdef on_trash(self):\\n\\t\\tNestedSet.on_trash(self)\\n\\t\\tWebsiteGenerator.on_trash(self)\\n\\n\\tdef validate_name_with_item(self):\\n\\t\\tif frappe.db.exists(\\"Item\\", self.name):\\n\\t\\t\\tfrappe.throw(frappe._(\\"An item exists with same name ({0}), please change the item group name or rename the item\\").format(self.name), frappe.NameError)\\n\\n\\tdef get_context(self, context):\\n\\t\\tcontext.show_search=True\\n\\t\\tcontext.page_length = cint(frappe.db.get_single_value(\'Products Settings\', \'products_per_page\')) or 6\\n\\t\\tcontext.search_link = \'/product_search\'\\n\\n\\t\\tstart = int(frappe.form_dict.start or 0)\\n\\t\\tif start < 0:\\n\\t\\t\\tstart = 0\\n\\t\\tcontext.update({\\n\\t\\t\\t\\"items\\": get_product_list_for_group(product_group = self.name, start=start,\\n\\t\\t\\t\\tlimit=context.page_length + 1, search=frappe.form_dict.get(\\"search\\")),\\n\\t\\t\\t\\"parents\\": get_parent_item_groups(self.parent_item_group),\\n\\t\\t\\t\\"title\\": self.name\\n\\t\\t})\\n\\n\\t\\tif self.slideshow:\\n\\t\\t\\tcontext.update(get_slideshow(self))\\n\\n\\t\\treturn context\\n\\n@frappe.whitelist(allow_guest=True)\\ndef get_product_list_for_group(product_group=None, start=0, limit=10, search=None):\\n\\tif product_group:\\n\\t\\titem_group = frappe.get_cached_doc(\'Item Group\', product_group)\\n\\t\\tif item_group.is_group:\\n\\t\\t\\t# return child item groups if the type is of \\"Is Group\\"\\n\\t\\t\\treturn get_child_groups_for_list_in_html(item_group, start, limit, search)\\n\\n\\tchild_groups = \\", \\".join([frappe.db.escape(i[0]) for i in get_child_groups(product_group)])\\n\\n\\t# base query\\n\\tquery = \\"\\"\\"select I.name, I.item_name, I.item_code, I.route, I.image, I.website_image, I.thumbnail, I.item_group,\\n\\t\\t\\tI.description, I.web_long_description as website_description, I.is_stock_item,\\n\\t\\t\\tcase when (S.actual_qty - S.reserved_qty) > 0 then 1 else 0 end as in_stock, I.website_warehouse,\\n\\t\\t\\tI.has_batch_no\\n\\t\\tfrom `tabItem` I\\n\\t\\tleft join tabBin S on I.item_code = S.item_code and I.website_warehouse = S.warehouse\\n\\t\\twhere I.show_in_website = 1\\n\\t\\t\\tand I.disabled = 0\\n\\t\\t\\tand (I.end_of_life is null or I.end_of_life=\'0000-00-00\' or I.end_of_life > %(today)s)\\n\\t\\t\\tand (I.variant_of = \'\' or I.variant_of is null)\\n\\t\\t\\tand (I.item_group in ({child_groups})\\n\\t\\t\\tor I.name in (select parent from `tabWebsite Item Group` where item_group in ({child_groups})))\\n\\t\\t\\t\\"\\"\\".format(child_groups=child_groups)\\n\\t# search term condition\\n\\tif search:\\n\\t\\tquery += \\"\\"\\" and (I.web_long_description like %(search)s\\n\\t\\t\\t\\tor I.item_name like %(search)s\\n\\t\\t\\t\\tor I.name like %(search)s)\\"\\"\\"\\n\\t\\tsearch = \\"%\\" + cstr(search) + \\"%\\"\\n\\n\\tquery += \\"\\"\\"order by I.weightage desc, in_stock desc, I.modified desc limit %s, %s\\"\\"\\" % (start, limit)\\n\\n\\tdata = frappe.db.sql(query, {\\"product_group\\": product_group,\\"search\\": search, \\"today\\": nowdate()} as_dict=1)\\n\\tdata = adjust_qty_for_expired_items(data)\\n\\n\\tif cint(frappe.db.get_single_value(\\"Shopping Cart Settings\\", \\"enabled\\")):\\n\\t\\tfor item in data:\\n\\t\\t\\tset_product_info_for_website(item)\\n\\n\\treturn data\\n\\ndef get_child_groups_for_list_in_html(item_group, start, limit, search):\\n\\tsearch_filters = None\\n\\tif search_filters:\\n\\t\\tsearch_filters = [\\n\\t\\t\\tdict(name = (\'like\', \'%{}%\'.format(search))),\\n\\t\\t\\tdict(description = (\'like\', \'%{}%\'.format(search)))\\n\\t\\t]\\n\\tdata = frappe.db.get_all(\'Item Group\',\\n\\t\\tfields = [\'name\', \'route\', \'description\', \'image\'],\\n\\t\\tfilters = dict(\\n\\t\\t\\tshow_in_website = 1,\\n\\t\\t\\tlft = (\'>\', item_group.lft),\\n\\t\\t\\trgt = (\'<\', item_group.rgt),\\n\\t\\t),\\n\\t\\tor_filters = search_filters,\\n\\t\\torder_by = \'weightage desc, name asc\',\\n\\t\\tstart = start,\\n\\t\\tlimit = limit\\n\\t)\\n\\n\\treturn data\\n\\ndef adjust_qty_for_expired_items(data):\\n\\tadjusted_data = []\\n\\n\\tfor item in data:\\n\\t\\tif item.get(\'has_batch_no\') and item.get(\'website_warehouse\'):\\n\\t\\t\\tstock_qty_dict = get_qty_in_stock(\\n\\t\\t\\t\\titem.get(\'name\'), \'website_warehouse\', item.get(\'website_warehouse\'))\\n\\t\\t\\tqty = stock_qty_dict.stock_qty[0][0] if stock_qty_dict.stock_qty else 0\\n\\t\\t\\titem[\'in_stock\'] = 1 if qty else 0\\n\\t\\tadjusted_data.append(item)\\n\\n\\treturn adjusted_data\\n\\n\\ndef get_child_groups(item_group_name):\\n\\titem_group = frappe.get_doc(\\"Item Group\\", item_group_name)\\n\\treturn frappe.db.sql(\\"\\"\\"select name\\n\\t\\tfrom `tabItem Group` where lft>=%(lft)s and rgt<=%(rgt)s\\n\\t\\t\\tand show_in_website = 1\\"\\"\\", {\\"lft\\": item_group.lft, \\"rgt\\": item_group.rgt})\\n\\ndef get_item_for_list_in_html(context):\\n\\t# add missing absolute link in files\\n\\t# user may forget it during upload\\n\\tif (context.get(\\"website_image\\") or \\"\\").startswith(\\"files/\\"):\\n\\t\\tcontext[\\"website_image\\"] = \\"/\\" + quote(context[\\"website_image\\"])\\n\\n\\tcontext[\\"show_availability_status\\"] = cint(frappe.db.get_single_value(\'Products Settings\',\\n\\t\\t\'show_availability_status\'))\\n\\n\\tproducts_template = \'templates/includes/products_as_list.html\'\\n\\n\\treturn frappe.get_template(products_template).render(context)\\n\\ndef get_group_item_count(item_group):\\n\\tchild_groups = \\", \\".join([\'\\"\' + i[0] + \'\\"\' for i in get_child_groups(item_group)])\\n\\treturn frappe.db.sql(\\"\\"\\"select count(*) from `tabItem`\\n\\t\\twhere docstatus = 0 and show_in_website = 1\\n\\t\\tand (item_group in (%s)\\n\\t\\t\\tor name in (select parent from `tabWebsite Item Group`\\n\\t\\t\\t\\twhere item_group in (%s))) \\"\\"\\" % (child_groups, child_groups))[0][0]\\n\\n\\ndef get_parent_item_groups(item_group_name):\\n\\tbase_parents = [\\n\\t\\t{\\"name\\": frappe._(\\"Home\\"), \\"route\\":\\"/\\"}\\n\\t\\t{\\"name\\": frappe._(\\"All Products\\"), \\"route\\":\\"/all-products\\"}\\n\\t]\\n\\tif not item_group_name:\\n\\t\\treturn base_parents\\n\\n\\titem_group = frappe.get_doc(\\"Item Group\\", item_group_name)\\n\\tparent_groups = frappe.db.sql(\\"\\"\\"select name, route from `tabItem Group`\\n\\t\\twhere lft <= %s and rgt >= %s\\n\\t\\tand show_in_website=1\\n\\t\\torder by lft asc\\"\\"\\", (item_group.lft, item_group.rgt), as_dict=True)\\n\\n\\treturn base_parents + parent_groups\\n\\ndef invalidate_cache_for(doc, item_group=None):\\n\\tif not item_group:\\n\\t\\titem_group = doc.name\\n\\n\\tfor d in get_parent_item_groups(item_group):\\n\\t\\titem_group_name = frappe.db.get_value(\\"Item Group\\", d.get(\'name\'))\\n\\t\\tif item_group_name:\\n\\t\\t\\tclear_cache(frappe.db.get_value(\'Item Group\', item_group_name, \'route\'))\\n\\ndef get_item_group_defaults(item, company):\\n\\titem = frappe.get_cached_doc(\\"Item\\", item)\\n\\titem_group = frappe.get_cached_doc(\\"Item Group\\", item.item_group)\\n\\n\\tfor d in item_group.item_group_defaults or []:\\n\\t\\tif d.company == company:\\n\\t\\t\\trow = copy.deepcopy(d.as_dict())\\n\\t\\t\\trow.pop(\\"name\\")\\n\\t\\t\\treturn row\\n\\n\\treturn frappe._dict()\\n" }\n'
line: b'{ "repo_name": "ChameleonCloud/horizon", "ref": "refs/heads/chameleoncloud/train", "path": "openstack_dashboard/contrib/developer/resource_browser/urls.py", "content": "#    (c) Copyright 2015 Hewlett-Packard Development Company, L.P.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n# not use this file except in compliance with the License. You may obtain\\n# a copy of the License at\\n#\\n#      http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n# License for the specific language governing permissions and limitations\\n# under the License.\\n\\nfrom django.conf.urls import url\\n\\nfrom horizon.browsers.views import AngularIndexView\\n\\nurlpatterns = [\\n    url(\'\', AngularIndexView.as_view(), name=\'index\'),\\n]\\n" }\n'
line: b'{ "repo_name": "idea4bsd/idea4bsd", "ref": "refs/heads/idea4bsd-master", "path": "python/testData/inspections/PyNumpyType/Sort.py", "content": "def sort(self, axis=-1, kind=\'quicksort\', order=None): # real signature unknown; restored from __doc__\\n    \\"\\"\\"\\n    a.sort(axis=-1, kind=\'quicksort\', order=None)\\n\\n        Sort an array, in-place.\\n\\n        Parameters\\n        ----------\\n        axis : int, optional\\n            Axis along which to sort. Default is -1, which means sort along the\\n            last axis.\\n        kind : {\'quicksort\', \'mergesort\', \'heapsort\'} optional\\n            Sorting algorithm. Default is \'quicksort\'.\\n        order : list, optional\\n            When `a` is an array with fields defined, this argument specifies\\n            which fields to compare first, second, etc.  Not all fields need be\\n            specified.\\n\\n    \\"\\"\\"\\n    pass\\n\\na = np.array([(\'a\', 2), (\'c\', 1)], dtype=[(\'x\', \'S1\'), (\'y\', int)])\\nprint(sort(a, order=\'y\'))" }\n'
line: b'{ "repo_name": "AmritaLonkar/trunk", "ref": "refs/heads/arcjet", "path": "SU2_PY/SU2/io/redirect.py", "content": "## \\\\file redirect.py\\n#  \\\\brief python package for file redirection \\n#  \\\\author Trent Lukaczyk, Aerospace Design Laboratory (Stanford University) <http://su2.stanford.edu>.\\n#  \\\\version 2.0.6\\n#\\n# Stanford University Unstructured (SU2) Code\\n# Copyright (C) 2012 Aerospace Design Laboratory\\n#\\n# This program is free software: you can redistribute it and/or modify\\n# it under the terms of the GNU General Public License as published by\\n# the Free Software Foundation, either version 3 of the License, or\\n# (at your option) any later version.\\n#\\n# This program is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU General Public License for more details.\\n#\\n# You should have received a copy of the GNU General Public License\\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\\n\\n# ----------------------------------------------------------------------\\n#  Imports\\n# ----------------------------------------------------------------------\\n\\nimport os, sys, shutil, copy, glob\\nfrom .tools import add_suffix, make_link, expand_part\\n\\n# -------------------------------------------------------------------\\n#  Output Redirection \\n# -------------------------------------------------------------------\\n# original source: http://stackoverflow.com/questions/6796492/python-temporarily-redirect-stdout-stderr\\nclass output(object):\\n    \'\'\' with SU2.io.redirect_output(stdout,stderr)\\n    \\n        Temporarily redirects sys.stdout and sys.stderr when used in\\n        a \'with\' contextmanager\\n        \\n        Example:\\n        with SU2.io.redirect_output(\'stdout.txt\',\'stderr.txt\'):\\n            sys.stdout.write(\\"standard out\\")\\n            sys.stderr.write(\\"stanrard error\\")\\n            # code\\n        #: with output redirection\\n        \\n        Inputs:\\n            stdout - None, a filename, or a file stream\\n            stderr - None, a filename, or a file stream\\n        None will not redirect outptu\\n        \\n    \'\'\'\\n    def __init__(self, stdout=None, stderr=None):\\n        \\n        _newout = False\\n        _newerr = False\\n        \\n        if isinstance(stdout,str):\\n            stdout = open(stdout,\'a\')\\n            _newout = True            \\n        if isinstance(stderr,str):\\n            stderr = open(stderr,\'a\')\\n            _newerr = True                   \\n                \\n        self._stdout = stdout or sys.stdout\\n        self._stderr = stderr or sys.stderr\\n        self._newout = _newout\\n        self._newerr = _newerr\\n\\n    def __enter__(self):\\n        self.old_stdout, self.old_stderr = sys.stdout, sys.stderr\\n        self.old_stdout.flush(); self.old_stderr.flush()\\n        sys.stdout, sys.stderr = self._stdout, self._stderr\\n\\n    def __exit__(self, exc_type, exc_value, traceback):\\n        self._stdout.flush(); self._stderr.flush()\\n        sys.stdout = self.old_stdout\\n        sys.stderr = self.old_stderr\\n        \\n        if self._newout:\\n            self._stdout.close()\\n        if self._newerr:\\n            self._stderr.close()           \\n\\n#: class output()\\n\\n\\n# -------------------------------------------------------------------\\n#  Folder Redirection \\n# -------------------------------------------------------------------\\nclass folder(object):\\n    \'\'\' with SU2.io.redirect_folder(folder,pull,link,force) as push\\n    \\n        Temporarily redirects to a working folder, pulling \\n        and pushing needed files\\n        \\n        Example:\\n        \\n        folder = \'temp\'                    \\n        pull   = [\'file1.txt\',\'file2.txt\'] \\n        link   = [\'file3.big\']             \\n        force  = True                      \\n        \\n        # original path\\n        import os\\n        print os.getcwd()\\n        \\n        # enter folder\\n        with SU2.io.redirect_folder(folder,pull,link,force) as push:\\n            print os.getcwd()\\n            # code\\n            push.append(\'file4.txt\')\\n        #: with folder redirection\\n        \\n        # returned to original path\\n        print os.getcwd()\\n        \\n        Inputs:\\n            folder - working folder, relative or absolute\\n            pull   - list of files to pull (copy to working folder)\\n            link   - list of files to link (symbolic link in working folder)\\n            force  - True/False overwrite existing files in working folder\\n        \\n        Targets:\\n            push   - list of files to push (copy to originating path)\\n        \\n        Notes:\\n            push must be appended or extended, not overwritten\\n            links in Windows not supported, will simply copy\\n    \'\'\'\\n    \\n    def __init__(self, folder, pull=None, link=None, force=True ):\\n        \'\'\' folder redirection initialization\\n            see help( folder ) for more info\\n        \'\'\'\\n        \\n        if pull is None: pull = []\\n        if link is None: link = []\\n        \\n        if not isinstance(pull,list) : pull = [pull]\\n        if not isinstance(link,list) : link = [link]\\n        \\n        origin = os.getcwd()\\n        origin = os.path.abspath(origin).rstrip(\'/\')+\'/\'\\n        folder = os.path.abspath(folder).rstrip(\'/\')+\'/\'\\n        \\n        self.origin = origin\\n        self.folder = folder\\n        self.pull   = copy.deepcopy(pull)\\n        self.push   = []\\n        self.link   = copy.deepcopy(link)\\n        self.force  = force\\n\\n    def __enter__(self): \\n        \\n        origin = self.origin  # absolute path\\n        folder = self.folder  # absolute path\\n        pull   = self.pull\\n        push   = self.push\\n        link   = self.link\\n        force  = self.force\\n        \\n        # check for no folder change\\n        if folder == origin:\\n            return []\\n        \\n        # relative folder path\\n        #relative = os.path.relpath(folder,origin)\\n        \\n        # check, make folder\\n        if not os.path.exists(folder):\\n            os.makedirs(folder)\\n        \\n        # copy pull files\\n        for name in pull:\\n            old_name = os.path.abspath(name)\\n            new_name = os.path.split(name)[-1]\\n            new_name = os.path.join(folder,new_name)\\n            if old_name == new_name: continue\\n            if os.path.exists( new_name ): \\n                if force: os.remove( new_name )\\n                else: continue\\n            shutil.copy(old_name,new_name)\\n\\n        # make links\\n        for name in link:\\n            old_name = os.path.abspath(name)\\n            new_name = os.path.split(name)[-1]\\n            new_name = os.path.join(folder,new_name)\\n            if old_name == new_name: continue\\n            if os.path.exists( new_name ): \\n                if force: os.remove( new_name )\\n                else: continue\\n            make_link(old_name,new_name)\\n            \\n        # change directory\\n        os.chdir(folder)\\n        \\n        # return empty list to append with files to push to super folder\\n        return push\\n\\n    def __exit__(self, exc_type, exc_value, traceback):\\n        \\n        origin = self.origin\\n        folder = self.folder\\n        push   = self.push\\n        force  = self.force\\n        \\n        # check for no folder change\\n        if folder == origin:\\n            return\\n        \\n        # move assets\\n        for name in push:\\n            \\n            old_name = os.path.abspath(name)\\n            name = os.path.split(name)[-1]\\n            new_name = os.path.join(origin,name)\\n            \\n            # links\\n            if os.path.islink(old_name):\\n                source = os.path.realpath(old_name)\\n                if source == new_name: continue\\n                if os.path.exists( new_name ):\\n                    if force: os.remove( new_name )\\n                    else: continue\\n                make_link(source,new_name)\\n            \\n            # moves\\n            else:\\n                if old_name == new_name: continue\\n                if os.path.exists( new_name ):\\n                    if force: os.remove( new_name )\\n                    else: continue\\n                shutil.move(old_name,new_name)\\n            \\n        # change directory\\n        os.chdir(origin)\\n        \\n#: class folder()\\n" }\n'
line: b'{ "repo_name": "mferenca/HMS-notifier", "ref": "refs/heads/HMS-Notifier", "path": "notifier/tests/test_commands.py", "content": "\\"\\"\\"\\n\\"\\"\\"\\nimport datetime\\nimport json\\nfrom os.path import dirname, join\\n\\nfrom django.conf import settings\\nfrom django.test import TestCase\\nfrom django.test.utils import override_settings\\nfrom mock import patch, Mock\\n\\nfrom notifier.management.commands import forums_digest\\n\\nclass CommandsTestCase(TestCase):\\n\\n    \\"\\"\\"\\n    \\"\\"\\"\\n\\n    @override_settings(CELERY_EAGER_PROPAGATES_EXCEPTIONS=True,\\n                       CELERY_ALWAYS_EAGER=True,\\n                       BROKER_BACKEND=\'memory\',)\\n    def test_forums_digest(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "BiznetGIO/horizon", "ref": "refs/heads/stable/pike-gio", "path": "openstack_dashboard/contrib/developer/resource_browser/urls.py", "content": "#    (c) Copyright 2015 Hewlett-Packard Development Company, L.P.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n# not use this file except in compliance with the License. You may obtain\\n# a copy of the License at\\n#\\n#      http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n# License for the specific language governing permissions and limitations\\n# under the License.\\n\\nfrom django.conf.urls import url\\n\\nfrom horizon.browsers.views import AngularIndexView\\n\\nurlpatterns = [\\n    url(\'\', AngularIndexView.as_view(), name=\'index\'),\\n]\\n" }\n'
line: b'{ "repo_name": "lucafavatella/intellij-community", "ref": "refs/heads/cli-wip", "path": "python/testData/inspections/PyNumpyType/Sort.py", "content": "def sort(self, axis=-1, kind=\'quicksort\', order=None): # real signature unknown; restored from __doc__\\n    \\"\\"\\"\\n    a.sort(axis=-1, kind=\'quicksort\', order=None)\\n\\n        Sort an array, in-place.\\n\\n        Parameters\\n        ----------\\n        axis : int, optional\\n            Axis along which to sort. Default is -1, which means sort along the\\n            last axis.\\n        kind : {\'quicksort\', \'mergesort\', \'heapsort\'} optional\\n            Sorting algorithm. Default is \'quicksort\'.\\n        order : list, optional\\n            When `a` is an array with fields defined, this argument specifies\\n            which fields to compare first, second, etc.  Not all fields need be\\n            specified.\\n\\n    \\"\\"\\"\\n    pass\\n\\na = np.array([(\'a\', 2), (\'c\', 1)], dtype=[(\'x\', \'S1\'), (\'y\', int)])\\nprint(sort(a, order=\'y\'))" }\n'
line: b'{ "repo_name": "appsembler/edx-platform", "ref": "refs/heads/appsembler/tahoe/master", "path": "cms/djangoapps/appsembler_tiers/views.py", "content": "\\"\\"\\"\\nStudio views for the tiers app.\\n\\"\\"\\"\\n\\nfrom django.contrib.auth.decorators import login_required\\nfrom django.utils.decorators import method_decorator\\nfrom django.views.generic import TemplateView\\n\\nfrom openedx.core.djangoapps.appsembler.sites.utils import get_single_user_organization\\n\\n\\nclass SiteUnavailableRedirectView(TemplateView):\\n    \\"\\"\\"\\n    Studio Site Unavailable view.\\n\\n    This works in the Studio and shows a message.\\n    \\"\\"\\"\\n    template_name = \'site-unavailable.html\'\\n\\n    def get_context_data(self, **kwargs):\\n        context = super(SiteUnavailableRedirectView, self).get_context_data(**kwargs)\\n        context[\'organization\'] = get_single_user_organization(self.request.user)\\n        return context\\n\\n    @method_decorator(login_required)\\n    def get(self, request, *args, **kwargs):\\n        return super(SiteUnavailableRedirectView, self).get(request, *args, **kwargs)\\n" }\n'
line: b'{ "repo_name": "NeCTAR-RC/horizon", "ref": "refs/heads/nectar/stein", "path": "openstack_dashboard/contrib/developer/resource_browser/urls.py", "content": "#    (c) Copyright 2015 Hewlett-Packard Development Company, L.P.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n# not use this file except in compliance with the License. You may obtain\\n# a copy of the License at\\n#\\n#      http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n# License for the specific language governing permissions and limitations\\n# under the License.\\n\\nfrom django.conf.urls import url\\n\\nfrom horizon.browsers.views import AngularIndexView\\n\\nurlpatterns = [\\n    url(\'\', AngularIndexView.as_view(), name=\'index\'),\\n]\\n" }\n'
line: b'{ "repo_name": "libracore/erpnext", "ref": "refs/heads/v12", "path": "erpnext/config/stock.py", "content": "from __future__ import unicode_literals\\nfrom frappe import _\\n\\ndef get_data():\\n\\treturn [\\n\\t\\t{\\n\\t\\t\\t\\"label\\": _(\\"Stock Transactions\\"),\\n\\t\\t\\t\\"items\\": [\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Stock Entry\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t\\t\\"dependencies\\": [\\"Item\\"],\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Delivery Note\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t\\t\\"dependencies\\": [\\"Item\\", \\"Customer\\"],\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Purchase Receipt\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t\\t\\"dependencies\\": [\\"Item\\", \\"Supplier\\"],\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Material Request\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t\\t\\"dependencies\\": [\\"Item\\"],\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Pick List\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t\\t\\"dependencies\\": [\\"Item\\"],\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Delivery Trip\\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t]\\n\\t\\t}\\n\\t\\t{\\n\\t\\t\\t\\"label\\": _(\\"Stock Reports\\"),\\n\\t\\t\\t\\"items\\": [\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"is_query_report\\": True,\\n\\t\\t\\t\\t\\t\\"name\\": \\"Stock Ledger\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Stock Ledger Entry\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t\\t\\"dependencies\\": [\\"Item\\"],\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"is_query_report\\": True,\\n\\t\\t\\t\\t\\t\\"name\\": \\"Stock Balance\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Stock Ledger Entry\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t\\t\\"dependencies\\": [\\"Item\\"],\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"is_query_report\\": True,\\n\\t\\t\\t\\t\\t\\"name\\": \\"Stock Projected Qty\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Item\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t\\t\\"dependencies\\": [\\"Item\\"],\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"page\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"stock-balance\\",\\n\\t\\t\\t\\t\\t\\"label\\": _(\\"Stock Summary\\"),\\n\\t\\t\\t\\t\\t\\"dependencies\\": [\\"Item\\"],\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"is_query_report\\": True,\\n\\t\\t\\t\\t\\t\\"name\\": \\"Stock Ageing\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Item\\",\\n\\t\\t\\t\\t\\t\\"dependencies\\": [\\"Item\\"],\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"is_query_report\\": True,\\n\\t\\t\\t\\t\\t\\"name\\": \\"Item Price Stock\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Item\\",\\n\\t\\t\\t\\t\\t\\"dependencies\\": [\\"Item\\"],\\n\\t\\t\\t\\t}\\n\\t\\t\\t]\\n\\t\\t}\\n\\t\\t{\\n\\t\\t\\t\\"label\\": _(\\"Settings\\"),\\n\\t\\t\\t\\"icon\\": \\"fa fa-cog\\",\\n\\t\\t\\t\\"items\\": [\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Stock Settings\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Warehouse\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"UOM\\",\\n\\t\\t\\t\\t\\t\\"label\\": _(\\"Unit of Measure\\") + \\" (UOM)\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Brand\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Item Attribute\\",\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Item Variant Settings\\",\\n\\t\\t\\t\\t}\\n\\t\\t\\t]\\n\\t\\t}\\n\\t\\t{\\n\\t\\t\\t\\"label\\": _(\\"Items and Pricing\\"),\\n\\t\\t\\t\\"items\\": [\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Item\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Product Bundle\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Item Group\\",\\n\\t\\t\\t\\t\\t\\"icon\\": \\"fa fa-sitemap\\",\\n\\t\\t\\t\\t\\t\\"label\\": _(\\"Item Group\\"),\\n\\t\\t\\t\\t\\t\\"link\\": \\"Tree/Item Group\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Price List\\",\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Item Price\\",\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Shipping Rule\\",\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Pricing Rule\\",\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Item Alternative\\",\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Item Manufacturer\\",\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Item Variant Settings\\",\\n\\t\\t\\t\\t}\\n\\t\\t\\t]\\n\\t\\t}\\n\\t\\t{\\n\\t\\t\\t\\"label\\": _(\\"Serial No and Batch\\"),\\n\\t\\t\\t\\"items\\": [\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Serial No\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t\\t\\"dependencies\\": [\\"Item\\"],\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Batch\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t\\t\\"dependencies\\": [\\"Item\\"],\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Installation Note\\",\\n\\t\\t\\t\\t\\t\\"dependencies\\": [\\"Item\\"],\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Serial No Service Contract Expiry\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Serial No\\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Serial No Status\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Serial No\\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Serial No Warranty Expiry\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Serial No\\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t]\\n\\t\\t}\\n\\t\\t{\\n\\t\\t\\t\\"label\\": _(\\"Tools\\"),\\n\\t\\t\\t\\"icon\\": \\"fa fa-wrench\\",\\n\\t\\t\\t\\"items\\": [\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Stock Reconciliation\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Landed Cost Voucher\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Packing Slip\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Quality Inspection\\",\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"doctype\\",\\n\\t\\t\\t\\t\\t\\"name\\": \\"Quality Inspection Template\\",\\n\\t\\t\\t\\t}\\n\\t\\t\\t]\\n\\t\\t}\\n\\t\\t{\\n\\t\\t\\t\\"label\\": _(\\"Key Reports\\"),\\n\\t\\t\\t\\"icon\\": \\"fa fa-table\\",\\n\\t\\t\\t\\"items\\": [\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"is_query_report\\": False,\\n\\t\\t\\t\\t\\t\\"name\\": \\"Item-wise Price List Rate\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Item Price\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"is_query_report\\": True,\\n\\t\\t\\t\\t\\t\\"name\\": \\"Stock Analytics\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Stock Entry\\",\\n\\t\\t\\t\\t\\t\\"onboard\\": 1,\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"is_query_report\\": True,\\n\\t\\t\\t\\t\\t\\"name\\": \\"Delivery Note Trends\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Delivery Note\\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"is_query_report\\": True,\\n\\t\\t\\t\\t\\t\\"name\\": \\"Purchase Receipt Trends\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Purchase Receipt\\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"is_query_report\\": True,\\n\\t\\t\\t\\t\\t\\"name\\": \\"Ordered Items To Be Delivered\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Delivery Note\\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"is_query_report\\": True,\\n\\t\\t\\t\\t\\t\\"name\\": \\"Purchase Order Items To Be Received\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Purchase Receipt\\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"is_query_report\\": True,\\n\\t\\t\\t\\t\\t\\"name\\": \\"Item Shortage Report\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Bin\\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"is_query_report\\": True,\\n\\t\\t\\t\\t\\t\\"name\\": \\"Batch-Wise Balance History\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Batch\\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t]\\n\\t\\t}\\n\\t\\t{\\n\\t\\t\\t\\"label\\": _(\\"Other Reports\\"),\\n\\t\\t\\t\\"icon\\": \\"fa fa-list\\",\\n\\t\\t\\t\\"items\\": [\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"is_query_report\\": True,\\n\\t\\t\\t\\t\\t\\"name\\": \\"Requested Items To Be Transferred\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Material Request\\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"is_query_report\\": True,\\n\\t\\t\\t\\t\\t\\"name\\": \\"Batch Item Expiry Status\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Stock Ledger Entry\\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"is_query_report\\": True,\\n\\t\\t\\t\\t\\t\\"name\\": \\"Item Prices\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Price List\\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"is_query_report\\": True,\\n\\t\\t\\t\\t\\t\\"name\\": \\"Itemwise Recommended Reorder Level\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Item\\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"type\\": \\"report\\",\\n\\t\\t\\t\\t\\t\\"is_query_report\\": True,\\n\\t\\t\\t\\t\\t\\"name\\": \\"Item Variant Details\\",\\n\\t\\t\\t\\t\\t\\"doctype\\": \\"Item\\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t]\\n\\t\\t}\\n\\n\\t]\\n" }\n'
line: b'{ "repo_name": "nttks/notifier", "ref": "refs/heads/gacco/dogwood", "path": "notifier/tests/test_commands.py", "content": "\\"\\"\\"\\n\\"\\"\\"\\nimport datetime\\nimport json\\nfrom os.path import dirname, join\\n\\nfrom django.conf import settings\\nfrom django.test import TestCase\\nfrom django.test.utils import override_settings\\nfrom mock import patch, Mock\\n\\nfrom notifier.management.commands import forums_digest\\n\\nclass CommandsTestCase(TestCase):\\n\\n    \\"\\"\\"\\n    \\"\\"\\"\\n\\n    @override_settings(CELERY_EAGER_PROPAGATES_EXCEPTIONS=True,\\n                       CELERY_ALWAYS_EAGER=True,\\n                       BROKER_BACKEND=\'memory\',)\\n    def test_forums_digest(self):\\n        pass\\n" }\n'
line: b'{ "repo_name": "pllim/astropy", "ref": "refs/heads/placeholder", "path": "astropy/nddata/tests/test_compat.py", "content": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\\n# This module contains tests of a class equivalent to pre-1.0 NDData.\\n\\n\\nimport pytest\\nimport numpy as np\\n\\nfrom astropy.nddata.nddata import NDData\\nfrom astropy.nddata.compat import NDDataArray\\nfrom astropy.nddata.nduncertainty import StdDevUncertainty\\nfrom astropy.wcs import WCS\\nfrom astropy import units as u\\n\\n\\nNDDATA_ATTRIBUTES = [\'mask\', \'flags\', \'uncertainty\', \'unit\', \'shape\', \'size\',\\n                     \'dtype\', \'ndim\', \'wcs\', \'convert_unit_to\']\\n\\n\\ndef test_nddataarray_has_attributes_of_old_nddata():\\n    ndd = NDDataArray([1, 2, 3])\\n    for attr in NDDATA_ATTRIBUTES:\\n        assert hasattr(ndd, attr)\\n\\n\\ndef test_nddata_simple():\\n    nd = NDDataArray(np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n\\n\\ndef test_nddata_parameters():\\n    # Test for issue 4620\\n    nd = NDDataArray(data=np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    # Change order; `data` has to be given explicitly here\\n    nd = NDDataArray(meta={} data=np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    # Pass uncertainty as second implicit argument\\n    data = np.zeros((10, 10))\\n    uncertainty = StdDevUncertainty(0.1 + np.zeros_like(data))\\n    nd = NDDataArray(data, uncertainty)\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    assert nd.uncertainty == uncertainty\\n\\n\\ndef test_nddata_conversion():\\n    nd = NDDataArray(np.array([[1, 2, 3], [4, 5, 6]]))\\n    assert nd.size == 6\\n    assert nd.dtype == np.dtype(int)\\n\\n\\n@pytest.mark.parametrize(\'flags_in\', [\\n                         np.array([True, False]),\\n                         np.array([1, 0]),\\n                         [True, False],\\n                         [1, 0],\\n                         np.array([\'a\', \'b\']),\\n                         [\'a\', \'b\']])\\ndef test_nddata_flags_init_without_np_array(flags_in):\\n    ndd = NDDataArray([1, 1], flags=flags_in)\\n    assert (ndd.flags == flags_in).all()\\n\\n\\n@pytest.mark.parametrize((\'shape\'), [(10,), (5, 5), (3, 10, 10)])\\ndef test_nddata_flags_invalid_shape(shape):\\n    with pytest.raises(ValueError) as exc:\\n        NDDataArray(np.zeros((10, 10)), flags=np.ones(shape))\\n    assert exc.value.args[0] == \'dimensions of flags do not match data\'\\n\\n\\ndef test_convert_unit_to():\\n    # convert_unit_to should return a copy of its input\\n    d = NDDataArray(np.ones((5, 5)))\\n    d.unit = \'km\'\\n    d.uncertainty = StdDevUncertainty(0.1 + np.zeros_like(d))\\n    # workaround because zeros_like does not support dtype arg until v1.6\\n    # and NDData accepts only bool ndarray as mask\\n    tmp = np.zeros_like(d.data)\\n    d.mask = np.array(tmp, dtype=bool)\\n    d1 = d.convert_unit_to(\'m\')\\n    assert np.all(d1.data == np.array(1000.0))\\n    assert np.all(d1.uncertainty.array == 1000.0 * d.uncertainty.array)\\n    assert d1.unit == u.m\\n    # changing the output mask should not change the original\\n    d1.mask[0, 0] = True\\n    assert d.mask[0, 0] != d1.mask[0, 0]\\n    d.flags = np.zeros_like(d.data)\\n    d1 = d.convert_unit_to(\'m\')\\n\\n\\n# check that subclasses can require wcs and/or unit to be present and use\\n# _arithmetic and convert_unit_to\\nclass SubNDData(NDDataArray):\\n    \\"\\"\\"\\n    Subclass for test initialization of subclasses in NDData._arithmetic and\\n    NDData.convert_unit_to\\n    \\"\\"\\"\\n    def __init__(self, *arg, **kwd):\\n        super().__init__(*arg, **kwd)\\n        if self.unit is None:\\n            raise ValueError(\\"Unit for subclass must be specified\\")\\n        if self.wcs is None:\\n            raise ValueError(\\"WCS for subclass must be specified\\")\\n\\n\\ndef test_init_of_subclass_in_convert_unit_to():\\n    data = np.ones([10, 10])\\n    arr1 = SubNDData(data, unit=\'m\', wcs=WCS(naxis=2))\\n    result = arr1.convert_unit_to(\'km\')\\n    np.testing.assert_array_equal(arr1.data, 1000 * result.data)\\n\\n\\n# Test for issue #4129:\\ndef test_nddataarray_from_nddataarray():\\n    ndd1 = NDDataArray([1., 4., 9.],\\n                       uncertainty=StdDevUncertainty([1., 2., 3.]),\\n                       flags=[0, 1, 0])\\n    ndd2 = NDDataArray(ndd1)\\n    # Test that the 2 instances point to the same objects and aren\'t just\\n    # equal; this is explicitly documented for the main data array and we\\n    # probably want to catch any future change in behavior for the other\\n    # attributes too and ensure they are intentional.\\n    assert ndd2.data is ndd1.data\\n    assert ndd2.uncertainty is ndd1.uncertainty\\n    assert ndd2.flags is ndd1.flags\\n    assert ndd2.meta == ndd1.meta\\n\\n\\n# Test for issue #4137:\\ndef test_nddataarray_from_nddata():\\n    ndd1 = NDData([1., 4., 9.],\\n                  uncertainty=StdDevUncertainty([1., 2., 3.]))\\n    ndd2 = NDDataArray(ndd1)\\n\\n    assert ndd2.data is ndd1.data\\n    assert ndd2.uncertainty is ndd1.uncertainty\\n    assert ndd2.meta == ndd1.meta\\n" }\n'
line: b'{ "repo_name": "arpitn30/open-event-orga-server", "ref": "refs/heads/development", "path": "migrations/versions/ed4b4ba3274e_.py", "content": "\\"\\"\\"empty message\\n\\nRevision ID: ed4b4ba3274e\\nRevises: 784a1fc57171\\nCreate Date: 2016-06-16 06:08:49.516538\\n\\n\\"\\"\\"\\n\\n# revision identifiers, used by Alembic.\\nrevision = \'ed4b4ba3274e\'\\ndown_revision = \'784a1fc57171\'\\n\\nfrom alembic import op\\nimport sqlalchemy as sa\\nimport sqlalchemy_utils\\n\\n\\ndef upgrade():\\n    ### commands auto generated by Alembic - please adjust! ###\\n    op.create_table(\'service\',\\n    sa.Column(\'id\', sa.Integer(), nullable=False),\\n    sa.Column(\'name\', sa.String(), nullable=False),\\n    sa.PrimaryKeyConstraint(\'id\'),\\n    sa.UniqueConstraint(\'name\')\\n    )\\n    op.add_column(u\'permissions\', sa.Column(\'can_create\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'can_delete\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'can_read\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'can_update\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'role_id\', sa.Integer(), nullable=True))\\n    op.alter_column(u\'permissions\', \'service_id\',\\n               existing_type=sa.INTEGER(),\\n               nullable=True)\\n    op.create_unique_constraint(\'role_service_uc\', \'permissions\', [\'role_id\', \'service_id\'])\\n    op.drop_constraint(u\'user_service_uc\', \'permissions\', type_=\'unique\')\\n    op.drop_constraint(u\'permissions_user_id_fkey\', \'permissions\', type_=\'foreignkey\')\\n    op.create_foreign_key(None, \'permissions\', \'role\', [\'role_id\'], [\'id\'])\\n    op.create_foreign_key(None, \'permissions\', \'service\', [\'service_id\'], [\'id\'])\\n    op.drop_column(u\'permissions\', \'user_id\')\\n    op.drop_column(u\'permissions\', \'modes\')\\n    op.drop_column(u\'permissions\', \'service\')\\n    op.alter_column(u\'role\', \'name\',\\n               existing_type=sa.VARCHAR(length=128),\\n               nullable=False)\\n    op.create_unique_constraint(None, \'role\', [\'name\'])\\n    op.drop_column(u\'user\', \'role\')\\n    ### end Alembic commands ###\\n\\n\\ndef downgrade():\\n    ### commands auto generated by Alembic - please adjust! ###\\n    op.add_column(u\'user\', sa.Column(\'role\', sa.VARCHAR(), autoincrement=False, nullable=True))\\n    op.drop_constraint(None, \'role\', type_=\'unique\')\\n    op.alter_column(u\'role\', \'name\',\\n               existing_type=sa.VARCHAR(length=128),\\n               nullable=True)\\n    op.add_column(u\'permissions\', sa.Column(\'service\', sa.VARCHAR(), autoincrement=False, nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'modes\', sa.INTEGER(), autoincrement=False, nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'user_id\', sa.INTEGER(), autoincrement=False, nullable=False))\\n    op.drop_constraint(None, \'permissions\', type_=\'foreignkey\')\\n    op.drop_constraint(None, \'permissions\', type_=\'foreignkey\')\\n    op.create_foreign_key(u\'permissions_user_id_fkey\', \'permissions\', \'user\', [\'user_id\'], [\'id\'])\\n    op.create_unique_constraint(u\'user_service_uc\', \'permissions\', [\'user_id\', \'service\', \'service_id\'])\\n    op.drop_constraint(\'role_service_uc\', \'permissions\', type_=\'unique\')\\n    op.alter_column(u\'permissions\', \'service_id\',\\n               existing_type=sa.INTEGER(),\\n               nullable=False)\\n    op.drop_column(u\'permissions\', \'role_id\')\\n    op.drop_column(u\'permissions\', \'can_update\')\\n    op.drop_column(u\'permissions\', \'can_read\')\\n    op.drop_column(u\'permissions\', \'can_delete\')\\n    op.drop_column(u\'permissions\', \'can_create\')\\n    op.drop_table(\'service\')\\n    ### end Alembic commands ###\\n" }\n'
line: b'{ "repo_name": "lpsinger/astropy", "ref": "refs/heads/main", "path": "astropy/nddata/tests/test_compat.py", "content": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\\n# This module contains tests of a class equivalent to pre-1.0 NDData.\\n\\n\\nimport pytest\\nimport numpy as np\\n\\nfrom astropy.nddata.nddata import NDData\\nfrom astropy.nddata.compat import NDDataArray\\nfrom astropy.nddata.nduncertainty import StdDevUncertainty\\nfrom astropy.wcs import WCS\\nfrom astropy import units as u\\n\\n\\nNDDATA_ATTRIBUTES = [\'mask\', \'flags\', \'uncertainty\', \'unit\', \'shape\', \'size\',\\n                     \'dtype\', \'ndim\', \'wcs\', \'convert_unit_to\']\\n\\n\\ndef test_nddataarray_has_attributes_of_old_nddata():\\n    ndd = NDDataArray([1, 2, 3])\\n    for attr in NDDATA_ATTRIBUTES:\\n        assert hasattr(ndd, attr)\\n\\n\\ndef test_nddata_simple():\\n    nd = NDDataArray(np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n\\n\\ndef test_nddata_parameters():\\n    # Test for issue 4620\\n    nd = NDDataArray(data=np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    # Change order; `data` has to be given explicitly here\\n    nd = NDDataArray(meta={} data=np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    # Pass uncertainty as second implicit argument\\n    data = np.zeros((10, 10))\\n    uncertainty = StdDevUncertainty(0.1 + np.zeros_like(data))\\n    nd = NDDataArray(data, uncertainty)\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    assert nd.uncertainty == uncertainty\\n\\n\\ndef test_nddata_conversion():\\n    nd = NDDataArray(np.array([[1, 2, 3], [4, 5, 6]]))\\n    assert nd.size == 6\\n    assert nd.dtype == np.dtype(int)\\n\\n\\n@pytest.mark.parametrize(\'flags_in\', [\\n                         np.array([True, False]),\\n                         np.array([1, 0]),\\n                         [True, False],\\n                         [1, 0],\\n                         np.array([\'a\', \'b\']),\\n                         [\'a\', \'b\']])\\ndef test_nddata_flags_init_without_np_array(flags_in):\\n    ndd = NDDataArray([1, 1], flags=flags_in)\\n    assert (ndd.flags == flags_in).all()\\n\\n\\n@pytest.mark.parametrize((\'shape\'), [(10,), (5, 5), (3, 10, 10)])\\ndef test_nddata_flags_invalid_shape(shape):\\n    with pytest.raises(ValueError) as exc:\\n        NDDataArray(np.zeros((10, 10)), flags=np.ones(shape))\\n    assert exc.value.args[0] == \'dimensions of flags do not match data\'\\n\\n\\ndef test_convert_unit_to():\\n    # convert_unit_to should return a copy of its input\\n    d = NDDataArray(np.ones((5, 5)))\\n    d.unit = \'km\'\\n    d.uncertainty = StdDevUncertainty(0.1 + np.zeros_like(d))\\n    # workaround because zeros_like does not support dtype arg until v1.6\\n    # and NDData accepts only bool ndarray as mask\\n    tmp = np.zeros_like(d.data)\\n    d.mask = np.array(tmp, dtype=bool)\\n    d1 = d.convert_unit_to(\'m\')\\n    assert np.all(d1.data == np.array(1000.0))\\n    assert np.all(d1.uncertainty.array == 1000.0 * d.uncertainty.array)\\n    assert d1.unit == u.m\\n    # changing the output mask should not change the original\\n    d1.mask[0, 0] = True\\n    assert d.mask[0, 0] != d1.mask[0, 0]\\n    d.flags = np.zeros_like(d.data)\\n    d1 = d.convert_unit_to(\'m\')\\n\\n\\n# check that subclasses can require wcs and/or unit to be present and use\\n# _arithmetic and convert_unit_to\\nclass SubNDData(NDDataArray):\\n    \\"\\"\\"\\n    Subclass for test initialization of subclasses in NDData._arithmetic and\\n    NDData.convert_unit_to\\n    \\"\\"\\"\\n    def __init__(self, *arg, **kwd):\\n        super().__init__(*arg, **kwd)\\n        if self.unit is None:\\n            raise ValueError(\\"Unit for subclass must be specified\\")\\n        if self.wcs is None:\\n            raise ValueError(\\"WCS for subclass must be specified\\")\\n\\n\\ndef test_init_of_subclass_in_convert_unit_to():\\n    data = np.ones([10, 10])\\n    arr1 = SubNDData(data, unit=\'m\', wcs=WCS(naxis=2))\\n    result = arr1.convert_unit_to(\'km\')\\n    np.testing.assert_array_equal(arr1.data, 1000 * result.data)\\n\\n\\n# Test for issue #4129:\\ndef test_nddataarray_from_nddataarray():\\n    ndd1 = NDDataArray([1., 4., 9.],\\n                       uncertainty=StdDevUncertainty([1., 2., 3.]),\\n                       flags=[0, 1, 0])\\n    ndd2 = NDDataArray(ndd1)\\n    # Test that the 2 instances point to the same objects and aren\'t just\\n    # equal; this is explicitly documented for the main data array and we\\n    # probably want to catch any future change in behavior for the other\\n    # attributes too and ensure they are intentional.\\n    assert ndd2.data is ndd1.data\\n    assert ndd2.uncertainty is ndd1.uncertainty\\n    assert ndd2.flags is ndd1.flags\\n    assert ndd2.meta == ndd1.meta\\n\\n\\n# Test for issue #4137:\\ndef test_nddataarray_from_nddata():\\n    ndd1 = NDData([1., 4., 9.],\\n                  uncertainty=StdDevUncertainty([1., 2., 3.]))\\n    ndd2 = NDDataArray(ndd1)\\n\\n    assert ndd2.data is ndd1.data\\n    assert ndd2.uncertainty is ndd1.uncertainty\\n    assert ndd2.meta == ndd1.meta\\n" }\n'
line: b'{ "repo_name": "idea4bsd/idea4bsd", "ref": "refs/heads/idea4bsd-master", "path": "python/lib/Lib/site-packages/django/contrib/admin/widgets.py", "content": "\\"\\"\\"\\nForm Widget classes specific to the Django admin site.\\n\\"\\"\\"\\n\\nimport django.utils.copycompat as copy\\n\\nfrom django import forms\\nfrom django.forms.widgets import RadioFieldRenderer\\nfrom django.forms.util import flatatt\\nfrom django.utils.html import escape\\nfrom django.utils.text import truncate_words\\nfrom django.utils.translation import ugettext as _\\nfrom django.utils.safestring import mark_safe\\nfrom django.utils.encoding import force_unicode\\nfrom django.conf import settings\\nfrom django.core.urlresolvers import reverse, NoReverseMatch\\n\\nclass FilteredSelectMultiple(forms.SelectMultiple):\\n    \\"\\"\\"\\n    A SelectMultiple with a JavaScript filter interface.\\n\\n    Note that the resulting JavaScript assumes that the jsi18n\\n    catalog has been loaded in the page\\n    \\"\\"\\"\\n    class Media:\\n        js = (settings.ADMIN_MEDIA_PREFIX + \\"js/core.js\\",\\n              settings.ADMIN_MEDIA_PREFIX + \\"js/SelectBox.js\\",\\n              settings.ADMIN_MEDIA_PREFIX + \\"js/SelectFilter2.js\\")\\n\\n    def __init__(self, verbose_name, is_stacked, attrs=None, choices=()):\\n        self.verbose_name = verbose_name\\n        self.is_stacked = is_stacked\\n        super(FilteredSelectMultiple, self).__init__(attrs, choices)\\n\\n    def render(self, name, value, attrs=None, choices=()):\\n        if attrs is None: attrs = {}\\n        attrs[\'class\'] = \'selectfilter\'\\n        if self.is_stacked: attrs[\'class\'] += \'stacked\'\\n        output = [super(FilteredSelectMultiple, self).render(name, value, attrs, choices)]\\n        output.append(u\'<script type=\\"text/javascript\\">addEvent(window, \\"load\\", function(e) {\')\\n        # TODO: \\"id_\\" is hard-coded here. This should instead use the correct\\n        # API to determine the ID dynamically.\\n        output.append(u\'SelectFilter.init(\\"id_%s\\", \\"%s\\", %s, \\"%s\\"); });</script>\\\\n\' % \\\\\\n            (name, self.verbose_name.replace(\'\\"\', \'\\\\\\\\\\"\'), int(self.is_stacked), settings.ADMIN_MEDIA_PREFIX))\\n        return mark_safe(u\'\'.join(output))\\n\\nclass AdminDateWidget(forms.DateInput):\\n    class Media:\\n        js = (settings.ADMIN_MEDIA_PREFIX + \\"js/calendar.js\\",\\n              settings.ADMIN_MEDIA_PREFIX + \\"js/admin/DateTimeShortcuts.js\\")\\n\\n    def __init__(self, attrs={} format=None):\\n        super(AdminDateWidget, self).__init__(attrs={\'class\': \'vDateField\', \'size\': \'10\'} format=format)\\n\\nclass AdminTimeWidget(forms.TimeInput):\\n    class Media:\\n        js = (settings.ADMIN_MEDIA_PREFIX + \\"js/calendar.js\\",\\n              settings.ADMIN_MEDIA_PREFIX + \\"js/admin/DateTimeShortcuts.js\\")\\n\\n    def __init__(self, attrs={} format=None):\\n        super(AdminTimeWidget, self).__init__(attrs={\'class\': \'vTimeField\', \'size\': \'8\'} format=format)\\n\\nclass AdminSplitDateTime(forms.SplitDateTimeWidget):\\n    \\"\\"\\"\\n    A SplitDateTime Widget that has some admin-specific styling.\\n    \\"\\"\\"\\n    def __init__(self, attrs=None):\\n        widgets = [AdminDateWidget, AdminTimeWidget]\\n        # Note that we\'re calling MultiWidget, not SplitDateTimeWidget, because\\n        # we want to define widgets.\\n        forms.MultiWidget.__init__(self, widgets, attrs)\\n\\n    def format_output(self, rendered_widgets):\\n        return mark_safe(u\'<p class=\\"datetime\\">%s %s<br />%s %s</p>\' % \\\\\\n            (_(\'Date:\'), rendered_widgets[0], _(\'Time:\'), rendered_widgets[1]))\\n\\nclass AdminRadioFieldRenderer(RadioFieldRenderer):\\n    def render(self):\\n        \\"\\"\\"Outputs a <ul> for this set of radio fields.\\"\\"\\"\\n        return mark_safe(u\'<ul%s>\\\\n%s\\\\n</ul>\' % (\\n            flatatt(self.attrs),\\n            u\'\\\\n\'.join([u\'<li>%s</li>\' % force_unicode(w) for w in self]))\\n        )\\n\\nclass AdminRadioSelect(forms.RadioSelect):\\n    renderer = AdminRadioFieldRenderer\\n\\nclass AdminFileWidget(forms.ClearableFileInput):\\n    template_with_initial = (u\'<p class=\\"file-upload\\">%s</p>\'\\n                            % forms.ClearableFileInput.template_with_initial)\\n    template_with_clear = (u\'<span class=\\"clearable-file-input\\">%s</span>\'\\n                           % forms.ClearableFileInput.template_with_clear)\\n\\n\\nclass ForeignKeyRawIdWidget(forms.TextInput):\\n    \\"\\"\\"\\n    A Widget for displaying ForeignKeys in the \\"raw_id\\" interface rather than\\n    in a <select> box.\\n    \\"\\"\\"\\n    def __init__(self, rel, attrs=None, using=None):\\n        self.rel = rel\\n        self.db = using\\n        super(ForeignKeyRawIdWidget, self).__init__(attrs)\\n\\n    def render(self, name, value, attrs=None):\\n        if attrs is None:\\n            attrs = {}\\n        related_url = \'../../../%s/%s/\' % (self.rel.to._meta.app_label, self.rel.to._meta.object_name.lower())\\n        params = self.url_parameters()\\n        if params:\\n            url = \'?\' + \'&amp;\'.join([\'%s=%s\' % (k, v) for k, v in params.items()])\\n        else:\\n            url = \'\'\\n        if \\"class\\" not in attrs:\\n            attrs[\'class\'] = \'vForeignKeyRawIdAdminField\' # The JavaScript looks for this hook.\\n        output = [super(ForeignKeyRawIdWidget, self).render(name, value, attrs)]\\n        # TODO: \\"id_\\" is hard-coded here. This should instead use the correct\\n        # API to determine the ID dynamically.\\n        output.append(\'<a href=\\"%s%s\\" class=\\"related-lookup\\" id=\\"lookup_id_%s\\" onclick=\\"return showRelatedObjectLookupPopup(this);\\"> \' % \\\\\\n            (related_url, url, name))\\n        output.append(\'<img src=\\"%simg/admin/selector-search.gif\\" width=\\"16\\" height=\\"16\\" alt=\\"%s\\" /></a>\' % (settings.ADMIN_MEDIA_PREFIX, _(\'Lookup\')))\\n        if value:\\n            output.append(self.label_for_value(value))\\n        return mark_safe(u\'\'.join(output))\\n\\n    def base_url_parameters(self):\\n        params = {}\\n        if self.rel.limit_choices_to and hasattr(self.rel.limit_choices_to, \'items\'):\\n            items = []\\n            for k, v in self.rel.limit_choices_to.items():\\n                if isinstance(v, list):\\n                    v = \',\'.join([str(x) for x in v])\\n                else:\\n                    v = str(v)\\n                items.append((k, v))\\n            params.update(dict(items))\\n        return params\\n\\n    def url_parameters(self):\\n        from django.contrib.admin.views.main import TO_FIELD_VAR\\n        params = self.base_url_parameters()\\n        params.update({TO_FIELD_VAR: self.rel.get_related_field().name})\\n        return params\\n\\n    def label_for_value(self, value):\\n        key = self.rel.get_related_field().name\\n        try:\\n            obj = self.rel.to._default_manager.using(self.db).get(**{key: value})\\n            return \'&nbsp;<strong>%s</strong>\' % escape(truncate_words(obj, 14))\\n        except (ValueError, self.rel.to.DoesNotExist):\\n            return \'\'\\n\\nclass ManyToManyRawIdWidget(ForeignKeyRawIdWidget):\\n    \\"\\"\\"\\n    A Widget for displaying ManyToMany ids in the \\"raw_id\\" interface rather than\\n    in a <select multiple> box.\\n    \\"\\"\\"\\n    def render(self, name, value, attrs=None):\\n        if attrs is None:\\n            attrs = {}\\n        attrs[\'class\'] = \'vManyToManyRawIdAdminField\'\\n        if value:\\n            value = \',\'.join([force_unicode(v) for v in value])\\n        else:\\n            value = \'\'\\n        return super(ManyToManyRawIdWidget, self).render(name, value, attrs)\\n\\n    def url_parameters(self):\\n        return self.base_url_parameters()\\n\\n    def label_for_value(self, value):\\n        return \'\'\\n\\n    def value_from_datadict(self, data, files, name):\\n        value = data.get(name)\\n        if value:\\n            return value.split(\',\')\\n\\n    def _has_changed(self, initial, data):\\n        if initial is None:\\n            initial = []\\n        if data is None:\\n            data = []\\n        if len(initial) != len(data):\\n            return True\\n        for pk1, pk2 in zip(initial, data):\\n            if force_unicode(pk1) != force_unicode(pk2):\\n                return True\\n        return False\\n\\nclass RelatedFieldWidgetWrapper(forms.Widget):\\n    \\"\\"\\"\\n    This class is a wrapper to a given widget to add the add icon for the\\n    admin interface.\\n    \\"\\"\\"\\n    def __init__(self, widget, rel, admin_site, can_add_related=None):\\n        self.is_hidden = widget.is_hidden\\n        self.needs_multipart_form = widget.needs_multipart_form\\n        self.attrs = widget.attrs\\n        self.choices = widget.choices\\n        self.widget = widget\\n        self.rel = rel\\n        # Backwards compatible check for whether a user can add related\\n        # objects.\\n        if can_add_related is None:\\n            can_add_related = rel.to in admin_site._registry\\n        self.can_add_related = can_add_related\\n        # so we can check if the related object is registered with this AdminSite\\n        self.admin_site = admin_site\\n\\n    def __deepcopy__(self, memo):\\n        obj = copy.copy(self)\\n        obj.widget = copy.deepcopy(self.widget, memo)\\n        obj.attrs = self.widget.attrs\\n        memo[id(self)] = obj\\n        return obj\\n\\n    def _media(self):\\n        return self.widget.media\\n    media = property(_media)\\n\\n    def render(self, name, value, *args, **kwargs):\\n        rel_to = self.rel.to\\n        info = (rel_to._meta.app_label, rel_to._meta.object_name.lower())\\n        try:\\n            related_url = reverse(\'admin:%s_%s_add\' % info, current_app=self.admin_site.name)\\n        except NoReverseMatch:\\n            info = (self.admin_site.root_path, rel_to._meta.app_label, rel_to._meta.object_name.lower())\\n            related_url = \'%s%s/%s/add/\' % info\\n        self.widget.choices = self.choices\\n        output = [self.widget.render(name, value, *args, **kwargs)]\\n        if self.can_add_related:\\n            # TODO: \\"id_\\" is hard-coded here. This should instead use the correct\\n            # API to determine the ID dynamically.\\n            output.append(u\'<a href=\\"%s\\" class=\\"add-another\\" id=\\"add_id_%s\\" onclick=\\"return showAddAnotherPopup(this);\\"> \' % \\\\\\n                (related_url, name))\\n            output.append(u\'<img src=\\"%simg/admin/icon_addlink.gif\\" width=\\"10\\" height=\\"10\\" alt=\\"%s\\"/></a>\' % (settings.ADMIN_MEDIA_PREFIX, _(\'Add Another\')))\\n        return mark_safe(u\'\'.join(output))\\n\\n    def build_attrs(self, extra_attrs=None, **kwargs):\\n        \\"Helper function for building an attribute dictionary.\\"\\n        self.attrs = self.widget.build_attrs(extra_attrs=None, **kwargs)\\n        return self.attrs\\n\\n    def value_from_datadict(self, data, files, name):\\n        return self.widget.value_from_datadict(data, files, name)\\n\\n    def _has_changed(self, initial, data):\\n        return self.widget._has_changed(initial, data)\\n\\n    def id_for_label(self, id_):\\n        return self.widget.id_for_label(id_)\\n\\nclass AdminTextareaWidget(forms.Textarea):\\n    def __init__(self, attrs=None):\\n        final_attrs = {\'class\': \'vLargeTextField\'}\\n        if attrs is not None:\\n            final_attrs.update(attrs)\\n        super(AdminTextareaWidget, self).__init__(attrs=final_attrs)\\n\\nclass AdminTextInputWidget(forms.TextInput):\\n    def __init__(self, attrs=None):\\n        final_attrs = {\'class\': \'vTextField\'}\\n        if attrs is not None:\\n            final_attrs.update(attrs)\\n        super(AdminTextInputWidget, self).__init__(attrs=final_attrs)\\n\\nclass AdminURLFieldWidget(forms.TextInput):\\n    def __init__(self, attrs=None):\\n        final_attrs = {\'class\': \'vURLField\'}\\n        if attrs is not None:\\n            final_attrs.update(attrs)\\n        super(AdminURLFieldWidget, self).__init__(attrs=final_attrs)\\n\\nclass AdminIntegerFieldWidget(forms.TextInput):\\n    def __init__(self, attrs=None):\\n        final_attrs = {\'class\': \'vIntegerField\'}\\n        if attrs is not None:\\n            final_attrs.update(attrs)\\n        super(AdminIntegerFieldWidget, self).__init__(attrs=final_attrs)\\n\\nclass AdminCommaSeparatedIntegerFieldWidget(forms.TextInput):\\n    def __init__(self, attrs=None):\\n        final_attrs = {\'class\': \'vCommaSeparatedIntegerField\'}\\n        if attrs is not None:\\n            final_attrs.update(attrs)\\n        super(AdminCommaSeparatedIntegerFieldWidget, self).__init__(attrs=final_attrs)\\n" }\n'
line: b'{ "repo_name": "mhvk/astropy", "ref": "refs/heads/placeholder", "path": "astropy/nddata/tests/test_compat.py", "content": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\\n# This module contains tests of a class equivalent to pre-1.0 NDData.\\n\\n\\nimport pytest\\nimport numpy as np\\n\\nfrom astropy.nddata.nddata import NDData\\nfrom astropy.nddata.compat import NDDataArray\\nfrom astropy.nddata.nduncertainty import StdDevUncertainty\\nfrom astropy.wcs import WCS\\nfrom astropy import units as u\\n\\n\\nNDDATA_ATTRIBUTES = [\'mask\', \'flags\', \'uncertainty\', \'unit\', \'shape\', \'size\',\\n                     \'dtype\', \'ndim\', \'wcs\', \'convert_unit_to\']\\n\\n\\ndef test_nddataarray_has_attributes_of_old_nddata():\\n    ndd = NDDataArray([1, 2, 3])\\n    for attr in NDDATA_ATTRIBUTES:\\n        assert hasattr(ndd, attr)\\n\\n\\ndef test_nddata_simple():\\n    nd = NDDataArray(np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n\\n\\ndef test_nddata_parameters():\\n    # Test for issue 4620\\n    nd = NDDataArray(data=np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    # Change order; `data` has to be given explicitly here\\n    nd = NDDataArray(meta={} data=np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    # Pass uncertainty as second implicit argument\\n    data = np.zeros((10, 10))\\n    uncertainty = StdDevUncertainty(0.1 + np.zeros_like(data))\\n    nd = NDDataArray(data, uncertainty)\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    assert nd.uncertainty == uncertainty\\n\\n\\ndef test_nddata_conversion():\\n    nd = NDDataArray(np.array([[1, 2, 3], [4, 5, 6]]))\\n    assert nd.size == 6\\n    assert nd.dtype == np.dtype(int)\\n\\n\\n@pytest.mark.parametrize(\'flags_in\', [\\n                         np.array([True, False]),\\n                         np.array([1, 0]),\\n                         [True, False],\\n                         [1, 0],\\n                         np.array([\'a\', \'b\']),\\n                         [\'a\', \'b\']])\\ndef test_nddata_flags_init_without_np_array(flags_in):\\n    ndd = NDDataArray([1, 1], flags=flags_in)\\n    assert (ndd.flags == flags_in).all()\\n\\n\\n@pytest.mark.parametrize((\'shape\'), [(10,), (5, 5), (3, 10, 10)])\\ndef test_nddata_flags_invalid_shape(shape):\\n    with pytest.raises(ValueError) as exc:\\n        NDDataArray(np.zeros((10, 10)), flags=np.ones(shape))\\n    assert exc.value.args[0] == \'dimensions of flags do not match data\'\\n\\n\\ndef test_convert_unit_to():\\n    # convert_unit_to should return a copy of its input\\n    d = NDDataArray(np.ones((5, 5)))\\n    d.unit = \'km\'\\n    d.uncertainty = StdDevUncertainty(0.1 + np.zeros_like(d))\\n    # workaround because zeros_like does not support dtype arg until v1.6\\n    # and NDData accepts only bool ndarray as mask\\n    tmp = np.zeros_like(d.data)\\n    d.mask = np.array(tmp, dtype=bool)\\n    d1 = d.convert_unit_to(\'m\')\\n    assert np.all(d1.data == np.array(1000.0))\\n    assert np.all(d1.uncertainty.array == 1000.0 * d.uncertainty.array)\\n    assert d1.unit == u.m\\n    # changing the output mask should not change the original\\n    d1.mask[0, 0] = True\\n    assert d.mask[0, 0] != d1.mask[0, 0]\\n    d.flags = np.zeros_like(d.data)\\n    d1 = d.convert_unit_to(\'m\')\\n\\n\\n# check that subclasses can require wcs and/or unit to be present and use\\n# _arithmetic and convert_unit_to\\nclass SubNDData(NDDataArray):\\n    \\"\\"\\"\\n    Subclass for test initialization of subclasses in NDData._arithmetic and\\n    NDData.convert_unit_to\\n    \\"\\"\\"\\n    def __init__(self, *arg, **kwd):\\n        super().__init__(*arg, **kwd)\\n        if self.unit is None:\\n            raise ValueError(\\"Unit for subclass must be specified\\")\\n        if self.wcs is None:\\n            raise ValueError(\\"WCS for subclass must be specified\\")\\n\\n\\ndef test_init_of_subclass_in_convert_unit_to():\\n    data = np.ones([10, 10])\\n    arr1 = SubNDData(data, unit=\'m\', wcs=WCS(naxis=2))\\n    result = arr1.convert_unit_to(\'km\')\\n    np.testing.assert_array_equal(arr1.data, 1000 * result.data)\\n\\n\\n# Test for issue #4129:\\ndef test_nddataarray_from_nddataarray():\\n    ndd1 = NDDataArray([1., 4., 9.],\\n                       uncertainty=StdDevUncertainty([1., 2., 3.]),\\n                       flags=[0, 1, 0])\\n    ndd2 = NDDataArray(ndd1)\\n    # Test that the 2 instances point to the same objects and aren\'t just\\n    # equal; this is explicitly documented for the main data array and we\\n    # probably want to catch any future change in behavior for the other\\n    # attributes too and ensure they are intentional.\\n    assert ndd2.data is ndd1.data\\n    assert ndd2.uncertainty is ndd1.uncertainty\\n    assert ndd2.flags is ndd1.flags\\n    assert ndd2.meta == ndd1.meta\\n\\n\\n# Test for issue #4137:\\ndef test_nddataarray_from_nddata():\\n    ndd1 = NDData([1., 4., 9.],\\n                  uncertainty=StdDevUncertainty([1., 2., 3.]))\\n    ndd2 = NDDataArray(ndd1)\\n\\n    assert ndd2.data is ndd1.data\\n    assert ndd2.uncertainty is ndd1.uncertainty\\n    assert ndd2.meta == ndd1.meta\\n" }\n'
line: b'{ "repo_name": "dhomeier/astropy", "ref": "refs/heads/wcs-datfix-unwarn", "path": "astropy/nddata/tests/test_compat.py", "content": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\\n# This module contains tests of a class equivalent to pre-1.0 NDData.\\n\\n\\nimport pytest\\nimport numpy as np\\n\\nfrom astropy.nddata.nddata import NDData\\nfrom astropy.nddata.compat import NDDataArray\\nfrom astropy.nddata.nduncertainty import StdDevUncertainty\\nfrom astropy.wcs import WCS\\nfrom astropy import units as u\\n\\n\\nNDDATA_ATTRIBUTES = [\'mask\', \'flags\', \'uncertainty\', \'unit\', \'shape\', \'size\',\\n                     \'dtype\', \'ndim\', \'wcs\', \'convert_unit_to\']\\n\\n\\ndef test_nddataarray_has_attributes_of_old_nddata():\\n    ndd = NDDataArray([1, 2, 3])\\n    for attr in NDDATA_ATTRIBUTES:\\n        assert hasattr(ndd, attr)\\n\\n\\ndef test_nddata_simple():\\n    nd = NDDataArray(np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n\\n\\ndef test_nddata_parameters():\\n    # Test for issue 4620\\n    nd = NDDataArray(data=np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    # Change order; `data` has to be given explicitly here\\n    nd = NDDataArray(meta={} data=np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    # Pass uncertainty as second implicit argument\\n    data = np.zeros((10, 10))\\n    uncertainty = StdDevUncertainty(0.1 + np.zeros_like(data))\\n    nd = NDDataArray(data, uncertainty)\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    assert nd.uncertainty == uncertainty\\n\\n\\ndef test_nddata_conversion():\\n    nd = NDDataArray(np.array([[1, 2, 3], [4, 5, 6]]))\\n    assert nd.size == 6\\n    assert nd.dtype == np.dtype(int)\\n\\n\\n@pytest.mark.parametrize(\'flags_in\', [\\n                         np.array([True, False]),\\n                         np.array([1, 0]),\\n                         [True, False],\\n                         [1, 0],\\n                         np.array([\'a\', \'b\']),\\n                         [\'a\', \'b\']])\\ndef test_nddata_flags_init_without_np_array(flags_in):\\n    ndd = NDDataArray([1, 1], flags=flags_in)\\n    assert (ndd.flags == flags_in).all()\\n\\n\\n@pytest.mark.parametrize((\'shape\'), [(10,), (5, 5), (3, 10, 10)])\\ndef test_nddata_flags_invalid_shape(shape):\\n    with pytest.raises(ValueError) as exc:\\n        NDDataArray(np.zeros((10, 10)), flags=np.ones(shape))\\n    assert exc.value.args[0] == \'dimensions of flags do not match data\'\\n\\n\\ndef test_convert_unit_to():\\n    # convert_unit_to should return a copy of its input\\n    d = NDDataArray(np.ones((5, 5)))\\n    d.unit = \'km\'\\n    d.uncertainty = StdDevUncertainty(0.1 + np.zeros_like(d))\\n    # workaround because zeros_like does not support dtype arg until v1.6\\n    # and NDData accepts only bool ndarray as mask\\n    tmp = np.zeros_like(d.data)\\n    d.mask = np.array(tmp, dtype=bool)\\n    d1 = d.convert_unit_to(\'m\')\\n    assert np.all(d1.data == np.array(1000.0))\\n    assert np.all(d1.uncertainty.array == 1000.0 * d.uncertainty.array)\\n    assert d1.unit == u.m\\n    # changing the output mask should not change the original\\n    d1.mask[0, 0] = True\\n    assert d.mask[0, 0] != d1.mask[0, 0]\\n    d.flags = np.zeros_like(d.data)\\n    d1 = d.convert_unit_to(\'m\')\\n\\n\\n# check that subclasses can require wcs and/or unit to be present and use\\n# _arithmetic and convert_unit_to\\nclass SubNDData(NDDataArray):\\n    \\"\\"\\"\\n    Subclass for test initialization of subclasses in NDData._arithmetic and\\n    NDData.convert_unit_to\\n    \\"\\"\\"\\n    def __init__(self, *arg, **kwd):\\n        super().__init__(*arg, **kwd)\\n        if self.unit is None:\\n            raise ValueError(\\"Unit for subclass must be specified\\")\\n        if self.wcs is None:\\n            raise ValueError(\\"WCS for subclass must be specified\\")\\n\\n\\ndef test_init_of_subclass_in_convert_unit_to():\\n    data = np.ones([10, 10])\\n    arr1 = SubNDData(data, unit=\'m\', wcs=WCS(naxis=2))\\n    result = arr1.convert_unit_to(\'km\')\\n    np.testing.assert_array_equal(arr1.data, 1000 * result.data)\\n\\n\\n# Test for issue #4129:\\ndef test_nddataarray_from_nddataarray():\\n    ndd1 = NDDataArray([1., 4., 9.],\\n                       uncertainty=StdDevUncertainty([1., 2., 3.]),\\n                       flags=[0, 1, 0])\\n    ndd2 = NDDataArray(ndd1)\\n    # Test that the 2 instances point to the same objects and aren\'t just\\n    # equal; this is explicitly documented for the main data array and we\\n    # probably want to catch any future change in behavior for the other\\n    # attributes too and ensure they are intentional.\\n    assert ndd2.data is ndd1.data\\n    assert ndd2.uncertainty is ndd1.uncertainty\\n    assert ndd2.flags is ndd1.flags\\n    assert ndd2.meta == ndd1.meta\\n\\n\\n# Test for issue #4137:\\ndef test_nddataarray_from_nddata():\\n    ndd1 = NDData([1., 4., 9.],\\n                  uncertainty=StdDevUncertainty([1., 2., 3.]))\\n    ndd2 = NDDataArray(ndd1)\\n\\n    assert ndd2.data is ndd1.data\\n    assert ndd2.uncertainty is ndd1.uncertainty\\n    assert ndd2.meta == ndd1.meta\\n" }\n'
line: b'{ "repo_name": "astropy/astropy", "ref": "refs/heads/main", "path": "astropy/nddata/tests/test_compat.py", "content": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\\n# This module contains tests of a class equivalent to pre-1.0 NDData.\\n\\n\\nimport pytest\\nimport numpy as np\\n\\nfrom astropy.nddata.nddata import NDData\\nfrom astropy.nddata.compat import NDDataArray\\nfrom astropy.nddata.nduncertainty import StdDevUncertainty\\nfrom astropy.wcs import WCS\\nfrom astropy import units as u\\n\\n\\nNDDATA_ATTRIBUTES = [\'mask\', \'flags\', \'uncertainty\', \'unit\', \'shape\', \'size\',\\n                     \'dtype\', \'ndim\', \'wcs\', \'convert_unit_to\']\\n\\n\\ndef test_nddataarray_has_attributes_of_old_nddata():\\n    ndd = NDDataArray([1, 2, 3])\\n    for attr in NDDATA_ATTRIBUTES:\\n        assert hasattr(ndd, attr)\\n\\n\\ndef test_nddata_simple():\\n    nd = NDDataArray(np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n\\n\\ndef test_nddata_parameters():\\n    # Test for issue 4620\\n    nd = NDDataArray(data=np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    # Change order; `data` has to be given explicitly here\\n    nd = NDDataArray(meta={} data=np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    # Pass uncertainty as second implicit argument\\n    data = np.zeros((10, 10))\\n    uncertainty = StdDevUncertainty(0.1 + np.zeros_like(data))\\n    nd = NDDataArray(data, uncertainty)\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    assert nd.uncertainty == uncertainty\\n\\n\\ndef test_nddata_conversion():\\n    nd = NDDataArray(np.array([[1, 2, 3], [4, 5, 6]]))\\n    assert nd.size == 6\\n    assert nd.dtype == np.dtype(int)\\n\\n\\n@pytest.mark.parametrize(\'flags_in\', [\\n                         np.array([True, False]),\\n                         np.array([1, 0]),\\n                         [True, False],\\n                         [1, 0],\\n                         np.array([\'a\', \'b\']),\\n                         [\'a\', \'b\']])\\ndef test_nddata_flags_init_without_np_array(flags_in):\\n    ndd = NDDataArray([1, 1], flags=flags_in)\\n    assert (ndd.flags == flags_in).all()\\n\\n\\n@pytest.mark.parametrize((\'shape\'), [(10,), (5, 5), (3, 10, 10)])\\ndef test_nddata_flags_invalid_shape(shape):\\n    with pytest.raises(ValueError) as exc:\\n        NDDataArray(np.zeros((10, 10)), flags=np.ones(shape))\\n    assert exc.value.args[0] == \'dimensions of flags do not match data\'\\n\\n\\ndef test_convert_unit_to():\\n    # convert_unit_to should return a copy of its input\\n    d = NDDataArray(np.ones((5, 5)))\\n    d.unit = \'km\'\\n    d.uncertainty = StdDevUncertainty(0.1 + np.zeros_like(d))\\n    # workaround because zeros_like does not support dtype arg until v1.6\\n    # and NDData accepts only bool ndarray as mask\\n    tmp = np.zeros_like(d.data)\\n    d.mask = np.array(tmp, dtype=bool)\\n    d1 = d.convert_unit_to(\'m\')\\n    assert np.all(d1.data == np.array(1000.0))\\n    assert np.all(d1.uncertainty.array == 1000.0 * d.uncertainty.array)\\n    assert d1.unit == u.m\\n    # changing the output mask should not change the original\\n    d1.mask[0, 0] = True\\n    assert d.mask[0, 0] != d1.mask[0, 0]\\n    d.flags = np.zeros_like(d.data)\\n    d1 = d.convert_unit_to(\'m\')\\n\\n\\n# check that subclasses can require wcs and/or unit to be present and use\\n# _arithmetic and convert_unit_to\\nclass SubNDData(NDDataArray):\\n    \\"\\"\\"\\n    Subclass for test initialization of subclasses in NDData._arithmetic and\\n    NDData.convert_unit_to\\n    \\"\\"\\"\\n    def __init__(self, *arg, **kwd):\\n        super().__init__(*arg, **kwd)\\n        if self.unit is None:\\n            raise ValueError(\\"Unit for subclass must be specified\\")\\n        if self.wcs is None:\\n            raise ValueError(\\"WCS for subclass must be specified\\")\\n\\n\\ndef test_init_of_subclass_in_convert_unit_to():\\n    data = np.ones([10, 10])\\n    arr1 = SubNDData(data, unit=\'m\', wcs=WCS(naxis=2))\\n    result = arr1.convert_unit_to(\'km\')\\n    np.testing.assert_array_equal(arr1.data, 1000 * result.data)\\n\\n\\n# Test for issue #4129:\\ndef test_nddataarray_from_nddataarray():\\n    ndd1 = NDDataArray([1., 4., 9.],\\n                       uncertainty=StdDevUncertainty([1., 2., 3.]),\\n                       flags=[0, 1, 0])\\n    ndd2 = NDDataArray(ndd1)\\n    # Test that the 2 instances point to the same objects and aren\'t just\\n    # equal; this is explicitly documented for the main data array and we\\n    # probably want to catch any future change in behavior for the other\\n    # attributes too and ensure they are intentional.\\n    assert ndd2.data is ndd1.data\\n    assert ndd2.uncertainty is ndd1.uncertainty\\n    assert ndd2.flags is ndd1.flags\\n    assert ndd2.meta == ndd1.meta\\n\\n\\n# Test for issue #4137:\\ndef test_nddataarray_from_nddata():\\n    ndd1 = NDData([1., 4., 9.],\\n                  uncertainty=StdDevUncertainty([1., 2., 3.]))\\n    ndd2 = NDDataArray(ndd1)\\n\\n    assert ndd2.data is ndd1.data\\n    assert ndd2.uncertainty is ndd1.uncertainty\\n    assert ndd2.meta == ndd1.meta\\n" }\n'
line: b'{ "repo_name": "daq-tools/kotori", "ref": "refs/heads/main", "path": "kotori/vendor/ilaundry/gpiodebug.py", "content": "#!/usr/bin/python\\nimport os\\nimport time\\nimport Adafruit_BBIO.GPIO as GPIO\\n\\ndef hwports():\\n    for portno in range(0, 25):\\n        port = \'P8_\' + str(portno)\\n        yield port\\n\\nfor port in hwports():\\n    GPIO.setup(port, GPIO.IN)\\n\\nwhile True:\\n    os.system(\'clear\')\\n    for port in hwports():\\n        value = GPIO.input(port)\\n        print(port, value)\\n    time.sleep(0.2)\\n" }\n'
line: b'{ "repo_name": "lucafavatella/intellij-community", "ref": "refs/heads/cli-wip", "path": "python/lib/Lib/site-packages/django/contrib/admin/widgets.py", "content": "\\"\\"\\"\\nForm Widget classes specific to the Django admin site.\\n\\"\\"\\"\\n\\nimport django.utils.copycompat as copy\\n\\nfrom django import forms\\nfrom django.forms.widgets import RadioFieldRenderer\\nfrom django.forms.util import flatatt\\nfrom django.utils.html import escape\\nfrom django.utils.text import truncate_words\\nfrom django.utils.translation import ugettext as _\\nfrom django.utils.safestring import mark_safe\\nfrom django.utils.encoding import force_unicode\\nfrom django.conf import settings\\nfrom django.core.urlresolvers import reverse, NoReverseMatch\\n\\nclass FilteredSelectMultiple(forms.SelectMultiple):\\n    \\"\\"\\"\\n    A SelectMultiple with a JavaScript filter interface.\\n\\n    Note that the resulting JavaScript assumes that the jsi18n\\n    catalog has been loaded in the page\\n    \\"\\"\\"\\n    class Media:\\n        js = (settings.ADMIN_MEDIA_PREFIX + \\"js/core.js\\",\\n              settings.ADMIN_MEDIA_PREFIX + \\"js/SelectBox.js\\",\\n              settings.ADMIN_MEDIA_PREFIX + \\"js/SelectFilter2.js\\")\\n\\n    def __init__(self, verbose_name, is_stacked, attrs=None, choices=()):\\n        self.verbose_name = verbose_name\\n        self.is_stacked = is_stacked\\n        super(FilteredSelectMultiple, self).__init__(attrs, choices)\\n\\n    def render(self, name, value, attrs=None, choices=()):\\n        if attrs is None: attrs = {}\\n        attrs[\'class\'] = \'selectfilter\'\\n        if self.is_stacked: attrs[\'class\'] += \'stacked\'\\n        output = [super(FilteredSelectMultiple, self).render(name, value, attrs, choices)]\\n        output.append(u\'<script type=\\"text/javascript\\">addEvent(window, \\"load\\", function(e) {\')\\n        # TODO: \\"id_\\" is hard-coded here. This should instead use the correct\\n        # API to determine the ID dynamically.\\n        output.append(u\'SelectFilter.init(\\"id_%s\\", \\"%s\\", %s, \\"%s\\"); });</script>\\\\n\' % \\\\\\n            (name, self.verbose_name.replace(\'\\"\', \'\\\\\\\\\\"\'), int(self.is_stacked), settings.ADMIN_MEDIA_PREFIX))\\n        return mark_safe(u\'\'.join(output))\\n\\nclass AdminDateWidget(forms.DateInput):\\n    class Media:\\n        js = (settings.ADMIN_MEDIA_PREFIX + \\"js/calendar.js\\",\\n              settings.ADMIN_MEDIA_PREFIX + \\"js/admin/DateTimeShortcuts.js\\")\\n\\n    def __init__(self, attrs={} format=None):\\n        super(AdminDateWidget, self).__init__(attrs={\'class\': \'vDateField\', \'size\': \'10\'} format=format)\\n\\nclass AdminTimeWidget(forms.TimeInput):\\n    class Media:\\n        js = (settings.ADMIN_MEDIA_PREFIX + \\"js/calendar.js\\",\\n              settings.ADMIN_MEDIA_PREFIX + \\"js/admin/DateTimeShortcuts.js\\")\\n\\n    def __init__(self, attrs={} format=None):\\n        super(AdminTimeWidget, self).__init__(attrs={\'class\': \'vTimeField\', \'size\': \'8\'} format=format)\\n\\nclass AdminSplitDateTime(forms.SplitDateTimeWidget):\\n    \\"\\"\\"\\n    A SplitDateTime Widget that has some admin-specific styling.\\n    \\"\\"\\"\\n    def __init__(self, attrs=None):\\n        widgets = [AdminDateWidget, AdminTimeWidget]\\n        # Note that we\'re calling MultiWidget, not SplitDateTimeWidget, because\\n        # we want to define widgets.\\n        forms.MultiWidget.__init__(self, widgets, attrs)\\n\\n    def format_output(self, rendered_widgets):\\n        return mark_safe(u\'<p class=\\"datetime\\">%s %s<br />%s %s</p>\' % \\\\\\n            (_(\'Date:\'), rendered_widgets[0], _(\'Time:\'), rendered_widgets[1]))\\n\\nclass AdminRadioFieldRenderer(RadioFieldRenderer):\\n    def render(self):\\n        \\"\\"\\"Outputs a <ul> for this set of radio fields.\\"\\"\\"\\n        return mark_safe(u\'<ul%s>\\\\n%s\\\\n</ul>\' % (\\n            flatatt(self.attrs),\\n            u\'\\\\n\'.join([u\'<li>%s</li>\' % force_unicode(w) for w in self]))\\n        )\\n\\nclass AdminRadioSelect(forms.RadioSelect):\\n    renderer = AdminRadioFieldRenderer\\n\\nclass AdminFileWidget(forms.ClearableFileInput):\\n    template_with_initial = (u\'<p class=\\"file-upload\\">%s</p>\'\\n                            % forms.ClearableFileInput.template_with_initial)\\n    template_with_clear = (u\'<span class=\\"clearable-file-input\\">%s</span>\'\\n                           % forms.ClearableFileInput.template_with_clear)\\n\\n\\nclass ForeignKeyRawIdWidget(forms.TextInput):\\n    \\"\\"\\"\\n    A Widget for displaying ForeignKeys in the \\"raw_id\\" interface rather than\\n    in a <select> box.\\n    \\"\\"\\"\\n    def __init__(self, rel, attrs=None, using=None):\\n        self.rel = rel\\n        self.db = using\\n        super(ForeignKeyRawIdWidget, self).__init__(attrs)\\n\\n    def render(self, name, value, attrs=None):\\n        if attrs is None:\\n            attrs = {}\\n        related_url = \'../../../%s/%s/\' % (self.rel.to._meta.app_label, self.rel.to._meta.object_name.lower())\\n        params = self.url_parameters()\\n        if params:\\n            url = \'?\' + \'&amp;\'.join([\'%s=%s\' % (k, v) for k, v in params.items()])\\n        else:\\n            url = \'\'\\n        if \\"class\\" not in attrs:\\n            attrs[\'class\'] = \'vForeignKeyRawIdAdminField\' # The JavaScript looks for this hook.\\n        output = [super(ForeignKeyRawIdWidget, self).render(name, value, attrs)]\\n        # TODO: \\"id_\\" is hard-coded here. This should instead use the correct\\n        # API to determine the ID dynamically.\\n        output.append(\'<a href=\\"%s%s\\" class=\\"related-lookup\\" id=\\"lookup_id_%s\\" onclick=\\"return showRelatedObjectLookupPopup(this);\\"> \' % \\\\\\n            (related_url, url, name))\\n        output.append(\'<img src=\\"%simg/admin/selector-search.gif\\" width=\\"16\\" height=\\"16\\" alt=\\"%s\\" /></a>\' % (settings.ADMIN_MEDIA_PREFIX, _(\'Lookup\')))\\n        if value:\\n            output.append(self.label_for_value(value))\\n        return mark_safe(u\'\'.join(output))\\n\\n    def base_url_parameters(self):\\n        params = {}\\n        if self.rel.limit_choices_to and hasattr(self.rel.limit_choices_to, \'items\'):\\n            items = []\\n            for k, v in self.rel.limit_choices_to.items():\\n                if isinstance(v, list):\\n                    v = \',\'.join([str(x) for x in v])\\n                else:\\n                    v = str(v)\\n                items.append((k, v))\\n            params.update(dict(items))\\n        return params\\n\\n    def url_parameters(self):\\n        from django.contrib.admin.views.main import TO_FIELD_VAR\\n        params = self.base_url_parameters()\\n        params.update({TO_FIELD_VAR: self.rel.get_related_field().name})\\n        return params\\n\\n    def label_for_value(self, value):\\n        key = self.rel.get_related_field().name\\n        try:\\n            obj = self.rel.to._default_manager.using(self.db).get(**{key: value})\\n            return \'&nbsp;<strong>%s</strong>\' % escape(truncate_words(obj, 14))\\n        except (ValueError, self.rel.to.DoesNotExist):\\n            return \'\'\\n\\nclass ManyToManyRawIdWidget(ForeignKeyRawIdWidget):\\n    \\"\\"\\"\\n    A Widget for displaying ManyToMany ids in the \\"raw_id\\" interface rather than\\n    in a <select multiple> box.\\n    \\"\\"\\"\\n    def render(self, name, value, attrs=None):\\n        if attrs is None:\\n            attrs = {}\\n        attrs[\'class\'] = \'vManyToManyRawIdAdminField\'\\n        if value:\\n            value = \',\'.join([force_unicode(v) for v in value])\\n        else:\\n            value = \'\'\\n        return super(ManyToManyRawIdWidget, self).render(name, value, attrs)\\n\\n    def url_parameters(self):\\n        return self.base_url_parameters()\\n\\n    def label_for_value(self, value):\\n        return \'\'\\n\\n    def value_from_datadict(self, data, files, name):\\n        value = data.get(name)\\n        if value:\\n            return value.split(\',\')\\n\\n    def _has_changed(self, initial, data):\\n        if initial is None:\\n            initial = []\\n        if data is None:\\n            data = []\\n        if len(initial) != len(data):\\n            return True\\n        for pk1, pk2 in zip(initial, data):\\n            if force_unicode(pk1) != force_unicode(pk2):\\n                return True\\n        return False\\n\\nclass RelatedFieldWidgetWrapper(forms.Widget):\\n    \\"\\"\\"\\n    This class is a wrapper to a given widget to add the add icon for the\\n    admin interface.\\n    \\"\\"\\"\\n    def __init__(self, widget, rel, admin_site, can_add_related=None):\\n        self.is_hidden = widget.is_hidden\\n        self.needs_multipart_form = widget.needs_multipart_form\\n        self.attrs = widget.attrs\\n        self.choices = widget.choices\\n        self.widget = widget\\n        self.rel = rel\\n        # Backwards compatible check for whether a user can add related\\n        # objects.\\n        if can_add_related is None:\\n            can_add_related = rel.to in admin_site._registry\\n        self.can_add_related = can_add_related\\n        # so we can check if the related object is registered with this AdminSite\\n        self.admin_site = admin_site\\n\\n    def __deepcopy__(self, memo):\\n        obj = copy.copy(self)\\n        obj.widget = copy.deepcopy(self.widget, memo)\\n        obj.attrs = self.widget.attrs\\n        memo[id(self)] = obj\\n        return obj\\n\\n    def _media(self):\\n        return self.widget.media\\n    media = property(_media)\\n\\n    def render(self, name, value, *args, **kwargs):\\n        rel_to = self.rel.to\\n        info = (rel_to._meta.app_label, rel_to._meta.object_name.lower())\\n        try:\\n            related_url = reverse(\'admin:%s_%s_add\' % info, current_app=self.admin_site.name)\\n        except NoReverseMatch:\\n            info = (self.admin_site.root_path, rel_to._meta.app_label, rel_to._meta.object_name.lower())\\n            related_url = \'%s%s/%s/add/\' % info\\n        self.widget.choices = self.choices\\n        output = [self.widget.render(name, value, *args, **kwargs)]\\n        if self.can_add_related:\\n            # TODO: \\"id_\\" is hard-coded here. This should instead use the correct\\n            # API to determine the ID dynamically.\\n            output.append(u\'<a href=\\"%s\\" class=\\"add-another\\" id=\\"add_id_%s\\" onclick=\\"return showAddAnotherPopup(this);\\"> \' % \\\\\\n                (related_url, name))\\n            output.append(u\'<img src=\\"%simg/admin/icon_addlink.gif\\" width=\\"10\\" height=\\"10\\" alt=\\"%s\\"/></a>\' % (settings.ADMIN_MEDIA_PREFIX, _(\'Add Another\')))\\n        return mark_safe(u\'\'.join(output))\\n\\n    def build_attrs(self, extra_attrs=None, **kwargs):\\n        \\"Helper function for building an attribute dictionary.\\"\\n        self.attrs = self.widget.build_attrs(extra_attrs=None, **kwargs)\\n        return self.attrs\\n\\n    def value_from_datadict(self, data, files, name):\\n        return self.widget.value_from_datadict(data, files, name)\\n\\n    def _has_changed(self, initial, data):\\n        return self.widget._has_changed(initial, data)\\n\\n    def id_for_label(self, id_):\\n        return self.widget.id_for_label(id_)\\n\\nclass AdminTextareaWidget(forms.Textarea):\\n    def __init__(self, attrs=None):\\n        final_attrs = {\'class\': \'vLargeTextField\'}\\n        if attrs is not None:\\n            final_attrs.update(attrs)\\n        super(AdminTextareaWidget, self).__init__(attrs=final_attrs)\\n\\nclass AdminTextInputWidget(forms.TextInput):\\n    def __init__(self, attrs=None):\\n        final_attrs = {\'class\': \'vTextField\'}\\n        if attrs is not None:\\n            final_attrs.update(attrs)\\n        super(AdminTextInputWidget, self).__init__(attrs=final_attrs)\\n\\nclass AdminURLFieldWidget(forms.TextInput):\\n    def __init__(self, attrs=None):\\n        final_attrs = {\'class\': \'vURLField\'}\\n        if attrs is not None:\\n            final_attrs.update(attrs)\\n        super(AdminURLFieldWidget, self).__init__(attrs=final_attrs)\\n\\nclass AdminIntegerFieldWidget(forms.TextInput):\\n    def __init__(self, attrs=None):\\n        final_attrs = {\'class\': \'vIntegerField\'}\\n        if attrs is not None:\\n            final_attrs.update(attrs)\\n        super(AdminIntegerFieldWidget, self).__init__(attrs=final_attrs)\\n\\nclass AdminCommaSeparatedIntegerFieldWidget(forms.TextInput):\\n    def __init__(self, attrs=None):\\n        final_attrs = {\'class\': \'vCommaSeparatedIntegerField\'}\\n        if attrs is not None:\\n            final_attrs.update(attrs)\\n        super(AdminCommaSeparatedIntegerFieldWidget, self).__init__(attrs=final_attrs)\\n" }\n'
line: b'{ "repo_name": "AnhellO/DAS_Sistemas", "ref": "refs/heads/development", "path": "Ene-Jun-2021/morales-ramos-manuel-gerardo/Primer Parcial/Ejercicio 1/ejercicio_1.py", "content": "import abc\\n\\n#Clase de la p\xc3\xa1gina web\\nclass WebPage:\\n    def __init__(self, url, route, page_format, content, title, slug, meta_tags = []):\\n        self._url: str = url\\n        self._route: str = route\\n        self._format: str = page_format\\n        self._content: str = content\\n        self._title: str = title\\n        self._slug: str = slug\\n        self._meta_tags: list = meta_tags\\n\\n    def __str__(self) -> str:\\n        return f\'\\\\nTitle: {self._title}\\\\nURL: {self._url}\\\\nSlug: {self._slug}\\\\nRoute: {self._route}\\\\nFormat: {self._format}\\\\nContent: {self._content}\\\\nMeta tags: {self.get_tags()}\'\\n    \\n    def get_tags(self):\\n        tags = \'\'\\n        for tag in self._meta_tags:\\n            tags += str(tag)\\n\\n        return tags\\n\\n#Interface para el proxy\\nclass ServiceInterface(metaclass=abc.ABCMeta):\\n    @abc.abstractmethod\\n    def login(self, user: str, passw: str):\\n        pass\\n\\n#Clase para el sitio web\\nclass WebSite(ServiceInterface):\\n    def __init__(self, domain, category, pages:list):\\n        self._domain: str = domain\\n        self._category: str = category\\n        self._pages: WebPage = pages\\n\\n    def __str__(self) -> str:\\n        return f\'\\\\r\\\\nCategory: {self._category}\\\\n\\\\rDomain: {self._domain}\\\\r\\\\n\\\\nWeb Pages: \\\\r\\\\n{self.get_pages()}\'\\n\\n    def get_pages(self) -> WebPage:\\n        pages = \'\'\\n        for page in self._pages:\\n            pages += str(page)\\n\\n        return pages\\n\\n    def login(self, user: str, passw: str):\\n        return f\'Welcome, {user}!\'\\n\\n#Clase que har\xc3\xa1 la autenticaci\xc3\xb3n\\nclass Authentication(ServiceInterface):\\n    def __init__(self, s: WebSite):\\n        self._service = s\\n\\n    def authenticate(self, user: str, passw: str):\\n        if(user == \'gerardo\' and passw == \'123456\'):\\n            return True\\n        else:\\n            return False\\n\\n    def login(self, user: str, passw: str):\\n        if(self.authenticate(user, passw)):\\n            return self._service.login(user, passw)\\n        else:\\n            return \'Invalid data!\'\\n\\ndef main():\\n    webpage1 = WebPage(\'https://www.wp.com/home\', \'https://www.wp.com/index.html\', \'HTML\', \'<main></main>\', \'Home\', \'home\', [\'<meta>\', \'<meta>\'])\\n    webpage2 = WebPage(\'https://www.wp.com/contact-me\', \'https://www.wp.com/contact-me.html\', \'HTML\', \'<main></main>\', \'Contact Me\', \'contact-me\', [\'<meta>\', \'<meta>\'])\\n    webpage3 = WebPage(\'https://www.wp.com/faq\', \'https://www.wp.com/faq.html\', \'HTML\', \'<main></main>\', \'FAQ\', \'faq\', [\'<meta>\', \'<meta>\'])\\n    \\n    pages = [webpage1, webpage2, webpage3]\\n    \\n    website = WebSite(\'wp\', \'Educational\', pages)\\n    print(website)\\n    print(\'--------------------------------\')\\n    print(Authentication(website).login(\'gerardo\', \'123456\'))\\n\\nif __name__ == \\"__main__\\":     \\n    main()" }\n'
line: b'{ "repo_name": "gaeun/open-event-orga-server", "ref": "refs/heads/development", "path": "migrations/versions/ed4b4ba3274e_.py", "content": "\\"\\"\\"empty message\\n\\nRevision ID: ed4b4ba3274e\\nRevises: 784a1fc57171\\nCreate Date: 2016-06-16 06:08:49.516538\\n\\n\\"\\"\\"\\n\\n# revision identifiers, used by Alembic.\\nrevision = \'ed4b4ba3274e\'\\ndown_revision = \'784a1fc57171\'\\n\\nfrom alembic import op\\nimport sqlalchemy as sa\\nimport sqlalchemy_utils\\n\\n\\ndef upgrade():\\n    ### commands auto generated by Alembic - please adjust! ###\\n    op.create_table(\'service\',\\n    sa.Column(\'id\', sa.Integer(), nullable=False),\\n    sa.Column(\'name\', sa.String(), nullable=False),\\n    sa.PrimaryKeyConstraint(\'id\'),\\n    sa.UniqueConstraint(\'name\')\\n    )\\n    op.add_column(u\'permissions\', sa.Column(\'can_create\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'can_delete\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'can_read\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'can_update\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'role_id\', sa.Integer(), nullable=True))\\n    op.alter_column(u\'permissions\', \'service_id\',\\n               existing_type=sa.INTEGER(),\\n               nullable=True)\\n    op.create_unique_constraint(\'role_service_uc\', \'permissions\', [\'role_id\', \'service_id\'])\\n    op.drop_constraint(u\'user_service_uc\', \'permissions\', type_=\'unique\')\\n    op.drop_constraint(u\'permissions_user_id_fkey\', \'permissions\', type_=\'foreignkey\')\\n    op.create_foreign_key(None, \'permissions\', \'role\', [\'role_id\'], [\'id\'])\\n    op.create_foreign_key(None, \'permissions\', \'service\', [\'service_id\'], [\'id\'])\\n    op.drop_column(u\'permissions\', \'user_id\')\\n    op.drop_column(u\'permissions\', \'modes\')\\n    op.drop_column(u\'permissions\', \'service\')\\n    op.alter_column(u\'role\', \'name\',\\n               existing_type=sa.VARCHAR(length=128),\\n               nullable=False)\\n    op.create_unique_constraint(None, \'role\', [\'name\'])\\n    op.drop_column(u\'user\', \'role\')\\n    ### end Alembic commands ###\\n\\n\\ndef downgrade():\\n    ### commands auto generated by Alembic - please adjust! ###\\n    op.add_column(u\'user\', sa.Column(\'role\', sa.VARCHAR(), autoincrement=False, nullable=True))\\n    op.drop_constraint(None, \'role\', type_=\'unique\')\\n    op.alter_column(u\'role\', \'name\',\\n               existing_type=sa.VARCHAR(length=128),\\n               nullable=True)\\n    op.add_column(u\'permissions\', sa.Column(\'service\', sa.VARCHAR(), autoincrement=False, nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'modes\', sa.INTEGER(), autoincrement=False, nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'user_id\', sa.INTEGER(), autoincrement=False, nullable=False))\\n    op.drop_constraint(None, \'permissions\', type_=\'foreignkey\')\\n    op.drop_constraint(None, \'permissions\', type_=\'foreignkey\')\\n    op.create_foreign_key(u\'permissions_user_id_fkey\', \'permissions\', \'user\', [\'user_id\'], [\'id\'])\\n    op.create_unique_constraint(u\'user_service_uc\', \'permissions\', [\'user_id\', \'service\', \'service_id\'])\\n    op.drop_constraint(\'role_service_uc\', \'permissions\', type_=\'unique\')\\n    op.alter_column(u\'permissions\', \'service_id\',\\n               existing_type=sa.INTEGER(),\\n               nullable=False)\\n    op.drop_column(u\'permissions\', \'role_id\')\\n    op.drop_column(u\'permissions\', \'can_update\')\\n    op.drop_column(u\'permissions\', \'can_read\')\\n    op.drop_column(u\'permissions\', \'can_delete\')\\n    op.drop_column(u\'permissions\', \'can_create\')\\n    op.drop_table(\'service\')\\n    ### end Alembic commands ###\\n" }\n'
line: b'{ "repo_name": "Princu7/open-event-orga-server", "ref": "refs/heads/development", "path": "migrations/versions/ed4b4ba3274e_.py", "content": "\\"\\"\\"empty message\\n\\nRevision ID: ed4b4ba3274e\\nRevises: 784a1fc57171\\nCreate Date: 2016-06-16 06:08:49.516538\\n\\n\\"\\"\\"\\n\\n# revision identifiers, used by Alembic.\\nrevision = \'ed4b4ba3274e\'\\ndown_revision = \'784a1fc57171\'\\n\\nfrom alembic import op\\nimport sqlalchemy as sa\\nimport sqlalchemy_utils\\n\\n\\ndef upgrade():\\n    ### commands auto generated by Alembic - please adjust! ###\\n    op.create_table(\'service\',\\n    sa.Column(\'id\', sa.Integer(), nullable=False),\\n    sa.Column(\'name\', sa.String(), nullable=False),\\n    sa.PrimaryKeyConstraint(\'id\'),\\n    sa.UniqueConstraint(\'name\')\\n    )\\n    op.add_column(u\'permissions\', sa.Column(\'can_create\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'can_delete\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'can_read\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'can_update\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'role_id\', sa.Integer(), nullable=True))\\n    op.alter_column(u\'permissions\', \'service_id\',\\n               existing_type=sa.INTEGER(),\\n               nullable=True)\\n    op.create_unique_constraint(\'role_service_uc\', \'permissions\', [\'role_id\', \'service_id\'])\\n    op.drop_constraint(u\'user_service_uc\', \'permissions\', type_=\'unique\')\\n    op.drop_constraint(u\'permissions_user_id_fkey\', \'permissions\', type_=\'foreignkey\')\\n    op.create_foreign_key(None, \'permissions\', \'role\', [\'role_id\'], [\'id\'])\\n    op.create_foreign_key(None, \'permissions\', \'service\', [\'service_id\'], [\'id\'])\\n    op.drop_column(u\'permissions\', \'user_id\')\\n    op.drop_column(u\'permissions\', \'modes\')\\n    op.drop_column(u\'permissions\', \'service\')\\n    op.alter_column(u\'role\', \'name\',\\n               existing_type=sa.VARCHAR(length=128),\\n               nullable=False)\\n    op.create_unique_constraint(None, \'role\', [\'name\'])\\n    op.drop_column(u\'user\', \'role\')\\n    ### end Alembic commands ###\\n\\n\\ndef downgrade():\\n    ### commands auto generated by Alembic - please adjust! ###\\n    op.add_column(u\'user\', sa.Column(\'role\', sa.VARCHAR(), autoincrement=False, nullable=True))\\n    op.drop_constraint(None, \'role\', type_=\'unique\')\\n    op.alter_column(u\'role\', \'name\',\\n               existing_type=sa.VARCHAR(length=128),\\n               nullable=True)\\n    op.add_column(u\'permissions\', sa.Column(\'service\', sa.VARCHAR(), autoincrement=False, nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'modes\', sa.INTEGER(), autoincrement=False, nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'user_id\', sa.INTEGER(), autoincrement=False, nullable=False))\\n    op.drop_constraint(None, \'permissions\', type_=\'foreignkey\')\\n    op.drop_constraint(None, \'permissions\', type_=\'foreignkey\')\\n    op.create_foreign_key(u\'permissions_user_id_fkey\', \'permissions\', \'user\', [\'user_id\'], [\'id\'])\\n    op.create_unique_constraint(u\'user_service_uc\', \'permissions\', [\'user_id\', \'service\', \'service_id\'])\\n    op.drop_constraint(\'role_service_uc\', \'permissions\', type_=\'unique\')\\n    op.alter_column(u\'permissions\', \'service_id\',\\n               existing_type=sa.INTEGER(),\\n               nullable=False)\\n    op.drop_column(u\'permissions\', \'role_id\')\\n    op.drop_column(u\'permissions\', \'can_update\')\\n    op.drop_column(u\'permissions\', \'can_read\')\\n    op.drop_column(u\'permissions\', \'can_delete\')\\n    op.drop_column(u\'permissions\', \'can_create\')\\n    op.drop_table(\'service\')\\n    ### end Alembic commands ###\\n" }\n'
line: b'{ "repo_name": "saimn/astropy", "ref": "refs/heads/main", "path": "astropy/nddata/tests/test_compat.py", "content": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\\n# This module contains tests of a class equivalent to pre-1.0 NDData.\\n\\n\\nimport pytest\\nimport numpy as np\\n\\nfrom astropy.nddata.nddata import NDData\\nfrom astropy.nddata.compat import NDDataArray\\nfrom astropy.nddata.nduncertainty import StdDevUncertainty\\nfrom astropy.wcs import WCS\\nfrom astropy import units as u\\n\\n\\nNDDATA_ATTRIBUTES = [\'mask\', \'flags\', \'uncertainty\', \'unit\', \'shape\', \'size\',\\n                     \'dtype\', \'ndim\', \'wcs\', \'convert_unit_to\']\\n\\n\\ndef test_nddataarray_has_attributes_of_old_nddata():\\n    ndd = NDDataArray([1, 2, 3])\\n    for attr in NDDATA_ATTRIBUTES:\\n        assert hasattr(ndd, attr)\\n\\n\\ndef test_nddata_simple():\\n    nd = NDDataArray(np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n\\n\\ndef test_nddata_parameters():\\n    # Test for issue 4620\\n    nd = NDDataArray(data=np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    # Change order; `data` has to be given explicitly here\\n    nd = NDDataArray(meta={} data=np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    # Pass uncertainty as second implicit argument\\n    data = np.zeros((10, 10))\\n    uncertainty = StdDevUncertainty(0.1 + np.zeros_like(data))\\n    nd = NDDataArray(data, uncertainty)\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    assert nd.uncertainty == uncertainty\\n\\n\\ndef test_nddata_conversion():\\n    nd = NDDataArray(np.array([[1, 2, 3], [4, 5, 6]]))\\n    assert nd.size == 6\\n    assert nd.dtype == np.dtype(int)\\n\\n\\n@pytest.mark.parametrize(\'flags_in\', [\\n                         np.array([True, False]),\\n                         np.array([1, 0]),\\n                         [True, False],\\n                         [1, 0],\\n                         np.array([\'a\', \'b\']),\\n                         [\'a\', \'b\']])\\ndef test_nddata_flags_init_without_np_array(flags_in):\\n    ndd = NDDataArray([1, 1], flags=flags_in)\\n    assert (ndd.flags == flags_in).all()\\n\\n\\n@pytest.mark.parametrize((\'shape\'), [(10,), (5, 5), (3, 10, 10)])\\ndef test_nddata_flags_invalid_shape(shape):\\n    with pytest.raises(ValueError) as exc:\\n        NDDataArray(np.zeros((10, 10)), flags=np.ones(shape))\\n    assert exc.value.args[0] == \'dimensions of flags do not match data\'\\n\\n\\ndef test_convert_unit_to():\\n    # convert_unit_to should return a copy of its input\\n    d = NDDataArray(np.ones((5, 5)))\\n    d.unit = \'km\'\\n    d.uncertainty = StdDevUncertainty(0.1 + np.zeros_like(d))\\n    # workaround because zeros_like does not support dtype arg until v1.6\\n    # and NDData accepts only bool ndarray as mask\\n    tmp = np.zeros_like(d.data)\\n    d.mask = np.array(tmp, dtype=bool)\\n    d1 = d.convert_unit_to(\'m\')\\n    assert np.all(d1.data == np.array(1000.0))\\n    assert np.all(d1.uncertainty.array == 1000.0 * d.uncertainty.array)\\n    assert d1.unit == u.m\\n    # changing the output mask should not change the original\\n    d1.mask[0, 0] = True\\n    assert d.mask[0, 0] != d1.mask[0, 0]\\n    d.flags = np.zeros_like(d.data)\\n    d1 = d.convert_unit_to(\'m\')\\n\\n\\n# check that subclasses can require wcs and/or unit to be present and use\\n# _arithmetic and convert_unit_to\\nclass SubNDData(NDDataArray):\\n    \\"\\"\\"\\n    Subclass for test initialization of subclasses in NDData._arithmetic and\\n    NDData.convert_unit_to\\n    \\"\\"\\"\\n    def __init__(self, *arg, **kwd):\\n        super().__init__(*arg, **kwd)\\n        if self.unit is None:\\n            raise ValueError(\\"Unit for subclass must be specified\\")\\n        if self.wcs is None:\\n            raise ValueError(\\"WCS for subclass must be specified\\")\\n\\n\\ndef test_init_of_subclass_in_convert_unit_to():\\n    data = np.ones([10, 10])\\n    arr1 = SubNDData(data, unit=\'m\', wcs=WCS(naxis=2))\\n    result = arr1.convert_unit_to(\'km\')\\n    np.testing.assert_array_equal(arr1.data, 1000 * result.data)\\n\\n\\n# Test for issue #4129:\\ndef test_nddataarray_from_nddataarray():\\n    ndd1 = NDDataArray([1., 4., 9.],\\n                       uncertainty=StdDevUncertainty([1., 2., 3.]),\\n                       flags=[0, 1, 0])\\n    ndd2 = NDDataArray(ndd1)\\n    # Test that the 2 instances point to the same objects and aren\'t just\\n    # equal; this is explicitly documented for the main data array and we\\n    # probably want to catch any future change in behavior for the other\\n    # attributes too and ensure they are intentional.\\n    assert ndd2.data is ndd1.data\\n    assert ndd2.uncertainty is ndd1.uncertainty\\n    assert ndd2.flags is ndd1.flags\\n    assert ndd2.meta == ndd1.meta\\n\\n\\n# Test for issue #4137:\\ndef test_nddataarray_from_nddata():\\n    ndd1 = NDData([1., 4., 9.],\\n                  uncertainty=StdDevUncertainty([1., 2., 3.]))\\n    ndd2 = NDDataArray(ndd1)\\n\\n    assert ndd2.data is ndd1.data\\n    assert ndd2.uncertainty is ndd1.uncertainty\\n    assert ndd2.meta == ndd1.meta\\n" }\n'
line: b'{ "repo_name": "Achint08/open-event-orga-server", "ref": "refs/heads/development", "path": "migrations/versions/ed4b4ba3274e_.py", "content": "\\"\\"\\"empty message\\n\\nRevision ID: ed4b4ba3274e\\nRevises: 784a1fc57171\\nCreate Date: 2016-06-16 06:08:49.516538\\n\\n\\"\\"\\"\\n\\n# revision identifiers, used by Alembic.\\nrevision = \'ed4b4ba3274e\'\\ndown_revision = \'784a1fc57171\'\\n\\nfrom alembic import op\\nimport sqlalchemy as sa\\nimport sqlalchemy_utils\\n\\n\\ndef upgrade():\\n    ### commands auto generated by Alembic - please adjust! ###\\n    op.create_table(\'service\',\\n    sa.Column(\'id\', sa.Integer(), nullable=False),\\n    sa.Column(\'name\', sa.String(), nullable=False),\\n    sa.PrimaryKeyConstraint(\'id\'),\\n    sa.UniqueConstraint(\'name\')\\n    )\\n    op.add_column(u\'permissions\', sa.Column(\'can_create\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'can_delete\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'can_read\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'can_update\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'role_id\', sa.Integer(), nullable=True))\\n    op.alter_column(u\'permissions\', \'service_id\',\\n               existing_type=sa.INTEGER(),\\n               nullable=True)\\n    op.create_unique_constraint(\'role_service_uc\', \'permissions\', [\'role_id\', \'service_id\'])\\n    op.drop_constraint(u\'user_service_uc\', \'permissions\', type_=\'unique\')\\n    op.drop_constraint(u\'permissions_user_id_fkey\', \'permissions\', type_=\'foreignkey\')\\n    op.create_foreign_key(None, \'permissions\', \'role\', [\'role_id\'], [\'id\'])\\n    op.create_foreign_key(None, \'permissions\', \'service\', [\'service_id\'], [\'id\'])\\n    op.drop_column(u\'permissions\', \'user_id\')\\n    op.drop_column(u\'permissions\', \'modes\')\\n    op.drop_column(u\'permissions\', \'service\')\\n    op.alter_column(u\'role\', \'name\',\\n               existing_type=sa.VARCHAR(length=128),\\n               nullable=False)\\n    op.create_unique_constraint(None, \'role\', [\'name\'])\\n    op.drop_column(u\'user\', \'role\')\\n    ### end Alembic commands ###\\n\\n\\ndef downgrade():\\n    ### commands auto generated by Alembic - please adjust! ###\\n    op.add_column(u\'user\', sa.Column(\'role\', sa.VARCHAR(), autoincrement=False, nullable=True))\\n    op.drop_constraint(None, \'role\', type_=\'unique\')\\n    op.alter_column(u\'role\', \'name\',\\n               existing_type=sa.VARCHAR(length=128),\\n               nullable=True)\\n    op.add_column(u\'permissions\', sa.Column(\'service\', sa.VARCHAR(), autoincrement=False, nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'modes\', sa.INTEGER(), autoincrement=False, nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'user_id\', sa.INTEGER(), autoincrement=False, nullable=False))\\n    op.drop_constraint(None, \'permissions\', type_=\'foreignkey\')\\n    op.drop_constraint(None, \'permissions\', type_=\'foreignkey\')\\n    op.create_foreign_key(u\'permissions_user_id_fkey\', \'permissions\', \'user\', [\'user_id\'], [\'id\'])\\n    op.create_unique_constraint(u\'user_service_uc\', \'permissions\', [\'user_id\', \'service\', \'service_id\'])\\n    op.drop_constraint(\'role_service_uc\', \'permissions\', type_=\'unique\')\\n    op.alter_column(u\'permissions\', \'service_id\',\\n               existing_type=sa.INTEGER(),\\n               nullable=False)\\n    op.drop_column(u\'permissions\', \'role_id\')\\n    op.drop_column(u\'permissions\', \'can_update\')\\n    op.drop_column(u\'permissions\', \'can_read\')\\n    op.drop_column(u\'permissions\', \'can_delete\')\\n    op.drop_column(u\'permissions\', \'can_create\')\\n    op.drop_table(\'service\')\\n    ### end Alembic commands ###\\n" }\n'
line: b'{ "repo_name": "larrybradley/astropy", "ref": "refs/heads/main", "path": "astropy/nddata/tests/test_compat.py", "content": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\\n# This module contains tests of a class equivalent to pre-1.0 NDData.\\n\\n\\nimport pytest\\nimport numpy as np\\n\\nfrom astropy.nddata.nddata import NDData\\nfrom astropy.nddata.compat import NDDataArray\\nfrom astropy.nddata.nduncertainty import StdDevUncertainty\\nfrom astropy.wcs import WCS\\nfrom astropy import units as u\\n\\n\\nNDDATA_ATTRIBUTES = [\'mask\', \'flags\', \'uncertainty\', \'unit\', \'shape\', \'size\',\\n                     \'dtype\', \'ndim\', \'wcs\', \'convert_unit_to\']\\n\\n\\ndef test_nddataarray_has_attributes_of_old_nddata():\\n    ndd = NDDataArray([1, 2, 3])\\n    for attr in NDDATA_ATTRIBUTES:\\n        assert hasattr(ndd, attr)\\n\\n\\ndef test_nddata_simple():\\n    nd = NDDataArray(np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n\\n\\ndef test_nddata_parameters():\\n    # Test for issue 4620\\n    nd = NDDataArray(data=np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    # Change order; `data` has to be given explicitly here\\n    nd = NDDataArray(meta={} data=np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    # Pass uncertainty as second implicit argument\\n    data = np.zeros((10, 10))\\n    uncertainty = StdDevUncertainty(0.1 + np.zeros_like(data))\\n    nd = NDDataArray(data, uncertainty)\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    assert nd.uncertainty == uncertainty\\n\\n\\ndef test_nddata_conversion():\\n    nd = NDDataArray(np.array([[1, 2, 3], [4, 5, 6]]))\\n    assert nd.size == 6\\n    assert nd.dtype == np.dtype(int)\\n\\n\\n@pytest.mark.parametrize(\'flags_in\', [\\n                         np.array([True, False]),\\n                         np.array([1, 0]),\\n                         [True, False],\\n                         [1, 0],\\n                         np.array([\'a\', \'b\']),\\n                         [\'a\', \'b\']])\\ndef test_nddata_flags_init_without_np_array(flags_in):\\n    ndd = NDDataArray([1, 1], flags=flags_in)\\n    assert (ndd.flags == flags_in).all()\\n\\n\\n@pytest.mark.parametrize((\'shape\'), [(10,), (5, 5), (3, 10, 10)])\\ndef test_nddata_flags_invalid_shape(shape):\\n    with pytest.raises(ValueError) as exc:\\n        NDDataArray(np.zeros((10, 10)), flags=np.ones(shape))\\n    assert exc.value.args[0] == \'dimensions of flags do not match data\'\\n\\n\\ndef test_convert_unit_to():\\n    # convert_unit_to should return a copy of its input\\n    d = NDDataArray(np.ones((5, 5)))\\n    d.unit = \'km\'\\n    d.uncertainty = StdDevUncertainty(0.1 + np.zeros_like(d))\\n    # workaround because zeros_like does not support dtype arg until v1.6\\n    # and NDData accepts only bool ndarray as mask\\n    tmp = np.zeros_like(d.data)\\n    d.mask = np.array(tmp, dtype=bool)\\n    d1 = d.convert_unit_to(\'m\')\\n    assert np.all(d1.data == np.array(1000.0))\\n    assert np.all(d1.uncertainty.array == 1000.0 * d.uncertainty.array)\\n    assert d1.unit == u.m\\n    # changing the output mask should not change the original\\n    d1.mask[0, 0] = True\\n    assert d.mask[0, 0] != d1.mask[0, 0]\\n    d.flags = np.zeros_like(d.data)\\n    d1 = d.convert_unit_to(\'m\')\\n\\n\\n# check that subclasses can require wcs and/or unit to be present and use\\n# _arithmetic and convert_unit_to\\nclass SubNDData(NDDataArray):\\n    \\"\\"\\"\\n    Subclass for test initialization of subclasses in NDData._arithmetic and\\n    NDData.convert_unit_to\\n    \\"\\"\\"\\n    def __init__(self, *arg, **kwd):\\n        super().__init__(*arg, **kwd)\\n        if self.unit is None:\\n            raise ValueError(\\"Unit for subclass must be specified\\")\\n        if self.wcs is None:\\n            raise ValueError(\\"WCS for subclass must be specified\\")\\n\\n\\ndef test_init_of_subclass_in_convert_unit_to():\\n    data = np.ones([10, 10])\\n    arr1 = SubNDData(data, unit=\'m\', wcs=WCS(naxis=2))\\n    result = arr1.convert_unit_to(\'km\')\\n    np.testing.assert_array_equal(arr1.data, 1000 * result.data)\\n\\n\\n# Test for issue #4129:\\ndef test_nddataarray_from_nddataarray():\\n    ndd1 = NDDataArray([1., 4., 9.],\\n                       uncertainty=StdDevUncertainty([1., 2., 3.]),\\n                       flags=[0, 1, 0])\\n    ndd2 = NDDataArray(ndd1)\\n    # Test that the 2 instances point to the same objects and aren\'t just\\n    # equal; this is explicitly documented for the main data array and we\\n    # probably want to catch any future change in behavior for the other\\n    # attributes too and ensure they are intentional.\\n    assert ndd2.data is ndd1.data\\n    assert ndd2.uncertainty is ndd1.uncertainty\\n    assert ndd2.flags is ndd1.flags\\n    assert ndd2.meta == ndd1.meta\\n\\n\\n# Test for issue #4137:\\ndef test_nddataarray_from_nddata():\\n    ndd1 = NDData([1., 4., 9.],\\n                  uncertainty=StdDevUncertainty([1., 2., 3.]))\\n    ndd2 = NDDataArray(ndd1)\\n\\n    assert ndd2.data is ndd1.data\\n    assert ndd2.uncertainty is ndd1.uncertainty\\n    assert ndd2.meta == ndd1.meta\\n" } \n'
line: b'{ "repo_name": "sridevikoushik31/nova", "ref": "refs/heads/port_id_in_vif_on_devide", "path": "nova/openstack/common/rootwrap/filters.py", "content": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\\n\\n# Copyright (c) 2011 OpenStack Foundation.\\n# All Rights Reserved.\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\nimport os\\nimport re\\n\\n\\nclass CommandFilter(object):\\n    \\"\\"\\"Command filter only checking that the 1st argument matches exec_path\\"\\"\\"\\n\\n    def __init__(self, exec_path, run_as, *args):\\n        self.name = \'\'\\n        self.exec_path = exec_path\\n        self.run_as = run_as\\n        self.args = args\\n        self.real_exec = None\\n\\n    def get_exec(self, exec_dirs=[]):\\n        \\"\\"\\"Returns existing executable, or empty string if none found\\"\\"\\"\\n        if self.real_exec is not None:\\n            return self.real_exec\\n        self.real_exec = \\"\\"\\n        if self.exec_path.startswith(\'/\'):\\n            if os.access(self.exec_path, os.X_OK):\\n                self.real_exec = self.exec_path\\n        else:\\n            for binary_path in exec_dirs:\\n                expanded_path = os.path.join(binary_path, self.exec_path)\\n                if os.access(expanded_path, os.X_OK):\\n                    self.real_exec = expanded_path\\n                    break\\n        return self.real_exec\\n\\n    def match(self, userargs):\\n        \\"\\"\\"Only check that the first argument (command) matches exec_path\\"\\"\\"\\n        if (os.path.basename(self.exec_path) == userargs[0]):\\n            return True\\n        return False\\n\\n    def get_command(self, userargs, exec_dirs=[]):\\n        \\"\\"\\"Returns command to execute (with sudo -u if run_as != root).\\"\\"\\"\\n        to_exec = self.get_exec(exec_dirs=exec_dirs) or self.exec_path\\n        if (self.run_as != \'root\'):\\n            # Used to run commands at lesser privileges\\n            return [\'sudo\', \'-u\', self.run_as, to_exec] + userargs[1:]\\n        return [to_exec] + userargs[1:]\\n\\n    def get_environment(self, userargs):\\n        \\"\\"\\"Returns specific environment to set, None if none\\"\\"\\"\\n        return None\\n\\n\\nclass RegExpFilter(CommandFilter):\\n    \\"\\"\\"Command filter doing regexp matching for every argument\\"\\"\\"\\n\\n    def match(self, userargs):\\n        # Early skip if command or number of args don\'t match\\n        if (len(self.args) != len(userargs)):\\n            # DENY: argument numbers don\'t match\\n            return False\\n        # Compare each arg (anchoring pattern explicitly at end of string)\\n        for (pattern, arg) in zip(self.args, userargs):\\n            try:\\n                if not re.match(pattern + \'$\', arg):\\n                    break\\n            except re.error:\\n                # DENY: Badly-formed filter\\n                return False\\n        else:\\n            # ALLOW: All arguments matched\\n            return True\\n\\n        # DENY: Some arguments did not match\\n        return False\\n\\n\\nclass PathFilter(CommandFilter):\\n    \\"\\"\\"Command filter checking that path arguments are within given dirs\\n\\n        One can specify the following constraints for command arguments:\\n            1) pass     - pass an argument as is to the resulting command\\n            2) some_str - check if an argument is equal to the given string\\n            3) abs path - check if a path argument is within the given base dir\\n\\n        A typical rootwrapper filter entry looks like this:\\n            # cmdname: filter name, raw command, user, arg_i_constraint [, ...]\\n            chown: PathFilter, /bin/chown, root, nova, /var/lib/images\\n\\n    \\"\\"\\"\\n\\n    def match(self, userargs):\\n        command, arguments = userargs[0], userargs[1:]\\n\\n        equal_args_num = len(self.args) == len(arguments)\\n        exec_is_valid = super(PathFilter, self).match(userargs)\\n        args_equal_or_pass = all(\\n            arg == \'pass\' or arg == value\\n            for arg, value in zip(self.args, arguments)\\n            if not os.path.isabs(arg)  # arguments not specifying abs paths\\n        )\\n        paths_are_within_base_dirs = all(\\n            os.path.commonprefix([arg, os.path.realpath(value)]) == arg\\n            for arg, value in zip(self.args, arguments)\\n            if os.path.isabs(arg)  # arguments specifying abs paths\\n        )\\n\\n        return (equal_args_num and\\n                exec_is_valid and\\n                args_equal_or_pass and\\n                paths_are_within_base_dirs)\\n\\n    def get_command(self, userargs, exec_dirs=[]):\\n        command, arguments = userargs[0], userargs[1:]\\n\\n        # convert path values to canonical ones; copy other args as is\\n        args = [os.path.realpath(value) if os.path.isabs(arg) else value\\n                for arg, value in zip(self.args, arguments)]\\n\\n        return super(PathFilter, self).get_command([command] + args,\\n                                                   exec_dirs)\\n\\n\\nclass DnsmasqFilter(CommandFilter):\\n    \\"\\"\\"Specific filter for the dnsmasq call (which includes env)\\"\\"\\"\\n\\n    CONFIG_FILE_ARG = \'CONFIG_FILE\'\\n\\n    def match(self, userargs):\\n        if (userargs[0] == \'env\' and\\n                userargs[1].startswith(self.CONFIG_FILE_ARG) and\\n                userargs[2].startswith(\'NETWORK_ID=\') and\\n                userargs[3] == \'dnsmasq\'):\\n            return True\\n        return False\\n\\n    def get_command(self, userargs, exec_dirs=[]):\\n        to_exec = self.get_exec(exec_dirs=exec_dirs) or self.exec_path\\n        dnsmasq_pos = userargs.index(\'dnsmasq\')\\n        return [to_exec] + userargs[dnsmasq_pos + 1:]\\n\\n    def get_environment(self, userargs):\\n        env = os.environ.copy()\\n        env[self.CONFIG_FILE_ARG] = userargs[1].split(\'=\')[-1]\\n        env[\'NETWORK_ID\'] = userargs[2].split(\'=\')[-1]\\n        return env\\n\\n\\nclass DeprecatedDnsmasqFilter(DnsmasqFilter):\\n    \\"\\"\\"Variant of dnsmasq filter to support old-style FLAGFILE\\"\\"\\"\\n    CONFIG_FILE_ARG = \'FLAGFILE\'\\n\\n\\nclass KillFilter(CommandFilter):\\n    \\"\\"\\"Specific filter for the kill calls.\\n       1st argument is the user to run /bin/kill under\\n       2nd argument is the location of the affected executable\\n       Subsequent arguments list the accepted signals (if any)\\n\\n       This filter relies on /proc to accurately determine affected\\n       executable, so it will only work on procfs-capable systems (not OSX).\\n    \\"\\"\\"\\n\\n    def __init__(self, *args):\\n        super(KillFilter, self).__init__(\\"/bin/kill\\", *args)\\n\\n    def match(self, userargs):\\n        if userargs[0] != \\"kill\\":\\n            return False\\n        args = list(userargs)\\n        if len(args) == 3:\\n            # A specific signal is requested\\n            signal = args.pop(1)\\n            if signal not in self.args[1:]:\\n                # Requested signal not in accepted list\\n                return False\\n        else:\\n            if len(args) != 2:\\n                # Incorrect number of arguments\\n                return False\\n            if len(self.args) > 1:\\n                # No signal requested, but filter requires specific signal\\n                return False\\n        try:\\n            command = os.readlink(\\"/proc/%d/exe\\" % int(args[1]))\\n            # NOTE(dprince): /proc/PID/exe may have \' (deleted)\' on\\n            # the end if an executable is updated or deleted\\n            if command.endswith(\\" (deleted)\\"):\\n                command = command[:command.rindex(\\" \\")]\\n            if command != self.args[0]:\\n                # Affected executable does not match\\n                return False\\n        except (ValueError, OSError):\\n            # Incorrect PID\\n            return False\\n        return True\\n\\n\\nclass ReadFileFilter(CommandFilter):\\n    \\"\\"\\"Specific filter for the utils.read_file_as_root call\\"\\"\\"\\n\\n    def __init__(self, file_path, *args):\\n        self.file_path = file_path\\n        super(ReadFileFilter, self).__init__(\\"/bin/cat\\", \\"root\\", *args)\\n\\n    def match(self, userargs):\\n        if userargs[0] != \'cat\':\\n            return False\\n        if userargs[1] != self.file_path:\\n            return False\\n        if len(userargs) != 2:\\n            return False\\n        return True\\n" }\n'
line: b'{ "repo_name": "rafalkowalski/open-event-orga-server", "ref": "refs/heads/development", "path": "migrations/versions/ed4b4ba3274e_.py", "content": "\\"\\"\\"empty message\\n\\nRevision ID: ed4b4ba3274e\\nRevises: 784a1fc57171\\nCreate Date: 2016-06-16 06:08:49.516538\\n\\n\\"\\"\\"\\n\\n# revision identifiers, used by Alembic.\\nrevision = \'ed4b4ba3274e\'\\ndown_revision = \'784a1fc57171\'\\n\\nfrom alembic import op\\nimport sqlalchemy as sa\\nimport sqlalchemy_utils\\n\\n\\ndef upgrade():\\n    ### commands auto generated by Alembic - please adjust! ###\\n    op.create_table(\'service\',\\n    sa.Column(\'id\', sa.Integer(), nullable=False),\\n    sa.Column(\'name\', sa.String(), nullable=False),\\n    sa.PrimaryKeyConstraint(\'id\'),\\n    sa.UniqueConstraint(\'name\')\\n    )\\n    op.add_column(u\'permissions\', sa.Column(\'can_create\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'can_delete\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'can_read\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'can_update\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'role_id\', sa.Integer(), nullable=True))\\n    op.alter_column(u\'permissions\', \'service_id\',\\n               existing_type=sa.INTEGER(),\\n               nullable=True)\\n    op.create_unique_constraint(\'role_service_uc\', \'permissions\', [\'role_id\', \'service_id\'])\\n    op.drop_constraint(u\'user_service_uc\', \'permissions\', type_=\'unique\')\\n    op.drop_constraint(u\'permissions_user_id_fkey\', \'permissions\', type_=\'foreignkey\')\\n    op.create_foreign_key(None, \'permissions\', \'role\', [\'role_id\'], [\'id\'])\\n    op.create_foreign_key(None, \'permissions\', \'service\', [\'service_id\'], [\'id\'])\\n    op.drop_column(u\'permissions\', \'user_id\')\\n    op.drop_column(u\'permissions\', \'modes\')\\n    op.drop_column(u\'permissions\', \'service\')\\n    op.alter_column(u\'role\', \'name\',\\n               existing_type=sa.VARCHAR(length=128),\\n               nullable=False)\\n    op.create_unique_constraint(None, \'role\', [\'name\'])\\n    op.drop_column(u\'user\', \'role\')\\n    ### end Alembic commands ###\\n\\n\\ndef downgrade():\\n    ### commands auto generated by Alembic - please adjust! ###\\n    op.add_column(u\'user\', sa.Column(\'role\', sa.VARCHAR(), autoincrement=False, nullable=True))\\n    op.drop_constraint(None, \'role\', type_=\'unique\')\\n    op.alter_column(u\'role\', \'name\',\\n               existing_type=sa.VARCHAR(length=128),\\n               nullable=True)\\n    op.add_column(u\'permissions\', sa.Column(\'service\', sa.VARCHAR(), autoincrement=False, nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'modes\', sa.INTEGER(), autoincrement=False, nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'user_id\', sa.INTEGER(), autoincrement=False, nullable=False))\\n    op.drop_constraint(None, \'permissions\', type_=\'foreignkey\')\\n    op.drop_constraint(None, \'permissions\', type_=\'foreignkey\')\\n    op.create_foreign_key(u\'permissions_user_id_fkey\', \'permissions\', \'user\', [\'user_id\'], [\'id\'])\\n    op.create_unique_constraint(u\'user_service_uc\', \'permissions\', [\'user_id\', \'service\', \'service_id\'])\\n    op.drop_constraint(\'role_service_uc\', \'permissions\', type_=\'unique\')\\n    op.alter_column(u\'permissions\', \'service_id\',\\n               existing_type=sa.INTEGER(),\\n               nullable=False)\\n    op.drop_column(u\'permissions\', \'role_id\')\\n    op.drop_column(u\'permissions\', \'can_update\')\\n    op.drop_column(u\'permissions\', \'can_read\')\\n    op.drop_column(u\'permissions\', \'can_delete\')\\n    op.drop_column(u\'permissions\', \'can_create\')\\n    op.drop_table(\'service\')\\n    ### end Alembic commands ###\\n" }\n'
line: b'{ "repo_name": "aviaryan/open-event-orga-server", "ref": "refs/heads/development", "path": "migrations/versions/ed4b4ba3274e_.py", "content": "\\"\\"\\"empty message\\n\\nRevision ID: ed4b4ba3274e\\nRevises: 784a1fc57171\\nCreate Date: 2016-06-16 06:08:49.516538\\n\\n\\"\\"\\"\\n\\n# revision identifiers, used by Alembic.\\nrevision = \'ed4b4ba3274e\'\\ndown_revision = \'784a1fc57171\'\\n\\nfrom alembic import op\\nimport sqlalchemy as sa\\nimport sqlalchemy_utils\\n\\n\\ndef upgrade():\\n    ### commands auto generated by Alembic - please adjust! ###\\n    op.create_table(\'service\',\\n    sa.Column(\'id\', sa.Integer(), nullable=False),\\n    sa.Column(\'name\', sa.String(), nullable=False),\\n    sa.PrimaryKeyConstraint(\'id\'),\\n    sa.UniqueConstraint(\'name\')\\n    )\\n    op.add_column(u\'permissions\', sa.Column(\'can_create\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'can_delete\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'can_read\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'can_update\', sa.Boolean(), nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'role_id\', sa.Integer(), nullable=True))\\n    op.alter_column(u\'permissions\', \'service_id\',\\n               existing_type=sa.INTEGER(),\\n               nullable=True)\\n    op.create_unique_constraint(\'role_service_uc\', \'permissions\', [\'role_id\', \'service_id\'])\\n    op.drop_constraint(u\'user_service_uc\', \'permissions\', type_=\'unique\')\\n    op.drop_constraint(u\'permissions_user_id_fkey\', \'permissions\', type_=\'foreignkey\')\\n    op.create_foreign_key(None, \'permissions\', \'role\', [\'role_id\'], [\'id\'])\\n    op.create_foreign_key(None, \'permissions\', \'service\', [\'service_id\'], [\'id\'])\\n    op.drop_column(u\'permissions\', \'user_id\')\\n    op.drop_column(u\'permissions\', \'modes\')\\n    op.drop_column(u\'permissions\', \'service\')\\n    op.alter_column(u\'role\', \'name\',\\n               existing_type=sa.VARCHAR(length=128),\\n               nullable=False)\\n    op.create_unique_constraint(None, \'role\', [\'name\'])\\n    op.drop_column(u\'user\', \'role\')\\n    ### end Alembic commands ###\\n\\n\\ndef downgrade():\\n    ### commands auto generated by Alembic - please adjust! ###\\n    op.add_column(u\'user\', sa.Column(\'role\', sa.VARCHAR(), autoincrement=False, nullable=True))\\n    op.drop_constraint(None, \'role\', type_=\'unique\')\\n    op.alter_column(u\'role\', \'name\',\\n               existing_type=sa.VARCHAR(length=128),\\n               nullable=True)\\n    op.add_column(u\'permissions\', sa.Column(\'service\', sa.VARCHAR(), autoincrement=False, nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'modes\', sa.INTEGER(), autoincrement=False, nullable=False))\\n    op.add_column(u\'permissions\', sa.Column(\'user_id\', sa.INTEGER(), autoincrement=False, nullable=False))\\n    op.drop_constraint(None, \'permissions\', type_=\'foreignkey\')\\n    op.drop_constraint(None, \'permissions\', type_=\'foreignkey\')\\n    op.create_foreign_key(u\'permissions_user_id_fkey\', \'permissions\', \'user\', [\'user_id\'], [\'id\'])\\n    op.create_unique_constraint(u\'user_service_uc\', \'permissions\', [\'user_id\', \'service\', \'service_id\'])\\n    op.drop_constraint(\'role_service_uc\', \'permissions\', type_=\'unique\')\\n    op.alter_column(u\'permissions\', \'service_id\',\\n               existing_type=sa.INTEGER(),\\n               nullable=False)\\n    op.drop_column(u\'permissions\', \'role_id\')\\n    op.drop_column(u\'permissions\', \'can_update\')\\n    op.drop_column(u\'permissions\', \'can_read\')\\n    op.drop_column(u\'permissions\', \'can_delete\')\\n    op.drop_column(u\'permissions\', \'can_create\')\\n    op.drop_table(\'service\')\\n    ### end Alembic commands ###\\n" }\n'
line: b'{ "repo_name": "DirectXMan12/nova-hacking", "ref": "refs/heads/feature_novnc_krb", "path": "nova/openstack/common/rootwrap/filters.py", "content": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\\n\\n# Copyright (c) 2011 OpenStack Foundation.\\n# All Rights Reserved.\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\nimport os\\nimport re\\n\\n\\nclass CommandFilter(object):\\n    \\"\\"\\"Command filter only checking that the 1st argument matches exec_path\\"\\"\\"\\n\\n    def __init__(self, exec_path, run_as, *args):\\n        self.name = \'\'\\n        self.exec_path = exec_path\\n        self.run_as = run_as\\n        self.args = args\\n        self.real_exec = None\\n\\n    def get_exec(self, exec_dirs=[]):\\n        \\"\\"\\"Returns existing executable, or empty string if none found\\"\\"\\"\\n        if self.real_exec is not None:\\n            return self.real_exec\\n        self.real_exec = \\"\\"\\n        if self.exec_path.startswith(\'/\'):\\n            if os.access(self.exec_path, os.X_OK):\\n                self.real_exec = self.exec_path\\n        else:\\n            for binary_path in exec_dirs:\\n                expanded_path = os.path.join(binary_path, self.exec_path)\\n                if os.access(expanded_path, os.X_OK):\\n                    self.real_exec = expanded_path\\n                    break\\n        return self.real_exec\\n\\n    def match(self, userargs):\\n        \\"\\"\\"Only check that the first argument (command) matches exec_path\\"\\"\\"\\n        if (os.path.basename(self.exec_path) == userargs[0]):\\n            return True\\n        return False\\n\\n    def get_command(self, userargs, exec_dirs=[]):\\n        \\"\\"\\"Returns command to execute (with sudo -u if run_as != root).\\"\\"\\"\\n        to_exec = self.get_exec(exec_dirs=exec_dirs) or self.exec_path\\n        if (self.run_as != \'root\'):\\n            # Used to run commands at lesser privileges\\n            return [\'sudo\', \'-u\', self.run_as, to_exec] + userargs[1:]\\n        return [to_exec] + userargs[1:]\\n\\n    def get_environment(self, userargs):\\n        \\"\\"\\"Returns specific environment to set, None if none\\"\\"\\"\\n        return None\\n\\n\\nclass RegExpFilter(CommandFilter):\\n    \\"\\"\\"Command filter doing regexp matching for every argument\\"\\"\\"\\n\\n    def match(self, userargs):\\n        # Early skip if command or number of args don\'t match\\n        if (len(self.args) != len(userargs)):\\n            # DENY: argument numbers don\'t match\\n            return False\\n        # Compare each arg (anchoring pattern explicitly at end of string)\\n        for (pattern, arg) in zip(self.args, userargs):\\n            try:\\n                if not re.match(pattern + \'$\', arg):\\n                    break\\n            except re.error:\\n                # DENY: Badly-formed filter\\n                return False\\n        else:\\n            # ALLOW: All arguments matched\\n            return True\\n\\n        # DENY: Some arguments did not match\\n        return False\\n\\n\\nclass PathFilter(CommandFilter):\\n    \\"\\"\\"Command filter checking that path arguments are within given dirs\\n\\n        One can specify the following constraints for command arguments:\\n            1) pass     - pass an argument as is to the resulting command\\n            2) some_str - check if an argument is equal to the given string\\n            3) abs path - check if a path argument is within the given base dir\\n\\n        A typical rootwrapper filter entry looks like this:\\n            # cmdname: filter name, raw command, user, arg_i_constraint [, ...]\\n            chown: PathFilter, /bin/chown, root, nova, /var/lib/images\\n\\n    \\"\\"\\"\\n\\n    def match(self, userargs):\\n        command, arguments = userargs[0], userargs[1:]\\n\\n        equal_args_num = len(self.args) == len(arguments)\\n        exec_is_valid = super(PathFilter, self).match(userargs)\\n        args_equal_or_pass = all(\\n            arg == \'pass\' or arg == value\\n            for arg, value in zip(self.args, arguments)\\n            if not os.path.isabs(arg)  # arguments not specifying abs paths\\n        )\\n        paths_are_within_base_dirs = all(\\n            os.path.commonprefix([arg, os.path.realpath(value)]) == arg\\n            for arg, value in zip(self.args, arguments)\\n            if os.path.isabs(arg)  # arguments specifying abs paths\\n        )\\n\\n        return (equal_args_num and\\n                exec_is_valid and\\n                args_equal_or_pass and\\n                paths_are_within_base_dirs)\\n\\n    def get_command(self, userargs, exec_dirs=[]):\\n        command, arguments = userargs[0], userargs[1:]\\n\\n        # convert path values to canonical ones; copy other args as is\\n        args = [os.path.realpath(value) if os.path.isabs(arg) else value\\n                for arg, value in zip(self.args, arguments)]\\n\\n        return super(PathFilter, self).get_command([command] + args,\\n                                                   exec_dirs)\\n\\n\\nclass DnsmasqFilter(CommandFilter):\\n    \\"\\"\\"Specific filter for the dnsmasq call (which includes env)\\"\\"\\"\\n\\n    CONFIG_FILE_ARG = \'CONFIG_FILE\'\\n\\n    def match(self, userargs):\\n        if (userargs[0] == \'env\' and\\n                userargs[1].startswith(self.CONFIG_FILE_ARG) and\\n                userargs[2].startswith(\'NETWORK_ID=\') and\\n                userargs[3] == \'dnsmasq\'):\\n            return True\\n        return False\\n\\n    def get_command(self, userargs, exec_dirs=[]):\\n        to_exec = self.get_exec(exec_dirs=exec_dirs) or self.exec_path\\n        dnsmasq_pos = userargs.index(\'dnsmasq\')\\n        return [to_exec] + userargs[dnsmasq_pos + 1:]\\n\\n    def get_environment(self, userargs):\\n        env = os.environ.copy()\\n        env[self.CONFIG_FILE_ARG] = userargs[1].split(\'=\')[-1]\\n        env[\'NETWORK_ID\'] = userargs[2].split(\'=\')[-1]\\n        return env\\n\\n\\nclass DeprecatedDnsmasqFilter(DnsmasqFilter):\\n    \\"\\"\\"Variant of dnsmasq filter to support old-style FLAGFILE\\"\\"\\"\\n    CONFIG_FILE_ARG = \'FLAGFILE\'\\n\\n\\nclass KillFilter(CommandFilter):\\n    \\"\\"\\"Specific filter for the kill calls.\\n       1st argument is the user to run /bin/kill under\\n       2nd argument is the location of the affected executable\\n       Subsequent arguments list the accepted signals (if any)\\n\\n       This filter relies on /proc to accurately determine affected\\n       executable, so it will only work on procfs-capable systems (not OSX).\\n    \\"\\"\\"\\n\\n    def __init__(self, *args):\\n        super(KillFilter, self).__init__(\\"/bin/kill\\", *args)\\n\\n    def match(self, userargs):\\n        if userargs[0] != \\"kill\\":\\n            return False\\n        args = list(userargs)\\n        if len(args) == 3:\\n            # A specific signal is requested\\n            signal = args.pop(1)\\n            if signal not in self.args[1:]:\\n                # Requested signal not in accepted list\\n                return False\\n        else:\\n            if len(args) != 2:\\n                # Incorrect number of arguments\\n                return False\\n            if len(self.args) > 1:\\n                # No signal requested, but filter requires specific signal\\n                return False\\n        try:\\n            command = os.readlink(\\"/proc/%d/exe\\" % int(args[1]))\\n            # NOTE(dprince): /proc/PID/exe may have \' (deleted)\' on\\n            # the end if an executable is updated or deleted\\n            if command.endswith(\\" (deleted)\\"):\\n                command = command[:command.rindex(\\" \\")]\\n            if command != self.args[0]:\\n                # Affected executable does not match\\n                return False\\n        except (ValueError, OSError):\\n            # Incorrect PID\\n            return False\\n        return True\\n\\n\\nclass ReadFileFilter(CommandFilter):\\n    \\"\\"\\"Specific filter for the utils.read_file_as_root call\\"\\"\\"\\n\\n    def __init__(self, file_path, *args):\\n        self.file_path = file_path\\n        super(ReadFileFilter, self).__init__(\\"/bin/cat\\", \\"root\\", *args)\\n\\n    def match(self, userargs):\\n        if userargs[0] != \'cat\':\\n            return False\\n        if userargs[1] != self.file_path:\\n            return False\\n        if len(userargs) != 2:\\n            return False\\n        return True\\n" }\n'
line: b'{ "repo_name": "stargaser/astropy", "ref": "refs/heads/placeholder", "path": "astropy/nddata/tests/test_compat.py", "content": "# Licensed under a 3-clause BSD style license - see LICENSE.rst\\n# This module contains tests of a class equivalent to pre-1.0 NDData.\\n\\n\\nimport pytest\\nimport numpy as np\\n\\nfrom astropy.nddata.nddata import NDData\\nfrom astropy.nddata.compat import NDDataArray\\nfrom astropy.nddata.nduncertainty import StdDevUncertainty\\nfrom astropy.wcs import WCS\\nfrom astropy import units as u\\n\\n\\nNDDATA_ATTRIBUTES = [\'mask\', \'flags\', \'uncertainty\', \'unit\', \'shape\', \'size\',\\n                     \'dtype\', \'ndim\', \'wcs\', \'convert_unit_to\']\\n\\n\\ndef test_nddataarray_has_attributes_of_old_nddata():\\n    ndd = NDDataArray([1, 2, 3])\\n    for attr in NDDATA_ATTRIBUTES:\\n        assert hasattr(ndd, attr)\\n\\n\\ndef test_nddata_simple():\\n    nd = NDDataArray(np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n\\n\\ndef test_nddata_parameters():\\n    # Test for issue 4620\\n    nd = NDDataArray(data=np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    # Change order; `data` has to be given explicitly here\\n    nd = NDDataArray(meta={} data=np.zeros((10, 10)))\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    # Pass uncertainty as second implicit argument\\n    data = np.zeros((10, 10))\\n    uncertainty = StdDevUncertainty(0.1 + np.zeros_like(data))\\n    nd = NDDataArray(data, uncertainty)\\n    assert nd.shape == (10, 10)\\n    assert nd.size == 100\\n    assert nd.dtype == np.dtype(float)\\n    assert nd.uncertainty == uncertainty\\n\\n\\ndef test_nddata_conversion():\\n    nd = NDDataArray(np.array([[1, 2, 3], [4, 5, 6]]))\\n    assert nd.size == 6\\n    assert nd.dtype == np.dtype(int)\\n\\n\\n@pytest.mark.parametrize(\'flags_in\', [\\n                         np.array([True, False]),\\n                         np.array([1, 0]),\\n                         [True, False],\\n                         [1, 0],\\n                         np.array([\'a\', \'b\']),\\n                         [\'a\', \'b\']])\\ndef test_nddata_flags_init_without_np_array(flags_in):\\n    ndd = NDDataArray([1, 1], flags=flags_in)\\n    assert (ndd.flags == flags_in).all()\\n\\n\\n@pytest.mark.parametrize((\'shape\'), [(10,), (5, 5), (3, 10, 10)])\\ndef test_nddata_flags_invalid_shape(shape):\\n    with pytest.raises(ValueError) as exc:\\n        NDDataArray(np.zeros((10, 10)), flags=np.ones(shape))\\n    assert exc.value.args[0] == \'dimensions of flags do not match data\'\\n\\n\\ndef test_convert_unit_to():\\n    # convert_unit_to should return a copy of its input\\n    d = NDDataArray(np.ones((5, 5)))\\n    d.unit = \'km\'\\n    d.uncertainty = StdDevUncertainty(0.1 + np.zeros_like(d))\\n    # workaround because zeros_like does not support dtype arg until v1.6\\n    # and NDData accepts only bool ndarray as mask\\n    tmp = np.zeros_like(d.data)\\n    d.mask = np.array(tmp, dtype=bool)\\n    d1 = d.convert_unit_to(\'m\')\\n    assert np.all(d1.data == np.array(1000.0))\\n    assert np.all(d1.uncertainty.array == 1000.0 * d.uncertainty.array)\\n    assert d1.unit == u.m\\n    # changing the output mask should not change the original\\n    d1.mask[0, 0] = True\\n    assert d.mask[0, 0] != d1.mask[0, 0]\\n    d.flags = np.zeros_like(d.data)\\n    d1 = d.convert_unit_to(\'m\')\\n\\n\\n# check that subclasses can require wcs and/or unit to be present and use\\n# _arithmetic and convert_unit_to\\nclass SubNDData(NDDataArray):\\n    \\"\\"\\"\\n    Subclass for test initialization of subclasses in NDData._arithmetic and\\n    NDData.convert_unit_to\\n    \\"\\"\\"\\n    def __init__(self, *arg, **kwd):\\n        super().__init__(*arg, **kwd)\\n        if self.unit is None:\\n            raise ValueError(\\"Unit for subclass must be specified\\")\\n        if self.wcs is None:\\n            raise ValueError(\\"WCS for subclass must be specified\\")\\n\\n\\ndef test_init_of_subclass_in_convert_unit_to():\\n    data = np.ones([10, 10])\\n    arr1 = SubNDData(data, unit=\'m\', wcs=WCS(naxis=2))\\n    result = arr1.convert_unit_to(\'km\')\\n    np.testing.assert_array_equal(arr1.data, 1000 * result.data)\\n\\n\\n# Test for issue #4129:\\ndef test_nddataarray_from_nddataarray():\\n    ndd1 = NDDataArray([1., 4., 9.],\\n                       uncertainty=StdDevUncertainty([1., 2., 3.]),\\n                       flags=[0, 1, 0])\\n    ndd2 = NDDataArray(ndd1)\\n    # Test that the 2 instances point to the same objects and aren\'t just\\n    # equal; this is explicitly documented for the main data array and we\\n    # probably want to catch any future change in behavior for the other\\n    # attributes too and ensure they are intentional.\\n    assert ndd2.data is ndd1.data\\n    assert ndd2.uncertainty is ndd1.uncertainty\\n    assert ndd2.flags is ndd1.flags\\n    assert ndd2.meta == ndd1.meta\\n\\n\\n# Test for issue #4137:\\ndef test_nddataarray_from_nddata():\\n    ndd1 = NDData([1., 4., 9.],\\n                  uncertainty=StdDevUncertainty([1., 2., 3.]))\\n    ndd2 = NDDataArray(ndd1)\\n\\n    assert ndd2.data is ndd1.data\\n    assert ndd2.uncertainty is ndd1.uncertainty\\n    assert ndd2.meta == ndd1.meta\\n" }\n'
line: b'{ "repo_name": "tangfeixiong/nova", "ref": "refs/heads/stable/juno", "path": "nova/scheduler/__init__.py", "content": "# Copyright (c) 2010 OpenStack Foundation\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\n\\"\\"\\"\\n:mod:`nova.scheduler` -- Scheduler Nodes\\n=====================================================\\n\\n.. automodule:: nova.scheduler\\n   :platform: Unix\\n   :synopsis: Module that picks a compute node to run a VM instance.\\n\\"\\"\\"\\n" }\n'
line: b'{ "repo_name": "BeyondTheClouds/nova", "ref": "refs/heads/disco/mitaka", "path": "nova/scheduler/__init__.py", "content": "# Copyright (c) 2010 OpenStack Foundation\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\n\\"\\"\\"\\n:mod:`nova.scheduler` -- Scheduler Nodes\\n=====================================================\\n\\n.. automodule:: nova.scheduler\\n   :platform: Unix\\n   :synopsis: Module that picks a compute node to run a VM instance.\\n\\"\\"\\"\\n" }\n'
line: b'{ "repo_name": "cyx1231st/nova", "ref": "refs/heads/eventually-consistent-host-state-mitaka", "path": "nova/scheduler/__init__.py", "content": "# Copyright (c) 2010 OpenStack Foundation\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\n\\"\\"\\"\\n:mod:`nova.scheduler` -- Scheduler Nodes\\n=====================================================\\n\\n.. automodule:: nova.scheduler\\n   :platform: Unix\\n   :synopsis: Module that picks a compute node to run a VM instance.\\n\\"\\"\\"\\n" }\n'
line: b'{ "repo_name": "projectcalico/calico-nova", "ref": "refs/heads/calico-readme", "path": "nova/scheduler/__init__.py", "content": "# Copyright (c) 2010 OpenStack Foundation\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\n\\"\\"\\"\\n:mod:`nova.scheduler` -- Scheduler Nodes\\n=====================================================\\n\\n.. automodule:: nova.scheduler\\n   :platform: Unix\\n   :synopsis: Module that picks a compute node to run a VM instance.\\n\\"\\"\\"\\n" }\n'
line: b'{ "repo_name": "changsimon/trove", "ref": "refs/heads/bug/1347114-dev", "path": "trove/openstack/common/notifier/rabbit_notifier.py", "content": "# Copyright 2012 Red Hat, Inc.\\n# All Rights Reserved.\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\n\\nfrom trove.openstack.common.gettextutils import _\\nfrom trove.openstack.common import log as logging\\nfrom trove.openstack.common.notifier import rpc_notifier\\n\\nLOG = logging.getLogger(__name__)\\n\\n\\ndef notify(context, message):\\n    \\"\\"\\"Deprecated in Grizzly. Please use rpc_notifier instead.\\"\\"\\"\\n\\n    LOG.deprecated(_(\\"The rabbit_notifier is now deprecated.\\"\\n                     \\" Please use rpc_notifier instead.\\"))\\n    rpc_notifier.notify(context, message)\\n" }\n'
line: b'{ "repo_name": "erdincay/pyload", "ref": "refs/heads/stable", "path": "module/plugins/internal/Container.py", "content": "# -*- coding: utf-8 -*-\\n\\nfrom __future__ import with_statement\\n\\nimport os\\nimport re\\nimport traceback\\n\\nfrom module.plugins.internal.Crypter import Crypter\\nfrom module.plugins.internal.Plugin import exists\\nfrom module.utils import save_join as fs_join\\n\\n\\nclass Container(Crypter):\\n    __name__    = \\"Container\\"\\n    __type__    = \\"container\\"\\n    __version__ = \\"0.06\\"\\n    __status__  = \\"testing\\"\\n\\n    __pattern__ = r\'^unmatchable$\'\\n    __config__  = []  #: [(\\"name\\", \\"type\\", \\"desc\\", \\"default\\")]\\n\\n    __description__ = \\"\\"\\"Base container decrypter plugin\\"\\"\\"\\n    __license__     = \\"GPLv3\\"\\n    __authors__     = [(\\"mkaay\\", \\"mkaay@mkaay.de\\")]\\n\\n\\n    def process(self, pyfile):\\n        \\"\\"\\"\\n        Main method\\n        \\"\\"\\"\\n        self._load2disk()\\n\\n        self.decrypt(pyfile)\\n\\n        self.delete_tmp()\\n\\n        if self.urls:\\n            self._generate_packages()\\n\\n        elif not self.packages:\\n            self.error(_(\\"No link grabbed\\"), \\"decrypt\\")\\n\\n        self._create_packages()\\n\\n\\n    #: Deprecated method, use `_load2disk` instead (Remove in 0.4.10)\\n    def loadToDisk(self, *args, **kwargs):\\n        return self._load2disk(*args, **kwargs)\\n\\n\\n    def _load2disk(self):\\n        \\"\\"\\"\\n        Loads container to disk if its stored remotely and overwrite url,\\n        or check existent on several places at disk\\n        \\"\\"\\"\\n        if self.pyfile.url.startswith(\\"http\\"):\\n            self.pyfile.name = re.findall(\\"([^\\\\/=]+)\\", self.pyfile.url)[-1]\\n            content = self.load(self.pyfile.url)\\n            self.pyfile.url = fs_join(self.pyload.config.get(\\"general\\", \\"download_folder\\"), self.pyfile.name)\\n            try:\\n                with open(self.pyfile.url, \\"wb\\") as f:\\n                    f.write(content)\\n\\n            except IOError, e:\\n                self.fail(e)\\n\\n        else:\\n            self.pyfile.name = os.path.basename(self.pyfile.url)\\n            if not exists(self.pyfile.url):\\n                if exists(fs_join(pypath, self.pyfile.url)):\\n                    self.pyfile.url = fs_join(pypath, self.pyfile.url)\\n                else:\\n                    self.fail(_(\\"File not exists\\"))\\n\\n\\n    #: Deprecated method, use `delete_tmp` instead (Remove in 0.4.10)\\n    def deleteTmp(self, *args, **kwargs):\\n        return self.delete_tmp(*args, **kwargs)\\n\\n\\n    def delete_tmp(self):\\n        if not self.pyfile.name.startswith(\\"tmp_\\"):\\n            return\\n\\n        try:\\n            os.remove(self.pyfile.url)\\n        except OSError, e:\\n            self.log_warning(_(\\"Error removing: %s\\") % self.pyfile.url, e)\\n            if self.pyload.debug:\\n                traceback.print_exc()\\n" }\n'
line: b'{ "repo_name": "pwong-mapr/private-hue", "ref": "refs/heads/HUE-1096-abe", "path": "desktop/core/ext-py/pysqlite/doc/includes/sqlite3/connect_db_1.py", "content": "from pysqlite2 import dbapi2 as sqlite3\\n\\ncon = sqlite3.connect(\\"mydb\\")\\n" }\n'
line: b'{ "repo_name": "NeCTAR-RC/nova", "ref": "refs/heads/nectar/mitaka", "path": "nova/scheduler/__init__.py", "content": "# Copyright (c) 2010 OpenStack Foundation\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\n\\"\\"\\"\\n:mod:`nova.scheduler` -- Scheduler Nodes\\n=====================================================\\n\\n.. automodule:: nova.scheduler\\n   :platform: Unix\\n   :synopsis: Module that picks a compute node to run a VM instance.\\n\\"\\"\\"\\n" }\n'
line: b'{ "repo_name": "CCI-MOC/nova", "ref": "refs/heads/k2k-liberty", "path": "nova/scheduler/__init__.py", "content": "# Copyright (c) 2010 OpenStack Foundation\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\n\\"\\"\\"\\n:mod:`nova.scheduler` -- Scheduler Nodes\\n=====================================================\\n\\n.. automodule:: nova.scheduler\\n   :platform: Unix\\n   :synopsis: Module that picks a compute node to run a VM instance.\\n\\"\\"\\"\\n" }\n'
line: b'{ "repo_name": "redhat-openstack/nova", "ref": "refs/heads/f22-patches", "path": "nova/scheduler/__init__.py", "content": "# Copyright (c) 2010 OpenStack Foundation\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\n\\"\\"\\"\\n:mod:`nova.scheduler` -- Scheduler Nodes\\n=====================================================\\n\\n.. automodule:: nova.scheduler\\n   :platform: Unix\\n   :synopsis: Module that picks a compute node to run a VM instance.\\n\\"\\"\\"\\n" }\n'
line: b'{ "repo_name": "citrix-openstack/build-trove", "ref": "refs/heads/ctx-nova-network-smoke-latest", "path": "trove/openstack/common/notifier/rabbit_notifier.py", "content": "# Copyright 2012 Red Hat, Inc.\\n# All Rights Reserved.\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\n\\nfrom trove.openstack.common.gettextutils import _\\nfrom trove.openstack.common import log as logging\\nfrom trove.openstack.common.notifier import rpc_notifier\\n\\nLOG = logging.getLogger(__name__)\\n\\n\\ndef notify(context, message):\\n    \\"\\"\\"Deprecated in Grizzly. Please use rpc_notifier instead.\\"\\"\\"\\n\\n    LOG.deprecated(_(\\"The rabbit_notifier is now deprecated.\\"\\n                     \\" Please use rpc_notifier instead.\\"))\\n    rpc_notifier.notify(context, message)\\n" }\n'
line: b'{ "repo_name": "Metaswitch/calico-nova", "ref": "refs/heads/calico-readme", "path": "nova/scheduler/__init__.py", "content": "# Copyright (c) 2010 OpenStack Foundation\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\n\\"\\"\\"\\n:mod:`nova.scheduler` -- Scheduler Nodes\\n=====================================================\\n\\n.. automodule:: nova.scheduler\\n   :platform: Unix\\n   :synopsis: Module that picks a compute node to run a VM instance.\\n\\"\\"\\"\\n" }\n'
line: b'{ "repo_name": "OCA/sale-reporting", "ref": "refs/heads/12.0", "path": "sale_comment_template/tests/test_sale_order_report.py", "content": "# Copyright 2017 Simone Rubino - Agile Business Group\\n# Copyright 2018 Tecnativa - Pedro M. Baeza\\n# License AGPL-3.0 or later (https://www.gnu.org/licenses/agpl).\\n\\nimport odoo.tests\\nfrom odoo.tests.common import TransactionCase\\n\\n\\n@odoo.tests.common.at_install(False)\\n@odoo.tests.common.post_install(True)\\nclass TestAccountInvoiceReport(TransactionCase):\\n    def setUp(self, *args, **kwargs):\\n        super(TestAccountInvoiceReport, self).setUp()\\n        self.base_comment_model = self.env[\'base.comment.template\']\\n        self.before_comment = self._create_comment(\'before_lines\')\\n        self.after_comment = self._create_comment(\'after_lines\')\\n        self.partner_id = self.env[\'res.partner\'].create({\\n            \'name\': \'Partner Test\'\\n      })\\n        self.sale_order = self.env.ref(\'sale.sale_order_7\')\\n        # Trigger qty_to_invoice again\\n        for order_line in self.sale_order.order_line:\\n            order_line.product_id.invoice_policy = \'order\'\\n        self.sale_order.action_confirm()\\n\\n        self.sale_order.update({\\n            \'comment_template1_id\': self.before_comment.id,\\n            \'comment_template2_id\': self.after_comment.id\\n      })\\n        self.sale_order._set_note1()\\n        self.sale_order._set_note2()\\n\\n    def _create_comment(self, position):\\n        return self.base_comment_model.create({\\n            \'name\': \'Comment \' + position,\\n            \'position\': position,\\n            \'text\': \'Text \' + position\\n      })\\n\\n    def test_comments_in_sale_order(self):\\n        res = self.env[\'ir.actions.report\']._get_report_from_name(\\n            \'sale.report_saleorder\'\\n        ).render_qweb_html(self.sale_order.ids)\\n        self.assertRegexpMatches(str(res[0]), self.before_comment.text)\\n        self.assertRegexpMatches(str(res[0]), self.after_comment.text)\\n\\n    def test_comments_in_generated_invoice(self):\\n        invoice_ids = self.sale_order.action_invoice_create()\\n        invoice = self.env[\'account.invoice\'].browse(invoice_ids)\\n        self.assertEqual(\\n            invoice.comment_template1_id,\\n            self.sale_order.comment_template1_id,\\n        )\\n        self.assertEqual(\\n            invoice.comment_template2_id,\\n            self.sale_order.comment_template2_id,\\n        )\\n\\n    def test_onchange_partner_id(self):\\n        self.partner_id.property_comment_template_id = self.after_comment.id\\n        vals = {\\n            \'partner_id\': self.partner_id.id,\\n      }\\n        new_sale = self.env[\'sale.order\'].new(vals)\\n        new_sale.onchange_partner_id_sale_comment()\\n        sale_dict = new_sale._convert_to_write(new_sale._cache)\\n        new_sale = self.env[\'sale.order\'].create(sale_dict)\\n        self.assertEqual(new_sale.comment_template2_id, self.after_comment)\\n        self.partner_id.property_comment_template_id = self.before_comment.id\\n        new_sale = self.env[\'sale.order\'].new(vals)\\n        new_sale.onchange_partner_id_sale_comment()\\n        sale_dict = new_sale._convert_to_write(new_sale._cache)\\n        new_sale = self.env[\'sale.order\'].create(sale_dict)\\n        self.assertEqual(new_sale.comment_template1_id, self.before_comment)\\n" }\n'
line: b'{ "repo_name": "alexryndin/ambari", "ref": "refs/heads/branch-adh-1.5", "path": "ambari-server/src/main/resources/stacks/ADH/1.4/services/KNOX/package/scripts/knox.py", "content": "\\"\\"\\"\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \\"AS IS\\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n\\"\\"\\"\\n\\nimport os\\nfrom resource_management.libraries.script.script import Script\\nfrom resource_management.libraries.resources.xml_config import XmlConfig\\nfrom resource_management.core.resources.service import ServiceConfig\\nfrom resource_management.libraries.functions.format import format\\nfrom resource_management.libraries.resources.template_config import TemplateConfig\\nfrom resource_management.core.resources.system import File, Execute, Directory\\nfrom resource_management.core.shell import as_user\\nfrom resource_management.core.source import InlineTemplate\\n\\nfrom ambari_commons import OSConst\\nfrom ambari_commons.os_family_impl import OsFamilyFuncImpl, OsFamilyImpl\\n\\nfrom resource_management.core.logger import Logger\\n\\n@OsFamilyFuncImpl(os_family=OSConst.WINSRV_FAMILY)\\ndef knox():\\n  import params\\n\\n  XmlConfig(\\"gateway-site.xml\\",\\n            conf_dir=params.knox_conf_dir,\\n            configurations=params.config[\'configurations\'][\'gateway-site\'],\\n            configuration_attributes=params.config[\'configuration_attributes\'][\'gateway-site\'],\\n            owner=params.knox_user\\n  )\\n\\n  # Manually overriding service logon user & password set by the installation package\\n  ServiceConfig(params.knox_gateway_win_service_name,\\n                action=\\"change_user\\",\\n                username = params.knox_user,\\n                password = Script.get_password(params.knox_user))\\n\\n  File(os.path.join(params.knox_conf_dir, \\"gateway-log4j.properties\\"),\\n       owner=params.knox_user,\\n       content=params.gateway_log4j\\n  )\\n\\n  File(os.path.join(params.knox_conf_dir, \\"topologies\\", \\"default.xml\\"),\\n       group=params.knox_group,\\n       owner=params.knox_user,\\n       content=InlineTemplate(params.topology_template)\\n  )\\n\\n  if params.security_enabled:\\n    TemplateConfig( os.path.join(params.knox_conf_dir, \\"krb5JAASLogin.conf\\"),\\n        owner = params.knox_user,\\n        template_tag = None\\n    )\\n\\n  if not os.path.isfile(params.knox_master_secret_path):\\n    cmd = format(\'cmd /C {knox_client_bin} create-master --master {knox_master_secret!p}\')\\n    Execute(cmd)\\n    cmd = format(\'cmd /C {knox_client_bin} create-cert --hostname {knox_host_name_in_cluster}\')\\n    Execute(cmd)\\n\\n@OsFamilyFuncImpl(os_family=OsFamilyImpl.DEFAULT)\\ndef knox():\\n    import params\\n\\n    directories = [params.knox_data_dir, params.knox_logs_dir, params.knox_pid_dir, params.knox_conf_dir, os.path.join(params.knox_conf_dir, \\"topologies\\")]\\n    for directory in directories:\\n      Directory(directory,\\n                owner = params.knox_user,\\n                group = params.knox_group,\\n                create_parents = True,\\n                cd_access = \\"a\\",\\n                mode = 0755,\\n      )\\n\\n    XmlConfig(\\"gateway-site.xml\\",\\n              conf_dir=params.knox_conf_dir,\\n              configurations=params.config[\'configurations\'][\'gateway-site\'],\\n              configuration_attributes=params.config[\'configuration_attributes\'][\'gateway-site\'],\\n              owner=params.knox_user,\\n              group=params.knox_group,\\n    )\\n\\n    File(format(\\"{params.knox_conf_dir}/gateway-log4j.properties\\"),\\n         mode=0644,\\n         group=params.knox_group,\\n         owner=params.knox_user,\\n         content=params.gateway_log4j\\n    )\\n\\n    File(format(\\"{params.knox_conf_dir}/topologies/default.xml\\"),\\n         group=params.knox_group,\\n         owner=params.knox_user,\\n         content=InlineTemplate(params.topology_template)\\n    )\\n    if params.security_enabled:\\n      TemplateConfig( format(\\"{knox_conf_dir}/krb5JAASLogin.conf\\"),\\n                      owner = params.knox_user,\\n                      template_tag = None\\n      )\\n\\n    dirs_to_chown = tuple(directories)\\n    cmd = (\'chown\',\'-R\',format(\'{knox_user}:{knox_group}\')) + dirs_to_chown\\n    Execute(cmd,\\n            sudo = True,\\n    )\\n\\n    cmd = format(\'{knox_client_bin} create-master --master {knox_master_secret!p}\')\\n    master_secret_exist = as_user(format(\'test -f {knox_master_secret_path}\'), params.knox_user)\\n\\n    Execute(cmd,\\n            user=params.knox_user,\\n            environment={\'JAVA_HOME\': params.java_home}\\n            not_if=master_secret_exist,\\n    )\\n\\n    cmd = format(\'{knox_client_bin} create-cert --hostname {knox_host_name_in_cluster}\')\\n    cert_store_exist = as_user(format(\'test -f {knox_cert_store_path}\'), params.knox_user)\\n\\n    Execute(cmd,\\n            user=params.knox_user,\\n            environment={\'JAVA_HOME\': params.java_home}\\n            not_if=cert_store_exist,\\n    )\\n\\n\\n@OsFamilyFuncImpl(os_family=OSConst.WINSRV_FAMILY)\\ndef update_knox_folder_permissions():\\n  import params\\n  Directory(params.knox_logs_dir,\\n            owner = params.knox_user,\\n            group = params.knox_group\\n            )\\n\\n\\n@OsFamilyFuncImpl(os_family=OsFamilyImpl.DEFAULT)\\ndef update_knox_logfolder_permissions():\\n  \\"\\"\\"\\n   Fix for the bug with rpm/deb packages. During installation of the package, they re-apply permissions to the\\n   folders below; such behaviour will affect installations with non-standard user name/group and will put\\n   cluster in non-working state\\n  \\"\\"\\"\\n  import params\\n  knox_dirs = [params.knox_logs_dir]\\n\\n  Directory(params.knox_logs_dir,\\n            owner = params.knox_user,\\n            group = params.knox_group,\\n            create_parents = True,\\n            cd_access = \\"a\\",\\n            mode = 0755,\\n            )\\n\\n  for d in knox_dirs:\\n    if len(d) > 1:  # If path is empty or a single slash, may corrupt filesystem permissions\\n      Execute((\'chown\', \'-R\', format(\\"{knox_user}:{knox_group}\\"), d),\\n              sudo=True\\n              )\\n    else:\\n      Logger.warning(\\"Permissions for the Knox folder \\\\\\"%s\\\\\\" was not updated due to empty path passed\\" % d)\\n" }\n'
line: b'{ "repo_name": "waqasbhatti/astroph-coffee", "ref": "refs/heads/github", "path": "pysqlite/doc/includes/sqlite3/connect_db_1.py", "content": "from pysqlite2 import dbapi2 as sqlite3\\n\\ncon = sqlite3.connect(\\"mydb\\")\\n" }\n'
line: b'{ "repo_name": "Mausy5043/upsdiagd", "ref": "refs/heads/v2", "path": "daemons/ups32d.py", "content": "#!/usr/bin/env python3\\n\\n# Communicates with the UPS.\\n\\nimport configparser\\nimport os\\nimport sys\\nimport syslog\\nimport subprocess\\nimport time\\nimport traceback\\n\\nfrom mausy5043libs.libdaemon3 import Daemon\\nimport mausy5043funcs.fileops3 as mf\\n\\n# constants\\nDEBUG       = False\\nIS_JOURNALD = os.path.isfile(\'/bin/journalctl\')\\nMYID        = \\"\\".join(list(filter(str.isdigit, os.path.realpath(__file__).split(\'/\')[-1])))\\nMYAPP       = os.path.realpath(__file__).split(\'/\')[-3]\\nMYAPPDIR    = \\"/\\".join(list(filter(str, os.path.realpath(__file__).split(\'/\')[:-2])))\\nNODE        = os.uname()[1]\\n\\n# initialise logging\\nsyslog.openlog(ident=MYAPP, facility=syslog.LOG_LOCAL0)\\n\\nclass MyDaemon(Daemon):\\n  \\"\\"\\"Definition of daemon.\\"\\"\\"\\n  @staticmethod\\n  def run():\\n    iniconf         = configparser.ConfigParser()\\n    inisection      = MYID\\n    s               = iniconf.read(\'/\' + MYAPPDIR + \'/config.ini\')\\n    mf.syslog_trace(\\"Config file   : {0}\\".format(s), False, DEBUG)\\n    mf.syslog_trace(\\"Options       : {0}\\".format(iniconf.items(inisection)), False, DEBUG)\\n    reporttime      = iniconf.getint(inisection, \\"reporttime\\")\\n    cycles          = iniconf.getint(inisection, \\"cycles\\")\\n    samplespercycle = iniconf.getint(inisection, \\"samplespercycle\\")\\n    flock           = iniconf.get(inisection, \\"lockfile\\")\\n    fdata           = iniconf.get(inisection, \\"resultfile\\")\\n\\n    samples         = samplespercycle * cycles           # total number of samples averaged\\n    sampletime      = reporttime/samplespercycle         # time [s] between samples\\n\\n    data            = []                                 # array for holding sampledata\\n    # raw             = [0] * 8                            # array for holding previous\\n\\n    while True:\\n      try:\\n        starttime     = time.time()\\n\\n        result        = do_work()\\n        result        = result.split(\',\')\\n        mf.syslog_trace(\\"Result   : {0}\\".format(result), False, DEBUG)\\n        # data.append(list(map(int, result)))\\n        data.append([float(d) for d in result])\\n        if (len(data) > samples):\\n          data.pop(0)\\n        mf.syslog_trace(\\"Data     : {0}\\".format(data),   False, DEBUG)\\n\\n        # report sample average\\n        if (starttime % reporttime < sampletime):\\n          # somma       = list(map(sum, zip(*data)))\\n          somma = [sum(d) for d in zip(*data)]\\n          # not all entries should be float\\n          # [\'234.000\', \'13.700\', \'100.000\', \'20.000\', \'1447.000\']\\n          averages = [float(format(d / len(data), \'.3f\')) for d in somma]\\n          mf.syslog_trace(\\"Averages : {0}\\".format(averages),  False, DEBUG)\\n          do_report(averages, flock, fdata)\\n\\n        waittime    = sampletime - (time.time() - starttime) - (starttime % sampletime)\\n        if (waittime > 0):\\n          mf.syslog_trace(\\"Waiting  : {0}s\\".format(waittime), False, DEBUG)\\n          mf.syslog_trace(\\"................................\\", False, DEBUG)\\n          time.sleep(waittime)\\n        else:\\n          mf.syslog_trace(\\"Behind   : {0}s\\".format(waittime), False, DEBUG)\\n          mf.syslog_trace(\\"................................\\", False, DEBUG)\\n      except Exception:\\n        mf.syslog_trace(\\"Unexpected error in run()\\", syslog.LOG_CRIT, DEBUG)\\n        mf.syslog_trace(traceback.format_exc(), syslog.LOG_CRIT, DEBUG)\\n        raise\\n\\ndef do_work():\\n  # 5 datapoints gathered here\\n  try:\\n    upsc = str(subprocess.check_output([\'upsc\', \'ups@localhost\']), \'utf-8\').splitlines()\\n  except subprocess.CalledProcessError:\\n    # mf.syslog_trace(\\"Unexpected error in do_work()\\", syslog.LOG_CRIT, DEBUG)\\n    # mf.syslog_trace(traceback.format_exc(), syslog.LOG_CRIT, DEBUG)\\n    syslog.syslog(syslog.LOG_ALERT, \\"Waiting 10s ...\\")\\n\\n    time.sleep(10)    # wait to let the driver crash properly\\n    # mf.syslog_trace(\\"*** RESTARTING nut-driver.service ***\\", syslog.LOG_ALERT, DEBUG)\\n    # r = str(subprocess.check_output([\'sudo\', \'systemctl\', \'restart\',  \'nut-driver.service\']), \'utf-8\').splitlines()\\n    mf.syslog_trace(\\"*** RESTARTING nut-server.service ***\\", syslog.LOG_ALERT, DEBUG)\\n    r = str(subprocess.check_output([\'sudo\', \'systemctl\', \'restart\',  \'nut-server.service\']), \'utf-8\').splitlines()\\n    mf.syslog_trace(\\"Returned : {0}\\".format(r), False, DEBUG)\\n\\n    time.sleep(15)\\n    mf.syslog_trace(\\"!!! Retrying communication with UPS !!!\\", syslog.LOG_ALERT, DEBUG)\\n    upsc = str(subprocess.check_output([\'upsc\', \'ups@localhost\']), \'utf-8\').splitlines()\\n    pass\\n\\n  # ups0 and ups1 are disabled, because the current UPS (EATON) does not supply\\n  # usable data for these graphs\\n  ups0 = -1.0\\n  ups1 = -1.0\\n  for element in range(0, len(upsc) - 1):\\n    var = upsc[element].split(\': \')\\n    # if (var[0] == \'input.voltage\'):\\n    if (var[0] == \'output.voltage\'):\\n      ups0 = float(var[1])\\n    if (var[0] == \'battery.voltage\'):\\n      ups1 = float(var[1])\\n    if (var[0] == \'battery.charge\'):\\n      ups2 = float(var[1])\\n    if (var[0] == \'ups.load\'):\\n      ups3 = float(var[1])*10\\n    if (var[0] == \'battery.runtime\'):\\n      ups4 = float(var[1])\\n\\n  return \'{0} {1} {2} {3} ,{4}\'.format(ups0, ups1, ups2, ups3, ups4)\\n\\ndef do_report(result, flock, fdata):\\n  # Get the time and date in human-readable form and UN*X-epoch...\\n  outdate  = time.strftime(\'%Y-%m-%dT%H:%M:%S\')\\n  outepoch = int(time.strftime(\'%s\'))\\n  # round to current minute to ease database JOINs\\n  # outEpoch = outEpoch - (outEpoch % 60)\\n  result   = \', \'.join(map(str, result))\\n  mf.lock(flock)\\n  with open(fdata, \'a\') as f:\\n    f.write(\'{0} {1} {2}\\\\n\'.format(outdate, outepoch, result))\\n  mf.unlock(flock)\\n\\n\\nif __name__ == \\"__main__\\":\\n  daemon = MyDaemon(\'/tmp/\' + MYAPP + \'/\' + MYID + \'.pid\')\\n  if len(sys.argv) == 2:\\n    if \'start\' == sys.argv[1]:\\n      daemon.start()\\n    elif \'stop\' == sys.argv[1]:\\n      daemon.stop()\\n    elif \'restart\' == sys.argv[1]:\\n      daemon.restart()\\n    elif \'foreground\' == sys.argv[1]:\\n      # assist with debugging.\\n      print(\\"Debug-mode started. Use <Ctrl>+C to stop.\\")\\n      DEBUG = True\\n      mf.syslog_trace(\\"Daemon logging is ON\\", syslog.LOG_DEBUG, DEBUG)\\n      daemon.run()\\n    else:\\n      print(\\"Unknown command\\")\\n      sys.exit(2)\\n    sys.exit(0)\\n  else:\\n    print(\\"usage: {0!s} start|stop|restart|foreground\\".format(sys.argv[0]))\\n    sys.exit(2)\\n" }\n'
line: b'{ "repo_name": "cloudbase/nova-virtualbox", "ref": "refs/heads/virtualbox_driver", "path": "nova/scheduler/__init__.py", "content": "# Copyright (c) 2010 OpenStack Foundation\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\n\\"\\"\\"\\n:mod:`nova.scheduler` -- Scheduler Nodes\\n=====================================================\\n\\n.. automodule:: nova.scheduler\\n   :platform: Unix\\n   :synopsis: Module that picks a compute node to run a VM instance.\\n\\"\\"\\"\\n" }\n'
line: b'{ "repo_name": "lumig242/Hue-Integration-with-CDAP", "ref": "refs/heads/pull3", "path": "desktop/core/ext-py/pysqlite/doc/includes/sqlite3/connect_db_1.py", "content": "from pysqlite2 import dbapi2 as sqlite3\\n\\ncon = sqlite3.connect(\\"mydb\\")\\n" }\n'
line: b'{ "repo_name": "mapr/hue", "ref": "refs/heads/hue-3.9.0-mapr", "path": "desktop/core/ext-py/pysqlite/doc/includes/sqlite3/connect_db_1.py", "content": "from pysqlite2 import dbapi2 as sqlite3\\n\\ncon = sqlite3.connect(\\"mydb\\")\\n" }\n'
line: b'{ "repo_name": "virtualopensystems/nova", "ref": "refs/heads/bp/vif-vhostuser", "path": "nova/scheduler/__init__.py", "content": "# Copyright (c) 2010 OpenStack Foundation\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\n\\"\\"\\"\\n:mod:`nova.scheduler` -- Scheduler Nodes\\n=====================================================\\n\\n.. automodule:: nova.scheduler\\n   :platform: Unix\\n   :synopsis: Module that picks a compute node to run a VM instance.\\n\\"\\"\\"\\n" }\n'
line: b'{ "repo_name": "tianweizhang/nova", "ref": "refs/heads/v0", "path": "nova/scheduler/__init__.py", "content": "# Copyright (c) 2010 OpenStack Foundation\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\n\\"\\"\\"\\n:mod:`nova.scheduler` -- Scheduler Nodes\\n=====================================================\\n\\n.. automodule:: nova.scheduler\\n   :platform: Unix\\n   :synopsis: Module that picks a compute node to run a VM instance.\\n\\"\\"\\"\\n" }\n'
line: b'{ "repo_name": "apple/llvm-project", "ref": "refs/heads/llvm.org/main", "path": "lldb/test/API/functionalities/thread/concurrent_events/TestConcurrentWatchpointWithDelayWatchpointThreads.py", "content": "\\nimport unittest2\\n\\nfrom lldbsuite.test.decorators import *\\nfrom lldbsuite.test.concurrent_base import ConcurrentEventsBase\\nfrom lldbsuite.test.lldbtest import TestBase\\n\\n\\n@skipIfWindows\\nclass ConcurrentWatchpointWithDelayWatchpointThreads(ConcurrentEventsBase):\\n\\n    mydir = ConcurrentEventsBase.compute_mydir(__file__)\\n\\n    # Atomic sequences are not supported yet for MIPS in LLDB.\\n    @skipIf(triple=\'^mips\')\\n    @add_test_categories([\\"watchpoint\\"])\\n    def test(self):\\n        \\"\\"\\"Test two threads that trigger a watchpoint where one thread has a 1 second delay. \\"\\"\\"\\n        self.build(dictionary=self.getBuildFlags())\\n        self.do_thread_actions(num_watchpoint_threads=1,\\n                               num_delay_watchpoint_threads=1)\\n" }\n'
line: b'{ "repo_name": "oscaro/django", "ref": "refs/heads/oscaro-backports-1.7.10", "path": "django/http/cookie.py", "content": "from __future__ import unicode_literals\\n\\nfrom django.utils.encoding import force_str\\nfrom django.utils import six\\nfrom django.utils.six.moves import http_cookies\\n\\n\\n# Some versions of Python 2.7 and later won\'t need this encoding bug fix:\\n_cookie_encodes_correctly = http_cookies.SimpleCookie().value_encode(\';\') == (\';\', \'\\"\\\\\\\\073\\"\')\\n# See ticket #13007, http://bugs.python.org/issue2193 and http://trac.edgewall.org/ticket/2256\\n_tc = http_cookies.SimpleCookie()\\ntry:\\n    _tc.load(str(\'foo:bar=1\'))\\n    _cookie_allows_colon_in_names = True\\nexcept http_cookies.CookieError:\\n    _cookie_allows_colon_in_names = False\\n\\nif _cookie_encodes_correctly and _cookie_allows_colon_in_names:\\n    SimpleCookie = http_cookies.SimpleCookie\\nelse:\\n    Morsel = http_cookies.Morsel\\n\\n    class SimpleCookie(http_cookies.SimpleCookie):\\n        if not _cookie_encodes_correctly:\\n            def value_encode(self, val):\\n                # Some browsers do not support quoted-string from RFC 2109,\\n                # including some versions of Safari and Internet Explorer.\\n                # These browsers split on \';\', and some versions of Safari\\n                # are known to split on \', \'. Therefore, we encode \';\' and \',\'\\n\\n                # SimpleCookie already does the hard work of encoding and decoding.\\n                # It uses octal sequences like \'\\\\\\\\012\' for newline etc.\\n                # and non-ASCII chars. We just make use of this mechanism, to\\n                # avoid introducing two encoding schemes which would be confusing\\n                # and especially awkward for javascript.\\n\\n                # NB, contrary to Python docs, value_encode returns a tuple containing\\n                # (real val, encoded_val)\\n                val, encoded = super(SimpleCookie, self).value_encode(val)\\n\\n                encoded = encoded.replace(\\";\\", \\"\\\\\\\\073\\").replace(\\",\\", \\"\\\\\\\\054\\")\\n                # If encoded now contains any quoted chars, we need double quotes\\n                # around the whole string.\\n                if \\"\\\\\\\\\\" in encoded and not encoded.startswith(\'\\"\'):\\n                    encoded = \'\\"\' + encoded + \'\\"\'\\n\\n                return val, encoded\\n\\n        if not _cookie_allows_colon_in_names:\\n            def load(self, rawdata):\\n                self.bad_cookies = set()\\n                if six.PY2 and isinstance(rawdata, six.text_type):\\n                    rawdata = force_str(rawdata)\\n                super(SimpleCookie, self).load(rawdata)\\n                for key in self.bad_cookies:\\n                    del self[key]\\n\\n            # override private __set() method:\\n            # (needed for using our Morsel, and for laxness with CookieError\\n            def _BaseCookie__set(self, key, real_value, coded_value):\\n                key = force_str(key)\\n                try:\\n                    M = self.get(key, Morsel())\\n                    M.set(key, real_value, coded_value)\\n                    dict.__setitem__(self, key, M)\\n                except http_cookies.CookieError:\\n                    if not hasattr(self, \'bad_cookies\'):\\n                        self.bad_cookies = set()\\n                    self.bad_cookies.add(key)\\n                    dict.__setitem__(self, key, http_cookies.Morsel())\\n\\n\\ndef parse_cookie(cookie):\\n    if cookie == \'\':\\n        return {}\\n    if not isinstance(cookie, http_cookies.BaseCookie):\\n        try:\\n            c = SimpleCookie()\\n            c.load(cookie)\\n        except http_cookies.CookieError:\\n            # Invalid cookie\\n            return {}\\n    else:\\n        c = cookie\\n    cookiedict = {}\\n    for key in c.keys():\\n        cookiedict[key] = c.get(key).value\\n    return cookiedict\\n" }\n'
line: b'{ "repo_name": "OCA/stock-logistics-workflow", "ref": "refs/heads/12.0", "path": "stock_change_price_at_date/wizards/__init__.py", "content": "from . import stock_change_standard_price\\n" }\n'
line: b'{ "repo_name": "leeon/annotated-django", "ref": "refs/heads/note", "path": "django/http/cookie.py", "content": "from __future__ import unicode_literals\\n\\nfrom django.utils.encoding import force_str\\nfrom django.utils import six\\nfrom django.utils.six.moves import http_cookies\\n\\n\\n# Some versions of Python 2.7 and later won\'t need this encoding bug fix:\\n_cookie_encodes_correctly = http_cookies.SimpleCookie().value_encode(\';\') == (\';\', \'\\"\\\\\\\\073\\"\')\\n# See ticket #13007, http://bugs.python.org/issue2193 and http://trac.edgewall.org/ticket/2256\\n_tc = http_cookies.SimpleCookie()\\ntry:\\n    _tc.load(str(\'foo:bar=1\'))\\n    _cookie_allows_colon_in_names = True\\nexcept http_cookies.CookieError:\\n    _cookie_allows_colon_in_names = False\\n\\nif _cookie_encodes_correctly and _cookie_allows_colon_in_names:\\n    SimpleCookie = http_cookies.SimpleCookie\\nelse:\\n    Morsel = http_cookies.Morsel\\n\\n    class SimpleCookie(http_cookies.SimpleCookie):\\n        if not _cookie_encodes_correctly:\\n            def value_encode(self, val):\\n                # Some browsers do not support quoted-string from RFC 2109,\\n                # including some versions of Safari and Internet Explorer.\\n                # These browsers split on \';\', and some versions of Safari\\n                # are known to split on \', \'. Therefore, we encode \';\' and \',\'\\n\\n                # SimpleCookie already does the hard work of encoding and decoding.\\n                # It uses octal sequences like \'\\\\\\\\012\' for newline etc.\\n                # and non-ASCII chars. We just make use of this mechanism, to\\n                # avoid introducing two encoding schemes which would be confusing\\n                # and especially awkward for javascript.\\n\\n                # NB, contrary to Python docs, value_encode returns a tuple containing\\n                # (real val, encoded_val)\\n                val, encoded = super(SimpleCookie, self).value_encode(val)\\n\\n                encoded = encoded.replace(\\";\\", \\"\\\\\\\\073\\").replace(\\",\\", \\"\\\\\\\\054\\")\\n                # If encoded now contains any quoted chars, we need double quotes\\n                # around the whole string.\\n                if \\"\\\\\\\\\\" in encoded and not encoded.startswith(\'\\"\'):\\n                    encoded = \'\\"\' + encoded + \'\\"\'\\n\\n                return val, encoded\\n\\n        if not _cookie_allows_colon_in_names:\\n            def load(self, rawdata):\\n                self.bad_cookies = set()\\n                if six.PY2 and isinstance(rawdata, six.text_type):\\n                    rawdata = force_str(rawdata)\\n                super(SimpleCookie, self).load(rawdata)\\n                for key in self.bad_cookies:\\n                    del self[key]\\n\\n            # override private __set() method:\\n            # (needed for using our Morsel, and for laxness with CookieError\\n            def _BaseCookie__set(self, key, real_value, coded_value):\\n                key = force_str(key)\\n                try:\\n                    M = self.get(key, Morsel())\\n                    M.set(key, real_value, coded_value)\\n                    dict.__setitem__(self, key, M)\\n                except http_cookies.CookieError:\\n                    if not hasattr(self, \'bad_cookies\'):\\n                        self.bad_cookies = set()\\n                    self.bad_cookies.add(key)\\n                    dict.__setitem__(self, key, http_cookies.Morsel())\\n\\n\\ndef parse_cookie(cookie):\\n    if cookie == \'\':\\n        return {}\\n    if not isinstance(cookie, http_cookies.BaseCookie):\\n        try:\\n            c = SimpleCookie()\\n            c.load(cookie)\\n        except http_cookies.CookieError:\\n            # Invalid cookie\\n            return {}\\n    else:\\n        c = cookie\\n    cookiedict = {}\\n    for key in c.keys():\\n        cookiedict[key] = c.get(key).value\\n    return cookiedict\\n" }\n'
line: b'{ "repo_name": "tturpen/django-csaesrapp", "ref": "refs/heads/version01", "path": "apps/elicitation/factories.py", "content": "from apps.common.factories import ModelFactory\\nfrom apps.elicitation.handlers import ElicitationModelHandler\\nfrom apps.audio.handlers import RecordingHandler, WavHandler\\nimport os\\nimport sys\\n\\n\\nclass ElicitationModelFactory(ModelFactory):\\n    def __init__(self):\\n        ModelFactory.__init__(self)\\n        self.mh = ElicitationModelHandler()\\n        self.rh = RecordingHandler()\\n        self.wh = WavHandler()\\n                \\n    def create_elicitation_hit_model(self,\\n                                     hit_id,\\n                                     hit_type_id,\\n                                     prompt_ids,\\n                                     prompt_source_name,\\n                                     template_name,\\n                                     redundancy):        \\n        if type(prompt_ids) != list:\\n            raise IOError\\n        search = {\\"hit_id\\":hit_id}\\n        document ={\\"hit_id\\":hit_id,\\n                     \\"hit_type_id\\": hit_type_id,\\n                     \\"prompts\\" : prompt_ids,\\n                     \\"prompt_source_name\\": prompt_source_name,\\n                     \\"template_name\\": template_name,\\n                     \\"redundancy\\": redundancy}\\n        return self.create_model(\\"hits\\",search,document)\\n    \\n    def create_word_prompt_model(self,source, words, normalized_words,line_number,prompt_id,word_count):\\n        \\"\\"\\"A -1 endtime means to the end of the clip.\\"\\"\\"\\n        search = {\\"source\\" : source,\\n                    \\"line_number\\" : line_number,\\n                    \\"prompt_id\\" : prompt_id,\\n                    \\"word_count\\": word_count}\\n        document = {\\"source\\" : source,\\n                    \\"line_number\\" : line_number,\\n                    \\"prompt_id\\" : prompt_id,\\n                    \\"words\\" : words,\\n                    \\"normalized_words\\": normalized_words,\\n                    \\"word_count\\": word_count}\\n        art_id = self.create_model(\\"prompts\\", search, document,update=False)\\n        return art_id\\n    \\n    def create_prompt_source_model(self,prompt_file_uri, disk_space, prompt_count):\\n        \\"\\"\\"Create the prompt source model give the location on disk,\\n            size on disk\\n            and number of prompts\\"\\"\\"\\n        search = {\\"uri\\" : prompt_file_uri,\\n                    \\"disk_space\\" : disk_space,\\n                    \\"prompt_count\\": prompt_count}    \\n        document = {\\"uri\\" : prompt_file_uri,\\n                    \\"disk_space\\" : disk_space,\\n                    \\"prompt_count\\": prompt_count} \\n        model= self.create_model(\\"prompt_sources\\", search, document)\\n        return model\\n    \\n    def create_recording_source_model(self,prompt,recording_url,worker=None,prompt_words=None):\\n        \\"\\"\\"Use the recording handler to download the recording\\n            and create the artifact\\"\\"\\"\\n        recording_uri = self.rh.download_vocaroo_recording(recording_url,\\n                                                           worker_id=worker.worker_id,\\n                                                           prompt_words=prompt_words)\\n        if not recording_uri:\\n            print(\\"Failed to retrieve url(%s)\\"%recording_url)\\n            return False\\n        disk_space = os.stat(recording_uri).st_size\\n        length_seconds = self.wh.get_audio_length(recording_uri)\\n        encoding = \\".wav\\"\\n        sample_rate = -1\\n        \\n        search = {\\"recording_url\\" : recording_url}\\n        document = {\\"recording_url\\": recording_url,\\n                    \\"prompt\\": prompt,\\n                    \\"disk_space\\" : disk_space,\\n                    \\"uri\\" : recording_uri,\\n                    \\"worker_id\\": worker.worker_id,\\n                    \\"length_seconds\\": length_seconds,\\n                    \\"encoding\\" : encoding,\\n                    \\"sample_rate\\" : sample_rate,\\n                    \\"filename\\": os.path.basename(recording_uri)} \\n        return self.create_model(\\"recording_sources\\", search, document)\\n    \\n    " }\n'
line: b'{ "repo_name": "redhat-openstack/trove", "ref": "refs/heads/mitaka-patches", "path": "trove/instance/tasks.py", "content": "# Copyright 2012 OpenStack Foundation\\n# All Rights Reserved.\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\"\\"\\"\\nCommon instance status code used across Trove API.\\n\\"\\"\\"\\n\\n\\nclass InstanceTask(object):\\n    \\"\\"\\"\\n    Stores the different kind of tasks being performed by an instance.\\n    \\"\\"\\"\\n    # TODO(tim.simpson): Figure out someway to migrate this to the TaskManager\\n    #                    once that revs up.\\n    _lookup = {}\\n\\n    def __init__(self, code, action, db_text, is_error=False):\\n        self._code = int(code)\\n        self._action = action\\n        self._db_text = db_text\\n        self._is_error = is_error\\n        InstanceTask._lookup[self._code] = self\\n\\n    @property\\n    def action(self):\\n        return self._action\\n\\n    @property\\n    def code(self):\\n        return self._code\\n\\n    @property\\n    def db_text(self):\\n        return self._db_text\\n\\n    @property\\n    def is_error(self):\\n        return self._is_error\\n\\n    def __eq__(self, other):\\n        if not isinstance(other, InstanceTask):\\n            return False\\n        return self._db_text == other._db_text\\n\\n    @classmethod\\n    def from_code(cls, code):\\n        if code not in cls._lookup:\\n            return None\\n        return cls._lookup[code]\\n\\n    def __str__(self):\\n        return \\"(%d %s %s)\\" % (self._code, self._action, self._db_text)\\n\\n    def __repr__(self):\\n        return \\"InstanceTask.%s (%s)\\" % (self._action, self._db_text)\\n\\n\\nclass InstanceTasks(object):\\n    NONE = InstanceTask(0x01, \'NONE\', \'No tasks for the instance.\')\\n    DELETING = InstanceTask(0x02, \'DELETING\', \'Deleting the instance.\')\\n    REBOOTING = InstanceTask(0x03, \'REBOOTING\', \'Rebooting the instance.\')\\n    RESIZING = InstanceTask(0x04, \'RESIZING\', \'Resizing the instance.\')\\n    BUILDING = InstanceTask(0x05, \'BUILDING\', \'The instance is building.\')\\n    MIGRATING = InstanceTask(0x06, \'MIGRATING\', \'Migrating the instance.\')\\n    RESTART_REQUIRED = InstanceTask(0x07, \'RESTART_REQUIRED\',\\n                                    \'Instance requires a restart.\')\\n    PROMOTING = InstanceTask(0x08, \'PROMOTING\',\\n                             \'Promoting the instance to replica source.\')\\n    EJECTING = InstanceTask(0x09, \'EJECTING\',\\n                            \'Ejecting the replica source.\')\\n    LOGGING = InstanceTask(0x0a, \'LOGGING\', \'Transferring guest logs.\')\\n\\n    BUILDING_ERROR_DNS = InstanceTask(0x50, \'BUILDING\', \'Build error: DNS.\',\\n                                      is_error=True)\\n    BUILDING_ERROR_SERVER = InstanceTask(0x51, \'BUILDING\',\\n                                         \'Build error: Server.\',\\n                                         is_error=True)\\n    BUILDING_ERROR_VOLUME = InstanceTask(0x52, \'BUILDING\',\\n                                         \'Build error: Volume.\',\\n                                         is_error=True)\\n    BUILDING_ERROR_TIMEOUT_GA = InstanceTask(0x54, \'ERROR\',\\n                                             \'Build error: \'\\n                                             \'guestagent timeout.\',\\n                                             is_error=True)\\n    BUILDING_ERROR_SEC_GROUP = InstanceTask(0x53, \'BUILDING\',\\n                                            \'Build error: Secgroup \'\\n                                            \'or rule.\',\\n                                            is_error=True)\\n    BUILDING_ERROR_REPLICA = InstanceTask(0x54, \'BUILDING\',\\n                                          \'Build error: Replica.\',\\n                                          is_error=True)\\n    PROMOTION_ERROR = InstanceTask(0x55, \'PROMOTING\',\\n                                         \'Replica Promotion Error.\',\\n                                         is_error=True)\\n    EJECTION_ERROR = InstanceTask(0x56, \'EJECTING\',\\n                                        \'Replica Source Ejection Error.\',\\n                                        is_error=True)\\n    GROWING_ERROR = InstanceTask(0x57, \'GROWING\',\\n                                       \'Growing Cluster Error.\',\\n                                       is_error=True)\\n    SHRINKING_ERROR = InstanceTask(0x58, \'SHRINKING\',\\n                                         \'Shrinking Cluster Error.\',\\n                                         is_error=True)\\n\\n# Dissuade further additions at run-time.\\nInstanceTask.__init__ = None\\n" }\n'
line: b'{ "repo_name": "matte1/mavlink", "ref": "refs/heads/matt_dev", "path": "pymavlink/tools/mavparms.py", "content": "#!/usr/bin/env python\\n\\n\'\'\'\\nextract mavlink parameter values\\n\'\'\'\\n\\nimport sys, time, os\\n\\nfrom optparse import OptionParser\\nparser = OptionParser(\\"mavparms.py [options]\\")\\nparser.add_option(\\"-c\\", \\"--changes\\", dest=\\"changesOnly\\", default=False, action=\\"store_true\\", help=\\"Show only changes to parameters.\\")\\n\\n(opts, args) = parser.parse_args()\\n\\nfrom pymavlink import mavutil\\n\\nif len(args) < 1:\\n    print(\\"Usage: mavparms.py [options] <LOGFILE...>\\")\\n    sys.exit(1)\\n\\nparms = {}\\n\\ndef mavparms(logfile):\\n    \'\'\'extract mavlink parameters\'\'\'\\n    mlog = mavutil.mavlink_connection(filename)\\n\\n    while True:\\n        try:\\n            m = mlog.recv_match(type=[\'PARAM_VALUE\', \'PARM\'])\\n            if m is None:\\n                return\\n        except Exception:\\n            return\\n        if m.get_type() == \'PARAM_VALUE\':\\n            pname = str(m.param_id).strip()\\n            value = m.param_value\\n        else:\\n            pname = m.Name\\n            value = m.Value\\n        if len(pname) > 0:\\n            if opts.changesOnly is True and pname in parms and parms[pname] != value:\\n                print(\\"%s %-15s %.6f -> %.6f\\" % (time.asctime(time.localtime(m._timestamp)), pname, parms[pname], value))\\n            \\n            parms[pname] = value\\n\\ntotal = 0.0\\nfor filename in args:\\n    mavparms(filename)\\n\\nif (opts.changesOnly is False):\\n    keys = parms.keys()\\n    keys.sort()\\n    for p in keys:\\n        print(\\"%-15s %.6f\\" % (p, parms[p]))\\n" }\n'
line: b'{ "repo_name": "idea4bsd/idea4bsd", "ref": "refs/heads/idea4bsd-master", "path": "python/testData/refactoring/extractsuperclass/withImport.before.py", "content": "import os\\n\\nclass A(object):\\n  def foo(self):\\n    os.stat_result.n_fields()\\n" }\n'
line: b'{ "repo_name": "semprebon/mapnik", "ref": "refs/heads/svg", "path": "scons/scons-local-1.2.0/SCons/Options/PathOption.py", "content": "#\\n# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008 The SCons Foundation\\n#\\n# Permission is hereby granted, free of charge, to any person obtaining\\n# a copy of this software and associated documentation files (the\\n# \\"Software\\"), to deal in the Software without restriction, including\\n# without limitation the rights to use, copy, modify, merge, publish,\\n# distribute, sublicense, and/or sell copies of the Software, and to\\n# permit persons to whom the Software is furnished to do so, subject to\\n# the following conditions:\\n#\\n# The above copyright notice and this permission notice shall be included\\n# in all copies or substantial portions of the Software.\\n#\\n# THE SOFTWARE IS PROVIDED \\"AS IS\\", WITHOUT WARRANTY OF ANY\\n# KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\\n# LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\\n# WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n#\\n\\n__revision__ = \\"src/engine/SCons/Options/PathOption.py 3842 2008/12/20 22:59:52 scons\\"\\n\\n__doc__ = \\"\\"\\"Place-holder for the old SCons.Options module hierarchy\\n\\nThis is for backwards compatibility.  The new equivalent is the Variables/\\nclass hierarchy.  These will have deprecation warnings added (some day),\\nand will then be removed entirely (some day).\\n\\"\\"\\"\\n\\nimport SCons.Variables\\nimport SCons.Warnings\\n\\nwarned = False\\n\\nclass _PathOptionClass:\\n    def warn(self):\\n        global warned\\n        if not warned:\\n            msg = \\"The PathOption() function is deprecated; use the PathVariable() function instead.\\"\\n            SCons.Warnings.warn(SCons.Warnings.DeprecatedOptionsWarning, msg)\\n            warned = True\\n\\n    def __call__(self, *args, **kw):\\n        self.warn()\\n        return apply(SCons.Variables.PathVariable, args, kw)\\n\\n    def PathAccept(self, *args, **kw):\\n        self.warn()\\n        return apply(SCons.Variables.PathVariable.PathAccept, args, kw)\\n\\n    def PathIsDir(self, *args, **kw):\\n        self.warn()\\n        return apply(SCons.Variables.PathVariable.PathIsDir, args, kw)\\n\\n    def PathIsDirCreate(self, *args, **kw):\\n        self.warn()\\n        return apply(SCons.Variables.PathVariable.PathIsDirCreate, args, kw)\\n\\n    def PathIsFile(self, *args, **kw):\\n        self.warn()\\n        return apply(SCons.Variables.PathVariable.PathIsFile, args, kw)\\n\\n    def PathExists(self, *args, **kw):\\n        self.warn()\\n        return apply(SCons.Variables.PathVariable.PathExists, args, kw)\\n\\nPathOption = _PathOptionClass()\\n" }\n'
line: b'{ "repo_name": "lucafavatella/intellij-community", "ref": "refs/heads/cli-wip", "path": "python/testData/refactoring/extractsuperclass/withImport.before.py", "content": "import os\\n\\nclass A(object):\\n  def foo(self):\\n    os.stat_result.n_fields()\\n" }\n'
line: b'{ "repo_name": "kylon/pacman-fakeroot", "ref": "refs/heads/upstream", "path": "test/pacman/tests/sync401.py", "content": "self.description = \\"Ensure we choose provider already in target list\\"\\n\\nsp1 = pmpkg(\\"pkg1\\")\\nsp1.depends = [\\"dep\\"]\\nself.addpkg2db(\\"sync\\", sp1)\\n\\nsp2 = pmpkg(\\"pkg2\\")\\nsp2.provides = [\\"dep\\"]\\nself.addpkg2db(\\"sync\\", sp2)\\n\\nsp3 = pmpkg(\\"pkg3\\")\\nsp3.conflicts = [\\"pkg2\\"]\\nsp3.provides = [\\"dep\\"]\\nself.addpkg2db(\\"sync\\", sp3)\\n\\nself.args = \\"-S pkg1 pkg2\\"\\n\\nself.addrule(\\"PACMAN_RETCODE=0\\")\\nself.addrule(\\"PKG_EXIST=pkg1\\")\\nself.addrule(\\"PKG_EXIST=pkg2\\")\\nself.addrule(\\"!PKG_EXIST=pkg3\\")\\n" }\n'
line: b'{ "repo_name": "rubikloud/scikit-learn", "ref": "refs/heads/0.17.1-RUBIKLOUD", "path": "sklearn/metrics/__init__.py", "content": "\\"\\"\\"\\nThe :mod:`sklearn.metrics` module includes score functions, performance metrics\\nand pairwise metrics and distance computations.\\n\\"\\"\\"\\n\\n\\nfrom .ranking import auc\\nfrom .ranking import average_precision_score\\nfrom .ranking import coverage_error\\nfrom .ranking import label_ranking_average_precision_score\\nfrom .ranking import label_ranking_loss\\nfrom .ranking import precision_recall_curve\\nfrom .ranking import roc_auc_score\\nfrom .ranking import roc_curve\\n\\nfrom .classification import accuracy_score\\nfrom .classification import classification_report\\nfrom .classification import cohen_kappa_score\\nfrom .classification import confusion_matrix\\nfrom .classification import f1_score\\nfrom .classification import fbeta_score\\nfrom .classification import hamming_loss\\nfrom .classification import hinge_loss\\nfrom .classification import jaccard_similarity_score\\nfrom .classification import log_loss\\nfrom .classification import matthews_corrcoef\\nfrom .classification import precision_recall_fscore_support\\nfrom .classification import precision_score\\nfrom .classification import recall_score\\nfrom .classification import zero_one_loss\\nfrom .classification import brier_score_loss\\n\\nfrom . import cluster\\nfrom .cluster import adjusted_mutual_info_score\\nfrom .cluster import adjusted_rand_score\\nfrom .cluster import completeness_score\\nfrom .cluster import consensus_score\\nfrom .cluster import homogeneity_completeness_v_measure\\nfrom .cluster import homogeneity_score\\nfrom .cluster import mutual_info_score\\nfrom .cluster import normalized_mutual_info_score\\nfrom .cluster import silhouette_samples\\nfrom .cluster import silhouette_score\\nfrom .cluster import v_measure_score\\n\\nfrom .pairwise import euclidean_distances\\nfrom .pairwise import pairwise_distances\\nfrom .pairwise import pairwise_distances_argmin\\nfrom .pairwise import pairwise_distances_argmin_min\\nfrom .pairwise import pairwise_kernels\\n\\nfrom .regression import explained_variance_score\\nfrom .regression import mean_absolute_error\\nfrom .regression import mean_squared_error\\nfrom .regression import median_absolute_error\\nfrom .regression import r2_score\\n\\nfrom .scorer import make_scorer\\nfrom .scorer import SCORERS\\nfrom .scorer import get_scorer\\n\\n__all__ = [\\n    \'accuracy_score\',\\n    \'adjusted_mutual_info_score\',\\n    \'adjusted_rand_score\',\\n    \'auc\',\\n    \'average_precision_score\',\\n    \'classification_report\',\\n    \'cluster\',\\n    \'completeness_score\',\\n    \'confusion_matrix\',\\n    \'consensus_score\',\\n    \'coverage_error\',\\n    \'euclidean_distances\',\\n    \'explained_variance_score\',\\n    \'f1_score\',\\n    \'fbeta_score\',\\n    \'get_scorer\',\\n    \'hamming_loss\',\\n    \'hinge_loss\',\\n    \'homogeneity_completeness_v_measure\',\\n    \'homogeneity_score\',\\n    \'jaccard_similarity_score\',\\n    \'label_ranking_average_precision_score\',\\n    \'label_ranking_loss\',\\n    \'log_loss\',\\n    \'make_scorer\',\\n    \'matthews_corrcoef\',\\n    \'mean_absolute_error\',\\n    \'mean_squared_error\',\\n    \'median_absolute_error\',\\n    \'mutual_info_score\',\\n    \'normalized_mutual_info_score\',\\n    \'pairwise_distances\',\\n    \'pairwise_distances_argmin\',\\n    \'pairwise_distances_argmin_min\',\\n    \'pairwise_distances_argmin_min\',\\n    \'pairwise_kernels\',\\n    \'precision_recall_curve\',\\n    \'precision_recall_fscore_support\',\\n    \'precision_score\',\\n    \'r2_score\',\\n    \'recall_score\',\\n    \'roc_auc_score\',\\n    \'roc_curve\',\\n    \'SCORERS\',\\n    \'silhouette_samples\',\\n    \'silhouette_score\',\\n    \'v_measure_score\',\\n    \'zero_one_loss\',\\n    \'brier_score_loss\',\\n]\\n" }\n'
line: b'{ "repo_name": "chinmaygarde/mojo", "ref": "refs/heads/ios", "path": "build/android/pylib/instrumentation/instrumentation_test_instance.py", "content": "# Copyright 2015 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\nimport logging\\nimport os\\nimport pickle\\nimport re\\nimport sys\\n\\nfrom pylib import cmd_helper\\nfrom pylib import constants\\nfrom pylib import flag_changer\\nfrom pylib.base import base_test_result\\nfrom pylib.base import test_instance\\nfrom pylib.instrumentation import test_result\\nfrom pylib.instrumentation import instrumentation_parser\\nfrom pylib.utils import apk_helper\\nfrom pylib.utils import md5sum\\nfrom pylib.utils import proguard\\n\\nsys.path.append(\\n    os.path.join(constants.DIR_SOURCE_ROOT, \'build\', \'util\', \'lib\', \'common\'))\\nimport unittest_util\\n\\n# Ref: http://developer.android.com/reference/android/app/Activity.html\\n_ACTIVITY_RESULT_CANCELED = 0\\n_ACTIVITY_RESULT_OK = -1\\n\\n_DEFAULT_ANNOTATIONS = [\\n    \'Smoke\', \'SmallTest\', \'MediumTest\', \'LargeTest\',\\n    \'EnormousTest\', \'IntegrationTest\']\\n_EXTRA_ENABLE_HTTP_SERVER = (\\n    \'org.chromium.chrome.test.ChromeInstrumentationTestRunner.\'\\n        + \'EnableTestHttpServer\')\\n_EXTRA_DRIVER_TEST_LIST = (\\n    \'org.chromium.test.driver.OnDeviceInstrumentationDriver.TestList\')\\n_EXTRA_DRIVER_TEST_LIST_FILE = (\\n    \'org.chromium.test.driver.OnDeviceInstrumentationDriver.TestListFile\')\\n_EXTRA_DRIVER_TARGET_PACKAGE = (\\n    \'org.chromium.test.driver.OnDeviceInstrumentationDriver.TargetPackage\')\\n_EXTRA_DRIVER_TARGET_CLASS = (\\n    \'org.chromium.test.driver.OnDeviceInstrumentationDriver.TargetClass\')\\n_NATIVE_CRASH_RE = re.compile(\'native crash\', re.IGNORECASE)\\n_PICKLE_FORMAT_VERSION = 10\\n\\n\\n# TODO(jbudorick): Make these private class methods of\\n# InstrumentationTestInstance once the instrumentation test_runner is\\n# deprecated.\\ndef ParseAmInstrumentRawOutput(raw_output):\\n  \\"\\"\\"Parses the output of an |am instrument -r| call.\\n\\n  Args:\\n    raw_output: the output of an |am instrument -r| call as a list of lines\\n  Returns:\\n    A 3-tuple containing:\\n      - the instrumentation code as an integer\\n      - the instrumentation result as a list of lines\\n      - the instrumentation statuses received as a list of 2-tuples\\n        containing:\\n        - the status code as an integer\\n        - the bundle dump as a dict mapping string keys to a list of\\n          strings, one for each line.\\n  \\"\\"\\"\\n  parser = instrumentation_parser.InstrumentationParser(raw_output)\\n  statuses = list(parser.IterStatus())\\n  code, bundle = parser.GetResult()\\n  return (code, bundle, statuses)\\n\\n\\ndef GenerateTestResults(\\n    result_code, result_bundle, statuses, start_ms, duration_ms):\\n  \\"\\"\\"Generate test results from |statuses|.\\n\\n  Args:\\n    result_code: The overall status code as an integer.\\n    result_bundle: The summary bundle dump as a dict.\\n    statuses: A list of 2-tuples containing:\\n      - the status code as an integer\\n      - the bundle dump as a dict mapping string keys to string values\\n      Note that this is the same as the third item in the 3-tuple returned by\\n      |_ParseAmInstrumentRawOutput|.\\n    start_ms: The start time of the test in milliseconds.\\n    duration_ms: The duration of the test in milliseconds.\\n\\n  Returns:\\n    A list containing an instance of InstrumentationTestResult for each test\\n    parsed.\\n  \\"\\"\\"\\n\\n  results = []\\n\\n  current_result = None\\n\\n  for status_code, bundle in statuses:\\n    test_class = bundle.get(\'class\', \'\')\\n    test_method = bundle.get(\'test\', \'\')\\n    if test_class and test_method:\\n      test_name = \'%s#%s\' % (test_class, test_method)\\n    else:\\n      continue\\n\\n    if status_code == instrumentation_parser.STATUS_CODE_START:\\n      if current_result:\\n        results.append(current_result)\\n      current_result = test_result.InstrumentationTestResult(\\n          test_name, base_test_result.ResultType.UNKNOWN, start_ms, duration_ms)\\n    else:\\n      if status_code == instrumentation_parser.STATUS_CODE_OK:\\n        if bundle.get(\'test_skipped\', \'\').lower() in (\'true\', \'1\', \'yes\'):\\n          current_result.SetType(base_test_result.ResultType.SKIP)\\n        elif current_result.GetType() == base_test_result.ResultType.UNKNOWN:\\n          current_result.SetType(base_test_result.ResultType.PASS)\\n      else:\\n        if status_code not in (instrumentation_parser.STATUS_CODE_ERROR,\\n                               instrumentation_parser.STATUS_CODE_FAILURE):\\n          logging.error(\'Unrecognized status code %d. Handling as an error.\',\\n                        status_code)\\n        current_result.SetType(base_test_result.ResultType.FAIL)\\n        if \'stack\' in bundle:\\n          current_result.SetLog(bundle[\'stack\'])\\n\\n  if current_result:\\n    if current_result.GetType() == base_test_result.ResultType.UNKNOWN:\\n      crashed = (result_code == _ACTIVITY_RESULT_CANCELED\\n                 and any(_NATIVE_CRASH_RE.search(l)\\n                         for l in result_bundle.itervalues()))\\n      if crashed:\\n        current_result.SetType(base_test_result.ResultType.CRASH)\\n\\n    results.append(current_result)\\n\\n  return results\\n\\n\\nclass InstrumentationTestInstance(test_instance.TestInstance):\\n\\n  def __init__(self, args, isolate_delegate, error_func):\\n    super(InstrumentationTestInstance, self).__init__()\\n\\n    self._apk_under_test = None\\n    self._package_info = None\\n    self._suite = None\\n    self._test_apk = None\\n    self._test_jar = None\\n    self._test_package = None\\n    self._test_runner = None\\n    self._test_support_apk = None\\n    self._initializeApkAttributes(args, error_func)\\n\\n    self._data_deps = None\\n    self._isolate_abs_path = None\\n    self._isolate_delegate = None\\n    self._isolated_abs_path = None\\n    self._test_data = None\\n    self._initializeDataDependencyAttributes(args, isolate_delegate)\\n\\n    self._annotations = None\\n    self._excluded_annotations = None\\n    self._test_filter = None\\n    self._initializeTestFilterAttributes(args)\\n\\n    self._flags = None\\n    self._initializeFlagAttributes(args)\\n\\n    self._driver_apk = None\\n    self._driver_package = None\\n    self._driver_name = None\\n    self._initializeDriverAttributes()\\n\\n  def _initializeApkAttributes(self, args, error_func):\\n    if args.apk_under_test.endswith(\'.apk\'):\\n      self._apk_under_test = args.apk_under_test\\n    else:\\n      self._apk_under_test = os.path.join(\\n          constants.GetOutDirectory(), constants.SDK_BUILD_APKS_DIR,\\n          \'%s.apk\' % args.apk_under_test)\\n\\n    if not os.path.exists(self._apk_under_test):\\n      error_func(\'Unable to find APK under test: %s\' % self._apk_under_test)\\n\\n    if args.test_apk.endswith(\'.apk\'):\\n      self._suite = os.path.splitext(os.path.basename(args.test_apk))[0]\\n      self._test_apk = args.test_apk\\n    else:\\n      self._suite = args.test_apk\\n      self._test_apk = os.path.join(\\n          constants.GetOutDirectory(), constants.SDK_BUILD_APKS_DIR,\\n          \'%s.apk\' % args.test_apk)\\n\\n    self._test_jar = os.path.join(\\n        constants.GetOutDirectory(), constants.SDK_BUILD_TEST_JAVALIB_DIR,\\n        \'%s.jar\' % self._suite)\\n    self._test_support_apk = os.path.join(\\n        constants.GetOutDirectory(), constants.SDK_BUILD_TEST_JAVALIB_DIR,\\n        \'%sSupport.apk\' % self._suite)\\n\\n    if not os.path.exists(self._test_apk):\\n      error_func(\'Unable to find test APK: %s\' % self._test_apk)\\n    if not os.path.exists(self._test_jar):\\n      error_func(\'Unable to find test JAR: %s\' % self._test_jar)\\n\\n    apk = apk_helper.ApkHelper(self.test_apk)\\n    self._test_package = apk.GetPackageName()\\n    self._test_runner = apk.GetInstrumentationName()\\n\\n    self._package_info = None\\n    for package_info in constants.PACKAGE_INFO.itervalues():\\n      if self._test_package == package_info.test_package:\\n        self._package_info = package_info\\n    if not self._package_info:\\n      logging.warning(\'Unable to find package info for %s\', self._test_package)\\n\\n  def _initializeDataDependencyAttributes(self, args, isolate_delegate):\\n    self._data_deps = []\\n    if args.isolate_file_path:\\n      self._isolate_abs_path = os.path.abspath(args.isolate_file_path)\\n      self._isolate_delegate = isolate_delegate\\n      self._isolated_abs_path = os.path.join(\\n          constants.GetOutDirectory(), \'%s.isolated\' % self._test_package)\\n    else:\\n      self._isolate_delegate = None\\n\\n    # TODO(jbudorick): Deprecate and remove --test-data once data dependencies\\n    # are fully converted to isolate.\\n    if args.test_data:\\n      logging.info(\'Data dependencies specified via --test-data\')\\n      self._test_data = args.test_data\\n    else:\\n      self._test_data = None\\n\\n    if not self._isolate_delegate and not self._test_data:\\n      logging.warning(\'No data dependencies will be pushed.\')\\n\\n  def _initializeTestFilterAttributes(self, args):\\n    self._test_filter = args.test_filter\\n\\n    def annotation_dict_element(a):\\n      a = a.split(\'=\')\\n      return (a[0], a[1] if len(a) == 2 else None)\\n\\n    if args.annotation_str:\\n      self._annotations = dict(\\n          annotation_dict_element(a)\\n          for a in args.annotation_str.split(\',\'))\\n    elif not self._test_filter:\\n      self._annotations = dict(\\n          annotation_dict_element(a)\\n          for a in _DEFAULT_ANNOTATIONS)\\n    else:\\n      self._annotations = {}\\n\\n    if args.exclude_annotation_str:\\n      self._excluded_annotations = dict(\\n          annotation_dict_element(a)\\n          for a in args.exclude_annotation_str.split(\',\'))\\n    else:\\n      self._excluded_annotations = {}\\n\\n  def _initializeFlagAttributes(self, args):\\n    self._flags = [\'--disable-fre\', \'--enable-test-intents\']\\n    # TODO(jbudorick): Transition \\"--device-flags\\" to \\"--device-flags-file\\"\\n    if hasattr(args, \'device_flags\') and args.device_flags:\\n      with open(args.device_flags) as device_flags_file:\\n        stripped_lines = (l.strip() for l in device_flags_file)\\n        self._flags.extend([flag for flag in stripped_lines if flag])\\n    if hasattr(args, \'device_flags_file\') and args.device_flags_file:\\n      with open(args.device_flags_file) as device_flags_file:\\n        stripped_lines = (l.strip() for l in device_flags_file)\\n        self._flags.extend([flag for flag in stripped_lines if flag])\\n\\n  def _initializeDriverAttributes(self):\\n    self._driver_apk = os.path.join(\\n        constants.GetOutDirectory(), constants.SDK_BUILD_APKS_DIR,\\n        \'OnDeviceInstrumentationDriver.apk\')\\n    if os.path.exists(self._driver_apk):\\n      driver_apk = apk_helper.ApkHelper(self._driver_apk)\\n      self._driver_package = driver_apk.GetPackageName()\\n      self._driver_name = driver_apk.GetInstrumentationName()\\n    else:\\n      self._driver_apk = None\\n\\n  @property\\n  def apk_under_test(self):\\n    return self._apk_under_test\\n\\n  @property\\n  def flags(self):\\n    return self._flags\\n\\n  @property\\n  def driver_apk(self):\\n    return self._driver_apk\\n\\n  @property\\n  def driver_package(self):\\n    return self._driver_package\\n\\n  @property\\n  def driver_name(self):\\n    return self._driver_name\\n\\n  @property\\n  def package_info(self):\\n    return self._package_info\\n\\n  @property\\n  def suite(self):\\n    return self._suite\\n\\n  @property\\n  def test_apk(self):\\n    return self._test_apk\\n\\n  @property\\n  def test_jar(self):\\n    return self._test_jar\\n\\n  @property\\n  def test_support_apk(self):\\n    return self._test_support_apk\\n\\n  @property\\n  def test_package(self):\\n    return self._test_package\\n\\n  @property\\n  def test_runner(self):\\n    return self._test_runner\\n\\n  #override\\n  def TestType(self):\\n    return \'instrumentation\'\\n\\n  #override\\n  def SetUp(self):\\n    if self._isolate_delegate:\\n      self._isolate_delegate.Remap(\\n          self._isolate_abs_path, self._isolated_abs_path)\\n      self._isolate_delegate.MoveOutputDeps()\\n      self._data_deps.extend([(constants.ISOLATE_DEPS_DIR, None)])\\n\\n    # TODO(jbudorick): Convert existing tests that depend on the --test-data\\n    # mechanism to isolate, then remove this.\\n    if self._test_data:\\n      for t in self._test_data:\\n        device_rel_path, host_rel_path = t.split(\':\')\\n        host_abs_path = os.path.join(constants.DIR_SOURCE_ROOT, host_rel_path)\\n        self._data_deps.extend(\\n            [(host_abs_path,\\n              [None, \'chrome\', \'test\', \'data\', device_rel_path])])\\n\\n  def GetDataDependencies(self):\\n    return self._data_deps\\n\\n  def GetTests(self):\\n    pickle_path = \'%s-proguard.pickle\' % self.test_jar\\n    try:\\n      tests = self._GetTestsFromPickle(pickle_path, self.test_jar)\\n    except self.ProguardPickleException as e:\\n      logging.info(\'Getting tests from JAR via proguard. (%s)\' % str(e))\\n      tests = self._GetTestsFromProguard(self.test_jar)\\n      self._SaveTestsToPickle(pickle_path, self.test_jar, tests)\\n    return self._InflateTests(self._FilterTests(tests))\\n\\n  class ProguardPickleException(Exception):\\n    pass\\n\\n  def _GetTestsFromPickle(self, pickle_path, jar_path):\\n    if not os.path.exists(pickle_path):\\n      raise self.ProguardPickleException(\'%s does not exist.\' % pickle_path)\\n    if os.path.getmtime(pickle_path) <= os.path.getmtime(jar_path):\\n      raise self.ProguardPickleException(\\n          \'%s newer than %s.\' % (jar_path, pickle_path))\\n\\n    with open(pickle_path, \'r\') as pickle_file:\\n      pickle_data = pickle.loads(pickle_file.read())\\n    jar_md5 = md5sum.CalculateHostMd5Sums(jar_path)[jar_path]\\n\\n    try:\\n      if pickle_data[\'VERSION\'] != _PICKLE_FORMAT_VERSION:\\n        raise self.ProguardPickleException(\'PICKLE_FORMAT_VERSION has changed.\')\\n      if pickle_data[\'JAR_MD5SUM\'] != jar_md5:\\n        raise self.ProguardPickleException(\'JAR file MD5 sum differs.\')\\n      return pickle_data[\'TEST_METHODS\']\\n    except TypeError as e:\\n      logging.error(pickle_data)\\n      raise self.ProguardPickleException(str(e))\\n\\n  def _GetTestsFromProguard(self, jar_path):\\n    p = proguard.Dump(jar_path)\\n\\n    def is_test_class(c):\\n      return c[\'class\'].endswith(\'Test\')\\n\\n    def is_test_method(m):\\n      return m[\'method\'].startswith(\'test\')\\n\\n    class_lookup = dict((c[\'class\'], c) for c in p[\'classes\'])\\n    def recursive_get_class_annotations(c):\\n      s = c[\'superclass\']\\n      if s in class_lookup:\\n        a = recursive_get_class_annotations(class_lookup[s])\\n      else:\\n        a = {}\\n      a.update(c[\'annotations\'])\\n      return a\\n\\n    def stripped_test_class(c):\\n      return {\\n        \'class\': c[\'class\'],\\n        \'annotations\': recursive_get_class_annotations(c),\\n        \'methods\': [m for m in c[\'methods\'] if is_test_method(m)],\\n    }\\n\\n    return [stripped_test_class(c) for c in p[\'classes\']\\n            if is_test_class(c)]\\n\\n  def _SaveTestsToPickle(self, pickle_path, jar_path, tests):\\n    jar_md5 = md5sum.CalculateHostMd5Sums(jar_path)[jar_path]\\n    pickle_data = {\\n      \'VERSION\': _PICKLE_FORMAT_VERSION,\\n      \'JAR_MD5SUM\': jar_md5,\\n      \'TEST_METHODS\': tests,\\n  }\\n    with open(pickle_path, \'w\') as pickle_file:\\n      pickle.dump(pickle_data, pickle_file)\\n\\n  def _FilterTests(self, tests):\\n\\n    def gtest_filter(c, m):\\n      t = [\'%s.%s\' % (c[\'class\'].split(\'.\')[-1], m[\'method\'])]\\n      return (not self._test_filter\\n              or unittest_util.FilterTestNames(t, self._test_filter))\\n\\n    def annotation_filter(all_annotations):\\n      if not self._annotations:\\n        return True\\n      return any_annotation_matches(self._annotations, all_annotations)\\n\\n    def excluded_annotation_filter(all_annotations):\\n      if not self._excluded_annotations:\\n        return True\\n      return not any_annotation_matches(self._excluded_annotations,\\n                                        all_annotations)\\n\\n    def any_annotation_matches(annotations, all_annotations):\\n      return any(\\n          ak in all_annotations and (av is None or av == all_annotations[ak])\\n          for ak, av in annotations.iteritems())\\n\\n    filtered_classes = []\\n    for c in tests:\\n      filtered_methods = []\\n      for m in c[\'methods\']:\\n        # Gtest filtering\\n        if not gtest_filter(c, m):\\n          continue\\n\\n        all_annotations = dict(c[\'annotations\'])\\n        all_annotations.update(m[\'annotations\'])\\n        if (not annotation_filter(all_annotations)\\n            or not excluded_annotation_filter(all_annotations)):\\n          continue\\n\\n        filtered_methods.append(m)\\n\\n      if filtered_methods:\\n        filtered_class = dict(c)\\n        filtered_class[\'methods\'] = filtered_methods\\n        filtered_classes.append(filtered_class)\\n\\n    return filtered_classes\\n\\n  def _InflateTests(self, tests):\\n    inflated_tests = []\\n    for c in tests:\\n      for m in c[\'methods\']:\\n        a = dict(c[\'annotations\'])\\n        a.update(m[\'annotations\'])\\n        inflated_tests.append({\\n            \'class\': c[\'class\'],\\n            \'method\': m[\'method\'],\\n            \'annotations\': a,\\n      })\\n    return inflated_tests\\n\\n  @staticmethod\\n  def GetHttpServerEnvironmentVars():\\n    return {\\n      _EXTRA_ENABLE_HTTP_SERVER: None,\\n  }\\n\\n  def GetDriverEnvironmentVars(\\n      self, test_list=None, test_list_file_path=None):\\n    env = {\\n      _EXTRA_DRIVER_TARGET_PACKAGE: self.test_package,\\n      _EXTRA_DRIVER_TARGET_CLASS: self.test_runner,\\n  }\\n\\n    if test_list:\\n      env[_EXTRA_DRIVER_TEST_LIST] = \',\'.join(test_list)\\n\\n    if test_list_file_path:\\n      env[_EXTRA_DRIVER_TEST_LIST_FILE] = (\\n          os.path.basename(test_list_file_path))\\n\\n    return env\\n\\n  @staticmethod\\n  def ParseAmInstrumentRawOutput(raw_output):\\n    return ParseAmInstrumentRawOutput(raw_output)\\n\\n  @staticmethod\\n  def GenerateTestResults(\\n      result_code, result_bundle, statuses, start_ms, duration_ms):\\n    return GenerateTestResults(result_code, result_bundle, statuses,\\n                               start_ms, duration_ms)\\n\\n  #override\\n  def TearDown(self):\\n    if self._isolate_delegate:\\n      self._isolate_delegate.Clear()\\n\\n" }\n'
line: b'{ "repo_name": "ronin13/Pacman", "ref": "refs/heads/rprabhu", "path": "test/pacman/tests/sync401.py", "content": "self.description = \\"Ensure we choose provider already in target list\\"\\n\\nsp1 = pmpkg(\\"pkg1\\")\\nsp1.depends = [\\"dep\\"]\\nself.addpkg2db(\\"sync\\", sp1)\\n\\nsp2 = pmpkg(\\"pkg2\\")\\nsp2.provides = [\\"dep\\"]\\nself.addpkg2db(\\"sync\\", sp2)\\n\\nsp3 = pmpkg(\\"pkg3\\")\\nsp3.conflicts = [\\"pkg2\\"]\\nsp3.provides = [\\"dep\\"]\\nself.addpkg2db(\\"sync\\", sp3)\\n\\nself.args = \\"-S pkg1 pkg2\\"\\n\\nself.addrule(\\"PACMAN_RETCODE=0\\")\\nself.addrule(\\"PKG_EXIST=pkg1\\")\\nself.addrule(\\"PKG_EXIST=pkg2\\")\\nself.addrule(\\"!PKG_EXIST=pkg3\\")\\n" }\n'
line: b'{ "repo_name": "TheSLinux-forks/pacman", "ref": "refs/heads/theslinux", "path": "test/pacman/tests/sync401.py", "content": "self.description = \\"Ensure we choose provider already in target list\\"\\n\\nsp1 = pmpkg(\\"pkg1\\")\\nsp1.depends = [\\"dep\\"]\\nself.addpkg2db(\\"sync\\", sp1)\\n\\nsp2 = pmpkg(\\"pkg2\\")\\nsp2.provides = [\\"dep\\"]\\nself.addpkg2db(\\"sync\\", sp2)\\n\\nsp3 = pmpkg(\\"pkg3\\")\\nsp3.conflicts = [\\"pkg2\\"]\\nsp3.provides = [\\"dep\\"]\\nself.addpkg2db(\\"sync\\", sp3)\\n\\nself.args = \\"-S pkg1 pkg2\\"\\n\\nself.addrule(\\"PACMAN_RETCODE=0\\")\\nself.addrule(\\"PKG_EXIST=pkg1\\")\\nself.addrule(\\"PKG_EXIST=pkg2\\")\\nself.addrule(\\"!PKG_EXIST=pkg3\\")\\n" }\n'
line: b'{ "repo_name": "ibarbech/learnbot", "ref": "refs/heads/version-3", "path": "learnbot_dsl/learnbotCode/guiTabLibrary.py", "content": "from __future__ import print_function, absolute_import\\nimport os, sys\\nfrom PySide2 import QtGui, QtWidgets\\nimport learnbot_dsl.guis.TabLibrary as TabLibrary\\nfrom learnbot_dsl.blocksConfig.parserConfigBlock import reload_functions\\nfrom learnbot_dsl.blocksConfig.blocks import *\\nfrom learnbot_dsl.blocksConfig.blocks import pathBlocks as imgPath\\nfrom learnbot_dsl.learnbotCode.Block import *\\nfrom learnbot_dsl.learnbotCode.Button import Block_Button\\n\\n\\nclass Library(QtWidgets.QWidget):\\n\\n    def __init__(self, parent, path):\\n        QtWidgets.QWidget.__init__(self)\\n        self.ui = TabLibrary.Ui_Form()\\n        self.parent = parent\\n        self.ui.setupUi(self)\\n        self.ui.tableLibrary.verticalHeader().setVisible(False)\\n        self.ui.tableLibrary.horizontalHeader().setVisible(False)\\n        self.ui.tableLibrary.setColumnCount(1)\\n        self.ui.tableLibrary.setRowCount(0)\\n        self.namesFunctions = []\\n        self.listButons = []\\n        if not os.path.exists(path):\\n            dirs = [self.parent.libraryPath]\\n            exist = False\\n            for dir in dirs:\\n                for p in os.listdir(dir):\\n                    if p == os.path.basename(path):\\n                        path = os.path.join(dir, p)\\n                        dirs = []\\n                        exist = True\\n                        break\\n                    if os.path.isdir(os.path.join(dir, p)):\\n                        dirs.append(os.path.join(dir, p))\\n                    if os.path.isfile(os.path.join(dir, p)):\\n                        continue\\n            if not exist:\\n                path = None\\n        self.pathLibrary = path\\n        if path is not None:\\n            for subPath in [os.path.join(path, x) for x in os.listdir(path)]:\\n                if os.path.isdir(os.path.abspath(subPath)):\\n                    for subsubPath in [os.path.join(subPath, x) for x in os.listdir(subPath)]:\\n                        if os.path.splitext(subsubPath)[-1] == \\".conf\\":\\n                            self.load(reload_functions(subsubPath))\\n\\n    def load(self, blocks):\\n        listRepitFuntions = []\\n        for b in blocks:\\n            if b[\\"name\\"] in self.parent.listNameUserFunctions or b[\\"name\\"] in self.parent.listNameLibraryFunctions:\\n                listRepitFuntions.append(b[\\"name\\"])\\n                continue\\n            self.namesFunctions.append(b[\\"name\\"])\\n            self.parent.listNameLibraryFunctions.append(b[\\"name\\"])\\n            variables = []\\n            funtionType = LIBRARY\\n            HUE = HUE_LIBRARY\\n            for img in b[\\"img\\"]:\\n                img = os.path.join(imgPath, img)\\n                blockType, connections = loadConfigBlock(img)\\n                table = self.ui.tableLibrary\\n                table.insertRow(table.rowCount())\\n                tooltip = {}\\n                languages = {}\\n                if \\"languages\\" in b:\\n                    languages = b[\\"languages\\"]\\n                if \\"tooltip\\" in b:\\n                    tooltip = b[\\"tooltip\\"]\\n                button = Block_Button((self.parent, b[\\"name\\"], languages, HUE, self.parent.view, self.parent.scene,\\n                                       img + \\".png\\", connections,\\n                                       variables, blockType, table, table.rowCount() - 1,\\n                                       funtionType, tooltip))\\n                self.parent.listButtons.append(button)\\n                self.listButons.append((button, table.rowCount() - 1))\\n                table.setCellWidget(table.rowCount() - 1, 0, button)\\n        if len(listRepitFuntions) is not 0:\\n            text = \\"\\"\\n            for name in listRepitFuntions:\\n                text += \\"\\\\t * \\" + name + \\"\\\\n\\"\\n            msgBox = QtWidgets.QMessageBox()\\n            msgBox.setWindowTitle(self.tr(\\"Warning\\"))\\n            msgBox.setIcon(QtWidgets.QMessageBox.Warning)\\n            msgBox.setText(\\n                self.tr(\\"The following functions have not been imported because there are others with the same name:\\"))\\n            msgBox.setInformativeText(text)\\n            msgBox.setStandardButtons(QtWidgets.QMessageBox.Ok)\\n            msgBox.setDefaultButton(QtWidgets.QMessageBox.Ok)\\n            ret = msgBox.exec_()\\n\\n    def delete(self):\\n        for button, row in self.listButons:\\n            self.parent.listButtons.remove(button)\\n            button.delete(row)\\n        for name in self.namesFunctions:\\n            self.parent.listNameLibraryFunctions.remove(name)\\n\\n    def __del__(self):\\n        print(\\"delete Library\\")\\n" }\n'
line: b'{ "repo_name": "MinchinWeb/topydo", "ref": "refs/heads/stable", "path": "topydo/lib/Colors.py", "content": "# Topydo - A todo.txt client written in Python.\\n# Copyright (C) 2014 - 2015 Bram Schoenmakers <me@bramschoenmakers.nl>\\n#\\n# This program is free software: you can redistribute it and/or modify\\n# it under the terms of the GNU General Public License as published by\\n# the Free Software Foundation, either version 3 of the License, or\\n# (at your option) any later version.\\n#\\n# This program is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU General Public License for more details.\\n#\\n# You should have received a copy of the GNU General Public License\\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\\n\\n\\"\\"\\" This module serves for managing output colors. \\"\\"\\"\\n\\nfrom topydo.lib.Config import config\\n\\nNEUTRAL_COLOR  = \'\\\\033[0m\'\\n\\nclass Colors(object):\\n    def __init__(self):\\n        self.priority_colors = config().priority_colors()\\n        self.project_color = config().project_color()\\n        self.context_color = config().context_color()\\n        self.metadata_color = config().metadata_color()\\n        self.link_color = config().link_color()\\n\\n    def _int_to_ansi(self, p_int, p_decorator=\'normal\', p_safe=True):\\n        \\"\\"\\"\\n        Returns ansi code for color based on xterm color id (0-255) and\\n        decoration, where decoration can be one of: normal, bold, faint,\\n        italic, or underline. When p_safe is True, resulting ansi code is\\n        constructed in most compatible way, but with support for only base 16\\n        colors.\\n        \\"\\"\\"\\n        decoration_dict = {\\n                \'normal\': \'0\',\\n                \'bold\': \'1\',\\n                \'faint\': \'2\',\\n                \'italic\': \'3\',\\n                \'underline\': \'4\'\\n      }\\n\\n        decoration = decoration_dict[p_decorator]\\n\\n        try:\\n            if p_safe:\\n                if 8 > int(p_int) >=0:\\n                    return \'\\\\033[{};3{}m\'.format(decoration, str(p_int))\\n                elif 16 > int(p_int):\\n                    p_int = int(p_int) - 8\\n                    return \'\\\\033[{};1;3{}m\'.format(decoration, str(p_int))\\n\\n            if 256 > int(p_int) >=0:\\n                return \'\\\\033[{};38;5;{}m\'.format(decoration, str(p_int))\\n            else:\\n                return NEUTRAL_COLOR\\n        except ValueError:\\n            return None\\n\\n    def _name_to_int(self, p_color_name):\\n        \\"\\"\\" Returns xterm color id from color name. \\"\\"\\"\\n        color_names_dict = {\\n                \'black\': 0,\\n                \'red\': 1,\\n                \'green\': 2,\\n                \'yellow\': 3,\\n                \'blue\': 4,\\n                \'magenta\': 5,\\n                \'cyan\': 6,\\n                \'gray\': 7,\\n                \'darkgray\': 8,\\n                \'light-red\': 9,\\n                \'light-green\': 10,\\n                \'light-yellow\': 11,\\n                \'light-blue\': 12,\\n                \'light-magenta\': 13,\\n                \'light-cyan\': 14,\\n                \'white\': 15,\\n      }\\n\\n        try:\\n            return color_names_dict[p_color_name]\\n        except KeyError:\\n            return 404\\n\\n    def _name_to_ansi(self, p_color_name, p_decorator):\\n        \\"\\"\\" Returns ansi color code from color name. \\"\\"\\"\\n        number = self._name_to_int(p_color_name)\\n\\n        return self._int_to_ansi(number, p_decorator)\\n\\n    def _get_ansi(self, p_color, p_decorator):\\n        \\"\\"\\" Returns ansi color code from color name or xterm color id. \\"\\"\\"\\n        if p_color == \'\':\\n            ansi = \'\'\\n        else:\\n            ansi = self._int_to_ansi(p_color, p_decorator, False)\\n\\n            if not ansi:\\n                ansi = self._name_to_ansi(p_color, p_decorator)\\n\\n        return ansi\\n\\n    def get_priority_colors(self):\\n        pri_ansi_colors = dict()\\n\\n        for pri in self.priority_colors:\\n            color = self._get_ansi(self.priority_colors[pri], \'normal\')\\n\\n            if color == \'\':\\n                color = NEUTRAL_COLOR\\n\\n            pri_ansi_colors[pri] = color\\n\\n        return pri_ansi_colors\\n\\n    def get_project_color(self):\\n        return self._get_ansi(self.project_color, \'bold\')\\n\\n    def get_context_color(self):\\n        return self._get_ansi(self.context_color, \'bold\')\\n\\n    def get_metadata_color(self):\\n        return self._get_ansi(self.metadata_color, \'bold\')\\n\\n    def get_link_color(self):\\n        return self._get_ansi(self.link_color, \'underline\')\\n" }\n'
line: b'{ "repo_name": "yelizariev/mail-addons", "ref": "refs/heads/9.0", "path": "mail_to/__openerp__.py", "content": "# -*- coding: utf-8 -*-\\n{\\n    \\"name\\": \\"\\"\\"Show message recipients\\"\\"\\",\\n    \\"summary\\": \\"\\"\\"Allows you be sure, that all discussion participants were notified\\"\\"\\",\\n    \\"category\\": \\"Discuss\\",\\n    \\"images\\": [\'images/1.png\'],\\n    \\"version\\": \\"1.0.0\\",\\n\\n    \\"author\\": \\"IT-Projects LLC, Pavel Romanchenko\\",\\n    \\"website\\": \\"https://it-projects.info\\",\\n    \\"license\\": \\"LGPL-3\\",\\n    \\"price\\": 40.00,\\n    \\"currency\\": \\"EUR\\",\\n\\n    \\"depends\\": [\\n        \'mail_base\',\\n    ],\\n    \\"external_dependencies\\": {\\"python\\": [], \\"bin\\": []}\\n    \\"data\\": [\\n        \'templates.xml\',\\n    ],\\n    \\"qweb\\": [\\n        \'static/src/xml/recipient.xml\',\\n    ],\\n    \\"demo\\": [],\\n    \\"installable\\": True,\\n    \\"auto_install\\": False,\\n}\\n" }\n'
line: b'{ "repo_name": "ProgVal/Limnoria-test", "ref": "refs/heads/debug-pypy-sqlite", "path": "plugins/Anonymous/plugin.py", "content": "###\\n# Copyright (c) 2005, Daniel DiPaolo\\n# Copyright (c) 2014, James McCoy\\n# All rights reserved.\\n#\\n# Redistribution and use in source and binary forms, with or without\\n# modification, are permitted provided that the following conditions are met:\\n#\\n#   * Redistributions of source code must retain the above copyright notice,\\n#     this list of conditions, and the following disclaimer.\\n#   * Redistributions in binary form must reproduce the above copyright notice,\\n#     this list of conditions, and the following disclaimer in the\\n#     documentation and/or other materials provided with the distribution.\\n#   * Neither the name of the author of this software nor the name of\\n#     contributors to this software may be used to endorse or promote products\\n#     derived from this software without specific prior written consent.\\n#\\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \\"AS IS\\"\\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\\n# POSSIBILITY OF SUCH DAMAGE.\\n###\\n\\nimport supybot.ircdb as ircdb\\nimport supybot.utils as utils\\nfrom supybot.commands import *\\nimport supybot.ircmsgs as ircmsgs\\nimport supybot.callbacks as callbacks\\nimport supybot.ircutils as ircutils\\nfrom supybot.i18n import PluginInternationalization, internationalizeDocstring\\n_ = PluginInternationalization(\'Anonymous\')\\n\\nclass Anonymous(callbacks.Plugin):\\n    \\"\\"\\"This plugin allows users to act through the bot anonymously.  The \'do\'\\n    command has the bot perform an anonymous action in a given channel, and\\n    the \'say\' command allows other people to speak through the bot.  Since\\n    this can be fairly well abused, you might want to set\\n    supybot.plugins.Anonymous.requireCapability so only users with that\\n    capability can use this plugin.  For extra security, you can require that\\n    the user be *in* the channel they are trying to address anonymously with\\n    supybot.plugins.Anonymous.requirePresenceInChannel, or you can require\\n    that the user be registered by setting\\n    supybot.plugins.Anonymous.requireRegistration.\\n    \\"\\"\\"\\n    def _preCheck(self, irc, msg, target, action):\\n        if self.registryValue(\'requireRegistration\', target):\\n            try:\\n                foo = ircdb.users.getUser(msg.prefix)\\n            except KeyError:\\n                irc.errorNotRegistered(Raise=True)\\n        capability = self.registryValue(\'requireCapability\', target)\\n        if capability:\\n            if not ircdb.checkCapability(msg.prefix, capability):\\n                irc.errorNoCapability(capability, Raise=True)\\n        if action != \'tell\':\\n            if self.registryValue(\'requirePresenceInChannel\', target) and \\\\\\n               msg.nick not in irc.state.channels[target].users:\\n                irc.error(format(_(\'You must be in %s to %q in there.\'),\\n                                 target, action), Raise=True)\\n            c = ircdb.channels.getChannel(target)\\n            if c.lobotomized:\\n                irc.error(format(_(\'I\\\\\'m lobotomized in %s.\'), target),\\n                          Raise=True)\\n            if not c._checkCapability(self.name()):\\n                irc.error(_(\'That channel has set its capabilities so as to \'\\n                          \'disallow the use of this plugin.\'), Raise=True)\\n        elif not self.registryValue(\'allowPrivateTarget\'):\\n            irc.error(_(\'This command is disabled (supybot.plugins.Anonymous.\'\\n                      \'allowPrivateTarget is False).\'), Raise=True)\\n\\n    @internationalizeDocstring\\n    def say(self, irc, msg, args, target, text):\\n        \\"\\"\\"<channel> <text>\\n\\n        Sends <text> to <channel>.\\n        \\"\\"\\"\\n        self._preCheck(irc, msg, target, \'say\')\\n        self.log.info(\'Saying %q in %s due to %s.\',\\n                      text, target, msg.prefix)\\n        irc.queueMsg(ircmsgs.privmsg(target, text))\\n        irc.noReply()\\n    say = wrap(say, [\'inChannel\', \'text\'])\\n\\n    def tell(self, irc, msg, args, target, text):\\n        \\"\\"\\"<nick> <text>\\n\\n        Sends <text> to <nick>.  Can only be used if\\n        supybot.plugins.Anonymous.allowPrivateTarget is True.\\n        \\"\\"\\"\\n        self._preCheck(irc, msg, target, \'tell\')\\n        self.log.info(\'Telling %q to %s due to %s.\',\\n                      text, target, msg.prefix)\\n        irc.queueMsg(ircmsgs.privmsg(target, text))\\n        irc.noReply()\\n    tell = wrap(tell, [\'nick\', \'text\'])\\n\\n    @internationalizeDocstring\\n    def do(self, irc, msg, args, channel, text):\\n        \\"\\"\\"<channel> <action>\\n\\n        Performs <action> in <channel>.\\n        \\"\\"\\"\\n        self._preCheck(irc, msg, channel, \'do\')\\n        self.log.info(\'Performing %q in %s due to %s.\',\\n                      text, channel, msg.prefix)\\n        irc.reply(text, action=True, to=channel)\\n    do = wrap(do, [\'inChannel\', \'text\'])\\nAnonymous = internationalizeDocstring(Anonymous)\\n\\nClass = Anonymous\\n\\n\\n# vim:set shiftwidth=4 softtabstop=4 expandtab textwidth=79:\\n" }\n'
line: b'{ "repo_name": "Bitl/RBXLegacy-src", "ref": "refs/heads/stable", "path": "Cut/RBXLegacyDiscordBot/lib/youtube_dl/extractor/rai.py", "content": "from __future__ import unicode_literals\\n\\nimport re\\n\\nfrom .common import InfoExtractor\\nfrom ..compat import (\\n    compat_urlparse,\\n    compat_str,\\n)\\nfrom ..utils import (\\n    ExtractorError,\\n    determine_ext,\\n    find_xpath_attr,\\n    fix_xml_ampersands,\\n    GeoRestrictedError,\\n    int_or_none,\\n    parse_duration,\\n    strip_or_none,\\n    try_get,\\n    unified_strdate,\\n    unified_timestamp,\\n    update_url_query,\\n    urljoin,\\n    xpath_text,\\n)\\n\\n\\nclass RaiBaseIE(InfoExtractor):\\n    _UUID_RE = r\'[\\\\da-f]{8}-[\\\\da-f]{4}-[\\\\da-f]{4}-[\\\\da-f]{4}-[\\\\da-f]{12}\'\\n    _GEO_COUNTRIES = [\'IT\']\\n    _GEO_BYPASS = False\\n\\n    def _extract_relinker_info(self, relinker_url, video_id):\\n        formats = []\\n        geoprotection = None\\n        is_live = None\\n        duration = None\\n\\n        for platform in (\'mon\', \'flash\', \'native\'):\\n            relinker = self._download_xml(\\n                relinker_url, video_id,\\n                note=\'Downloading XML metadata for platform %s\' % platform,\\n                transform_source=fix_xml_ampersands,\\n                query={\'output\': 45, \'pl\': platform}\\n                headers=self.geo_verification_headers())\\n\\n            if not geoprotection:\\n                geoprotection = xpath_text(\\n                    relinker, \'./geoprotection\', default=None) == \'Y\'\\n\\n            if not is_live:\\n                is_live = xpath_text(\\n                    relinker, \'./is_live\', default=None) == \'Y\'\\n            if not duration:\\n                duration = parse_duration(xpath_text(\\n                    relinker, \'./duration\', default=None))\\n\\n            url_elem = find_xpath_attr(relinker, \'./url\', \'type\', \'content\')\\n            if url_elem is None:\\n                continue\\n\\n            media_url = url_elem.text\\n\\n            # This does not imply geo restriction (e.g.\\n            # http://www.raisport.rai.it/dl/raiSport/media/rassegna-stampa-04a9f4bd-b563-40cf-82a6-aad3529cb4a9.html)\\n            if media_url == \'http://download.rai.it/video_no_available.mp4\':\\n                continue\\n\\n            ext = determine_ext(media_url)\\n            if (ext == \'m3u8\' and platform != \'mon\') or (ext == \'f4m\' and platform != \'flash\'):\\n                continue\\n\\n            if ext == \'m3u8\':\\n                formats.extend(self._extract_m3u8_formats(\\n                    media_url, video_id, \'mp4\', \'m3u8_native\',\\n                    m3u8_id=\'hls\', fatal=False))\\n            elif ext == \'f4m\':\\n                manifest_url = update_url_query(\\n                    media_url.replace(\'manifest#live_hds.f4m\', \'manifest.f4m\'),\\n                  {\'hdcore\': \'3.7.0\', \'plugin\': \'aasp-3.7.0.39.44\'})\\n                formats.extend(self._extract_f4m_formats(\\n                    manifest_url, video_id, f4m_id=\'hds\', fatal=False))\\n            else:\\n                bitrate = int_or_none(xpath_text(relinker, \'bitrate\'))\\n                formats.append({\\n                    \'url\': media_url,\\n                    \'tbr\': bitrate if bitrate > 0 else None,\\n                    \'format_id\': \'http-%d\' % bitrate if bitrate > 0 else \'http\',\\n              })\\n\\n        if not formats and geoprotection is True:\\n            self.raise_geo_restricted(countries=self._GEO_COUNTRIES)\\n\\n        return dict((k, v) for k, v in {\\n            \'is_live\': is_live,\\n            \'duration\': duration,\\n            \'formats\': formats,\\n      }.items() if v is not None)\\n\\n    @staticmethod\\n    def _extract_subtitles(url, subtitle_url):\\n        subtitles = {}\\n        if subtitle_url and isinstance(subtitle_url, compat_str):\\n            subtitle_url = urljoin(url, subtitle_url)\\n            STL_EXT = \'.stl\'\\n            SRT_EXT = \'.srt\'\\n            subtitles[\'it\'] = [{\\n                \'ext\': \'stl\',\\n                \'url\': subtitle_url,\\n          }]\\n            if subtitle_url.endswith(STL_EXT):\\n                srt_url = subtitle_url[:-len(STL_EXT)] + SRT_EXT\\n                subtitles[\'it\'].append({\\n                    \'ext\': \'srt\',\\n                    \'url\': srt_url,\\n              })\\n        return subtitles\\n\\n\\nclass RaiPlayIE(RaiBaseIE):\\n    _VALID_URL = r\'(?P<url>https?://(?:www\\\\.)?raiplay\\\\.it/.+?-(?P<id>%s)\\\\.html)\' % RaiBaseIE._UUID_RE\\n    _TESTS = [{\\n        \'url\': \'http://www.raiplay.it/video/2016/10/La-Casa-Bianca-e06118bb-59a9-4636-b914-498e4cfd2c66.html?source=twitter\',\\n        \'md5\': \'340aa3b7afb54bfd14a8c11786450d76\',\\n        \'info_dict\': {\\n            \'id\': \'e06118bb-59a9-4636-b914-498e4cfd2c66\',\\n            \'ext\': \'mp4\',\\n            \'title\': \'La Casa Bianca\',\\n            \'alt_title\': \'S2016 - Puntata del 23/10/2016\',\\n            \'description\': \'md5:a09d45890850458077d1f68bb036e0a5\',\\n            \'thumbnail\': r\'re:^https?://.*\\\\.jpg$\',\\n            \'uploader\': \'Rai 3\',\\n            \'creator\': \'Rai 3\',\\n            \'duration\': 3278,\\n            \'timestamp\': 1477764300,\\n            \'upload_date\': \'20161029\',\\n            \'series\': \'La Casa Bianca\',\\n            \'season\': \'2016\',\\n      }\\n  } {\\n        \'url\': \'http://www.raiplay.it/video/2014/04/Report-del-07042014-cb27157f-9dd0-4aee-b788-b1f67643a391.html\',\\n        \'md5\': \'8970abf8caf8aef4696e7b1f2adfc696\',\\n        \'info_dict\': {\\n            \'id\': \'cb27157f-9dd0-4aee-b788-b1f67643a391\',\\n            \'ext\': \'mp4\',\\n            \'title\': \'Report del 07/04/2014\',\\n            \'alt_title\': \'S2013/14 - Puntata del 07/04/2014\',\\n            \'description\': \'md5:f27c544694cacb46a078db84ec35d2d9\',\\n            \'thumbnail\': r\'re:^https?://.*\\\\.jpg$\',\\n            \'uploader\': \'Rai 5\',\\n            \'creator\': \'Rai 5\',\\n            \'duration\': 6160,\\n            \'series\': \'Report\',\\n            \'season_number\': 5,\\n            \'season\': \'2013/14\',\\n      }\\n        \'params\': {\\n            \'skip_download\': True,\\n      }\\n  } {\\n        \'url\': \'http://www.raiplay.it/video/2016/11/gazebotraindesi-efebe701-969c-4593-92f3-285f0d1ce750.html?\',\\n        \'only_matching\': True,\\n  }]\\n\\n    def _real_extract(self, url):\\n        mobj = re.match(self._VALID_URL, url)\\n        url, video_id = mobj.group(\'url\', \'id\')\\n\\n        media = self._download_json(\\n            \'%s?json\' % url, video_id, \'Downloading video JSON\')\\n\\n        title = media[\'name\']\\n\\n        video = media[\'video\']\\n\\n        relinker_info = self._extract_relinker_info(video[\'contentUrl\'], video_id)\\n        self._sort_formats(relinker_info[\'formats\'])\\n\\n        thumbnails = []\\n        if \'images\' in media:\\n            for _, value in media.get(\'images\').items():\\n                if value:\\n                    thumbnails.append({\\n                        \'url\': value.replace(\'[RESOLUTION]\', \'600x400\')\\n                  })\\n\\n        timestamp = unified_timestamp(try_get(\\n            media, lambda x: x[\'availabilities\'][0][\'start\'], compat_str))\\n\\n        subtitles = self._extract_subtitles(url, video.get(\'subtitles\'))\\n\\n        info = {\\n            \'id\': video_id,\\n            \'title\': self._live_title(title) if relinker_info.get(\\n                \'is_live\') else title,\\n            \'alt_title\': media.get(\'subtitle\'),\\n            \'description\': media.get(\'description\'),\\n            \'uploader\': strip_or_none(media.get(\'channel\')),\\n            \'creator\': strip_or_none(media.get(\'editor\')),\\n            \'duration\': parse_duration(video.get(\'duration\')),\\n            \'timestamp\': timestamp,\\n            \'thumbnails\': thumbnails,\\n            \'series\': try_get(\\n                media, lambda x: x[\'isPartOf\'][\'name\'], compat_str),\\n            \'season_number\': int_or_none(try_get(\\n                media, lambda x: x[\'isPartOf\'][\'numeroStagioni\'])),\\n            \'season\': media.get(\'stagione\') or None,\\n            \'subtitles\': subtitles,\\n      }\\n\\n        info.update(relinker_info)\\n        return info\\n\\n\\nclass RaiPlayLiveIE(RaiBaseIE):\\n    _VALID_URL = r\'https?://(?:www\\\\.)?raiplay\\\\.it/dirette/(?P<id>[^/?#&]+)\'\\n    _TEST = {\\n        \'url\': \'http://www.raiplay.it/dirette/rainews24\',\\n        \'info_dict\': {\\n            \'id\': \'d784ad40-e0ae-4a69-aa76-37519d238a9c\',\\n            \'display_id\': \'rainews24\',\\n            \'ext\': \'mp4\',\\n            \'title\': \'re:^Diretta di Rai News 24 [0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}$\',\\n            \'description\': \'md5:6eca31500550f9376819f174e5644754\',\\n            \'uploader\': \'Rai News 24\',\\n            \'creator\': \'Rai News 24\',\\n            \'is_live\': True,\\n      }\\n        \'params\': {\\n            \'skip_download\': True,\\n      }\\n  }\\n\\n    def _real_extract(self, url):\\n        display_id = self._match_id(url)\\n\\n        webpage = self._download_webpage(url, display_id)\\n\\n        video_id = self._search_regex(\\n            r\'data-uniquename=[\\"\\\\\']ContentItem-(%s)\' % RaiBaseIE._UUID_RE,\\n            webpage, \'content id\')\\n\\n        return {\\n            \'_type\': \'url_transparent\',\\n            \'ie_key\': RaiPlayIE.ie_key(),\\n            \'url\': \'http://www.raiplay.it/dirette/ContentItem-%s.html\' % video_id,\\n            \'id\': video_id,\\n            \'display_id\': display_id,\\n      }\\n\\n\\nclass RaiIE(RaiBaseIE):\\n    _VALID_URL = r\'https?://[^/]+\\\\.(?:rai\\\\.(?:it|tv)|rainews\\\\.it)/dl/.+?-(?P<id>%s)(?:-.+?)?\\\\.html\' % RaiBaseIE._UUID_RE\\n    _TESTS = [{\\n        # var uniquename = \\"ContentItem-...\\"\\n        # data-id=\\"ContentItem-...\\"\\n        \'url\': \'http://www.raisport.rai.it/dl/raiSport/media/rassegna-stampa-04a9f4bd-b563-40cf-82a6-aad3529cb4a9.html\',\\n        \'info_dict\': {\\n            \'id\': \'04a9f4bd-b563-40cf-82a6-aad3529cb4a9\',\\n            \'ext\': \'mp4\',\\n            \'title\': \'TG PRIMO TEMPO\',\\n            \'thumbnail\': r\'re:^https?://.*\\\\.jpg$\',\\n            \'duration\': 1758,\\n            \'upload_date\': \'20140612\',\\n      }\\n  } {\\n        # with ContentItem in many metas\\n        \'url\': \'http://www.rainews.it/dl/rainews/media/Weekend-al-cinema-da-Hollywood-arriva-il-thriller-di-Tate-Taylor-La-ragazza-del-treno-1632c009-c843-4836-bb65-80c33084a64b.html\',\\n        \'info_dict\': {\\n            \'id\': \'1632c009-c843-4836-bb65-80c33084a64b\',\\n            \'ext\': \'mp4\',\\n            \'title\': \'Weekend al cinema, da Hollywood arriva il thriller di Tate Taylor \\"La ragazza del treno\\"\',\\n            \'description\': \'I film in uscita questa settimana.\',\\n            \'thumbnail\': r\'re:^https?://.*\\\\.png$\',\\n            \'duration\': 833,\\n            \'upload_date\': \'20161103\',\\n      }\\n  } {\\n        # with ContentItem in og:url\\n        \'url\': \'http://www.rai.it/dl/RaiTV/programmi/media/ContentItem-efb17665-691c-45d5-a60c-5301333cbb0c.html\',\\n        \'md5\': \'11959b4e44fa74de47011b5799490adf\',\\n        \'info_dict\': {\\n            \'id\': \'efb17665-691c-45d5-a60c-5301333cbb0c\',\\n            \'ext\': \'mp4\',\\n            \'title\': \'TG1 ore 20:00 del 03/11/2016\',\\n            \'description\': \'TG1 edizione integrale ore 20:00 del giorno 03/11/2016\',\\n            \'thumbnail\': r\'re:^https?://.*\\\\.jpg$\',\\n            \'duration\': 2214,\\n            \'upload_date\': \'20161103\',\\n      }\\n  } {\\n        # drawMediaRaiTV(...)\\n        \'url\': \'http://www.report.rai.it/dl/Report/puntata/ContentItem-0c7a664b-d0f4-4b2c-8835-3f82e46f433e.html\',\\n        \'md5\': \'2dd727e61114e1ee9c47f0da6914e178\',\\n        \'info_dict\': {\\n            \'id\': \'59d69d28-6bb6-409d-a4b5-ed44096560af\',\\n            \'ext\': \'mp4\',\\n            \'title\': \'Il pacco\',\\n            \'description\': \'md5:4b1afae1364115ce5d78ed83cd2e5b3a\',\\n            \'thumbnail\': r\'re:^https?://.*\\\\.jpg$\',\\n            \'upload_date\': \'20141221\',\\n      }\\n  } {\\n        # initEdizione(\'ContentItem-...\'\\n        \'url\': \'http://www.tg1.rai.it/dl/tg1/2010/edizioni/ContentSet-9b6e0cba-4bef-4aef-8cf0-9f7f665b7dfb-tg1.html?item=undefined\',\\n        \'info_dict\': {\\n            \'id\': \'c2187016-8484-4e3a-8ac8-35e475b07303\',\\n            \'ext\': \'mp4\',\\n            \'title\': r\'re:TG1 ore \\\\d{2}:\\\\d{2} del \\\\d{2}/\\\\d{2}/\\\\d{4}\',\\n            \'duration\': 2274,\\n            \'upload_date\': \'20170401\',\\n      }\\n        \'skip\': \'Changes daily\',\\n  } {\\n        # HDS live stream with only relinker URL\\n        \'url\': \'http://www.rai.tv/dl/RaiTV/dirette/PublishingBlock-1912dbbf-3f96-44c3-b4cf-523681fbacbc.html?channel=EuroNews\',\\n        \'info_dict\': {\\n            \'id\': \'1912dbbf-3f96-44c3-b4cf-523681fbacbc\',\\n            \'ext\': \'flv\',\\n            \'title\': \'EuroNews\',\\n      }\\n        \'params\': {\\n            \'skip_download\': True,\\n      }\\n  } {\\n        # HLS live stream with ContentItem in og:url\\n        \'url\': \'http://www.rainews.it/dl/rainews/live/ContentItem-3156f2f2-dc70-4953-8e2f-70d7489d4ce9.html\',\\n        \'info_dict\': {\\n            \'id\': \'3156f2f2-dc70-4953-8e2f-70d7489d4ce9\',\\n            \'ext\': \'mp4\',\\n            \'title\': \'La diretta di Rainews24\',\\n      }\\n        \'params\': {\\n            \'skip_download\': True,\\n      }\\n  }]\\n\\n    def _extract_from_content_id(self, content_id, url):\\n        media = self._download_json(\\n            \'http://www.rai.tv/dl/RaiTV/programmi/media/ContentItem-%s.html?json\' % content_id,\\n            content_id, \'Downloading video JSON\')\\n\\n        title = media[\'name\'].strip()\\n\\n        media_type = media[\'type\']\\n        if \'Audio\' in media_type:\\n            relinker_info = {\\n                \'formats\': {\\n                    \'format_id\': media.get(\'formatoAudio\'),\\n                    \'url\': media[\'audioUrl\'],\\n                    \'ext\': media.get(\'formatoAudio\'),\\n              }\\n          }\\n        elif \'Video\' in media_type:\\n            relinker_info = self._extract_relinker_info(media[\'mediaUri\'], content_id)\\n        else:\\n            raise ExtractorError(\'not a media file\')\\n\\n        self._sort_formats(relinker_info[\'formats\'])\\n\\n        thumbnails = []\\n        for image_type in (\'image\', \'image_medium\', \'image_300\'):\\n            thumbnail_url = media.get(image_type)\\n            if thumbnail_url:\\n                thumbnails.append({\\n                    \'url\': compat_urlparse.urljoin(url, thumbnail_url),\\n              })\\n\\n        subtitles = self._extract_subtitles(url, media.get(\'subtitlesUrl\'))\\n\\n        info = {\\n            \'id\': content_id,\\n            \'title\': title,\\n            \'description\': strip_or_none(media.get(\'desc\')),\\n            \'thumbnails\': thumbnails,\\n            \'uploader\': media.get(\'author\'),\\n            \'upload_date\': unified_strdate(media.get(\'date\')),\\n            \'duration\': parse_duration(media.get(\'length\')),\\n            \'subtitles\': subtitles,\\n      }\\n\\n        info.update(relinker_info)\\n\\n        return info\\n\\n    def _real_extract(self, url):\\n        video_id = self._match_id(url)\\n\\n        webpage = self._download_webpage(url, video_id)\\n\\n        content_item_id = None\\n\\n        content_item_url = self._html_search_meta(\\n            (\'og:url\', \'og:video\', \'og:video:secure_url\', \'twitter:url\',\\n             \'twitter:player\', \'jsonlink\'), webpage, default=None)\\n        if content_item_url:\\n            content_item_id = self._search_regex(\\n                r\'ContentItem-(%s)\' % self._UUID_RE, content_item_url,\\n                \'content item id\', default=None)\\n\\n        if not content_item_id:\\n            content_item_id = self._search_regex(\\n                r\'\'\'(?x)\\n                    (?:\\n                        (?:initEdizione|drawMediaRaiTV)\\\\(|\\n                        <(?:[^>]+\\\\bdata-id|var\\\\s+uniquename)=\\n                    )\\n                    ([\\"\\\\\'])\\n                    (?:(?!\\\\1).)*\\\\bContentItem-(?P<id>%s)\\n                \'\'\' % self._UUID_RE,\\n                webpage, \'content item id\', default=None, group=\'id\')\\n\\n        content_item_ids = set()\\n        if content_item_id:\\n            content_item_ids.add(content_item_id)\\n        if video_id not in content_item_ids:\\n            content_item_ids.add(video_id)\\n\\n        for content_item_id in content_item_ids:\\n            try:\\n                return self._extract_from_content_id(content_item_id, url)\\n            except GeoRestrictedError:\\n                raise\\n            except ExtractorError:\\n                pass\\n\\n        relinker_url = self._search_regex(\\n            r\'\'\'(?x)\\n                (?:\\n                    var\\\\s+videoURL|\\n                    mediaInfo\\\\.mediaUri\\n                )\\\\s*=\\\\s*\\n                ([\\\\\'\\"])\\n                (?P<url>\\n                    (?:https?:)?\\n                    //mediapolis(?:vod)?\\\\.rai\\\\.it/relinker/relinkerServlet\\\\.htm\\\\?\\n                    (?:(?!\\\\1).)*\\\\bcont=(?:(?!\\\\1).)+)\\\\1\\n            \'\'\',\\n            webpage, \'relinker URL\', group=\'url\')\\n\\n        relinker_info = self._extract_relinker_info(\\n            urljoin(url, relinker_url), video_id)\\n        self._sort_formats(relinker_info[\'formats\'])\\n\\n        title = self._search_regex(\\n            r\'var\\\\s+videoTitolo\\\\s*=\\\\s*([\\\\\'\\"])(?P<title>[^\\\\\'\\"]+)\\\\1\',\\n            webpage, \'title\', group=\'title\',\\n            default=None) or self._og_search_title(webpage)\\n\\n        info = {\\n            \'id\': video_id,\\n            \'title\': title,\\n      }\\n\\n        info.update(relinker_info)\\n\\n        return info\\n" }\n'
line: b'{ "repo_name": "mhuwiler/rootauto", "ref": "refs/heads/mhuwiler", "path": "tutorials/pyroot/tree.py", "content": "## \\\\file\\n## \\\\ingroup tutorial_pyroot\\n## \\\\notebook\\n## This macro displays the Tree data structures\\n##\\n## \\\\macro_image\\n## \\\\macro_code\\n##\\n## \\\\author Wim Lavrijsen\\n\\nfrom ROOT import TCanvas, TPaveLabel, TPaveText, TPavesText, TText\\nfrom ROOT import TArrow, TLine\\nfrom ROOT import gROOT, gBenchmark\\n\\n#gROOT.Reset()\\n\\nc1 = TCanvas(\'c1\',\'Tree Data Structure\',200,10,750,940)\\nc1.Range(0,-0.1,1,1.15)\\n\\ngBenchmark.Start(\'tree\')\\n\\nbranchcolor = 26\\nleafcolor   = 30\\nbasketcolor = 42\\noffsetcolor = 43\\n#title = TPaveLabel(.3,1.05,.8,1.13,c1.GetTitle())\\ntitle = TPaveLabel(.3,1.05,.8,1.13,\'Tree Data Structure\')\\ntitle.SetFillColor(16)\\ntitle.Draw()\\ntree = TPaveText(.01,.75,.15,1.00)\\ntree.SetFillColor(18)\\ntree.SetTextAlign(12)\\ntnt = tree.AddText(\'Tree\')\\ntnt.SetTextAlign(22)\\ntnt.SetTextSize(0.030)\\ntree.AddText(\'fScanField\')\\ntree.AddText(\'fMaxEventLoop\')\\ntree.AddText(\'fMaxVirtualSize\')\\ntree.AddText(\'fEntries\')\\ntree.AddText(\'fDimension\')\\ntree.AddText(\'fSelectedRows\')\\ntree.Draw()\\nfarm = TPavesText(.01,1.02,.15,1.1,9,\'tr\')\\ntfarm = farm.AddText(\'CHAIN\')\\ntfarm.SetTextSize(0.024)\\nfarm.AddText(\'Collection\')\\nfarm.AddText(\'of Trees\')\\nfarm.Draw()\\nlink = TLine(.15,.92,.80,.92)\\nlink.SetLineWidth(2)\\nlink.SetLineColor(1)\\nlink.Draw()\\nlink.DrawLine(.21,.87,.21,.275)\\nlink.DrawLine(.23,.87,.23,.375)\\nlink.DrawLine(.25,.87,.25,.775)\\nlink.DrawLine(.41,.25,.41,-.025)\\nlink.DrawLine(.43,.25,.43,.075)\\nlink.DrawLine(.45,.25,.45,.175)\\nbranch0 = TPaveLabel(.20,.87,.35,.97,\'Branch 0\')\\nbranch0.SetTextSize(0.35)\\nbranch0.SetFillColor(branchcolor)\\nbranch0.Draw()\\nbranch1 = TPaveLabel(.40,.87,.55,.97,\'Branch 1\')\\nbranch1.SetTextSize(0.35)\\nbranch1.SetFillColor(branchcolor)\\nbranch1.Draw()\\nbranch2 = TPaveLabel(.60,.87,.75,.97,\'Branch 2\')\\nbranch2.SetTextSize(0.35)\\nbranch2.SetFillColor(branchcolor)\\nbranch2.Draw()\\nbranch3 = TPaveLabel(.80,.87,.95,.97,\'Branch 3\')\\nbranch3.SetTextSize(0.35)\\nbranch3.SetFillColor(branchcolor)\\nbranch3.Draw()\\nleaf0 = TPaveLabel(.4,.75,.5,.8,\'Leaf 0\')\\nleaf0.SetFillColor(leafcolor)\\nleaf0.Draw()\\nleaf1 = TPaveLabel(.6,.75,.7,.8,\'Leaf 1\')\\nleaf1.SetFillColor(leafcolor)\\nleaf1.Draw()\\nleaf2 = TPaveLabel(.8,.75,.9,.8,\'Leaf 2\')\\nleaf2.SetFillColor(leafcolor)\\nleaf2.Draw()\\nfirstevent = TPaveText(.4,.35,.9,.4)\\nfirstevent.AddText(\'First event of each basket\')\\nfirstevent.AddText(\'Array of fMaxBaskets Integers\')\\nfirstevent.SetFillColor(basketcolor)\\nfirstevent.Draw()\\nbasket0 = TPaveLabel(.4,.25,.5,.3,\'Basket 0\')\\nbasket0.SetFillColor(basketcolor)\\nbasket0.Draw()\\nbasket1 = TPaveLabel(.6,.25,.7,.3,\'Basket 1\')\\nbasket1.SetFillColor(basketcolor)\\nbasket1.Draw()\\nbasket2 = TPaveLabel(.8,.25,.9,.3,\'Basket 2\')\\nbasket2.SetFillColor(basketcolor)\\nbasket2.Draw()\\n\\noffset = TPaveText(.55,.15,.9,.2)\\noffset.AddText(\'Offset of events in fBuffer\')\\noffset.AddText(\'Array of fEventOffsetLen Integers\')\\noffset.AddText(\'(if variable length structure)\')\\noffset.SetFillColor(offsetcolor)\\noffset.Draw()\\nbuffer = TPaveText(.55,.05,.9,.1)\\nbuffer.AddText(\'Basket buffer\')\\nbuffer.AddText(\'Array of fBasketSize chars\')\\nbuffer.SetFillColor(offsetcolor)\\nbuffer.Draw()\\nzipbuffer = TPaveText(.55,-.05,.75,.0)\\nzipbuffer.AddText(\'Basket compressed buffer\')\\nzipbuffer.AddText(\'(if compression)\')\\nzipbuffer.SetFillColor(offsetcolor)\\nzipbuffer.Draw()\\nar1 = TArrow()\\nar1.SetLineWidth(2)\\nar1.SetLineColor(1)\\nar1.SetFillStyle(1001)\\nar1.SetFillColor(1)\\nar1.DrawArrow(.21,.275,.39,.275,0.015,\'|>\')\\nar1.DrawArrow(.23,.375,.39,.375,0.015,\'|>\')\\nar1.DrawArrow(.25,.775,.39,.775,0.015,\'|>\')\\nar1.DrawArrow(.50,.775,.59,.775,0.015,\'|>\')\\nar1.DrawArrow(.70,.775,.79,.775,0.015,\'|>\')\\nar1.DrawArrow(.50,.275,.59,.275,0.015,\'|>\')\\nar1.DrawArrow(.70,.275,.79,.275,0.015,\'|>\')\\nar1.DrawArrow(.45,.175,.54,.175,0.015,\'|>\')\\nar1.DrawArrow(.43,.075,.54,.075,0.015,\'|>\')\\nar1.DrawArrow(.41,-.025,.54,-.025,0.015,\'|>\')\\nldot = TLine(.95,.92,.99,.92)\\nldot.SetLineStyle(3)\\nldot.Draw()\\nldot.DrawLine(.9,.775,.99,.775)\\nldot.DrawLine(.9,.275,.99,.275)\\nldot.DrawLine(.55,.05,.55,0)\\nldot.DrawLine(.9,.05,.75,0)\\npname = TText(.46,.21,\'fEventOffset\')\\npname.SetTextFont(72)\\npname.SetTextSize(0.018)\\npname.Draw()\\npname.DrawText(.44,.11,\'fBuffer\')\\npname.DrawText(.42,.01,\'fZipBuffer\')\\npname.DrawText(.26,.81,\'fLeaves = TObjArray of TLeaf\')\\npname.DrawText(.24,.40,\'fBasketEvent\')\\npname.DrawText(.22,.31,\'fBaskets = TObjArray of TBasket\')\\npname.DrawText(.20,1.0,\'fBranches = TObjArray of TBranch\')\\nntleaf = TPaveText(0.30,.42,.62,.7)\\nntleaf.SetTextSize(0.014)\\nntleaf.SetFillColor(leafcolor)\\nntleaf.SetTextAlign(12)\\nntleaf.AddText(\'fLen: number of fixed elements\')\\nntleaf.AddText(\'fLenType: number of bytes of data type\')\\nntleaf.AddText(\'fOffset: relative to Leaf0-fAddress\')\\nntleaf.AddText(\'fNbytesIO: number of bytes used for I/O\')\\nntleaf.AddText(\'fIsPointer: True if pointer\')\\nntleaf.AddText(\'fIsRange: True if leaf has a range\')\\nntleaf.AddText(\'fIsUnsigned: True if unsigned\')\\nntleaf.AddText(\'*fLeafCount: points to Leaf counter\')\\nntleaf.AddText(\' \')\\nntleaf.AddLine(0,0,0,0)\\nntleaf.AddText(\'fName = Leaf name\')\\nntleaf.AddText(\'fTitle = Leaf type (see Type codes)\')\\nntleaf.Draw()\\ntype = TPaveText(.65,.42,.95,.7)\\ntype.SetTextAlign(12)\\ntype.SetFillColor(leafcolor)\\ntype.AddText(\' \')\\ntype.AddText(\'C : a character string\')\\ntype.AddText(\'B : an 8 bit signed integer\')\\ntype.AddText(\'b : an 8 bit unsigned integer\')\\ntype.AddText(\'S : a 16 bit signed short integer\')\\ntype.AddText(\'s : a 16 bit unsigned short integer\')\\ntype.AddText(\'I : a 32 bit signed integer\')\\ntype.AddText(\'i : a 32 bit unsigned integer\')\\ntype.AddText(\'F : a 32 bit floating point\')\\ntype.AddText(\'D : a 64 bit floating point\')\\ntype.AddText(\'TXXXX : a class name TXXXX\')\\ntype.Draw()\\ntypecode = TPaveLabel(.7,.68,.9,.72,\'fType codes\')\\ntypecode.SetFillColor(leafcolor)\\ntypecode.Draw()\\nldot.DrawLine(.4,.75,.30,.7)\\nldot.DrawLine(.5,.75,.62,.7)\\nntbasket = TPaveText(0.02,-0.07,0.35,.25)\\nntbasket.SetFillColor(basketcolor)\\nntbasket.SetTextSize(0.014)\\nntbasket.SetTextAlign(12)\\nntbasket.AddText(\'fNbytes: Size of compressed Basket\')\\nntbasket.AddText(\'fObjLen: Size of uncompressed Basket\')\\nntbasket.AddText(\'fDatime: Date/Time when written to store\')\\nntbasket.AddText(\'fKeylen: Number of bytes for the key\')\\nntbasket.AddText(\'fCycle : Cycle number\')\\nntbasket.AddText(\'fSeekKey: Pointer to Basket on file\')\\nntbasket.AddText(\'fSeekPdir: Pointer to directory on file\')\\nntbasket.AddText(\\"fClassName: \'TBasket\'\\")\\nntbasket.AddText(\'fName: Branch name\')\\nntbasket.AddText(\'fTitle: Tree name\')\\nntbasket.AddText(\' \')\\nntbasket.AddLine(0,0,0,0)\\nntbasket.AddText(\'fNevBuf: Number of events in Basket\')\\nntbasket.AddText(\'fLast: pointer to last used byte in Basket\')\\nntbasket.Draw()\\nldot.DrawLine(.4,.3,0.02,0.25)\\nldot.DrawLine(.5,.25,0.35,-.07)\\nldot.DrawLine(.5,.3,0.35,0.25)\\nntbranch = TPaveText(0.02,0.40,0.18,0.68)\\nntbranch.SetFillColor(branchcolor)\\nntbranch.SetTextSize(0.015)\\nntbranch.SetTextAlign(12)\\nntbranch.AddText(\'fBasketSize\')\\nntbranch.AddText(\'fEventOffsetLen\')\\nntbranch.AddText(\'fMaxBaskets\')\\nntbranch.AddText(\'fEntries\')\\nntbranch.AddText(\'fAddress of Leaf0\')\\nntbranch.AddText(\' \')\\nntbranch.AddLine(0,0,0,0)\\nntbranch.AddText(\'fName: Branchname\')\\nntbranch.AddText(\'fTitle: leaflist\')\\nntbranch.Draw()\\nldot.DrawLine(.2,.97,.02,.68)\\nldot.DrawLine(.35,.97,.18,.68)\\nldot.DrawLine(.35,.87,.18,.40)\\nbasketstore = TPavesText(.8,-0.088,0.952,-0.0035,7,\'tr\')\\nbasketstore.SetFillColor(28)\\nbasketstore.AddText(\'Baskets\')\\nbasketstore.AddText(\'Stores\')\\nbasketstore.Draw()\\nc1.Update()\\n\\ngBenchmark.Show(\'tree\')\\n" }\n'
line: b'{ "repo_name": "schleichdi2/OPENNFR-6.3-CORE", "ref": "refs/heads/nextp3-ssl111", "path": "bitbake/lib/toaster/toastermain/settings_test.py", "content": "#\\n# BitBake Toaster Implementation\\n#\\n# Copyright (C) 2016        Intel Corporation\\n#\\n# SPDX-License-Identifier: GPL-2.0-only\\n#\\n\\n# Django settings for Toaster project.\\n\\n# Settings overlay to use for running tests\\n# DJANGO_SETTINGS_MODULE=toastermain.settings-test\\n\\nfrom toastermain.settings import *\\n\\nDEBUG = True\\nTEMPLATE_DEBUG = DEBUG\\n\\nDATABASES = {\\n    \'default\': {\\n        \'ENGINE\': \'django.db.backends.sqlite3\',\\n        \'NAME\': \'/tmp/toaster-test-db.sqlite\',\\n        \'TEST\': {\\n            \'ENGINE\': \'django.db.backends.sqlite3\',\\n            \'NAME\': \'/tmp/toaster-test-db.sqlite\',\\n      }\\n  }\\n}\\n" }\n'
line: b'{ "repo_name": "cselis86/edx-platform", "ref": "refs/heads/installer", "path": "openedx/core/djangoapps/course_groups/management/commands/remove_users_from_multiple_cohorts.py", "content": "\\"\\"\\"\\nScript for removing users with multiple cohorts of a course from cohorts\\nto ensure user\'s uniqueness for a course cohorts\\n\\"\\"\\"\\nfrom django.contrib.auth.models import User\\nfrom django.core.management.base import BaseCommand\\nfrom django.db.models import Count\\n\\nfrom openedx.core.djangoapps.course_groups.models import CourseUserGroup\\n\\n\\nclass Command(BaseCommand):\\n    \\"\\"\\"\\n    Remove users with multiple cohorts of a course from all cohorts\\n    \\"\\"\\"\\n    help = \'Remove all users from multiple cohorts (except one) of each course\'\\n\\n    def handle(self, *args, **options):\\n        \\"\\"\\"\\n        Execute the command\\n        \\"\\"\\"\\n        # Get entries of cohorts which have same user added multiple times for a single course\\n        multiple_objects_cohorts = CourseUserGroup.objects.filter(group_type=CourseUserGroup.COHORT).\\\\\\n            values_list(\'users\', \'course_id\').annotate(user_count=Count(\'users\')).filter(user_count__gt=1).\\\\\\n            order_by(\'users\')\\n        multiple_objects_cohorts_count = multiple_objects_cohorts.count()\\n        multiple_course_cohorts_users = set(multiple_objects_cohorts.values_list(\'users\', flat=True))\\n        users_failed_to_cleanup = []\\n\\n        for user in User.objects.filter(id__in=multiple_course_cohorts_users):\\n            print u\\"Removing user with id \'{0}\' from cohort groups\\".format(user.id)\\n            try:\\n                # remove user from only cohorts\\n                user.course_groups.remove(*user.course_groups.filter(group_type=CourseUserGroup.COHORT))\\n            except AttributeError as err:\\n                users_failed_to_cleanup.append(user.email)\\n                print u\\"Failed to remove user with id {0} from cohort groups, error: {1}\\".format(user.id, err)\\n\\n        print \\"=\\" * 80\\n        print u\\"=\\" * 30 + u\\"> Cohorts summary\\"\\n        print(\\n            u\\"Total number of CourseUserGroup of type \'{0}\' with multiple users: {1}\\".format(\\n                CourseUserGroup.COHORT, multiple_objects_cohorts_count\\n            )\\n        )\\n        print(\\n            u\\"Total number of unique users with multiple course cohorts: {0}\\".format(\\n                len(multiple_course_cohorts_users)\\n            )\\n        )\\n        print(\\n            u\\"Users which failed on cohorts cleanup [{0}]: [{1}]\\".format(\\n                len(users_failed_to_cleanup), (\', \'.join(users_failed_to_cleanup))\\n            )\\n        )\\n        print \\"=\\" * 80\\n" }\n'
line: b'{ "repo_name": "thinker3197/zeroclickinfo-fathead", "ref": "refs/heads/jquery-fathead-issue319", "path": "lib/fathead/firefox_about_config/parse.py", "content": "#!/usr/bin/env python2\\n\\nfrom BeautifulSoup import BeautifulSoup, NavigableString\\nimport urllib\\nimport string\\nimport re\\n\\n\\nclass Entry(object):\\n    def __init__(self, name, value, description, url):\\n        self.name = name\\n        self.value = value\\n        self.description = description\\n        self.url = url\\n\\n    def __str__(self):\\n        fields = [\\n                self.name,              # title\\n                \'A\',                    # type\\n                \'\',                     # redirect\\n                \'\',                     # otheruses\\n                \'\',                     # categories\\n                \'\',                     # references\\n                \'\',                     # see_also\\n                \'\',                     # further_reading\\n                \'\',                     # external_links\\n                \'\',                     # disambiguation\\n                \'\',                     # images\\n                self.description,       # abstract\\n                self.url                # source_url\\n                ]\\n        return \'%s\' % (\'\\\\t\'.join(fields))\\n\\n\\nclass Parser(object):\\n    def __init__(self, input=\'download/About:config_entries\'):\\n        self.soup = BeautifulSoup(open(input))\\n        # Requires trailing / for relative link replacement\\n        self.baseURL = \\"http://kb.mozillazine.org/\\"\\n\\n    def findEntries(self):\\n        self.entries = []\\n        headers = map(lambda x: x.string, self.soup.findAll(\'h1\')[2:])\\n        table = self.soup.findAll(\'div\', id=\\"bodyContent\\")[0]\\n        for table in table.findAll(\'table\'):\\n            header = True\\n            for tr in table.findAll(\'tr\'):\\n                if header:\\n                    header = False\\n                    continue\\n                i = 0\\n                for th in tr.findAll(\'td\'):\\n                    description = \'\'\\n                    if i == 0:\\n                        name = \'\'.join(th.b.findAll(text=True)).replace(\' \',\'\')\\n                        anchor = string.capitalize(urllib.quote(name.split(\'.\')[0])) + \\".\\"\\n                        if anchor in headers:\\n                            url = self.baseURL + \'About:config_entries#\' + anchor\\n                        else:\\n                            url = self.baseURL + \'About:config_entries\'\\n                    elif i == 1:\\n                        value = th.text\\n                    elif i == 2:\\n                        if value:\\n                            article = \'a\'\\n                            if value[0] == \'I\': article += \'n\'\\n                            optionType = \\"it accepts \\" + article + \\" \\" + value.lower() + \\".\\"\\n                        synopsis = \'\\"\' + name + \'\\"\'  + \' is a configuration option \' \\\\\\n                                \'for the Firefox web browser; \' + optionType + \\"<br>\\"\\n                        for tag in th.findAll(\'br\'):\\n                            tag.insert(0, NavigableString(\\"\\\\n\\"))\\n                        description = \'\'.join(th.findAll(text=True))\\n                        description = description.rstrip().replace(\'\\\\n\', \'<br>\').strip()\\n                        expandedURL = \'href=\\"\' + self.baseURL\\n                        description = description.replace(\'href=\\"/\', expandedURL)\\n                        description = re.sub(\'<\\\\s*b\\\\s*>\', \'<i>\', description)\\n                        description = re.sub(\'<\\\\s*/\\\\s*b\\\\s*>\', \'</i>\', description)\\n                        description = \'<blockquote>\' + description + \'</blockquote>\'\\n                        description = synopsis + description\\n                        i = -1\\n                        self.entries.append(Entry(name, value, description.strip(), url))\\n                    i += 1\\n\\n\\nif __name__ == \\"__main__\\":\\n    parser = Parser()\\n    parser.findEntries()\\n    with open(\'output.txt\', \'w\') as file:\\n        for entry in parser.entries:\\n            file.write(entry.__str__().encode(\'UTF-8\') + \'\\\\n\')\\n" }\n'
line: b'{ "repo_name": "hadesbox/luigi", "ref": "refs/heads/cloud-tasks", "path": "examples/foo.py", "content": "# -*- coding: utf-8 -*-\\n#\\n# Copyright 2012-2015 Spotify AB\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n# http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n\\nimport os\\nimport shutil\\nimport time\\n\\nimport luigi\\n\\n\\nclass MyExternal(luigi.ExternalTask):\\n\\n    def complete(self):\\n        return False\\n\\n\\nclass Foo(luigi.Task):\\n\\n    def run(self):\\n        print \\"Running Foo\\"\\n\\n    def requires(self):\\n        #        yield MyExternal()\\n        for i in xrange(10):\\n            yield Bar(i)\\n\\n\\nclass Bar(luigi.Task):\\n    num = luigi.IntParameter()\\n\\n    def run(self):\\n        time.sleep(1)\\n        self.output().open(\'w\').close()\\n\\n    def output(self):\\n        \\"\\"\\"\\n        Returns the target output for this task.\\n\\n        :return: the target output for this task.\\n        :rtype: object (:py:class:`~luigi.target.Target`)\\n        \\"\\"\\"\\n        time.sleep(1)\\n        return luigi.LocalTarget(\'/tmp/bar/%d\' % self.num)\\n\\n\\nif __name__ == \\"__main__\\":\\n    if os.path.exists(\'/tmp/bar\'):\\n        shutil.rmtree(\'/tmp/bar\')\\n\\n    luigi.run([\'--task\', \'Foo\', \'--workers\', \'2\'], use_optparse=True)\\n" }\n'
line: b'{ "repo_name": "Linaro/lava-dispatcher", "ref": "refs/heads/release", "path": "lava_dispatcher/utils/decorator.py", "content": "# -*- coding: utf-8 -*-\\n#\\n# Copyright (C) 2017 Linaro Limited\\n#\\n# Author: Remi Duraffort <remi.duraffort@linaro.org>\\n#\\n# This file is part of LAVA Dispatcher.\\n#\\n# LAVA Dispatcher is free software; you can redistribute it and/or modify\\n# it under the terms of the GNU General Public License as published by\\n# the Free Software Foundation; either version 2 of the License, or\\n# (at your option) any later version.\\n#\\n# LAVA Dispatcher is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU General Public License for more details.\\n#\\n# You should have received a copy of the GNU General Public License\\n# along with this program; if not, see <http://www.gnu.org/licenses>.\\n\\nfrom functools import wraps\\n\\n\\ndef replace_exception(cls_from, cls_to):\\n    def replace_exception_wrapper(func):\\n        @wraps(func)\\n        def function_wrapper(*args, **kwargs):\\n            try:\\n                return func(*args, **kwargs)\\n            except cls_from as exc:\\n                raise cls_to(exc)\\n        return function_wrapper\\n    return replace_exception_wrapper\\n" }\n'
line: b'{ "repo_name": "ubc/edx-platform", "ref": "refs/heads/release", "path": "openedx/core/djangoapps/course_groups/management/commands/remove_users_from_multiple_cohorts.py", "content": "\\"\\"\\"\\nScript for removing users with multiple cohorts of a course from cohorts\\nto ensure user\'s uniqueness for a course cohorts\\n\\"\\"\\"\\nfrom django.contrib.auth.models import User\\nfrom django.core.management.base import BaseCommand\\nfrom django.db.models import Count\\n\\nfrom openedx.core.djangoapps.course_groups.models import CourseUserGroup\\n\\n\\nclass Command(BaseCommand):\\n    \\"\\"\\"\\n    Remove users with multiple cohorts of a course from all cohorts\\n    \\"\\"\\"\\n    help = \'Remove all users from multiple cohorts (except one) of each course\'\\n\\n    def handle(self, *args, **options):\\n        \\"\\"\\"\\n        Execute the command\\n        \\"\\"\\"\\n        # Get entries of cohorts which have same user added multiple times for a single course\\n        multiple_objects_cohorts = CourseUserGroup.objects.filter(group_type=CourseUserGroup.COHORT).\\\\\\n            values_list(\'users\', \'course_id\').annotate(user_count=Count(\'users\')).filter(user_count__gt=1).\\\\\\n            order_by(\'users\')\\n        multiple_objects_cohorts_count = multiple_objects_cohorts.count()\\n        multiple_course_cohorts_users = set(multiple_objects_cohorts.values_list(\'users\', flat=True))\\n        users_failed_to_cleanup = []\\n\\n        for user in User.objects.filter(id__in=multiple_course_cohorts_users):\\n            print u\\"Removing user with id \'{0}\' from cohort groups\\".format(user.id)\\n            try:\\n                # remove user from only cohorts\\n                user.course_groups.remove(*user.course_groups.filter(group_type=CourseUserGroup.COHORT))\\n            except AttributeError as err:\\n                users_failed_to_cleanup.append(user.email)\\n                print u\\"Failed to remove user with id {0} from cohort groups, error: {1}\\".format(user.id, err)\\n\\n        print \\"=\\" * 80\\n        print u\\"=\\" * 30 + u\\"> Cohorts summary\\"\\n        print(\\n            u\\"Total number of CourseUserGroup of type \'{0}\' with multiple users: {1}\\".format(\\n                CourseUserGroup.COHORT, multiple_objects_cohorts_count\\n            )\\n        )\\n        print(\\n            u\\"Total number of unique users with multiple course cohorts: {0}\\".format(\\n                len(multiple_course_cohorts_users)\\n            )\\n        )\\n        print(\\n            u\\"Users which failed on cohorts cleanup [{0}]: [{1}]\\".format(\\n                len(users_failed_to_cleanup), (\', \'.join(users_failed_to_cleanup))\\n            )\\n        )\\n        print \\"=\\" * 80\\n" }\n'
line: b'{ "repo_name": "chand3040/cloud_that", "ref": "refs/heads/named-release/cypress.rc", "path": "openedx/core/djangoapps/course_groups/management/commands/remove_users_from_multiple_cohorts.py", "content": "\\"\\"\\"\\nScript for removing users with multiple cohorts of a course from cohorts\\nto ensure user\'s uniqueness for a course cohorts\\n\\"\\"\\"\\nfrom django.contrib.auth.models import User\\nfrom django.core.management.base import BaseCommand\\nfrom django.db.models import Count\\n\\nfrom openedx.core.djangoapps.course_groups.models import CourseUserGroup\\n\\n\\nclass Command(BaseCommand):\\n    \\"\\"\\"\\n    Remove users with multiple cohorts of a course from all cohorts\\n    \\"\\"\\"\\n    help = \'Remove all users from multiple cohorts (except one) of each course\'\\n\\n    def handle(self, *args, **options):\\n        \\"\\"\\"\\n        Execute the command\\n        \\"\\"\\"\\n        # Get entries of cohorts which have same user added multiple times for a single course\\n        multiple_objects_cohorts = CourseUserGroup.objects.filter(group_type=CourseUserGroup.COHORT).\\\\\\n            values_list(\'users\', \'course_id\').annotate(user_count=Count(\'users\')).filter(user_count__gt=1).\\\\\\n            order_by(\'users\')\\n        multiple_objects_cohorts_count = multiple_objects_cohorts.count()\\n        multiple_course_cohorts_users = set(multiple_objects_cohorts.values_list(\'users\', flat=True))\\n        users_failed_to_cleanup = []\\n\\n        for user in User.objects.filter(id__in=multiple_course_cohorts_users):\\n            print u\\"Removing user with id \'{0}\' from cohort groups\\".format(user.id)\\n            try:\\n                # remove user from only cohorts\\n                user.course_groups.remove(*user.course_groups.filter(group_type=CourseUserGroup.COHORT))\\n            except AttributeError as err:\\n                users_failed_to_cleanup.append(user.email)\\n                print u\\"Failed to remove user with id {0} from cohort groups, error: {1}\\".format(user.id, err)\\n\\n        print \\"=\\" * 80\\n        print u\\"=\\" * 30 + u\\"> Cohorts summary\\"\\n        print(\\n            u\\"Total number of CourseUserGroup of type \'{0}\' with multiple users: {1}\\".format(\\n                CourseUserGroup.COHORT, multiple_objects_cohorts_count\\n            )\\n        )\\n        print(\\n            u\\"Total number of unique users with multiple course cohorts: {0}\\".format(\\n                len(multiple_course_cohorts_users)\\n            )\\n        )\\n        print(\\n            u\\"Users which failed on cohorts cleanup [{0}]: [{1}]\\".format(\\n                len(users_failed_to_cleanup), (\', \'.join(users_failed_to_cleanup))\\n            )\\n        )\\n        print \\"=\\" * 80\\n" }\n'
line: b'{ "repo_name": "motion2015/a3", "ref": "refs/heads/a3", "path": "openedx/core/djangoapps/course_groups/management/commands/remove_users_from_multiple_cohorts.py", "content": "\\"\\"\\"\\nScript for removing users with multiple cohorts of a course from cohorts\\nto ensure user\'s uniqueness for a course cohorts\\n\\"\\"\\"\\nfrom django.contrib.auth.models import User\\nfrom django.core.management.base import BaseCommand\\nfrom django.db.models import Count\\n\\nfrom openedx.core.djangoapps.course_groups.models import CourseUserGroup\\n\\n\\nclass Command(BaseCommand):\\n    \\"\\"\\"\\n    Remove users with multiple cohorts of a course from all cohorts\\n    \\"\\"\\"\\n    help = \'Remove all users from multiple cohorts (except one) of each course\'\\n\\n    def handle(self, *args, **options):\\n        \\"\\"\\"\\n        Execute the command\\n        \\"\\"\\"\\n        # Get entries of cohorts which have same user added multiple times for a single course\\n        multiple_objects_cohorts = CourseUserGroup.objects.filter(group_type=CourseUserGroup.COHORT).\\\\\\n            values_list(\'users\', \'course_id\').annotate(user_count=Count(\'users\')).filter(user_count__gt=1).\\\\\\n            order_by(\'users\')\\n        multiple_objects_cohorts_count = multiple_objects_cohorts.count()\\n        multiple_course_cohorts_users = set(multiple_objects_cohorts.values_list(\'users\', flat=True))\\n        users_failed_to_cleanup = []\\n\\n        for user in User.objects.filter(id__in=multiple_course_cohorts_users):\\n            print u\\"Removing user with id \'{0}\' from cohort groups\\".format(user.id)\\n            try:\\n                # remove user from only cohorts\\n                user.course_groups.remove(*user.course_groups.filter(group_type=CourseUserGroup.COHORT))\\n            except AttributeError as err:\\n                users_failed_to_cleanup.append(user.email)\\n                print u\\"Failed to remove user with id {0} from cohort groups, error: {1}\\".format(user.id, err)\\n\\n        print \\"=\\" * 80\\n        print u\\"=\\" * 30 + u\\"> Cohorts summary\\"\\n        print(\\n            u\\"Total number of CourseUserGroup of type \'{0}\' with multiple users: {1}\\".format(\\n                CourseUserGroup.COHORT, multiple_objects_cohorts_count\\n            )\\n        )\\n        print(\\n            u\\"Total number of unique users with multiple course cohorts: {0}\\".format(\\n                len(multiple_course_cohorts_users)\\n            )\\n        )\\n        print(\\n            u\\"Users which failed on cohorts cleanup [{0}]: [{1}]\\".format(\\n                len(users_failed_to_cleanup), (\', \'.join(users_failed_to_cleanup))\\n            )\\n        )\\n        print \\"=\\" * 80\\n" }\n'
line: b'{ "repo_name": "kamalx/edx-platform", "ref": "refs/heads/release", "path": "openedx/core/djangoapps/course_groups/management/commands/remove_users_from_multiple_cohorts.py", "content": "\\"\\"\\"\\nScript for removing users with multiple cohorts of a course from cohorts\\nto ensure user\'s uniqueness for a course cohorts\\n\\"\\"\\"\\nfrom django.contrib.auth.models import User\\nfrom django.core.management.base import BaseCommand\\nfrom django.db.models import Count\\n\\nfrom openedx.core.djangoapps.course_groups.models import CourseUserGroup\\n\\n\\nclass Command(BaseCommand):\\n    \\"\\"\\"\\n    Remove users with multiple cohorts of a course from all cohorts\\n    \\"\\"\\"\\n    help = \'Remove all users from multiple cohorts (except one) of each course\'\\n\\n    def handle(self, *args, **options):\\n        \\"\\"\\"\\n        Execute the command\\n        \\"\\"\\"\\n        # Get entries of cohorts which have same user added multiple times for a single course\\n        multiple_objects_cohorts = CourseUserGroup.objects.filter(group_type=CourseUserGroup.COHORT).\\\\\\n            values_list(\'users\', \'course_id\').annotate(user_count=Count(\'users\')).filter(user_count__gt=1).\\\\\\n            order_by(\'users\')\\n        multiple_objects_cohorts_count = multiple_objects_cohorts.count()\\n        multiple_course_cohorts_users = set(multiple_objects_cohorts.values_list(\'users\', flat=True))\\n        users_failed_to_cleanup = []\\n\\n        for user in User.objects.filter(id__in=multiple_course_cohorts_users):\\n            print u\\"Removing user with id \'{0}\' from cohort groups\\".format(user.id)\\n            try:\\n                # remove user from only cohorts\\n                user.course_groups.remove(*user.course_groups.filter(group_type=CourseUserGroup.COHORT))\\n            except AttributeError as err:\\n                users_failed_to_cleanup.append(user.email)\\n                print u\\"Failed to remove user with id {0} from cohort groups, error: {1}\\".format(user.id, err)\\n\\n        print \\"=\\" * 80\\n        print u\\"=\\" * 30 + u\\"> Cohorts summary\\"\\n        print(\\n            u\\"Total number of CourseUserGroup of type \'{0}\' with multiple users: {1}\\".format(\\n                CourseUserGroup.COHORT, multiple_objects_cohorts_count\\n            )\\n        )\\n        print(\\n            u\\"Total number of unique users with multiple course cohorts: {0}\\".format(\\n                len(multiple_course_cohorts_users)\\n            )\\n        )\\n        print(\\n            u\\"Users which failed on cohorts cleanup [{0}]: [{1}]\\".format(\\n                len(users_failed_to_cleanup), (\', \'.join(users_failed_to_cleanup))\\n            )\\n        )\\n        print \\"=\\" * 80\\n" }\n'
line: b'{ "repo_name": "zzxuanyuan/root-compressor-dummy", "ref": "refs/heads/compressionbench", "path": "tutorials/pyroot/tree.py", "content": "## \\\\file\\n## \\\\ingroup tutorial_pyroot\\n## \\\\notebook\\n## This macro displays the Tree data structures\\n##\\n## \\\\macro_image\\n## \\\\macro_code\\n##\\n## \\\\author Wim Lavrijsen\\n\\nfrom ROOT import TCanvas, TPaveLabel, TPaveText, TPavesText, TText\\nfrom ROOT import TArrow, TLine\\nfrom ROOT import gROOT, gBenchmark\\n\\n#gROOT.Reset()\\n\\nc1 = TCanvas(\'c1\',\'Tree Data Structure\',200,10,750,940)\\nc1.Range(0,-0.1,1,1.15)\\n\\ngBenchmark.Start(\'tree\')\\n\\nbranchcolor = 26\\nleafcolor   = 30\\nbasketcolor = 42\\noffsetcolor = 43\\n#title = TPaveLabel(.3,1.05,.8,1.13,c1.GetTitle())\\ntitle = TPaveLabel(.3,1.05,.8,1.13,\'Tree Data Structure\')\\ntitle.SetFillColor(16)\\ntitle.Draw()\\ntree = TPaveText(.01,.75,.15,1.00)\\ntree.SetFillColor(18)\\ntree.SetTextAlign(12)\\ntnt = tree.AddText(\'Tree\')\\ntnt.SetTextAlign(22)\\ntnt.SetTextSize(0.030)\\ntree.AddText(\'fScanField\')\\ntree.AddText(\'fMaxEventLoop\')\\ntree.AddText(\'fMaxVirtualSize\')\\ntree.AddText(\'fEntries\')\\ntree.AddText(\'fDimension\')\\ntree.AddText(\'fSelectedRows\')\\ntree.Draw()\\nfarm = TPavesText(.01,1.02,.15,1.1,9,\'tr\')\\ntfarm = farm.AddText(\'CHAIN\')\\ntfarm.SetTextSize(0.024)\\nfarm.AddText(\'Collection\')\\nfarm.AddText(\'of Trees\')\\nfarm.Draw()\\nlink = TLine(.15,.92,.80,.92)\\nlink.SetLineWidth(2)\\nlink.SetLineColor(1)\\nlink.Draw()\\nlink.DrawLine(.21,.87,.21,.275)\\nlink.DrawLine(.23,.87,.23,.375)\\nlink.DrawLine(.25,.87,.25,.775)\\nlink.DrawLine(.41,.25,.41,-.025)\\nlink.DrawLine(.43,.25,.43,.075)\\nlink.DrawLine(.45,.25,.45,.175)\\nbranch0 = TPaveLabel(.20,.87,.35,.97,\'Branch 0\')\\nbranch0.SetTextSize(0.35)\\nbranch0.SetFillColor(branchcolor)\\nbranch0.Draw()\\nbranch1 = TPaveLabel(.40,.87,.55,.97,\'Branch 1\')\\nbranch1.SetTextSize(0.35)\\nbranch1.SetFillColor(branchcolor)\\nbranch1.Draw()\\nbranch2 = TPaveLabel(.60,.87,.75,.97,\'Branch 2\')\\nbranch2.SetTextSize(0.35)\\nbranch2.SetFillColor(branchcolor)\\nbranch2.Draw()\\nbranch3 = TPaveLabel(.80,.87,.95,.97,\'Branch 3\')\\nbranch3.SetTextSize(0.35)\\nbranch3.SetFillColor(branchcolor)\\nbranch3.Draw()\\nleaf0 = TPaveLabel(.4,.75,.5,.8,\'Leaf 0\')\\nleaf0.SetFillColor(leafcolor)\\nleaf0.Draw()\\nleaf1 = TPaveLabel(.6,.75,.7,.8,\'Leaf 1\')\\nleaf1.SetFillColor(leafcolor)\\nleaf1.Draw()\\nleaf2 = TPaveLabel(.8,.75,.9,.8,\'Leaf 2\')\\nleaf2.SetFillColor(leafcolor)\\nleaf2.Draw()\\nfirstevent = TPaveText(.4,.35,.9,.4)\\nfirstevent.AddText(\'First event of each basket\')\\nfirstevent.AddText(\'Array of fMaxBaskets Integers\')\\nfirstevent.SetFillColor(basketcolor)\\nfirstevent.Draw()\\nbasket0 = TPaveLabel(.4,.25,.5,.3,\'Basket 0\')\\nbasket0.SetFillColor(basketcolor)\\nbasket0.Draw()\\nbasket1 = TPaveLabel(.6,.25,.7,.3,\'Basket 1\')\\nbasket1.SetFillColor(basketcolor)\\nbasket1.Draw()\\nbasket2 = TPaveLabel(.8,.25,.9,.3,\'Basket 2\')\\nbasket2.SetFillColor(basketcolor)\\nbasket2.Draw()\\n\\noffset = TPaveText(.55,.15,.9,.2)\\noffset.AddText(\'Offset of events in fBuffer\')\\noffset.AddText(\'Array of fEventOffsetLen Integers\')\\noffset.AddText(\'(if variable length structure)\')\\noffset.SetFillColor(offsetcolor)\\noffset.Draw()\\nbuffer = TPaveText(.55,.05,.9,.1)\\nbuffer.AddText(\'Basket buffer\')\\nbuffer.AddText(\'Array of fBasketSize chars\')\\nbuffer.SetFillColor(offsetcolor)\\nbuffer.Draw()\\nzipbuffer = TPaveText(.55,-.05,.75,.0)\\nzipbuffer.AddText(\'Basket compressed buffer\')\\nzipbuffer.AddText(\'(if compression)\')\\nzipbuffer.SetFillColor(offsetcolor)\\nzipbuffer.Draw()\\nar1 = TArrow()\\nar1.SetLineWidth(2)\\nar1.SetLineColor(1)\\nar1.SetFillStyle(1001)\\nar1.SetFillColor(1)\\nar1.DrawArrow(.21,.275,.39,.275,0.015,\'|>\')\\nar1.DrawArrow(.23,.375,.39,.375,0.015,\'|>\')\\nar1.DrawArrow(.25,.775,.39,.775,0.015,\'|>\')\\nar1.DrawArrow(.50,.775,.59,.775,0.015,\'|>\')\\nar1.DrawArrow(.70,.775,.79,.775,0.015,\'|>\')\\nar1.DrawArrow(.50,.275,.59,.275,0.015,\'|>\')\\nar1.DrawArrow(.70,.275,.79,.275,0.015,\'|>\')\\nar1.DrawArrow(.45,.175,.54,.175,0.015,\'|>\')\\nar1.DrawArrow(.43,.075,.54,.075,0.015,\'|>\')\\nar1.DrawArrow(.41,-.025,.54,-.025,0.015,\'|>\')\\nldot = TLine(.95,.92,.99,.92)\\nldot.SetLineStyle(3)\\nldot.Draw()\\nldot.DrawLine(.9,.775,.99,.775)\\nldot.DrawLine(.9,.275,.99,.275)\\nldot.DrawLine(.55,.05,.55,0)\\nldot.DrawLine(.9,.05,.75,0)\\npname = TText(.46,.21,\'fEventOffset\')\\npname.SetTextFont(72)\\npname.SetTextSize(0.018)\\npname.Draw()\\npname.DrawText(.44,.11,\'fBuffer\')\\npname.DrawText(.42,.01,\'fZipBuffer\')\\npname.DrawText(.26,.81,\'fLeaves = TObjArray of TLeaf\')\\npname.DrawText(.24,.40,\'fBasketEvent\')\\npname.DrawText(.22,.31,\'fBaskets = TObjArray of TBasket\')\\npname.DrawText(.20,1.0,\'fBranches = TObjArray of TBranch\')\\nntleaf = TPaveText(0.30,.42,.62,.7)\\nntleaf.SetTextSize(0.014)\\nntleaf.SetFillColor(leafcolor)\\nntleaf.SetTextAlign(12)\\nntleaf.AddText(\'fLen: number of fixed elements\')\\nntleaf.AddText(\'fLenType: number of bytes of data type\')\\nntleaf.AddText(\'fOffset: relative to Leaf0-fAddress\')\\nntleaf.AddText(\'fNbytesIO: number of bytes used for I/O\')\\nntleaf.AddText(\'fIsPointer: True if pointer\')\\nntleaf.AddText(\'fIsRange: True if leaf has a range\')\\nntleaf.AddText(\'fIsUnsigned: True if unsigned\')\\nntleaf.AddText(\'*fLeafCount: points to Leaf counter\')\\nntleaf.AddText(\' \')\\nntleaf.AddLine(0,0,0,0)\\nntleaf.AddText(\'fName = Leaf name\')\\nntleaf.AddText(\'fTitle = Leaf type (see Type codes)\')\\nntleaf.Draw()\\ntype = TPaveText(.65,.42,.95,.7)\\ntype.SetTextAlign(12)\\ntype.SetFillColor(leafcolor)\\ntype.AddText(\' \')\\ntype.AddText(\'C : a character string\')\\ntype.AddText(\'B : an 8 bit signed integer\')\\ntype.AddText(\'b : an 8 bit unsigned integer\')\\ntype.AddText(\'S : a 16 bit signed short integer\')\\ntype.AddText(\'s : a 16 bit unsigned short integer\')\\ntype.AddText(\'I : a 32 bit signed integer\')\\ntype.AddText(\'i : a 32 bit unsigned integer\')\\ntype.AddText(\'F : a 32 bit floating point\')\\ntype.AddText(\'D : a 64 bit floating point\')\\ntype.AddText(\'TXXXX : a class name TXXXX\')\\ntype.Draw()\\ntypecode = TPaveLabel(.7,.68,.9,.72,\'fType codes\')\\ntypecode.SetFillColor(leafcolor)\\ntypecode.Draw()\\nldot.DrawLine(.4,.75,.30,.7)\\nldot.DrawLine(.5,.75,.62,.7)\\nntbasket = TPaveText(0.02,-0.07,0.35,.25)\\nntbasket.SetFillColor(basketcolor)\\nntbasket.SetTextSize(0.014)\\nntbasket.SetTextAlign(12)\\nntbasket.AddText(\'fNbytes: Size of compressed Basket\')\\nntbasket.AddText(\'fObjLen: Size of uncompressed Basket\')\\nntbasket.AddText(\'fDatime: Date/Time when written to store\')\\nntbasket.AddText(\'fKeylen: Number of bytes for the key\')\\nntbasket.AddText(\'fCycle : Cycle number\')\\nntbasket.AddText(\'fSeekKey: Pointer to Basket on file\')\\nntbasket.AddText(\'fSeekPdir: Pointer to directory on file\')\\nntbasket.AddText(\\"fClassName: \'TBasket\'\\")\\nntbasket.AddText(\'fName: Branch name\')\\nntbasket.AddText(\'fTitle: Tree name\')\\nntbasket.AddText(\' \')\\nntbasket.AddLine(0,0,0,0)\\nntbasket.AddText(\'fNevBuf: Number of events in Basket\')\\nntbasket.AddText(\'fLast: pointer to last used byte in Basket\')\\nntbasket.Draw()\\nldot.DrawLine(.4,.3,0.02,0.25)\\nldot.DrawLine(.5,.25,0.35,-.07)\\nldot.DrawLine(.5,.3,0.35,0.25)\\nntbranch = TPaveText(0.02,0.40,0.18,0.68)\\nntbranch.SetFillColor(branchcolor)\\nntbranch.SetTextSize(0.015)\\nntbranch.SetTextAlign(12)\\nntbranch.AddText(\'fBasketSize\')\\nntbranch.AddText(\'fEventOffsetLen\')\\nntbranch.AddText(\'fMaxBaskets\')\\nntbranch.AddText(\'fEntries\')\\nntbranch.AddText(\'fAddress of Leaf0\')\\nntbranch.AddText(\' \')\\nntbranch.AddLine(0,0,0,0)\\nntbranch.AddText(\'fName: Branchname\')\\nntbranch.AddText(\'fTitle: leaflist\')\\nntbranch.Draw()\\nldot.DrawLine(.2,.97,.02,.68)\\nldot.DrawLine(.35,.97,.18,.68)\\nldot.DrawLine(.35,.87,.18,.40)\\nbasketstore = TPavesText(.8,-0.088,0.952,-0.0035,7,\'tr\')\\nbasketstore.SetFillColor(28)\\nbasketstore.AddText(\'Baskets\')\\nbasketstore.AddText(\'Stores\')\\nbasketstore.Draw()\\nc1.Update()\\n\\ngBenchmark.Show(\'tree\')\\n" }\n'
line: b'{ "repo_name": "knehez/edx-platform", "ref": "refs/heads/memooc", "path": "openedx/core/djangoapps/course_groups/management/commands/remove_users_from_multiple_cohorts.py", "content": "\\"\\"\\"\\nScript for removing users with multiple cohorts of a course from cohorts\\nto ensure user\'s uniqueness for a course cohorts\\n\\"\\"\\"\\nfrom django.contrib.auth.models import User\\nfrom django.core.management.base import BaseCommand\\nfrom django.db.models import Count\\n\\nfrom openedx.core.djangoapps.course_groups.models import CourseUserGroup\\n\\n\\nclass Command(BaseCommand):\\n    \\"\\"\\"\\n    Remove users with multiple cohorts of a course from all cohorts\\n    \\"\\"\\"\\n    help = \'Remove all users from multiple cohorts (except one) of each course\'\\n\\n    def handle(self, *args, **options):\\n        \\"\\"\\"\\n        Execute the command\\n        \\"\\"\\"\\n        # Get entries of cohorts which have same user added multiple times for a single course\\n        multiple_objects_cohorts = CourseUserGroup.objects.filter(group_type=CourseUserGroup.COHORT).\\\\\\n            values_list(\'users\', \'course_id\').annotate(user_count=Count(\'users\')).filter(user_count__gt=1).\\\\\\n            order_by(\'users\')\\n        multiple_objects_cohorts_count = multiple_objects_cohorts.count()\\n        multiple_course_cohorts_users = set(multiple_objects_cohorts.values_list(\'users\', flat=True))\\n        users_failed_to_cleanup = []\\n\\n        for user in User.objects.filter(id__in=multiple_course_cohorts_users):\\n            print u\\"Removing user with id \'{0}\' from cohort groups\\".format(user.id)\\n            try:\\n                # remove user from only cohorts\\n                user.course_groups.remove(*user.course_groups.filter(group_type=CourseUserGroup.COHORT))\\n            except AttributeError as err:\\n                users_failed_to_cleanup.append(user.email)\\n                print u\\"Failed to remove user with id {0} from cohort groups, error: {1}\\".format(user.id, err)\\n\\n        print \\"=\\" * 80\\n        print u\\"=\\" * 30 + u\\"> Cohorts summary\\"\\n        print(\\n            u\\"Total number of CourseUserGroup of type \'{0}\' with multiple users: {1}\\".format(\\n                CourseUserGroup.COHORT, multiple_objects_cohorts_count\\n            )\\n        )\\n        print(\\n            u\\"Total number of unique users with multiple course cohorts: {0}\\".format(\\n                len(multiple_course_cohorts_users)\\n            )\\n        )\\n        print(\\n            u\\"Users which failed on cohorts cleanup [{0}]: [{1}]\\".format(\\n                len(users_failed_to_cleanup), (\', \'.join(users_failed_to_cleanup))\\n            )\\n        )\\n        print \\"=\\" * 80\\n" }\n'
line: b'{ "repo_name": "michael-dev2rights/ansible", "ref": "refs/heads/ansible-d2r", "path": "lib/ansible/module_utils/aruba.py", "content": "# This code is part of Ansible, but is an independent component.\\n# This particular file snippet, and this file snippet only, is BSD licensed.\\n# Modules you write using this snippet, which is embedded dynamically by Ansible\\n# still belong to the author of the module, and may assign their own license\\n# to the complete work.\\n#\\n# (c) 2016 Red Hat Inc.\\n#\\n# Redistribution and use in source and binary forms, with or without modification,\\n# are permitted provided that the following conditions are met:\\n#\\n#    * Redistributions of source code must retain the above copyright\\n#      notice, this list of conditions and the following disclaimer.\\n#    * Redistributions in binary form must reproduce the above copyright notice,\\n#      this list of conditions and the following disclaimer in the documentation\\n#      and/or other materials provided with the distribution.\\n#\\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \\"AS IS\\" AND\\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\n# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\\n# IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\\n# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\\n# USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n#\\nfrom ansible.module_utils._text import to_text\\nfrom ansible.module_utils.basic import env_fallback, return_values\\nfrom ansible.module_utils.network_common import to_list, ComplexList\\nfrom ansible.module_utils.connection import exec_command\\n\\n_DEVICE_CONFIGS = {}\\n\\naruba_provider_spec = {\\n    \'host\': dict(),\\n    \'port\': dict(type=\'int\'),\\n    \'username\': dict(fallback=(env_fallback, [\'ANSIBLE_NET_USERNAME\'])),\\n    \'password\': dict(fallback=(env_fallback, [\'ANSIBLE_NET_PASSWORD\']), no_log=True),\\n    \'ssh_keyfile\': dict(fallback=(env_fallback, [\'ANSIBLE_NET_SSH_KEYFILE\']), type=\'path\'),\\n    \'timeout\': dict(type=\'int\'),\\n}\\naruba_argument_spec = {\\n    \'provider\': dict(type=\'dict\', options=aruba_provider_spec)\\n}\\naruba_argument_spec.update(aruba_provider_spec)\\n\\n# Add argument\'s default value here\\nARGS_DEFAULT_VALUE = {}\\n\\n\\ndef get_argspec():\\n    return aruba_argument_spec\\n\\n\\ndef check_args(module, warnings):\\n    for key in aruba_argument_spec:\\n        if key not in [\'provider\', \'authorize\'] and module.params[key]:\\n            warnings.append(\'argument %s has been deprecated and will be removed in a future version\' % key)\\n\\n    # set argument\'s default value if not provided in input\\n    # This is done to avoid unwanted argument deprecation warning\\n    # in case argument is not given as input (outside provider).\\n    for key in ARGS_DEFAULT_VALUE:\\n        if not module.params.get(key, None):\\n            module.params[key] = ARGS_DEFAULT_VALUE[key]\\n\\n\\ndef get_config(module, flags=[]):\\n    cmd = \'show running-config \'\\n    cmd += \' \'.join(flags)\\n    cmd = cmd.strip()\\n\\n    try:\\n        return _DEVICE_CONFIGS[cmd]\\n    except KeyError:\\n        rc, out, err = exec_command(module, cmd)\\n        if rc != 0:\\n            module.fail_json(msg=\'unable to retrieve current config\', stderr=to_text(err, errors=\'surrogate_then_replace\'))\\n        cfg = to_text(out, errors=\'surrogate_then_replace\').strip()\\n        _DEVICE_CONFIGS[cmd] = cfg\\n        return cfg\\n\\n\\ndef to_commands(module, commands):\\n    spec = {\\n        \'command\': dict(key=True),\\n        \'prompt\': dict(),\\n        \'answer\': dict()\\n  }\\n    transform = ComplexList(spec, module)\\n    return transform(commands)\\n\\n\\ndef run_commands(module, commands, check_rc=True):\\n    responses = list()\\n    commands = to_commands(module, to_list(commands))\\n    for cmd in commands:\\n        cmd = module.jsonify(cmd)\\n        rc, out, err = exec_command(module, cmd)\\n        if check_rc and rc != 0:\\n            module.fail_json(msg=to_text(err, errors=\'surrogate_then_replace\'), rc=rc)\\n        responses.append(to_text(out, errors=\'surrogate_then_replace\'))\\n    return responses\\n\\n\\ndef load_config(module, commands):\\n\\n    rc, out, err = exec_command(module, \'configure terminal\')\\n    if rc != 0:\\n        module.fail_json(msg=\'unable to enter configuration mode\', err=to_text(out, errors=\'surrogate_then_replace\'))\\n\\n    for command in to_list(commands):\\n        if command == \'end\':\\n            continue\\n        rc, out, err = exec_command(module, command)\\n        if rc != 0:\\n            module.fail_json(msg=to_text(err, errors=\'surrogate_then_replace\'), command=command, rc=rc)\\n\\n    exec_command(module, \'end\')\\n" }\n'
line: b'{ "repo_name": "jrmendozat/mtvm", "ref": "refs/heads/nuevomaster", "path": "Segmento/migrations/0003_auto_20150320_0842.py", "content": "# -*- coding: utf-8 -*-\\nfrom __future__ import unicode_literals\\n\\nfrom django.db import models, migrations\\n\\n\\nclass Migration(migrations.Migration):\\n\\n    dependencies = [\\n        (\'Segmento\', \'0002_auto_20150313_1430\'),\\n    ]\\n\\n    operations = [\\n        migrations.AlterField(\\n            model_name=\'segmento\',\\n            name=\'segmento\',\\n            field=models.CharField(unique=True, max_length=50),\\n            preserve_default=True,\\n        ),\\n    ]\\n" }\n'
line: b'{ "repo_name": "SomethingExplosive/android_external_chromium_org", "ref": "refs/heads/somex-4.4", "path": "chrome/common/extensions/docs/server2/test_data/canned_data.py", "content": "# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\nimport json\\n\\nCANNED_CHANNELS = {\\n  \'trunk\': \'trunk\',\\n  \'dev\': 28,\\n  \'beta\': 27,\\n  \'stable\': 26\\n}\\n\\nCANNED_BRANCHES = {\\n  \'trunk\': \'trunk\',\\n  28: 1500,\\n  27: 1453,\\n  26: 1410,\\n  25: 1364,\\n  24: 1312,\\n  23: 1271,\\n  22: 1229,\\n  21: 1180,\\n  20: 1132,\\n  19: 1084,\\n  18: 1025,\\n  17: 963,\\n  16: 912,\\n  15: 874,\\n  14: 835,\\n  13: 782,\\n  12: 742,\\n  11: 696,\\n  10: 648,\\n   9: 597,\\n   8: 552,\\n   7: 544,\\n   6: 495,\\n   5: 396\\n}\\n\\nCANNED_TEST_FILE_SYSTEM_DATA = {\\n  \'api\': {\\n    \'_api_features.json\': json.dumps({\\n      \'ref_test\': { \'dependencies\': [\'permission:ref_test\'] }\\n      \'tester\': { \'dependencies\': [\'permission:tester\', \'manifest:tester\'] }\\n  }),\\n    \'_manifest_features.json\': json.dumps({\\n      \'manifest\': \'features\'\\n  }),\\n    \'_permission_features.json\': json.dumps({\\n      \'permission\': \'features\'\\n  })\\n}\\n  \'docs\': {\\n    \'templates\': {\\n      \'intros\': {\\n        \'test.html\': \'<h1>hi</h1>you<h2>first</h2><h3>inner</h3><h2>second</h2>\'\\n    }\\n      \'json\': {\\n        \'api_availabilities.json\': json.dumps({\\n          \'tester\': {\\n              \'channel\': \'stable\',\\n              \'version\': 42\\n          }\\n        }),\\n        \'intro_tables.json\': json.dumps({\\n          \'tester\': {\\n            \'Permissions\': [\\n            {\\n                \'class\': \'override\',\\n                \'text\': \'\\"tester\\"\'\\n            }\\n            {\\n                \'text\': \'is an API for testing things.\'\\n            }\\n            ],\\n            \'Learn More\': [\\n            {\\n                \'link\': \'https://tester.test.com/welcome.html\',\\n                \'text\': \'Welcome!\'\\n            }\\n            ]\\n        }\\n      })\\n    }\\n      \'private\': {\\n        \'intro_tables\': {\\n          \'trunk_message.html\': \'available on trunk\'\\n      }\\n    }\\n  }\\n}\\n}\\n\\nCANNED_API_FILE_SYSTEM_DATA = {\\n  \'trunk\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'events\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'history\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'falseBetaAPI\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'trunkAPI\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n    \'docs\': {\\n      \'templates\': {\\n        \'json\': {\\n          \'api_availabilities.json\': json.dumps({\\n            \'jsonAPI1\': {\\n              \'channel\': \'stable\',\\n              \'version\': 10\\n          }\\n            \'jsonAPI2\': {\\n              \'channel\': \'trunk\'\\n          }\\n            \'jsonAPI3\': {\\n              \'channel\': \'dev\'\\n          }\\n        }),\\n          \'intro_tables.json\': json.dumps({\\n            \'test\': [\\n            {\\n                \'Permissions\': \'probably none\'\\n            }\\n            ]\\n        })\\n      }\\n    }\\n  }\\n}\\n  \'1500\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1453\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.cpu\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1410\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1364\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1312\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1271\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'system_info_display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1229\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'system_info_display.idl\': \'systemInfo.display contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1180\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1132\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1084\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'contents\': \'nothing of interest here,really\'\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'1025\': {\\n    \'api\': {\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'963\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'912\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'experimental.webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'874\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'835\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'782\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'742\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'696\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'648\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'597\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'552\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'544\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'495\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'396\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'experimental.menus\'\\n      }\\n      ])\\n  }\\n}\\n}\\n" }\n'
line: b'{ "repo_name": "ModdedPA/android_external_chromium_org", "ref": "refs/heads/kitkat", "path": "chrome/common/extensions/docs/server2/test_data/canned_data.py", "content": "# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\nimport json\\n\\nCANNED_CHANNELS = {\\n  \'trunk\': \'trunk\',\\n  \'dev\': 28,\\n  \'beta\': 27,\\n  \'stable\': 26\\n}\\n\\nCANNED_BRANCHES = {\\n  \'trunk\': \'trunk\',\\n  28: 1500,\\n  27: 1453,\\n  26: 1410,\\n  25: 1364,\\n  24: 1312,\\n  23: 1271,\\n  22: 1229,\\n  21: 1180,\\n  20: 1132,\\n  19: 1084,\\n  18: 1025,\\n  17: 963,\\n  16: 912,\\n  15: 874,\\n  14: 835,\\n  13: 782,\\n  12: 742,\\n  11: 696,\\n  10: 648,\\n   9: 597,\\n   8: 552,\\n   7: 544,\\n   6: 495,\\n   5: 396\\n}\\n\\nCANNED_TEST_FILE_SYSTEM_DATA = {\\n  \'api\': {\\n    \'_api_features.json\': json.dumps({\\n      \'ref_test\': { \'dependencies\': [\'permission:ref_test\'] }\\n      \'tester\': { \'dependencies\': [\'permission:tester\', \'manifest:tester\'] }\\n  }),\\n    \'_manifest_features.json\': json.dumps({\\n      \'manifest\': \'features\'\\n  }),\\n    \'_permission_features.json\': json.dumps({\\n      \'permission\': \'features\'\\n  })\\n}\\n  \'docs\': {\\n    \'templates\': {\\n      \'intros\': {\\n        \'test.html\': \'<h1>hi</h1>you<h2>first</h2><h3>inner</h3><h2>second</h2>\'\\n    }\\n      \'json\': {\\n        \'api_availabilities.json\': json.dumps({\\n          \'tester\': {\\n              \'channel\': \'stable\',\\n              \'version\': 42\\n          }\\n        }),\\n        \'intro_tables.json\': json.dumps({\\n          \'tester\': {\\n            \'Permissions\': [\\n            {\\n                \'class\': \'override\',\\n                \'text\': \'\\"tester\\"\'\\n            }\\n            {\\n                \'text\': \'is an API for testing things.\'\\n            }\\n            ],\\n            \'Learn More\': [\\n            {\\n                \'link\': \'https://tester.test.com/welcome.html\',\\n                \'text\': \'Welcome!\'\\n            }\\n            ]\\n        }\\n      })\\n    }\\n      \'private\': {\\n        \'intro_tables\': {\\n          \'trunk_message.html\': \'available on trunk\'\\n      }\\n    }\\n  }\\n}\\n}\\n\\nCANNED_API_FILE_SYSTEM_DATA = {\\n  \'trunk\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'events\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'history\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'falseBetaAPI\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'trunkAPI\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n    \'docs\': {\\n      \'templates\': {\\n        \'json\': {\\n          \'api_availabilities.json\': json.dumps({\\n            \'jsonAPI1\': {\\n              \'channel\': \'stable\',\\n              \'version\': 10\\n          }\\n            \'jsonAPI2\': {\\n              \'channel\': \'trunk\'\\n          }\\n            \'jsonAPI3\': {\\n              \'channel\': \'dev\'\\n          }\\n        }),\\n          \'intro_tables.json\': json.dumps({\\n            \'test\': [\\n            {\\n                \'Permissions\': \'probably none\'\\n            }\\n            ]\\n        })\\n      }\\n    }\\n  }\\n}\\n  \'1500\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1453\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.cpu\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1410\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1364\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1312\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1271\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'system_info_display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1229\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'system_info_display.idl\': \'systemInfo.display contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1180\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1132\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1084\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'contents\': \'nothing of interest here,really\'\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'1025\': {\\n    \'api\': {\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'963\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'912\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'experimental.webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'874\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'835\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'782\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'742\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'696\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'648\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'597\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'552\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'544\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'495\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'396\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'experimental.menus\'\\n      }\\n      ])\\n  }\\n}\\n}\\n" }\n'
line: b'{ "repo_name": "timduru/platform-external-chromium_org", "ref": "refs/heads/katkiss-4.4", "path": "chrome/common/extensions/docs/server2/test_data/canned_data.py", "content": "# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\nimport json\\n\\nCANNED_CHANNELS = {\\n  \'trunk\': \'trunk\',\\n  \'dev\': 28,\\n  \'beta\': 27,\\n  \'stable\': 26\\n}\\n\\nCANNED_BRANCHES = {\\n  \'trunk\': \'trunk\',\\n  28: 1500,\\n  27: 1453,\\n  26: 1410,\\n  25: 1364,\\n  24: 1312,\\n  23: 1271,\\n  22: 1229,\\n  21: 1180,\\n  20: 1132,\\n  19: 1084,\\n  18: 1025,\\n  17: 963,\\n  16: 912,\\n  15: 874,\\n  14: 835,\\n  13: 782,\\n  12: 742,\\n  11: 696,\\n  10: 648,\\n   9: 597,\\n   8: 552,\\n   7: 544,\\n   6: 495,\\n   5: 396\\n}\\n\\nCANNED_TEST_FILE_SYSTEM_DATA = {\\n  \'api\': {\\n    \'_api_features.json\': json.dumps({\\n      \'ref_test\': { \'dependencies\': [\'permission:ref_test\'] }\\n      \'tester\': { \'dependencies\': [\'permission:tester\', \'manifest:tester\'] }\\n  }),\\n    \'_manifest_features.json\': json.dumps({\\n      \'manifest\': \'features\'\\n  }),\\n    \'_permission_features.json\': json.dumps({\\n      \'permission\': \'features\'\\n  })\\n}\\n  \'docs\': {\\n    \'templates\': {\\n      \'intros\': {\\n        \'test.html\': \'<h1>hi</h1>you<h2>first</h2><h3>inner</h3><h2>second</h2>\'\\n    }\\n      \'json\': {\\n        \'api_availabilities.json\': json.dumps({\\n          \'tester\': {\\n              \'channel\': \'stable\',\\n              \'version\': 42\\n          }\\n        }),\\n        \'intro_tables.json\': json.dumps({\\n          \'tester\': {\\n            \'Permissions\': [\\n            {\\n                \'class\': \'override\',\\n                \'text\': \'\\"tester\\"\'\\n            }\\n            {\\n                \'text\': \'is an API for testing things.\'\\n            }\\n            ],\\n            \'Learn More\': [\\n            {\\n                \'link\': \'https://tester.test.com/welcome.html\',\\n                \'text\': \'Welcome!\'\\n            }\\n            ]\\n        }\\n      })\\n    }\\n      \'private\': {\\n        \'intro_tables\': {\\n          \'trunk_message.html\': \'available on trunk\'\\n      }\\n    }\\n  }\\n}\\n}\\n\\nCANNED_API_FILE_SYSTEM_DATA = {\\n  \'trunk\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'events\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'history\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'falseBetaAPI\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'trunkAPI\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n    \'docs\': {\\n      \'templates\': {\\n        \'json\': {\\n          \'api_availabilities.json\': json.dumps({\\n            \'jsonAPI1\': {\\n              \'channel\': \'stable\',\\n              \'version\': 10\\n          }\\n            \'jsonAPI2\': {\\n              \'channel\': \'trunk\'\\n          }\\n            \'jsonAPI3\': {\\n              \'channel\': \'dev\'\\n          }\\n        }),\\n          \'intro_tables.json\': json.dumps({\\n            \'test\': [\\n            {\\n                \'Permissions\': \'probably none\'\\n            }\\n            ]\\n        })\\n      }\\n    }\\n  }\\n}\\n  \'1500\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1453\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.cpu\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1410\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1364\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1312\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1271\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'system_info_display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1229\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'system_info_display.idl\': \'systemInfo.display contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1180\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1132\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1084\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'contents\': \'nothing of interest here,really\'\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'1025\': {\\n    \'api\': {\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'963\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'912\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'experimental.webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'874\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'835\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'782\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'742\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'696\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'648\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'597\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'552\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'544\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'495\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'396\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'experimental.menus\'\\n      }\\n      ])\\n  }\\n}\\n}\\n" }\n'
line: b'{ "repo_name": "ustramooner/CouchPotato", "ref": "refs/heads/NzbIndexCom", "path": "library/sqlalchemy/connectors/mxodbc.py", "content": "\\"\\"\\"\\nProvide an SQLALchemy connector for the eGenix mxODBC commercial\\nPython adapter for ODBC. This is not a free product, but eGenix\\nprovides SQLAlchemy with a license for use in continuous integration\\ntesting.\\n\\nThis has been tested for use with mxODBC 3.1.2 on SQL Server 2005\\nand 2008, using the SQL Server Native driver. However, it is\\npossible for this to be used on other database platforms.\\n\\nFor more info on mxODBC, see http://www.egenix.com/\\n\\n\\"\\"\\"\\n\\nimport sys\\nimport re\\nimport warnings\\nfrom decimal import Decimal\\n\\nfrom sqlalchemy.connectors import Connector\\nfrom sqlalchemy import types as sqltypes\\nimport sqlalchemy.processors as processors\\n\\nclass MxODBCConnector(Connector):\\n    driver=\'mxodbc\'\\n    \\n    supports_sane_multi_rowcount = False\\n    supports_unicode_statements = False\\n    supports_unicode_binds = False\\n    \\n    supports_native_decimal = True\\n    \\n    @classmethod\\n    def dbapi(cls):\\n        # this classmethod will normally be replaced by an instance\\n        # attribute of the same name, so this is normally only called once.\\n        cls._load_mx_exceptions()\\n        platform = sys.platform\\n        if platform == \'win32\':\\n            from mx.ODBC import Windows as module\\n        # this can be the string \\"linux2\\", and possibly others\\n        elif \'linux\' in platform:\\n            from mx.ODBC import unixODBC as module\\n        elif platform == \'darwin\':\\n            from mx.ODBC import iODBC as module\\n        else:\\n            raise ImportError, \\"Unrecognized platform for mxODBC import\\"\\n        return module\\n\\n    @classmethod\\n    def _load_mx_exceptions(cls):\\n        \\"\\"\\" Import mxODBC exception classes into the module namespace,\\n        as if they had been imported normally. This is done here\\n        to avoid requiring all SQLAlchemy users to install mxODBC.\\n        \\"\\"\\"\\n        global InterfaceError, ProgrammingError\\n        from mx.ODBC import InterfaceError\\n        from mx.ODBC import ProgrammingError\\n\\n    def on_connect(self):\\n        def connect(conn):\\n            conn.stringformat = self.dbapi.MIXED_STRINGFORMAT\\n            conn.datetimeformat = self.dbapi.PYDATETIME_DATETIMEFORMAT\\n            conn.decimalformat = self.dbapi.DECIMAL_DECIMALFORMAT\\n            conn.errorhandler = self._error_handler()\\n        return connect\\n    \\n    def _error_handler(self):\\n        \\"\\"\\" Return a handler that adjusts mxODBC\'s raised Warnings to\\n        emit Python standard warnings.\\n        \\"\\"\\"\\n        from mx.ODBC.Error import Warning as MxOdbcWarning\\n        def error_handler(connection, cursor, errorclass, errorvalue):\\n\\n            if issubclass(errorclass, MxOdbcWarning):\\n                errorclass.__bases__ = (Warning,)\\n                warnings.warn(message=str(errorvalue),\\n                          category=errorclass,\\n                          stacklevel=2)\\n            else:\\n                raise errorclass, errorvalue\\n        return error_handler\\n\\n    def create_connect_args(self, url):\\n        \\"\\"\\" Return a tuple of *args,**kwargs for creating a connection.\\n\\n        The mxODBC 3.x connection constructor looks like this:\\n\\n            connect(dsn, user=\'\', password=\'\',\\n                    clear_auto_commit=1, errorhandler=None)\\n\\n        This method translates the values in the provided uri\\n        into args and kwargs needed to instantiate an mxODBC Connection.\\n\\n        The arg \'errorhandler\' is not used by SQLAlchemy and will\\n        not be populated.\\n        \\n        \\"\\"\\"\\n        opts = url.translate_connect_args(username=\'user\')\\n        opts.update(url.query)\\n        args = opts.pop(\'host\')\\n        opts.pop(\'port\', None)\\n        opts.pop(\'database\', None)\\n        return (args,), opts\\n\\n    def is_disconnect(self, e):\\n        # eGenix recommends checking connection.closed here,\\n        # but how can we get a handle on the current connection?\\n        if isinstance(e, self.dbapi.ProgrammingError):\\n            return \\"connection already closed\\" in str(e)\\n        elif isinstance(e, self.dbapi.Error):\\n            return \'[08S01]\' in str(e)\\n        else:\\n            return False\\n\\n    def _get_server_version_info(self, connection):\\n        # eGenix suggests using conn.dbms_version instead \\n        # of what we\'re doing here\\n        dbapi_con = connection.connection\\n        version = []\\n        r = re.compile(\'[.\\\\-]\')\\n        # 18 == pyodbc.SQL_DBMS_VER\\n        for n in r.split(dbapi_con.getinfo(18)[1]):\\n            try:\\n                version.append(int(n))\\n            except ValueError:\\n                version.append(n)\\n        return tuple(version)\\n\\n    def do_execute(self, cursor, statement, parameters, context=None):\\n        if context:\\n            native_odbc_execute = context.execution_options.\\\\\\n                                        get(\'native_odbc_execute\', \'auto\')\\n            if native_odbc_execute is True:\\n                # user specified native_odbc_execute=True\\n                cursor.execute(statement, parameters)\\n            elif native_odbc_execute is False:\\n                # user specified native_odbc_execute=False\\n                cursor.executedirect(statement, parameters)\\n            elif context.is_crud:\\n                # statement is UPDATE, DELETE, INSERT\\n                cursor.execute(statement, parameters)\\n            else:\\n                # all other statements\\n                cursor.executedirect(statement, parameters)\\n        else:\\n            cursor.executedirect(statement, parameters)\\n" }\n'
line: b'{ "repo_name": "msincenselee/vnpy", "ref": "refs/heads/vnpy2", "path": "vnpy/app/cta_strategy/strategies/multi_signal_strategy.py", "content": "from vnpy.app.cta_strategy import (\\n    StopOrder,\\n    TickData,\\n    BarData,\\n    TradeData,\\n    OrderData,\\n    BarGenerator,\\n    ArrayManager,\\n    CtaSignal,\\n    TargetPosTemplate\\n)\\n\\n\\nclass RsiSignal(CtaSignal):\\n    \\"\\"\\"\\"\\"\\"\\n\\n    def __init__(self, rsi_window: int, rsi_level: float):\\n        \\"\\"\\"Constructor\\"\\"\\"\\n        super().__init__()\\n\\n        self.rsi_window = rsi_window\\n        self.rsi_level = rsi_level\\n        self.rsi_long = 50 + self.rsi_level\\n        self.rsi_short = 50 - self.rsi_level\\n\\n        self.bg = BarGenerator(self.on_bar)\\n        self.am = ArrayManager()\\n\\n    def on_tick(self, tick: TickData):\\n        \\"\\"\\"\\n        Callback of new tick data update.\\n        \\"\\"\\"\\n        self.bg.update_tick(tick)\\n\\n    def on_bar(self, bar: BarData):\\n        \\"\\"\\"\\n        Callback of new bar data update.\\n        \\"\\"\\"\\n        self.am.update_bar(bar)\\n        if not self.am.inited:\\n            self.set_signal_pos(0)\\n\\n        rsi_value = self.am.rsi(self.rsi_window)\\n\\n        if rsi_value >= self.rsi_long:\\n            self.set_signal_pos(1)\\n        elif rsi_value <= self.rsi_short:\\n            self.set_signal_pos(-1)\\n        else:\\n            self.set_signal_pos(0)\\n\\n\\nclass CciSignal(CtaSignal):\\n    \\"\\"\\"\\"\\"\\"\\n\\n    def __init__(self, cci_window: int, cci_level: float):\\n        \\"\\"\\"\\"\\"\\"\\n        super().__init__()\\n\\n        self.cci_window = cci_window\\n        self.cci_level = cci_level\\n        self.cci_long = self.cci_level\\n        self.cci_short = -self.cci_level\\n\\n        self.bg = BarGenerator(self.on_bar)\\n        self.am = ArrayManager()\\n\\n    def on_tick(self, tick: TickData):\\n        \\"\\"\\"\\n        Callback of new tick data update.\\n        \\"\\"\\"\\n        self.bg.update_tick(tick)\\n\\n    def on_bar(self, bar: BarData):\\n        \\"\\"\\"\\n        Callback of new bar data update.\\n        \\"\\"\\"\\n        self.am.update_bar(bar)\\n        if not self.am.inited:\\n            self.set_signal_pos(0)\\n\\n        cci_value = self.am.cci(self.cci_window)\\n\\n        if cci_value >= self.cci_long:\\n            self.set_signal_pos(1)\\n        elif cci_value <= self.cci_short:\\n            self.set_signal_pos(-1)\\n        else:\\n            self.set_signal_pos(0)\\n\\n\\nclass MaSignal(CtaSignal):\\n    \\"\\"\\"\\"\\"\\"\\n\\n    def __init__(self, fast_window: int, slow_window: int):\\n        \\"\\"\\"\\"\\"\\"\\n        super().__init__()\\n\\n        self.fast_window = fast_window\\n        self.slow_window = slow_window\\n\\n        self.bg = BarGenerator(self.on_bar, 5, self.on_5min_bar)\\n        self.am = ArrayManager()\\n\\n    def on_tick(self, tick: TickData):\\n        \\"\\"\\"\\n        Callback of new tick data update.\\n        \\"\\"\\"\\n        self.bg.update_tick(tick)\\n\\n    def on_bar(self, bar: BarData):\\n        \\"\\"\\"\\n        Callback of new bar data update.\\n        \\"\\"\\"\\n        self.bg.update_bar(bar)\\n\\n    def on_5min_bar(self, bar: BarData):\\n        \\"\\"\\"\\"\\"\\"\\n        self.am.update_bar(bar)\\n        if not self.am.inited:\\n            self.set_signal_pos(0)\\n\\n        fast_ma = self.am.sma(self.fast_window)\\n        slow_ma = self.am.sma(self.slow_window)\\n\\n        if fast_ma > slow_ma:\\n            self.set_signal_pos(1)\\n        elif fast_ma < slow_ma:\\n            self.set_signal_pos(-1)\\n        else:\\n            self.set_signal_pos(0)\\n\\n\\nclass MultiSignalStrategy(TargetPosTemplate):\\n    \\"\\"\\"\\"\\"\\"\\n\\n    author = \\"\xe7\x94\xa8Python\xe7\x9a\x84\xe4\xba\xa4\xe6\x98\x93\xe5\x91\x98\\"\\n\\n    rsi_window = 14\\n    rsi_level = 20\\n    cci_window = 30\\n    cci_level = 10\\n    fast_window = 5\\n    slow_window = 20\\n\\n    signal_pos = {}\\n\\n    parameters = [\\"rsi_window\\", \\"rsi_level\\", \\"cci_window\\",\\n                  \\"cci_level\\", \\"fast_window\\", \\"slow_window\\"]\\n    variables = [\\"signal_pos\\", \\"target_pos\\"]\\n\\n    def __init__(self, cta_engine, strategy_name, vt_symbol, setting):\\n        \\"\\"\\"\\"\\"\\"\\n        super().__init__(cta_engine, strategy_name, vt_symbol, setting)\\n\\n        self.rsi_signal = RsiSignal(self.rsi_window, self.rsi_level)\\n        self.cci_signal = CciSignal(self.cci_window, self.cci_level)\\n        self.ma_signal = MaSignal(self.fast_window, self.slow_window)\\n\\n        self.signal_pos = {\\n            \\"rsi\\": 0,\\n            \\"cci\\": 0,\\n            \\"ma\\": 0\\n      }\\n\\n    def on_init(self):\\n        \\"\\"\\"\\n        Callback when strategy is inited.\\n        \\"\\"\\"\\n        self.write_log(\\"\xe7\xad\x96\xe7\x95\xa5\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\\")\\n        self.load_bar(10)\\n\\n    def on_start(self):\\n        \\"\\"\\"\\n        Callback when strategy is started.\\n        \\"\\"\\"\\n        self.write_log(\\"\xe7\xad\x96\xe7\x95\xa5\xe5\x90\xaf\xe5\x8a\xa8\\")\\n\\n    def on_stop(self):\\n        \\"\\"\\"\\n        Callback when strategy is stopped.\\n        \\"\\"\\"\\n        self.write_log(\\"\xe7\xad\x96\xe7\x95\xa5\xe5\x81\x9c\xe6\xad\xa2\\")\\n\\n    def on_tick(self, tick: TickData):\\n        \\"\\"\\"\\n        Callback of new tick data update.\\n        \\"\\"\\"\\n        super(MultiSignalStrategy, self).on_tick(tick)\\n\\n        self.rsi_signal.on_tick(tick)\\n        self.cci_signal.on_tick(tick)\\n        self.ma_signal.on_tick(tick)\\n\\n        self.calculate_target_pos()\\n\\n    def on_bar(self, bar: BarData):\\n        \\"\\"\\"\\n        Callback of new bar data update.\\n        \\"\\"\\"\\n        super(MultiSignalStrategy, self).on_bar(bar)\\n\\n        self.rsi_signal.on_bar(bar)\\n        self.cci_signal.on_bar(bar)\\n        self.ma_signal.on_bar(bar)\\n\\n        self.calculate_target_pos()\\n\\n    def calculate_target_pos(self):\\n        \\"\\"\\"\\"\\"\\"\\n        self.signal_pos[\\"rsi\\"] = self.rsi_signal.get_signal_pos()\\n        self.signal_pos[\\"cci\\"] = self.cci_signal.get_signal_pos()\\n        self.signal_pos[\\"ma\\"] = self.ma_signal.get_signal_pos()\\n\\n        target_pos = 0\\n        for v in self.signal_pos.values():\\n            target_pos += v\\n\\n        self.set_target_pos(target_pos)\\n\\n    def on_order(self, order: OrderData):\\n        \\"\\"\\"\\n        Callback of new order data update.\\n        \\"\\"\\"\\n        super(MultiSignalStrategy, self).on_order(order)\\n\\n    def on_trade(self, trade: TradeData):\\n        \\"\\"\\"\\n        Callback of new trade data update.\\n        \\"\\"\\"\\n        self.put_event()\\n\\n    def on_stop_order(self, stop_order: StopOrder):\\n        \\"\\"\\"\\n        Callback of stop order update.\\n        \\"\\"\\"\\n        pass\\n" }\n'
line: b'{ "repo_name": "JCROM-Android/jcrom_external_chromium_org", "ref": "refs/heads/kitkat", "path": "chrome/common/extensions/docs/server2/test_data/canned_data.py", "content": "# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\nimport json\\n\\nCANNED_CHANNELS = {\\n  \'trunk\': \'trunk\',\\n  \'dev\': 28,\\n  \'beta\': 27,\\n  \'stable\': 26\\n}\\n\\nCANNED_BRANCHES = {\\n  \'trunk\': \'trunk\',\\n  28: 1500,\\n  27: 1453,\\n  26: 1410,\\n  25: 1364,\\n  24: 1312,\\n  23: 1271,\\n  22: 1229,\\n  21: 1180,\\n  20: 1132,\\n  19: 1084,\\n  18: 1025,\\n  17: 963,\\n  16: 912,\\n  15: 874,\\n  14: 835,\\n  13: 782,\\n  12: 742,\\n  11: 696,\\n  10: 648,\\n   9: 597,\\n   8: 552,\\n   7: 544,\\n   6: 495,\\n   5: 396\\n}\\n\\nCANNED_TEST_FILE_SYSTEM_DATA = {\\n  \'api\': {\\n    \'_api_features.json\': json.dumps({\\n      \'ref_test\': { \'dependencies\': [\'permission:ref_test\'] }\\n      \'tester\': { \'dependencies\': [\'permission:tester\', \'manifest:tester\'] }\\n  }),\\n    \'_manifest_features.json\': json.dumps({\\n      \'manifest\': \'features\'\\n  }),\\n    \'_permission_features.json\': json.dumps({\\n      \'permission\': \'features\'\\n  })\\n}\\n  \'docs\': {\\n    \'templates\': {\\n      \'intros\': {\\n        \'test.html\': \'<h1>hi</h1>you<h2>first</h2><h3>inner</h3><h2>second</h2>\'\\n    }\\n      \'json\': {\\n        \'api_availabilities.json\': json.dumps({\\n          \'tester\': {\\n              \'channel\': \'stable\',\\n              \'version\': 42\\n          }\\n        }),\\n        \'intro_tables.json\': json.dumps({\\n          \'tester\': {\\n            \'Permissions\': [\\n            {\\n                \'class\': \'override\',\\n                \'text\': \'\\"tester\\"\'\\n            }\\n            {\\n                \'text\': \'is an API for testing things.\'\\n            }\\n            ],\\n            \'Learn More\': [\\n            {\\n                \'link\': \'https://tester.test.com/welcome.html\',\\n                \'text\': \'Welcome!\'\\n            }\\n            ]\\n        }\\n      })\\n    }\\n      \'private\': {\\n        \'intro_tables\': {\\n          \'trunk_message.html\': \'available on trunk\'\\n      }\\n    }\\n  }\\n}\\n}\\n\\nCANNED_API_FILE_SYSTEM_DATA = {\\n  \'trunk\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'events\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'history\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'falseBetaAPI\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'trunkAPI\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n    \'docs\': {\\n      \'templates\': {\\n        \'json\': {\\n          \'api_availabilities.json\': json.dumps({\\n            \'jsonAPI1\': {\\n              \'channel\': \'stable\',\\n              \'version\': 10\\n          }\\n            \'jsonAPI2\': {\\n              \'channel\': \'trunk\'\\n          }\\n            \'jsonAPI3\': {\\n              \'channel\': \'dev\'\\n          }\\n        }),\\n          \'intro_tables.json\': json.dumps({\\n            \'test\': [\\n            {\\n                \'Permissions\': \'probably none\'\\n            }\\n            ]\\n        })\\n      }\\n    }\\n  }\\n}\\n  \'1500\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1453\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.cpu\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1410\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1364\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1312\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1271\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'system_info_display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1229\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'system_info_display.idl\': \'systemInfo.display contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1180\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1132\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1084\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'contents\': \'nothing of interest here,really\'\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'1025\': {\\n    \'api\': {\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'963\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'912\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'experimental.webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'874\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'835\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'782\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'742\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'696\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'648\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'597\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'552\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'544\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'495\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'396\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'experimental.menus\'\\n      }\\n      ])\\n  }\\n}\\n}\\n" }\n'
line: b'{ "repo_name": "XXMrHyde/android_external_chromium_org", "ref": "refs/heads/darkkat-4.4", "path": "chrome/common/extensions/docs/server2/test_data/canned_data.py", "content": "# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\nimport json\\n\\nCANNED_CHANNELS = {\\n  \'trunk\': \'trunk\',\\n  \'dev\': 28,\\n  \'beta\': 27,\\n  \'stable\': 26\\n}\\n\\nCANNED_BRANCHES = {\\n  \'trunk\': \'trunk\',\\n  28: 1500,\\n  27: 1453,\\n  26: 1410,\\n  25: 1364,\\n  24: 1312,\\n  23: 1271,\\n  22: 1229,\\n  21: 1180,\\n  20: 1132,\\n  19: 1084,\\n  18: 1025,\\n  17: 963,\\n  16: 912,\\n  15: 874,\\n  14: 835,\\n  13: 782,\\n  12: 742,\\n  11: 696,\\n  10: 648,\\n   9: 597,\\n   8: 552,\\n   7: 544,\\n   6: 495,\\n   5: 396\\n}\\n\\nCANNED_TEST_FILE_SYSTEM_DATA = {\\n  \'api\': {\\n    \'_api_features.json\': json.dumps({\\n      \'ref_test\': { \'dependencies\': [\'permission:ref_test\'] }\\n      \'tester\': { \'dependencies\': [\'permission:tester\', \'manifest:tester\'] }\\n  }),\\n    \'_manifest_features.json\': json.dumps({\\n      \'manifest\': \'features\'\\n  }),\\n    \'_permission_features.json\': json.dumps({\\n      \'permission\': \'features\'\\n  })\\n}\\n  \'docs\': {\\n    \'templates\': {\\n      \'intros\': {\\n        \'test.html\': \'<h1>hi</h1>you<h2>first</h2><h3>inner</h3><h2>second</h2>\'\\n    }\\n      \'json\': {\\n        \'api_availabilities.json\': json.dumps({\\n          \'tester\': {\\n              \'channel\': \'stable\',\\n              \'version\': 42\\n          }\\n        }),\\n        \'intro_tables.json\': json.dumps({\\n          \'tester\': {\\n            \'Permissions\': [\\n            {\\n                \'class\': \'override\',\\n                \'text\': \'\\"tester\\"\'\\n            }\\n            {\\n                \'text\': \'is an API for testing things.\'\\n            }\\n            ],\\n            \'Learn More\': [\\n            {\\n                \'link\': \'https://tester.test.com/welcome.html\',\\n                \'text\': \'Welcome!\'\\n            }\\n            ]\\n        }\\n      })\\n    }\\n      \'private\': {\\n        \'intro_tables\': {\\n          \'trunk_message.html\': \'available on trunk\'\\n      }\\n    }\\n  }\\n}\\n}\\n\\nCANNED_API_FILE_SYSTEM_DATA = {\\n  \'trunk\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'events\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'history\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'falseBetaAPI\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'trunkAPI\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n    \'docs\': {\\n      \'templates\': {\\n        \'json\': {\\n          \'api_availabilities.json\': json.dumps({\\n            \'jsonAPI1\': {\\n              \'channel\': \'stable\',\\n              \'version\': 10\\n          }\\n            \'jsonAPI2\': {\\n              \'channel\': \'trunk\'\\n          }\\n            \'jsonAPI3\': {\\n              \'channel\': \'dev\'\\n          }\\n        }),\\n          \'intro_tables.json\': json.dumps({\\n            \'test\': [\\n            {\\n                \'Permissions\': \'probably none\'\\n            }\\n            ]\\n        })\\n      }\\n    }\\n  }\\n}\\n  \'1500\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1453\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.cpu\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1410\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1364\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1312\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1271\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'system_info_display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1229\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'system_info_display.idl\': \'systemInfo.display contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1180\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1132\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1084\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'contents\': \'nothing of interest here,really\'\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'1025\': {\\n    \'api\': {\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'963\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'912\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'experimental.webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'874\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'835\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'782\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'742\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'696\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'648\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'597\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'552\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'544\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'495\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'396\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'experimental.menus\'\\n      }\\n      ])\\n  }\\n}\\n}\\n" }\n'
line: b'{ "repo_name": "ThinkingBridge/platform_external_chromium_org", "ref": "refs/heads/kitkat", "path": "chrome/common/extensions/docs/server2/test_data/canned_data.py", "content": "# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\nimport json\\n\\nCANNED_CHANNELS = {\\n  \'trunk\': \'trunk\',\\n  \'dev\': 28,\\n  \'beta\': 27,\\n  \'stable\': 26\\n}\\n\\nCANNED_BRANCHES = {\\n  \'trunk\': \'trunk\',\\n  28: 1500,\\n  27: 1453,\\n  26: 1410,\\n  25: 1364,\\n  24: 1312,\\n  23: 1271,\\n  22: 1229,\\n  21: 1180,\\n  20: 1132,\\n  19: 1084,\\n  18: 1025,\\n  17: 963,\\n  16: 912,\\n  15: 874,\\n  14: 835,\\n  13: 782,\\n  12: 742,\\n  11: 696,\\n  10: 648,\\n   9: 597,\\n   8: 552,\\n   7: 544,\\n   6: 495,\\n   5: 396\\n}\\n\\nCANNED_TEST_FILE_SYSTEM_DATA = {\\n  \'api\': {\\n    \'_api_features.json\': json.dumps({\\n      \'ref_test\': { \'dependencies\': [\'permission:ref_test\'] }\\n      \'tester\': { \'dependencies\': [\'permission:tester\', \'manifest:tester\'] }\\n  }),\\n    \'_manifest_features.json\': json.dumps({\\n      \'manifest\': \'features\'\\n  }),\\n    \'_permission_features.json\': json.dumps({\\n      \'permission\': \'features\'\\n  })\\n}\\n  \'docs\': {\\n    \'templates\': {\\n      \'intros\': {\\n        \'test.html\': \'<h1>hi</h1>you<h2>first</h2><h3>inner</h3><h2>second</h2>\'\\n    }\\n      \'json\': {\\n        \'api_availabilities.json\': json.dumps({\\n          \'tester\': {\\n              \'channel\': \'stable\',\\n              \'version\': 42\\n          }\\n        }),\\n        \'intro_tables.json\': json.dumps({\\n          \'tester\': {\\n            \'Permissions\': [\\n            {\\n                \'class\': \'override\',\\n                \'text\': \'\\"tester\\"\'\\n            }\\n            {\\n                \'text\': \'is an API for testing things.\'\\n            }\\n            ],\\n            \'Learn More\': [\\n            {\\n                \'link\': \'https://tester.test.com/welcome.html\',\\n                \'text\': \'Welcome!\'\\n            }\\n            ]\\n        }\\n      })\\n    }\\n      \'private\': {\\n        \'intro_tables\': {\\n          \'trunk_message.html\': \'available on trunk\'\\n      }\\n    }\\n  }\\n}\\n}\\n\\nCANNED_API_FILE_SYSTEM_DATA = {\\n  \'trunk\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'events\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'history\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'falseBetaAPI\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'trunkAPI\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n    \'docs\': {\\n      \'templates\': {\\n        \'json\': {\\n          \'api_availabilities.json\': json.dumps({\\n            \'jsonAPI1\': {\\n              \'channel\': \'stable\',\\n              \'version\': 10\\n          }\\n            \'jsonAPI2\': {\\n              \'channel\': \'trunk\'\\n          }\\n            \'jsonAPI3\': {\\n              \'channel\': \'dev\'\\n          }\\n        }),\\n          \'intro_tables.json\': json.dumps({\\n            \'test\': [\\n            {\\n                \'Permissions\': \'probably none\'\\n            }\\n            ]\\n        })\\n      }\\n    }\\n  }\\n}\\n  \'1500\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1453\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.cpu\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1410\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1364\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1312\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1271\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'system_info_display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1229\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'system_info_display.idl\': \'systemInfo.display contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1180\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1132\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1084\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'contents\': \'nothing of interest here,really\'\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'1025\': {\\n    \'api\': {\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'963\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'912\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'experimental.webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'874\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'835\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'782\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'742\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'696\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'648\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'597\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'552\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'544\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'495\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'396\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'experimental.menus\'\\n      }\\n      ])\\n  }\\n}\\n}\\n" }\n'
line: b'{ "repo_name": "klusark/android_external_chromium_org", "ref": "refs/heads/cm-11.0", "path": "chrome/common/extensions/docs/server2/test_data/canned_data.py", "content": "# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\nimport json\\n\\nCANNED_CHANNELS = {\\n  \'trunk\': \'trunk\',\\n  \'dev\': 28,\\n  \'beta\': 27,\\n  \'stable\': 26\\n}\\n\\nCANNED_BRANCHES = {\\n  \'trunk\': \'trunk\',\\n  28: 1500,\\n  27: 1453,\\n  26: 1410,\\n  25: 1364,\\n  24: 1312,\\n  23: 1271,\\n  22: 1229,\\n  21: 1180,\\n  20: 1132,\\n  19: 1084,\\n  18: 1025,\\n  17: 963,\\n  16: 912,\\n  15: 874,\\n  14: 835,\\n  13: 782,\\n  12: 742,\\n  11: 696,\\n  10: 648,\\n   9: 597,\\n   8: 552,\\n   7: 544,\\n   6: 495,\\n   5: 396\\n}\\n\\nCANNED_TEST_FILE_SYSTEM_DATA = {\\n  \'api\': {\\n    \'_api_features.json\': json.dumps({\\n      \'ref_test\': { \'dependencies\': [\'permission:ref_test\'] }\\n      \'tester\': { \'dependencies\': [\'permission:tester\', \'manifest:tester\'] }\\n  }),\\n    \'_manifest_features.json\': json.dumps({\\n      \'manifest\': \'features\'\\n  }),\\n    \'_permission_features.json\': json.dumps({\\n      \'permission\': \'features\'\\n  })\\n}\\n  \'docs\': {\\n    \'templates\': {\\n      \'intros\': {\\n        \'test.html\': \'<h1>hi</h1>you<h2>first</h2><h3>inner</h3><h2>second</h2>\'\\n    }\\n      \'json\': {\\n        \'api_availabilities.json\': json.dumps({\\n          \'tester\': {\\n              \'channel\': \'stable\',\\n              \'version\': 42\\n          }\\n        }),\\n        \'intro_tables.json\': json.dumps({\\n          \'tester\': {\\n            \'Permissions\': [\\n            {\\n                \'class\': \'override\',\\n                \'text\': \'\\"tester\\"\'\\n            }\\n            {\\n                \'text\': \'is an API for testing things.\'\\n            }\\n            ],\\n            \'Learn More\': [\\n            {\\n                \'link\': \'https://tester.test.com/welcome.html\',\\n                \'text\': \'Welcome!\'\\n            }\\n            ]\\n        }\\n      })\\n    }\\n      \'private\': {\\n        \'intro_tables\': {\\n          \'trunk_message.html\': \'available on trunk\'\\n      }\\n    }\\n  }\\n}\\n}\\n\\nCANNED_API_FILE_SYSTEM_DATA = {\\n  \'trunk\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'events\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'history\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'falseBetaAPI\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'trunkAPI\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n    \'docs\': {\\n      \'templates\': {\\n        \'json\': {\\n          \'api_availabilities.json\': json.dumps({\\n            \'jsonAPI1\': {\\n              \'channel\': \'stable\',\\n              \'version\': 10\\n          }\\n            \'jsonAPI2\': {\\n              \'channel\': \'trunk\'\\n          }\\n            \'jsonAPI3\': {\\n              \'channel\': \'dev\'\\n          }\\n        }),\\n          \'intro_tables.json\': json.dumps({\\n            \'test\': [\\n            {\\n                \'Permissions\': \'probably none\'\\n            }\\n            ]\\n        })\\n      }\\n    }\\n  }\\n}\\n  \'1500\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1453\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.cpu\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1410\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1364\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1312\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1271\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'system_info_display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1229\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'system_info_display.idl\': \'systemInfo.display contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1180\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1132\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1084\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'contents\': \'nothing of interest here,really\'\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'1025\': {\\n    \'api\': {\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'963\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'912\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'experimental.webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'874\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'835\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'782\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'742\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'696\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'648\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'597\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'552\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'544\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'495\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'396\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'experimental.menus\'\\n      }\\n      ])\\n  }\\n}\\n}\\n" }\n'
line: b'{ "repo_name": "MotorolaMobilityLLC/external-chromium_org", "ref": "refs/heads/kitkat-mr1-release-falcon-gpe", "path": "chrome/common/extensions/docs/server2/test_data/canned_data.py", "content": "# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\nimport json\\n\\nCANNED_CHANNELS = {\\n  \'trunk\': \'trunk\',\\n  \'dev\': 28,\\n  \'beta\': 27,\\n  \'stable\': 26\\n}\\n\\nCANNED_BRANCHES = {\\n  \'trunk\': \'trunk\',\\n  28: 1500,\\n  27: 1453,\\n  26: 1410,\\n  25: 1364,\\n  24: 1312,\\n  23: 1271,\\n  22: 1229,\\n  21: 1180,\\n  20: 1132,\\n  19: 1084,\\n  18: 1025,\\n  17: 963,\\n  16: 912,\\n  15: 874,\\n  14: 835,\\n  13: 782,\\n  12: 742,\\n  11: 696,\\n  10: 648,\\n   9: 597,\\n   8: 552,\\n   7: 544,\\n   6: 495,\\n   5: 396\\n}\\n\\nCANNED_TEST_FILE_SYSTEM_DATA = {\\n  \'api\': {\\n    \'_api_features.json\': json.dumps({\\n      \'ref_test\': { \'dependencies\': [\'permission:ref_test\'] }\\n      \'tester\': { \'dependencies\': [\'permission:tester\', \'manifest:tester\'] }\\n  }),\\n    \'_manifest_features.json\': json.dumps({\\n      \'manifest\': \'features\'\\n  }),\\n    \'_permission_features.json\': json.dumps({\\n      \'permission\': \'features\'\\n  })\\n}\\n  \'docs\': {\\n    \'templates\': {\\n      \'intros\': {\\n        \'test.html\': \'<h1>hi</h1>you<h2>first</h2><h3>inner</h3><h2>second</h2>\'\\n    }\\n      \'json\': {\\n        \'api_availabilities.json\': json.dumps({\\n          \'tester\': {\\n              \'channel\': \'stable\',\\n              \'version\': 42\\n          }\\n        }),\\n        \'intro_tables.json\': json.dumps({\\n          \'tester\': {\\n            \'Permissions\': [\\n            {\\n                \'class\': \'override\',\\n                \'text\': \'\\"tester\\"\'\\n            }\\n            {\\n                \'text\': \'is an API for testing things.\'\\n            }\\n            ],\\n            \'Learn More\': [\\n            {\\n                \'link\': \'https://tester.test.com/welcome.html\',\\n                \'text\': \'Welcome!\'\\n            }\\n            ]\\n        }\\n      })\\n    }\\n      \'private\': {\\n        \'intro_tables\': {\\n          \'trunk_message.html\': \'available on trunk\'\\n      }\\n    }\\n  }\\n}\\n}\\n\\nCANNED_API_FILE_SYSTEM_DATA = {\\n  \'trunk\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'events\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'history\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'falseBetaAPI\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'trunkAPI\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n    \'docs\': {\\n      \'templates\': {\\n        \'json\': {\\n          \'api_availabilities.json\': json.dumps({\\n            \'jsonAPI1\': {\\n              \'channel\': \'stable\',\\n              \'version\': 10\\n          }\\n            \'jsonAPI2\': {\\n              \'channel\': \'trunk\'\\n          }\\n            \'jsonAPI3\': {\\n              \'channel\': \'dev\'\\n          }\\n        }),\\n          \'intro_tables.json\': json.dumps({\\n            \'test\': [\\n            {\\n                \'Permissions\': \'probably none\'\\n            }\\n            ]\\n        })\\n      }\\n    }\\n  }\\n}\\n  \'1500\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1453\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.cpu\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1410\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1364\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1312\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1271\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'system_info_display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1229\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'system_info_display.idl\': \'systemInfo.display contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1180\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1132\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1084\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'contents\': \'nothing of interest here,really\'\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'1025\': {\\n    \'api\': {\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'963\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'912\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'experimental.webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'874\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'835\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'782\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'742\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'696\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'648\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'597\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'552\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'544\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'495\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'396\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'experimental.menus\'\\n      }\\n      ])\\n  }\\n}\\n}\\n" }\n'
line: b'{ "repo_name": "KitKatXperience/platform_external_chromium_org", "ref": "refs/heads/kk", "path": "chrome/common/extensions/docs/server2/test_data/canned_data.py", "content": "# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\nimport json\\n\\nCANNED_CHANNELS = {\\n  \'trunk\': \'trunk\',\\n  \'dev\': 28,\\n  \'beta\': 27,\\n  \'stable\': 26\\n}\\n\\nCANNED_BRANCHES = {\\n  \'trunk\': \'trunk\',\\n  28: 1500,\\n  27: 1453,\\n  26: 1410,\\n  25: 1364,\\n  24: 1312,\\n  23: 1271,\\n  22: 1229,\\n  21: 1180,\\n  20: 1132,\\n  19: 1084,\\n  18: 1025,\\n  17: 963,\\n  16: 912,\\n  15: 874,\\n  14: 835,\\n  13: 782,\\n  12: 742,\\n  11: 696,\\n  10: 648,\\n   9: 597,\\n   8: 552,\\n   7: 544,\\n   6: 495,\\n   5: 396\\n}\\n\\nCANNED_TEST_FILE_SYSTEM_DATA = {\\n  \'api\': {\\n    \'_api_features.json\': json.dumps({\\n      \'ref_test\': { \'dependencies\': [\'permission:ref_test\'] }\\n      \'tester\': { \'dependencies\': [\'permission:tester\', \'manifest:tester\'] }\\n  }),\\n    \'_manifest_features.json\': json.dumps({\\n      \'manifest\': \'features\'\\n  }),\\n    \'_permission_features.json\': json.dumps({\\n      \'permission\': \'features\'\\n  })\\n}\\n  \'docs\': {\\n    \'templates\': {\\n      \'intros\': {\\n        \'test.html\': \'<h1>hi</h1>you<h2>first</h2><h3>inner</h3><h2>second</h2>\'\\n    }\\n      \'json\': {\\n        \'api_availabilities.json\': json.dumps({\\n          \'tester\': {\\n              \'channel\': \'stable\',\\n              \'version\': 42\\n          }\\n        }),\\n        \'intro_tables.json\': json.dumps({\\n          \'tester\': {\\n            \'Permissions\': [\\n            {\\n                \'class\': \'override\',\\n                \'text\': \'\\"tester\\"\'\\n            }\\n            {\\n                \'text\': \'is an API for testing things.\'\\n            }\\n            ],\\n            \'Learn More\': [\\n            {\\n                \'link\': \'https://tester.test.com/welcome.html\',\\n                \'text\': \'Welcome!\'\\n            }\\n            ]\\n        }\\n      })\\n    }\\n      \'private\': {\\n        \'intro_tables\': {\\n          \'trunk_message.html\': \'available on trunk\'\\n      }\\n    }\\n  }\\n}\\n}\\n\\nCANNED_API_FILE_SYSTEM_DATA = {\\n  \'trunk\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'events\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'history\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'falseBetaAPI\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'trunkAPI\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n    \'docs\': {\\n      \'templates\': {\\n        \'json\': {\\n          \'api_availabilities.json\': json.dumps({\\n            \'jsonAPI1\': {\\n              \'channel\': \'stable\',\\n              \'version\': 10\\n          }\\n            \'jsonAPI2\': {\\n              \'channel\': \'trunk\'\\n          }\\n            \'jsonAPI3\': {\\n              \'channel\': \'dev\'\\n          }\\n        }),\\n          \'intro_tables.json\': json.dumps({\\n            \'test\': [\\n            {\\n                \'Permissions\': \'probably none\'\\n            }\\n            ]\\n        })\\n      }\\n    }\\n  }\\n}\\n  \'1500\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1453\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.cpu\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1410\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1364\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1312\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1271\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'system_info_display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1229\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'system_info_display.idl\': \'systemInfo.display contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1180\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1132\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1084\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'contents\': \'nothing of interest here,really\'\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'1025\': {\\n    \'api\': {\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'963\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'912\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'experimental.webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'874\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'835\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'782\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'742\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'696\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'648\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'597\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'552\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'544\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'495\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'396\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'experimental.menus\'\\n      }\\n      ])\\n  }\\n}\\n}\\n" }\n'
line: b'{ "repo_name": "BigBrother1984/android_external_chromium_org", "ref": "refs/heads/kitkat", "path": "chrome/common/extensions/docs/server2/test_data/canned_data.py", "content": "# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\nimport json\\n\\nCANNED_CHANNELS = {\\n  \'trunk\': \'trunk\',\\n  \'dev\': 28,\\n  \'beta\': 27,\\n  \'stable\': 26\\n}\\n\\nCANNED_BRANCHES = {\\n  \'trunk\': \'trunk\',\\n  28: 1500,\\n  27: 1453,\\n  26: 1410,\\n  25: 1364,\\n  24: 1312,\\n  23: 1271,\\n  22: 1229,\\n  21: 1180,\\n  20: 1132,\\n  19: 1084,\\n  18: 1025,\\n  17: 963,\\n  16: 912,\\n  15: 874,\\n  14: 835,\\n  13: 782,\\n  12: 742,\\n  11: 696,\\n  10: 648,\\n   9: 597,\\n   8: 552,\\n   7: 544,\\n   6: 495,\\n   5: 396\\n}\\n\\nCANNED_TEST_FILE_SYSTEM_DATA = {\\n  \'api\': {\\n    \'_api_features.json\': json.dumps({\\n      \'ref_test\': { \'dependencies\': [\'permission:ref_test\'] }\\n      \'tester\': { \'dependencies\': [\'permission:tester\', \'manifest:tester\'] }\\n  }),\\n    \'_manifest_features.json\': json.dumps({\\n      \'manifest\': \'features\'\\n  }),\\n    \'_permission_features.json\': json.dumps({\\n      \'permission\': \'features\'\\n  })\\n}\\n  \'docs\': {\\n    \'templates\': {\\n      \'intros\': {\\n        \'test.html\': \'<h1>hi</h1>you<h2>first</h2><h3>inner</h3><h2>second</h2>\'\\n    }\\n      \'json\': {\\n        \'api_availabilities.json\': json.dumps({\\n          \'tester\': {\\n              \'channel\': \'stable\',\\n              \'version\': 42\\n          }\\n        }),\\n        \'intro_tables.json\': json.dumps({\\n          \'tester\': {\\n            \'Permissions\': [\\n            {\\n                \'class\': \'override\',\\n                \'text\': \'\\"tester\\"\'\\n            }\\n            {\\n                \'text\': \'is an API for testing things.\'\\n            }\\n            ],\\n            \'Learn More\': [\\n            {\\n                \'link\': \'https://tester.test.com/welcome.html\',\\n                \'text\': \'Welcome!\'\\n            }\\n            ]\\n        }\\n      })\\n    }\\n      \'private\': {\\n        \'intro_tables\': {\\n          \'trunk_message.html\': \'available on trunk\'\\n      }\\n    }\\n  }\\n}\\n}\\n\\nCANNED_API_FILE_SYSTEM_DATA = {\\n  \'trunk\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'events\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'history\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'falseBetaAPI\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'trunkAPI\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n    \'docs\': {\\n      \'templates\': {\\n        \'json\': {\\n          \'api_availabilities.json\': json.dumps({\\n            \'jsonAPI1\': {\\n              \'channel\': \'stable\',\\n              \'version\': 10\\n          }\\n            \'jsonAPI2\': {\\n              \'channel\': \'trunk\'\\n          }\\n            \'jsonAPI3\': {\\n              \'channel\': \'dev\'\\n          }\\n        }),\\n          \'intro_tables.json\': json.dumps({\\n            \'test\': [\\n            {\\n                \'Permissions\': \'probably none\'\\n            }\\n            ]\\n        })\\n      }\\n    }\\n  }\\n}\\n  \'1500\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1453\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.cpu\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1410\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1364\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1312\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1271\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'system_info_display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1229\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'system_info_display.idl\': \'systemInfo.display contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1180\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1132\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1084\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'contents\': \'nothing of interest here,really\'\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'1025\': {\\n    \'api\': {\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'963\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'912\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'experimental.webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'874\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'835\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'782\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'742\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'696\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'648\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'597\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'552\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'544\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'495\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'396\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'experimental.menus\'\\n      }\\n      ])\\n  }\\n}\\n}\\n" }\n'
line: b'{ "repo_name": "AOKP/external_chromium_org", "ref": "refs/heads/kitkat", "path": "chrome/common/extensions/docs/server2/test_data/canned_data.py", "content": "# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\nimport json\\n\\nCANNED_CHANNELS = {\\n  \'trunk\': \'trunk\',\\n  \'dev\': 28,\\n  \'beta\': 27,\\n  \'stable\': 26\\n}\\n\\nCANNED_BRANCHES = {\\n  \'trunk\': \'trunk\',\\n  28: 1500,\\n  27: 1453,\\n  26: 1410,\\n  25: 1364,\\n  24: 1312,\\n  23: 1271,\\n  22: 1229,\\n  21: 1180,\\n  20: 1132,\\n  19: 1084,\\n  18: 1025,\\n  17: 963,\\n  16: 912,\\n  15: 874,\\n  14: 835,\\n  13: 782,\\n  12: 742,\\n  11: 696,\\n  10: 648,\\n   9: 597,\\n   8: 552,\\n   7: 544,\\n   6: 495,\\n   5: 396\\n}\\n\\nCANNED_TEST_FILE_SYSTEM_DATA = {\\n  \'api\': {\\n    \'_api_features.json\': json.dumps({\\n      \'ref_test\': { \'dependencies\': [\'permission:ref_test\'] }\\n      \'tester\': { \'dependencies\': [\'permission:tester\', \'manifest:tester\'] }\\n  }),\\n    \'_manifest_features.json\': json.dumps({\\n      \'manifest\': \'features\'\\n  }),\\n    \'_permission_features.json\': json.dumps({\\n      \'permission\': \'features\'\\n  })\\n}\\n  \'docs\': {\\n    \'templates\': {\\n      \'intros\': {\\n        \'test.html\': \'<h1>hi</h1>you<h2>first</h2><h3>inner</h3><h2>second</h2>\'\\n    }\\n      \'json\': {\\n        \'api_availabilities.json\': json.dumps({\\n          \'tester\': {\\n              \'channel\': \'stable\',\\n              \'version\': 42\\n          }\\n        }),\\n        \'intro_tables.json\': json.dumps({\\n          \'tester\': {\\n            \'Permissions\': [\\n            {\\n                \'class\': \'override\',\\n                \'text\': \'\\"tester\\"\'\\n            }\\n            {\\n                \'text\': \'is an API for testing things.\'\\n            }\\n            ],\\n            \'Learn More\': [\\n            {\\n                \'link\': \'https://tester.test.com/welcome.html\',\\n                \'text\': \'Welcome!\'\\n            }\\n            ]\\n        }\\n      })\\n    }\\n      \'private\': {\\n        \'intro_tables\': {\\n          \'trunk_message.html\': \'available on trunk\'\\n      }\\n    }\\n  }\\n}\\n}\\n\\nCANNED_API_FILE_SYSTEM_DATA = {\\n  \'trunk\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'events\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'history\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'falseBetaAPI\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'trunkAPI\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n    \'docs\': {\\n      \'templates\': {\\n        \'json\': {\\n          \'api_availabilities.json\': json.dumps({\\n            \'jsonAPI1\': {\\n              \'channel\': \'stable\',\\n              \'version\': 10\\n          }\\n            \'jsonAPI2\': {\\n              \'channel\': \'trunk\'\\n          }\\n            \'jsonAPI3\': {\\n              \'channel\': \'dev\'\\n          }\\n        }),\\n          \'intro_tables.json\': json.dumps({\\n            \'test\': [\\n            {\\n                \'Permissions\': \'probably none\'\\n            }\\n            ]\\n        })\\n      }\\n    }\\n  }\\n}\\n  \'1500\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1453\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.cpu\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1410\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1364\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1312\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1271\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'system_info_display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1229\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'system_info_display.idl\': \'systemInfo.display contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1180\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1132\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1084\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'contents\': \'nothing of interest here,really\'\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'1025\': {\\n    \'api\': {\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'963\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'912\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'experimental.webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'874\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'835\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'782\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'742\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'696\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'648\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'597\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'552\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'544\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'495\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'396\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'experimental.menus\'\\n      }\\n      ])\\n  }\\n}\\n}\\n" }\n'
line: b'{ "repo_name": "Evervolv/android_external_chromium_org", "ref": "refs/heads/kitkat", "path": "chrome/common/extensions/docs/server2/test_data/canned_data.py", "content": "# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\nimport json\\n\\nCANNED_CHANNELS = {\\n  \'trunk\': \'trunk\',\\n  \'dev\': 28,\\n  \'beta\': 27,\\n  \'stable\': 26\\n}\\n\\nCANNED_BRANCHES = {\\n  \'trunk\': \'trunk\',\\n  28: 1500,\\n  27: 1453,\\n  26: 1410,\\n  25: 1364,\\n  24: 1312,\\n  23: 1271,\\n  22: 1229,\\n  21: 1180,\\n  20: 1132,\\n  19: 1084,\\n  18: 1025,\\n  17: 963,\\n  16: 912,\\n  15: 874,\\n  14: 835,\\n  13: 782,\\n  12: 742,\\n  11: 696,\\n  10: 648,\\n   9: 597,\\n   8: 552,\\n   7: 544,\\n   6: 495,\\n   5: 396\\n}\\n\\nCANNED_TEST_FILE_SYSTEM_DATA = {\\n  \'api\': {\\n    \'_api_features.json\': json.dumps({\\n      \'ref_test\': { \'dependencies\': [\'permission:ref_test\'] }\\n      \'tester\': { \'dependencies\': [\'permission:tester\', \'manifest:tester\'] }\\n  }),\\n    \'_manifest_features.json\': json.dumps({\\n      \'manifest\': \'features\'\\n  }),\\n    \'_permission_features.json\': json.dumps({\\n      \'permission\': \'features\'\\n  })\\n}\\n  \'docs\': {\\n    \'templates\': {\\n      \'intros\': {\\n        \'test.html\': \'<h1>hi</h1>you<h2>first</h2><h3>inner</h3><h2>second</h2>\'\\n    }\\n      \'json\': {\\n        \'api_availabilities.json\': json.dumps({\\n          \'tester\': {\\n              \'channel\': \'stable\',\\n              \'version\': 42\\n          }\\n        }),\\n        \'intro_tables.json\': json.dumps({\\n          \'tester\': {\\n            \'Permissions\': [\\n            {\\n                \'class\': \'override\',\\n                \'text\': \'\\"tester\\"\'\\n            }\\n            {\\n                \'text\': \'is an API for testing things.\'\\n            }\\n            ],\\n            \'Learn More\': [\\n            {\\n                \'link\': \'https://tester.test.com/welcome.html\',\\n                \'text\': \'Welcome!\'\\n            }\\n            ]\\n        }\\n      })\\n    }\\n      \'private\': {\\n        \'intro_tables\': {\\n          \'trunk_message.html\': \'available on trunk\'\\n      }\\n    }\\n  }\\n}\\n}\\n\\nCANNED_API_FILE_SYSTEM_DATA = {\\n  \'trunk\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'events\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'history\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'falseBetaAPI\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'trunkAPI\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n    \'docs\': {\\n      \'templates\': {\\n        \'json\': {\\n          \'api_availabilities.json\': json.dumps({\\n            \'jsonAPI1\': {\\n              \'channel\': \'stable\',\\n              \'version\': 10\\n          }\\n            \'jsonAPI2\': {\\n              \'channel\': \'trunk\'\\n          }\\n            \'jsonAPI3\': {\\n              \'channel\': \'dev\'\\n          }\\n        }),\\n          \'intro_tables.json\': json.dumps({\\n            \'test\': [\\n            {\\n                \'Permissions\': \'probably none\'\\n            }\\n            ]\\n        })\\n      }\\n    }\\n  }\\n}\\n  \'1500\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1453\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.cpu\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1410\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1364\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1312\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1271\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'system_info_display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1229\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'system_info_display.idl\': \'systemInfo.display contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1180\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1132\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1084\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'contents\': \'nothing of interest here,really\'\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'1025\': {\\n    \'api\': {\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'963\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'912\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'experimental.webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'874\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'835\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'782\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'742\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'696\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'648\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'597\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'552\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'544\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'495\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'396\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'experimental.menus\'\\n      }\\n      ])\\n  }\\n}\\n}\\n" }\n'
line: b'{ "repo_name": "brototyp/CouchPotato", "ref": "refs/heads/german", "path": "library/sqlalchemy/connectors/mxodbc.py", "content": "\\"\\"\\"\\nProvide an SQLALchemy connector for the eGenix mxODBC commercial\\nPython adapter for ODBC. This is not a free product, but eGenix\\nprovides SQLAlchemy with a license for use in continuous integration\\ntesting.\\n\\nThis has been tested for use with mxODBC 3.1.2 on SQL Server 2005\\nand 2008, using the SQL Server Native driver. However, it is\\npossible for this to be used on other database platforms.\\n\\nFor more info on mxODBC, see http://www.egenix.com/\\n\\n\\"\\"\\"\\n\\nimport sys\\nimport re\\nimport warnings\\nfrom decimal import Decimal\\n\\nfrom sqlalchemy.connectors import Connector\\nfrom sqlalchemy import types as sqltypes\\nimport sqlalchemy.processors as processors\\n\\nclass MxODBCConnector(Connector):\\n    driver=\'mxodbc\'\\n    \\n    supports_sane_multi_rowcount = False\\n    supports_unicode_statements = False\\n    supports_unicode_binds = False\\n    \\n    supports_native_decimal = True\\n    \\n    @classmethod\\n    def dbapi(cls):\\n        # this classmethod will normally be replaced by an instance\\n        # attribute of the same name, so this is normally only called once.\\n        cls._load_mx_exceptions()\\n        platform = sys.platform\\n        if platform == \'win32\':\\n            from mx.ODBC import Windows as module\\n        # this can be the string \\"linux2\\", and possibly others\\n        elif \'linux\' in platform:\\n            from mx.ODBC import unixODBC as module\\n        elif platform == \'darwin\':\\n            from mx.ODBC import iODBC as module\\n        else:\\n            raise ImportError, \\"Unrecognized platform for mxODBC import\\"\\n        return module\\n\\n    @classmethod\\n    def _load_mx_exceptions(cls):\\n        \\"\\"\\" Import mxODBC exception classes into the module namespace,\\n        as if they had been imported normally. This is done here\\n        to avoid requiring all SQLAlchemy users to install mxODBC.\\n        \\"\\"\\"\\n        global InterfaceError, ProgrammingError\\n        from mx.ODBC import InterfaceError\\n        from mx.ODBC import ProgrammingError\\n\\n    def on_connect(self):\\n        def connect(conn):\\n            conn.stringformat = self.dbapi.MIXED_STRINGFORMAT\\n            conn.datetimeformat = self.dbapi.PYDATETIME_DATETIMEFORMAT\\n            conn.decimalformat = self.dbapi.DECIMAL_DECIMALFORMAT\\n            conn.errorhandler = self._error_handler()\\n        return connect\\n    \\n    def _error_handler(self):\\n        \\"\\"\\" Return a handler that adjusts mxODBC\'s raised Warnings to\\n        emit Python standard warnings.\\n        \\"\\"\\"\\n        from mx.ODBC.Error import Warning as MxOdbcWarning\\n        def error_handler(connection, cursor, errorclass, errorvalue):\\n\\n            if issubclass(errorclass, MxOdbcWarning):\\n                errorclass.__bases__ = (Warning,)\\n                warnings.warn(message=str(errorvalue),\\n                          category=errorclass,\\n                          stacklevel=2)\\n            else:\\n                raise errorclass, errorvalue\\n        return error_handler\\n\\n    def create_connect_args(self, url):\\n        \\"\\"\\" Return a tuple of *args,**kwargs for creating a connection.\\n\\n        The mxODBC 3.x connection constructor looks like this:\\n\\n            connect(dsn, user=\'\', password=\'\',\\n                    clear_auto_commit=1, errorhandler=None)\\n\\n        This method translates the values in the provided uri\\n        into args and kwargs needed to instantiate an mxODBC Connection.\\n\\n        The arg \'errorhandler\' is not used by SQLAlchemy and will\\n        not be populated.\\n        \\n        \\"\\"\\"\\n        opts = url.translate_connect_args(username=\'user\')\\n        opts.update(url.query)\\n        args = opts.pop(\'host\')\\n        opts.pop(\'port\', None)\\n        opts.pop(\'database\', None)\\n        return (args,), opts\\n\\n    def is_disconnect(self, e):\\n        # eGenix recommends checking connection.closed here,\\n        # but how can we get a handle on the current connection?\\n        if isinstance(e, self.dbapi.ProgrammingError):\\n            return \\"connection already closed\\" in str(e)\\n        elif isinstance(e, self.dbapi.Error):\\n            return \'[08S01]\' in str(e)\\n        else:\\n            return False\\n\\n    def _get_server_version_info(self, connection):\\n        # eGenix suggests using conn.dbms_version instead \\n        # of what we\'re doing here\\n        dbapi_con = connection.connection\\n        version = []\\n        r = re.compile(\'[.\\\\-]\')\\n        # 18 == pyodbc.SQL_DBMS_VER\\n        for n in r.split(dbapi_con.getinfo(18)[1]):\\n            try:\\n                version.append(int(n))\\n            except ValueError:\\n                version.append(n)\\n        return tuple(version)\\n\\n    def do_execute(self, cursor, statement, parameters, context=None):\\n        if context:\\n            native_odbc_execute = context.execution_options.\\\\\\n                                        get(\'native_odbc_execute\', \'auto\')\\n            if native_odbc_execute is True:\\n                # user specified native_odbc_execute=True\\n                cursor.execute(statement, parameters)\\n            elif native_odbc_execute is False:\\n                # user specified native_odbc_execute=False\\n                cursor.executedirect(statement, parameters)\\n            elif context.is_crud:\\n                # statement is UPDATE, DELETE, INSERT\\n                cursor.execute(statement, parameters)\\n            else:\\n                # all other statements\\n                cursor.executedirect(statement, parameters)\\n        else:\\n            cursor.executedirect(statement, parameters)\\n" }\n'
line: b'{ "repo_name": "aospx-kitkat/platform_external_chromium_org", "ref": "refs/heads/kitkat", "path": "chrome/common/extensions/docs/server2/test_data/canned_data.py", "content": "# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\nimport json\\n\\nCANNED_CHANNELS = {\\n  \'trunk\': \'trunk\',\\n  \'dev\': 28,\\n  \'beta\': 27,\\n  \'stable\': 26\\n}\\n\\nCANNED_BRANCHES = {\\n  \'trunk\': \'trunk\',\\n  28: 1500,\\n  27: 1453,\\n  26: 1410,\\n  25: 1364,\\n  24: 1312,\\n  23: 1271,\\n  22: 1229,\\n  21: 1180,\\n  20: 1132,\\n  19: 1084,\\n  18: 1025,\\n  17: 963,\\n  16: 912,\\n  15: 874,\\n  14: 835,\\n  13: 782,\\n  12: 742,\\n  11: 696,\\n  10: 648,\\n   9: 597,\\n   8: 552,\\n   7: 544,\\n   6: 495,\\n   5: 396\\n}\\n\\nCANNED_TEST_FILE_SYSTEM_DATA = {\\n  \'api\': {\\n    \'_api_features.json\': json.dumps({\\n      \'ref_test\': { \'dependencies\': [\'permission:ref_test\'] }\\n      \'tester\': { \'dependencies\': [\'permission:tester\', \'manifest:tester\'] }\\n  }),\\n    \'_manifest_features.json\': json.dumps({\\n      \'manifest\': \'features\'\\n  }),\\n    \'_permission_features.json\': json.dumps({\\n      \'permission\': \'features\'\\n  })\\n}\\n  \'docs\': {\\n    \'templates\': {\\n      \'intros\': {\\n        \'test.html\': \'<h1>hi</h1>you<h2>first</h2><h3>inner</h3><h2>second</h2>\'\\n    }\\n      \'json\': {\\n        \'api_availabilities.json\': json.dumps({\\n          \'tester\': {\\n              \'channel\': \'stable\',\\n              \'version\': 42\\n          }\\n        }),\\n        \'intro_tables.json\': json.dumps({\\n          \'tester\': {\\n            \'Permissions\': [\\n            {\\n                \'class\': \'override\',\\n                \'text\': \'\\"tester\\"\'\\n            }\\n            {\\n                \'text\': \'is an API for testing things.\'\\n            }\\n            ],\\n            \'Learn More\': [\\n            {\\n                \'link\': \'https://tester.test.com/welcome.html\',\\n                \'text\': \'Welcome!\'\\n            }\\n            ]\\n        }\\n      })\\n    }\\n      \'private\': {\\n        \'intro_tables\': {\\n          \'trunk_message.html\': \'available on trunk\'\\n      }\\n    }\\n  }\\n}\\n}\\n\\nCANNED_API_FILE_SYSTEM_DATA = {\\n  \'trunk\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'events\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'history\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'falseBetaAPI\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'trunkAPI\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n    \'docs\': {\\n      \'templates\': {\\n        \'json\': {\\n          \'api_availabilities.json\': json.dumps({\\n            \'jsonAPI1\': {\\n              \'channel\': \'stable\',\\n              \'version\': 10\\n          }\\n            \'jsonAPI2\': {\\n              \'channel\': \'trunk\'\\n          }\\n            \'jsonAPI3\': {\\n              \'channel\': \'dev\'\\n          }\\n        }),\\n          \'intro_tables.json\': json.dumps({\\n            \'test\': [\\n            {\\n                \'Permissions\': \'probably none\'\\n            }\\n            ]\\n        })\\n      }\\n    }\\n  }\\n}\\n  \'1500\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1453\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.cpu\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1410\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1364\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1312\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1271\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'system_info_display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1229\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'system_info_display.idl\': \'systemInfo.display contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1180\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1132\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1084\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'contents\': \'nothing of interest here,really\'\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'1025\': {\\n    \'api\': {\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'963\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'912\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'experimental.webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'874\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'835\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'782\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'742\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'696\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'648\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'597\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'552\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'544\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'495\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'396\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'experimental.menus\'\\n      }\\n      ])\\n  }\\n}\\n}\\n" }\n'
line: b'{ "repo_name": "gfreed/android_external_chromium-org", "ref": "refs/heads/android-4.4", "path": "chrome/common/extensions/docs/server2/test_data/canned_data.py", "content": "# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\nimport json\\n\\nCANNED_CHANNELS = {\\n  \'trunk\': \'trunk\',\\n  \'dev\': 28,\\n  \'beta\': 27,\\n  \'stable\': 26\\n}\\n\\nCANNED_BRANCHES = {\\n  \'trunk\': \'trunk\',\\n  28: 1500,\\n  27: 1453,\\n  26: 1410,\\n  25: 1364,\\n  24: 1312,\\n  23: 1271,\\n  22: 1229,\\n  21: 1180,\\n  20: 1132,\\n  19: 1084,\\n  18: 1025,\\n  17: 963,\\n  16: 912,\\n  15: 874,\\n  14: 835,\\n  13: 782,\\n  12: 742,\\n  11: 696,\\n  10: 648,\\n   9: 597,\\n   8: 552,\\n   7: 544,\\n   6: 495,\\n   5: 396\\n}\\n\\nCANNED_TEST_FILE_SYSTEM_DATA = {\\n  \'api\': {\\n    \'_api_features.json\': json.dumps({\\n      \'ref_test\': { \'dependencies\': [\'permission:ref_test\'] }\\n      \'tester\': { \'dependencies\': [\'permission:tester\', \'manifest:tester\'] }\\n  }),\\n    \'_manifest_features.json\': json.dumps({\\n      \'manifest\': \'features\'\\n  }),\\n    \'_permission_features.json\': json.dumps({\\n      \'permission\': \'features\'\\n  })\\n}\\n  \'docs\': {\\n    \'templates\': {\\n      \'intros\': {\\n        \'test.html\': \'<h1>hi</h1>you<h2>first</h2><h3>inner</h3><h2>second</h2>\'\\n    }\\n      \'json\': {\\n        \'api_availabilities.json\': json.dumps({\\n          \'tester\': {\\n              \'channel\': \'stable\',\\n              \'version\': 42\\n          }\\n        }),\\n        \'intro_tables.json\': json.dumps({\\n          \'tester\': {\\n            \'Permissions\': [\\n            {\\n                \'class\': \'override\',\\n                \'text\': \'\\"tester\\"\'\\n            }\\n            {\\n                \'text\': \'is an API for testing things.\'\\n            }\\n            ],\\n            \'Learn More\': [\\n            {\\n                \'link\': \'https://tester.test.com/welcome.html\',\\n                \'text\': \'Welcome!\'\\n            }\\n            ]\\n        }\\n      })\\n    }\\n      \'private\': {\\n        \'intro_tables\': {\\n          \'trunk_message.html\': \'available on trunk\'\\n      }\\n    }\\n  }\\n}\\n}\\n\\nCANNED_API_FILE_SYSTEM_DATA = {\\n  \'trunk\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'events\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'history\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'falseBetaAPI\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'trunkAPI\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n    \'docs\': {\\n      \'templates\': {\\n        \'json\': {\\n          \'api_availabilities.json\': json.dumps({\\n            \'jsonAPI1\': {\\n              \'channel\': \'stable\',\\n              \'version\': 10\\n          }\\n            \'jsonAPI2\': {\\n              \'channel\': \'trunk\'\\n          }\\n            \'jsonAPI3\': {\\n              \'channel\': \'dev\'\\n          }\\n        }),\\n          \'intro_tables.json\': json.dumps({\\n            \'test\': [\\n            {\\n                \'Permissions\': \'probably none\'\\n            }\\n            ]\\n        })\\n      }\\n    }\\n  }\\n}\\n  \'1500\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1453\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.cpu\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1410\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1364\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1312\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1271\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'system_info_display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1229\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'system_info_display.idl\': \'systemInfo.display contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1180\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1132\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1084\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'contents\': \'nothing of interest here,really\'\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'1025\': {\\n    \'api\': {\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'963\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'912\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'experimental.webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'874\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'835\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'782\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'742\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'696\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'648\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'597\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'552\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'544\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'495\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'396\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'experimental.menus\'\\n      }\\n      ])\\n  }\\n}\\n}\\n" }\n'
line: b'{ "repo_name": "ROMFactory/android_external_chromium_org", "ref": "refs/heads/kitkat", "path": "chrome/common/extensions/docs/server2/test_data/canned_data.py", "content": "# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\nimport json\\n\\nCANNED_CHANNELS = {\\n  \'trunk\': \'trunk\',\\n  \'dev\': 28,\\n  \'beta\': 27,\\n  \'stable\': 26\\n}\\n\\nCANNED_BRANCHES = {\\n  \'trunk\': \'trunk\',\\n  28: 1500,\\n  27: 1453,\\n  26: 1410,\\n  25: 1364,\\n  24: 1312,\\n  23: 1271,\\n  22: 1229,\\n  21: 1180,\\n  20: 1132,\\n  19: 1084,\\n  18: 1025,\\n  17: 963,\\n  16: 912,\\n  15: 874,\\n  14: 835,\\n  13: 782,\\n  12: 742,\\n  11: 696,\\n  10: 648,\\n   9: 597,\\n   8: 552,\\n   7: 544,\\n   6: 495,\\n   5: 396\\n}\\n\\nCANNED_TEST_FILE_SYSTEM_DATA = {\\n  \'api\': {\\n    \'_api_features.json\': json.dumps({\\n      \'ref_test\': { \'dependencies\': [\'permission:ref_test\'] }\\n      \'tester\': { \'dependencies\': [\'permission:tester\', \'manifest:tester\'] }\\n  }),\\n    \'_manifest_features.json\': json.dumps({\\n      \'manifest\': \'features\'\\n  }),\\n    \'_permission_features.json\': json.dumps({\\n      \'permission\': \'features\'\\n  })\\n}\\n  \'docs\': {\\n    \'templates\': {\\n      \'intros\': {\\n        \'test.html\': \'<h1>hi</h1>you<h2>first</h2><h3>inner</h3><h2>second</h2>\'\\n    }\\n      \'json\': {\\n        \'api_availabilities.json\': json.dumps({\\n          \'tester\': {\\n              \'channel\': \'stable\',\\n              \'version\': 42\\n          }\\n        }),\\n        \'intro_tables.json\': json.dumps({\\n          \'tester\': {\\n            \'Permissions\': [\\n            {\\n                \'class\': \'override\',\\n                \'text\': \'\\"tester\\"\'\\n            }\\n            {\\n                \'text\': \'is an API for testing things.\'\\n            }\\n            ],\\n            \'Learn More\': [\\n            {\\n                \'link\': \'https://tester.test.com/welcome.html\',\\n                \'text\': \'Welcome!\'\\n            }\\n            ]\\n        }\\n      })\\n    }\\n      \'private\': {\\n        \'intro_tables\': {\\n          \'trunk_message.html\': \'available on trunk\'\\n      }\\n    }\\n  }\\n}\\n}\\n\\nCANNED_API_FILE_SYSTEM_DATA = {\\n  \'trunk\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'events\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'history\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'falseBetaAPI\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'trunkAPI\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n    \'docs\': {\\n      \'templates\': {\\n        \'json\': {\\n          \'api_availabilities.json\': json.dumps({\\n            \'jsonAPI1\': {\\n              \'channel\': \'stable\',\\n              \'version\': 10\\n          }\\n            \'jsonAPI2\': {\\n              \'channel\': \'trunk\'\\n          }\\n            \'jsonAPI3\': {\\n              \'channel\': \'dev\'\\n          }\\n        }),\\n          \'intro_tables.json\': json.dumps({\\n            \'test\': [\\n            {\\n                \'Permissions\': \'probably none\'\\n            }\\n            ]\\n        })\\n      }\\n    }\\n  }\\n}\\n  \'1500\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1453\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.cpu\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1410\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1364\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1312\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1271\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'system_info_display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1229\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'system_info_display.idl\': \'systemInfo.display contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1180\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1132\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1084\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'contents\': \'nothing of interest here,really\'\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'1025\': {\\n    \'api\': {\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'963\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'912\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'experimental.webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'874\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'835\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'782\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'742\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'696\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'648\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'597\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'552\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'544\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'495\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'396\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'experimental.menus\'\\n      }\\n      ])\\n  }\\n}\\n}\\n" }\n'
line: b'{ "repo_name": "MIPS/external-chromium_org", "ref": "refs/heads/dev-mips-jb-kitkat", "path": "chrome/common/extensions/docs/server2/test_data/canned_data.py", "content": "# Copyright 2013 The Chromium Authors. All rights reserved.\\n# Use of this source code is governed by a BSD-style license that can be\\n# found in the LICENSE file.\\n\\nimport json\\n\\nCANNED_CHANNELS = {\\n  \'trunk\': \'trunk\',\\n  \'dev\': 28,\\n  \'beta\': 27,\\n  \'stable\': 26\\n}\\n\\nCANNED_BRANCHES = {\\n  \'trunk\': \'trunk\',\\n  28: 1500,\\n  27: 1453,\\n  26: 1410,\\n  25: 1364,\\n  24: 1312,\\n  23: 1271,\\n  22: 1229,\\n  21: 1180,\\n  20: 1132,\\n  19: 1084,\\n  18: 1025,\\n  17: 963,\\n  16: 912,\\n  15: 874,\\n  14: 835,\\n  13: 782,\\n  12: 742,\\n  11: 696,\\n  10: 648,\\n   9: 597,\\n   8: 552,\\n   7: 544,\\n   6: 495,\\n   5: 396\\n}\\n\\nCANNED_TEST_FILE_SYSTEM_DATA = {\\n  \'api\': {\\n    \'_api_features.json\': json.dumps({\\n      \'ref_test\': { \'dependencies\': [\'permission:ref_test\'] }\\n      \'tester\': { \'dependencies\': [\'permission:tester\', \'manifest:tester\'] }\\n  }),\\n    \'_manifest_features.json\': json.dumps({\\n      \'manifest\': \'features\'\\n  }),\\n    \'_permission_features.json\': json.dumps({\\n      \'permission\': \'features\'\\n  })\\n}\\n  \'docs\': {\\n    \'templates\': {\\n      \'intros\': {\\n        \'test.html\': \'<h1>hi</h1>you<h2>first</h2><h3>inner</h3><h2>second</h2>\'\\n    }\\n      \'json\': {\\n        \'api_availabilities.json\': json.dumps({\\n          \'tester\': {\\n              \'channel\': \'stable\',\\n              \'version\': 42\\n          }\\n        }),\\n        \'intro_tables.json\': json.dumps({\\n          \'tester\': {\\n            \'Permissions\': [\\n            {\\n                \'class\': \'override\',\\n                \'text\': \'\\"tester\\"\'\\n            }\\n            {\\n                \'text\': \'is an API for testing things.\'\\n            }\\n            ],\\n            \'Learn More\': [\\n            {\\n                \'link\': \'https://tester.test.com/welcome.html\',\\n                \'text\': \'Welcome!\'\\n            }\\n            ]\\n        }\\n      })\\n    }\\n      \'private\': {\\n        \'intro_tables\': {\\n          \'trunk_message.html\': \'available on trunk\'\\n      }\\n    }\\n  }\\n}\\n}\\n\\nCANNED_API_FILE_SYSTEM_DATA = {\\n  \'trunk\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'events\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'history\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'falseBetaAPI\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'trunkAPI\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n    \'docs\': {\\n      \'templates\': {\\n        \'json\': {\\n          \'api_availabilities.json\': json.dumps({\\n            \'jsonAPI1\': {\\n              \'channel\': \'stable\',\\n              \'version\': 10\\n          }\\n            \'jsonAPI2\': {\\n              \'channel\': \'trunk\'\\n          }\\n            \'jsonAPI3\': {\\n              \'channel\': \'dev\'\\n          }\\n        }),\\n          \'intro_tables.json\': json.dumps({\\n            \'test\': [\\n            {\\n                \'Permissions\': \'probably none\'\\n            }\\n            ]\\n        })\\n      }\\n    }\\n  }\\n}\\n  \'1500\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'contextMenus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'sync\': {\\n          \'channel\': \'trunk\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'cookies\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1453\': {\\n    \'api\': {\\n      \'_api_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'extension\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.cpu\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.stuff\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_manifest_features.json\': json.dumps({\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'storage\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'downloads\': {\\n          \'channel\': \'dev\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1410\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'events\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'notifications\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bluetooth\': {\\n          \'channel\': \'dev\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'context_menus\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeContent\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'declarativeWebRequest\': [\\n        { \'channel\': \'beta\' }\\n          # whitelist\\n        { \'channel\': \'stable\'}\\n        ],\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1364\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1312\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1271\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'system_info_display\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'alarms\': {\\n          \'channel\': \'beta\'\\n      }\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'windows.json\': \'windows contents\'\\n  }\\n}\\n  \'1229\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'web_request\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'systemInfo.display\': {\\n          \'channel\': \'beta\'\\n      }\\n    }),\\n      \'alarms.idl\': \'alarms contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'system_info_display.idl\': \'systemInfo.display contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1180\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'runtime\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'stable\'\\n      }\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input_ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1132\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'bookmarks\': {\\n          \'channel\': \'trunk\'\\n      }\\n        \'page_action\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'_permission_features.json\': json.dumps({\\n        \'webRequest\': {\\n          \'channel\': \'stable\'\\n      }\\n    }),\\n      \'bookmarks.json\': \'bookmarks contents\',\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'tabs.json\': \'tabs contents\'\\n  }\\n}\\n  \'1084\': {\\n    \'api\': {\\n      \'_manifest_features.json\': json.dumps({\\n        \'contents\': \'nothing of interest here,really\'\\n    }),\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'1025\': {\\n    \'api\': {\\n      \'idle.json\': \'idle contents\',\\n      \'input.ime.json\': \'input.ime contents\',\\n      \'menus.json\': \'menus contents\',\\n      \'pageAction.json\': \'pageAction contents\',\\n      \'tabs.json\': \'tabs contents\',\\n      \'webRequest.json\': \'webRequest contents\'\\n  }\\n}\\n  \'963\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'912\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      {\\n          \'namespace\': \'experimental.webRequest\'\\n      }\\n      ])\\n  }\\n}\\n  \'874\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'835\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'782\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'742\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'696\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'648\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'597\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'552\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      {\\n          \'namespace\': \'pageAction\'\\n      }\\n      ])\\n  }\\n}\\n  \'544\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'495\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'menus\'\\n      }\\n      ])\\n  }\\n}\\n  \'396\': {\\n    \'api\': {\\n      \'extension_api.json\': json.dumps([\\n      {\\n          \'namespace\': \'idle\'\\n      }\\n      {\\n          \'namespace\': \'experimental.menus\'\\n      }\\n      ])\\n  }\\n}\\n}\\n" }'
input_path: /home/gcloud/TransCoder/data/test_dataset/python/python.000.json.gz
language: python
output_path: /home/gcloud/TransCoder/data/test_dataset/python/python.000.with_comments.tok
line: b'{"repo_name":"coronary/RandomEpisode","ref":"refs/heads/master","path":"depends/Lib/encodings/cp1006.py","content":"\\"\\"\\" Python Character Mapping Codec cp1006 generated from \'MAPPINGS/VENDORS/MISC/CP1006.TXT\' with gencodec.py.\\n\\n\\"\\"\\"#\\"\\n\\nimport codecs\\n\\n### Codec APIs\\n\\nclass Codec(codecs.Codec):\\n\\n    def encode(self,input,errors=\'strict\'):\\n        return codecs.charmap_encode(input,errors,encoding_table)\\n\\n    def decode(self,input,errors=\'strict\'):\\n        return codecs.charmap_decode(input,errors,decoding_table)\\n\\nclass IncrementalEncoder(codecs.IncrementalEncoder):\\n    def encode(self, input, final=False):\\n        return codecs.charmap_encode(input,self.errors,encoding_table)[0]\\n\\nclass IncrementalDecoder(codecs.IncrementalDecoder):\\n    def decode(self, input, final=False):\\n        return codecs.charmap_decode(input,self.errors,decoding_table)[0]\\n\\nclass StreamWriter(Codec,codecs.StreamWriter):\\n    pass\\n\\nclass StreamReader(Codec,codecs.StreamReader):\\n    pass\\n\\n### encodings module API\\n\\ndef getregentry():\\n    return codecs.CodecInfo(\\n        name=\'cp1006\',\\n        encode=Codec().encode,\\n        decode=Codec().decode,\\n        incrementalencoder=IncrementalEncoder,\\n        incrementaldecoder=IncrementalDecoder,\\n        streamreader=StreamReader,\\n        streamwriter=StreamWriter,\\n    )\\n\\n\\n### Decoding Table\\n\\ndecoding_table = (\\n    \'\\\\x00\'     #  0x00 -\\u003e NULL\\n    \'\\\\x01\'     #  0x01 -\\u003e START OF HEADING\\n    \'\\\\x02\'     #  0x02 -\\u003e START OF TEXT\\n    \'\\\\x03\'     #  0x03 -\\u003e END OF TEXT\\n    \'\\\\x04\'     #  0x04 -\\u003e END OF TRANSMISSION\\n    \'\\\\x05\'     #  0x05 -\\u003e ENQUIRY\\n    \'\\\\x06\'     #  0x06 -\\u003e ACKNOWLEDGE\\n    \'\\\\x07\'     #  0x07 -\\u003e BELL\\n    \'\\\\x08\'     #  0x08 -\\u003e BACKSPACE\\n    \'\\\\t\'       #  0x09 -\\u003e HORIZONTAL TABULATION\\n    \'\\\\n\'       #  0x0A -\\u003e LINE FEED\\n    \'\\\\x0b\'     #  0x0B -\\u003e VERTICAL TABULATION\\n    \'\\\\x0c\'     #  0x0C -\\u003e FORM FEED\\n    \'\\\\r\'       #  0x0D -\\u003e CARRIAGE RETURN\\n    \'\\\\x0e\'     #  0x0E -\\u003e SHIFT OUT\\n    \'\\\\x0f\'     #  0x0F -\\u003e SHIFT IN\\n    \'\\\\x10\'     #  0x10 -\\u003e DATA LINK ESCAPE\\n    \'\\\\x11\'     #  0x11 -\\u003e DEVICE CONTROL ONE\\n    \'\\\\x12\'     #  0x12 -\\u003e DEVICE CONTROL TWO\\n    \'\\\\x13\'     #  0x13 -\\u003e DEVICE CONTROL THREE\\n    \'\\\\x14\'     #  0x14 -\\u003e DEVICE CONTROL FOUR\\n    \'\\\\x15\'     #  0x15 -\\u003e NEGATIVE ACKNOWLEDGE\\n    \'\\\\x16\'     #  0x16 -\\u003e SYNCHRONOUS IDLE\\n    \'\\\\x17\'     #  0x17 -\\u003e END OF TRANSMISSION BLOCK\\n    \'\\\\x18\'     #  0x18 -\\u003e CANCEL\\n    \'\\\\x19\'     #  0x19 -\\u003e END OF MEDIUM\\n    \'\\\\x1a\'     #  0x1A -\\u003e SUBSTITUTE\\n    \'\\\\x1b\'     #  0x1B -\\u003e ESCAPE\\n    \'\\\\x1c\'     #  0x1C -\\u003e FILE SEPARATOR\\n    \'\\\\x1d\'     #  0x1D -\\u003e GROUP SEPARATOR\\n    \'\\\\x1e\'     #  0x1E -\\u003e RECORD SEPARATOR\\n    \'\\\\x1f\'     #  0x1F -\\u003e UNIT SEPARATOR\\n    \' \'        #  0x20 -\\u003e SPACE\\n    \'!\'        #  0x21 -\\u003e EXCLAMATION MARK\\n    \'\\"\'        #  0x22 -\\u003e QUOTATION MARK\\n    \'#\'        #  0x23 -\\u003e NUMBER SIGN\\n    \'$\'        #  0x24 -\\u003e DOLLAR SIGN\\n    \'%\'        #  0x25 -\\u003e PERCENT SIGN\\n    \'\\u0026\'        #  0x26 -\\u003e AMPERSAND\\n    \\"\'\\"        #  0x27 -\\u003e APOSTROPHE\\n    \'(\'        #  0x28 -\\u003e LEFT PARENTHESIS\\n    \')\'        #  0x29 -\\u003e RIGHT PARENTHESIS\\n    \'*\'        #  0x2A -\\u003e ASTERISK\\n    \'+\'        #  0x2B -\\u003e PLUS SIGN\\n    \',\'        #  0x2C -\\u003e COMMA\\n    \'-\'        #  0x2D -\\u003e HYPHEN-MINUS\\n    \'.\'        #  0x2E -\\u003e FULL STOP\\n    \'/\'        #  0x2F -\\u003e SOLIDUS\\n    \'0\'        #  0x30 -\\u003e DIGIT ZERO\\n    \'1\'        #  0x31 -\\u003e DIGIT ONE\\n    \'2\'        #  0x32 -\\u003e DIGIT TWO\\n    \'3\'        #  0x33 -\\u003e DIGIT THREE\\n    \'4\'        #  0x34 -\\u003e DIGIT FOUR\\n    \'5\'        #  0x35 -\\u003e DIGIT FIVE\\n    \'6\'        #  0x36 -\\u003e DIGIT SIX\\n    \'7\'        #  0x37 -\\u003e DIGIT SEVEN\\n    \'8\'        #  0x38 -\\u003e DIGIT EIGHT\\n    \'9\'        #  0x39 -\\u003e DIGIT NINE\\n    \':\'        #  0x3A -\\u003e COLON\\n    \';\'        #  0x3B -\\u003e SEMICOLON\\n    \'\\u003c\'        #  0x3C -\\u003e LESS-THAN SIGN\\n    \'=\'        #  0x3D -\\u003e EQUALS SIGN\\n    \'\\u003e\'        #  0x3E -\\u003e GREATER-THAN SIGN\\n    \'?\'        #  0x3F -\\u003e QUESTION MARK\\n    \'@\'        #  0x40 -\\u003e COMMERCIAL AT\\n    \'A\'        #  0x41 -\\u003e LATIN CAPITAL LETTER A\\n    \'B\'        #  0x42 -\\u003e LATIN CAPITAL LETTER B\\n    \'C\'        #  0x43 -\\u003e LATIN CAPITAL LETTER C\\n    \'D\'        #  0x44 -\\u003e LATIN CAPITAL LETTER D\\n    \'E\'        #  0x45 -\\u003e LATIN CAPITAL LETTER E\\n    \'F\'        #  0x46 -\\u003e LATIN CAPITAL LETTER F\\n    \'G\'        #  0x47 -\\u003e LATIN CAPITAL LETTER G\\n    \'H\'        #  0x48 -\\u003e LATIN CAPITAL LETTER H\\n    \'I\'        #  0x49 -\\u003e LATIN CAPITAL LETTER I\\n    \'J\'        #  0x4A -\\u003e LATIN CAPITAL LETTER J\\n    \'K\'        #  0x4B -\\u003e LATIN CAPITAL LETTER K\\n    \'L\'        #  0x4C -\\u003e LATIN CAPITAL LETTER L\\n    \'M\'        #  0x4D -\\u003e LATIN CAPITAL LETTER M\\n    \'N\'        #  0x4E -\\u003e LATIN CAPITAL LETTER N\\n    \'O\'        #  0x4F -\\u003e LATIN CAPITAL LETTER O\\n    \'P\'        #  0x50 -\\u003e LATIN CAPITAL LETTER P\\n    \'Q\'        #  0x51 -\\u003e LATIN CAPITAL LETTER Q\\n    \'R\'        #  0x52 -\\u003e LATIN CAPITAL LETTER R\\n    \'S\'        #  0x53 -\\u003e LATIN CAPITAL LETTER S\\n    \'T\'        #  0x54 -\\u003e LATIN CAPITAL LETTER T\\n    \'U\'        #  0x55 -\\u003e LATIN CAPITAL LETTER U\\n    \'V\'        #  0x56 -\\u003e LATIN CAPITAL LETTER V\\n    \'W\'        #  0x57 -\\u003e LATIN CAPITAL LETTER W\\n    \'X\'        #  0x58 -\\u003e LATIN CAPITAL LETTER X\\n    \'Y\'        #  0x59 -\\u003e LATIN CAPITAL LETTER Y\\n    \'Z\'        #  0x5A -\\u003e LATIN CAPITAL LETTER Z\\n    \'[\'        #  0x5B -\\u003e LEFT SQUARE BRACKET\\n    \'\\\\\\\\\'       #  0x5C -\\u003e REVERSE SOLIDUS\\n    \']\'        #  0x5D -\\u003e RIGHT SQUARE BRACKET\\n    \'^\'        #  0x5E -\\u003e CIRCUMFLEX ACCENT\\n    \'_\'        #  0x5F -\\u003e LOW LINE\\n    \'`\'        #  0x60 -\\u003e GRAVE ACCENT\\n    \'a\'        #  0x61 -\\u003e LATIN SMALL LETTER A\\n    \'b\'        #  0x62 -\\u003e LATIN SMALL LETTER B\\n    \'c\'        #  0x63 -\\u003e LATIN SMALL LETTER C\\n    \'d\'        #  0x64 -\\u003e LATIN SMALL LETTER D\\n    \'e\'        #  0x65 -\\u003e LATIN SMALL LETTER E\\n    \'f\'        #  0x66 -\\u003e LATIN SMALL LETTER F\\n    \'g\'        #  0x67 -\\u003e LATIN SMALL LETTER G\\n    \'h\'        #  0x68 -\\u003e LATIN SMALL LETTER H\\n    \'i\'        #  0x69 -\\u003e LATIN SMALL LETTER I\\n    \'j\'        #  0x6A -\\u003e LATIN SMALL LETTER J\\n    \'k\'        #  0x6B -\\u003e LATIN SMALL LETTER K\\n    \'l\'        #  0x6C -\\u003e LATIN SMALL LETTER L\\n    \'m\'        #  0x6D -\\u003e LATIN SMALL LETTER M\\n    \'n\'        #  0x6E -\\u003e LATIN SMALL LETTER N\\n    \'o\'        #  0x6F -\\u003e LATIN SMALL LETTER O\\n    \'p\'        #  0x70 -\\u003e LATIN SMALL LETTER P\\n    \'q\'        #  0x71 -\\u003e LATIN SMALL LETTER Q\\n    \'r\'        #  0x72 -\\u003e LATIN SMALL LETTER R\\n    \'s\'        #  0x73 -\\u003e LATIN SMALL LETTER S\\n    \'t\'        #  0x74 -\\u003e LATIN SMALL LETTER T\\n    \'u\'        #  0x75 -\\u003e LATIN SMALL LETTER U\\n    \'v\'        #  0x76 -\\u003e LATIN SMALL LETTER V\\n    \'w\'        #  0x77 -\\u003e LATIN SMALL LETTER W\\n    \'x\'        #  0x78 -\\u003e LATIN SMALL LETTER X\\n    \'y\'        #  0x79 -\\u003e LATIN SMALL LETTER Y\\n    \'z\'        #  0x7A -\\u003e LATIN SMALL LETTER Z\\n    \'{\'        #  0x7B -\\u003e LEFT CURLY BRACKET\\n    \'|\'        #  0x7C -\\u003e VERTICAL LINE\\n    \'}\'        #  0x7D -\\u003e RIGHT CURLY BRACKET\\n    \'~\'        #  0x7E -\\u003e TILDE\\n    \'\\\\x7f\'     #  0x7F -\\u003e DELETE\\n    \'\\\\x80\'     #  0x80 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x81\'     #  0x81 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x82\'     #  0x82 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x83\'     #  0x83 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x84\'     #  0x84 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x85\'     #  0x85 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x86\'     #  0x86 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x87\'     #  0x87 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x88\'     #  0x88 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x89\'     #  0x89 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x8a\'     #  0x8A -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x8b\'     #  0x8B -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x8c\'     #  0x8C -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x8d\'     #  0x8D -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x8e\'     #  0x8E -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x8f\'     #  0x8F -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x90\'     #  0x90 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x91\'     #  0x91 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x92\'     #  0x92 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x93\'     #  0x93 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x94\'     #  0x94 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x95\'     #  0x95 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x96\'     #  0x96 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x97\'     #  0x97 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x98\'     #  0x98 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x99\'     #  0x99 -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x9a\'     #  0x9A -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x9b\'     #  0x9B -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x9c\'     #  0x9C -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x9d\'     #  0x9D -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x9e\'     #  0x9E -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\x9f\'     #  0x9F -\\u003e \\u003ccontrol\\u003e\\n    \'\\\\xa0\'     #  0xA0 -\\u003e NO-BREAK SPACE\\n    \'\\\\u06f0\'   #  0xA1 -\\u003e EXTENDED ARABIC-INDIC DIGIT ZERO\\n    \'\\\\u06f1\'   #  0xA2 -\\u003e EXTENDED ARABIC-INDIC DIGIT ONE\\n    \'\\\\u06f2\'   #  0xA3 -\\u003e EXTENDED ARABIC-INDIC DIGIT TWO\\n    \'\\\\u06f3\'   #  0xA4 -\\u003e EXTENDED ARABIC-INDIC DIGIT THREE\\n    \'\\\\u06f4\'   #  0xA5 -\\u003e EXTENDED ARABIC-INDIC DIGIT FOUR\\n    \'\\\\u06f5\'   #  0xA6 -\\u003e EXTENDED ARABIC-INDIC DIGIT FIVE\\n    \'\\\\u06f6\'   #  0xA7 -\\u003e EXTENDED ARABIC-INDIC DIGIT SIX\\n    \'\\\\u06f7\'   #  0xA8 -\\u003e EXTENDED ARABIC-INDIC DIGIT SEVEN\\n    \'\\\\u06f8\'   #  0xA9 -\\u003e EXTENDED ARABIC-INDIC DIGIT EIGHT\\n    \'\\\\u06f9\'   #  0xAA -\\u003e EXTENDED ARABIC-INDIC DIGIT NINE\\n    \'\\\\u060c\'   #  0xAB -\\u003e ARABIC COMMA\\n    \'\\\\u061b\'   #  0xAC -\\u003e ARABIC SEMICOLON\\n    \'\\\\xad\'     #  0xAD -\\u003e SOFT HYPHEN\\n    \'\\\\u061f\'   #  0xAE -\\u003e ARABIC QUESTION MARK\\n    \'\\\\ufe81\'   #  0xAF -\\u003e ARABIC LETTER ALEF WITH MADDA ABOVE ISOLATED FORM\\n    \'\\\\ufe8d\'   #  0xB0 -\\u003e ARABIC LETTER ALEF ISOLATED FORM\\n    \'\\\\ufe8e\'   #  0xB1 -\\u003e ARABIC LETTER ALEF FINAL FORM\\n    \'\\\\ufe8e\'   #  0xB2 -\\u003e ARABIC LETTER ALEF FINAL FORM\\n    \'\\\\ufe8f\'   #  0xB3 -\\u003e ARABIC LETTER BEH ISOLATED FORM\\n    \'\\\\ufe91\'   #  0xB4 -\\u003e ARABIC LETTER BEH INITIAL FORM\\n    \'\\\\ufb56\'   #  0xB5 -\\u003e ARABIC LETTER PEH ISOLATED FORM\\n    \'\\\\ufb58\'   #  0xB6 -\\u003e ARABIC LETTER PEH INITIAL FORM\\n    \'\\\\ufe93\'   #  0xB7 -\\u003e ARABIC LETTER TEH MARBUTA ISOLATED FORM\\n    \'\\\\ufe95\'   #  0xB8 -\\u003e ARABIC LETTER TEH ISOLATED FORM\\n    \'\\\\ufe97\'   #  0xB9 -\\u003e ARABIC LETTER TEH INITIAL FORM\\n    \'\\\\ufb66\'   #  0xBA -\\u003e ARABIC LETTER TTEH ISOLATED FORM\\n    \'\\\\ufb68\'   #  0xBB -\\u003e ARABIC LETTER TTEH INITIAL FORM\\n    \'\\\\ufe99\'   #  0xBC -\\u003e ARABIC LETTER THEH ISOLATED FORM\\n    \'\\\\ufe9b\'   #  0xBD -\\u003e ARABIC LETTER THEH INITIAL FORM\\n    \'\\\\ufe9d\'   #  0xBE -\\u003e ARABIC LETTER JEEM ISOLATED FORM\\n    \'\\\\ufe9f\'   #  0xBF -\\u003e ARABIC LETTER JEEM INITIAL FORM\\n    \'\\\\ufb7a\'   #  0xC0 -\\u003e ARABIC LETTER TCHEH ISOLATED FORM\\n    \'\\\\ufb7c\'   #  0xC1 -\\u003e ARABIC LETTER TCHEH INITIAL FORM\\n    \'\\\\ufea1\'   #  0xC2 -\\u003e ARABIC LETTER HAH ISOLATED FORM\\n    \'\\\\ufea3\'   #  0xC3 -\\u003e ARABIC LETTER HAH INITIAL FORM\\n    \'\\\\ufea5\'   #  0xC4 -\\u003e ARABIC LETTER KHAH ISOLATED FORM\\n    \'\\\\ufea7\'   #  0xC5 -\\u003e ARABIC LETTER KHAH INITIAL FORM\\n    \'\\\\ufea9\'   #  0xC6 -\\u003e ARABIC LETTER DAL ISOLATED FORM\\n    \'\\\\ufb84\'   #  0xC7 -\\u003e ARABIC LETTER DAHAL ISOLATED FORMN\\n    \'\\\\ufeab\'   #  0xC8 -\\u003e ARABIC LETTER THAL ISOLATED FORM\\n    \'\\\\ufead\'   #  0xC9 -\\u003e ARABIC LETTER REH ISOLATED FORM\\n    \'\\\\ufb8c\'   #  0xCA -\\u003e ARABIC LETTER RREH ISOLATED FORM\\n    \'\\\\ufeaf\'   #  0xCB -\\u003e ARABIC LETTER ZAIN ISOLATED FORM\\n    \'\\\\ufb8a\'   #  0xCC -\\u003e ARABIC LETTER JEH ISOLATED FORM\\n    \'\\\\ufeb1\'   #  0xCD -\\u003e ARABIC LETTER SEEN ISOLATED FORM\\n    \'\\\\ufeb3\'   #  0xCE -\\u003e ARABIC LETTER SEEN INITIAL FORM\\n    \'\\\\ufeb5\'   #  0xCF -\\u003e ARABIC LETTER SHEEN ISOLATED FORM\\n    \'\\\\ufeb7\'   #  0xD0 -\\u003e ARABIC LETTER SHEEN INITIAL FORM\\n    \'\\\\ufeb9\'   #  0xD1 -\\u003e ARABIC LETTER SAD ISOLATED FORM\\n    \'\\\\ufebb\'   #  0xD2 -\\u003e ARABIC LETTER SAD INITIAL FORM\\n    \'\\\\ufebd\'   #  0xD3 -\\u003e ARABIC LETTER DAD ISOLATED FORM\\n    \'\\\\ufebf\'   #  0xD4 -\\u003e ARABIC LETTER DAD INITIAL FORM\\n    \'\\\\ufec1\'   #  0xD5 -\\u003e ARABIC LETTER TAH ISOLATED FORM\\n    \'\\\\ufec5\'   #  0xD6 -\\u003e ARABIC LETTER ZAH ISOLATED FORM\\n    \'\\\\ufec9\'   #  0xD7 -\\u003e ARABIC LETTER AIN ISOLATED FORM\\n    \'\\\\ufeca\'   #  0xD8 -\\u003e ARABIC LETTER AIN FINAL FORM\\n    \'\\\\ufecb\'   #  0xD9 -\\u003e ARABIC LETTER AIN INITIAL FORM\\n    \'\\\\ufecc\'   #  0xDA -\\u003e ARABIC LETTER AIN MEDIAL FORM\\n    \'\\\\ufecd\'   #  0xDB -\\u003e ARABIC LETTER GHAIN ISOLATED FORM\\n    \'\\\\ufece\'   #  0xDC -\\u003e ARABIC LETTER GHAIN FINAL FORM\\n    \'\\\\ufecf\'   #  0xDD -\\u003e ARABIC LETTER GHAIN INITIAL FORM\\n    \'\\\\ufed0\'   #  0xDE -\\u003e ARABIC LETTER GHAIN MEDIAL FORM\\n    \'\\\\ufed1\'   #  0xDF -\\u003e ARABIC LETTER FEH ISOLATED FORM\\n    \'\\\\ufed3\'   #  0xE0 -\\u003e ARABIC LETTER FEH INITIAL FORM\\n    \'\\\\ufed5\'   #  0xE1 -\\u003e ARABIC LETTER QAF ISOLATED FORM\\n    \'\\\\ufed7\'   #  0xE2 -\\u003e ARABIC LETTER QAF INITIAL FORM\\n    \'\\\\ufed9\'   #  0xE3 -\\u003e ARABIC LETTER KAF ISOLATED FORM\\n    \'\\\\ufedb\'   #  0xE4 -\\u003e ARABIC LETTER KAF INITIAL FORM\\n    \'\\\\ufb92\'   #  0xE5 -\\u003e ARABIC LETTER GAF ISOLATED FORM\\n    \'\\\\ufb94\'   #  0xE6 -\\u003e ARABIC LETTER GAF INITIAL FORM\\n    \'\\\\ufedd\'   #  0xE7 -\\u003e ARABIC LETTER LAM ISOLATED FORM\\n    \'\\\\ufedf\'   #  0xE8 -\\u003e ARABIC LETTER LAM INITIAL FORM\\n    \'\\\\ufee0\'   #  0xE9 -\\u003e ARABIC LETTER LAM MEDIAL FORM\\n    \'\\\\ufee1\'   #  0xEA -\\u003e ARABIC LETTER MEEM ISOLATED FORM\\n    \'\\\\ufee3\'   #  0xEB -\\u003e ARABIC LETTER MEEM INITIAL FORM\\n    \'\\\\ufb9e\'   #  0xEC -\\u003e ARABIC LETTER NOON GHUNNA ISOLATED FORM\\n    \'\\\\ufee5\'   #  0xED -\\u003e ARABIC LETTER NOON ISOLATED FORM\\n    \'\\\\ufee7\'   #  0xEE -\\u003e ARABIC LETTER NOON INITIAL FORM\\n    \'\\\\ufe85\'   #  0xEF -\\u003e ARABIC LETTER WAW WITH HAMZA ABOVE ISOLATED FORM\\n    \'\\\\ufeed\'   #  0xF0 -\\u003e ARABIC LETTER WAW ISOLATED FORM\\n    \'\\\\ufba6\'   #  0xF1 -\\u003e ARABIC LETTER HEH GOAL ISOLATED FORM\\n    \'\\\\ufba8\'   #  0xF2 -\\u003e ARABIC LETTER HEH GOAL INITIAL FORM\\n    \'\\\\ufba9\'   #  0xF3 -\\u003e ARABIC LETTER HEH GOAL MEDIAL FORM\\n    \'\\\\ufbaa\'   #  0xF4 -\\u003e ARABIC LETTER HEH DOACHASHMEE ISOLATED FORM\\n    \'\\\\ufe80\'   #  0xF5 -\\u003e ARABIC LETTER HAMZA ISOLATED FORM\\n    \'\\\\ufe89\'   #  0xF6 -\\u003e ARABIC LETTER YEH WITH HAMZA ABOVE ISOLATED FORM\\n    \'\\\\ufe8a\'   #  0xF7 -\\u003e ARABIC LETTER YEH WITH HAMZA ABOVE FINAL FORM\\n    \'\\\\ufe8b\'   #  0xF8 -\\u003e ARABIC LETTER YEH WITH HAMZA ABOVE INITIAL FORM\\n    \'\\\\ufef1\'   #  0xF9 -\\u003e ARABIC LETTER YEH ISOLATED FORM\\n    \'\\\\ufef2\'   #  0xFA -\\u003e ARABIC LETTER YEH FINAL FORM\\n    \'\\\\ufef3\'   #  0xFB -\\u003e ARABIC LETTER YEH INITIAL FORM\\n    \'\\\\ufbb0\'   #  0xFC -\\u003e ARABIC LETTER YEH BARREE WITH HAMZA ABOVE ISOLATED FORM\\n    \'\\\\ufbae\'   #  0xFD -\\u003e ARABIC LETTER YEH BARREE ISOLATED FORM\\n    \'\\\\ufe7c\'   #  0xFE -\\u003e ARABIC SHADDA ISOLATED FORM\\n    \'\\\\ufe7d\'   #  0xFF -\\u003e ARABIC SHADDA MEDIAL FORM\\n)\\n\\n### Encoding table\\nencoding_table=codecs.charmap_build(decoding_table)\\n"}\n'
line: b'{"repo_name":"fernandog/Medusa","ref":"refs/heads/optimized","path":"ext/click/termui.py","content":"import os\\nimport sys\\nimport struct\\n\\nfrom ._compat import raw_input, text_type, string_types, \\\\\\n     isatty, strip_ansi, get_winterm_size, DEFAULT_COLUMNS, WIN\\nfrom .utils import echo\\nfrom .exceptions import Abort, UsageError\\nfrom .types import convert_type\\nfrom .globals import resolve_color_default\\n\\n\\n# The prompt functions to use.  The doc tools currently override these\\n# functions to customize how they work.\\nvisible_prompt_func = raw_input\\n\\n_ansi_colors = (\'black\', \'red\', \'green\', \'yellow\', \'blue\', \'magenta\',\\n                \'cyan\', \'white\', \'reset\')\\n_ansi_reset_all = \'\\\\033[0m\'\\n\\n\\ndef hidden_prompt_func(prompt):\\n    import getpass\\n    return getpass.getpass(prompt)\\n\\n\\ndef _build_prompt(text, suffix, show_default=False, default=None):\\n    prompt = text\\n    if default is not None and show_default:\\n        prompt = \'%s [%s]\' % (prompt, default)\\n    return prompt + suffix\\n\\n\\ndef prompt(text, default=None, hide_input=False,\\n           confirmation_prompt=False, type=None,\\n           value_proc=None, prompt_suffix=\': \',\\n           show_default=True, err=False):\\n    \\"\\"\\"Prompts a user for input.  This is a convenience function that can\\n    be used to prompt a user for input later.\\n\\n    If the user aborts the input by sending a interrupt signal, this\\n    function will catch it and raise a :exc:`Abort` exception.\\n\\n    .. versionadded:: 6.0\\n       Added unicode support for cmd.exe on Windows.\\n\\n    .. versionadded:: 4.0\\n       Added the `err` parameter.\\n\\n    :param text: the text to show for the prompt.\\n    :param default: the default value to use if no input happens.  If this\\n                    is not given it will prompt until it\'s aborted.\\n    :param hide_input: if this is set to true then the input value will\\n                       be hidden.\\n    :param confirmation_prompt: asks for confirmation for the value.\\n    :param type: the type to use to check the value against.\\n    :param value_proc: if this parameter is provided it\'s a function that\\n                       is invoked instead of the type conversion to\\n                       convert a value.\\n    :param prompt_suffix: a suffix that should be added to the prompt.\\n    :param show_default: shows or hides the default value in the prompt.\\n    :param err: if set to true the file defaults to ``stderr`` instead of\\n                ``stdout``, the same as with echo.\\n    \\"\\"\\"\\n    result = None\\n\\n    def prompt_func(text):\\n        f = hide_input and hidden_prompt_func or visible_prompt_func\\n        try:\\n            # Write the prompt separately so that we get nice\\n            # coloring through colorama on Windows\\n            echo(text, nl=False, err=err)\\n            return f(\'\')\\n        except (KeyboardInterrupt, EOFError):\\n            # getpass doesn\'t print a newline if the user aborts input with ^C.\\n            # Allegedly this behavior is inherited from getpass(3).\\n            # A doc bug has been filed at https://bugs.python.org/issue24711\\n            if hide_input:\\n                echo(None, err=err)\\n            raise Abort()\\n\\n    if value_proc is None:\\n        value_proc = convert_type(type, default)\\n\\n    prompt = _build_prompt(text, prompt_suffix, show_default, default)\\n\\n    while 1:\\n        while 1:\\n            value = prompt_func(prompt)\\n            if value:\\n                break\\n            # If a default is set and used, then the confirmation\\n            # prompt is always skipped because that\'s the only thing\\n            # that really makes sense.\\n            elif default is not None:\\n                return default\\n        try:\\n            result = value_proc(value)\\n        except UsageError as e:\\n            echo(\'Error: %s\' % e.message, err=err)\\n            continue\\n        if not confirmation_prompt:\\n            return result\\n        while 1:\\n            value2 = prompt_func(\'Repeat for confirmation: \')\\n            if value2:\\n                break\\n        if value == value2:\\n            return result\\n        echo(\'Error: the two entered values do not match\', err=err)\\n\\n\\ndef confirm(text, default=False, abort=False, prompt_suffix=\': \',\\n            show_default=True, err=False):\\n    \\"\\"\\"Prompts for confirmation (yes/no question).\\n\\n    If the user aborts the input by sending a interrupt signal this\\n    function will catch it and raise a :exc:`Abort` exception.\\n\\n    .. versionadded:: 4.0\\n       Added the `err` parameter.\\n\\n    :param text: the question to ask.\\n    :param default: the default for the prompt.\\n    :param abort: if this is set to `True` a negative answer aborts the\\n                  exception by raising :exc:`Abort`.\\n    :param prompt_suffix: a suffix that should be added to the prompt.\\n    :param show_default: shows or hides the default value in the prompt.\\n    :param err: if set to true the file defaults to ``stderr`` instead of\\n                ``stdout``, the same as with echo.\\n    \\"\\"\\"\\n    prompt = _build_prompt(text, prompt_suffix, show_default,\\n                           default and \'Y/n\' or \'y/N\')\\n    while 1:\\n        try:\\n            # Write the prompt separately so that we get nice\\n            # coloring through colorama on Windows\\n            echo(prompt, nl=False, err=err)\\n            value = visible_prompt_func(\'\').lower().strip()\\n        except (KeyboardInterrupt, EOFError):\\n            raise Abort()\\n        if value in (\'y\', \'yes\'):\\n            rv = True\\n        elif value in (\'n\', \'no\'):\\n            rv = False\\n        elif value == \'\':\\n            rv = default\\n        else:\\n            echo(\'Error: invalid input\', err=err)\\n            continue\\n        break\\n    if abort and not rv:\\n        raise Abort()\\n    return rv\\n\\n\\ndef get_terminal_size():\\n    \\"\\"\\"Returns the current size of the terminal as tuple in the form\\n    ``(width, height)`` in columns and rows.\\n    \\"\\"\\"\\n    # If shutil has get_terminal_size() (Python 3.3 and later) use that\\n    if sys.version_info \\u003e= (3, 3):\\n        import shutil\\n        shutil_get_terminal_size = getattr(shutil, \'get_terminal_size\', None)\\n        if shutil_get_terminal_size:\\n            sz = shutil_get_terminal_size()\\n            return sz.columns, sz.lines\\n\\n    if get_winterm_size is not None:\\n        return get_winterm_size()\\n\\n    def ioctl_gwinsz(fd):\\n        try:\\n            import fcntl\\n            import termios\\n            cr = struct.unpack(\\n                \'hh\', fcntl.ioctl(fd, termios.TIOCGWINSZ, \'1234\'))\\n        except Exception:\\n            return\\n        return cr\\n\\n    cr = ioctl_gwinsz(0) or ioctl_gwinsz(1) or ioctl_gwinsz(2)\\n    if not cr:\\n        try:\\n            fd = os.open(os.ctermid(), os.O_RDONLY)\\n            try:\\n                cr = ioctl_gwinsz(fd)\\n            finally:\\n                os.close(fd)\\n        except Exception:\\n            pass\\n    if not cr or not cr[0] or not cr[1]:\\n        cr = (os.environ.get(\'LINES\', 25),\\n              os.environ.get(\'COLUMNS\', DEFAULT_COLUMNS))\\n    return int(cr[1]), int(cr[0])\\n\\n\\ndef echo_via_pager(text, color=None):\\n    \\"\\"\\"This function takes a text and shows it via an environment specific\\n    pager on stdout.\\n\\n    .. versionchanged:: 3.0\\n       Added the `color` flag.\\n\\n    :param text: the text to page.\\n    :param color: controls if the pager supports ANSI colors or not.  The\\n                  default is autodetection.\\n    \\"\\"\\"\\n    color = resolve_color_default(color)\\n    if not isinstance(text, string_types):\\n        text = text_type(text)\\n    from ._termui_impl import pager\\n    return pager(text + \'\\\\n\', color)\\n\\n\\ndef progressbar(iterable=None, length=None, label=None, show_eta=True,\\n                show_percent=None, show_pos=False,\\n                item_show_func=None, fill_char=\'#\', empty_char=\'-\',\\n                bar_template=\'%(label)s  [%(bar)s]  %(info)s\',\\n                info_sep=\'  \', width=36, file=None, color=None):\\n    \\"\\"\\"This function creates an iterable context manager that can be used\\n    to iterate over something while showing a progress bar.  It will\\n    either iterate over the `iterable` or `length` items (that are counted\\n    up).  While iteration happens, this function will print a rendered\\n    progress bar to the given `file` (defaults to stdout) and will attempt\\n    to calculate remaining time and more.  By default, this progress bar\\n    will not be rendered if the file is not a terminal.\\n\\n    The context manager creates the progress bar.  When the context\\n    manager is entered the progress bar is already displayed.  With every\\n    iteration over the progress bar, the iterable passed to the bar is\\n    advanced and the bar is updated.  When the context manager exits,\\n    a newline is printed and the progress bar is finalized on screen.\\n\\n    No printing must happen or the progress bar will be unintentionally\\n    destroyed.\\n\\n    Example usage::\\n\\n        with progressbar(items) as bar:\\n            for item in bar:\\n                do_something_with(item)\\n\\n    Alternatively, if no iterable is specified, one can manually update the\\n    progress bar through the `update()` method instead of directly\\n    iterating over the progress bar.  The update method accepts the number\\n    of steps to increment the bar with::\\n\\n        with progressbar(length=chunks.total_bytes) as bar:\\n            for chunk in chunks:\\n                process_chunk(chunk)\\n                bar.update(chunks.bytes)\\n\\n    .. versionadded:: 2.0\\n\\n    .. versionadded:: 4.0\\n       Added the `color` parameter.  Added a `update` method to the\\n       progressbar object.\\n\\n    :param iterable: an iterable to iterate over.  If not provided the length\\n                     is required.\\n    :param length: the number of items to iterate over.  By default the\\n                   progressbar will attempt to ask the iterator about its\\n                   length, which might or might not work.  If an iterable is\\n                   also provided this parameter can be used to override the\\n                   length.  If an iterable is not provided the progress bar\\n                   will iterate over a range of that length.\\n    :param label: the label to show next to the progress bar.\\n    :param show_eta: enables or disables the estimated time display.  This is\\n                     automatically disabled if the length cannot be\\n                     determined.\\n    :param show_percent: enables or disables the percentage display.  The\\n                         default is `True` if the iterable has a length or\\n                         `False` if not.\\n    :param show_pos: enables or disables the absolute position display.  The\\n                     default is `False`.\\n    :param item_show_func: a function called with the current item which\\n                           can return a string to show the current item\\n                           next to the progress bar.  Note that the current\\n                           item can be `None`!\\n    :param fill_char: the character to use to show the filled part of the\\n                      progress bar.\\n    :param empty_char: the character to use to show the non-filled part of\\n                       the progress bar.\\n    :param bar_template: the format string to use as template for the bar.\\n                         The parameters in it are ``label`` for the label,\\n                         ``bar`` for the progress bar and ``info`` for the\\n                         info section.\\n    :param info_sep: the separator between multiple info items (eta etc.)\\n    :param width: the width of the progress bar in characters, 0 means full\\n                  terminal width\\n    :param file: the file to write to.  If this is not a terminal then\\n                 only the label is printed.\\n    :param color: controls if the terminal supports ANSI colors or not.  The\\n                  default is autodetection.  This is only needed if ANSI\\n                  codes are included anywhere in the progress bar output\\n                  which is not the case by default.\\n    \\"\\"\\"\\n    from ._termui_impl import ProgressBar\\n    color = resolve_color_default(color)\\n    return ProgressBar(iterable=iterable, length=length, show_eta=show_eta,\\n                       show_percent=show_percent, show_pos=show_pos,\\n                       item_show_func=item_show_func, fill_char=fill_char,\\n                       empty_char=empty_char, bar_template=bar_template,\\n                       info_sep=info_sep, file=file, label=label,\\n                       width=width, color=color)\\n\\n\\ndef clear():\\n    \\"\\"\\"Clears the terminal screen.  This will have the effect of clearing\\n    the whole visible space of the terminal and moving the cursor to the\\n    top left.  This does not do anything if not connected to a terminal.\\n\\n    .. versionadded:: 2.0\\n    \\"\\"\\"\\n    if not isatty(sys.stdout):\\n        return\\n    # If we\'re on Windows and we don\'t have colorama available, then we\\n    # clear the screen by shelling out.  Otherwise we can use an escape\\n    # sequence.\\n    if WIN:\\n        os.system(\'cls\')\\n    else:\\n        sys.stdout.write(\'\\\\033[2J\\\\033[1;1H\')\\n\\n\\ndef style(text, fg=None, bg=None, bold=None, dim=None, underline=None,\\n          blink=None, reverse=None, reset=True):\\n    \\"\\"\\"Styles a text with ANSI styles and returns the new string.  By\\n    default the styling is self contained which means that at the end\\n    of the string a reset code is issued.  This can be prevented by\\n    passing ``reset=False``.\\n\\n    Examples::\\n\\n        click.echo(click.style(\'Hello World!\', fg=\'green\'))\\n        click.echo(click.style(\'ATTENTION!\', blink=True))\\n        click.echo(click.style(\'Some things\', reverse=True, fg=\'cyan\'))\\n\\n    Supported color names:\\n\\n    * ``black`` (might be a gray)\\n    * ``red``\\n    * ``green``\\n    * ``yellow`` (might be an orange)\\n    * ``blue``\\n    * ``magenta``\\n    * ``cyan``\\n    * ``white`` (might be light gray)\\n    * ``reset`` (reset the color code only)\\n\\n    .. versionadded:: 2.0\\n\\n    :param text: the string to style with ansi codes.\\n    :param fg: if provided this will become the foreground color.\\n    :param bg: if provided this will become the background color.\\n    :param bold: if provided this will enable or disable bold mode.\\n    :param dim: if provided this will enable or disable dim mode.  This is\\n                badly supported.\\n    :param underline: if provided this will enable or disable underline.\\n    :param blink: if provided this will enable or disable blinking.\\n    :param reverse: if provided this will enable or disable inverse\\n                    rendering (foreground becomes background and the\\n                    other way round).\\n    :param reset: by default a reset-all code is added at the end of the\\n                  string which means that styles do not carry over.  This\\n                  can be disabled to compose styles.\\n    \\"\\"\\"\\n    bits = []\\n    if fg:\\n        try:\\n            bits.append(\'\\\\033[%dm\' % (_ansi_colors.index(fg) + 30))\\n        except ValueError:\\n            raise TypeError(\'Unknown color %r\' % fg)\\n    if bg:\\n        try:\\n            bits.append(\'\\\\033[%dm\' % (_ansi_colors.index(bg) + 40))\\n        except ValueError:\\n            raise TypeError(\'Unknown color %r\' % bg)\\n    if bold is not None:\\n        bits.append(\'\\\\033[%dm\' % (1 if bold else 22))\\n    if dim is not None:\\n        bits.append(\'\\\\033[%dm\' % (2 if dim else 22))\\n    if underline is not None:\\n        bits.append(\'\\\\033[%dm\' % (4 if underline else 24))\\n    if blink is not None:\\n        bits.append(\'\\\\033[%dm\' % (5 if blink else 25))\\n    if reverse is not None:\\n        bits.append(\'\\\\033[%dm\' % (7 if reverse else 27))\\n    bits.append(text)\\n    if reset:\\n        bits.append(_ansi_reset_all)\\n    return \'\'.join(bits)\\n\\n\\ndef unstyle(text):\\n    \\"\\"\\"Removes ANSI styling information from a string.  Usually it\'s not\\n    necessary to use this function as Click\'s echo function will\\n    automatically remove styling if necessary.\\n\\n    .. versionadded:: 2.0\\n\\n    :param text: the text to remove style information from.\\n    \\"\\"\\"\\n    return strip_ansi(text)\\n\\n\\ndef secho(text, file=None, nl=True, err=False, color=None, **styles):\\n    \\"\\"\\"This function combines :func:`echo` and :func:`style` into one\\n    call.  As such the following two calls are the same::\\n\\n        click.secho(\'Hello World!\', fg=\'green\')\\n        click.echo(click.style(\'Hello World!\', fg=\'green\'))\\n\\n    All keyword arguments are forwarded to the underlying functions\\n    depending on which one they go with.\\n\\n    .. versionadded:: 2.0\\n    \\"\\"\\"\\n    return echo(style(text, **styles), file=file, nl=nl, err=err, color=color)\\n\\n\\ndef edit(text=None, editor=None, env=None, require_save=True,\\n         extension=\'.txt\', filename=None):\\n    r\\"\\"\\"Edits the given text in the defined editor.  If an editor is given\\n    (should be the full path to the executable but the regular operating\\n    system search path is used for finding the executable) it overrides\\n    the detected editor.  Optionally, some environment variables can be\\n    used.  If the editor is closed without changes, `None` is returned.  In\\n    case a file is edited directly the return value is always `None` and\\n    `require_save` and `extension` are ignored.\\n\\n    If the editor cannot be opened a :exc:`UsageError` is raised.\\n\\n    Note for Windows: to simplify cross-platform usage, the newlines are\\n    automatically converted from POSIX to Windows and vice versa.  As such,\\n    the message here will have ``\\\\n`` as newline markers.\\n\\n    :param text: the text to edit.\\n    :param editor: optionally the editor to use.  Defaults to automatic\\n                   detection.\\n    :param env: environment variables to forward to the editor.\\n    :param require_save: if this is true, then not saving in the editor\\n                         will make the return value become `None`.\\n    :param extension: the extension to tell the editor about.  This defaults\\n                      to `.txt` but changing this might change syntax\\n                      highlighting.\\n    :param filename: if provided it will edit this file instead of the\\n                     provided text contents.  It will not use a temporary\\n                     file as an indirection in that case.\\n    \\"\\"\\"\\n    from ._termui_impl import Editor\\n    editor = Editor(editor=editor, env=env, require_save=require_save,\\n                    extension=extension)\\n    if filename is None:\\n        return editor.edit(text)\\n    editor.edit_file(filename)\\n\\n\\ndef launch(url, wait=False, locate=False):\\n    \\"\\"\\"This function launches the given URL (or filename) in the default\\n    viewer application for this file type.  If this is an executable, it\\n    might launch the executable in a new session.  The return value is\\n    the exit code of the launched application.  Usually, ``0`` indicates\\n    success.\\n\\n    Examples::\\n\\n        click.launch(\'http://click.pocoo.org/\')\\n        click.launch(\'/my/downloaded/file\', locate=True)\\n\\n    .. versionadded:: 2.0\\n\\n    :param url: URL or filename of the thing to launch.\\n    :param wait: waits for the program to stop.\\n    :param locate: if this is set to `True` then instead of launching the\\n                   application associated with the URL it will attempt to\\n                   launch a file manager with the file located.  This\\n                   might have weird effects if the URL does not point to\\n                   the filesystem.\\n    \\"\\"\\"\\n    from ._termui_impl import open_url\\n    return open_url(url, wait=wait, locate=locate)\\n\\n\\n# If this is provided, getchar() calls into this instead.  This is used\\n# for unittesting purposes.\\n_getchar = None\\n\\n\\ndef getchar(echo=False):\\n    \\"\\"\\"Fetches a single character from the terminal and returns it.  This\\n    will always return a unicode character and under certain rare\\n    circumstances this might return more than one character.  The\\n    situations which more than one character is returned is when for\\n    whatever reason multiple characters end up in the terminal buffer or\\n    standard input was not actually a terminal.\\n\\n    Note that this will always read from the terminal, even if something\\n    is piped into the standard input.\\n\\n    .. versionadded:: 2.0\\n\\n    :param echo: if set to `True`, the character read will also show up on\\n                 the terminal.  The default is to not show it.\\n    \\"\\"\\"\\n    f = _getchar\\n    if f is None:\\n        from ._termui_impl import getchar as f\\n    return f(echo)\\n\\n\\ndef pause(info=\'Press any key to continue ...\', err=False):\\n    \\"\\"\\"This command stops execution and waits for the user to press any\\n    key to continue.  This is similar to the Windows batch \\"pause\\"\\n    command.  If the program is not run through a terminal, this command\\n    will instead do nothing.\\n\\n    .. versionadded:: 2.0\\n\\n    .. versionadded:: 4.0\\n       Added the `err` parameter.\\n\\n    :param info: the info string to print before pausing.\\n    :param err: if set to message goes to ``stderr`` instead of\\n                ``stdout``, the same as with echo.\\n    \\"\\"\\"\\n    if not isatty(sys.stdin) or not isatty(sys.stdout):\\n        return\\n    try:\\n        if info:\\n            echo(info, nl=False, err=err)\\n        try:\\n            getchar()\\n        except (KeyboardInterrupt, EOFError):\\n            pass\\n    finally:\\n        if info:\\n            echo(err=err)\\n"}\n'
line: b'{"repo_name":"robertglen/flask","ref":"refs/heads/master","path":"tests/test_instance_config.py","content":"# -*- coding: utf-8 -*-\\n\\"\\"\\"\\n    tests.test_instance\\n    ~~~~~~~~~~~~~~~~~~~\\n\\n    :copyright: (c) 2015 by the Flask Team, see AUTHORS for more details.\\n    :license: BSD, see LICENSE for more details.\\n\\"\\"\\"\\nimport os\\nimport sys\\n\\nimport pytest\\nimport flask\\nfrom flask._compat import PY2\\n\\n\\ndef test_explicit_instance_paths(modules_tmpdir):\\n    with pytest.raises(ValueError) as excinfo:\\n        flask.Flask(__name__, instance_path=\'instance\')\\n    assert \'must be absolute\' in str(excinfo.value)\\n\\n    app = flask.Flask(__name__, instance_path=str(modules_tmpdir))\\n    assert app.instance_path == str(modules_tmpdir)\\n\\n\\ndef test_main_module_paths(modules_tmpdir, purge_module):\\n    app = modules_tmpdir.join(\'main_app.py\')\\n    app.write(\'import flask\\\\n\\\\napp = flask.Flask(\\"__main__\\")\')\\n    purge_module(\'main_app\')\\n\\n    from main_app import app\\n    here = os.path.abspath(os.getcwd())\\n    assert app.instance_path == os.path.join(here, \'instance\')\\n\\n\\ndef test_uninstalled_module_paths(modules_tmpdir, purge_module):\\n    app = modules_tmpdir.join(\'config_module_app.py\').write(\\n        \'import os\\\\n\'\\n        \'import flask\\\\n\'\\n        \'here = os.path.abspath(os.path.dirname(__file__))\\\\n\'\\n        \'app = flask.Flask(__name__)\\\\n\'\\n    )\\n    purge_module(\'config_module_app\')\\n\\n    from config_module_app import app\\n    assert app.instance_path == str(modules_tmpdir.join(\'instance\'))\\n\\n\\ndef test_uninstalled_package_paths(modules_tmpdir, purge_module):\\n    app = modules_tmpdir.mkdir(\'config_package_app\')\\n    init = app.join(\'__init__.py\')\\n    init.write(\\n        \'import os\\\\n\'\\n        \'import flask\\\\n\'\\n        \'here = os.path.abspath(os.path.dirname(__file__))\\\\n\'\\n        \'app = flask.Flask(__name__)\\\\n\'\\n    )\\n    purge_module(\'config_package_app\')\\n\\n    from config_package_app import app\\n    assert app.instance_path == str(modules_tmpdir.join(\'instance\'))\\n\\n\\ndef test_installed_module_paths(modules_tmpdir, modules_tmpdir_prefix,\\n                                purge_module, site_packages, limit_loader):\\n    site_packages.join(\'site_app.py\').write(\\n        \'import flask\\\\n\'\\n        \'app = flask.Flask(__name__)\\\\n\'\\n    )\\n    purge_module(\'site_app\')\\n\\n    from site_app import app\\n    assert app.instance_path == \\\\\\n        modules_tmpdir.join(\'var\').join(\'site_app-instance\')\\n\\n\\ndef test_installed_package_paths(limit_loader, modules_tmpdir,\\n                                 modules_tmpdir_prefix, purge_module,\\n                                 monkeypatch):\\n    installed_path = modules_tmpdir.mkdir(\'path\')\\n    monkeypatch.syspath_prepend(installed_path)\\n\\n    app = installed_path.mkdir(\'installed_package\')\\n    init = app.join(\'__init__.py\')\\n    init.write(\'import flask\\\\napp = flask.Flask(__name__)\')\\n    purge_module(\'installed_package\')\\n\\n    from installed_package import app\\n    assert app.instance_path == \\\\\\n        modules_tmpdir.join(\'var\').join(\'installed_package-instance\')\\n\\n\\ndef test_prefix_package_paths(limit_loader, modules_tmpdir,\\n                              modules_tmpdir_prefix, purge_module,\\n                              site_packages):\\n    app = site_packages.mkdir(\'site_package\')\\n    init = app.join(\'__init__.py\')\\n    init.write(\'import flask\\\\napp = flask.Flask(__name__)\')\\n    purge_module(\'site_package\')\\n\\n    import site_package\\n    assert site_package.app.instance_path == \\\\\\n        modules_tmpdir.join(\'var\').join(\'site_package-instance\')\\n\\n\\ndef test_egg_installed_paths(install_egg, modules_tmpdir,\\n                             modules_tmpdir_prefix):\\n    modules_tmpdir.mkdir(\'site_egg\').join(\'__init__.py\').write(\\n        \'import flask\\\\n\\\\napp = flask.Flask(__name__)\'\\n    )\\n    install_egg(\'site_egg\')\\n    try:\\n        import site_egg\\n        assert site_egg.app.instance_path == \\\\\\n            str(modules_tmpdir.join(\'var/\').join(\'site_egg-instance\'))\\n    finally:\\n        if \'site_egg\' in sys.modules:\\n            del sys.modules[\'site_egg\']\\n\\n\\n@pytest.mark.skipif(not PY2, reason=\'This only works under Python 2.\')\\ndef test_meta_path_loader_without_is_package(request, modules_tmpdir):\\n    app = modules_tmpdir.join(\'unimportable.py\')\\n    app.write(\'import flask\\\\napp = flask.Flask(__name__)\')\\n\\n    class Loader(object):\\n        def find_module(self, name, path=None):\\n            return self\\n\\n    sys.meta_path.append(Loader())\\n    request.addfinalizer(sys.meta_path.pop)\\n\\n    with pytest.raises(AttributeError):\\n        import unimportable\\n"}\n'
line: b'{"repo_name":"thomasgilgenast/gilgistatus-nonrel","ref":"refs/heads/master","path":"django/db/backends/creation.py","content":"import sys\\nimport time\\n\\nfrom django.conf import settings\\nfrom django.utils.datastructures import DictWrapper\\n\\n# The prefix to put on the default database name when creating\\n# the test database.\\nTEST_DATABASE_PREFIX = \'test_\'\\n\\nclass BaseDatabaseCreation(object):\\n    \\"\\"\\"\\n    This class encapsulates all backend-specific differences that pertain to\\n    database *creation*, such as the column types to use for particular Django\\n    Fields, the SQL used to create and destroy tables, and the creation and\\n    destruction of test databases.\\n    \\"\\"\\"\\n    data_types = {}\\n\\n    def __init__(self, connection):\\n        self.connection = connection\\n\\n    def _digest(self, *args):\\n        \\"\\"\\"\\n        Generates a 32-bit digest of a set of arguments that can be used to\\n        shorten identifying names.\\n        \\"\\"\\"\\n        return \'%x\' % (abs(hash(args)) % 4294967296L)  # 2**32\\n    \\n    def db_type(self, field):\\n        return self._db_type(field, field.get_internal_type())\\n\\n    def related_db_type(self, field):\\n        return self._db_type(field, field.get_related_internal_type())\\n\\n    def _db_type(self, field, internal_type):\\n        data = DictWrapper(field.__dict__, self.connection.ops.quote_name, \\"qn_\\")\\n        try:\\n            return self.connection.creation.data_types[internal_type] % data\\n        except KeyError:\\n            return None\\n\\n    def sql_create_model(self, model, style, known_models=set()):\\n        \\"\\"\\"\\n        Returns the SQL required to create a single model, as a tuple of:\\n            (list_of_sql, pending_references_dict)\\n        \\"\\"\\"\\n        opts = model._meta\\n        if not opts.managed or opts.proxy:\\n            return [], {}\\n        final_output = []\\n        table_output = []\\n        pending_references = {}\\n        qn = self.connection.ops.quote_name\\n        for f in opts.local_fields:\\n            col_type = f.db_type(connection=self.connection)\\n            tablespace = f.db_tablespace or opts.db_tablespace\\n            if col_type is None:\\n                # Skip ManyToManyFields, because they\'re not represented as\\n                # database columns in this table.\\n                continue\\n            # Make the definition (e.g. \'foo VARCHAR(30)\') for this field.\\n            field_output = [style.SQL_FIELD(qn(f.column)),\\n                style.SQL_COLTYPE(col_type)]\\n            if not f.null:\\n                field_output.append(style.SQL_KEYWORD(\'NOT NULL\'))\\n            if f.primary_key:\\n                field_output.append(style.SQL_KEYWORD(\'PRIMARY KEY\'))\\n            elif f.unique:\\n                field_output.append(style.SQL_KEYWORD(\'UNIQUE\'))\\n            if tablespace and f.unique:\\n                # We must specify the index tablespace inline, because we\\n                # won\'t be generating a CREATE INDEX statement for this field.\\n                field_output.append(self.connection.ops.tablespace_sql(tablespace, inline=True))\\n            if f.rel:\\n                ref_output, pending = self.sql_for_inline_foreign_key_references(f, known_models, style)\\n                if pending:\\n                    pr = pending_references.setdefault(f.rel.to, []).append((model, f))\\n                else:\\n                    field_output.extend(ref_output)\\n            table_output.append(\' \'.join(field_output))\\n        for field_constraints in opts.unique_together:\\n            table_output.append(style.SQL_KEYWORD(\'UNIQUE\') + \' (%s)\' % \\\\\\n                \\", \\".join([style.SQL_FIELD(qn(opts.get_field(f).column)) for f in field_constraints]))\\n\\n        full_statement = [style.SQL_KEYWORD(\'CREATE TABLE\') + \' \' + style.SQL_TABLE(qn(opts.db_table)) + \' (\']\\n        for i, line in enumerate(table_output): # Combine and add commas.\\n            full_statement.append(\'    %s%s\' % (line, i \\u003c len(table_output)-1 and \',\' or \'\'))\\n        full_statement.append(\')\')\\n        if opts.db_tablespace:\\n            full_statement.append(self.connection.ops.tablespace_sql(opts.db_tablespace))\\n        full_statement.append(\';\')\\n        final_output.append(\'\\\\n\'.join(full_statement))\\n\\n        if opts.has_auto_field:\\n            # Add any extra SQL needed to support auto-incrementing primary keys.\\n            auto_column = opts.auto_field.db_column or opts.auto_field.name\\n            autoinc_sql = self.connection.ops.autoinc_sql(opts.db_table, auto_column)\\n            if autoinc_sql:\\n                for stmt in autoinc_sql:\\n                    final_output.append(stmt)\\n\\n        return final_output, pending_references\\n\\n    def sql_for_inline_foreign_key_references(self, field, known_models, style):\\n        \\"Return the SQL snippet defining the foreign key reference for a field\\"\\n        qn = self.connection.ops.quote_name\\n        if field.rel.to in known_models:\\n            output = [style.SQL_KEYWORD(\'REFERENCES\') + \' \' + \\\\\\n                style.SQL_TABLE(qn(field.rel.to._meta.db_table)) + \' (\' + \\\\\\n                style.SQL_FIELD(qn(field.rel.to._meta.get_field(field.rel.field_name).column)) + \')\' +\\n                self.connection.ops.deferrable_sql()\\n            ]\\n            pending = False\\n        else:\\n            # We haven\'t yet created the table to which this field\\n            # is related, so save it for later.\\n            output = []\\n            pending = True\\n\\n        return output, pending\\n\\n    def sql_for_pending_references(self, model, style, pending_references):\\n        \\"Returns any ALTER TABLE statements to add constraints after the fact.\\"\\n        from django.db.backends.util import truncate_name\\n\\n        if not model._meta.managed or model._meta.proxy:\\n            return []\\n        qn = self.connection.ops.quote_name\\n        final_output = []\\n        opts = model._meta\\n        if model in pending_references:\\n            for rel_class, f in pending_references[model]:\\n                rel_opts = rel_class._meta\\n                r_table = rel_opts.db_table\\n                r_col = f.column\\n                table = opts.db_table\\n                col = opts.get_field(f.rel.field_name).column\\n                # For MySQL, r_name must be unique in the first 64 characters.\\n                # So we are careful with character usage here.\\n                r_name = \'%s_refs_%s_%s\' % (r_col, col, self._digest(r_table, table))\\n                final_output.append(style.SQL_KEYWORD(\'ALTER TABLE\') + \' %s ADD CONSTRAINT %s FOREIGN KEY (%s) REFERENCES %s (%s)%s;\' % \\\\\\n                    (qn(r_table), qn(truncate_name(r_name, self.connection.ops.max_name_length())),\\n                    qn(r_col), qn(table), qn(col),\\n                    self.connection.ops.deferrable_sql()))\\n            del pending_references[model]\\n        return final_output\\n\\n    def sql_for_many_to_many(self, model, style):\\n        \\"Return the CREATE TABLE statments for all the many-to-many tables defined on a model\\"\\n        import warnings\\n        warnings.warn(\\n            \'Database creation API for m2m tables has been deprecated. M2M models are now automatically generated\',\\n            DeprecationWarning\\n        )\\n\\n        output = []\\n        for f in model._meta.local_many_to_many:\\n            if model._meta.managed or f.rel.to._meta.managed:\\n                output.extend(self.sql_for_many_to_many_field(model, f, style))\\n        return output\\n\\n    def sql_for_many_to_many_field(self, model, f, style):\\n        \\"Return the CREATE TABLE statements for a single m2m field\\"\\n        import warnings\\n        warnings.warn(\\n            \'Database creation API for m2m tables has been deprecated. M2M models are now automatically generated\',\\n            DeprecationWarning\\n        )\\n\\n        from django.db import models\\n        from django.db.backends.util import truncate_name\\n\\n        output = []\\n        if f.auto_created:\\n            opts = model._meta\\n            qn = self.connection.ops.quote_name\\n            tablespace = f.db_tablespace or opts.db_tablespace\\n            if tablespace:\\n                sql = self.connection.ops.tablespace_sql(tablespace, inline=True)\\n                if sql:\\n                    tablespace_sql = \' \' + sql\\n                else:\\n                    tablespace_sql = \'\'\\n            else:\\n                tablespace_sql = \'\'\\n            table_output = [style.SQL_KEYWORD(\'CREATE TABLE\') + \' \' + \\\\\\n                style.SQL_TABLE(qn(f.m2m_db_table())) + \' (\']\\n            table_output.append(\'    %s %s %s%s,\' %\\n                (style.SQL_FIELD(qn(\'id\')),\\n                style.SQL_COLTYPE(models.AutoField(primary_key=True).db_type(connection=self.connection)),\\n                style.SQL_KEYWORD(\'NOT NULL PRIMARY KEY\'),\\n                tablespace_sql))\\n\\n            deferred = []\\n            inline_output, deferred = self.sql_for_inline_many_to_many_references(model, f, style)\\n            table_output.extend(inline_output)\\n\\n            table_output.append(\'    %s (%s, %s)%s\' %\\n                (style.SQL_KEYWORD(\'UNIQUE\'),\\n                style.SQL_FIELD(qn(f.m2m_column_name())),\\n                style.SQL_FIELD(qn(f.m2m_reverse_name())),\\n                tablespace_sql))\\n            table_output.append(\')\')\\n            if opts.db_tablespace:\\n                # f.db_tablespace is only for indices, so ignore its value here.\\n                table_output.append(self.connection.ops.tablespace_sql(opts.db_tablespace))\\n            table_output.append(\';\')\\n            output.append(\'\\\\n\'.join(table_output))\\n\\n            for r_table, r_col, table, col in deferred:\\n                r_name = \'%s_refs_%s_%s\' % (r_col, col, self._digest(r_table, table))\\n                output.append(style.SQL_KEYWORD(\'ALTER TABLE\') + \' %s ADD CONSTRAINT %s FOREIGN KEY (%s) REFERENCES %s (%s)%s;\' %\\n                (qn(r_table),\\n                qn(truncate_name(r_name, self.connection.ops.max_name_length())),\\n                qn(r_col), qn(table), qn(col),\\n                self.connection.ops.deferrable_sql()))\\n\\n            # Add any extra SQL needed to support auto-incrementing PKs\\n            autoinc_sql = self.connection.ops.autoinc_sql(f.m2m_db_table(), \'id\')\\n            if autoinc_sql:\\n                for stmt in autoinc_sql:\\n                    output.append(stmt)\\n        return output\\n\\n    def sql_for_inline_many_to_many_references(self, model, field, style):\\n        \\"Create the references to other tables required by a many-to-many table\\"\\n        import warnings\\n        warnings.warn(\\n            \'Database creation API for m2m tables has been deprecated. M2M models are now automatically generated\',\\n            DeprecationWarning\\n        )\\n\\n        from django.db import models\\n        opts = model._meta\\n        qn = self.connection.ops.quote_name\\n\\n        table_output = [\\n            \'    %s %s %s %s (%s)%s,\' %\\n                (style.SQL_FIELD(qn(field.m2m_column_name())),\\n                style.SQL_COLTYPE(models.ForeignKey(model).db_type(connection=self.connection)),\\n                style.SQL_KEYWORD(\'NOT NULL REFERENCES\'),\\n                style.SQL_TABLE(qn(opts.db_table)),\\n                style.SQL_FIELD(qn(opts.pk.column)),\\n                self.connection.ops.deferrable_sql()),\\n            \'    %s %s %s %s (%s)%s,\' %\\n                (style.SQL_FIELD(qn(field.m2m_reverse_name())),\\n                style.SQL_COLTYPE(models.ForeignKey(field.rel.to).db_type(connection=self.connection)),\\n                style.SQL_KEYWORD(\'NOT NULL REFERENCES\'),\\n                style.SQL_TABLE(qn(field.rel.to._meta.db_table)),\\n                style.SQL_FIELD(qn(field.rel.to._meta.pk.column)),\\n                self.connection.ops.deferrable_sql())\\n        ]\\n        deferred = []\\n\\n        return table_output, deferred\\n\\n    def sql_indexes_for_model(self, model, style):\\n        \\"Returns the CREATE INDEX SQL statements for a single model\\"\\n        if not model._meta.managed or model._meta.proxy:\\n            return []\\n        output = []\\n        for f in model._meta.local_fields:\\n            output.extend(self.sql_indexes_for_field(model, f, style))\\n        return output\\n\\n    def sql_indexes_for_field(self, model, f, style):\\n        \\"Return the CREATE INDEX SQL statements for a single model field\\"\\n        from django.db.backends.util import truncate_name\\n\\n        if f.db_index and not f.unique:\\n            qn = self.connection.ops.quote_name\\n            tablespace = f.db_tablespace or model._meta.db_tablespace\\n            if tablespace:\\n                sql = self.connection.ops.tablespace_sql(tablespace)\\n                if sql:\\n                    tablespace_sql = \' \' + sql\\n                else:\\n                    tablespace_sql = \'\'\\n            else:\\n                tablespace_sql = \'\'\\n            i_name = \'%s_%s\' % (model._meta.db_table, self._digest(f.column))\\n            output = [style.SQL_KEYWORD(\'CREATE INDEX\') + \' \' +\\n                style.SQL_TABLE(qn(truncate_name(i_name, self.connection.ops.max_name_length()))) + \' \' +\\n                style.SQL_KEYWORD(\'ON\') + \' \' +\\n                style.SQL_TABLE(qn(model._meta.db_table)) + \' \' +\\n                \\"(%s)\\" % style.SQL_FIELD(qn(f.column)) +\\n                \\"%s;\\" % tablespace_sql]\\n        else:\\n            output = []\\n        return output\\n\\n    def sql_destroy_model(self, model, references_to_delete, style):\\n        \\"Return the DROP TABLE and restraint dropping statements for a single model\\"\\n        if not model._meta.managed or model._meta.proxy:\\n            return []\\n        # Drop the table now\\n        qn = self.connection.ops.quote_name\\n        output = [\'%s %s;\' % (style.SQL_KEYWORD(\'DROP TABLE\'),\\n                              style.SQL_TABLE(qn(model._meta.db_table)))]\\n        if model in references_to_delete:\\n            output.extend(self.sql_remove_table_constraints(model, references_to_delete, style))\\n\\n        if model._meta.has_auto_field:\\n            ds = self.connection.ops.drop_sequence_sql(model._meta.db_table)\\n            if ds:\\n                output.append(ds)\\n        return output\\n\\n    def sql_remove_table_constraints(self, model, references_to_delete, style):\\n        from django.db.backends.util import truncate_name\\n\\n        if not model._meta.managed or model._meta.proxy:\\n            return []\\n        output = []\\n        qn = self.connection.ops.quote_name\\n        for rel_class, f in references_to_delete[model]:\\n            table = rel_class._meta.db_table\\n            col = f.column\\n            r_table = model._meta.db_table\\n            r_col = model._meta.get_field(f.rel.field_name).column\\n            r_name = \'%s_refs_%s_%s\' % (col, r_col, self._digest(table, r_table))\\n            output.append(\'%s %s %s %s;\' % \\\\\\n                (style.SQL_KEYWORD(\'ALTER TABLE\'),\\n                style.SQL_TABLE(qn(table)),\\n                style.SQL_KEYWORD(self.connection.ops.drop_foreignkey_sql()),\\n                style.SQL_FIELD(qn(truncate_name(r_name, self.connection.ops.max_name_length())))))\\n        del references_to_delete[model]\\n        return output\\n\\n    def sql_destroy_many_to_many(self, model, f, style):\\n        \\"Returns the DROP TABLE statements for a single m2m field\\"\\n        import warnings\\n        warnings.warn(\\n            \'Database creation API for m2m tables has been deprecated. M2M models are now automatically generated\',\\n            DeprecationWarning\\n        )\\n\\n        qn = self.connection.ops.quote_name\\n        output = []\\n        if f.auto_created:\\n            output.append(\\"%s %s;\\" % (style.SQL_KEYWORD(\'DROP TABLE\'),\\n                style.SQL_TABLE(qn(f.m2m_db_table()))))\\n            ds = self.connection.ops.drop_sequence_sql(\\"%s_%s\\" % (model._meta.db_table, f.column))\\n            if ds:\\n                output.append(ds)\\n        return output\\n\\n    def create_test_db(self, verbosity=1, autoclobber=False):\\n        \\"\\"\\"\\n        Creates a test database, prompting the user for confirmation if the\\n        database already exists. Returns the name of the test database created.\\n        \\"\\"\\"\\n        # Don\'t import django.core.management if it isn\'t needed.\\n        from django.core.management import call_command\\n\\n        test_database_name = self._get_test_db_name()\\n\\n        if verbosity \\u003e= 1:\\n            test_db_repr = \'\'\\n            if verbosity \\u003e= 2:\\n                test_db_repr = \\" (\'%s\')\\" % test_database_name\\n            print \\"Creating test database for alias \'%s\'%s...\\" % (self.connection.alias, test_db_repr)\\n\\n        self._create_test_db(verbosity, autoclobber)\\n\\n        self.connection.close()\\n        self.connection.settings_dict[\\"NAME\\"] = test_database_name\\n\\n        # Confirm the feature set of the test database\\n        self.connection.features.confirm()\\n\\n        # Report syncdb messages at one level lower than that requested.\\n        # This ensures we don\'t get flooded with messages during testing\\n        # (unless you really ask to be flooded)\\n        call_command(\'syncdb\',\\n            verbosity=max(verbosity - 1, 0),\\n            interactive=False,\\n            database=self.connection.alias,\\n            load_initial_data=False)\\n\\n        # We need to then do a flush to ensure that any data installed by\\n        # custom SQL has been removed. The only test data should come from\\n        # test fixtures, or autogenerated from post_syncdb triggers.\\n        # This has the side effect of loading initial data (which was\\n        # intentionally skipped in the syncdb).\\n        call_command(\'flush\',\\n            verbosity=max(verbosity - 1, 0),\\n            interactive=False,\\n            database=self.connection.alias)\\n\\n        from django.core.cache import get_cache\\n        from django.core.cache.backends.db import BaseDatabaseCache\\n        for cache_alias in settings.CACHES:\\n            cache = get_cache(cache_alias)\\n            if isinstance(cache, BaseDatabaseCache):\\n                from django.db import router\\n                if router.allow_syncdb(self.connection.alias, cache.cache_model_class):\\n                    call_command(\'createcachetable\', cache._table, database=self.connection.alias)\\n\\n        # Get a cursor (even though we don\'t need one yet). This has\\n        # the side effect of initializing the test database.\\n        cursor = self.connection.cursor()\\n\\n        return test_database_name\\n\\n    def _get_test_db_name(self):\\n        \\"\\"\\"\\n        Internal implementation - returns the name of the test DB that will be\\n        created. Only useful when called from create_test_db() and\\n        _create_test_db() and when no external munging is done with the \'NAME\'\\n        or \'TEST_NAME\' settings.\\n        \\"\\"\\"\\n        if self.connection.settings_dict[\'TEST_NAME\']:\\n            return self.connection.settings_dict[\'TEST_NAME\']\\n        return TEST_DATABASE_PREFIX + self.connection.settings_dict[\'NAME\']\\n\\n    def _create_test_db(self, verbosity, autoclobber):\\n        \\"Internal implementation - creates the test db tables.\\"\\n        suffix = self.sql_table_creation_suffix()\\n\\n        test_database_name = self._get_test_db_name()\\n\\n        qn = self.connection.ops.quote_name\\n\\n        # Create the test database and connect to it. We need to autocommit\\n        # if the database supports it because PostgreSQL doesn\'t allow\\n        # CREATE/DROP DATABASE statements within transactions.\\n        cursor = self.connection.cursor()\\n        self.set_autocommit()\\n        try:\\n            cursor.execute(\\"CREATE DATABASE %s %s\\" % (qn(test_database_name), suffix))\\n        except Exception, e:\\n            sys.stderr.write(\\"Got an error creating the test database: %s\\\\n\\" % e)\\n            if not autoclobber:\\n                confirm = raw_input(\\"Type \'yes\' if you would like to try deleting the test database \'%s\', or \'no\' to cancel: \\" % test_database_name)\\n            if autoclobber or confirm == \'yes\':\\n                try:\\n                    if verbosity \\u003e= 1:\\n                        print \\"Destroying old test database \'%s\'...\\" % self.connection.alias\\n                    cursor.execute(\\"DROP DATABASE %s\\" % qn(test_database_name))\\n                    cursor.execute(\\"CREATE DATABASE %s %s\\" % (qn(test_database_name), suffix))\\n                except Exception, e:\\n                    sys.stderr.write(\\"Got an error recreating the test database: %s\\\\n\\" % e)\\n                    sys.exit(2)\\n            else:\\n                print \\"Tests cancelled.\\"\\n                sys.exit(1)\\n\\n        return test_database_name\\n\\n    def destroy_test_db(self, old_database_name, verbosity=1):\\n        \\"\\"\\"\\n        Destroy a test database, prompting the user for confirmation if the\\n        database already exists. Returns the name of the test database created.\\n        \\"\\"\\"\\n        self.connection.close()\\n        test_database_name = self.connection.settings_dict[\'NAME\']\\n        if verbosity \\u003e= 1:\\n            test_db_repr = \'\'\\n            if verbosity \\u003e= 2:\\n                test_db_repr = \\" (\'%s\')\\" % test_database_name\\n            print \\"Destroying test database for alias \'%s\'%s...\\" % (self.connection.alias, test_db_repr)\\n        self.connection.settings_dict[\'NAME\'] = old_database_name\\n\\n        self._destroy_test_db(test_database_name, verbosity)\\n\\n    def _destroy_test_db(self, test_database_name, verbosity):\\n        \\"Internal implementation - remove the test db tables.\\"\\n        # Remove the test database to clean up after\\n        # ourselves. Connect to the previous database (not the test database)\\n        # to do so, because it\'s not allowed to delete a database while being\\n        # connected to it.\\n        cursor = self.connection.cursor()\\n        self.set_autocommit()\\n        time.sleep(1) # To avoid \\"database is being accessed by other users\\" errors.\\n        cursor.execute(\\"DROP DATABASE %s\\" % self.connection.ops.quote_name(test_database_name))\\n        self.connection.close()\\n\\n    def set_autocommit(self):\\n        \\"Make sure a connection is in autocommit mode.\\"\\n        if hasattr(self.connection.connection, \\"autocommit\\"):\\n            if callable(self.connection.connection.autocommit):\\n                self.connection.connection.autocommit(True)\\n            else:\\n                self.connection.connection.autocommit = True\\n        elif hasattr(self.connection.connection, \\"set_isolation_level\\"):\\n            self.connection.connection.set_isolation_level(0)\\n\\n    def sql_table_creation_suffix(self):\\n        \\"SQL to append to the end of the test table creation statements\\"\\n        return \'\'\\n\\n    def test_db_signature(self):\\n        \\"\\"\\"\\n        Returns a tuple with elements of self.connection.settings_dict (a\\n        DATABASES setting value) that uniquely identify a database\\n        accordingly to the RDBMS particularities.\\n        \\"\\"\\"\\n        settings_dict = self.connection.settings_dict\\n        return (\\n            settings_dict[\'HOST\'],\\n            settings_dict[\'PORT\'],\\n            settings_dict[\'ENGINE\'],\\n            settings_dict[\'NAME\']\\n        )\\n"}\n'
line: b'{"repo_name":"moijes12/oh-mainline","ref":"refs/heads/master","path":"vendor/packages/sphinx/tests/test_intersphinx.py","content":"# -*- coding: utf-8 -*-\\n\\"\\"\\"\\n    test_intersphinx\\n    ~~~~~~~~~~~~~~~~\\n\\n    Test the intersphinx extension.\\n\\n    :copyright: Copyright 2007-2013 by the Sphinx team, see AUTHORS.\\n    :license: BSD, see LICENSE for details.\\n\\"\\"\\"\\n\\nimport zlib\\nimport posixpath\\ntry:\\n    from io import BytesIO\\nexcept ImportError:\\n    from cStringIO import StringIO as BytesIO\\n\\nfrom docutils import nodes\\n\\nfrom sphinx import addnodes\\nfrom sphinx.ext.intersphinx import read_inventory_v1, read_inventory_v2, \\\\\\n     load_mappings, missing_reference\\n\\nfrom util import with_app, with_tempdir, write_file\\n\\n\\ninventory_v1 = \'\'\'\\\\\\n# Sphinx inventory version 1\\n# Project: foo\\n# Version: 1.0\\nmodule mod foo.html\\nmodule.cls class foo.html\\n\'\'\'.encode(\'utf-8\')\\n\\ninventory_v2 = \'\'\'\\\\\\n# Sphinx inventory version 2\\n# Project: foo\\n# Version: 2.0\\n# The remainder of this file is compressed with zlib.\\n\'\'\'.encode(\'utf-8\') + zlib.compress(\'\'\'\\\\\\nmodule1 py:module 0 foo.html#module-module1 Long Module desc\\nmodule2 py:module 0 foo.html#module-$ -\\nmodule1.func py:function 1 sub/foo.html#$ -\\nCFunc c:function 2 cfunc.html#CFunc -\\na term std:term -1 glossary.html#term-a-term -\\n\'\'\'.encode(\'utf-8\'))\\n\\n\\ndef test_read_inventory_v1():\\n    f = BytesIO(inventory_v1)\\n    f.readline()\\n    invdata = read_inventory_v1(f, \'/util\', posixpath.join)\\n    assert invdata[\'py:module\'][\'module\'] == \\\\\\n           (\'foo\', \'1.0\', \'/util/foo.html#module-module\', \'-\')\\n    assert invdata[\'py:class\'][\'module.cls\'] == \\\\\\n           (\'foo\', \'1.0\', \'/util/foo.html#module.cls\', \'-\')\\n\\n\\ndef test_read_inventory_v2():\\n    f = BytesIO(inventory_v2)\\n    f.readline()\\n    invdata1 = read_inventory_v2(f, \'/util\', posixpath.join)\\n\\n    # try again with a small buffer size to test the chunking algorithm\\n    f = BytesIO(inventory_v2)\\n    f.readline()\\n    invdata2 = read_inventory_v2(f, \'/util\', posixpath.join, bufsize=5)\\n\\n    assert invdata1 == invdata2\\n\\n    assert len(invdata1[\'py:module\']) == 2\\n    assert invdata1[\'py:module\'][\'module1\'] == \\\\\\n           (\'foo\', \'2.0\', \'/util/foo.html#module-module1\', \'Long Module desc\')\\n    assert invdata1[\'py:module\'][\'module2\'] == \\\\\\n           (\'foo\', \'2.0\', \'/util/foo.html#module-module2\', \'-\')\\n    assert invdata1[\'py:function\'][\'module1.func\'][2] == \\\\\\n           \'/util/sub/foo.html#module1.func\'\\n    assert invdata1[\'c:function\'][\'CFunc\'][2] == \'/util/cfunc.html#CFunc\'\\n    assert invdata1[\'std:term\'][\'a term\'][2] == \\\\\\n           \'/util/glossary.html#term-a-term\'\\n\\n\\n@with_app(confoverrides={\'extensions\': \'sphinx.ext.intersphinx\'})\\n@with_tempdir\\ndef test_missing_reference(tempdir, app):\\n    inv_file = tempdir / \'inventory\'\\n    write_file(inv_file, inventory_v2)\\n    app.config.intersphinx_mapping = {\\n        \'http://docs.python.org/\': inv_file,\\n        \'py3k\': (\'http://docs.python.org/py3k/\', inv_file),\\n    }\\n    app.config.intersphinx_cache_limit = 0\\n\\n    # load the inventory and check if it\'s done correctly\\n    load_mappings(app)\\n    inv = app.env.intersphinx_inventory\\n\\n    assert inv[\'py:module\'][\'module2\'] == \\\\\\n           (\'foo\', \'2.0\', \'http://docs.python.org/foo.html#module-module2\', \'-\')\\n\\n    # create fake nodes and check referencing\\n\\n    def fake_node(domain, type, target, content, **attrs):\\n        contnode = nodes.emphasis(content, content)\\n        node = addnodes.pending_xref(\'\')\\n        node[\'reftarget\'] = target\\n        node[\'reftype\'] = type\\n        node[\'refdomain\'] = domain\\n        node.attributes.update(attrs)\\n        node += contnode\\n        return node, contnode\\n\\n    def reference_check(*args, **kwds):\\n        node, contnode = fake_node(*args, **kwds)\\n        return missing_reference(app, app.env, node, contnode)\\n\\n    # check resolution when a target is found\\n    rn = reference_check(\'py\', \'func\', \'module1.func\', \'foo\')\\n    assert isinstance(rn, nodes.reference)\\n    assert rn[\'refuri\'] == \'http://docs.python.org/sub/foo.html#module1.func\'\\n    assert rn[\'reftitle\'] == \'(in foo v2.0)\'\\n    assert rn[0].astext() == \'foo\'\\n\\n    # create unresolvable nodes and check None return value\\n    assert reference_check(\'py\', \'foo\', \'module1.func\', \'foo\') is None\\n    assert reference_check(\'py\', \'func\', \'foo\', \'foo\') is None\\n    assert reference_check(\'py\', \'func\', \'foo\', \'foo\') is None\\n\\n    # check handling of prefixes\\n\\n    # prefix given, target found: prefix is stripped\\n    rn = reference_check(\'py\', \'mod\', \'py3k:module2\', \'py3k:module2\')\\n    assert rn[0].astext() == \'module2\'\\n\\n    # prefix given, but not in title: nothing stripped\\n    rn = reference_check(\'py\', \'mod\', \'py3k:module2\', \'module2\')\\n    assert rn[0].astext() == \'module2\'\\n\\n    # prefix given, but explicit: nothing stripped\\n    rn = reference_check(\'py\', \'mod\', \'py3k:module2\', \'py3k:module2\',\\n                         refexplicit=True)\\n    assert rn[0].astext() == \'py3k:module2\'\\n\\n    # prefix given, target not found and nonexplicit title: prefix is stripped\\n    node, contnode = fake_node(\'py\', \'mod\', \'py3k:unknown\', \'py3k:unknown\',\\n                               refexplicit=False)\\n    rn = missing_reference(app, app.env, node, contnode)\\n    assert rn is None\\n    assert contnode[0].astext() == \'unknown\'\\n\\n    # prefix given, target not found and explicit title: nothing is changed\\n    node, contnode = fake_node(\'py\', \'mod\', \'py3k:unknown\', \'py3k:unknown\',\\n                               refexplicit=True)\\n    rn = missing_reference(app, app.env, node, contnode)\\n    assert rn is None\\n    assert contnode[0].astext() == \'py3k:unknown\'\\n\\n\\n@with_app(confoverrides={\'extensions\': \'sphinx.ext.intersphinx\'})\\n@with_tempdir\\ndef test_load_mappings_warnings(tempdir, app):\\n    \\"\\"\\"\\n    load_mappings issues a warning if new-style mapping\\n    identifiers are not alphanumeric\\n    \\"\\"\\"\\n    inv_file = tempdir / \'inventory\'\\n    write_file(inv_file, inventory_v2)\\n    app.config.intersphinx_mapping = {\\n        \'http://docs.python.org/\': inv_file,\\n        \'py3k\': (\'http://docs.python.org/py3k/\', inv_file),\\n        \'repoze.workflow\': (\'http://docs.repoze.org/workflow/\', inv_file),\\n        \'django-taggit\': (\'http://django-taggit.readthedocs.org/en/latest/\',\\n                          inv_file)\\n    }\\n\\n    app.config.intersphinx_cache_limit = 0\\n    # load the inventory and check if it\'s done correctly\\n    load_mappings(app)\\n    assert len(app._warning.content) == 2\\n"}\n'
line: b'{"repo_name":"xianjunzhengbackup/Cloud-Native-Python","ref":"refs/heads/master","path":"env/lib/python3.6/site-packages/pip/_vendor/progress/__init__.py","content":"# Copyright (c) 2012 Giorgos Verigakis \\u003cverigak@gmail.com\\u003e\\n#\\n# Permission to use, copy, modify, and distribute this software for any\\n# purpose with or without fee is hereby granted, provided that the above\\n# copyright notice and this permission notice appear in all copies.\\n#\\n# THE SOFTWARE IS PROVIDED \\"AS IS\\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\\n# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\\n# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\\n# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\\n# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\\n# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\\n# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\\n\\nfrom __future__ import division\\n\\nfrom collections import deque\\nfrom datetime import timedelta\\nfrom math import ceil\\nfrom sys import stderr\\nfrom time import time\\n\\n\\n__version__ = \'1.2\'\\n\\n\\nclass Infinite(object):\\n    file = stderr\\n    sma_window = 10\\n\\n    def __init__(self, *args, **kwargs):\\n        self.index = 0\\n        self.start_ts = time()\\n        self._ts = self.start_ts\\n        self._dt = deque(maxlen=self.sma_window)\\n        for key, val in kwargs.items():\\n            setattr(self, key, val)\\n\\n    def __getitem__(self, key):\\n        if key.startswith(\'_\'):\\n            return None\\n        return getattr(self, key, None)\\n\\n    @property\\n    def avg(self):\\n        return sum(self._dt) / len(self._dt) if self._dt else 0\\n\\n    @property\\n    def elapsed(self):\\n        return int(time() - self.start_ts)\\n\\n    @property\\n    def elapsed_td(self):\\n        return timedelta(seconds=self.elapsed)\\n\\n    def update(self):\\n        pass\\n\\n    def start(self):\\n        pass\\n\\n    def finish(self):\\n        pass\\n\\n    def next(self, n=1):\\n        if n \\u003e 0:\\n            now = time()\\n            dt = (now - self._ts) / n\\n            self._dt.append(dt)\\n            self._ts = now\\n\\n        self.index = self.index + n\\n        self.update()\\n\\n    def iter(self, it):\\n        for x in it:\\n            yield x\\n            self.next()\\n        self.finish()\\n\\n\\nclass Progress(Infinite):\\n    def __init__(self, *args, **kwargs):\\n        super(Progress, self).__init__(*args, **kwargs)\\n        self.max = kwargs.get(\'max\', 100)\\n\\n    @property\\n    def eta(self):\\n        return int(ceil(self.avg * self.remaining))\\n\\n    @property\\n    def eta_td(self):\\n        return timedelta(seconds=self.eta)\\n\\n    @property\\n    def percent(self):\\n        return self.progress * 100\\n\\n    @property\\n    def progress(self):\\n        return min(1, self.index / self.max)\\n\\n    @property\\n    def remaining(self):\\n        return max(self.max - self.index, 0)\\n\\n    def start(self):\\n        self.update()\\n\\n    def goto(self, index):\\n        incr = index - self.index\\n        self.next(incr)\\n\\n    def iter(self, it):\\n        try:\\n            self.max = len(it)\\n        except TypeError:\\n            pass\\n\\n        for x in it:\\n            yield x\\n            self.next()\\n        self.finish()\\n"}\n'
line: b'{"repo_name":"thierry1985/project-1022","ref":"refs/heads/master","path":"MISC/TFD-0.2.2/translate/pddl/parser.py","content":"__all__ = [\\"ParseError\\", \\"parse_nested_list\\"]\\n\\nclass ParseError(Exception):\\n  pass\\n\\n# Basic functions for parsing PDDL (Lisp) files.\\ndef parse_nested_list(input_file):\\n  tokens = tokenize(input_file)\\n  next_token = tokens.next()\\n  if next_token != \\"(\\":\\n    raise ParseError(\\"Expected \'(\', got %s.\\" % next_token)\\n  result = list(parse_list_aux(tokens))\\n  for tok in tokens:  # Check that generator is exhausted.\\n    raise ParseError(\\"Unexpected token: %s.\\" % tok)\\n  return result\\n  \\ndef tokenize(input):\\n  for line in input:\\n    line = line.split(\\";\\", 1)[0]  # Strip comments.\\n    line = line.replace(\\"(\\", \\" ( \\").replace(\\")\\", \\" ) \\").replace(\\"?\\", \\" ?\\")\\n    for token in line.split():\\n      yield token.lower()\\n\\ndef parse_list_aux(tokenstream):\\n  # Leading \\"(\\" has already been swallowed.\\n  while True:\\n    try:\\n      token = tokenstream.next()\\n    except StopIteration:\\n      raise ParseError()\\n    if token == \\")\\":\\n      return\\n    elif token == \\"(\\":\\n      yield list(parse_list_aux(tokenstream))\\n    else:\\n      yield token\\n\\n"}\n'
line: b'{"repo_name":"andrewburnheimer/ptpop","ref":"refs/heads/master","path":"ptpop/Console.py","content":"#!/usr/local/bin/python\\n\'\'\'\\nConsole Class\\n\'\'\'\\n\'\'\'\\nTo Do:\\n    -\\n\'\'\'\\n\\nfrom Listener import Listener\\nfrom _version import __version__\\nimport time\\n\\n# =============================================================================\\n# Console\\n# \\n# Inheriting from `object` (top-level class)\\n# =============================================================================\\nclass Console(object):\\n    def __init__(self, args=None):\\n        \'\'\'\\n        Console Initialization\\n        Input Attributes:\\n        -----------------\\n        self.args -\\u003e argparse.Namespace: object holding attributes set\\n                                         on command-line.\\n        \'\'\'\\n\\n        # Default Values\\n        delay = 3.0\\n        number = 1 # XXX should be = 0\\n        command = [ ]\\n        interface = \'eth0\'\\n        listen = False\\n        host = \'localhost\'\\n        if args:\\n            delay = float(args.delay) if args.delay else delay\\n            number = args.number if (args.number != None) else number\\n            command = args.command if args.command else command\\n            interface = args.interface if args.interface else interface\\n            listen = args.listen if args.listen else listen\\n            host = args.host if args.host else host\\n\\n        # Input Checks\\n        if command != [ ]:\\n            raise NotImplementedError(\'Issuing commands to hosts has \' +\\n                    \'not been implemented yet\')\\n\\n        # init ...\\n        if listen:\\n            self.listener = Listener(interface)\\n            key = \'\'\'\\nremote          Dly St Dom Pr1  Cl Acc   Var  Pr2       Uniq       SyncT  DlyT  AnnT\\n====================================================================================\'\'\'.strip()\\n\\n\\n            while number \\u003e 0:\\n                # Report output directly to console\\n                fmt=\'%a %b %d %Y %H:%M:%S\'\\n                t = time.time()\\n                time_str = time.strftime(fmt, time.localtime(t))\\n                time_msecs = int((t - int(t)) * 1000)\\n\\n                print time_str + \'.%03d \' % (time_msecs) + time.tzname[0]\\n                print key\\n\\n                # output data seen in since last iteration\\n                neighbor_stats = self.listener.ptp_neighbors\\n                for neighbor in neighbor_stats:\\n                    print self.listener.ptp_neighbors[neighbor]\\n\\n                print\\n                number -= 1\\n                if number \\u003c= 0:\\n                    exit(0)\\n                    # No need to wait after the last iteration\\n                time.sleep(delay)\\n\\n            # Enter into the interactive environment, exit when q is\\n            # issued\\n\\n        else:\\n            for supplied_command in command:\\n                command = supplied_command.lower()\\n\\n                if command == \'rv\' or command == \'readvar\':\\n                    None\\n                    # Assuming to be similar to NTPQ\\n            # root@raspberrypi:/home/puppet# ntpq -n -c rv -c peers\\n            #associd=0 status=0615 leap_none, sync_ntp, 1 event, clock_sync,\\n            #version=\\"ntpd 4.2.6p5@1.2349-o Mon Nov  2 04:29:47 UTC 2015 (1)\\",\\n            #processor=\\"armv6l\\", system=\\"Linux/4.1.17+\\", leap=00, stratum=3,\\n            #precision=-20, rootdelay=2.916, rootdisp=60.561,\\n            #refid=3.44.174.43,\\n            #reftime=da7f3666.54078831  Mon, Feb 29 2016 21:28:06.328,\\n            #clock=da7f38cd.57a72dc6  Mon, Feb 29 2016 21:38:21.342,\\n            #peer=7185, tc=8,\\n            #mintc=3, offset=9.208, frequency=-48.954, sys_jitter=0.000,\\n            #clk_jitter=16.919, clk_wander=4.216\\n                elif command == \'peers\':\\n                    None\\n                    # Assuming to be similar to NTPQ\\n            #     remote           refid      st t when poll reach   delay   offset  jitter\\n            #==============================================================================\\n            #*useclsifl158.tf 3.199.96.254     2 u   14 1024  377    1.582    0.186   0.919\\n                else:\\n                    raise NotImplementedError(\'Unknown command, \\\\\'\' +\\n                            command + \'\\\\\'\')\\n\\n# __main__.py is executed when the package is instantiated\\nimport argparse\\n\\ndef main():\\n    parser = argparse.ArgumentParser(prog=\'ptpop\', description=\'Gain \' +\\n        \'insight into the operations of IEEE 1588 Precision Time Protocol \' +\\n        \'domains on a network. Press the \\\\\'q\\\\\' key to quit.\')\\n\\n    command_choices=[\'readvar\', \'rv\', \'peers\']\\n    parser.add_argument(\'host\', type=str, nargs=\'?\', help=\'each of the \' +\\n                        \'commands will be sent to the PTP servers \' +\\n                        \'running on the host provided, localhost by \' +\\n                        \'default.\')\\n    parser.add_argument(\'-c\', \'--command\', type=str, action=\'append\',\\n                        help=\'a command to run on the provided host, \' +\\n                        \'i.e. \' + str(command_choices) + \', \\\\\'readvar\\\\\' \' +\\n                        \'by default. Multiple commands can be issued.\')\\n    parser.add_argument(\'-i\', \'--interface\', type=str,\\n                        help=\'interface to issue commands on or to \' +\\n                        \'observe on in listen mode.\')\\n    parser.add_argument(\'-l\', \'--listen\', action=\'store_true\',\\n                        help=\'don\\\\\'t contact any PTP servers, but \' +\\n                        \'report on any services currently observed \' +\\n                        \'on the network, instead.\')\\n    parser.add_argument(\'-d\', \'--delay\', metavar=\'SECS.TENTHS\', type=str,\\n                        help=\'Specifies the delay between screen \' +\\n                        \'updates when interactive. Can be changed while \' +\\n                        \'running using the \\\\\'d\\\\\' key. Negative \' +\\n                        \'numbers are not allowed. Setting this value \' +\\n                        \'to 0 is the same as issuing the \\\\\'-n 1\\\\\' \' +\\n                        \'option.\')\\n    parser.add_argument(\'-n\', \'--number\', metavar=\'COUNT\', type=int,\\n                        help=\'Specifies the maximum number of iterations \' +\\n                        \'in interactive mode before ending.\')\\n    parser.add_argument(\'-v\', \'--version\', action=\'version\',\\n                        version=\'%(prog)s \' + __version__)\\n\\n    args = parser.parse_args()\\n    try:\\n        c = Console(args)\\n\\n    except Exception as e:\\n        print type(e).__name__ + \\": \\" + str(e.message)\\n        exit(-1)\\n\\nif __name__ == \'__main__\':\\n    main()\\n"}\n'
line: b'{"repo_name":"achoy/cwapi","ref":"refs/heads/master","path":"backend/py-server/flask/lib/python3.6/site-packages/six.py","content":"\\"\\"\\"Utilities for writing code that runs on Python 2 and 3\\"\\"\\"\\n\\n# Copyright (c) 2010-2015 Benjamin Peterson\\n#\\n# Permission is hereby granted, free of charge, to any person obtaining a copy\\n# of this software and associated documentation files (the \\"Software\\"), to deal\\n# in the Software without restriction, including without limitation the rights\\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n# copies of the Software, and to permit persons to whom the Software is\\n# furnished to do so, subject to the following conditions:\\n#\\n# The above copyright notice and this permission notice shall be included in all\\n# copies or substantial portions of the Software.\\n#\\n# THE SOFTWARE IS PROVIDED \\"AS IS\\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n# SOFTWARE.\\n\\nfrom __future__ import absolute_import\\n\\nimport functools\\nimport itertools\\nimport operator\\nimport sys\\nimport types\\n\\n__author__ = \\"Benjamin Peterson \\u003cbenjamin@python.org\\u003e\\"\\n__version__ = \\"1.10.0\\"\\n\\n\\n# Useful for very coarse version differentiation.\\nPY2 = sys.version_info[0] == 2\\nPY3 = sys.version_info[0] == 3\\nPY34 = sys.version_info[0:2] \\u003e= (3, 4)\\n\\nif PY3:\\n    string_types = str,\\n    integer_types = int,\\n    class_types = type,\\n    text_type = str\\n    binary_type = bytes\\n\\n    MAXSIZE = sys.maxsize\\nelse:\\n    string_types = basestring,\\n    integer_types = (int, long)\\n    class_types = (type, types.ClassType)\\n    text_type = unicode\\n    binary_type = str\\n\\n    if sys.platform.startswith(\\"java\\"):\\n        # Jython always uses 32 bits.\\n        MAXSIZE = int((1 \\u003c\\u003c 31) - 1)\\n    else:\\n        # It\'s possible to have sizeof(long) != sizeof(Py_ssize_t).\\n        class X(object):\\n\\n            def __len__(self):\\n                return 1 \\u003c\\u003c 31\\n        try:\\n            len(X())\\n        except OverflowError:\\n            # 32-bit\\n            MAXSIZE = int((1 \\u003c\\u003c 31) - 1)\\n        else:\\n            # 64-bit\\n            MAXSIZE = int((1 \\u003c\\u003c 63) - 1)\\n        del X\\n\\n\\ndef _add_doc(func, doc):\\n    \\"\\"\\"Add documentation to a function.\\"\\"\\"\\n    func.__doc__ = doc\\n\\n\\ndef _import_module(name):\\n    \\"\\"\\"Import module, returning the module after the last dot.\\"\\"\\"\\n    __import__(name)\\n    return sys.modules[name]\\n\\n\\nclass _LazyDescr(object):\\n\\n    def __init__(self, name):\\n        self.name = name\\n\\n    def __get__(self, obj, tp):\\n        result = self._resolve()\\n        setattr(obj, self.name, result)  # Invokes __set__.\\n        try:\\n            # This is a bit ugly, but it avoids running this again by\\n            # removing this descriptor.\\n            delattr(obj.__class__, self.name)\\n        except AttributeError:\\n            pass\\n        return result\\n\\n\\nclass MovedModule(_LazyDescr):\\n\\n    def __init__(self, name, old, new=None):\\n        super(MovedModule, self).__init__(name)\\n        if PY3:\\n            if new is None:\\n                new = name\\n            self.mod = new\\n        else:\\n            self.mod = old\\n\\n    def _resolve(self):\\n        return _import_module(self.mod)\\n\\n    def __getattr__(self, attr):\\n        _module = self._resolve()\\n        value = getattr(_module, attr)\\n        setattr(self, attr, value)\\n        return value\\n\\n\\nclass _LazyModule(types.ModuleType):\\n\\n    def __init__(self, name):\\n        super(_LazyModule, self).__init__(name)\\n        self.__doc__ = self.__class__.__doc__\\n\\n    def __dir__(self):\\n        attrs = [\\"__doc__\\", \\"__name__\\"]\\n        attrs += [attr.name for attr in self._moved_attributes]\\n        return attrs\\n\\n    # Subclasses should override this\\n    _moved_attributes = []\\n\\n\\nclass MovedAttribute(_LazyDescr):\\n\\n    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):\\n        super(MovedAttribute, self).__init__(name)\\n        if PY3:\\n            if new_mod is None:\\n                new_mod = name\\n            self.mod = new_mod\\n            if new_attr is None:\\n                if old_attr is None:\\n                    new_attr = name\\n                else:\\n                    new_attr = old_attr\\n            self.attr = new_attr\\n        else:\\n            self.mod = old_mod\\n            if old_attr is None:\\n                old_attr = name\\n            self.attr = old_attr\\n\\n    def _resolve(self):\\n        module = _import_module(self.mod)\\n        return getattr(module, self.attr)\\n\\n\\nclass _SixMetaPathImporter(object):\\n\\n    \\"\\"\\"\\n    A meta path importer to import six.moves and its submodules.\\n\\n    This class implements a PEP302 finder and loader. It should be compatible\\n    with Python 2.5 and all existing versions of Python3\\n    \\"\\"\\"\\n\\n    def __init__(self, six_module_name):\\n        self.name = six_module_name\\n        self.known_modules = {}\\n\\n    def _add_module(self, mod, *fullnames):\\n        for fullname in fullnames:\\n            self.known_modules[self.name + \\".\\" + fullname] = mod\\n\\n    def _get_module(self, fullname):\\n        return self.known_modules[self.name + \\".\\" + fullname]\\n\\n    def find_module(self, fullname, path=None):\\n        if fullname in self.known_modules:\\n            return self\\n        return None\\n\\n    def __get_module(self, fullname):\\n        try:\\n            return self.known_modules[fullname]\\n        except KeyError:\\n            raise ImportError(\\"This loader does not know module \\" + fullname)\\n\\n    def load_module(self, fullname):\\n        try:\\n            # in case of a reload\\n            return sys.modules[fullname]\\n        except KeyError:\\n            pass\\n        mod = self.__get_module(fullname)\\n        if isinstance(mod, MovedModule):\\n            mod = mod._resolve()\\n        else:\\n            mod.__loader__ = self\\n        sys.modules[fullname] = mod\\n        return mod\\n\\n    def is_package(self, fullname):\\n        \\"\\"\\"\\n        Return true, if the named module is a package.\\n\\n        We need this method to get correct spec objects with\\n        Python 3.4 (see PEP451)\\n        \\"\\"\\"\\n        return hasattr(self.__get_module(fullname), \\"__path__\\")\\n\\n    def get_code(self, fullname):\\n        \\"\\"\\"Return None\\n\\n        Required, if is_package is implemented\\"\\"\\"\\n        self.__get_module(fullname)  # eventually raises ImportError\\n        return None\\n    get_source = get_code  # same as get_code\\n\\n_importer = _SixMetaPathImporter(__name__)\\n\\n\\nclass _MovedItems(_LazyModule):\\n\\n    \\"\\"\\"Lazy loading of moved objects\\"\\"\\"\\n    __path__ = []  # mark as package\\n\\n\\n_moved_attributes = [\\n    MovedAttribute(\\"cStringIO\\", \\"cStringIO\\", \\"io\\", \\"StringIO\\"),\\n    MovedAttribute(\\"filter\\", \\"itertools\\", \\"builtins\\", \\"ifilter\\", \\"filter\\"),\\n    MovedAttribute(\\"filterfalse\\", \\"itertools\\", \\"itertools\\", \\"ifilterfalse\\", \\"filterfalse\\"),\\n    MovedAttribute(\\"input\\", \\"__builtin__\\", \\"builtins\\", \\"raw_input\\", \\"input\\"),\\n    MovedAttribute(\\"intern\\", \\"__builtin__\\", \\"sys\\"),\\n    MovedAttribute(\\"map\\", \\"itertools\\", \\"builtins\\", \\"imap\\", \\"map\\"),\\n    MovedAttribute(\\"getcwd\\", \\"os\\", \\"os\\", \\"getcwdu\\", \\"getcwd\\"),\\n    MovedAttribute(\\"getcwdb\\", \\"os\\", \\"os\\", \\"getcwd\\", \\"getcwdb\\"),\\n    MovedAttribute(\\"range\\", \\"__builtin__\\", \\"builtins\\", \\"xrange\\", \\"range\\"),\\n    MovedAttribute(\\"reload_module\\", \\"__builtin__\\", \\"importlib\\" if PY34 else \\"imp\\", \\"reload\\"),\\n    MovedAttribute(\\"reduce\\", \\"__builtin__\\", \\"functools\\"),\\n    MovedAttribute(\\"shlex_quote\\", \\"pipes\\", \\"shlex\\", \\"quote\\"),\\n    MovedAttribute(\\"StringIO\\", \\"StringIO\\", \\"io\\"),\\n    MovedAttribute(\\"UserDict\\", \\"UserDict\\", \\"collections\\"),\\n    MovedAttribute(\\"UserList\\", \\"UserList\\", \\"collections\\"),\\n    MovedAttribute(\\"UserString\\", \\"UserString\\", \\"collections\\"),\\n    MovedAttribute(\\"xrange\\", \\"__builtin__\\", \\"builtins\\", \\"xrange\\", \\"range\\"),\\n    MovedAttribute(\\"zip\\", \\"itertools\\", \\"builtins\\", \\"izip\\", \\"zip\\"),\\n    MovedAttribute(\\"zip_longest\\", \\"itertools\\", \\"itertools\\", \\"izip_longest\\", \\"zip_longest\\"),\\n    MovedModule(\\"builtins\\", \\"__builtin__\\"),\\n    MovedModule(\\"configparser\\", \\"ConfigParser\\"),\\n    MovedModule(\\"copyreg\\", \\"copy_reg\\"),\\n    MovedModule(\\"dbm_gnu\\", \\"gdbm\\", \\"dbm.gnu\\"),\\n    MovedModule(\\"_dummy_thread\\", \\"dummy_thread\\", \\"_dummy_thread\\"),\\n    MovedModule(\\"http_cookiejar\\", \\"cookielib\\", \\"http.cookiejar\\"),\\n    MovedModule(\\"http_cookies\\", \\"Cookie\\", \\"http.cookies\\"),\\n    MovedModule(\\"html_entities\\", \\"htmlentitydefs\\", \\"html.entities\\"),\\n    MovedModule(\\"html_parser\\", \\"HTMLParser\\", \\"html.parser\\"),\\n    MovedModule(\\"http_client\\", \\"httplib\\", \\"http.client\\"),\\n    MovedModule(\\"email_mime_multipart\\", \\"email.MIMEMultipart\\", \\"email.mime.multipart\\"),\\n    MovedModule(\\"email_mime_nonmultipart\\", \\"email.MIMENonMultipart\\", \\"email.mime.nonmultipart\\"),\\n    MovedModule(\\"email_mime_text\\", \\"email.MIMEText\\", \\"email.mime.text\\"),\\n    MovedModule(\\"email_mime_base\\", \\"email.MIMEBase\\", \\"email.mime.base\\"),\\n    MovedModule(\\"BaseHTTPServer\\", \\"BaseHTTPServer\\", \\"http.server\\"),\\n    MovedModule(\\"CGIHTTPServer\\", \\"CGIHTTPServer\\", \\"http.server\\"),\\n    MovedModule(\\"SimpleHTTPServer\\", \\"SimpleHTTPServer\\", \\"http.server\\"),\\n    MovedModule(\\"cPickle\\", \\"cPickle\\", \\"pickle\\"),\\n    MovedModule(\\"queue\\", \\"Queue\\"),\\n    MovedModule(\\"reprlib\\", \\"repr\\"),\\n    MovedModule(\\"socketserver\\", \\"SocketServer\\"),\\n    MovedModule(\\"_thread\\", \\"thread\\", \\"_thread\\"),\\n    MovedModule(\\"tkinter\\", \\"Tkinter\\"),\\n    MovedModule(\\"tkinter_dialog\\", \\"Dialog\\", \\"tkinter.dialog\\"),\\n    MovedModule(\\"tkinter_filedialog\\", \\"FileDialog\\", \\"tkinter.filedialog\\"),\\n    MovedModule(\\"tkinter_scrolledtext\\", \\"ScrolledText\\", \\"tkinter.scrolledtext\\"),\\n    MovedModule(\\"tkinter_simpledialog\\", \\"SimpleDialog\\", \\"tkinter.simpledialog\\"),\\n    MovedModule(\\"tkinter_tix\\", \\"Tix\\", \\"tkinter.tix\\"),\\n    MovedModule(\\"tkinter_ttk\\", \\"ttk\\", \\"tkinter.ttk\\"),\\n    MovedModule(\\"tkinter_constants\\", \\"Tkconstants\\", \\"tkinter.constants\\"),\\n    MovedModule(\\"tkinter_dnd\\", \\"Tkdnd\\", \\"tkinter.dnd\\"),\\n    MovedModule(\\"tkinter_colorchooser\\", \\"tkColorChooser\\",\\n                \\"tkinter.colorchooser\\"),\\n    MovedModule(\\"tkinter_commondialog\\", \\"tkCommonDialog\\",\\n                \\"tkinter.commondialog\\"),\\n    MovedModule(\\"tkinter_tkfiledialog\\", \\"tkFileDialog\\", \\"tkinter.filedialog\\"),\\n    MovedModule(\\"tkinter_font\\", \\"tkFont\\", \\"tkinter.font\\"),\\n    MovedModule(\\"tkinter_messagebox\\", \\"tkMessageBox\\", \\"tkinter.messagebox\\"),\\n    MovedModule(\\"tkinter_tksimpledialog\\", \\"tkSimpleDialog\\",\\n                \\"tkinter.simpledialog\\"),\\n    MovedModule(\\"urllib_parse\\", __name__ + \\".moves.urllib_parse\\", \\"urllib.parse\\"),\\n    MovedModule(\\"urllib_error\\", __name__ + \\".moves.urllib_error\\", \\"urllib.error\\"),\\n    MovedModule(\\"urllib\\", __name__ + \\".moves.urllib\\", __name__ + \\".moves.urllib\\"),\\n    MovedModule(\\"urllib_robotparser\\", \\"robotparser\\", \\"urllib.robotparser\\"),\\n    MovedModule(\\"xmlrpc_client\\", \\"xmlrpclib\\", \\"xmlrpc.client\\"),\\n    MovedModule(\\"xmlrpc_server\\", \\"SimpleXMLRPCServer\\", \\"xmlrpc.server\\"),\\n]\\n# Add windows specific modules.\\nif sys.platform == \\"win32\\":\\n    _moved_attributes += [\\n        MovedModule(\\"winreg\\", \\"_winreg\\"),\\n    ]\\n\\nfor attr in _moved_attributes:\\n    setattr(_MovedItems, attr.name, attr)\\n    if isinstance(attr, MovedModule):\\n        _importer._add_module(attr, \\"moves.\\" + attr.name)\\ndel attr\\n\\n_MovedItems._moved_attributes = _moved_attributes\\n\\nmoves = _MovedItems(__name__ + \\".moves\\")\\n_importer._add_module(moves, \\"moves\\")\\n\\n\\nclass Module_six_moves_urllib_parse(_LazyModule):\\n\\n    \\"\\"\\"Lazy loading of moved objects in six.moves.urllib_parse\\"\\"\\"\\n\\n\\n_urllib_parse_moved_attributes = [\\n    MovedAttribute(\\"ParseResult\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"SplitResult\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"parse_qs\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"parse_qsl\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"urldefrag\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"urljoin\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"urlparse\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"urlsplit\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"urlunparse\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"urlunsplit\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"quote\\", \\"urllib\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"quote_plus\\", \\"urllib\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"unquote\\", \\"urllib\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"unquote_plus\\", \\"urllib\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"urlencode\\", \\"urllib\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"splitquery\\", \\"urllib\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"splittag\\", \\"urllib\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"splituser\\", \\"urllib\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"uses_fragment\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"uses_netloc\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"uses_params\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"uses_query\\", \\"urlparse\\", \\"urllib.parse\\"),\\n    MovedAttribute(\\"uses_relative\\", \\"urlparse\\", \\"urllib.parse\\"),\\n]\\nfor attr in _urllib_parse_moved_attributes:\\n    setattr(Module_six_moves_urllib_parse, attr.name, attr)\\ndel attr\\n\\nModule_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes\\n\\n_importer._add_module(Module_six_moves_urllib_parse(__name__ + \\".moves.urllib_parse\\"),\\n                      \\"moves.urllib_parse\\", \\"moves.urllib.parse\\")\\n\\n\\nclass Module_six_moves_urllib_error(_LazyModule):\\n\\n    \\"\\"\\"Lazy loading of moved objects in six.moves.urllib_error\\"\\"\\"\\n\\n\\n_urllib_error_moved_attributes = [\\n    MovedAttribute(\\"URLError\\", \\"urllib2\\", \\"urllib.error\\"),\\n    MovedAttribute(\\"HTTPError\\", \\"urllib2\\", \\"urllib.error\\"),\\n    MovedAttribute(\\"ContentTooShortError\\", \\"urllib\\", \\"urllib.error\\"),\\n]\\nfor attr in _urllib_error_moved_attributes:\\n    setattr(Module_six_moves_urllib_error, attr.name, attr)\\ndel attr\\n\\nModule_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes\\n\\n_importer._add_module(Module_six_moves_urllib_error(__name__ + \\".moves.urllib.error\\"),\\n                      \\"moves.urllib_error\\", \\"moves.urllib.error\\")\\n\\n\\nclass Module_six_moves_urllib_request(_LazyModule):\\n\\n    \\"\\"\\"Lazy loading of moved objects in six.moves.urllib_request\\"\\"\\"\\n\\n\\n_urllib_request_moved_attributes = [\\n    MovedAttribute(\\"urlopen\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"install_opener\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"build_opener\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"pathname2url\\", \\"urllib\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"url2pathname\\", \\"urllib\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"getproxies\\", \\"urllib\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"Request\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"OpenerDirector\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPDefaultErrorHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPRedirectHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPCookieProcessor\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"ProxyHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"BaseHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPPasswordMgr\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPPasswordMgrWithDefaultRealm\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"AbstractBasicAuthHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPBasicAuthHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"ProxyBasicAuthHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"AbstractDigestAuthHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPDigestAuthHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"ProxyDigestAuthHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPSHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"FileHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"FTPHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"CacheFTPHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"UnknownHandler\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"HTTPErrorProcessor\\", \\"urllib2\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"urlretrieve\\", \\"urllib\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"urlcleanup\\", \\"urllib\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"URLopener\\", \\"urllib\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"FancyURLopener\\", \\"urllib\\", \\"urllib.request\\"),\\n    MovedAttribute(\\"proxy_bypass\\", \\"urllib\\", \\"urllib.request\\"),\\n]\\nfor attr in _urllib_request_moved_attributes:\\n    setattr(Module_six_moves_urllib_request, attr.name, attr)\\ndel attr\\n\\nModule_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes\\n\\n_importer._add_module(Module_six_moves_urllib_request(__name__ + \\".moves.urllib.request\\"),\\n                      \\"moves.urllib_request\\", \\"moves.urllib.request\\")\\n\\n\\nclass Module_six_moves_urllib_response(_LazyModule):\\n\\n    \\"\\"\\"Lazy loading of moved objects in six.moves.urllib_response\\"\\"\\"\\n\\n\\n_urllib_response_moved_attributes = [\\n    MovedAttribute(\\"addbase\\", \\"urllib\\", \\"urllib.response\\"),\\n    MovedAttribute(\\"addclosehook\\", \\"urllib\\", \\"urllib.response\\"),\\n    MovedAttribute(\\"addinfo\\", \\"urllib\\", \\"urllib.response\\"),\\n    MovedAttribute(\\"addinfourl\\", \\"urllib\\", \\"urllib.response\\"),\\n]\\nfor attr in _urllib_response_moved_attributes:\\n    setattr(Module_six_moves_urllib_response, attr.name, attr)\\ndel attr\\n\\nModule_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes\\n\\n_importer._add_module(Module_six_moves_urllib_response(__name__ + \\".moves.urllib.response\\"),\\n                      \\"moves.urllib_response\\", \\"moves.urllib.response\\")\\n\\n\\nclass Module_six_moves_urllib_robotparser(_LazyModule):\\n\\n    \\"\\"\\"Lazy loading of moved objects in six.moves.urllib_robotparser\\"\\"\\"\\n\\n\\n_urllib_robotparser_moved_attributes = [\\n    MovedAttribute(\\"RobotFileParser\\", \\"robotparser\\", \\"urllib.robotparser\\"),\\n]\\nfor attr in _urllib_robotparser_moved_attributes:\\n    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\\ndel attr\\n\\nModule_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes\\n\\n_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + \\".moves.urllib.robotparser\\"),\\n                      \\"moves.urllib_robotparser\\", \\"moves.urllib.robotparser\\")\\n\\n\\nclass Module_six_moves_urllib(types.ModuleType):\\n\\n    \\"\\"\\"Create a six.moves.urllib namespace that resembles the Python 3 namespace\\"\\"\\"\\n    __path__ = []  # mark as package\\n    parse = _importer._get_module(\\"moves.urllib_parse\\")\\n    error = _importer._get_module(\\"moves.urllib_error\\")\\n    request = _importer._get_module(\\"moves.urllib_request\\")\\n    response = _importer._get_module(\\"moves.urllib_response\\")\\n    robotparser = _importer._get_module(\\"moves.urllib_robotparser\\")\\n\\n    def __dir__(self):\\n        return [\'parse\', \'error\', \'request\', \'response\', \'robotparser\']\\n\\n_importer._add_module(Module_six_moves_urllib(__name__ + \\".moves.urllib\\"),\\n                      \\"moves.urllib\\")\\n\\n\\ndef add_move(move):\\n    \\"\\"\\"Add an item to six.moves.\\"\\"\\"\\n    setattr(_MovedItems, move.name, move)\\n\\n\\ndef remove_move(name):\\n    \\"\\"\\"Remove item from six.moves.\\"\\"\\"\\n    try:\\n        delattr(_MovedItems, name)\\n    except AttributeError:\\n        try:\\n            del moves.__dict__[name]\\n        except KeyError:\\n            raise AttributeError(\\"no such move, %r\\" % (name,))\\n\\n\\nif PY3:\\n    _meth_func = \\"__func__\\"\\n    _meth_self = \\"__self__\\"\\n\\n    _func_closure = \\"__closure__\\"\\n    _func_code = \\"__code__\\"\\n    _func_defaults = \\"__defaults__\\"\\n    _func_globals = \\"__globals__\\"\\nelse:\\n    _meth_func = \\"im_func\\"\\n    _meth_self = \\"im_self\\"\\n\\n    _func_closure = \\"func_closure\\"\\n    _func_code = \\"func_code\\"\\n    _func_defaults = \\"func_defaults\\"\\n    _func_globals = \\"func_globals\\"\\n\\n\\ntry:\\n    advance_iterator = next\\nexcept NameError:\\n    def advance_iterator(it):\\n        return it.next()\\nnext = advance_iterator\\n\\n\\ntry:\\n    callable = callable\\nexcept NameError:\\n    def callable(obj):\\n        return any(\\"__call__\\" in klass.__dict__ for klass in type(obj).__mro__)\\n\\n\\nif PY3:\\n    def get_unbound_function(unbound):\\n        return unbound\\n\\n    create_bound_method = types.MethodType\\n\\n    def create_unbound_method(func, cls):\\n        return func\\n\\n    Iterator = object\\nelse:\\n    def get_unbound_function(unbound):\\n        return unbound.im_func\\n\\n    def create_bound_method(func, obj):\\n        return types.MethodType(func, obj, obj.__class__)\\n\\n    def create_unbound_method(func, cls):\\n        return types.MethodType(func, None, cls)\\n\\n    class Iterator(object):\\n\\n        def next(self):\\n            return type(self).__next__(self)\\n\\n    callable = callable\\n_add_doc(get_unbound_function,\\n         \\"\\"\\"Get the function out of a possibly unbound function\\"\\"\\")\\n\\n\\nget_method_function = operator.attrgetter(_meth_func)\\nget_method_self = operator.attrgetter(_meth_self)\\nget_function_closure = operator.attrgetter(_func_closure)\\nget_function_code = operator.attrgetter(_func_code)\\nget_function_defaults = operator.attrgetter(_func_defaults)\\nget_function_globals = operator.attrgetter(_func_globals)\\n\\n\\nif PY3:\\n    def iterkeys(d, **kw):\\n        return iter(d.keys(**kw))\\n\\n    def itervalues(d, **kw):\\n        return iter(d.values(**kw))\\n\\n    def iteritems(d, **kw):\\n        return iter(d.items(**kw))\\n\\n    def iterlists(d, **kw):\\n        return iter(d.lists(**kw))\\n\\n    viewkeys = operator.methodcaller(\\"keys\\")\\n\\n    viewvalues = operator.methodcaller(\\"values\\")\\n\\n    viewitems = operator.methodcaller(\\"items\\")\\nelse:\\n    def iterkeys(d, **kw):\\n        return d.iterkeys(**kw)\\n\\n    def itervalues(d, **kw):\\n        return d.itervalues(**kw)\\n\\n    def iteritems(d, **kw):\\n        return d.iteritems(**kw)\\n\\n    def iterlists(d, **kw):\\n        return d.iterlists(**kw)\\n\\n    viewkeys = operator.methodcaller(\\"viewkeys\\")\\n\\n    viewvalues = operator.methodcaller(\\"viewvalues\\")\\n\\n    viewitems = operator.methodcaller(\\"viewitems\\")\\n\\n_add_doc(iterkeys, \\"Return an iterator over the keys of a dictionary.\\")\\n_add_doc(itervalues, \\"Return an iterator over the values of a dictionary.\\")\\n_add_doc(iteritems,\\n         \\"Return an iterator over the (key, value) pairs of a dictionary.\\")\\n_add_doc(iterlists,\\n         \\"Return an iterator over the (key, [values]) pairs of a dictionary.\\")\\n\\n\\nif PY3:\\n    def b(s):\\n        return s.encode(\\"latin-1\\")\\n\\n    def u(s):\\n        return s\\n    unichr = chr\\n    import struct\\n    int2byte = struct.Struct(\\"\\u003eB\\").pack\\n    del struct\\n    byte2int = operator.itemgetter(0)\\n    indexbytes = operator.getitem\\n    iterbytes = iter\\n    import io\\n    StringIO = io.StringIO\\n    BytesIO = io.BytesIO\\n    _assertCountEqual = \\"assertCountEqual\\"\\n    if sys.version_info[1] \\u003c= 1:\\n        _assertRaisesRegex = \\"assertRaisesRegexp\\"\\n        _assertRegex = \\"assertRegexpMatches\\"\\n    else:\\n        _assertRaisesRegex = \\"assertRaisesRegex\\"\\n        _assertRegex = \\"assertRegex\\"\\nelse:\\n    def b(s):\\n        return s\\n    # Workaround for standalone backslash\\n\\n    def u(s):\\n        return unicode(s.replace(r\'\\\\\\\\\', r\'\\\\\\\\\\\\\\\\\'), \\"unicode_escape\\")\\n    unichr = unichr\\n    int2byte = chr\\n\\n    def byte2int(bs):\\n        return ord(bs[0])\\n\\n    def indexbytes(buf, i):\\n        return ord(buf[i])\\n    iterbytes = functools.partial(itertools.imap, ord)\\n    import StringIO\\n    StringIO = BytesIO = StringIO.StringIO\\n    _assertCountEqual = \\"assertItemsEqual\\"\\n    _assertRaisesRegex = \\"assertRaisesRegexp\\"\\n    _assertRegex = \\"assertRegexpMatches\\"\\n_add_doc(b, \\"\\"\\"Byte literal\\"\\"\\")\\n_add_doc(u, \\"\\"\\"Text literal\\"\\"\\")\\n\\n\\ndef assertCountEqual(self, *args, **kwargs):\\n    return getattr(self, _assertCountEqual)(*args, **kwargs)\\n\\n\\ndef assertRaisesRegex(self, *args, **kwargs):\\n    return getattr(self, _assertRaisesRegex)(*args, **kwargs)\\n\\n\\ndef assertRegex(self, *args, **kwargs):\\n    return getattr(self, _assertRegex)(*args, **kwargs)\\n\\n\\nif PY3:\\n    exec_ = getattr(moves.builtins, \\"exec\\")\\n\\n    def reraise(tp, value, tb=None):\\n        if value is None:\\n            value = tp()\\n        if value.__traceback__ is not tb:\\n            raise value.with_traceback(tb)\\n        raise value\\n\\nelse:\\n    def exec_(_code_, _globs_=None, _locs_=None):\\n        \\"\\"\\"Execute code in a namespace.\\"\\"\\"\\n        if _globs_ is None:\\n            frame = sys._getframe(1)\\n            _globs_ = frame.f_globals\\n            if _locs_ is None:\\n                _locs_ = frame.f_locals\\n            del frame\\n        elif _locs_ is None:\\n            _locs_ = _globs_\\n        exec(\\"\\"\\"exec _code_ in _globs_, _locs_\\"\\"\\")\\n\\n    exec_(\\"\\"\\"def reraise(tp, value, tb=None):\\n    raise tp, value, tb\\n\\"\\"\\")\\n\\n\\nif sys.version_info[:2] == (3, 2):\\n    exec_(\\"\\"\\"def raise_from(value, from_value):\\n    if from_value is None:\\n        raise value\\n    raise value from from_value\\n\\"\\"\\")\\nelif sys.version_info[:2] \\u003e (3, 2):\\n    exec_(\\"\\"\\"def raise_from(value, from_value):\\n    raise value from from_value\\n\\"\\"\\")\\nelse:\\n    def raise_from(value, from_value):\\n        raise value\\n\\n\\nprint_ = getattr(moves.builtins, \\"print\\", None)\\nif print_ is None:\\n    def print_(*args, **kwargs):\\n        \\"\\"\\"The new-style print function for Python 2.4 and 2.5.\\"\\"\\"\\n        fp = kwargs.pop(\\"file\\", sys.stdout)\\n        if fp is None:\\n            return\\n\\n        def write(data):\\n            if not isinstance(data, basestring):\\n                data = str(data)\\n            # If the file has an encoding, encode unicode with it.\\n            if (isinstance(fp, file) and\\n                    isinstance(data, unicode) and\\n                    fp.encoding is not None):\\n                errors = getattr(fp, \\"errors\\", None)\\n                if errors is None:\\n                    errors = \\"strict\\"\\n                data = data.encode(fp.encoding, errors)\\n            fp.write(data)\\n        want_unicode = False\\n        sep = kwargs.pop(\\"sep\\", None)\\n        if sep is not None:\\n            if isinstance(sep, unicode):\\n                want_unicode = True\\n            elif not isinstance(sep, str):\\n                raise TypeError(\\"sep must be None or a string\\")\\n        end = kwargs.pop(\\"end\\", None)\\n        if end is not None:\\n            if isinstance(end, unicode):\\n                want_unicode = True\\n            elif not isinstance(end, str):\\n                raise TypeError(\\"end must be None or a string\\")\\n        if kwargs:\\n            raise TypeError(\\"invalid keyword arguments to print()\\")\\n        if not want_unicode:\\n            for arg in args:\\n                if isinstance(arg, unicode):\\n                    want_unicode = True\\n                    break\\n        if want_unicode:\\n            newline = unicode(\\"\\\\n\\")\\n            space = unicode(\\" \\")\\n        else:\\n            newline = \\"\\\\n\\"\\n            space = \\" \\"\\n        if sep is None:\\n            sep = space\\n        if end is None:\\n            end = newline\\n        for i, arg in enumerate(args):\\n            if i:\\n                write(sep)\\n            write(arg)\\n        write(end)\\nif sys.version_info[:2] \\u003c (3, 3):\\n    _print = print_\\n\\n    def print_(*args, **kwargs):\\n        fp = kwargs.get(\\"file\\", sys.stdout)\\n        flush = kwargs.pop(\\"flush\\", False)\\n        _print(*args, **kwargs)\\n        if flush and fp is not None:\\n            fp.flush()\\n\\n_add_doc(reraise, \\"\\"\\"Reraise an exception.\\"\\"\\")\\n\\nif sys.version_info[0:2] \\u003c (3, 4):\\n    def wraps(wrapped, assigned=functools.WRAPPER_ASSIGNMENTS,\\n              updated=functools.WRAPPER_UPDATES):\\n        def wrapper(f):\\n            f = functools.wraps(wrapped, assigned, updated)(f)\\n            f.__wrapped__ = wrapped\\n            return f\\n        return wrapper\\nelse:\\n    wraps = functools.wraps\\n\\n\\ndef with_metaclass(meta, *bases):\\n    \\"\\"\\"Create a base class with a metaclass.\\"\\"\\"\\n    # This requires a bit of explanation: the basic idea is to make a dummy\\n    # metaclass for one level of class instantiation that replaces itself with\\n    # the actual metaclass.\\n    class metaclass(meta):\\n\\n        def __new__(cls, name, this_bases, d):\\n            return meta(name, bases, d)\\n    return type.__new__(metaclass, \'temporary_class\', (), {})\\n\\n\\ndef add_metaclass(metaclass):\\n    \\"\\"\\"Class decorator for creating a class with a metaclass.\\"\\"\\"\\n    def wrapper(cls):\\n        orig_vars = cls.__dict__.copy()\\n        slots = orig_vars.get(\'__slots__\')\\n        if slots is not None:\\n            if isinstance(slots, str):\\n                slots = [slots]\\n            for slots_var in slots:\\n                orig_vars.pop(slots_var)\\n        orig_vars.pop(\'__dict__\', None)\\n        orig_vars.pop(\'__weakref__\', None)\\n        return metaclass(cls.__name__, cls.__bases__, orig_vars)\\n    return wrapper\\n\\n\\ndef python_2_unicode_compatible(klass):\\n    \\"\\"\\"\\n    A decorator that defines __unicode__ and __str__ methods under Python 2.\\n    Under Python 3 it does nothing.\\n\\n    To support Python 2 and 3 with a single code base, define a __str__ method\\n    returning text and apply this decorator to the class.\\n    \\"\\"\\"\\n    if PY2:\\n        if \'__str__\' not in klass.__dict__:\\n            raise ValueError(\\"@python_2_unicode_compatible cannot be applied \\"\\n                             \\"to %s because it doesn\'t define __str__().\\" %\\n                             klass.__name__)\\n        klass.__unicode__ = klass.__str__\\n        klass.__str__ = lambda self: self.__unicode__().encode(\'utf-8\')\\n    return klass\\n\\n\\n# Complete the moves implementation.\\n# This code is at the end of this module to speed up module loading.\\n# Turn this module into a package.\\n__path__ = []  # required for PEP 302 and PEP 451\\n__package__ = __name__  # see PEP 366 @ReservedAssignment\\nif globals().get(\\"__spec__\\") is not None:\\n    __spec__.submodule_search_locations = []  # PEP 451 @UndefinedVariable\\n# Remove other six meta path importers, since they cause problems. This can\\n# happen if six is removed from sys.modules and then reloaded. (Setuptools does\\n# this for some reason.)\\nif sys.meta_path:\\n    for i, importer in enumerate(sys.meta_path):\\n        # Here\'s some real nastiness: Another \\"instance\\" of the six module might\\n        # be floating around. Therefore, we can\'t use isinstance() to check for\\n        # the six meta path importer, since the other six instance will have\\n        # inserted an importer with different class.\\n        if (type(importer).__name__ == \\"_SixMetaPathImporter\\" and\\n                importer.name == __name__):\\n            del sys.meta_path[i]\\n            break\\n    del i, importer\\n# Finally, add the importer to the meta path import hook.\\nsys.meta_path.append(_importer)\\n"}\n'
line: b'{"repo_name":"sbstp/streamlink","ref":"refs/heads/master","path":"src/streamlink/plugins/alieztv.py","content":"import re\\n\\nfrom os.path import splitext\\n\\nfrom streamlink.compat import urlparse, unquote\\nfrom streamlink.plugin import Plugin\\nfrom streamlink.plugin.api import http, validate\\nfrom streamlink.stream import HTTPStream, RTMPStream\\n\\n_url_re = re.compile(\\"\\"\\"\\n    http(s)?://(\\\\w+\\\\.)?aliez.tv\\n    (?:\\n        /live/[^/]+\\n    )?\\n    (?:\\n        /video/\\\\d+/[^/]+\\n    )?\\n\\"\\"\\", re.VERBOSE)\\n_file_re = re.compile(\\"\\\\\\"?file\\\\\\"?:\\\\s+[\'\\\\\\"]([^\'\\\\\\"]+)[\'\\\\\\"]\\")\\n_swf_url_re = re.compile(\\"swfobject.embedSWF\\\\(\\\\\\"([^\\\\\\"]+)\\\\\\",\\")\\n\\n_schema = validate.Schema(\\n    validate.union({\\n        \\"urls\\": validate.all(\\n            validate.transform(_file_re.findall),\\n            validate.map(unquote),\\n            [validate.url()]\\n        ),\\n        \\"swf\\": validate.all(\\n            validate.transform(_swf_url_re.search),\\n            validate.any(\\n                None,\\n                validate.all(\\n                    validate.get(1),\\n                    validate.url(\\n                        scheme=\\"http\\",\\n                        path=validate.endswith(\\"swf\\")\\n                    )\\n                )\\n            )\\n        )\\n    })\\n)\\n\\n\\nclass Aliez(Plugin):\\n    @classmethod\\n    def can_handle_url(self, url):\\n        return _url_re.match(url)\\n\\n    def _get_streams(self):\\n        res = http.get(self.url, schema=_schema)\\n        streams = {}\\n        for url in res[\\"urls\\"]:\\n            parsed = urlparse(url)\\n            if parsed.scheme.startswith(\\"rtmp\\"):\\n                params = {\\n                    \\"rtmp\\": url,\\n                    \\"pageUrl\\": self.url,\\n                    \\"live\\": True\\n                }\\n                if res[\\"swf\\"]:\\n                    params[\\"swfVfy\\"] = res[\\"swf\\"]\\n\\n                stream = RTMPStream(self.session, params)\\n                streams[\\"live\\"] = stream\\n            elif parsed.scheme.startswith(\\"http\\"):\\n                name = splitext(parsed.path)[1][1:]\\n                stream = HTTPStream(self.session, url)\\n                streams[name] = stream\\n\\n        return streams\\n\\n__plugin__ = Aliez\\n"}\n'
line: b'{"repo_name":"astronaut1712/taiga-back","ref":"refs/heads/master","path":"taiga/projects/wiki/models.py","content":"# Copyright (C) 2014 Andrey Antukh \\u003cniwi@niwi.be\\u003e\\n# Copyright (C) 2014 Jes\xc3\xbas Espino \\u003cjespinog@gmail.com\\u003e\\n# Copyright (C) 2014 David Barrag\xc3\xa1n \\u003cbameda@dbarragan.com\\u003e\\n# This program is free software: you can redistribute it and/or modify\\n# it under the terms of the GNU Affero General Public License as\\n# published by the Free Software Foundation, either version 3 of the\\n# License, or (at your option) any later version.\\n#\\n# This program is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU Affero General Public License for more details.\\n#\\n# You should have received a copy of the GNU Affero General Public License\\n# along with this program.  If not, see \\u003chttp://www.gnu.org/licenses/\\u003e.\\n\\nfrom django.db import models\\nfrom django.contrib.contenttypes import generic\\nfrom django.conf import settings\\nfrom django.utils.translation import ugettext_lazy as _\\nfrom django.utils import timezone\\nfrom taiga.projects.notifications.mixins import WatchedModelMixin\\nfrom taiga.projects.occ import OCCModelMixin\\n\\n\\nclass WikiPage(OCCModelMixin, WatchedModelMixin, models.Model):\\n    project = models.ForeignKey(\\"projects.Project\\", null=False, blank=False,\\n                                related_name=\\"wiki_pages\\", verbose_name=_(\\"project\\"))\\n    slug = models.SlugField(max_length=500, db_index=True, null=False, blank=False,\\n                            verbose_name=_(\\"slug\\"))\\n    content = models.TextField(null=False, blank=True,\\n                               verbose_name=_(\\"content\\"))\\n    owner = models.ForeignKey(settings.AUTH_USER_MODEL, null=True, blank=True,\\n                              related_name=\\"owned_wiki_pages\\", verbose_name=_(\\"owner\\"))\\n    last_modifier = models.ForeignKey(settings.AUTH_USER_MODEL, null=True, blank=True,\\n                              related_name=\\"last_modified_wiki_pages\\", verbose_name=_(\\"last modifier\\"))\\n    created_date = models.DateTimeField(null=False, blank=False,\\n                                        verbose_name=_(\\"created date\\"),\\n                                        default=timezone.now)\\n    modified_date = models.DateTimeField(null=False, blank=False,\\n                                         verbose_name=_(\\"modified date\\"))\\n    attachments = generic.GenericRelation(\\"attachments.Attachment\\")\\n    _importing = None\\n\\n    class Meta:\\n        verbose_name = \\"wiki page\\"\\n        verbose_name_plural = \\"wiki pages\\"\\n        ordering = [\\"project\\", \\"slug\\"]\\n        unique_together = (\\"project\\", \\"slug\\",)\\n        permissions = (\\n            (\\"view_wikipage\\", \\"Can view wiki page\\"),\\n        )\\n\\n    def __str__(self):\\n        return \\"project {0} - {1}\\".format(self.project_id, self.slug)\\n\\n    def save(self, *args, **kwargs):\\n        if not self._importing or not self.modified_date:\\n            self.modified_date = timezone.now()\\n\\n        return super().save(*args, **kwargs)\\n\\n\\nclass WikiLink(models.Model):\\n    project = models.ForeignKey(\\"projects.Project\\", null=False, blank=False,\\n                                related_name=\\"wiki_links\\", verbose_name=_(\\"project\\"))\\n    title = models.CharField(max_length=500, null=False, blank=False)\\n    href = models.SlugField(max_length=500, db_index=True, null=False, blank=False,\\n                            verbose_name=_(\\"href\\"))\\n    order = models.PositiveSmallIntegerField(default=1, null=False, blank=False,\\n                                             verbose_name=_(\\"order\\"))\\n\\n    class Meta:\\n        verbose_name = \\"wiki link\\"\\n        verbose_name_plural = \\"wiki links\\"\\n        ordering = [\\"project\\", \\"order\\"]\\n        unique_together = (\\"project\\", \\"href\\")\\n\\n    def __str__(self):\\n        return self.title\\n"}\n'
line: b'{"repo_name":"helenst/django","ref":"refs/heads/master","path":"django/contrib/gis/db/backends/mysql/introspection.py","content":"from MySQLdb.constants import FIELD_TYPE\\n\\nfrom django.contrib.gis.gdal import OGRGeomType\\nfrom django.db.backends.mysql.introspection import DatabaseIntrospection\\n\\n\\nclass MySQLIntrospection(DatabaseIntrospection):\\n    # Updating the data_types_reverse dictionary with the appropriate\\n    # type for Geometry fields.\\n    data_types_reverse = DatabaseIntrospection.data_types_reverse.copy()\\n    data_types_reverse[FIELD_TYPE.GEOMETRY] = \'GeometryField\'\\n\\n    def get_geometry_type(self, table_name, geo_col):\\n        cursor = self.connection.cursor()\\n        try:\\n            # In order to get the specific geometry type of the field,\\n            # we introspect on the table definition using `DESCRIBE`.\\n            cursor.execute(\'DESCRIBE %s\' %\\n                           self.connection.ops.quote_name(table_name))\\n            # Increment over description info until we get to the geometry\\n            # column.\\n            for column, typ, null, key, default, extra in cursor.fetchall():\\n                if column == geo_col:\\n                    # Using OGRGeomType to convert from OGC name to Django field.\\n                    # MySQL does not support 3D or SRIDs, so the field params\\n                    # are empty.\\n                    field_type = OGRGeomType(typ).django\\n                    field_params = {}\\n                    break\\n        finally:\\n            cursor.close()\\n\\n        return field_type, field_params\\n\\n    def supports_spatial_index(self, cursor, table_name):\\n        # Supported with MyISAM, or InnoDB on MySQL 5.7.5+\\n        storage_engine = self.get_storage_engine(cursor, table_name)\\n        return (\\n            (storage_engine == \'InnoDB\' and self.connection.mysql_version \\u003e= (5, 7, 5)) or\\n            storage_engine == \'MyISAM\'\\n        )\\n"}\n'
line: b'{"repo_name":"PaulWay/insights-core","ref":"refs/heads/master","path":"insights/parsers/tests/test_foreman_log.py","content":"from insights.tests import context_wrap\\nfrom insights.parsers.foreman_log import SatelliteLog, ProductionLog\\nfrom insights.parsers.foreman_log import CandlepinLog, ProxyLog\\n\\n\\nPRODUCTION_LOG = \\"\\"\\"\\n2015-11-13 03:30:07 [I] Completed 200 OK in 1783ms (Views: 0.2ms | ActiveRecord: 172.9ms)\\n2015-11-13 03:30:07 [I] Processing by Katello::Api::V2::RepositoriesController#sync_complete as JSON\\n2015-11-13 03:30:07 [I]   Parameters: {\\"call_report\\"=\\u003e\\"[FILTERED]\\", \\"event_type\\"=\\u003e\\"repo.sync.finish\\", \\"payload\\"=\\u003e{\\"importer_id\\"=\\u003e\\"yum_importer\\", \\"exception\\"=\\u003enil, \\"repo_id\\"=\\u003e\\"1-Gulfstream_Aerospace_Corp_-Red_Hat_Enterprise_Linux_Server-Red_Hat_Satellite_Tools_6_1_for_RHEL_6_Server_RPMs_i386\\", \\"traceback\\"=\\u003enil, \\"started\\"=\\u003e\\"2015-11-13T08:30:00Z\\", \\"_ns\\"=\\u003e\\"repo_sync_results\\", \\"completed\\"=\\u003e\\"2015-11-13T08:30:06Z\\", \\"importer_type_id\\"=\\u003e\\"yum_importer\\", \\"error_message\\"=\\u003enil, \\"summary\\"=\\u003e{\\"content\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"comps\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"distribution\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"errata\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"metadata\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}}, \\"added_count\\"=\\u003e0, \\"result\\"=\\u003e\\"success\\", \\"updated_count\\"=\\u003e3, \\"details\\"=\\u003e{\\"content\\"=\\u003e{\\"size_total\\"=\\u003e0, \\"items_left\\"=\\u003e0, \\"items_total\\"=\\u003e0, \\"state\\"=\\u003e\\"FINISHED\\", \\"size_left\\"=\\u003e0, \\"details\\"=\\u003e{\\"rpm_total\\"=\\u003e0, \\"rpm_done\\"=\\u003e0, \\"drpm_total\\"=\\u003e0, \\"drpm_done\\"=\\u003e0}, \\"error_details\\"=\\u003e[]}, \\"comps\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"distribution\\"=\\u003e{\\"items_total\\"=\\u003e0, \\"state\\"=\\u003e\\"FINISHED\\", \\"error_details\\"=\\u003e[], \\"items_left\\"=\\u003e0}, \\"errata\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"metadata\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}}, \\"id\\"=\\u003e\\"56459f8ef301a213bbfd87bb\\", \\"removed_count\\"=\\u003e0}, \\"token\\"=\\u003e\\"oQumn3XsKrdRkijuvpCNhKF2PDWZt6az\\", \\"api_version\\"=\\u003e\\"v2\\", \\"repository\\"=\\u003e{}}\\n2015-11-13 03:30:07 [I] Sync_complete called for Red Hat Satellite Tools 6.1 for RHEL 6 Server RPMs i386, running after_sync.\\n2015-11-13 03:30:09 [I] Completed 200 OK in 1995ms (Views: 0.2ms | ActiveRecord: 81.5ms)\\n2015-11-13 03:30:10 [I] Processing by Katello::Api::V2::RepositoriesController#sync_complete as JSON\\n2015-11-13 03:30:10 [I]   Parameters: {\\"call_report\\"=\\u003e\\"[FILTERED]\\", \\"event_type\\"=\\u003e\\"repo.sync.finish\\", \\"payload\\"=\\u003e{\\"importer_id\\"=\\u003e\\"yum_importer\\", \\"exception\\"=\\u003enil, \\"repo_id\\"=\\u003e\\"1-Gulfstream_Aerospace_Corp_-Red_Hat_Enterprise_Linux_Server-Red_Hat_Satellite_Tools_6_1_for_RHEL_5_Server_RPMs_i386\\", \\"traceback\\"=\\u003enil, \\"started\\"=\\u003e\\"2015-11-13T08:30:05Z\\", \\"_ns\\"=\\u003e\\"repo_sync_results\\", \\"completed\\"=\\u003e\\"2015-11-13T08:30:10Z\\", \\"importer_type_id\\"=\\u003e\\"yum_importer\\", \\"error_message\\"=\\u003enil, \\"summary\\"=\\u003e{\\"content\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"comps\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"distribution\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"errata\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"metadata\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}}, \\"added_count\\"=\\u003e0, \\"result\\"=\\u003e\\"success\\", \\"updated_count\\"=\\u003e3, \\"details\\"=\\u003e{\\"content\\"=\\u003e{\\"size_total\\"=\\u003e0, \\"items_left\\"=\\u003e0, \\"items_total\\"=\\u003e0, \\"state\\"=\\u003e\\"FINISHED\\", \\"size_left\\"=\\u003e0, \\"details\\"=\\u003e{\\"rpm_total\\"=\\u003e0, \\"rpm_done\\"=\\u003e0, \\"drpm_total\\"=\\u003e0, \\"drpm_done\\"=\\u003e0}, \\"error_details\\"=\\u003e[]}, \\"comps\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"distribution\\"=\\u003e{\\"items_total\\"=\\u003e0, \\"state\\"=\\u003e\\"FINISHED\\", \\"error_details\\"=\\u003e[], \\"items_left\\"=\\u003e0}, \\"errata\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}, \\"metadata\\"=\\u003e{\\"state\\"=\\u003e\\"FINISHED\\"}}, \\"id\\"=\\u003e\\"56459f92f301a2137cd6b802\\", \\"removed_count\\"=\\u003e0}, \\"token\\"=\\u003e\\"oQumn3XsKrdRkijuvpCNhKF2PDWZt6az\\", \\"api_version\\"=\\u003e\\"v2\\", \\"repository\\"=\\u003e{}}\\n2015-11-13 03:30:10 [I] Sync_complete called for Red Hat Satellite Tools 6.1 for RHEL 5 Server RPMs i386, running after_sync.\\n2015-11-13 03:30:11 [I] Connecting to database specified by database.yml\\n2015-11-13 03:30:11 [I] Connecting to database specified by database.yml\\n2015-11-13 03:30:11 [I] Completed 200 OK in 818ms (Views: 0.2ms | ActiveRecord: 77.2ms)\\n2015-11-13 03:30:17 [I] Connecting to database specified by database.yml\\n2015-11-13 03:30:26 [I] Sync_complete called for RHN Tools for Red Hat Enterprise Linux 5 Server RPMs x86_64 5Server, running after_sync.\\n2015-11-13 03:50:46 [I] Completed 200 OK in 2583ms (Views: 2.7ms | ActiveRecord: 0.3ms)\\n2015-11-13 06:58:25 [I]   Parameters: {\\"id\\"=\\u003e\\"cfd7275b-8cce-4323-8d1f-55ef85eca883\\"}\\n2015-11-13 06:58:25 [I] Completed 200 OK in 249ms (Views: 3.1ms | ActiveRecord: 0.3ms)\\n2015-11-13 06:59:26 [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#consumer_show as JSON\\n2015-11-13 06:59:26 [I]   Parameters: {\\"id\\"=\\u003e\\"cfd7275b-8cce-4323-8d1f-55ef85eca883\\"}\\n2015-11-13 06:59:26 [I] Completed 200 OK in 84ms (Views: 3.1ms | ActiveRecord: 0.3ms)\\n2015-11-13 07:00:12 [I] Connecting to database specified by database.yml\\n2015-11-13 07:00:12 [I] Connecting to database specified by database.yml\\n2015-11-13 07:00:12 [I] Connecting to database specified by database.yml\\n2015-11-13 07:00:18 [W] Creating scope :completer_scope. Overwriting existing method Organization.completer_scope.\\n2015-11-13 07:00:18 [W] Creating scope :completer_scope. Overwriting existing method Organization.completer_scope.\\n2015-11-13 07:00:18 [W] Creating scope :completer_scope. Overwriting existing method Location.completer_scope.\\n2015-11-13 07:00:18 [W] Creating scope :completer_scope. Overwriting existing method Location.completer_scope.\\n2015-11-13 07:00:18 [W] Creating scope :completer_scope. Overwriting existing method Organization.completer_scope.\\n2015-11-13 07:09:22 [I]   Parameters: {\\"facts\\"=\\u003e\\"[FILTERED]\\", \\"name\\"=\\u003e\\"infrhnpl002.gac.gulfaero.com\\", \\"certname\\"=\\u003e\\"infrhnpl002.gac.gulfaero.com\\", \\"apiv\\"=\\u003e\\"v2\\", :host=\\u003e{\\"name\\"=\\u003e\\"infrhnpl002.gac.gulfaero.com\\", \\"certname\\"=\\u003e\\"infrhnpl002.gac.gulfaero.com\\"}}\\n2015-11-13 07:09:22 [I] Import facts for \'infrhnpl002.gac.gulfaero.com\' completed. Added: 0, Updated: 6, Deleted 0 facts\\n2015-11-13 07:09:22 [I] Completed 201 Created in 251ms (Views: 179.3ms | ActiveRecord: 0.0ms)\\n2015-11-13 07:09:22 [I] Processing by HostsController#externalNodes as YML\\n2015-11-13 07:09:22 [I]   Parameters: {\\"name\\"=\\u003e\\"infrhnpl002.gac.gulfaero.com\\"}\\n2015-11-13 07:09:22 [I]   Rendered text template (0.0ms)\\n2015-11-13 07:09:22 [I] Completed 200 OK in 48ms (Views: 0.5ms | ActiveRecord: 6.6ms)\\n2015-11-13 07:09:22 [I] Processing by Api::V2::ReportsController#create as JSON\\n2015-11-13 07:09:22 [I]   Parameters: {\\"report\\"=\\u003e\\"[FILTERED]\\", \\"apiv\\"=\\u003e\\"v2\\"}\\n2015-11-13 07:09:22 [I]   Rendered text template (0.0ms)\\n2015-11-13 07:09:22 [I] processing report for infrhnpl002.gac.gulfaero.com\\n2015-11-13 07:09:22 [I] Imported report for infrhnpl002.gac.gulfaero.com in 0.02 seconds\\n2015-11-13 07:09:22 [I] Completed 201 Created in 28ms (Views: 1.2ms | ActiveRecord: 0.0ms)\\n2015-11-13 07:30:17 [W] Creating scope :completer_scope. Overwriting existing method Organization.completer_scope.\\n2015-11-13 07:30:18 [W] Creating scope :completer_scope. Overwriting existing method Location.completer_scope.\\n2015-11-13 07:30:18 [W] Creating scope :completer_scope. Overwriting existing method Location.completer_scope.\\n2015-11-13 07:30:18 [W] Creating scope :completer_scope. Overwriting existing method Location.completer_scope.\\n2015-11-13 07:30:25 [I] Client connected.\\n2015-11-13 07:30:25 [I] Connected to server.\\n2015-11-13 07:30:25 [I] Client connected.\\n2015-11-13 07:30:25 [I] Connected to server.\\n2015-11-13 07:30:25 [I] Client connected.\\n2015-11-13 07:30:25 [I] Connected to server.\\n2015-11-13 07:30:30 [I] init config for SecureHeaders::Configuration\\n2015-11-13 07:30:30 [I] init config for SecureHeaders::Configuration\\n2015-11-13 07:30:30 [I] init config for SecureHeaders::Configuration\\n2015-11-13 07:30:32 [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#consumer_show as JSON\\n2015-11-13 07:30:32 [I]   Parameters: {\\"id\\"=\\u003e\\"cfd7275b-8cce-4323-8d1f-55ef85eca883\\"}\\n2015-11-13 07:30:32 [I] Completed 200 OK in 110ms (Views: 2.7ms | ActiveRecord: 0.3ms)\\n2015-11-13 07:30:33 [I] 2015-11-13 07:30:33 -0500: Expired 48 Reports\\n2015-11-13 07:30:33 [I] Client disconnected.\\n2015-11-13 09:41:58 [I] Completed 200 OK in 93ms (Views: 2.9ms | ActiveRecord: 0.3ms)\\n2015-11-13 09:42:58 [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#consumer_show as JSON\\n2015-11-13 09:42:58 [I]   Parameters: {\\"id\\"=\\u003e\\"cfd7275b-8cce-4323-8d1f-55ef85eca883\\"}\\n2015-11-13 09:42:58 [I] Completed 200 OK in 80ms (Views: 3.6ms | ActiveRecord: 0.3ms)\\n2015-11-13 09:43:58 [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#consumer_show as JSON\\n2015-11-13 09:43:58 [I]   Parameters: {\\"id\\"=\\u003e\\"cfd7275b-8cce-4323-8d1f-55ef85eca883\\"}\\n2015-11-13 09:43:59 [I] Completed 200 OK in 80ms (Views: 2.9ms | ActiveRecord: 0.3ms)\\n\\"\\"\\".strip()\\n\\n\\nSATELLITE_OUT = \\"\\"\\"\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config::Pulp_client/Foreman_config_entry[pulp_client_cert]/require: requires Class[Certs::Pulp_client]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config::Pulp_client/Foreman_config_entry[pulp_client_cert]/require: requires Exec[foreman-rake-db:seed]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config::Pulp_client/Foreman_config_entry[pulp_client_key]/require: requires Class[Certs::Pulp_client]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config::Pulp_client/Foreman_config_entry[pulp_client_key]/require: requires Exec[foreman-rake-db:seed]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config/File[/etc/foreman/plugins/katello.yaml]/before: requires Class[Foreman::Database]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config/File[/etc/foreman/plugins/katello.yaml]/before: requires Exec[foreman-rake-db:migrate]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config/File[/etc/foreman/plugins/katello.yaml]/notify: subscribes to Service[foreman-tasks]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config/File[/etc/foreman/plugins/katello.yaml]/notify: subscribes to Class[Foreman::Service]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Katello::Config/Foreman::Config::Passenger::Fragment[katello]/require: requires Class[Foreman::Config::Passenger]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/notify: subscribes to Class[Certs::Candlepin]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Cert[kam1opapp999.connex.bclc.com-qpid-broker]/notify: subscribes to Pubkey[/etc/pki/katello/certs/kam1opapp999.connex.bclc.com-qpid-broker.crt]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Pubkey[/etc/pki/katello/certs/kam1opapp999.connex.bclc.com-qpid-broker.crt]/notify: subscribes to Privkey[/etc/pki/katello/private/kam1opapp999.connex.bclc.com-qpid-broker.key]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Privkey[/etc/pki/katello/private/kam1opapp999.connex.bclc.com-qpid-broker.key]/notify: subscribes to File[/etc/pki/katello/private/kam1opapp999.connex.bclc.com-qpid-broker.key]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/File[/etc/pki/katello/private/kam1opapp999.connex.bclc.com-qpid-broker.key]/notify: subscribes to File[/etc/pki/katello/nssdb]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/File[/etc/pki/katello/nssdb]/notify: subscribes to Exec[generate-nss-password]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[generate-nss-password]/before: requires File[/etc/pki/katello/nssdb/nss_db_password-file]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/File[/etc/pki/katello/nssdb/nss_db_password-file]/notify: subscribes to Exec[create-nss-db]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[create-nss-db]/before: requires Exec[delete ca]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[create-nss-db]/before: requires Exec[delete broker]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[create-nss-db]/before: requires Exec[delete amqp-client]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[create-nss-db]/notify: subscribes to Certs::Ssltools::Certutil[ca]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Certs::Ssltools::Certutil[ca]/notify: subscribes to File[/etc/pki/katello/nssdb/cert8.db]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Certs::Ssltools::Certutil[ca]/notify: subscribes to File[/etc/pki/katello/nssdb/key3.db]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Certs::Ssltools::Certutil[ca]/notify: subscribes to File[/etc/pki/katello/nssdb/secmod.db]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/File[/etc/pki/katello/nssdb/cert8.db]/notify: subscribes to Certs::Ssltools::Certutil[broker]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/File[/etc/pki/katello/nssdb/key3.db]/notify: subscribes to Certs::Ssltools::Certutil[broker]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/File[/etc/pki/katello/nssdb/secmod.db]/notify: subscribes to Certs::Ssltools::Certutil[broker]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Certs::Ssltools::Certutil[broker]/notify: subscribes to Exec[generate-pfx-for-nss-db]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[generate-pfx-for-nss-db]/notify: subscribes to Exec[add-private-key-to-nss-db]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Qpid/Exec[add-private-key-to-nss-db]/notify: subscribes to Service[qpidd]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/notify: subscribes to Class[Candlepin]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Cert[java-client]/notify: subscribes to Pubkey[/etc/pki/katello/certs/java-client.crt]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/File[/etc/pki/katello/keystore_password-file]/notify: subscribes to Exec[candlepin-generate-ssl-keystore]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Exec[candlepin-generate-ssl-keystore]/notify: subscribes to File[/usr/share/tomcat/conf/keystore]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/File[/usr/share/tomcat/conf/keystore]/notify: subscribes to Service[tomcat]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Pubkey[/etc/pki/katello/certs/java-client.crt]/notify: subscribes to Privkey[/etc/pki/katello/private/java-client.key]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Privkey[/etc/pki/katello/private/java-client.key]/notify: subscribes to Certs::Ssltools::Certutil[amqp-client]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Certs::Ssltools::Certutil[amqp-client]/subscribe: subscribes to Exec[create-nss-db]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Certs::Ssltools::Certutil[amqp-client]/notify: subscribes to Service[qpidd]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Certs::Ssltools::Certutil[amqp-client]/notify: subscribes to File[/etc/candlepin/certs/amqp]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/File[/etc/candlepin/certs/amqp]/notify: subscribes to Exec[create candlepin qpid exchange]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Exec[create candlepin qpid exchange]/require: requires Service[qpidd]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Exec[create candlepin qpid exchange]/notify: subscribes to Exec[import CA into Candlepin truststore]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Exec[import CA into Candlepin truststore]/notify: subscribes to Exec[import client certificate into Candlepin keystore]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/Exec[import client certificate into Candlepin keystore]/notify: subscribes to File[/etc/candlepin/certs/amqp/candlepin.jks]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Certs::Candlepin/File[/etc/candlepin/certs/amqp/candlepin.jks]/notify: subscribes to Service[tomcat]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Candlepin/notify: subscribes to Class[Qpid]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Candlepin::Install/notify: subscribes to Class[Candlepin::Config]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Candlepin::Config/notify: subscribes to Class[Candlepin::Database]\\n[DEBUG 2016-08-11 13:09:49 main]  /Stage[main]/Candlepin::Database/notify: subscribes to Class[Candlepin::Service]\\n\\"\\"\\".strip()\\n\\nCANDLEPIN_LOG = \\"\\"\\"\\n2016-09-09 13:45:52,650 [req=bd5a4284-d280-4fc5-a3d5-fc976b7aa5cc, org=] INFO org.candlepin.common.filter.LoggingFilter - Request: verb=GET, uri=/candlepin/consumers/f7677b4b-c470-4626-86a4-2fdf2546af4b\\n2016-09-09 13:45:52,784 [req=bd5a4284-d280-4fc5-a3d5-fc976b7aa5cc, org=ING_Luxembourg_SA] INFO  org.candlepin.common.filter.LoggingFilter - Response: status=200, content-type=\\"application/json\\", time=134\\n2016-09-09 13:45:52,947 [req=909ca4c5-f24e-4212-8f23-cc754d06ac57, org=] INFO org.candlepin.common.filter.LoggingFilter - Request: verb=GET, uri=/candlepin/consumers/f7677b4b-c470-4626-86a4-2fdf2546af4b/content_overrides\\n2016-09-09 13:45:52,976 [req=909ca4c5-f24e-4212-8f23-cc754d06ac57, org=] INFO org.candlepin.common.filter.LoggingFilter - Response: status=200, content-type=\\"application/json\\", time=29\\n2016-09-09 13:45:53,072 [req=49becd26-5dfe-4d2f-8667-470519230d88, org=] INFO org.candlepin.common.filter.LoggingFilter - Request: verb=GET, uri=/candlepin/consumers/f7677b4b-c470-4626-86a4-2fdf2546af4b/release\\n2016-09-09 13:45:53,115 [req=49becd26-5dfe-4d2f-8667-470519230d88, org=ING_Luxembourg_SA] INFO  org.candlepin.common.filter.LoggingFilter - Response: status=200, content-type=\\"application/json\\", time=43\\n\\"\\"\\".strip()\\n\\nPROXY_LOG = \\"\\"\\"\\n127.0.0.1 - - [31/May/2016:09:42:28 -0400] \\"GET /puppet/environments/KT_Encore_Library_RHEL_6_5/classes HTTP/1.1\\" 200 76785 6.1205\\n127.0.0.1 - - [31/May/2016:09:42:38 -0400] \\"GET /puppet/environments/KT_Encore_Library_RHEL_7_6/classes HTTP/1.1\\" 200 76785 4.4754\\n127.0.0.1 - - [31/May/2016:09:42:49 -0400] \\"GET /puppet/environments/KT_Encore_Library_RHEL6_8/classes HTTP/1.1\\" 200 76785 4.5776\\n127.0.0.1 - - [31/May/2016:09:57:34 -0400] \\"GET /tftp/serverName HTTP/1.1\\" 200 38 0.0014\\nE, [2016-05-31T09:57:34.884636 #4494] ERROR -- : Record 172.16.100.0/172.16.100.17 not found ]\\n\\"\\"\\".strip()\\n\\n\\ndef test_production_log():\\n    fm_log = ProductionLog(context_wrap(PRODUCTION_LOG))\\n    assert 2 == len(fm_log.get(\\"Rendered text template\\"))\\n    assert \\"Expired 48 Reports\\" in fm_log\\n    assert fm_log.get(\\"Completed 200 OK in 93\\")[0] == \\\\\\n        \\"2015-11-13 09:41:58 [I] Completed 200 OK in 93ms (Views: 2.9ms | ActiveRecord: 0.3ms)\\"\\n\\n\\ndef test_proxy_log():\\n    px_log = ProxyLog(context_wrap(PROXY_LOG))\\n    assert \\"ERROR -- \\" in px_log\\n    assert len(px_log.get(\\"KT_Encore_Library_RHEL\\")) == 3\\n\\n\\ndef test_candlepin_log():\\n    cp_log = CandlepinLog(context_wrap(CANDLEPIN_LOG))\\n    assert \\"req=49becd26-5dfe-4d2f-8667-470519230d88\\" in cp_log\\n    assert len(cp_log.get(\\"req=bd5a4284-d280-4fc5-a3d5-fc976b7aa5cc\\")) == 2\\n\\n\\ndef test_satellite_log():\\n    sat_log = SatelliteLog(context_wrap(SATELLITE_OUT))\\n    assert \\"subscribes to Class[Qpid]\\" in sat_log\\n    assert len(sat_log.get(\\"notify: subscribes to Class[\\")) == 7\\n"}\n'
line: b'{"repo_name":"Thraxis/pymedusa","ref":"refs/heads/master","path":"lib/html5lib/treewalkers/etree.py","content":"from __future__ import absolute_import, division, unicode_literals\\n\\ntry:\\n    from collections import OrderedDict\\nexcept ImportError:\\n    try:\\n        from ordereddict import OrderedDict\\n    except ImportError:\\n        OrderedDict = dict\\n\\nimport re\\n\\nfrom six import string_types\\n\\nfrom . import _base\\nfrom ..utils import moduleFactoryFactory\\n\\ntag_regexp = re.compile(\\"{([^}]*)}(.*)\\")\\n\\n\\ndef getETreeBuilder(ElementTreeImplementation):\\n    ElementTree = ElementTreeImplementation\\n    ElementTreeCommentType = ElementTree.Comment(\\"asd\\").tag\\n\\n    class TreeWalker(_base.NonRecursiveTreeWalker):\\n        \\"\\"\\"Given the particular ElementTree representation, this implementation,\\n        to avoid using recursion, returns \\"nodes\\" as tuples with the following\\n        content:\\n\\n        1. The current element\\n\\n        2. The index of the element relative to its parent\\n\\n        3. A stack of ancestor elements\\n\\n        4. A flag \\"text\\", \\"tail\\" or None to indicate if the current node is a\\n           text node; either the text or tail of the current element (1)\\n        \\"\\"\\"\\n        def getNodeDetails(self, node):\\n            if isinstance(node, tuple):  # It might be the root Element\\n                elt, key, parents, flag = node\\n                if flag in (\\"text\\", \\"tail\\"):\\n                    return _base.TEXT, getattr(elt, flag)\\n                else:\\n                    node = elt\\n\\n            if not(hasattr(node, \\"tag\\")):\\n                node = node.getroot()\\n\\n            if node.tag in (\\"DOCUMENT_ROOT\\", \\"DOCUMENT_FRAGMENT\\"):\\n                return (_base.DOCUMENT,)\\n\\n            elif node.tag == \\"\\u003c!DOCTYPE\\u003e\\":\\n                return (_base.DOCTYPE, node.text,\\n                        node.get(\\"publicId\\"), node.get(\\"systemId\\"))\\n\\n            elif node.tag == ElementTreeCommentType:\\n                return _base.COMMENT, node.text\\n\\n            else:\\n                assert isinstance(node.tag, string_types), type(node.tag)\\n                # This is assumed to be an ordinary element\\n                match = tag_regexp.match(node.tag)\\n                if match:\\n                    namespace, tag = match.groups()\\n                else:\\n                    namespace = None\\n                    tag = node.tag\\n                attrs = OrderedDict()\\n                for name, value in list(node.attrib.items()):\\n                    match = tag_regexp.match(name)\\n                    if match:\\n                        attrs[(match.group(1), match.group(2))] = value\\n                    else:\\n                        attrs[(None, name)] = value\\n                return (_base.ELEMENT, namespace, tag,\\n                        attrs, len(node) or node.text)\\n\\n        def getFirstChild(self, node):\\n            if isinstance(node, tuple):\\n                element, key, parents, flag = node\\n            else:\\n                element, key, parents, flag = node, None, [], None\\n\\n            if flag in (\\"text\\", \\"tail\\"):\\n                return None\\n            else:\\n                if element.text:\\n                    return element, key, parents, \\"text\\"\\n                elif len(element):\\n                    parents.append(element)\\n                    return element[0], 0, parents, None\\n                else:\\n                    return None\\n\\n        def getNextSibling(self, node):\\n            if isinstance(node, tuple):\\n                element, key, parents, flag = node\\n            else:\\n                return None\\n\\n            if flag == \\"text\\":\\n                if len(element):\\n                    parents.append(element)\\n                    return element[0], 0, parents, None\\n                else:\\n                    return None\\n            else:\\n                if element.tail and flag != \\"tail\\":\\n                    return element, key, parents, \\"tail\\"\\n                elif key \\u003c len(parents[-1]) - 1:\\n                    return parents[-1][key + 1], key + 1, parents, None\\n                else:\\n                    return None\\n\\n        def getParentNode(self, node):\\n            if isinstance(node, tuple):\\n                element, key, parents, flag = node\\n            else:\\n                return None\\n\\n            if flag == \\"text\\":\\n                if not parents:\\n                    return element\\n                else:\\n                    return element, key, parents, None\\n            else:\\n                parent = parents.pop()\\n                if not parents:\\n                    return parent\\n                else:\\n                    return parent, list(parents[-1]).index(parent), parents, None\\n\\n    return locals()\\n\\ngetETreeModule = moduleFactoryFactory(getETreeBuilder)\\n"}\n'
line: b'{"repo_name":"xxxIsaacPeralxxx/anim-studio-tools","ref":"refs/heads/master","path":"kip/houdini/code/kip_houdini/convert.py","content":"#                 Dr. D Studios - Software Disclaimer\\n#\\n# Copyright 2009 Dr D Studios Pty Limited (ACN 127 184 954) (Dr. D Studios), its\\n# affiliates and/or its licensors.\\n#\\n###############################################################################\\n\\"\\"\\"\\nThis module will help TD\'s to convert houdini animation curve to nuke or maya\\nthe other way around also.\\n\\n.. note::\\n\\n    Please make sure you are running in proper kipHoudini environment\\n\\n.. warning::\\n\\n    Dont import this module as standalone , use this module with kip project\\n\\n\\"\\"\\"\\n\\n__authors__ = [\\"kurian.os\\"]\\n__version__ = \\"$Revision: 104961 $\\".split()[1]\\n__revision__ = __version__\\n__date__ = \\"$Date:  July 19, 2011 12:00:00 PM$\\".split()[1]\\n__copyright__ = \\"2011\\"\\n__license__ = \\"Copyright 2011 Dr D Studios Pty Limited\\"\\n__contact__ = \\"kurian.os@drdstudios.com\\"\\n__status__ = \\"Development\\"\\n\\n\\nimport os\\nimport traceback\\n#import hou\\nimport napalm.core as nap_core\\nimport node_curves as node_curves\\nimport kip.kip_reader as kip_reader\\nreload(node_curves)\\nreload(kip_reader)\\nfrom rodin import logging\\nfrom kip.kip_curve_class import *\\nfrom kip.kip_napalm_class import *\\nfrom kip.utils.kipError import *\\nfrom kip.template import *\\n\\n\\nrodin_logger = logging.get_logger(\'kipHoudini\')\\nnapalm_func = Napalm()\\n\\nGLOBAL_FPS = 24\\nGLOBAL_TIME = 1\\n\\n\\nclass HoudiniWriter(object):\\n\\n    \\"\\"\\"\\n    Creating houdini curve writer class\\n\\n    *Parents:*\\n\\n        None\\n\\n    *Children:*\\n\\n        * :func:`writeOutCurves`\\n\\n    \\"\\"\\"\\n\\n    def __init__(self):\\n        \\"\\"\\"\\n        Base init function for houdini convert.write Class.\\n        \\"\\"\\"\\n        rodin_logger.info(\\"kip houdini writing class initialized\\")\\n        self.houdini_version = \\"houdini,%s\\" % hou.applicationVersionString()\\n        self.kip_houdini_version = \\"kipHoudini%s\\" % os.getenv(\\"DRD_KIPHOUDINI_VERSION\\")\\n\\n    def writeOutCurves(self, nap_file_name = None , houdini_nodes = [],\\n                            houdini_node_attributes = [], start_frame = None,\\n                            end_frame = None, write_xml = False, silent = False,\\n                            left_eyes = [], right_eyes = [], map_file_name = None):\\n\\n        \\"\\"\\"\\n        This function will create a curve class object first and then it will write out the napalm file.\\n\\n        .. warning::\\n\\n            If you are unable to write out napalm file or write_status=False that means napalm failed to write out.\\n\\n        :param  nap_file_name: User must pass a file where he want to write out curves and make sure you supply a .nap or .xml file format(strict)\\n\\n        :type nap_file_name: string\\n\\n        :param houdini_nodes: list of houdini objects(strict)\\n\\n        :type houdini_nodes: list\\n\\n        :param houdini_node_attribute: if you want to replace attribute from the map file then you can specify the override attribute here\\n\\n        :type houdini_node_attribute: list\\n\\n        :param start_frame: start frame to capture\\n\\n        :type start_frame: int\\n\\n        :param end_frame: end frame to capture\\n\\n        :type end_frame: int\\n\\n        :param write_xml: If you want to write out a xml file instead of napalm file then this should be true\\n\\n        :type end_frame: string\\n\\n        :param left_eyes: Left eye objects\\n\\n        :type left_eyes: list\\n\\n        :param right_eyes: Right eye objects\\n\\n        :type right_eyes: list\\n\\n        :param map_file_name: Filepath of napalm channel data\\n\\n        :type map_file_name: string\\n\\n        :return: Status,channel file , map file\\n\\n        :rtype: boot,string,string\\n\\n        Example\\n\\n            \\u003e\\u003e\\u003e import kip_houdini.convert as kh\\n            \\u003e\\u003e\\u003e reload(kh)\\n            \\u003e\\u003e\\u003e khcw = kh.HoudiniWriter()\\n            \\u003e\\u003e\\u003e status,nap_file,map_file=khcw.writeOutCurves(nap_file_name = \\"/tmp/houdini_kip_test_s.nap\\",map_file_name= \\"/tmp/houdini_kip_test_m.nap\\",houdini_nodes = [\\"/obj/geo/xform_1\\",\\"/obj/geo/xform_2\\"],left_eyes=[\\"/obj/geo/xform_1\\"],right_eyes=[\\"/obj/geo/xform_2\\"])\\n\\n        \\"\\"\\"\\n        if nap_file_name:\\n            node_curv = node_curves.NodeCurves()\\n            get_all_curves = node_curv.getCurves(houdini_node_curves = houdini_nodes, \\\\\\n                                houdini_attribute_curves = houdini_node_attributes, \\\\\\n                                start_frame = start_frame, end_frame = end_frame, \\\\\\n                                silent = silent, left_eye_curves = left_eyes, \\\\\\n                                right_eye_curves = right_eyes)\\n            if write_xml:\\n                if not nap_file_name.endswith(\\".xml\\"):\\n                    split_base_ext = os.path.splitext(nap_file_name)\\n                    if split_base_ext[-1]:\\n                        nap_file_name = \\"%s/.xml\\" % (split_base_ext[0])\\n                    else:\\n                        nap_file_name = \\"%s/.xml\\" % (nap_file_name)\\n            else:\\n                if not nap_file_name.endswith(\\".nap\\"):\\n                    raise KipBaseError(\\"Unknown file extension found in %s !\\" % nap_file_name)\\n\\n            write_status, map_file, nap_file = napalm_func.write(nap_file_name, get_all_curves, \\\\\\n                                        debug = True, map_file_name = map_file_name, \\\\\\n                                        software = self.houdini_version, \\\\\\n                                        app_version = self.kip_houdini_version)\\n\\n            rodin_logger.info(\\"%s %s %s\\" % (write_status, map_file, nap_file))\\n            return (write_status, map_file, nap_file)\\n        else:\\n            raise KipBaseError(\\"Expected napalm file name for write curve !\\")\\n\\nclass HoudiniReader(object):\\n    \\"\\"\\"\\n\\n    Creating houdini curve reader class\\n\\n    *Parents:*\\n\\n        None\\n\\n    *Children:*\\n\\n        * :func:`houSetAttr`\\n\\n    \\"\\"\\"\\n    def __init__(self):\\n        \\"\\"\\"\\n        Base init function for houdini convert.write Class.\\n        \\"\\"\\"\\n        rodin_logger.info(\\"kip houdini read class initialized\\")\\n        self.nuke_tan_types = {\\"spline\\":\\"spline()\\", \\"linear\\":\\"linear()\\", \\\\\\n                                    \\"constant\\":\\"constant()\\", \\"cubic\\":\\"bezier()\\"}\\n\\n        self.channel_match = {\'translateX\':\'tx\', \'translateY\':\'ty\', \'translateZ\':\'tz\', \\\\\\n                                \'rotateX\':\'rx\', \'rotateY\':\'ry\', \'rotateZ\':\'rz\', \\\\\\n                                \'scaleX\':\'sx\', \'scaleY\':\'sy\', \'scaleZ\':\'sz\'}\\n\\n    def houSetAttr(self, nap_file_name = None, houdini_nodes = [], houdini_node_attribute = None,\\n                            map_file_name = None, offset_value = 0, start_frame = None,\\n                            end_frame = None, attribute_map = None):\\n        \\"\\"\\"\\n        This function will get all curve data from a map and channel file then those data will be applied to proper nodes\\n\\n        :param  nap_file_name: User must pass a file where he want to write out curves and make sure you supply a .nap or .xml file format\\n\\n        :type nap_file_name: string\\n\\n        :param houdini_nodes: list of houdini objects\\n\\n        :type houdini_nodes: list\\n\\n        :param houdini_node_attribute: if you want to replace attribute from the map file then you can specify the override attribute here\\n\\n        :type houdini_node_attribute: string\\n\\n        :param map_file_name: Filepath of napalm channel data\\n\\n        :type map_file_name: string\\n\\n        :param offset_value: Animation key offset value\\n\\n        :type offset_value: int\\n\\n        :param start_frame: start frame to capture\\n\\n        :type start_frame: int\\n\\n        :param end_frame: end frame to capture\\n\\n        :type end_frame: int\\n\\n        :param attribute_map: This a template object from template module\\n\\n        :type attribute_map: list of tuple\\n\\n        Example\\n\\n            \\u003e\\u003e\\u003e import kip_houdini.convert as kh\\n            \\u003e\\u003e\\u003e reload(kh)\\n            \\u003e\\u003e\\u003e khpr=kh.HoudiniReader()\\n            \\u003e\\u003e\\u003e import kip.template as template\\n            \\u003e\\u003e\\u003e attr_mp = template.KipTemplates()\\n            \\u003e\\u003e\\u003e attr_mp.ATTRMAP={\\"t1.cutatt1\\":\\"/obj/geo1/xform1.ottr_1\\",\\"t1.cutatt2\\":\\"/obj/geo1/xform1.ottr_2\\",\\"t2.cutatt1\\":\\"/obj/geo1/xform1.ottr_3\\",\\"t2.cutatt2\\":\\"/obj/geo1/xform1.ottr_4\\"}\\n            \\u003e\\u003e\\u003e a = attr_mp.ATTRMAP\\n            \\u003e\\u003e\\u003e khpr.houSetAttr(nap_file_name=\\"/tmp/single_maya_test.nap\\",houdini_nodes=\\"/obj/geo1/xform1\\",attribute_map=a)\\n\\n        \\"\\"\\"\\n        if nap_file_name:\\n\\n            if not map_file_name:\\n                map_file_name = kip_reader.build_map_file_name(nap_file_name)\\n            header_info = kip_reader.header(map_file_name)\\n            array_index = kip_reader.find_software_index(header_info[\\"client_software\\"])\\n\\n            houdini_node_list = houdini_nodes\\n            knob_read = kip_reader.ReadCurve()\\n            get_curve_class = knob_read.getCurves(nap_file_name = nap_file_name, \\\\\\n                                map_file_name = map_file_name, offset_value = offset_value)\\n\\n        for each_node in get_curve_class:\\n            node_key = get_curve_class.index(each_node)\\n            current_node_curve = each_node[2]\\n            curent_source_node = each_node[0]\\n            for each_curve in current_node_curve:\\n                curve_attr\\t\\t= each_curve[1]\\n                current_key_dict = each_curve[2]\\n                time_keys \\t\\t= current_key_dict[\\"time\\"]\\n                key_value\\t\\t= current_key_dict[\\"key_value\\"]\\n                in_angle\\t\\t= current_key_dict[\\"in_angle\\"]\\n                out_angle\\t\\t= current_key_dict[\\"out_angle\\"]\\n                in_weight\\t\\t= current_key_dict[\\"in_weight\\"]\\n                out_weight\\t\\t= current_key_dict[\\"out_weight\\"]\\n                in_tan_type\\t\\t= current_key_dict[\\"in_tan_type\\"]\\n                out_tan_type\\t= current_key_dict[\\"out_tan_type\\"]\\n                in_slope\\t\\t= current_key_dict[\\"in_slope\\"]\\n                out_slope\\t\\t= current_key_dict[\\"out_slope\\"]\\n                try:\\n                    for time in time_keys:\\n\\n                        if houdini_node_attribute:\\n                            curve_attr = houdini_node_attribute\\n                        else:\\n                            if attribute_map:\\n                                temp_attr_keys = attribute_map.keys()\\n                                for each_template in temp_attr_keys:\\n                                    source_details = each_template.split(\\".\\")\\n                                    current_node_attr = \\"%s.%s\\" % (curent_source_node, \\\\\\n                                                                        each_curve[1])\\n                                    if current_node_attr == each_template:\\n                                        destenation_details = attribute_map[each_template]\\\\\\n                                                                                .split(\\".\\")\\n                                        curve_attr = destenation_details[1]\\n                                        current_houdini_node = destenation_details[0]\\n                                        current_houdini_node = hou.node(current_houdini_node)\\n                                        break\\n                            else:\\n                                current_houdini_node = hou.node(houdini_node_list[node_key])\\n                        if start_frame and end_frame:\\n                            if time in range(start_frame, end_frame + 1):\\n                                key_index = time_keys.index(time)\\n                            else:\\n                                print \\"%s not in range not applying the key\\" % time\\n                                continue\\n                        else:\\n                            key_index = time_keys.index(time)\\n                        in_tan_v = in_tan_type[key_index]\\n                        if self.nuke_tan_types.has_key(in_tan_v):\\n                            in_tan_v = self.nuke_tan_types[in_tan_v]\\n                        else:\\n                            in_tan_v = \\"bezier()\\"\\n                        hkey = hou.Keyframe()\\n                        hkey.setTime((time_keys[key_index]/GLOBAL_FPS))\\n                        hkey.setValue(key_value[key_index])\\n                        hkey.setExpression(\\"bezier()\\")\\n                        hkey.setExpression(\\"spline()\\")\\n                        hkey.setInAccel(in_weight[key_index])\\n                        hkey.setAccel(out_weight[key_index])\\n                        hkey.setInSlope(in_slope[key_index])\\n                        hkey.setSlope(out_slope[key_index])\\n                        this_node_attr = curve_attr\\n                        if self.channel_match.has_key(curve_attr):\\n                            this_node_attr = self.channel_match[curve_attr]\\n                        hou_nod = current_houdini_node.parm(this_node_attr).setKeyframe(hkey)\\n                except:\\n                    traceback.print_exc()\\n                    raise KipBaseError(\\"No objects found in node list!\\")\\n        rodin_logger.info(\\"Aniamtion curve trasfer is finished !\\")\\n        return True\\n\\ndef header(map_file_name):\\n    \\"\\"\\"\\n\\n    This function will return a dict of header details from map file\\n\\n    :param map_file_name: Filepath of napalm channel data\\n\\n    :type map_file_name: string\\n\\n    :return: header details\\n\\n    :rtype: dict\\n\\n    \\"\\"\\"\\n    if os.path.exists(map_file_name):\\n        nap_header = kip_reader.header(map_file_name)\\n        return nap_header\\n    return None\\n\\n# Copyright 2008-2012 Dr D Studios Pty Limited (ACN 127 184 954) (Dr. D Studios)\\n#\\n# This file is part of anim-studio-tools.\\n#\\n# anim-studio-tools is free software: you can redistribute it and/or modify\\n# it under the terms of the GNU Lesser General Public License as published by\\n# the Free Software Foundation, either version 3 of the License, or\\n# (at your option) any later version.\\n#\\n# anim-studio-tools is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU General Public License for more details.\\n#\\n# You should have received a copy of the GNU Lesser General Public License\\n# along with anim-studio-tools.  If not, see \\u003chttp://www.gnu.org/licenses/\\u003e.\\n\\n"}\n'
line: b'{"repo_name":"dentaku65/plugin.video.italyalacarta","ref":"refs/heads/master","path":"lib/gdata/tlslite/utils/PyCrypto_RSAKey.py","content":"\\"\\"\\"PyCrypto RSA implementation.\\"\\"\\"\\n\\nfrom cryptomath import *\\n\\nfrom RSAKey import *\\nfrom Python_RSAKey import Python_RSAKey\\n\\nif pycryptoLoaded:\\n\\n    from Crypto.PublicKey import RSA\\n\\n    class PyCrypto_RSAKey(RSAKey):\\n        def __init__(self, n=0, e=0, d=0, p=0, q=0, dP=0, dQ=0, qInv=0):\\n            if not d:\\n                self.rsa = RSA.construct( (n, e) )\\n            else:\\n                self.rsa = RSA.construct( (n, e, d, p, q) )\\n\\n        def __getattr__(self, name):\\n            return getattr(self.rsa, name)\\n\\n        def hasPrivateKey(self):\\n            return self.rsa.has_private()\\n\\n        def hash(self):\\n            return Python_RSAKey(self.n, self.e).hash()\\n\\n        def _rawPrivateKeyOp(self, m):\\n            s = numberToString(m)\\n            byteLength = numBytes(self.n)\\n            if len(s)== byteLength:\\n                pass\\n            elif len(s) == byteLength-1:\\n                s = \'\\\\0\' + s\\n            else:\\n                raise AssertionError()\\n            c = stringToNumber(self.rsa.decrypt((s,)))\\n            return c\\n\\n        def _rawPublicKeyOp(self, c):\\n            s = numberToString(c)\\n            byteLength = numBytes(self.n)\\n            if len(s)== byteLength:\\n                pass\\n            elif len(s) == byteLength-1:\\n                s = \'\\\\0\' + s\\n            else:\\n                raise AssertionError()\\n            m = stringToNumber(self.rsa.encrypt(s, None)[0])\\n            return m\\n\\n        def writeXMLPublicKey(self, indent=\'\'):\\n            return Python_RSAKey(self.n, self.e).write(indent)\\n\\n        def generate(bits):\\n            key = PyCrypto_RSAKey()\\n            def f(numBytes):\\n                return bytesToString(getRandomBytes(numBytes))\\n            key.rsa = RSA.generate(bits, f)\\n            return key\\n        generate = staticmethod(generate)\\n"}\n'
line: b'{"repo_name":"itsjeyd/edx-platform","ref":"refs/heads/master","path":"cms/djangoapps/contentstore/tests/test_import_draft_order.py","content":"\\"\\"\\"\\nTests Draft import order.\\n\\"\\"\\"\\nfrom xmodule.modulestore.xml_importer import import_course_from_xml\\n\\nfrom xmodule.modulestore.tests.django_utils import ModuleStoreTestCase\\nfrom xmodule.modulestore.django import modulestore\\nfrom django.conf import settings\\n\\nTEST_DATA_DIR = settings.COMMON_TEST_DATA_ROOT\\n\\n\\n# This test is in the CMS module because the test configuration to use a draft\\n# modulestore is dependent on django.\\nclass DraftReorderTestCase(ModuleStoreTestCase):\\n\\n    def test_order(self):\\n        \\"\\"\\"\\n        Verify that drafts are imported in the correct order.\\n        \\"\\"\\"\\n        store = modulestore()\\n        course_items = import_course_from_xml(\\n            store, self.user.id, TEST_DATA_DIR, [\'import_draft_order\'], create_if_not_present=True\\n        )\\n        course_key = course_items[0].id\\n        sequential = store.get_item(course_key.make_usage_key(\'sequential\', \'0f4f7649b10141b0bdc9922dcf94515a\'))\\n        verticals = sequential.children\\n\\n        # The order that files are read in from the file system is not guaranteed (cannot rely on\\n        # alphabetical ordering, for example). Therefore, I have added a lot of variation in filename and desired\\n        # ordering so that the test reliably failed with the bug, at least on Linux.\\n        #\\n        # \'a\', \'b\', \'c\', \'d\', and \'z\' are all drafts, with \'index_in_children_list\' of\\n        #  2 ,  4 ,  6 ,  5 , and  0  respectively.\\n        #\\n        # \'5a05be9d59fc4bb79282c94c9e6b88c7\' and \'second\' are public verticals.\\n        self.assertEqual(7, len(verticals))\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'z\'), verticals[0])\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'5a05be9d59fc4bb79282c94c9e6b88c7\'), verticals[1])\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'a\'), verticals[2])\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'second\'), verticals[3])\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'b\'), verticals[4])\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'d\'), verticals[5])\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'c\'), verticals[6])\\n\\n        # Now also test that the verticals in a second sequential are correct.\\n        sequential = store.get_item(course_key.make_usage_key(\'sequential\', \'secondseq\'))\\n        verticals = sequential.children\\n        # \'asecond\' and \'zsecond\' are drafts with \'index_in_children_list\' 0 and 2, respectively.\\n        # \'secondsubsection\' is a public vertical.\\n        self.assertEqual(3, len(verticals))\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'asecond\'), verticals[0])\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'secondsubsection\'), verticals[1])\\n        self.assertEqual(course_key.make_usage_key(\'vertical\', \'zsecond\'), verticals[2])\\n"}\n'
line: b'{"repo_name":"Baumelbi/IntroPython2016","ref":"refs/heads/master","path":"Solutions/Session06/test_mailroom2.py","content":"#!/usr/bin/env python\\n\\n\\"\\"\\"\\nunit tests for the mailroom program\\n\\"\\"\\"\\nimport os\\n\\nimport mailroom2 as mailroom\\n\\n# so that it\'s there for the tests\\nmailroom.donor_db = mailroom.get_donor_db()\\n\\n\\ndef test_list_donors():\\n    listing = mailroom.list_donors()\\n\\n    # hard to test this throughly -- better not to hard code the entire\\n    # thing. But check for a few aspects -- this will catch the likely\\n    # errors\\n    assert listing.startswith(\\"Donor list:\\\\n\\")\\n    assert \\"Jeff Bezos\\" in listing\\n    assert \\"William Gates III\\" in listing\\n    assert len(listing.split(\'\\\\n\')) == 5\\n\\n\\ndef test_find_donor():\\n    \\"\\"\\" checks a donor that is there, but with odd case and spaces\\"\\"\\"\\n    donor = mailroom.find_donor(\\"jefF beZos \\")\\n\\n    assert donor[0] == \\"Jeff Bezos\\"\\n\\n\\ndef test_find_donor_not():\\n    \\"test one that\'s not there\\"\\n    donor = mailroom.find_donor(\\"Jeff Bzos\\")\\n\\n    assert donor is None\\n\\n\\ndef test_gen_letter():\\n    \\"\\"\\" test the donor letter \\"\\"\\"\\n\\n    # create a sample donor\\n    donor = (\\"Fred Flintstone\\", [432.45, 65.45, 230.0])\\n    letter = mailroom.gen_letter(donor)\\n    # what to test? tricky!\\n    assert letter.startswith(\\"Dear Fred Flintstone\\")\\n    assert letter.endswith(\\"-The Team\\\\n\\")\\n    assert \\"donation of $230.00\\" in letter\\n\\n\\ndef test_add_donor():\\n    name = \\"Fred Flintstone  \\"\\n\\n    donor = mailroom.add_donor(name)\\n    donor[1].append(300)\\n    assert donor[0] == \\"Fred Flintstone\\"\\n    assert donor[1] == [300]\\n    assert mailroom.find_donor(name) == donor\\n\\n\\ndef test_generate_donor_report():\\n\\n    report = mailroom.generate_donor_report()\\n\\n    print(report)  # printing so you can see it if it fails\\n    # this is pretty tough to test\\n    # these are not great, because they will fail if unimportant parts of the\\n    # report are changed.\\n    # but at least you know that codes working now.\\n    assert report.startswith(\\"Donor Name                | Total Given | Num Gifts | Average Gift\\")\\n\\n    assert \\"Jeff Bezos                  $    877.33           1   $     877.33\\" in report\\n\\n\\ndef test_save_letters_to_disk():\\n    \\"\\"\\"\\n    This only tests that the files get created, but that\'s a start\\n\\n    Note that the contents of the letter was already\\n    tested with test_gen_letter\\n    \\"\\"\\"\\n\\n    mailroom.save_letters_to_disk()\\n\\n    assert os.path.isfile(\'Jeff_Bezos.txt\')\\n    assert os.path.isfile(\'William_Gates_III.txt\')\\n    # check that it\'snot empty:\\n    with open(\'William_Gates_III.txt\') as f:\\n        size = len(f.read())\\n    assert size \\u003e 0\\n\\n\\nif __name__ == \\"__main__\\":\\n    # this is best run with a test runner, like pytest\\n    # But if not, at least this will run them all.\\n    test_list_donors()\\n    test_find_donor()\\n    test_find_donor_not()\\n    test_gen_letter()\\n    test_add_donor()\\n    test_generate_donor_report()\\n    test_save_letters_to_disk()\\n    print(\\"All tests Passed\\")\\n"}\n'
line: b'{"repo_name":"odoousers2014/odoo","ref":"refs/heads/master","path":"addons/website_sale_delivery/models/sale_order.py","content":"# -*- coding: utf-8 -*-\\n\\nfrom openerp.osv import orm, fields\\nfrom openerp import SUPERUSER_ID\\nfrom openerp.addons import decimal_precision\\n\\n\\nclass delivery_carrier(orm.Model):\\n    _name = \'delivery.carrier\'\\n    _inherit = [\'delivery.carrier\', \'website.published.mixin\']\\n\\n    _columns = {\\n        \'website_description\': fields.text(\'Description for the website\'),\\n    }\\n    _defaults = {\\n        \'website_published\': True\\n    }\\n\\n\\nclass SaleOrder(orm.Model):\\n    _inherit = \'sale.order\'\\n\\n    def _amount_all_wrapper(self, cr, uid, ids, field_name, arg, context=None):        \\n        \\"\\"\\" Wrapper because of direct method passing as parameter for function fields \\"\\"\\"\\n        return self._amount_all(cr, uid, ids, field_name, arg, context=context)\\n\\n    def _amount_all(self, cr, uid, ids, field_name, arg, context=None):\\n        res = super(SaleOrder, self)._amount_all(cr, uid, ids, field_name, arg, context=context)\\n        currency_pool = self.pool.get(\'res.currency\')\\n        for order in self.browse(cr, uid, ids, context=context):\\n            line_amount = sum([line.price_subtotal for line in order.order_line if line.is_delivery])\\n            currency = order.pricelist_id.currency_id\\n            res[order.id][\'amount_delivery\'] = currency_pool.round(cr, uid, currency, line_amount)\\n        return res\\n\\n    def _get_order(self, cr, uid, ids, context=None):\\n        result = {}\\n        for line in self.pool.get(\'sale.order.line\').browse(cr, uid, ids, context=context):\\n            result[line.order_id.id] = True\\n        return result.keys()\\n\\n    _columns = {\\n        \'amount_delivery\': fields.function(\\n            _amount_all_wrapper, type=\'float\', digits_compute=decimal_precision.get_precision(\'Account\'),\\n            string=\'Delivery Amount\',\\n            store={\\n                \'sale.order\': (lambda self, cr, uid, ids, c={}: ids, [\'order_line\'], 10),\\n                \'sale.order.line\': (_get_order, [\'price_unit\', \'tax_id\', \'discount\', \'product_uom_qty\'], 10),\\n            },\\n            multi=\'sums\', help=\\"The amount without tax.\\", track_visibility=\'always\'\\n        ),\\n        \'website_order_line\': fields.one2many(\\n            \'sale.order.line\', \'order_id\',\\n            string=\'Order Lines displayed on Website\', readonly=True,\\n            domain=[(\'is_delivery\', \'=\', False)],\\n            help=\'Order Lines to be displayed on the website. They should not be used for computation purpose.\',\\n        ),\\n    }\\n\\n    def _check_carrier_quotation(self, cr, uid, order, force_carrier_id=None, context=None):\\n        carrier_obj = self.pool.get(\'delivery.carrier\')\\n\\n        # check to add or remove carrier_id\\n        if not order:\\n            return False\\n        if all(line.product_id.type == \\"service\\" for line in order.website_order_line):\\n            order.write({\'carrier_id\': None})\\n            self.pool[\'sale.order\']._delivery_unset(cr, SUPERUSER_ID, [order.id], context=context)\\n            return True\\n        else: \\n            carrier_id = force_carrier_id or order.carrier_id.id\\n            carrier_ids = self._get_delivery_methods(cr, uid, order, context=context)\\n            if carrier_id:\\n                if carrier_id not in carrier_ids:\\n                    carrier_id = False\\n                else:\\n                    carrier_ids.remove(carrier_id)\\n                    carrier_ids.insert(0, carrier_id)\\n            if force_carrier_id or not carrier_id or not carrier_id in carrier_ids:\\n                for delivery_id in carrier_ids:\\n                    grid_id = carrier_obj.grid_get(cr, SUPERUSER_ID, [delivery_id], order.partner_shipping_id.id)\\n                    if grid_id:\\n                        carrier_id = delivery_id\\n                        break\\n                order.write({\'carrier_id\': carrier_id})\\n            if carrier_id:\\n                order.delivery_set()\\n            else:\\n                order._delivery_unset()                    \\n\\n        return bool(carrier_id)\\n\\n    def _get_delivery_methods(self, cr, uid, order, context=None):\\n        carrier_obj = self.pool.get(\'delivery.carrier\')\\n        delivery_ids = carrier_obj.search(cr, uid, [(\'website_published\',\'=\',True)], context=context)\\n        # Following loop is done to avoid displaying delivery methods who are not available for this order\\n        # This can surely be done in a more efficient way, but at the moment, it mimics the way it\'s\\n        # done in delivery_set method of sale.py, from delivery module\\n        for delivery_id in carrier_obj.browse(cr, SUPERUSER_ID, delivery_ids, context=dict(context, order_id=order.id)):\\n            if not delivery_id.available:\\n                delivery_ids.remove(delivery_id.id)\\n        return delivery_ids\\n\\n    def _get_errors(self, cr, uid, order, context=None):\\n        errors = super(SaleOrder, self)._get_errors(cr, uid, order, context=context)\\n        if not self._get_delivery_methods(cr, uid, order, context=context):\\n            errors.append((\'No delivery method available\', \'There is no available delivery method for your order\'))            \\n        return errors\\n\\n    def _get_website_data(self, cr, uid, order, context=None):\\n        \\"\\"\\" Override to add delivery-related website data. \\"\\"\\"\\n        values = super(SaleOrder, self)._get_website_data(cr, uid, order, context=context)\\n        # We need a delivery only if we have stockable products\\n        has_stockable_products = False\\n        for line in order.order_line:\\n            if line.product_id.type in (\'consu\', \'product\'):\\n                has_stockable_products = True\\n        if not has_stockable_products:\\n            return values\\n\\n        delivery_ctx = dict(context, order_id=order.id)\\n        DeliveryCarrier = self.pool.get(\'delivery.carrier\')\\n        delivery_ids = self._get_delivery_methods(cr, uid, order, context=context)\\n\\n        values[\'deliveries\'] = DeliveryCarrier.browse(cr, SUPERUSER_ID, delivery_ids, context=delivery_ctx)\\n        return values\\n"}\n'
line: b'{"repo_name":"bollu/sandhi","ref":"refs/heads/master","path":"modules/gr36/grc/gui/Connection.py","content":"\\"\\"\\"\\nCopyright 2007, 2008, 2009 Free Software Foundation, Inc.\\nThis file is part of GNU Radio\\n\\nGNU Radio Companion is free software; you can redistribute it and/or\\nmodify it under the terms of the GNU General Public License\\nas published by the Free Software Foundation; either version 2\\nof the License, or (at your option) any later version.\\n\\nGNU Radio Companion is distributed in the hope that it will be useful,\\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\nGNU General Public License for more details.\\n\\nYou should have received a copy of the GNU General Public License\\nalong with this program; if not, write to the Free Software\\nFoundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA\\n\\"\\"\\"\\n\\nimport Utils\\nfrom Element import Element\\nimport Colors\\nfrom Constants import CONNECTOR_ARROW_BASE, CONNECTOR_ARROW_HEIGHT\\n\\nclass Connection(Element):\\n\\t\\"\\"\\"\\n\\tA graphical connection for ports.\\n\\tThe connection has 2 parts, the arrow and the wire.\\n\\tThe coloring of the arrow and wire exposes the status of 3 states:\\n\\t\\tenabled/disabled, valid/invalid, highlighted/non-highlighted.\\n\\tThe wire coloring exposes the enabled and highlighted states.\\n\\tThe arrow coloring exposes the enabled and valid states.\\n\\t\\"\\"\\"\\n\\n\\tdef __init__(self): Element.__init__(self)\\n\\n\\tdef get_coordinate(self):\\n\\t\\t\\"\\"\\"\\n\\t\\tGet the 0,0 coordinate.\\n\\t\\tCoordinates are irrelevant in connection.\\n\\t\\t@return 0, 0\\n\\t\\t\\"\\"\\"\\n\\t\\treturn (0, 0)\\n\\n\\tdef get_rotation(self):\\n\\t\\t\\"\\"\\"\\n\\t\\tGet the 0 degree rotation.\\n\\t\\tRotations are irrelevant in connection.\\n\\t\\t@return 0\\n\\t\\t\\"\\"\\"\\n\\t\\treturn 0\\n\\n\\tdef create_shapes(self):\\n\\t\\t\\"\\"\\"Precalculate relative coordinates.\\"\\"\\"\\n\\t\\tElement.create_shapes(self)\\n\\t\\tself._sink_rot = None\\n\\t\\tself._source_rot = None\\n\\t\\tself._sink_coor = None\\n\\t\\tself._source_coor = None\\n\\t\\t#get the source coordinate\\n\\t\\tconnector_length = self.get_source().get_connector_length()\\n\\t\\tself.x1, self.y1 = Utils.get_rotated_coordinate((connector_length, 0), self.get_source().get_rotation())\\n\\t\\t#get the sink coordinate\\n\\t\\tconnector_length = self.get_sink().get_connector_length() + CONNECTOR_ARROW_HEIGHT\\n\\t\\tself.x2, self.y2 = Utils.get_rotated_coordinate((-connector_length, 0), self.get_sink().get_rotation())\\n\\t\\t#build the arrow\\n\\t\\tself.arrow = [(0, 0),\\n\\t\\t\\tUtils.get_rotated_coordinate((-CONNECTOR_ARROW_HEIGHT, -CONNECTOR_ARROW_BASE/2), self.get_sink().get_rotation()),\\n\\t\\t\\tUtils.get_rotated_coordinate((-CONNECTOR_ARROW_HEIGHT, CONNECTOR_ARROW_BASE/2), self.get_sink().get_rotation()),\\n\\t\\t]\\n\\t\\tself._update_after_move()\\n\\t\\tif not self.get_enabled(): self._arrow_color = Colors.CONNECTION_DISABLED_COLOR\\n\\t\\telif not self.is_valid(): self._arrow_color = Colors.CONNECTION_ERROR_COLOR\\n\\t\\telse: self._arrow_color = Colors.CONNECTION_ENABLED_COLOR\\n\\n\\tdef _update_after_move(self):\\n\\t\\t\\"\\"\\"Calculate coordinates.\\"\\"\\"\\n\\t\\tself.clear() #FIXME do i want this here?\\n\\t\\t#source connector\\n\\t\\tsource = self.get_source()\\n\\t\\tX, Y = source.get_connector_coordinate()\\n\\t\\tx1, y1 = self.x1 + X, self.y1 + Y\\n\\t\\tself.add_line((x1, y1), (X, Y))\\n\\t\\t#sink connector\\n\\t\\tsink = self.get_sink()\\n\\t\\tX, Y = sink.get_connector_coordinate()\\n\\t\\tx2, y2 = self.x2 + X, self.y2 + Y\\n\\t\\tself.add_line((x2, y2), (X, Y))\\n\\t\\t#adjust arrow\\n\\t\\tself._arrow = [(x+X, y+Y) for x,y in self.arrow]\\n\\t\\t#add the horizontal and vertical lines in this connection\\n\\t\\tif abs(source.get_connector_direction() - sink.get_connector_direction()) == 180:\\n\\t\\t\\t#2 possible point sets to create a 3-line connector\\n\\t\\t\\tmid_x, mid_y = (x1 + x2)/2.0, (y1 + y2)/2.0\\n\\t\\t\\tpoints = [((mid_x, y1), (mid_x, y2)), ((x1, mid_y), (x2, mid_y))]\\n\\t\\t\\t#source connector -\\u003e points[0][0] should be in the direction of source (if possible)\\n\\t\\t\\tif Utils.get_angle_from_coordinates((x1, y1), points[0][0]) != source.get_connector_direction(): points.reverse()\\n\\t\\t\\t#points[0][0] -\\u003e sink connector should not be in the direction of sink\\n\\t\\t\\tif Utils.get_angle_from_coordinates(points[0][0], (x2, y2)) == sink.get_connector_direction(): points.reverse()\\n\\t\\t\\t#points[0][0] -\\u003e source connector should not be in the direction of source\\n\\t\\t\\tif Utils.get_angle_from_coordinates(points[0][0], (x1, y1)) == source.get_connector_direction(): points.reverse()\\n\\t\\t\\t#create 3-line connector\\n\\t\\t\\tp1, p2 = map(int, points[0][0]), map(int, points[0][1])\\n\\t\\t\\tself.add_line((x1, y1), p1)\\n\\t\\t\\tself.add_line(p1, p2)\\n\\t\\t\\tself.add_line((x2, y2), p2)\\n\\t\\telse:\\n\\t\\t\\t#2 possible points to create a right-angled connector\\n\\t\\t\\tpoints = [(x1, y2), (x2, y1)]\\n\\t\\t\\t#source connector -\\u003e points[0] should be in the direction of source (if possible)\\n\\t\\t\\tif Utils.get_angle_from_coordinates((x1, y1), points[0]) != source.get_connector_direction(): points.reverse()\\n\\t\\t\\t#points[0] -\\u003e sink connector should not be in the direction of sink\\n\\t\\t\\tif Utils.get_angle_from_coordinates(points[0], (x2, y2)) == sink.get_connector_direction(): points.reverse()\\n\\t\\t\\t#points[0] -\\u003e source connector should not be in the direction of source\\n\\t\\t\\tif Utils.get_angle_from_coordinates(points[0], (x1, y1)) == source.get_connector_direction(): points.reverse()\\n\\t\\t\\t#create right-angled connector\\n\\t\\t\\tself.add_line((x1, y1), points[0])\\n\\t\\t\\tself.add_line((x2, y2), points[0])\\n\\n\\tdef draw(self, gc, window):\\n\\t\\t\\"\\"\\"\\n\\t\\tDraw the connection.\\n\\t\\t@param gc the graphics context\\n\\t\\t@param window the gtk window to draw on\\n\\t\\t\\"\\"\\"\\n\\t\\tsink = self.get_sink()\\n\\t\\tsource = self.get_source()\\n\\t\\t#check for changes\\n\\t\\tif self._sink_rot != sink.get_rotation() or self._source_rot != source.get_rotation(): self.create_shapes()\\n\\t\\telif self._sink_coor != sink.get_coordinate() or self._source_coor != source.get_coordinate(): self._update_after_move()\\n\\t\\t#cache values\\n\\t\\tself._sink_rot = sink.get_rotation()\\n\\t\\tself._source_rot = source.get_rotation()\\n\\t\\tself._sink_coor = sink.get_coordinate()\\n\\t\\tself._source_coor = source.get_coordinate()\\n\\t\\t#draw\\n\\t\\tif self.is_highlighted(): border_color = Colors.HIGHLIGHT_COLOR\\n\\t\\telif self.get_enabled(): border_color = Colors.CONNECTION_ENABLED_COLOR\\n\\t\\telse: border_color = Colors.CONNECTION_DISABLED_COLOR\\n\\t\\tElement.draw(self, gc, window, bg_color=None, border_color=border_color)\\n\\t\\t#draw arrow on sink port\\n\\t\\tgc.set_foreground(self._arrow_color)\\n\\t\\twindow.draw_polygon(gc, True, self._arrow)\\n"}\n'
line: b'{"repo_name":"agabrown/PyGaia","ref":"refs/heads/master","path":"pygaia/utils.py","content":"__all__ = [\'enum\', \'degreesToRadians\', \'radiansToDegrees\']\\n\\nimport numpy as np\\n\\nfrom pygaia.astrometry.constants import auKmYearPerSec\\n\\ndef enum(typename, field_names):\\n    \\"\\"\\"\\n    Create a new enumeration type.\\n  \\n    Code is copyright (c) Gabriel Genellina, 2010, MIT License.\\n\\n    Parameters\\n    ----------\\n\\n    typename - Name of the enumerated type\\n    field_names - Names of the fields of the enumerated type\\n    \\"\\"\\"\\n\\n    if isinstance(field_names, str):\\n        field_names = field_names.replace(\',\', \' \').split()\\n    d = dict((reversed(nv) for nv in enumerate(field_names)), __slots__ = ())\\n    return type(typename, (object,), d)()\\n\\ndef degreesToRadians(angle):\\n    \\"\\"\\"\\n    Convert from degrees to radians.\\n\\n    Parameters\\n    ----------\\n\\n    angle - angle in degrees\\n\\n    Returns\\n    -------\\n\\n    Angle in radians.\\n    \\"\\"\\"\\n    return angle/180.0*np.pi\\n\\ndef radiansToDegrees(angle):\\n    \\"\\"\\"\\n    Convert from radians to degrees.\\n\\n    Parameters\\n    ----------\\n\\n    angle - angle in radians.\\n\\n    Returns\\n    -------\\n \\n    Angle in degrees.\\n    \\"\\"\\"\\n    return angle/np.pi*180.0\\n\\ndef construct_covariance_matrix(cvec, parallax, radial_velocity, radial_velocity_error):\\n    \\"\\"\\"\\n    Take the astrometric parameter standard uncertainties and the uncertainty correlations as quoted in\\n    the Gaia catalogue and construct the covariance matrix.\\n\\n    Parameters\\n    ----------\\n\\n    cvec : array_like\\n        Array of shape (15,) (1 source) or (n,15) (n sources) for the astrometric parameter standard\\n        uncertainties and their correlations, as listed in the Gaia catalogue [ra_error, dec_error,\\n        parallax_error, pmra_error, pmdec_error, ra_dec_corr, ra_parallax_corr, ra_pmra_corr,\\n        ra_pmdec_corr, dec_parallax_corr, dec_pmra_corr, dec_pmdec_corr, parallax_pmra_corr,\\n        parallax_pmdec_corr, pmra_pmdec_corr]. Units are (mas^2, mas^2/yr, mas^2/yr^2).\\n    \\n    parallax : array_like (n elements)\\n        Source parallax (mas).\\n    \\n    radial_velocity : array_like (n elements)\\n        Source radial velocity (km/s, does not have to be from Gaia RVS!). If the radial velocity is not\\n        known it can be set to zero.\\n\\n    radial_velocity_error : array_like (n elements)\\n        Source radial velocity  uncertainty (km/s). If the radial velocity is not know this can be set to\\n        the radial velocity dispersion for the population the source was drawn from.\\n\\n    Returns\\n    -------\\n\\n    Covariance matrix as a 6x6 array.\\n    \\"\\"\\"\\n\\n    if np.ndim(cvec)==1:\\n        cmat = np.zeros((1,6,6))\\n        nsources = 1\\n        cv = np.atleast_2d(cvec)\\n    else:\\n        nsources = cvec.shape[0]\\n        cmat = np.zeros((nsources,6,6))\\n        cv = cvec\\n    for k in range(nsources):\\n        cmat[k,0:5,0:5] = cv[k,0:5]**2\\n\\n    iu = np.triu_indices(5,k=1)\\n    for k in range(10):\\n        i = iu[0][k]\\n        j = iu[1][k]\\n        cmat[:,i,j] = cv[:,i]*cv[:,j]*cv[:,k+5]\\n        cmat[:,j,i] = cmat[:,i,j]\\n\\n    for k in range(nsources):\\n        cmat[k,0:5,5] = cmat[k,0:5,2]*np.atleast_1d(radial_velocity)[k]/auKmYearPerSec\\n    cmat[:,5,0:5] = cmat[:,0:5,5]\\n    cmat[:,5,5] = cmat[:,2,2]*(radial_velocity**2 + radial_velocity_error**2)/auKmYearPerSec**2 + \\\\\\n            (parallax*radial_velocity_error/auKmYearPerSec)**2\\n\\n    return np.squeeze(cmat)\\n"}\n'
line: b'{"repo_name":"brandonivey/django-marimo","ref":"refs/heads/master","path":"marimo/templatetags/writecapture.py","content":"import json\\nimport random\\n\\nfrom django import template\\n\\nimport logging\\nlogger = logging.getLogger(__name__)\\n\\nregister = template.Library()\\n\\ndef jsescape(string):\\n    \\"\\"\\" escaping so that javascript can be safely put into json dicts\\n        for some reason json newline escaping isn\'t enough??\\n    \\"\\"\\"\\n    return string.replace(\'\\u003cscript\',\'$BEGINSCRIPT\').replace(\'\\u003c/script\\u003e\',\'$ENDSCRIPT\').replace(\'\\\\n\', \'$NEWLINE\').replace(\'\\\\r\',\'\')\\n\\n@register.tag(name=\'writecapture\')\\ndef write_capture(parser, token):\\n    \\"\\"\\"\\n        Syntax::\\n            {% writecapture [filter] [\\"prototype\\"] [\\"widget_id\\"] %}\\n                \\u003cscript src=\\"evil.js\\"\\u003e\\n                    document.write(\'this is evil\')\\n                \\u003cscript\\u003e\\n            {% endwritecapture %}\\n\\n        Wraps the enclosed HTML inside of a marimo writecapture widget.\\n\\n        The ``filter`` argument is a boolean (default False) that turns on a\\n        writecapture feature called writeOnGetElementById. This fixes some\\n        extra-bad scripts.\\n\\n        The ``prototype`` argument defaults to \'writecapture.\' You will only\\n        need to use this if you have subclassed marimo\'s built-in writecapture\\n        widget and want to use that instead.\\n\\n        The ``widget_id`` argument defaults to a \'writecapture_\\u003crandomnumber\\u003e.\'\\n        Use this only if you need to specify an alternate element id in the DOM\\n        to write to (otherwise one will be created for you at the site of the\\n        {%writecapture%} invocation)..\\n\\n    \\"\\"\\"\\n    # TODO should work with marimo fast and widget_id should be resolved maybe\\n    tokens = token.split_contents()\\n    if len(tokens) \\u003e 4:\\n        raise template.TemplateSyntaxError(\\"writecapture block takes at most 3 arguments\\")\\n    nodelist = parser.parse((\'endwritecapture\',))\\n    parser.delete_first_token()\\n\\n    if len(tokens) \\u003e 1:\\n        script_filter = tokens[1]\\n        if script_filter == \'False\':\\n            script_filter = False\\n        elif script_filter == \'True\':\\n            script_filter = True\\n        else:\\n            script_filter = template.Variable(script_filter)\\n    else:\\n        script_filter = False\\n\\n    return WriteCaptureNode(nodelist, script_filter, *tokens[2:])\\n\\nclass WriteCaptureNode(template.Node):\\n    def __init__(self, nodelist, script_filter=False, prototype=\'writecapture_widget\', widget_id=None):\\n        self.nodelist = nodelist\\n        self.script_filter = script_filter\\n        self.prototype = prototype\\n        self.widget_id = widget_id\\n        if not self.widget_id:\\n            self.widget_id = \'writecapture\' + str(random.randint(0,99999999))\\n\\n    def render(self, context):\\n        eviloutput = jsescape(self.nodelist.render(context))\\n        if isinstance(self.script_filter, template.Variable):\\n            self.script_filter = bool(self.script_filter.resolve(context))\\n        # Set this flag in your template tag for advanced write capture widget sanitation.\\n        # Source: https://github.com/iamnoah/writeCapture/wiki/Usage\\n\\n        global_compatibility_mode = context.get(\'wc_compatibility_mode\', None)\\n        if global_compatibility_mode is None:\\n            wc_compatibility_mode = self.script_filter\\n        else:\\n            wc_compatibility_mode = global_compatibility_mode\\n\\n        widget_dict = dict(widget_prototype=self.prototype,\\n                            id=self.widget_id,\\n                            html=eviloutput,\\n                            wc_compatibility_mode = wc_compatibility_mode,\\n                         )\\n        output = \\"\\"\\"\\u003cdiv id=\\"{widget_id}\\"\\u003e\\u003c/div\\u003e\\n\\u003cscript type=\\"text/javascript\\"\\u003e\\n    marimo.emit(\'{widget_id}_ready\');\\n    marimo.add_widget({widget_json});\\n\\u003c/script\\u003e\\"\\"\\"\\n        output = output.format(\\n            widget_id=self.widget_id,\\n            widget_json=json.dumps(widget_dict),\\n        )\\n        return output\\n\\n@register.tag(name=\'writecapture_delay\')\\ndef write_capture_delay(parser, token):\\n    \\"\\"\\"\\n        Syntax::\\n            {% writecapture_delay [event_name] %}\\n    \\"\\"\\"\\n    tokens = token.split_contents()\\n    if len(tokens) \\u003e 2:\\n        raise template.TemplateSyntaxError(\\"writecapture_delay takes at most 1 argument\\")\\n    if len(tokens) == 2:\\n        return WriteCaptureDelayNode(tokens[1])\\n    return WriteCaptureDelayNode()\\n\\nclass WriteCaptureDelayNode(template.Node):\\n    def __init__(self, event=None):\\n        self.event = event\\n\\n    def render(self, context):\\n        output = \'\'\\n        if self.event is None:\\n            self.event = \'write_\' + str(random.randint(0,999999))\\n            output = \\"\\"\\"\\u003cscript type=\\"text/javascript\\"\\u003emarimo.emit(\'%s\');\\u003c/script\\u003e\\"\\"\\" % self.event\\n\\n        # this should only be used once per page if it\'s uses a second time\\n        # overwrite but log an error\\n        wc_delay = context.get(\'marimo_writecapture_delay\', None)\\n        if not wc_delay:\\n            logger.error(\\"The writecapture_delay was called but didn\'t find \\"\\n                         \\"marimo_writecapture_delay in the context. The tag \\"\\n                         \\"depends on the Marimo middleware and context_processor.\\")\\n            return output\\n        if wc_delay.marimo_event:\\n            logger.error(\'Overwriting the marimo event delay %s with %s\' %\\n                         (wc_delay.marimo_event, self.event))\\n        wc_delay.marimo_event = self.event\\n        return output\\n\\n@register.tag(name=\'writecapture_delay\')\\ndef write_capture_delay(parser, token):\\n    \\"\\"\\"\\n        Syntax::\\n            {% writecapture_delay [event_name] %}\\n    \\"\\"\\"\\n    tokens = token.split_contents()\\n    if len(tokens) \\u003e 2:\\n        raise template.TemplateSyntaxError(\\"writecapture_delay takes at most 1 argument\\")\\n    if len(tokens) == 2:\\n        return WriteCaptureDelayNode(tokens[1])\\n    return WriteCaptureDelayNode()\\n\\nclass WriteCaptureDelayNode(template.Node):\\n    def __init__(self, event=None):\\n        self.event = event\\n\\n    def render(self, context):\\n        output = \'\'\\n        if self.event is None:\\n            self.event = \'write_\' + str(random.randint(0,999999))\\n            output = \\"\\"\\"\\u003cscript type=\\"text/javascript\\"\\u003emarimo.emit(\'%s\');\\u003c/script\\u003e\\"\\"\\" % self.event\\n\\n        # this should only be used once per page if it\'s uses a second time\\n        # overwrite but log an error\\n        wc_delay = context.get(\'marimo_writecapture_delay\', None)\\n        if not wc_delay:\\n            logger.error(\\"The writecapture_delay was called but didn\'t find \\"\\n                         \\"marimo_writecapture_delay in the context. The tag \\"\\n                         \\"depends on the Marimo middleware and context_processor.\\")\\n            return output\\n        if wc_delay.marimo_event:\\n            logger.error(\'Overwriting the marimo event delay %s with %s\' %\\n                         (wc_delay.marimo_event, self.event))\\n        wc_delay.marimo_event = self.event\\n        return output\\n\\n@register.tag(name=\'writecapture_delay\')\\ndef write_capture_delay(parser, token):\\n    \\"\\"\\"\\n        Syntax::\\n            {% writecapture_delay [event_name] %}\\n    \\"\\"\\"\\n    tokens = token.split_contents()\\n    if len(tokens) \\u003e 2:\\n        raise template.TemplateSyntaxError(\\"writecapture_delay takes at most 1 argument\\")\\n    if len(tokens) == 2:\\n        return WriteCaptureDelayNode(tokens[1])\\n    return WriteCaptureDelayNode()\\n\\nclass WriteCaptureDelayNode(template.Node):\\n    def __init__(self, event=None):\\n        self.event = event\\n\\n    def render(self, context):\\n        output = \'\'\\n        if self.event is None:\\n            self.event = \'write_\' + str(random.randint(0,999999))\\n            output = \\"\\"\\"\\u003cscript type=\\"text/javascript\\"\\u003emarimo.emit(\'%s\');\\u003c/script\\u003e\\"\\"\\" % self.event\\n\\n        # this should only be used once per page if it\'s uses a second time\\n        # overwrite but log an error\\n        wc_delay = context.get(\'marimo_writecapture_delay\', None)\\n        if not wc_delay:\\n            logger.error(\\"The writecapture_delay was called but didn\'t find \\"\\n                         \\"marimo_writecapture_delay in the context. The tag \\"\\n                         \\"depends on the Marimo middleware and context_processor.\\")\\n            return output\\n        if wc_delay.marimo_event:\\n            logger.error(\'Overwriting the marimo event delay %s with %s\' %\\n                         (wc_delay.marimo_event, self.event))\\n        wc_delay.marimo_event = self.event\\n        return output\\n"}\n'
line: b'{"repo_name":"jonathonwalz/ansible","ref":"refs/heads/devel","path":"test/units/modules/network/f5/test_bigip_iapp_service.py","content":"# -*- coding: utf-8 -*-\\n#\\n# Copyright 2017 F5 Networks Inc.\\n#\\n# This file is part of Ansible\\n#\\n# Ansible is free software: you can redistribute it and/or modify\\n# it under the terms of the GNU General Public License as published by\\n# the Free Software Foundation, either version 3 of the License, or\\n# (at your option) any later version.\\n#\\n# Ansible is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU General Public License for more details.\\n#\\n# You should have received a copy of the GNU General Public License\\n# along with Ansible.  If not, see \\u003chttp://www.gnu.org/licenses/\\u003e.\\n\\nfrom __future__ import (absolute_import, division, print_function)\\n__metaclass__ = type\\n\\nimport os\\nimport json\\nimport sys\\n\\nfrom nose.plugins.skip import SkipTest\\nif sys.version_info \\u003c (2, 7):\\n    raise SkipTest(\\"F5 Ansible modules require Python \\u003e= 2.7\\")\\n\\nfrom ansible.compat.tests import unittest\\nfrom ansible.compat.tests.mock import patch, Mock\\nfrom ansible.module_utils import basic\\nfrom ansible.module_utils._text import to_bytes\\nfrom ansible.module_utils.f5_utils import AnsibleF5Client\\n\\ntry:\\n    from library.bigip_iapp_service import Parameters\\n    from library.bigip_iapp_service import ModuleManager\\n    from library.bigip_iapp_service import ArgumentSpec\\nexcept ImportError:\\n    try:\\n        from ansible.modules.network.f5.bigip_iapp_service import Parameters\\n        from ansible.modules.network.f5.bigip_iapp_service import ModuleManager\\n        from ansible.modules.network.f5.bigip_iapp_service import ArgumentSpec\\n    except ImportError:\\n        raise SkipTest(\\"F5 Ansible modules require the f5-sdk Python library\\")\\n\\nfixture_path = os.path.join(os.path.dirname(__file__), \'fixtures\')\\nfixture_data = {}\\n\\n\\ndef set_module_args(args):\\n    args = json.dumps({\'ANSIBLE_MODULE_ARGS\': args})\\n    basic._ANSIBLE_ARGS = to_bytes(args)\\n\\n\\ndef load_fixture(name):\\n    path = os.path.join(fixture_path, name)\\n\\n    if path in fixture_data:\\n        return fixture_data[path]\\n\\n    with open(path) as f:\\n        data = f.read()\\n\\n    try:\\n        data = json.loads(data)\\n    except Exception:\\n        pass\\n\\n    fixture_data[path] = data\\n    return data\\n\\n\\nclass TestParameters(unittest.TestCase):\\n\\n    def test_module_parameters_keys(self):\\n        args = load_fixture(\'create_iapp_service_parameters_f5_http.json\')\\n        p = Parameters(args)\\n\\n        # Assert the top-level keys\\n        assert p.name == \'http_example\'\\n        assert p.partition == \'Common\'\\n        assert p.template == \'/Common/f5.http\'\\n\\n    def test_module_parameters_lists(self):\\n        args = load_fixture(\'create_iapp_service_parameters_f5_http.json\')\\n        p = Parameters(args)\\n\\n        assert \'lists\' in p._values\\n\\n        assert p.lists[0][\'name\'] == \'irules__irules\'\\n        assert p.lists[0][\'encrypted\'] == \'no\'\\n        assert len(p.lists[0][\'value\']) == 1\\n        assert p.lists[0][\'value\'][0] == \'/Common/lgyft\'\\n\\n        assert p.lists[1][\'name\'] == \'net__client_vlan\'\\n        assert p.lists[1][\'encrypted\'] == \'no\'\\n        assert len(p.lists[1][\'value\']) == 1\\n        assert p.lists[1][\'value\'][0] == \'/Common/net2\'\\n\\n    def test_module_parameters_tables(self):\\n        args = load_fixture(\'create_iapp_service_parameters_f5_http.json\')\\n        p = Parameters(args)\\n\\n        assert \'tables\' in p._values\\n\\n        assert \'columnNames\' in p.tables[0]\\n        assert len(p.tables[0][\'columnNames\']) == 1\\n        assert p.tables[0][\'columnNames\'][0] == \'name\'\\n\\n        assert \'name\' in p.tables[0]\\n        assert p.tables[0][\'name\'] == \'pool__hosts\'\\n\\n        assert \'rows\' in p.tables[0]\\n        assert len(p.tables[0][\'rows\']) == 1\\n        assert \'row\' in p.tables[0][\'rows\'][0]\\n        assert len(p.tables[0][\'rows\'][0][\'row\']) == 1\\n        assert p.tables[0][\'rows\'][0][\'row\'][0] == \'demo.example.com\'\\n\\n        assert len(p.tables[1][\'rows\']) == 2\\n        assert \'row\' in p.tables[0][\'rows\'][0]\\n        assert len(p.tables[1][\'rows\'][0][\'row\']) == 2\\n        assert p.tables[1][\'rows\'][0][\'row\'][0] == \'10.1.1.1\'\\n        assert p.tables[1][\'rows\'][0][\'row\'][1] == \'0\'\\n        assert p.tables[1][\'rows\'][1][\'row\'][0] == \'10.1.1.2\'\\n        assert p.tables[1][\'rows\'][1][\'row\'][1] == \'0\'\\n\\n    def test_module_parameters_variables(self):\\n        args = load_fixture(\'create_iapp_service_parameters_f5_http.json\')\\n        p = Parameters(args)\\n\\n        assert \'variables\' in p._values\\n        assert len(p.variables) == 34\\n\\n        # Assert one configuration value\\n        assert \'name\' in p.variables[0]\\n        assert \'value\' in p.variables[0]\\n        assert p.variables[0][\'name\'] == \'afm__dos_security_profile\'\\n        assert p.variables[0][\'value\'] == \'/#do_not_use#\'\\n\\n        # Assert a second configuration value\\n        assert \'name\' in p.variables[1]\\n        assert \'value\' in p.variables[1]\\n        assert p.variables[1][\'name\'] == \'afm__policy\'\\n        assert p.variables[1][\'value\'] == \'/#do_not_use#\'\\n\\n    def test_api_parameters_variables(self):\\n        args = dict(\\n            variables=[\\n                dict(\\n                    name=\\"client__http_compression\\",\\n                    encrypted=\\"no\\",\\n                    value=\\"/#create_new#\\"\\n                )\\n            ]\\n        )\\n        p = Parameters(args)\\n        assert p.variables[0][\'name\'] == \'client__http_compression\'\\n\\n    def test_api_parameters_tables(self):\\n        args = dict(\\n            tables=[\\n                {\\n                    \\"name\\": \\"pool__members\\",\\n                    \\"columnNames\\": [\\n                        \\"addr\\",\\n                        \\"port\\",\\n                        \\"connection_limit\\"\\n                    ],\\n                    \\"rows\\": [\\n                        {\\n                            \\"row\\": [\\n                                \\"12.12.12.12\\",\\n                                \\"80\\",\\n                                \\"0\\"\\n                            ]\\n                        },\\n                        {\\n                            \\"row\\": [\\n                                \\"13.13.13.13\\",\\n                                \\"443\\",\\n                                10\\n                            ]\\n                        }\\n                    ]\\n                }\\n            ]\\n        )\\n        p = Parameters(args)\\n        assert p.tables[0][\'name\'] == \'pool__members\'\\n        assert p.tables[0][\'columnNames\'] == [\'addr\', \'port\', \'connection_limit\']\\n        assert len(p.tables[0][\'rows\']) == 2\\n        assert \'row\' in p.tables[0][\'rows\'][0]\\n        assert \'row\' in p.tables[0][\'rows\'][1]\\n        assert p.tables[0][\'rows\'][0][\'row\'] == [\'12.12.12.12\', \'80\', \'0\']\\n        assert p.tables[0][\'rows\'][1][\'row\'] == [\'13.13.13.13\', \'443\', \'10\']\\n\\n    def test_module_template_same_partition(self):\\n        args = dict(\\n            template=\'foo\',\\n            partition=\'bar\'\\n        )\\n        p = Parameters(args)\\n        assert p.template == \'/bar/foo\'\\n\\n    def test_module_template_same_partition_full_path(self):\\n        args = dict(\\n            template=\'/bar/foo\',\\n            partition=\'bar\'\\n        )\\n        p = Parameters(args)\\n        assert p.template == \'/bar/foo\'\\n\\n    def test_module_template_different_partition_full_path(self):\\n        args = dict(\\n            template=\'/Common/foo\',\\n            partition=\'bar\'\\n        )\\n        p = Parameters(args)\\n        assert p.template == \'/Common/foo\'\\n\\n\\n@patch(\'ansible.module_utils.f5_utils.AnsibleF5Client._get_mgmt_root\',\\n       return_value=True)\\nclass TestManager(unittest.TestCase):\\n\\n    def setUp(self):\\n        self.spec = ArgumentSpec()\\n\\n    def test_create_service(self, *args):\\n        parameters = load_fixture(\'create_iapp_service_parameters_f5_http.json\')\\n        set_module_args(dict(\\n            name=\'foo\',\\n            template=\'f5.http\',\\n            parameters=parameters,\\n            state=\'present\',\\n            password=\'passsword\',\\n            server=\'localhost\',\\n            user=\'admin\'\\n        ))\\n\\n        client = AnsibleF5Client(\\n            argument_spec=self.spec.argument_spec,\\n            supports_check_mode=self.spec.supports_check_mode,\\n            f5_product_name=self.spec.f5_product_name\\n        )\\n        mm = ModuleManager(client)\\n\\n        # Override methods to force specific logic in the module to happen\\n        mm.exists = Mock(return_value=False)\\n        mm.create_on_device = Mock(return_value=True)\\n\\n        results = mm.exec_module()\\n        assert results[\'changed\'] is True\\n\\n    def test_update_agent_status_traps(self, *args):\\n        parameters = load_fixture(\'update_iapp_service_parameters_f5_http.json\')\\n        set_module_args(dict(\\n            name=\'foo\',\\n            template=\'f5.http\',\\n            parameters=parameters,\\n            state=\'present\',\\n            password=\'passsword\',\\n            server=\'localhost\',\\n            user=\'admin\'\\n        ))\\n\\n        # Configure the parameters that would be returned by querying the\\n        # remote device\\n        parameters = load_fixture(\'create_iapp_service_parameters_f5_http.json\')\\n        current = Parameters(parameters)\\n\\n        client = AnsibleF5Client(\\n            argument_spec=self.spec.argument_spec,\\n            supports_check_mode=self.spec.supports_check_mode,\\n            f5_product_name=self.spec.f5_product_name\\n        )\\n        mm = ModuleManager(client)\\n\\n        # Override methods to force specific logic in the module to happen\\n        mm.exists = Mock(return_value=True)\\n        mm.update_on_device = Mock(return_value=True)\\n        mm.read_current_from_device = Mock(return_value=current)\\n\\n        results = mm.exec_module()\\n        assert results[\'changed\'] is True\\n"}\n'
line: b'{"repo_name":"eaglexmw/seascope","ref":"refs/heads/master","path":"src/view/filecontext/plugins/ctags_view/CtagsManager.py","content":"# Copyright (c) 2010 Anil Kumar\\n# All rights reserved.\\n#\\n# License: BSD \\n\\nimport subprocess\\nimport re, os\\n\\ndef _eintr_retry_call(func, *args):\\n\\twhile True:\\n\\t\\ttry:\\n\\t\\t\\treturn func(*args)\\n\\t\\texcept OSError, e:\\n\\t\\t\\tif e.errno == errno.EINTR:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\traise\\n\\ndef cmdForFile(f):\\n\\tsuffix_cmd_map = []\\n\\tcustom_map = os.getenv(\'SEASCOPE_CTAGS_SUFFIX_CMD_MAP\')\\n\\tif custom_map:\\n\\t\\tcustom_map = eval(custom_map)\\n\\t\\tsuffix_cmd_map += custom_map\\n\\t#args = \'ctags -n -u --fields=+K -f - --extra=+q\'\\n\\t#args = \'ctags -n -u --fields=+Ki -f -\'\\n\\targs = \'ctags -n -u --fields=+K -f -\'\\n\\tsuffix_cmd_map.append( [\'\', args] )\\n\\tfor (suffix, cmd) in suffix_cmd_map:\\n\\t\\tif f.endswith(suffix):\\n\\t\\t\\treturn cmd\\n\\treturn None\\n\\ndef ct_query(filename):\\n\\targs = cmdForFile(filename)\\n\\targs = args.split()\\n\\targs.append(filename)\\n\\ttry:\\n\\t\\tproc = subprocess.Popen(args, stdout=subprocess.PIPE)\\n\\t\\t(out_data, err_data) = _eintr_retry_call(proc.communicate)\\n\\t\\tout_data = out_data.split(\'\\\\n\')\\n\\texcept Exception as e:\\n\\t\\tout_data =  [\\n\\t\\t\\t\\t\'Failed to run ctags cmd\\\\tignore\\\\t0;\\\\t \',\\n\\t\\t\\t\\t\'cmd: %s\\\\tignore\\\\t0;\\\\t \' % \' \'.join(args),\\n\\t\\t\\t\\t\'error: %s\\\\tignore\\\\t0;\\\\t \' % str(e),\\n\\t\\t\\t\\t\'ctags not installed ?\\\\tignore\\\\t0;\\\\t \',\\n\\t\\t\\t]\\n\\tres = []\\n\\tfor line in out_data:\\n\\t\\tif (line == \'\'):\\n\\t\\t\\tbreak\\n\\t\\tline = line.split(\'\\\\t\')\\n\\t\\tnum = line[2].split(\';\', 1)[0]\\n\\t\\tline = [line[0], num, line[3]]\\n\\t\\tres.append(line)\\n\\treturn res\\n\\nis_OrderedDict_available = False\\ntry:\\n\\t# OrderedDict available only in python \\u003e= 2.7\\n\\tfrom collections import OrderedDict\\n\\tis_OrderedDict_available = True\\nexcept:\\n\\tpass\\n\\ndef emptyOrderedDict():\\n\\tif is_OrderedDict_available:\\n\\t\\treturn OrderedDict({})\\n\\treturn {}\\n\\nclass CtagsTreeBuilder:\\n\\tdef __init__(self):\\n\\t\\tself.symTree = emptyOrderedDict()\\n\\n\\tdef cmdForFile(self, f):\\n\\t\\tsuffix_cmd_map = []\\n\\t\\tcustom_map = os.getenv(\'SEASCOPE_CTAGS_SUFFIX_CMD_MAP\')\\n\\t\\tif custom_map:\\n\\t\\t\\tcustom_map = eval(custom_map)\\n\\t\\t\\tsuffix_cmd_map += custom_map\\n\\t\\t#args = \'ctags -n -u --fields=+K -f - --extra=+q\'\\n\\t\\t#args = \'ctags -n -u --fields=+Ki -f -\'\\n\\t\\targs = \'ctags -n -u --fields=+K-f-t -f -\'\\n\\t\\tsuffix_cmd_map.append( [\'\', args] )\\n\\t\\tfor (suffix, cmd) in suffix_cmd_map:\\n\\t\\t\\tif f.endswith(suffix):\\n\\t\\t\\t\\treturn cmd\\n\\t\\treturn None\\n\\n\\tdef runCtags(self, f):\\n\\t\\targs = self.cmdForFile(f)\\n\\t\\targs = args.split()\\n\\t\\targs.append(f)\\n\\t\\t# In python \\u003e= 2.7 can use subprocess.check_output\\n\\t\\t# output = subprocess.check_output(args)\\n\\t\\t# return output\\n\\t\\tproc = subprocess.Popen(args, stdout=subprocess.PIPE)\\n\\t\\t(out_data, err_data) = proc.communicate()\\n\\t\\treturn out_data\\n\\n\\tdef parseCtagsOutput(self, data):\\n\\t\\tdata = re.split(\'\\\\r?\\\\n\', data)\\n\\t\\tres = []\\n\\t\\tfor line in data:\\n\\t\\t\\tif line == \'\':\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tline = line.split(\'\\\\t\', 4)\\n\\t\\t\\t\\tres.append(line)\\n\\t\\t\\texcept:\\n\\t\\t\\t\\tprint \'bad line:\', line\\n\\t\\treturn res\\n\\n\\n\\tdef addToSymLayout(self, sc):\\n\\t\\tt = self.symTree\\n\\t\\tif sc and sc != \'\':\\n\\t\\t\\tfor s in re.split(\'::|\\\\.\', sc):\\n\\t\\t\\t\\tif s not in t:\\n\\t\\t\\t\\t\\tt[s] = emptyOrderedDict()\\n\\t\\t\\t\\tt = t[s]\\n\\n\\tdef addToSymTree(self, sc, line):\\n\\t\\tt = self.symTree\\n\\t\\tif sc and sc != \'\':\\n\\t\\t\\tfor s in re.split(\'::|\\\\.\', sc):\\n\\t\\t\\t\\tassert s in t\\n\\t\\t\\t\\tt = t[s]\\n\\n\\t\\tcline = [line[0], line[2].split(\';\')[0], line[3]]\\n\\t\\tif line[0] in t:\\n\\t\\t\\t#print line[0], \'in\', t\\n\\t\\t\\tx = t[line[0]]\\n\\t\\t\\tif \'+\' not in x:\\n\\t\\t\\t\\tx[\'+\'] = cline\\n\\t\\t\\t\\treturn\\n\\t\\tif \'*\' not in t:\\n\\t\\t\\tt[\'*\'] = []\\n\\t\\tt[\'*\'].append(cline)\\n\\t\\t#print \'...\', t, line\\n\\n\\tdef buildTree(self, data):\\n\\t\\ttype_list = [ \'namespace\', \'class\', \'interface\', \'struct\', \'union\', \'enum\', \'function\' ]\\n\\t\\t# build layout using 5th field\\n\\t\\tfor line in data:\\n\\t\\t\\tif len(line) == 4:\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tsd = dict([ x.split(\':\', 1) for x in line[4].split(\'\\\\t\')])\\n\\t\\t\\texcept:\\n\\t\\t\\t\\tprint \'bad line\', line\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tline[4] = sd\\n\\t\\t\\tcount = 0\\n\\t\\t\\tfor t in type_list:\\n\\t\\t\\t\\tif t in sd:\\n\\t\\t\\t\\t\\tself.addToSymLayout(sd[t])\\n\\t\\t\\t\\t\\tcount = count + 1\\n\\t\\t\\tif count != 1:\\n\\t\\t\\t\\tprint \'******** count == 1 *********\'\\n\\t\\t\\t\\tprint data\\n\\t\\t\\t\\tprint line\\n\\t\\t\\t#assert count == 1\\n\\t\\t\\n\\t\\tif len(self.symTree) == 0:\\n\\t\\t\\treturn (data, False)\\n\\t\\t\\n\\t\\tfor line in data:\\n\\t\\t\\tif len(line) == 4:\\n\\t\\t\\t\\tself.addToSymTree(None, line)\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tsd = line[4]\\n\\t\\t\\tcount = 0\\n\\t\\t\\tfor t in type_list:\\n\\t\\t\\t\\tif t in sd:\\n\\t\\t\\t\\t\\tself.addToSymTree(sd[t], line)\\n\\t\\t\\t\\t\\tcount = count + 1\\n\\t\\t\\tif count != 1:\\n\\t\\t\\t\\tprint \'******** count == 1 *********\'\\n\\t\\t\\t\\tprint data\\n\\t\\t\\t\\tprint line\\n\\t\\t\\t#assert count == 1\\n\\n\\t\\treturn (self.symTree, True)\\n\\n\\tdef doQuery(self, filename):\\n\\t\\ttry:\\n\\t\\t\\toutput = self.runCtags(filename)\\n\\t\\t\\toutput = self.parseCtagsOutput(output)\\n\\t\\t\\toutput = self.buildTree(output)\\n\\t\\texcept Exception as e:\\n\\t\\t\\tprint str(e)\\n\\t\\t\\toutput = [None, False]\\n\\t\\treturn output\\n\\n\\ndef ct_tree_query(filename):\\n\\tct = CtagsTreeBuilder()\\n\\toutput = ct.doQuery(filename)\\n\\treturn output\\n\\nif __name__ == \'__main__\':\\n\\timport optparse\\n\\timport sys\\n\\tdepth = 0\\n\\tdef recursePrint(t):\\n\\t\\tglobal depth\\n\\t\\tfor k, v in t.items():\\n\\t\\t\\tif k == \'*\':\\n\\t\\t\\t\\tfor line in v:\\n\\t\\t\\t\\t\\tprint \'%s%s\' % (\' \' * depth, line)\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\tif k == \'+\':\\n\\t\\t\\t\\tcontinue\\n\\n\\t\\t\\tif \'+\' in v:\\n\\t\\t\\t\\tk = v[\'+\']\\n\\t\\t\\tprint \'%s%s\' % (\' \' * depth, k)\\n\\t\\t\\t\\t\\n\\t\\t\\tdepth = depth + 4\\n\\t\\t\\trecursePrint(v)\\n\\t\\t\\tdepth = depth - 4\\n\\n\\top = optparse.OptionParser()\\n\\t(options, args) = op.parse_args()\\n\\tif len(args) != 1:\\n\\t\\tprint \'Please specify a file\'\\n\\t\\tsys.exit(-1)\\n\\n\\t(output, isTree) = ct_tree_query(args[0])\\n\\tif isTree:\\n\\t\\trecursePrint(output)\\n\\telse:\\n\\t\\tfor line in output:\\n\\t\\t\\tprint line\\n\\n"}\n'
line: b'{"repo_name":"Thhhza/XlsxWriter","ref":"refs/heads/master","path":"xlsxwriter/test/comparison/test_chart_column04.py","content":"###############################################################################\\n#\\n# Tests for XlsxWriter.\\n#\\n# Copyright (c), 2013-2015, John McNamara, jmcnamara@cpan.org\\n#\\n\\nfrom ..excel_comparsion_test import ExcelComparisonTest\\nfrom ...workbook import Workbook\\n\\n\\nclass TestCompareXLSXFiles(ExcelComparisonTest):\\n    \\"\\"\\"\\n    Test file created by XlsxWriter against a file created by Excel.\\n\\n    \\"\\"\\"\\n\\n    def setUp(self):\\n        self.maxDiff = None\\n\\n        filename = \'chart_column04.xlsx\'\\n\\n        test_dir = \'xlsxwriter/test/comparison/\'\\n        self.got_filename = test_dir + \'_test_\' + filename\\n        self.exp_filename = test_dir + \'xlsx_files/\' + filename\\n\\n        self.ignore_files = []\\n        self.ignore_elements = {\'xl/workbook.xml\': [\'\\u003cfileVersion\', \'\\u003ccalcPr\']}\\n\\n    def test_create_file(self):\\n        \\"\\"\\"Test the creation of a simple XlsxWriter file.\\"\\"\\"\\n\\n        workbook = Workbook(self.got_filename)\\n\\n        worksheet = workbook.add_worksheet()\\n        chart = workbook.add_chart({\'type\': \'column\'})\\n\\n        chart.axis_ids = [63591936, 63593856]\\n        chart.axis2_ids = [63613568, 63612032]\\n\\n        data = [[1, 2, 3, 4, 5],\\n                [6, 8, 6, 4, 2]]\\n\\n        worksheet.write_column(\'A1\', data[0])\\n        worksheet.write_column(\'B1\', data[1])\\n\\n        chart.add_series({\'values\': \'=Sheet1!$A$1:$A$5\'})\\n        chart.add_series({\'values\': \'=Sheet1!$B$1:$B$5\', \'y2_axis\': 1})\\n\\n        worksheet.insert_chart(\'E9\', chart)\\n\\n        workbook.close()\\n\\n        self.assertExcelEqual()\\n"}\n'
line: b'{"repo_name":"mbrukman/libcloud","ref":"refs/heads/trunk","path":"libcloud/test/compute/test_gogrid.py","content":"# Licensed to the Apache Software Foundation (ASF) under one or more\\n# contributor license agreements.  See the NOTICE file distributed with\\n# this work for additional information regarding copyright ownership.\\n# The ASF licenses this file to You under the Apache License, Version 2.0\\n# (the \\"License\\"); you may not use this file except in compliance with\\n# the License.  You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\nimport sys\\nimport unittest\\n\\nfrom libcloud.utils.py3 import httplib\\nfrom libcloud.utils.py3 import urlparse\\nfrom libcloud.utils.py3 import parse_qs\\n\\nfrom libcloud.compute.base import NodeState, NodeLocation\\nfrom libcloud.common.types import LibcloudError, InvalidCredsError\\nfrom libcloud.common.gogrid import GoGridIpAddress\\nfrom libcloud.compute.drivers.gogrid import GoGridNodeDriver\\nfrom libcloud.compute.base import Node, NodeImage, NodeSize\\n\\nfrom libcloud.test import MockHttp               # pylint: disable-msg=E0611\\nfrom libcloud.test.compute import TestCaseMixin  # pylint: disable-msg=E0611\\nfrom libcloud.test.file_fixtures import ComputeFileFixtures  # pylint: disable-msg=E0611\\n\\n\\nclass GoGridTests(unittest.TestCase, TestCaseMixin):\\n\\n    def setUp(self):\\n        GoGridNodeDriver.connectionCls.conn_classes = (None, GoGridMockHttp)\\n        GoGridMockHttp.type = None\\n        self.driver = GoGridNodeDriver(\\"foo\\", \\"bar\\")\\n\\n    def _get_test_512Mb_node_size(self):\\n        return NodeSize(id=\'512Mb\',\\n                        name=None,\\n                        ram=None,\\n                        disk=None,\\n                        bandwidth=None,\\n                        price=None,\\n                        driver=self.driver)\\n\\n    def test_create_node(self):\\n        image = NodeImage(1531, None, self.driver)\\n        node = self.driver.create_node(\\n            name=\'test1\',\\n            image=image,\\n            size=self._get_test_512Mb_node_size())\\n        self.assertEqual(node.name, \'test1\')\\n        self.assertTrue(node.id is not None)\\n        self.assertEqual(node.extra[\'password\'], \'bebebe\')\\n\\n    def test_list_nodes(self):\\n        node = self.driver.list_nodes()[0]\\n\\n        self.assertEqual(node.id, \'90967\')\\n        self.assertEqual(node.extra[\'password\'], \'bebebe\')\\n        self.assertEqual(node.extra[\'description\'], \'test server\')\\n\\n    def test_reboot_node(self):\\n        node = Node(90967, None, None, None, None, self.driver)\\n        ret = self.driver.reboot_node(node)\\n        self.assertTrue(ret)\\n\\n    def test_reboot_node_not_successful(self):\\n        GoGridMockHttp.type = \'FAIL\'\\n        node = Node(90967, None, None, None, None, self.driver)\\n\\n        try:\\n            self.driver.reboot_node(node)\\n        except Exception:\\n            pass\\n        else:\\n            self.fail(\'Exception was not thrown\')\\n\\n    def test_destroy_node(self):\\n        node = Node(90967, None, None, None, None, self.driver)\\n        ret = self.driver.destroy_node(node)\\n        self.assertTrue(ret)\\n\\n    def test_list_images(self):\\n        images = self.driver.list_images()\\n        image = images[0]\\n        self.assertEqual(len(images), 4)\\n        self.assertEqual(image.name, \'CentOS 5.3 (32-bit) w/ None\')\\n        self.assertEqual(image.id, \'1531\')\\n\\n        location = NodeLocation(\\n            id=\'gogrid/GSI-939ef909-84b8-4a2f-ad56-02ccd7da05ff.img\',\\n            name=\'test location\', country=\'Slovenia\',\\n            driver=self.driver)\\n        images = self.driver.list_images(location=location)\\n        image = images[0]\\n        self.assertEqual(len(images), 4)\\n        self.assertEqual(image.name, \'CentOS 5.3 (32-bit) w/ None\')\\n        self.assertEqual(image.id, \'1531\')\\n\\n    def test_malformed_reply(self):\\n        GoGridMockHttp.type = \'FAIL\'\\n        try:\\n            self.driver.list_images()\\n        except LibcloudError:\\n            e = sys.exc_info()[1]\\n            self.assertTrue(isinstance(e, LibcloudError))\\n        else:\\n            self.fail(\\"test should have thrown\\")\\n\\n    def test_invalid_creds(self):\\n        GoGridMockHttp.type = \'FAIL\'\\n        try:\\n            self.driver.list_nodes()\\n        except InvalidCredsError:\\n            e = sys.exc_info()[1]\\n            self.assertTrue(e.driver is not None)\\n            self.assertEqual(e.driver.name, self.driver.name)\\n        else:\\n            self.fail(\\"test should have thrown\\")\\n\\n    def test_node_creation_without_free_public_ips(self):\\n        GoGridMockHttp.type = \'NOPUBIPS\'\\n        try:\\n            image = NodeImage(1531, None, self.driver)\\n            self.driver.create_node(\\n                name=\'test1\',\\n                image=image,\\n                size=self._get_test_512Mb_node_size())\\n        except LibcloudError:\\n            e = sys.exc_info()[1]\\n            self.assertTrue(isinstance(e, LibcloudError))\\n            self.assertTrue(e.driver is not None)\\n            self.assertEqual(e.driver.name, self.driver.name)\\n        else:\\n            self.fail(\\"test should have thrown\\")\\n\\n    def test_list_locations(self):\\n        locations = self.driver.list_locations()\\n        location_names = [location.name for location in locations]\\n\\n        self.assertEqual(len(locations), 2)\\n        for i in 0, 1:\\n            self.assertTrue(isinstance(locations[i], NodeLocation))\\n        self.assertTrue(\\"US-West-1\\" in location_names)\\n        self.assertTrue(\\"US-East-1\\" in location_names)\\n\\n    def test_ex_save_image(self):\\n        node = self.driver.list_nodes()[0]\\n        image = self.driver.ex_save_image(node, \\"testimage\\")\\n        self.assertEqual(image.name, \\"testimage\\")\\n\\n    def test_ex_edit_image(self):\\n        image = self.driver.list_images()[0]\\n        ret = self.driver.ex_edit_image(image=image, public=False,\\n                                        ex_description=\\"test\\", name=\\"testname\\")\\n\\n        self.assertTrue(isinstance(ret, NodeImage))\\n\\n    def test_ex_edit_node(self):\\n        node = Node(id=90967, name=None, state=None,\\n                    public_ips=None, private_ips=None, driver=self.driver)\\n        ret = self.driver.ex_edit_node(node=node,\\n                                       size=self._get_test_512Mb_node_size())\\n\\n        self.assertTrue(isinstance(ret, Node))\\n\\n    def test_ex_list_ips(self):\\n        ips = self.driver.ex_list_ips()\\n\\n        expected_ips = {\\"192.168.75.66\\": GoGridIpAddress(id=\\"5348099\\",\\n                                                         ip=\\"192.168.75.66\\", public=True, state=\\"Unassigned\\",\\n                                                         subnet=\\"192.168.75.64/255.255.255.240\\"),\\n                        \\"192.168.75.67\\": GoGridIpAddress(id=\\"5348100\\",\\n                                                         ip=\\"192.168.75.67\\", public=True, state=\\"Assigned\\",\\n                                                         subnet=\\"192.168.75.64/255.255.255.240\\"),\\n                        \\"192.168.75.68\\": GoGridIpAddress(id=\\"5348101\\",\\n                                                         ip=\\"192.168.75.68\\", public=False, state=\\"Unassigned\\",\\n                                                         subnet=\\"192.168.75.64/255.255.255.240\\")}\\n\\n        self.assertEqual(len(expected_ips), 3)\\n\\n        for ip in ips:\\n            self.assertTrue(ip.ip in expected_ips)\\n            self.assertEqual(ip.public, expected_ips[ip.ip].public)\\n            self.assertEqual(ip.state, expected_ips[ip.ip].state)\\n            self.assertEqual(ip.subnet, expected_ips[ip.ip].subnet)\\n\\n            del expected_ips[ip.ip]\\n\\n        self.assertEqual(len(expected_ips), 0)\\n\\n    def test_get_state_invalid(self):\\n        state = self.driver._get_state(\'invalid\')\\n        self.assertEqual(state, NodeState.UNKNOWN)\\n\\n\\nclass GoGridMockHttp(MockHttp):\\n\\n    fixtures = ComputeFileFixtures(\'gogrid\')\\n\\n    def _api_grid_image_list(self, method, url, body, headers):\\n        body = self.fixtures.load(\'image_list.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    def _api_grid_image_list_FAIL(self, method, url, body, headers):\\n        body = \\"\\u003ch3\\u003esome non valid json here\\u003c/h3\\u003e\\"\\n        return (httplib.SERVICE_UNAVAILABLE, body, {},\\n                httplib.responses[httplib.SERVICE_UNAVAILABLE])\\n\\n    def _api_grid_server_list(self, method, url, body, headers):\\n        body = self.fixtures.load(\'server_list.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    _api_grid_server_list_NOPUBIPS = _api_grid_server_list\\n\\n    def _api_grid_server_list_FAIL(self, method, url, body, headers):\\n        return (httplib.FORBIDDEN,\\n                \\"123\\", {}, httplib.responses[httplib.FORBIDDEN])\\n\\n    def _api_grid_ip_list(self, method, url, body, headers):\\n        body = self.fixtures.load(\'ip_list.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    def _api_grid_ip_list_NOPUBIPS(self, method, url, body, headers):\\n        body = self.fixtures.load(\'ip_list_empty.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    def _api_grid_server_power(self, method, url, body, headers):\\n        body = self.fixtures.load(\'server_power.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    def _api_grid_server_power_FAIL(self, method, url, body, headers):\\n        body = self.fixtures.load(\'server_power_fail.json\')\\n        return (httplib.NOT_FOUND, body, {}, httplib.responses[httplib.OK])\\n\\n    def _api_grid_server_add(self, method, url, body, headers):\\n        body = self.fixtures.load(\'server_add.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    _api_grid_server_add_NOPUBIPS = _api_grid_server_add\\n\\n    def _api_grid_server_delete(self, method, url, body, headers):\\n        body = self.fixtures.load(\'server_delete.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    def _api_grid_server_edit(self, method, url, body, headers):\\n        body = self.fixtures.load(\'server_edit.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    def _api_support_password_list(self, method, url, body, headers):\\n        body = self.fixtures.load(\'password_list.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    _api_support_password_list_NOPUBIPS = _api_support_password_list\\n\\n    def _api_grid_image_save(self, method, url, body, headers):\\n        body = self.fixtures.load(\'image_save.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    def _api_grid_image_edit(self, method, url, body, headers):\\n        # edit method is quite similar to save method from the response\\n        # perspective\\n        body = self.fixtures.load(\'image_save.json\')\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\n    def _api_common_lookup_list(self, method, url, body, headers):\\n        _valid_lookups = (\\"ip.datacenter\\",)\\n\\n        lookup = parse_qs(urlparse.urlparse(url).query)[\\"lookup\\"][0]\\n        if lookup in _valid_lookups:\\n            fixture_path = \\"lookup_list_%s.json\\" % \\\\\\n                (lookup.replace(\\".\\", \\"_\\"))\\n        else:\\n            raise NotImplementedError\\n        body = self.fixtures.load(fixture_path)\\n        return (httplib.OK, body, {}, httplib.responses[httplib.OK])\\n\\nif __name__ == \'__main__\':\\n    sys.exit(unittest.main())\\n"}\n'
line: b'{"repo_name":"s40523220/2016fallcp_hw","ref":"refs/heads/gh-pages","path":"plugin/liquid_tags/test_generation.py","content":"# -*- coding: utf-8 -*-\\r\\nfrom __future__ import print_function\\r\\n\\r\\nimport filecmp\\r\\nimport os\\r\\nimport unittest\\r\\nfrom shutil import rmtree\\r\\nfrom tempfile import mkdtemp\\r\\n\\r\\nimport pytest\\r\\nfrom pelican import Pelican\\r\\nfrom pelican.settings import read_settings\\r\\n\\r\\nfrom .notebook import IPYTHON_VERSION\\r\\n\\r\\nPLUGIN_DIR = os.path.dirname(__file__)\\r\\nTEST_DATA_DIR = os.path.join(PLUGIN_DIR, \'test_data\')\\r\\n\\r\\n\\r\\nclass TestFullRun(unittest.TestCase):\\r\\n    \'\'\'Test running Pelican with the Plugin\'\'\'\\r\\n\\r\\n    def setUp(self):\\r\\n        \'\'\'Create temporary output and cache folders\'\'\'\\r\\n        self.temp_path = mkdtemp(prefix=\'pelicantests.\')\\r\\n        self.temp_cache = mkdtemp(prefix=\'pelican_cache.\')\\r\\n        os.chdir(TEST_DATA_DIR)\\r\\n\\r\\n    def tearDown(self):\\r\\n        \'\'\'Remove output and cache folders\'\'\'\\r\\n        rmtree(self.temp_path)\\r\\n        rmtree(self.temp_cache)\\r\\n        os.chdir(PLUGIN_DIR)\\r\\n\\r\\n    @pytest.mark.skipif(IPYTHON_VERSION \\u003e= 3,\\r\\n                        reason=\\"output must be created with ipython version 2\\")\\r\\n    def test_generate_with_ipython3(self):\\r\\n        \'\'\'Test generation of site with the plugin.\'\'\'\\r\\n\\r\\n        base_path = os.path.dirname(os.path.abspath(__file__))\\r\\n        base_path = os.path.join(base_path, \'test_data\')\\r\\n        content_path = os.path.join(base_path, \'content\')\\r\\n        output_path = os.path.join(base_path, \'output\')\\r\\n        settings_path = os.path.join(base_path, \'pelicanconf.py\')\\r\\n        settings = read_settings(path=settings_path,\\r\\n                                 override={\'PATH\': content_path,\\r\\n                                           \'OUTPUT_PATH\': self.temp_path,\\r\\n                                           \'CACHE_PATH\': self.temp_cache,\\r\\n                                           }\\r\\n                                 )\\r\\n\\r\\n        pelican = Pelican(settings)\\r\\n        pelican.run()\\r\\n\\r\\n        # test existence\\r\\n        assert os.path.exists(os.path.join(self.temp_path,\\r\\n                                           \'test-ipython-notebook-nb-format-3.html\'))\\r\\n        assert os.path.exists(os.path.join(self.temp_path,\\r\\n                                           \'test-ipython-notebook-nb-format-4.html\'))\\r\\n\\r\\n        # test differences\\r\\n        #assert filecmp.cmp(os.path.join(output_path,\\r\\n        #                                \'test-ipython-notebook-v2.html\'),\\r\\n        #                   os.path.join(self.temp_path,\\r\\n        #                                \'test-ipython-notebook.html\'))\\r\\n\\r\\n    @pytest.mark.skipif(IPYTHON_VERSION \\u003c 3,\\r\\n                        reason=\\"output must be created with ipython version 3\\")\\r\\n    def test_generate_with_ipython2(self):\\r\\n        \'\'\'Test generation of site with the plugin.\'\'\'\\r\\n\\r\\n        base_path = os.path.dirname(os.path.abspath(__file__))\\r\\n        base_path = os.path.join(base_path, \'test_data\')\\r\\n        content_path = os.path.join(base_path, \'content\')\\r\\n        output_path = os.path.join(base_path, \'output\')\\r\\n        settings_path = os.path.join(base_path, \'pelicanconf.py\')\\r\\n        settings = read_settings(path=settings_path,\\r\\n                                 override={\'PATH\': content_path,\\r\\n                                           \'OUTPUT_PATH\': self.temp_path,\\r\\n                                           \'CACHE_PATH\': self.temp_cache,\\r\\n                                           }\\r\\n                                 )\\r\\n\\r\\n        pelican = Pelican(settings)\\r\\n        pelican.run()\\r\\n\\r\\n        # test existence\\r\\n        assert os.path.exists(os.path.join(self.temp_path,\\r\\n                                           \'test-ipython-notebook-nb-format-3.html\'))\\r\\n        assert os.path.exists(os.path.join(self.temp_path,\\r\\n                                           \'test-ipython-notebook-nb-format-4.html\'))\\r\\n\\r\\n        # test differences\\r\\n        #assert filecmp.cmp(os.path.join(output_path,\\r\\n        #                                \'test-ipython-notebook-v3.html\'),\\r\\n        #                   os.path.join(self.temp_path,\\r\\n        #                                \'test-ipython-notebook.html\'))\\r\\n"}\n'
line: b'{"repo_name":"ffantast/magnum","ref":"refs/heads/master","path":"magnum/tests/unit/objects/test_objects.py","content":"#    Copyright 2015 IBM Corp.\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\nimport datetime\\nimport gettext\\n\\nimport iso8601\\nimport netaddr\\nfrom oslo_utils import timeutils\\nfrom oslo_versionedobjects import fields\\n\\nfrom magnum.common import context as magnum_context\\nfrom magnum.common import exception\\nfrom magnum.objects import base\\nfrom magnum.objects import utils\\nfrom magnum.tests import base as test_base\\n\\ngettext.install(\'magnum\')\\n\\n\\n@base.MagnumObjectRegistry.register\\nclass MyObj(base.MagnumObject):\\n    VERSION = \'1.0\'\\n\\n    fields = {\'foo\': fields.IntegerField(),\\n              \'bar\': fields.StringField(),\\n              \'missing\': fields.StringField(),\\n              }\\n\\n    def obj_load_attr(self, attrname):\\n        setattr(self, attrname, \'loaded!\')\\n\\n    @base.remotable_classmethod\\n    def query(cls, context):\\n        obj = cls(context)\\n        obj.foo = 1\\n        obj.bar = \'bar\'\\n        obj.obj_reset_changes()\\n        return obj\\n\\n    @base.remotable\\n    def marco(self, context):\\n        return \'polo\'\\n\\n    @base.remotable\\n    def update_test(self, context):\\n        if context.project_id == \'alternate\':\\n            self.bar = \'alternate-context\'\\n        else:\\n            self.bar = \'updated\'\\n\\n    @base.remotable\\n    def save(self, context):\\n        self.obj_reset_changes()\\n\\n    @base.remotable\\n    def refresh(self, context):\\n        self.foo = 321\\n        self.bar = \'refreshed\'\\n        self.obj_reset_changes()\\n\\n    @base.remotable\\n    def modify_save_modify(self, context):\\n        self.bar = \'meow\'\\n        self.save()\\n        self.foo = 42\\n\\n\\nclass MyObj2(object):\\n    @classmethod\\n    def obj_name(cls):\\n        return \'MyObj\'\\n\\n    @base.remotable_classmethod\\n    def get(cls, *args, **kwargs):\\n        pass\\n\\n\\nclass TestSubclassedObject(MyObj):\\n    fields = {\'new_field\': fields.StringField()}\\n\\n\\nclass TestUtils(test_base.TestCase):\\n\\n    def test_datetime_or_none(self):\\n        naive_dt = datetime.datetime.now()\\n        dt = timeutils.parse_isotime(timeutils.isotime(naive_dt))\\n        self.assertEqual(utils.datetime_or_none(dt), dt)\\n        self.assertEqual(utils.datetime_or_none(dt),\\n                         naive_dt.replace(tzinfo=iso8601.iso8601.Utc(),\\n                                          microsecond=0))\\n        self.assertIsNone(utils.datetime_or_none(None))\\n        self.assertRaises(ValueError, utils.datetime_or_none, \'foo\')\\n\\n    def test_datetime_or_str_or_none(self):\\n        dts = timeutils.isotime()\\n        dt = timeutils.parse_isotime(dts)\\n        self.assertEqual(utils.datetime_or_str_or_none(dt), dt)\\n        self.assertIsNone(utils.datetime_or_str_or_none(None))\\n        self.assertEqual(utils.datetime_or_str_or_none(dts), dt)\\n        self.assertRaises(ValueError, utils.datetime_or_str_or_none, \'foo\')\\n\\n    def test_int_or_none(self):\\n        self.assertEqual(utils.int_or_none(1), 1)\\n        self.assertEqual(utils.int_or_none(\'1\'), 1)\\n        self.assertIsNone(utils.int_or_none(None))\\n        self.assertRaises(ValueError, utils.int_or_none, \'foo\')\\n\\n    def test_str_or_none(self):\\n        class Obj(object):\\n            pass\\n        self.assertEqual(utils.str_or_none(\'foo\'), \'foo\')\\n        self.assertEqual(utils.str_or_none(1), \'1\')\\n        self.assertIsNone(utils.str_or_none(None))\\n\\n    def test_ip_or_none(self):\\n        ip4 = netaddr.IPAddress(\'1.2.3.4\', 4)\\n        ip6 = netaddr.IPAddress(\'1::2\', 6)\\n        self.assertEqual(utils.ip_or_none(4)(\'1.2.3.4\'), ip4)\\n        self.assertEqual(utils.ip_or_none(6)(\'1::2\'), ip6)\\n        self.assertIsNone(utils.ip_or_none(4)(None))\\n        self.assertIsNone(utils.ip_or_none(6)(None))\\n        self.assertRaises(netaddr.AddrFormatError, utils.ip_or_none(4), \'foo\')\\n        self.assertRaises(netaddr.AddrFormatError, utils.ip_or_none(6), \'foo\')\\n\\n    def test_dt_serializer(self):\\n        class Obj(object):\\n            foo = utils.dt_serializer(\'bar\')\\n\\n        obj = Obj()\\n        obj.bar = timeutils.parse_isotime(\'1955-11-05T00:00:00Z\')\\n        self.assertEqual(\'1955-11-05T00:00:00Z\', obj.foo())\\n        obj.bar = None\\n        self.assertIsNone(obj.foo())\\n        obj.bar = \'foo\'\\n        self.assertRaises(AttributeError, obj.foo)\\n\\n    def test_dt_deserializer(self):\\n        dt = timeutils.parse_isotime(\'1955-11-05T00:00:00Z\')\\n        self.assertEqual(utils.dt_deserializer(None, timeutils.isotime(dt)),\\n                         dt)\\n        self.assertIsNone(utils.dt_deserializer(None, None))\\n        self.assertRaises(ValueError, utils.dt_deserializer, None, \'foo\')\\n\\n\\nclass _TestObject(object):\\n    def test_hydration_type_error(self):\\n        primitive = {\'magnum_object.name\': \'MyObj\',\\n                     \'magnum_object.namespace\': \'magnum\',\\n                     \'magnum_object.version\': \'1.5\',\\n                     \'magnum_object.data\': {\'foo\': \'a\'}}\\n        self.assertRaises(ValueError, MyObj.obj_from_primitive, primitive)\\n\\n    def test_hydration(self):\\n        primitive = {\'magnum_object.name\': \'MyObj\',\\n                     \'magnum_object.namespace\': \'magnum\',\\n                     \'magnum_object.version\': \'1.5\',\\n                     \'magnum_object.data\': {\'foo\': 1}}\\n        obj = MyObj.obj_from_primitive(primitive)\\n        self.assertEqual(1, obj.foo)\\n\\n    def test_hydration_bad_ns(self):\\n        primitive = {\'magnum_object.name\': \'MyObj\',\\n                     \'magnum_object.namespace\': \'foo\',\\n                     \'magnum_object.version\': \'1.5\',\\n                     \'magnum_object.data\': {\'foo\': 1}}\\n        self.assertRaises(exception.UnsupportedObjectError,\\n                          MyObj.obj_from_primitive, primitive)\\n\\n    def test_dehydration(self):\\n        expected = {\'magnum_object.name\': \'MyObj\',\\n                    \'magnum_object.namespace\': \'magnum\',\\n                    \'magnum_object.version\': \'1.5\',\\n                    \'magnum_object.data\': {\'foo\': 1}}\\n        obj = MyObj(self.context)\\n        obj.foo = 1\\n        obj.obj_reset_changes()\\n        self.assertEqual(expected, obj.obj_to_primitive())\\n\\n    def test_get_updates(self):\\n        obj = MyObj(self.context)\\n        self.assertEqual({}, obj.obj_get_changes())\\n        obj.foo = 123\\n        self.assertEqual({\'foo\': 123}, obj.obj_get_changes())\\n        obj.bar = \'test\'\\n        self.assertEqual({\'foo\': 123, \'bar\': \'test\'}, obj.obj_get_changes())\\n        obj.obj_reset_changes()\\n        self.assertEqual({}, obj.obj_get_changes())\\n\\n    def test_object_property(self):\\n        obj = MyObj(self.context, foo=1)\\n        self.assertEqual(1, obj.foo)\\n\\n    def test_object_property_type_error(self):\\n        obj = MyObj(self.context)\\n\\n        def fail():\\n            obj.foo = \'a\'\\n        self.assertRaises(ValueError, fail)\\n\\n    def test_load(self):\\n        obj = MyObj(self.context)\\n        self.assertEqual(\'loaded!\', obj.bar)\\n\\n    def test_load_in_base(self):\\n        class Foo(base.MagnumObject):\\n            fields = {\'foobar\': fields.IntegerField()}\\n        obj = Foo(self.context)\\n        # NOTE(danms): Can\'t use assertRaisesRegexp() because of py26\\n        raised = False\\n        try:\\n            obj.foobar\\n        except NotImplementedError as ex:\\n            raised = True\\n        self.assertTrue(raised)\\n        self.assertTrue(\'foobar\' in str(ex))\\n\\n    def test_loaded_in_primitive(self):\\n        obj = MyObj(self.context)\\n        obj.foo = 1\\n        obj.obj_reset_changes()\\n        self.assertEqual(\'loaded!\', obj.bar)\\n        expected = {\'magnum_object.name\': \'MyObj\',\\n                    \'magnum_object.namespace\': \'magnum\',\\n                    \'magnum_object.version\': \'1.0\',\\n                    \'magnum_object.changes\': [\'bar\'],\\n                    \'magnum_object.data\': {\'foo\': 1,\\n                                           \'bar\': \'loaded!\'}}\\n        self.assertEqual(expected, obj.obj_to_primitive())\\n\\n    def test_changes_in_primitive(self):\\n        obj = MyObj(self.context)\\n        obj.foo = 123\\n        self.assertEqual(set([\'foo\']), obj.obj_what_changed())\\n        primitive = obj.obj_to_primitive()\\n        self.assertTrue(\'magnum_object.changes\' in primitive)\\n        obj2 = MyObj.obj_from_primitive(primitive)\\n        self.assertEqual(set([\'foo\']), obj2.obj_what_changed())\\n        obj2.obj_reset_changes()\\n        self.assertEqual(set(), obj2.obj_what_changed())\\n\\n    def test_unknown_objtype(self):\\n        self.assertRaises(exception.UnsupportedObjectError,\\n                          base.MagnumObject.obj_class_from_name, \'foo\', \'1.0\')\\n\\n    def test_with_alternate_context(self):\\n        context1 = magnum_context.RequestContext(\'foo\', \'foo\')\\n        context2 = magnum_context.RequestContext(\'bar\', project_id=\'alternate\')\\n        obj = MyObj.query(context1)\\n        obj.update_test(context2)\\n        self.assertEqual(\'alternate-context\', obj.bar)\\n        self.assertRemotes()\\n\\n    def test_orphaned_object(self):\\n        obj = MyObj.query(self.context)\\n        obj._context = None\\n        self.assertRaises(exception.OrphanedObjectError,\\n                          obj.update_test)\\n        self.assertRemotes()\\n\\n    def test_changed_1(self):\\n        obj = MyObj.query(self.context)\\n        obj.foo = 123\\n        self.assertEqual(set([\'foo\']), obj.obj_what_changed())\\n        obj.update_test(self.context)\\n        self.assertEqual(set([\'foo\', \'bar\']), obj.obj_what_changed())\\n        self.assertEqual(123, obj.foo)\\n        self.assertRemotes()\\n\\n    def test_changed_2(self):\\n        obj = MyObj.query(self.context)\\n        obj.foo = 123\\n        self.assertEqual(set([\'foo\']), obj.obj_what_changed())\\n        obj.save()\\n        self.assertEqual(set([]), obj.obj_what_changed())\\n        self.assertEqual(123, obj.foo)\\n        self.assertRemotes()\\n\\n    def test_changed_3(self):\\n        obj = MyObj.query(self.context)\\n        obj.foo = 123\\n        self.assertEqual(set([\'foo\']), obj.obj_what_changed())\\n        obj.refresh()\\n        self.assertEqual(set([]), obj.obj_what_changed())\\n        self.assertEqual(321, obj.foo)\\n        self.assertEqual(\'refreshed\', obj.bar)\\n        self.assertRemotes()\\n\\n    def test_changed_4(self):\\n        obj = MyObj.query(self.context)\\n        obj.bar = \'something\'\\n        self.assertEqual(set([\'bar\']), obj.obj_what_changed())\\n        obj.modify_save_modify(self.context)\\n        self.assertEqual(set([\'foo\']), obj.obj_what_changed())\\n        self.assertEqual(42, obj.foo)\\n        self.assertEqual(\'meow\', obj.bar)\\n        self.assertRemotes()\\n\\n    def test_static_result(self):\\n        obj = MyObj.query(self.context)\\n        self.assertEqual(\'bar\', obj.bar)\\n        result = obj.marco()\\n        self.assertEqual(\'polo\', result)\\n        self.assertRemotes()\\n\\n    def test_updates(self):\\n        obj = MyObj.query(self.context)\\n        self.assertEqual(1, obj.foo)\\n        obj.update_test()\\n        self.assertEqual(\'updated\', obj.bar)\\n        self.assertRemotes()\\n\\n    def test_base_attributes(self):\\n        dt = datetime.datetime(1955, 11, 5)\\n        obj = MyObj(self.context)\\n        obj.created_at = dt\\n        obj.updated_at = dt\\n        expected = {\'magnum_object.name\': \'MyObj\',\\n                    \'magnum_object.namespace\': \'magnum\',\\n                    \'magnum_object.version\': \'1.0\',\\n                    \'magnum_object.changes\':\\n                        [\'created_at\', \'updated_at\'],\\n                    \'magnum_object.data\':\\n                        {\'created_at\': timeutils.isotime(dt),\\n                         \'updated_at\': timeutils.isotime(dt)}\\n                    }\\n        actual = obj.obj_to_primitive()\\n        # magnum_object.changes is built from a set and order is undefined\\n        self.assertEqual(sorted(expected[\'magnum_object.changes\']),\\n                         sorted(actual[\'magnum_object.changes\']))\\n        del expected[\'magnum_object.changes\'], actual[\'magnum_object.changes\']\\n        self.assertEqual(expected, actual)\\n\\n    def test_contains(self):\\n        obj = MyObj(self.context)\\n        self.assertFalse(\'foo\' in obj)\\n        obj.foo = 1\\n        self.assertTrue(\'foo\' in obj)\\n        self.assertFalse(\'does_not_exist\' in obj)\\n\\n    def test_obj_attr_is_set(self):\\n        obj = MyObj(self.context, foo=1)\\n        self.assertTrue(obj.obj_attr_is_set(\'foo\'))\\n        self.assertFalse(obj.obj_attr_is_set(\'bar\'))\\n        self.assertRaises(AttributeError, obj.obj_attr_is_set, \'bang\')\\n\\n    def test_get(self):\\n        obj = MyObj(self.context, foo=1)\\n        # Foo has value, should not get the default\\n        self.assertEqual(obj.get(\'foo\', 2), 1)\\n        # Foo has value, should return the value without error\\n        self.assertEqual(obj.get(\'foo\'), 1)\\n        # Bar is not loaded, so we should get the default\\n        self.assertEqual(obj.get(\'bar\', \'not-loaded\'), \'not-loaded\')\\n        # Bar without a default should lazy-load\\n        self.assertEqual(obj.get(\'bar\'), \'loaded!\')\\n        # Bar now has a default, but loaded value should be returned\\n        self.assertEqual(obj.get(\'bar\', \'not-loaded\'), \'loaded!\')\\n        # Invalid attribute should raise AttributeError\\n        self.assertRaises(AttributeError, obj.get, \'nothing\')\\n        # ...even with a default\\n        self.assertRaises(AttributeError, obj.get, \'nothing\', 3)\\n\\n    def test_object_inheritance(self):\\n        base_fields = list(base.MagnumObject.fields.keys())\\n        myobj_fields = [\'foo\', \'bar\', \'missing\'] + base_fields\\n        myobj3_fields = [\'new_field\']\\n        self.assertTrue(issubclass(TestSubclassedObject, MyObj))\\n        self.assertEqual(len(myobj_fields), len(MyObj.fields))\\n        self.assertEqual(set(myobj_fields), set(MyObj.fields.keys()))\\n        self.assertEqual(len(myobj_fields) + len(myobj3_fields),\\n                         len(TestSubclassedObject.fields))\\n        self.assertEqual(set(myobj_fields) | set(myobj3_fields),\\n                         set(TestSubclassedObject.fields.keys()))\\n\\n    def test_get_changes(self):\\n        obj = MyObj(self.context)\\n        self.assertEqual({}, obj.obj_get_changes())\\n        obj.foo = 123\\n        self.assertEqual({\'foo\': 123}, obj.obj_get_changes())\\n        obj.bar = \'test\'\\n        self.assertEqual({\'foo\': 123, \'bar\': \'test\'}, obj.obj_get_changes())\\n        obj.obj_reset_changes()\\n        self.assertEqual({}, obj.obj_get_changes())\\n\\n    def test_obj_fields(self):\\n        class TestObj(base.MagnumObject):\\n            fields = {\'foo\': fields.IntegerField()}\\n            obj_extra_fields = [\'bar\']\\n\\n            @property\\n            def bar(self):\\n                return \'this is bar\'\\n\\n        obj = TestObj(self.context)\\n        self.assertEqual(set([\'created_at\', \'updated_at\', \'foo\', \'bar\']),\\n                         set(obj.obj_fields))\\n\\n    def test_obj_constructor(self):\\n        obj = MyObj(self.context, foo=123, bar=\'abc\')\\n        self.assertEqual(123, obj.foo)\\n        self.assertEqual(\'abc\', obj.bar)\\n        self.assertEqual(set([\'foo\', \'bar\']), obj.obj_what_changed())\\n\\n\\nclass TestObjectSerializer(test_base.TestCase):\\n\\n    def test_object_serialization(self):\\n        ser = base.MagnumObjectSerializer()\\n        obj = MyObj(self.context)\\n        primitive = ser.serialize_entity(self.context, obj)\\n        self.assertTrue(\'magnum_object.name\' in primitive)\\n        obj2 = ser.deserialize_entity(self.context, primitive)\\n        self.assertIsInstance(obj2, MyObj)\\n        self.assertEqual(self.context, obj2._context)\\n\\n    def test_object_serialization_iterables(self):\\n        ser = base.MagnumObjectSerializer()\\n        obj = MyObj(self.context)\\n        for iterable in (list, tuple, set):\\n            thing = iterable([obj])\\n            primitive = ser.serialize_entity(self.context, thing)\\n            self.assertEqual(1, len(primitive))\\n            for item in primitive:\\n                self.assertFalse(isinstance(item, base.MagnumObject))\\n            thing2 = ser.deserialize_entity(self.context, primitive)\\n            self.assertEqual(1, len(thing2))\\n            for item in thing2:\\n                self.assertIsInstance(item, MyObj)\\n"}\n'
line: b'{"repo_name":"xboxfanj/android_kernel_htc_msm8974","ref":"refs/heads/lp5.0","path":"tools/perf/scripts/python/check-perf-trace.py","content":"# perf script event handlers, generated by perf script -g python\\n# (c) 2010, Tom Zanussi \\u003ctzanussi@gmail.com\\u003e\\n# Licensed under the terms of the GNU GPL License version 2\\n#\\n# This script tests basic functionality such as flag and symbol\\n# strings, common_xxx() calls back into perf, begin, end, unhandled\\n# events, etc.  Basically, if this script runs successfully and\\n# displays expected results, Python scripting support should be ok.\\n\\nimport os\\nimport sys\\n\\nsys.path.append(os.environ[\'PERF_EXEC_PATH\'] + \\\\\\n\\t\'/scripts/python/Perf-Trace-Util/lib/Perf/Trace\')\\n\\nfrom Core import *\\nfrom perf_trace_context import *\\n\\nunhandled = autodict()\\n\\ndef trace_begin():\\n\\tprint \\"trace_begin\\"\\n\\tpass\\n\\ndef trace_end():\\n        print_unhandled()\\n\\ndef irq__softirq_entry(event_name, context, common_cpu,\\n\\tcommon_secs, common_nsecs, common_pid, common_comm,\\n\\tvec):\\n\\t\\tprint_header(event_name, common_cpu, common_secs, common_nsecs,\\n\\t\\t\\tcommon_pid, common_comm)\\n\\n                print_uncommon(context)\\n\\n\\t\\tprint \\"vec=%s\\\\n\\" % \\\\\\n\\t\\t(symbol_str(\\"irq__softirq_entry\\", \\"vec\\", vec)),\\n\\ndef kmem__kmalloc(event_name, context, common_cpu,\\n\\tcommon_secs, common_nsecs, common_pid, common_comm,\\n\\tcall_site, ptr, bytes_req, bytes_alloc,\\n\\tgfp_flags):\\n\\t\\tprint_header(event_name, common_cpu, common_secs, common_nsecs,\\n\\t\\t\\tcommon_pid, common_comm)\\n\\n                print_uncommon(context)\\n\\n\\t\\tprint \\"call_site=%u, ptr=%u, bytes_req=%u, \\" \\\\\\n\\t\\t\\"bytes_alloc=%u, gfp_flags=%s\\\\n\\" % \\\\\\n\\t\\t(call_site, ptr, bytes_req, bytes_alloc,\\n\\n\\t\\tflag_str(\\"kmem__kmalloc\\", \\"gfp_flags\\", gfp_flags)),\\n\\ndef trace_unhandled(event_name, context, event_fields_dict):\\n    try:\\n        unhandled[event_name] += 1\\n    except TypeError:\\n        unhandled[event_name] = 1\\n\\ndef print_header(event_name, cpu, secs, nsecs, pid, comm):\\n\\tprint \\"%-20s %5u %05u.%09u %8u %-20s \\" % \\\\\\n\\t(event_name, cpu, secs, nsecs, pid, comm),\\n\\n# print trace fields not included in handler args\\ndef print_uncommon(context):\\n    print \\"common_preempt_count=%d, common_flags=%s, common_lock_depth=%d, \\" \\\\\\n        % (common_pc(context), trace_flag_str(common_flags(context)), \\\\\\n               common_lock_depth(context))\\n\\ndef print_unhandled():\\n    keys = unhandled.keys()\\n    if not keys:\\n        return\\n\\n    print \\"\\\\nunhandled events:\\\\n\\\\n\\",\\n\\n    print \\"%-40s  %10s\\\\n\\" % (\\"event\\", \\"count\\"),\\n    print \\"%-40s  %10s\\\\n\\" % (\\"----------------------------------------\\", \\\\\\n                                 \\"-----------\\"),\\n\\n    for event_name in keys:\\n\\tprint \\"%-40s  %10d\\\\n\\" % (event_name, unhandled[event_name])\\n"}\n'
line: b'{"repo_name":"Affix/CouchPotatoServer","ref":"refs/heads/master","path":"couchpotato/core/media/_base/providers/torrent/sceneaccess.py","content":"import traceback\\n\\nfrom bs4 import BeautifulSoup\\nfrom couchpotato.core.helpers.encoding import toUnicode\\nfrom couchpotato.core.helpers.variable import tryInt\\nfrom couchpotato.core.logger import CPLog\\nfrom couchpotato.core.media._base.providers.torrent.base import TorrentProvider\\n\\n\\nlog = CPLog(__name__)\\n\\n\\nclass Base(TorrentProvider):\\n\\n    urls = {\\n        \'test\': \'https://www.sceneaccess.eu/\',\\n        \'login\': \'https://www.sceneaccess.eu/login\',\\n        \'login_check\': \'https://www.sceneaccess.eu/inbox\',\\n        \'detail\': \'https://www.sceneaccess.eu/details?id=%s\',\\n        \'search\': \'https://www.sceneaccess.eu/browse?c%d=%d\',\\n        \'archive\': \'https://www.sceneaccess.eu/archive?\\u0026c%d=%d\',\\n        \'download\': \'https://www.sceneaccess.eu/%s\',\\n    }\\n\\n    http_time_between_calls = 1  # Seconds\\n\\n    def _searchOnTitle(self, title, media, quality, results):\\n\\n        url = self.buildUrl(title, media, quality)\\n        data = self.getHTMLData(url)\\n\\n        if data:\\n            html = BeautifulSoup(data)\\n\\n            try:\\n                resultsTable = html.find(\'table\', attrs = {\'id\': \'torrents-table\'})\\n                if resultsTable is None:\\n                    return\\n\\n                entries = resultsTable.find_all(\'tr\', attrs = {\'class\': \'tt_row\'})\\n                for result in entries:\\n\\n                    link = result.find(\'td\', attrs = {\'class\': \'ttr_name\'}).find(\'a\')\\n                    url = result.find(\'td\', attrs = {\'class\': \'td_dl\'}).find(\'a\')\\n                    leechers = result.find(\'td\', attrs = {\'class\': \'ttr_leechers\'}).find(\'a\')\\n                    torrent_id = link[\'href\'].replace(\'details?id=\', \'\')\\n\\n                    results.append({\\n                        \'id\': torrent_id,\\n                        \'name\': link[\'title\'],\\n                        \'url\': self.urls[\'download\'] % url[\'href\'],\\n                        \'detail_url\': self.urls[\'detail\'] % torrent_id,\\n                        \'size\': self.parseSize(result.find(\'td\', attrs = {\'class\': \'ttr_size\'}).contents[0]),\\n                        \'seeders\': tryInt(result.find(\'td\', attrs = {\'class\': \'ttr_seeders\'}).find(\'a\').string),\\n                        \'leechers\': tryInt(leechers.string) if leechers else 0,\\n                        \'get_more_info\': self.getMoreInfo,\\n                    })\\n\\n            except:\\n                log.error(\'Failed getting results from %s: %s\', (self.getName(), traceback.format_exc()))\\n\\n    def getMoreInfo(self, item):\\n        full_description = self.getCache(\'sceneaccess.%s\' % item[\'id\'], item[\'detail_url\'], cache_timeout = 25920000)\\n        html = BeautifulSoup(full_description)\\n        nfo_pre = html.find(\'div\', attrs = {\'id\': \'details_table\'})\\n        description = toUnicode(nfo_pre.text) if nfo_pre else \'\'\\n\\n        item[\'description\'] = description\\n        return item\\n\\n    # Login\\n    def getLoginParams(self):\\n        return {\\n            \'username\': self.conf(\'username\'),\\n            \'password\': self.conf(\'password\'),\\n            \'submit\': \'come on in\',\\n        }\\n\\n    def loginSuccess(self, output):\\n        return \'/inbox\' in output.lower()\\n\\n    loginCheckSuccess = loginSuccess\\n\\n\\nconfig = [{\\n    \'name\': \'sceneaccess\',\\n    \'groups\': [\\n        {\\n            \'tab\': \'searcher\',\\n            \'list\': \'torrent_providers\',\\n            \'name\': \'SceneAccess\',\\n            \'description\': \'\\u003ca href=\\"https://sceneaccess.eu/\\"\\u003eSceneAccess\\u003c/a\\u003e\',\\n            \'wizard\': True,\\n            \'icon\': \'iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAIAAACQkWg2AAAABnRSTlMAAAAAAABupgeRAAACT0lEQVR4AYVQS0sbURidO3OTmajJ5FElTTOkPmZ01GhHrIq0aoWAj1Vc+A/cuRMXbl24V9SlCGqrLhVFCrooEhCp2BAx0mobTY2kaR7qmOm87EXL1EWxh29xL+c7nPMdgGHYO5bF/gdbefnr6WlbWRnxluMwAB4Z0uEgXa7nwaDL7+/RNPzxbYvb/XJ0FBYVfd/ayh0fQ4qCGEHcm0KLRZUk7Pb2YRJPRwcsKMidnKD3t9VVT3s7BDh+z5FOZ3Vfn3h+Hltfx00mRRSRWFcUmmVNhYVqPn8dj3va2oh+txvcQRVF9ebm1fi4k+dRFbosY5rm4Hk7xxULQnJnx93S4g0EIEEQRoDLo6PrWEw8Pc0eHLwYGopMTDirqlJ7eyhYYGHhfgfHCcKYksZGVB/NcXI2mw6HhZERqrjYTNPHi4tFPh8aJIYIhgPlcCRDoZLW1s75+Z/7+59nZ/OJhLWigqAoKZX6Mjf3dXkZ3pydGYLc4aEoCCkInzQ1fRobS2xuvllaonkedfArnY5OTdGVldBkOADgqq2Nr6z8CIWaJietDHOhKB+HhwFKC6Gnq4ukKJvP9zcSbjYDXbeVlkKzuZBhnnV3e3t6UOmaJO0ODibW1hB1GYkg8R/gup7Z3TVZLJ5AILW9LcZiVpYtYBhw16O3t7cauckyeF9Tgz0ATpL2+nopmWycmbnY2LiKRjFk6/d7+/vRJfl4HGzV1T0UIM43MGBvaIBWK/YvwM5w+IMgGH8tkyEgvIpE7M3Nt6qqZrNyOq1kMmouh455Ggz+BhKY4GEc2CfwAAAAAElFTkSuQmCC\',\\n            \'options\': [\\n                {\\n                    \'name\': \'enabled\',\\n                    \'type\': \'enabler\',\\n                    \'default\': False,\\n                },\\n                {\\n                    \'name\': \'username\',\\n                    \'default\': \'\',\\n                },\\n                {\\n                    \'name\': \'password\',\\n                    \'default\': \'\',\\n                    \'type\': \'password\',\\n                },\\n                {\\n                    \'name\': \'seed_ratio\',\\n                    \'label\': \'Seed ratio\',\\n                    \'type\': \'float\',\\n                    \'default\': 1,\\n                    \'description\': \'Will not be (re)moved until this seed ratio is met.\',\\n                },\\n                {\\n                    \'name\': \'seed_time\',\\n                    \'label\': \'Seed time\',\\n                    \'type\': \'int\',\\n                    \'default\': 40,\\n                    \'description\': \'Will not be (re)moved until this seed time (in hours) is met.\',\\n                },\\n                {\\n                    \'name\': \'extra_score\',\\n                    \'advanced\': True,\\n                    \'label\': \'Extra Score\',\\n                    \'type\': \'int\',\\n                    \'default\': 20,\\n                    \'description\': \'Starting score for each release found via this provider.\',\\n                }\\n            ],\\n        },\\n    ],\\n}]\\n"}\n'
line: b'{"repo_name":"vqw/frappe","ref":"refs/heads/develop","path":"frappe/model/delete_doc.py","content":"# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\\n# MIT License. See license.txt\\n\\nfrom __future__ import unicode_literals\\n\\nimport frappe\\nimport frappe.model.meta\\nfrom frappe.model.dynamic_links import get_dynamic_link_map\\nimport frappe.defaults\\nfrom frappe.utils.file_manager import remove_all\\nfrom frappe.utils.password import delete_all_passwords_for\\nfrom frappe import _\\nfrom frappe.model.naming import revert_series_if_last\\n\\ndef delete_doc(doctype=None, name=None, force=0, ignore_doctypes=None, for_reload=False,\\n\\tignore_permissions=False, flags=None, ignore_on_trash=False):\\n\\t\\"\\"\\"\\n\\t\\tDeletes a doc(dt, dn) and validates if it is not submitted and not linked in a live record\\n\\t\\"\\"\\"\\n\\tif not ignore_doctypes: ignore_doctypes = []\\n\\n\\t# get from form\\n\\tif not doctype:\\n\\t\\tdoctype = frappe.form_dict.get(\'dt\')\\n\\t\\tname = frappe.form_dict.get(\'dn\')\\n\\n\\tnames = name\\n\\tif isinstance(name, basestring):\\n\\t\\tnames = [name]\\n\\n\\tfor name in names or []:\\n\\n\\t\\t# already deleted..?\\n\\t\\tif not frappe.db.exists(doctype, name):\\n\\t\\t\\treturn\\n\\n\\t\\t# delete attachments\\n\\t\\tremove_all(doctype, name)\\n\\n\\t\\t# delete passwords\\n\\t\\tdelete_all_passwords_for(doctype, name)\\n\\n\\t\\tdoc = None\\n\\t\\tif doctype==\\"DocType\\":\\n\\t\\t\\tif for_reload:\\n\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\tdoc = frappe.get_doc(doctype, name)\\n\\t\\t\\t\\texcept frappe.DoesNotExistError:\\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\tdoc.run_method(\\"before_reload\\")\\n\\n\\t\\t\\telse:\\n\\t\\t\\t\\tdoc = frappe.get_doc(doctype, name)\\n\\n\\t\\t\\t\\tupdate_flags(doc, flags, ignore_permissions)\\n\\t\\t\\t\\tcheck_permission_and_not_submitted(doc)\\n\\n\\t\\t\\t\\tfrappe.db.sql(\\"delete from `tabCustom Field` where dt = %s\\", name)\\n\\t\\t\\t\\tfrappe.db.sql(\\"delete from `tabCustom Script` where dt = %s\\", name)\\n\\t\\t\\t\\tfrappe.db.sql(\\"delete from `tabProperty Setter` where doc_type = %s\\", name)\\n\\t\\t\\t\\tfrappe.db.sql(\\"delete from `tabReport` where ref_doctype=%s\\", name)\\n\\n\\t\\t\\tdelete_from_table(doctype, name, ignore_doctypes, None)\\n\\n\\t\\telse:\\n\\t\\t\\tdoc = frappe.get_doc(doctype, name)\\n\\n\\t\\t\\tif not for_reload:\\n\\t\\t\\t\\tupdate_flags(doc, flags, ignore_permissions)\\n\\t\\t\\t\\tcheck_permission_and_not_submitted(doc)\\n\\n\\t\\t\\t\\tif not ignore_on_trash:\\n\\t\\t\\t\\t\\tdoc.run_method(\\"on_trash\\")\\n\\t\\t\\t\\t\\tdoc.run_method(\'on_change\')\\n\\n\\t\\t\\t\\tdynamic_linked_doctypes = [df.parent for df in get_dynamic_link_map().get(doc.doctype, [])]\\n\\t\\t\\t\\tif \\"ToDo\\" in dynamic_linked_doctypes:\\n\\t\\t\\t\\t\\tdelete_linked_todos(doc)\\n\\n\\t\\t\\t\\tif \\"Communication\\" in dynamic_linked_doctypes:\\n\\t\\t\\t\\t\\tdelete_linked_communications(doc)\\n\\n\\t\\t\\t\\tif \\"DocShare\\" in dynamic_linked_doctypes:\\n\\t\\t\\t\\t\\tdelete_shared(doc)\\n\\n\\t\\t\\t\\tif \\"Email Unsubscribe\\" in dynamic_linked_doctypes:\\n\\t\\t\\t\\t\\tdelete_email_subscribe(doc)\\n\\n\\t\\t\\t\\t# check if links exist\\n\\t\\t\\t\\tif not force:\\n\\t\\t\\t\\t\\tcheck_if_doc_is_linked(doc)\\n\\t\\t\\t\\t\\tcheck_if_doc_is_dynamically_linked(doc)\\n\\n\\t\\t\\tupdate_naming_series(doc)\\n\\t\\t\\tdelete_from_table(doctype, name, ignore_doctypes, doc)\\n\\t\\t\\tdoc.run_method(\\"after_delete\\")\\n\\n\\t\\tif doc and not frappe.flags.in_patch:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\tdoc.notify_update()\\n\\t\\t\\t\\tinsert_feed(doc)\\n\\t\\t\\texcept ImportError:\\n\\t\\t\\t\\tpass\\n\\n\\t\\t# delete user_permissions\\n\\t\\tfrappe.defaults.clear_default(parenttype=\\"User Permission\\", key=doctype, value=name)\\n\\ndef update_naming_series(doc):\\n\\tif doc.meta.autoname:\\n\\t\\tif doc.meta.autoname.startswith(\\"naming_series:\\") \\\\\\n\\t\\t\\tand getattr(doc, \\"naming_series\\", None):\\n\\t\\t\\trevert_series_if_last(doc.naming_series, doc.name)\\n\\n\\t\\telif doc.meta.autoname.split(\\":\\")[0] not in (\\"Prompt\\", \\"field\\", \\"hash\\"):\\n\\t\\t\\trevert_series_if_last(doc.meta.autoname, doc.name)\\n\\ndef delete_from_table(doctype, name, ignore_doctypes, doc):\\n\\tif doctype!=\\"DocType\\" and doctype==name:\\n\\t\\tfrappe.db.sql(\\"delete from `tabSingles` where doctype=%s\\", name)\\n\\telse:\\n\\t\\tfrappe.db.sql(\\"delete from `tab%s` where name=%s\\" % (frappe.db.escape(doctype), \\"%s\\"), (name,))\\n\\n\\t# get child tables\\n\\tif doc:\\n\\t\\ttables = [d.options for d in doc.meta.get_table_fields()]\\n\\n\\telse:\\n\\t\\tdef get_table_fields(field_doctype):\\n\\t\\t\\treturn frappe.db.sql_list(\\"\\"\\"select options from `tab{}` where fieldtype=\'Table\'\\n\\t\\t\\t\\tand parent=%s\\"\\"\\".format(field_doctype), doctype)\\n\\n\\t\\ttables = get_table_fields(\\"DocField\\")\\n\\t\\tif not frappe.flags.in_install==\\"frappe\\":\\n\\t\\t\\ttables += get_table_fields(\\"Custom Field\\")\\n\\n\\t# delete from child tables\\n\\tfor t in list(set(tables)):\\n\\t\\tif t not in ignore_doctypes:\\n\\t\\t\\tfrappe.db.sql(\\"delete from `tab%s` where parenttype=%s and parent = %s\\" % (t, \'%s\', \'%s\'), (doctype, name))\\n\\ndef update_flags(doc, flags=None, ignore_permissions=False):\\n\\tif ignore_permissions:\\n\\t\\tif not flags: flags = {}\\n\\t\\tflags[\\"ignore_permissions\\"] = ignore_permissions\\n\\n\\tif flags:\\n\\t\\tdoc.flags.update(flags)\\n\\ndef check_permission_and_not_submitted(doc):\\n\\t# permission\\n\\tif not doc.flags.ignore_permissions and frappe.session.user!=\\"Administrator\\" and (not doc.has_permission(\\"delete\\") or (doc.doctype==\\"DocType\\" and not doc.custom)):\\n\\t\\tfrappe.msgprint(_(\\"User not allowed to delete {0}: {1}\\").format(doc.doctype, doc.name), raise_exception=True)\\n\\n\\t# check if submitted\\n\\tif doc.docstatus == 1:\\n\\t\\tfrappe.msgprint(_(\\"{0} {1}: Submitted Record cannot be deleted.\\").format(doc.doctype, doc.name),\\n\\t\\t\\traise_exception=True)\\n\\ndef check_if_doc_is_linked(doc, method=\\"Delete\\"):\\n\\t\\"\\"\\"\\n\\t\\tRaises excption if the given doc(dt, dn) is linked in another record.\\n\\t\\"\\"\\"\\n\\tfrom frappe.model.rename_doc import get_link_fields\\n\\tlink_fields = get_link_fields(doc.doctype)\\n\\tlink_fields = [[lf[\'parent\'], lf[\'fieldname\'], lf[\'issingle\']] for lf in link_fields]\\n\\n\\tfor link_dt, link_field, issingle in link_fields:\\n\\t\\tif not issingle:\\n\\t\\t\\titem = frappe.db.get_value(link_dt, {link_field:doc.name},\\n\\t\\t\\t\\t[\\"name\\", \\"parent\\", \\"parenttype\\", \\"docstatus\\"], as_dict=True)\\n\\t\\t\\tif item and ((item.parent or item.name) != doc.name) \\\\\\n\\t\\t\\t\\t\\tand ((method==\\"Delete\\" and item.docstatus\\u003c2) or (method==\\"Cancel\\" and item.docstatus==1)):\\n\\t\\t\\t\\t# raise exception only if\\n\\t\\t\\t\\t# linked to an non-cancelled doc when deleting\\n\\t\\t\\t\\t# or linked to a submitted doc when cancelling\\n\\t\\t\\t\\tfrappe.throw(_(\\"Cannot delete or cancel because {0} {1} is linked with {2} {3}\\")\\n\\t\\t\\t\\t\\t.format(doc.doctype, doc.name, item.parenttype if item.parent else link_dt,\\n\\t\\t\\t\\t\\titem.parent or item.name), frappe.LinkExistsError)\\n\\ndef check_if_doc_is_dynamically_linked(doc, method=\\"Delete\\"):\\n\\t\'\'\'Raise `frappe.LinkExistsError` if the document is dynamically linked\'\'\'\\n\\tfor df in get_dynamic_link_map().get(doc.doctype, []):\\n\\t\\tif df.parent in (\\"Communication\\", \\"ToDo\\", \\"DocShare\\", \\"Email Unsubscribe\\"):\\n\\t\\t\\t# don\'t check for communication and todo!\\n\\t\\t\\tcontinue\\n\\n\\t\\tmeta = frappe.get_meta(df.parent)\\n\\t\\tif meta.issingle:\\n\\t\\t\\t# dynamic link in single doc\\n\\t\\t\\trefdoc = frappe.db.get_singles_dict(df.parent)\\n\\t\\t\\tif (refdoc.get(df.options)==doc.doctype\\n\\t\\t\\t\\tand refdoc.get(df.fieldname)==doc.name\\n\\t\\t\\t\\tand ((method==\\"Delete\\" and refdoc.docstatus \\u003c 2)\\n\\t\\t\\t\\t\\tor (method==\\"Cancel\\" and refdoc.docstatus==1))\\n\\t\\t\\t\\t):\\n\\t\\t\\t\\t# raise exception only if\\n\\t\\t\\t\\t# linked to an non-cancelled doc when deleting\\n\\t\\t\\t\\t# or linked to a submitted doc when cancelling\\n\\t\\t\\t\\tfrappe.throw(_(\\"Cannot delete or cancel because {0} {1} is linked with {2} {3}\\").format(doc.doctype,\\n\\t\\t\\t\\t\\tdoc.name, df.parent, \\"\\"), frappe.LinkExistsError)\\n\\t\\telse:\\n\\t\\t\\t# dynamic link in table\\n\\t\\t\\tfor refdoc in frappe.db.sql(\\"\\"\\"select name, docstatus from `tab{parent}` where\\n\\t\\t\\t\\t{options}=%s and {fieldname}=%s\\"\\"\\".format(**df), (doc.doctype, doc.name), as_dict=True):\\n\\n\\t\\t\\t\\tif ((method==\\"Delete\\" and refdoc.docstatus \\u003c 2) or (method==\\"Cancel\\" and refdoc.docstatus==1)):\\n\\t\\t\\t\\t\\t# raise exception only if\\n\\t\\t\\t\\t\\t# linked to an non-cancelled doc when deleting\\n\\t\\t\\t\\t\\t# or linked to a submitted doc when cancelling\\n\\t\\t\\t\\t\\tfrappe.throw(_(\\"Cannot delete or cancel because {0} {1} is linked with {2} {3}\\")\\\\\\n\\t\\t\\t\\t\\t\\t.format(doc.doctype, doc.name, df.parent, refdoc.name), frappe.LinkExistsError)\\n\\ndef delete_linked_todos(doc):\\n\\tdelete_doc(\\"ToDo\\", frappe.db.sql_list(\\"\\"\\"select name from `tabToDo`\\n\\t\\twhere reference_type=%s and reference_name=%s\\"\\"\\", (doc.doctype, doc.name)),\\n\\t\\tignore_permissions=True)\\n\\ndef delete_email_subscribe(doc):\\n\\tfrappe.db.sql(\'\'\'delete from `tabEmail Unsubscribe`\\n\\t\\twhere reference_doctype=%s and reference_name=%s\'\'\', (doc.doctype, doc.name))\\n\\ndef delete_linked_communications(doc):\\n\\t# delete comments\\n\\tfrappe.db.sql(\\"\\"\\"delete from `tabCommunication`\\n\\t\\twhere\\n\\t\\t\\tcommunication_type = \'Comment\'\\n\\t\\t\\tand reference_doctype=%s and reference_name=%s\\"\\"\\", (doc.doctype, doc.name))\\n\\n\\t# make communications orphans\\n\\tfrappe.db.sql(\\"\\"\\"update `tabCommunication`\\n\\t\\tset reference_doctype=null, reference_name=null\\n\\t\\twhere\\n\\t\\t\\tcommunication_type = \'Communication\'\\n\\t\\t\\tand reference_doctype=%s\\n\\t\\t\\tand reference_name=%s\\"\\"\\", (doc.doctype, doc.name))\\n\\n\\t# make secondary references orphans\\n\\tfrappe.db.sql(\\"\\"\\"update `tabCommunication`\\n\\t\\tset link_doctype=null, link_name=null\\n\\t\\twhere link_doctype=%s and link_name=%s\\"\\"\\", (doc.doctype, doc.name))\\n\\n\\tfrappe.db.sql(\\"\\"\\"update `tabCommunication`\\n\\t\\tset timeline_doctype=null, timeline_name=null\\n\\t\\twhere timeline_doctype=%s and timeline_name=%s\\"\\"\\", (doc.doctype, doc.name))\\n\\ndef insert_feed(doc):\\n\\tfrom frappe.utils import get_fullname\\n\\n\\tif frappe.flags.in_install or frappe.flags.in_import or getattr(doc, \\"no_feed_on_delete\\", False):\\n\\t\\treturn\\n\\n\\tfrappe.get_doc({\\n\\t\\t\\"doctype\\": \\"Communication\\",\\n\\t\\t\\"communication_type\\": \\"Comment\\",\\n\\t\\t\\"comment_type\\": \\"Deleted\\",\\n\\t\\t\\"reference_doctype\\": doc.doctype,\\n\\t\\t\\"subject\\": \\"{0} {1}\\".format(_(doc.doctype), doc.name),\\n\\t\\t\\"full_name\\": get_fullname(doc.owner)\\n\\t}).insert(ignore_permissions=True)\\n\\ndef delete_shared(doc):\\n\\tdelete_doc(\\"DocShare\\", frappe.db.sql_list(\\"\\"\\"select name from `tabDocShare`\\n\\t\\twhere share_doctype=%s and share_name=%s\\"\\"\\", (doc.doctype, doc.name)), ignore_on_trash=True)\\n"}\n'
line: b'{"repo_name":"kcompher/BuildingMachineLearningSystemsWithPython","ref":"refs/heads/master","path":"ch03/rel_post_01.py","content":"# This code is supporting material for the book\\n# Building Machine Learning Systems with Python\\n# by Willi Richert and Luis Pedro Coelho\\n# published by PACKT Publishing\\n#\\n# It is made available under the MIT License\\n\\nimport os\\nimport sys\\n\\nimport scipy as sp\\n\\nfrom sklearn.feature_extraction.text import CountVectorizer\\n\\nDIR = r\\"../data/toy\\"\\nposts = [open(os.path.join(DIR, f)).read() for f in os.listdir(DIR)]\\n\\nnew_post = \\"imaging databases\\"\\n\\nimport nltk.stem\\nenglish_stemmer = nltk.stem.SnowballStemmer(\'english\')\\n\\n\\nclass StemmedCountVectorizer(CountVectorizer):\\n\\n    def build_analyzer(self):\\n        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\\n        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\\n\\n# vectorizer = CountVectorizer(min_df=1, stop_words=\'english\',\\n# preprocessor=stemmer)\\nvectorizer = StemmedCountVectorizer(min_df=1, stop_words=\'english\')\\n\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\n\\nclass StemmedTfidfVectorizer(TfidfVectorizer):\\n\\n    def build_analyzer(self):\\n        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\\n        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\\n\\nvectorizer = StemmedTfidfVectorizer(\\n    min_df=1, stop_words=\'english\', charset_error=\'ignore\')\\nprint(vectorizer)\\n\\nX_train = vectorizer.fit_transform(posts)\\n\\nnum_samples, num_features = X_train.shape\\nprint(\\"#samples: %d, #features: %d\\" % (num_samples, num_features))\\n\\nnew_post_vec = vectorizer.transform([new_post])\\nprint(new_post_vec, type(new_post_vec))\\nprint(new_post_vec.toarray())\\nprint(vectorizer.get_feature_names())\\n\\n\\ndef dist_raw(v1, v2):\\n    delta = v1 - v2\\n    return sp.linalg.norm(delta.toarray())\\n\\n\\ndef dist_norm(v1, v2):\\n    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\\n    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\\n\\n    delta = v1_normalized - v2_normalized\\n\\n    return sp.linalg.norm(delta.toarray())\\n\\ndist = dist_norm\\n\\nbest_dist = sys.maxsize\\nbest_i = None\\n\\nfor i in range(0, num_samples):\\n    post = posts[i]\\n    if post == new_post:\\n        continue\\n    post_vec = X_train.getrow(i)\\n    d = dist(post_vec, new_post_vec)\\n\\n    print(\\"=== Post %i with dist=%.2f: %s\\" % (i, d, post))\\n\\n    if d \\u003c best_dist:\\n        best_dist = d\\n        best_i = i\\n\\nprint(\\"Best post is %i with dist=%.2f\\" % (best_i, best_dist))\\n"}\n'
line: b'{"repo_name":"petecummings/django-cms","ref":"refs/heads/develop","path":"cms/south_migrations/0015_modified_by_added.py","content":"# -*- coding: utf-8 -*-\\nimport datetime\\nfrom south.db import db\\nfrom south.v2 import SchemaMigration\\nfrom django.db import models\\n\\n\\ntry:\\n    from django.contrib.auth import get_user_model\\nexcept ImportError: # django \\u003c 1.5\\n    from django.contrib.auth.models import User\\nelse:\\n    User = get_user_model()\\n\\nuser_orm_label = \'%s.%s\' % (User._meta.app_label, User._meta.object_name)\\nuser_model_label = \'%s.%s\' % (User._meta.app_label, User._meta.model_name)\\nuser_ptr_name = \'%s_ptr\' % User._meta.object_name.lower()\\n\\nclass Migration(SchemaMigration):\\n\\n    def forwards(self, orm):\\n        # Dummy migration\\n        pass\\n\\n\\n\\n    def backwards(self, orm):\\n    # Dummy migration\\n        pass\\n\\n\\n    models = {\\n        \'auth.group\': {\\n            \'Meta\': {\'object_name\': \'Group\'},\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [],\\n                     {\'unique\': \'True\', \'max_length\': \'80\'}),\\n            \'permissions\': (\'django.db.models.fields.related.ManyToManyField\', [],\\n                            {\'to\': \\"orm[\'auth.Permission\']\\", \'symmetrical\': \'False\',\\n                             \'blank\': \'True\'})\\n        },\\n        \'auth.permission\': {\\n            \'Meta\': {\\n                \'ordering\': \\"(\'content_type__app_label\', \'content_type__model\', \'codename\')\\",\\n                \'unique_together\': \\"((\'content_type\', \'codename\'),)\\",\\n                \'object_name\': \'Permission\'},\\n            \'codename\': (\\n                \'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'content_type\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                             {\'to\': \\"orm[\'contenttypes.ContentType\']\\"}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\'})\\n        },\\n        user_model_label: {\\n            \'Meta\': {\'object_name\': User.__name__, \'db_table\': \\"\'%s\'\\" % User._meta.db_table},\\n            \'date_joined\': (\'django.db.models.fields.DateTimeField\', [],\\n                            {\'default\': \'datetime.datetime.now\'}),\\n            \'email\': (\'django.db.models.fields.EmailField\', [],\\n                      {\'max_length\': \'75\', \'blank\': \'True\'}),\\n            \'first_name\': (\'django.db.models.fields.CharField\', [],\\n                           {\'max_length\': \'30\', \'blank\': \'True\'}),\\n            \'groups\': (\'django.db.models.fields.related.ManyToManyField\', [],\\n                       {\'to\': \\"orm[\'auth.Group\']\\", \'symmetrical\': \'False\',\\n                        \'blank\': \'True\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'is_active\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'is_staff\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'is_superuser\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'last_login\': (\'django.db.models.fields.DateTimeField\', [],\\n                           {\'default\': \'datetime.datetime.now\'}),\\n            \'last_name\': (\'django.db.models.fields.CharField\', [],\\n                          {\'max_length\': \'30\', \'blank\': \'True\'}),\\n            \'password\': (\\n                \'django.db.models.fields.CharField\', [], {\'max_length\': \'128\'}),\\n            \'user_permissions\': (\\n                \'django.db.models.fields.related.ManyToManyField\', [],\\n                {\'to\': \\"orm[\'auth.Permission\']\\", \'symmetrical\': \'False\',\\n                 \'blank\': \'True\'}),\\n            \'username\': (\'django.db.models.fields.CharField\', [],\\n                         {\'unique\': \'True\', \'max_length\': \'30\'})\\n        },\\n        \'cms.cmsplugin\': {\\n            \'Meta\': {\'object_name\': \'CMSPlugin\'},\\n            \'changed_date\': (\'django.db.models.fields.DateTimeField\', [],\\n                             {\'auto_now\': \'True\', \'blank\': \'True\'}),\\n            \'creation_date\': (\'django.db.models.fields.DateTimeField\', [],\\n                              {\'default\': \'datetime.datetime.now\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'language\': (\'django.db.models.fields.CharField\', [],\\n                         {\'max_length\': \'15\', \'db_index\': \'True\'}),\\n            \'level\': (\'django.db.models.fields.PositiveIntegerField\', [],\\n                      {\'db_index\': \'True\'}),\\n            \'lft\': (\'django.db.models.fields.PositiveIntegerField\', [],\\n                    {\'db_index\': \'True\'}),\\n            \'parent\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                       {\'to\': \\"orm[\'cms.CMSPlugin\']\\", \'null\': \'True\',\\n                        \'blank\': \'True\'}),\\n            \'placeholder\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                            {\'to\': \\"orm[\'cms.Placeholder\']\\", \'null\': \'True\'}),\\n            \'plugin_type\': (\'django.db.models.fields.CharField\', [],\\n                            {\'max_length\': \'50\', \'db_index\': \'True\'}),\\n            \'position\': (\'django.db.models.fields.PositiveSmallIntegerField\', [],\\n                         {\'null\': \'True\', \'blank\': \'True\'}),\\n            \'rght\': (\'django.db.models.fields.PositiveIntegerField\', [],\\n                     {\'db_index\': \'True\'}),\\n            \'tree_id\': (\'django.db.models.fields.PositiveIntegerField\', [],\\n                        {\'db_index\': \'True\'})\\n        },\\n        \'cms.globalpagepermission\': {\\n            \'Meta\': {\'object_name\': \'GlobalPagePermission\'},\\n            \'can_add\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_change\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_change_advanced_settings\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'can_change_permissions\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'can_delete\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_moderate\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_move_page\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_publish\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_recover_page\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_view\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'group\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                      {\'to\': \\"orm[\'auth.Group\']\\", \'null\': \'True\', \'blank\': \'True\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'sites\': (\'django.db.models.fields.related.ManyToManyField\', [],\\n                      {\'symmetrical\': \'False\', \'to\': \\"orm[\'sites.Site\']\\",\\n                       \'null\': \'True\', \'blank\': \'True\'}),\\n            \'user\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                     {\'to\': \\"orm[\'%s\']\\" % user_orm_label, \'null\': \'True\', \'blank\': \'True\'})\\n        },\\n        \'cms.page\': {\\n            \'Meta\': {\'ordering\': \\"(\'site\', \'tree_id\', \'lft\')\\",\\n                     \'object_name\': \'Page\'},\\n            \'changed_by\': (\\n                \'django.db.models.fields.CharField\', [], {\'max_length\': \'70\'}),\\n            \'changed_date\': (\'django.db.models.fields.DateTimeField\', [],\\n                             {\'auto_now\': \'True\', \'blank\': \'True\'}),\\n            \'created_by\': (\\n                \'django.db.models.fields.CharField\', [], {\'max_length\': \'70\'}),\\n            \'creation_date\': (\'django.db.models.fields.DateTimeField\', [],\\n                              {\'auto_now_add\': \'True\', \'blank\': \'True\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'in_navigation\': (\'django.db.models.fields.BooleanField\', [],\\n                              {\'default\': \'True\', \'db_index\': \'True\'}),\\n            \'level\': (\'django.db.models.fields.PositiveIntegerField\', [],\\n                      {\'db_index\': \'True\'}),\\n            \'lft\': (\'django.db.models.fields.PositiveIntegerField\', [],\\n                    {\'db_index\': \'True\'}),\\n            \'limit_visibility_in_menu\': (\\n                \'django.db.models.fields.SmallIntegerField\', [],\\n                {\'default\': \'None\', \'null\': \'True\', \'db_index\': \'True\',\\n                 \'blank\': \'True\'}),\\n            \'login_required\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'moderator_state\': (\'django.db.models.fields.SmallIntegerField\', [],\\n                                {\'default\': \'1\', \'blank\': \'True\'}),\\n            \'navigation_extenders\': (\'django.db.models.fields.CharField\', [],\\n                                     {\'db_index\': \'True\', \'max_length\': \'80\',\\n                                      \'null\': \'True\', \'blank\': \'True\'}),\\n            \'parent\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                       {\'blank\': \'True\', \'related_name\': \\"\'children\'\\",\\n                        \'null\': \'True\', \'to\': \\"orm[\'cms.Page\']\\"}),\\n            \'placeholders\': (\'django.db.models.fields.related.ManyToManyField\', [],\\n                             {\'to\': \\"orm[\'cms.Placeholder\']\\",\\n                              \'symmetrical\': \'False\'}),\\n            \'publication_date\': (\'django.db.models.fields.DateTimeField\', [],\\n                                 {\'db_index\': \'True\', \'null\': \'True\',\\n                                  \'blank\': \'True\'}),\\n            \'publication_end_date\': (\'django.db.models.fields.DateTimeField\', [],\\n                                     {\'db_index\': \'True\', \'null\': \'True\',\\n                                      \'blank\': \'True\'}),\\n            \'published\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'publisher_is_draft\': (\'django.db.models.fields.BooleanField\', [],\\n                                   {\'default\': \'True\', \'db_index\': \'True\'}),\\n            \'publisher_public\': (\\n                \'django.db.models.fields.related.OneToOneField\', [],\\n                {\'related_name\': \\"\'publisher_draft\'\\", \'unique\': \'True\', \'null\': \'True\',\\n                 \'to\': \\"orm[\'cms.Page\']\\"}),\\n            \'publisher_state\': (\'django.db.models.fields.SmallIntegerField\', [],\\n                                {\'default\': \'0\', \'db_index\': \'True\'}),\\n            \'reverse_id\': (\'django.db.models.fields.CharField\', [],\\n                           {\'db_index\': \'True\', \'max_length\': \'40\', \'null\': \'True\',\\n                            \'blank\': \'True\'}),\\n            \'rght\': (\'django.db.models.fields.PositiveIntegerField\', [],\\n                     {\'db_index\': \'True\'}),\\n            \'site\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                     {\'to\': \\"orm[\'sites.Site\']\\"}),\\n            \'soft_root\': (\'django.db.models.fields.BooleanField\', [],\\n                          {\'default\': \'False\', \'db_index\': \'True\'}),\\n            \'template\': (\\n                \'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'tree_id\': (\'django.db.models.fields.PositiveIntegerField\', [],\\n                        {\'db_index\': \'True\'})\\n        },\\n        \'cms.pagemoderator\': {\\n            \'Meta\': {\'object_name\': \'PageModerator\'},\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'moderate_children\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'moderate_descendants\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'moderate_page\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'page\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                     {\'to\': \\"orm[\'cms.Page\']\\"}),\\n            \'user\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                     {\'to\': \\"orm[\'%s\']\\" % user_orm_label})\\n        },\\n        \'cms.pagemoderatorstate\': {\\n            \'Meta\': {\'ordering\': \\"(\'page\', \'action\', \'-created\')\\",\\n                     \'object_name\': \'PageModeratorState\'},\\n            \'action\': (\'django.db.models.fields.CharField\', [],\\n                       {\'max_length\': \'3\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'created\': (\'django.db.models.fields.DateTimeField\', [],\\n                        {\'auto_now_add\': \'True\', \'blank\': \'True\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'message\': (\'django.db.models.fields.TextField\', [],\\n                        {\'default\': \\"\'\'\\", \'max_length\': \'1000\', \'blank\': \'True\'}),\\n            \'page\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                     {\'to\': \\"orm[\'cms.Page\']\\"}),\\n            \'user\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                     {\'to\': \\"orm[\'%s\']\\" % user_orm_label, \'null\': \'True\'})\\n        },\\n        \'cms.pagepermission\': {\\n            \'Meta\': {\'object_name\': \'PagePermission\'},\\n            \'can_add\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_change\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_change_advanced_settings\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'can_change_permissions\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'can_delete\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_moderate\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_move_page\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_publish\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'can_view\': (\\n                \'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'grant_on\': (\\n                \'django.db.models.fields.IntegerField\', [], {\'default\': \'5\'}),\\n            \'group\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                      {\'to\': \\"orm[\'auth.Group\']\\", \'null\': \'True\', \'blank\': \'True\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'page\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                     {\'to\': \\"orm[\'cms.Page\']\\", \'null\': \'True\', \'blank\': \'True\'}),\\n            \'user\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                     {\'to\': \\"orm[\'%s\']\\" % user_orm_label, \'null\': \'True\', \'blank\': \'True\'})\\n        },\\n        \'cms.pageuser\': {\\n            \'Meta\': {\'object_name\': \'PageUser\', \'_ormbases\': [user_orm_label]},\\n            \'created_by\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                           {\'related_name\': \\"\'created_users\'\\",\\n                            \'to\': \\"orm[\'%s\']\\" % user_orm_label}),\\n            \'user_ptr\': (\'django.db.models.fields.related.OneToOneField\', [],\\n                         {\'to\': \\"orm[\'%s\']\\" % user_orm_label, \'unique\': \'True\',\\n                          \'primary_key\': \'True\'})\\n        },\\n        \'cms.pageusergroup\': {\\n            \'Meta\': {\'object_name\': \'PageUserGroup\', \'_ormbases\': [\'auth.Group\']},\\n            \'created_by\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                           {\'related_name\': \\"\'created_usergroups\'\\",\\n                            \'to\': \\"orm[\'%s\']\\" % user_orm_label}),\\n            \'group_ptr\': (\'django.db.models.fields.related.OneToOneField\', [],\\n                          {\'to\': \\"orm[\'auth.Group\']\\", \'unique\': \'True\',\\n                           \'primary_key\': \'True\'})\\n        },\\n        \'cms.placeholder\': {\\n            \'Meta\': {\'object_name\': \'Placeholder\'},\\n            \'default_width\': (\\n                \'django.db.models.fields.PositiveSmallIntegerField\', [],\\n                {\'null\': \'True\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'slot\': (\'django.db.models.fields.CharField\', [],\\n                     {\'max_length\': \'50\', \'db_index\': \'True\'})\\n        },\\n        \'cms.title\': {\\n            \'Meta\': {\'unique_together\': \\"((\'language\', \'page\'),)\\",\\n                     \'object_name\': \'Title\'},\\n            \'application_urls\': (\'django.db.models.fields.CharField\', [],\\n                                 {\'db_index\': \'True\', \'max_length\': \'200\',\\n                                  \'null\': \'True\', \'blank\': \'True\'}),\\n            \'creation_date\': (\'django.db.models.fields.DateTimeField\', [],\\n                              {\'default\': \'datetime.datetime.now\'}),\\n            \'has_url_overwrite\': (\'django.db.models.fields.BooleanField\', [],\\n                                  {\'default\': \'False\', \'db_index\': \'True\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'language\': (\'django.db.models.fields.CharField\', [],\\n                         {\'max_length\': \'15\', \'db_index\': \'True\'}),\\n            \'menu_title\': (\'django.db.models.fields.CharField\', [],\\n                           {\'max_length\': \'255\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'meta_description\': (\'django.db.models.fields.TextField\', [],\\n                                 {\'max_length\': \'255\', \'null\': \'True\',\\n                                  \'blank\': \'True\'}),\\n            \'meta_keywords\': (\'django.db.models.fields.CharField\', [],\\n                              {\'max_length\': \'255\', \'null\': \'True\',\\n                               \'blank\': \'True\'}),\\n            \'page\': (\'django.db.models.fields.related.ForeignKey\', [],\\n                     {\'related_name\': \\"\'title_set\'\\", \'to\': \\"orm[\'cms.Page\']\\"}),\\n            \'page_title\': (\'django.db.models.fields.CharField\', [],\\n                           {\'max_length\': \'255\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'path\': (\'django.db.models.fields.CharField\', [],\\n                     {\'max_length\': \'255\', \'db_index\': \'True\'}),\\n            \'redirect\': (\'django.db.models.fields.CharField\', [],\\n                         {\'max_length\': \'255\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'slug\': (\\n                \'django.db.models.fields.SlugField\', [], {\'max_length\': \'255\'}),\\n            \'title\': (\\n                \'django.db.models.fields.CharField\', [], {\'max_length\': \'255\'})\\n        },\\n        \'contenttypes.contenttype\': {\\n            \'Meta\': {\'ordering\': \\"(\'name\',)\\",\\n                     \'unique_together\': \\"((\'app_label\', \'model\'),)\\",\\n                     \'object_name\': \'ContentType\',\\n                     \'db_table\': \\"\'django_content_type\'\\"},\\n            \'app_label\': (\\n                \'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'model\': (\\n                \'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'})\\n        },\\n        \'sites.site\': {\\n            \'Meta\': {\'ordering\': \\"(\'domain\',)\\", \'object_name\': \'Site\',\\n                     \'db_table\': \\"\'django_site\'\\"},\\n            \'domain\': (\\n                \'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'id\': (\\n                \'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\'})\\n        }\\n    }\\n\\n    complete_apps = [\'cms\']\\n"}\n'
line: b'{"repo_name":"chouseknecht/ansible","ref":"refs/heads/devel","path":"lib/ansible/module_utils/network/eos/argspec/facts/facts.py","content":"# -*- coding: utf-8 -*-\\n# Copyright 2019 Red Hat\\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\\n\\"\\"\\"\\nThe arg spec for the eos facts module.\\n\\"\\"\\"\\n\\nfrom __future__ import (absolute_import, division, print_function)\\n__metaclass__ = type\\n\\n\\nclass FactsArgs(object):\\n    \\"\\"\\" The arg spec for the eos facts module\\n    \\"\\"\\"\\n\\n    def __init__(self, **kwargs):\\n        pass\\n\\n    argument_spec = {\\n        \'gather_subset\': dict(default=[\'!config\'], type=\'list\'),\\n        \'gather_network_resources\': dict(type=\'list\'),\\n    }\\n"}\n'
line: b'{"repo_name":"alokotosh/mm-master","ref":"refs/heads/master","path":"mm/commands/misc.py","content":"import os\\nimport json\\nimport mm.util as util\\nimport mm.config as config\\nfrom mm.exceptions import *\\nfrom mm.basecommand import Command\\nfrom mm.sfdc_client import MavensMateClient\\n\\nclass GetActiveSessionCommand(Command):\\n    def execute(self):\\n        if \'username\' not in self.params or self.params[\'username\'] == None or self.params[\'username\'] == \'\':\\n            raise MMException(\'Please enter a Salesforce.com username\')\\n        if \'password\' not in self.params or self.params[\'password\'] == None or self.params[\'password\'] == \'\':\\n            raise MMException(\'Please enter a Salesforce.com password\')\\n        if \'org_type\' not in self.params or self.params[\'org_type\'] == None or self.params[\'org_type\'] == \'\':\\n            raise MMException(\'Please select an org type\')\\n        if \'org_type\' in self.params and self.params[\'org_type\'] == \\"custom\\" and \\"org_url\\" not in self.params:\\n            raise MMException(\'To use a custom org type, please include a org_url parameter\') \\n        if \'org_type\' in self.params and self.params[\'org_type\'] == \\"custom\\" and \\"org_url\\" in self.params and self.params[\\"org_url\\"] == \\"\\":\\n            raise MMException(\'Please specify the org url\')    \\n\\n        config.logger.debug(\'=================\\u003e\')\\n        config.logger.debug(self.params)\\n\\n        client = MavensMateClient(credentials={\\n            \\"username\\" : self.params[\'username\'],\\n            \\"password\\" : self.params[\'password\'],\\n            \\"org_type\\" : self.params[\'org_type\'],\\n            \\"org_url\\"  : self.params.get(\'org_url\', None)\\n        }) \\n        \\n        response = {\\n            \\"sid\\"                   : client.sid,\\n            \\"user_id\\"               : client.user_id,\\n            \\"metadata_server_url\\"   : client.metadata_server_url,\\n            \\"server_url\\"            : client.server_url,\\n            \\"metadata\\"              : client.get_org_metadata(subscription=self.params.get(\'subscription\', None)),\\n            \\"org_metadata_types\\"    : util.metadata_types(),\\n            \\"success\\"               : True\\n        }\\n        return util.generate_response(response)\\n\\nclass IndexApexSymbolsCommand(Command):\\n    aliases=[\\"index_apex\\",\\"index_apex_file_properties\\"]\\n    \\"\\"\\"\\n        Updates symbol index for one or more Apex Classes. If files is not included or empty, will force a full refresh\\n    \\"\\"\\"\\n    def execute(self):\\n        return config.project.index_apex_symbols(self.params.get(\\"files\\", None))\\n\\nclass ResetMetadataContainerCommand(Command):\\n    def execute(self):\\n        return config.project.reset_metadata_container(accept=\\"json\\")\\n\\nclass OpenFileInClientCommand(Command):\\n    \\"\\"\\"\\n        Opens the requested files in the plugin client (Sublime Text, etc.)\\n    \\"\\"\\"\\n    def execute(self):\\n        file_name = self.params[\\"file_name\\"]\\n        extension = util.get_file_extension_no_period(file_name)\\n        mtype = util.get_meta_type_by_suffix(extension)\\n        full_file_path = os.path.join(config.project.location, \\"src\\", mtype[\\"directoryName\\"], file_name)\\n        params = {\\n            \\"project_name\\"  : config.project.project_name,\\n            \\"file_name\\"     : full_file_path,\\n            \\"line_number\\"   : self.params.get(\\"line_number\\", 0)\\n        } \\n        config.connection.run_subl_command(\\"open_file_in_project\\", json.dumps(params))\\n        return util.generate_success_response(\\"ok\\")\\n\\nclass ExecuteApexCommand(Command):\\n    aliases=[\\"run_apex_script\\"]\\n    \\"\\"\\"\\n        executes a string of apex\\n    \\"\\"\\"\\n    def execute(self):\\n        if \'script_name\' in self.params: #running an apex script\\n            self.params[\\"body\\"] = util.get_file_as_string(os.path.join(config.project.location,\\"apex-scripts\\",self.params[\\"script_name\\"]))\\n        if \'debug_categories\' not in self.params and not os.path.isfile(os.path.join(config.project.location,\\"config\\",\\".apex_script\\")):\\n            self.params[\\"debug_categories\\"] = [\\n                {\\n                    \\"category\\"  : \\"Apex_code\\",\\n                    \\"level\\"     :  \\"DEBUG\\"\\n                }\\n            ]\\n        elif os.path.isfile(os.path.join(config.project.location,\\"config\\",\\".apex_script\\")):\\n            log_settings = util.parse_json_from_file(os.path.join(config.project.location,\\"config\\",\\".apex_script\\"))\\n            categories = []\\n            levels = log_settings[\\"levels\\"]\\n            for category in levels.keys():\\n                categories.append({\\n                    \\"category\\"  : category,\\n                    \\"level\\"     : levels[category]\\n                })\\n            self.params[\\"debug_categories\\"] = categories\\n        elif \'debug_categories\' not in self.params:\\n            self.params[\\"debug_categories\\"] = [\\n                {\\n                    \\"category\\"  : \\"Apex_code\\",\\n                    \\"level\\"     :  \\"DEBUG\\"\\n                }\\n            ]\\n        return_log = self.params.get(\\"return_log\\", True)\\n\\n        execute_result = config.sfdc_client.execute_apex(self.params)\\n        result = {\\n            \'column\'                : execute_result[\'column\'],\\n            \'compileProblem\'        : execute_result[\'compileProblem\'],\\n            \'compiled\'              : execute_result[\'compiled\'],\\n            \'exceptionMessage\'      : execute_result[\'exceptionMessage\'],\\n            \'exceptionStackTrace\'   : execute_result[\'exceptionStackTrace\'],\\n            \'line\'                  : execute_result[\'line\'],\\n            \'success\'               : execute_result[\'success\'],\\n        }\\n        if \'log\' in execute_result and return_log:\\n            result[\'log\'] = execute_result[\'log\']\\n        if result[\'success\']:\\n            log_apex = config.connection.get_plugin_client_setting(\'mm_log_anonymous_apex\', False)\\n            if log_apex:\\n                location = config.project.log_anonymous_apex(self.params[\'body\'], execute_result[\'log\'], self.params.get(\\"script_name\\", None))\\n                result[\\"log_location\\"] = location\\n        return util.generate_response(result)\\n\\nclass SignInWithGithubCommand(Command):\\n    def execute(self):\\n        return config.connection.sign_in_with_github(self.params)"}\n'
line: b'{"repo_name":"nischalsheth/contrail-controller","ref":"refs/heads/master","path":"src/vnsw/provisioning/contrail_vrouter_provisioning/network.py","content":"#!/usr/bin/env python\\n#\\n# Copyright (c) 2013 Juniper Networks, Inc. All rights reserved.\\n#\\n\\nimport os\\nimport re\\nimport glob\\nimport struct\\nimport socket\\nimport logging\\nimport netifaces\\n\\nfrom contrail_vrouter_provisioning import local\\n\\n\\nlog = logging.getLogger(\'contrail_vrouter_provisioning.network\')\\n\\n\\nclass ComputeNetworkSetup(object):\\n    def find_gateway(self, dev):\\n        gateway = \'\'\\n        cmd = \\"sudo netstat -rn | sudo grep ^\\\\\\"0.0.0.0\\\\\\" | \\"\\n        cmd += \\"sudo head -n 1 | sudo grep %s | sudo awk \'{ print $2 }\'\\" % dev\\n        gateway = local(cmd, capture=True).strip()\\n        return gateway\\n    # end find_gateway\\n\\n    def get_dns_servers(self, dev):\\n        cmd = \\"sudo grep \\\\\\"^nameserver\\\\\\\\\\u003e\\\\\\" /etc/resolv.conf | \\"\\n        cmd += \\"sudo awk  \'{print $2}\'\\"\\n        dns_list = local(cmd, capture=True)\\n        return dns_list.split()\\n    # end get_dns_servers\\n\\n    def get_domain_search_list(self):\\n        domain_list = \'\'\\n        cmd = \\"sudo grep ^\\\\\\"search\\\\\\" /etc/resolv.conf | \\"\\n        cmd += \\"sudo awk \'{$1=\\\\\\"\\\\\\";print $0}\'\\"\\n        domain_list = local(cmd, capture=True).strip()\\n        if not domain_list:\\n            cmd = \\"sudo grep ^\\\\\\"domain\\\\\\" /etc/resolv.conf | \\"\\n            cmd += \\"sudo awk \'{$1=\\\\\\"\\\\\\"; print $0}\'\\"\\n            domain_list = local(cmd, capture=True).strip()\\n        return domain_list\\n\\n    def get_if_mtu(self, dev):\\n        cmd = \\"sudo ifconfig %s | sudo grep mtu | sudo awk \'{ print $NF }\'\\" %\\\\\\n               dev\\n        mtu = local(cmd, capture=True).strip()\\n        if not mtu:\\n            # for debian\\n            cmd = r\\"sudo ifconfig %s | sudo grep MTU | \\" % dev\\n            cmd += r\\"sudo sed \'s/.*MTU.\\\\([0-9]\\\\+\\\\).*/\\\\1/g\'\\"\\n            mtu = local(cmd, capture=True).strip()\\n        if (mtu and mtu != \'1500\'):\\n            return mtu\\n        return \'\'\\n    # end if_mtu\\n\\n    def get_device_by_ip(self, ip):\\n        for i in netifaces.interfaces():\\n            try:\\n                if i == \'pkt1\':\\n                    continue\\n                if netifaces.AF_INET in netifaces.ifaddresses(i):\\n                    interfaces = netifaces.ifaddresses(i)[netifaces.AF_INET]\\n                    for interface in interfaces:\\n                        if ip == interface[\'addr\']:\\n                            if i == \'vhost0\':\\n                                log.info(\\"vhost0 is already present!\\")\\n                            return i\\n            except ValueError:\\n                log.info(\\"Skipping interface %s\\", i)\\n        raise RuntimeError(\'%s not configured, rerun w/ --physical_interface\' %\\n                           ip)\\n    # end get_device_by_ip\\n\\n    def get_device_info(self, ip):\\n        reprov = False\\n        cfg_file = \\"/etc/contrail/contrail-vrouter-agent.conf\\"\\n        try:\\n            dev = self.get_device_by_ip(ip)\\n            if dev == \\"vhost0\\":\\n                dev = self.get_config(cfg_file,\\n                                      \\"VIRTUAL-HOST-INTERFACE\\",\\n                                      \\"physical_interface\\")\\n                log.info(\\"Re-provision. vhost0 present\\")\\n                reprov = True\\n            else:\\n                log.info(\\"Fresh Install. vhost0 not present\\")\\n        except RuntimeError:\\n            dev = self.get_config(cfg_file,\\n                                  \\"VIRTUAL-HOST-INTERFACE\\",\\n                                  \\"physical_interface\\")\\n            if not dev.succeeded:\\n                raise\\n            log.info(\\"vhost0 not present, vrouter not running\\")\\n            reprov = True\\n        return (dev.strip(), reprov)\\n    # end get_device_info\\n\\n    def get_secondary_device(self, primary):\\n        for i in netifaces.interfaces():\\n            try:\\n                if i == \'pkt1\':\\n                    continue\\n                if i == primary:\\n                    continue\\n                if i == \'vhost0\':\\n                    continue\\n                if netifaces.AF_INET not in netifaces.ifaddresses(i):\\n                    return i\\n            except ValueError:\\n                log.info(\\"Skipping interface %s\\" % i)\\n        raise RuntimeError(\'Secondary interace  not configured,\',\\n                           \'rerun w/ --physical_interface\')\\n    # end get_secondary_device\\n\\n    def get_if_mac(self, dev):\\n        iface_addr = netifaces.ifaddresses(dev)\\n        link_info = iface_addr[netifaces.AF_LINK]\\n        mac_addr = link_info[0][\'addr\']\\n        return mac_addr\\n    # end get_if_mac\\n\\n    @staticmethod\\n    def is_interface_vlan(interface):\\n        iface = local(\\"sudo ip link show %s | head -1\\" % interface +\\n                      \\"| cut -f2 -d\':\' | grep \'@\'\\",\\n                      capture=True, warn_only=True)\\n        if iface.succeeded:\\n            return True\\n        else:\\n            return False\\n\\n    @staticmethod\\n    def get_physical_interface_of_vlan(interface):\\n        iface = local(\\"sudo ip link show %s | head -1 | cut -f2 -d\':\'\\" %\\n                      interface + \\"| cut -f2 -d\'@\'\\", capture=True)\\n        return iface\\n\\n    def _rewrite_ifcfg_file(self, filename, dev, prsv_cfg):\\n        bond = False\\n        mac = \'\'\\n        temp_dir_name = self._temp_dir_name\\n\\n        vlan = False\\n        if os.path.isfile(\'/proc/net/vlan/%s\' % dev):\\n            vlan_info = open(\'/proc/net/vlan/config\').readlines()\\n            match = re.search(\'^%s.*\\\\|\\\\s+(\\\\S+)$\' % dev, \\"\\\\n\\".join(vlan_info),\\n                              flags=re.M | re.I)\\n            if not match:\\n                raise RuntimeError(\\"Configured vlan %s is not found in\\",\\n                                   \\"/proc/net/vlan/config\\" % dev)\\n            vlan = True\\n\\n        if os.path.isdir(\'/sys/class/net/%s/bonding\' % dev):\\n            bond = True\\n        # end if os.path.isdir...\\n\\n        mac = netifaces.ifaddresses(dev)[netifaces.AF_LINK][0][\'addr\']\\n        ifcfg_file = \'/etc/sysconfig/network-scripts/ifcfg-%s\' % dev\\n        if not os.path.isfile(ifcfg_file):\\n            ifcfg_file = temp_dir_name + \'ifcfg-\' + dev\\n            with open(ifcfg_file, \'w\') as f:\\n                f.write(\'\'\'#Contrail %s\\nTYPE=Ethernet\\nONBOOT=yes\\nDEVICE=\\"%s\\"\\nUSERCTL=yes\\nNM_CONTROLLED=no\\nHWADDR=%s\\n\'\'\' % (dev, dev, mac))\\n                for dcfg in prsv_cfg:\\n                    f.write(dcfg+\'\\\\n\')\\n                if vlan:\\n                    f.write(\'VLAN=yes\\\\n\')\\n        fd = open(ifcfg_file)\\n        f_lines = fd.readlines()\\n        fd.close()\\n        local(\\"sudo rm -f %s\\" % ifcfg_file)\\n        new_f_lines = []\\n        remove_items = [\'IPADDR\', \'NETMASK\', \'PREFIX\', \'GATEWAY\', \'HWADDR\',\\n                        \'DNS1\', \'DNS2\', \'BOOTPROTO\', \'NM_CONTROLLED\',\\n                        \'#Contrail\']\\n\\n        remove_items.append(\'DEVICE\')\\n        new_f_lines.append(\'#Contrail %s\\\\n\' % dev)\\n        new_f_lines.append(\'DEVICE=%s\\\\n\' % dev)\\n\\n        for line in f_lines:\\n            found = False\\n            for text in remove_items:\\n                if text in line:\\n                    found = True\\n            if not found:\\n                new_f_lines.append(line)\\n\\n        new_f_lines.append(\'NM_CONTROLLED=no\\\\n\')\\n        if bond:\\n            new_f_lines.append(\'SUBCHANNELS=1,2,3\\\\n\')\\n        elif not vlan:\\n            new_f_lines.append(\'HWADDR=%s\\\\n\' % mac)\\n\\n        fdw = open(filename, \'w\')\\n        fdw.writelines(new_f_lines)\\n        fdw.close()\\n\\n    def migrate_routes(self, device):\\n        \'\'\'\\n        add route entries in /proc/net/route\\n        \'\'\'\\n        temp_dir_name = self._temp_dir_name\\n        cfg_file = \'/etc/sysconfig/network-scripts/route-vhost0\'\\n        tmp_file = \'%s/route-vhost0\' % temp_dir_name\\n        with open(tmp_file, \'w\') as route_cfg_file:\\n            for route in open(\'/proc/net/route\', \'r\').readlines():\\n                if route.startswith(device):\\n                    route_fields = route.split()\\n                    destination = int(route_fields[1], 16)\\n                    gateway = int(route_fields[2], 16)\\n                    flags = int(route_fields[3], 16)\\n                    mask = int(route_fields[7], 16)\\n                    if flags \\u0026 0x2:\\n                        if destination != 0:\\n                            route_cfg_file.write(socket.inet_ntoa(\\n                                struct.pack(\'I\', destination)))\\n                            route_cfg_file.write(\\n                                    \'/\' + str(bin(mask).count(\'1\')) + \' \')\\n                            route_cfg_file.write(\'via \')\\n                            route_cfg_file.write(socket.inet_ntoa(\\n                                struct.pack(\'I\', gateway)) + \' \')\\n                            route_cfg_file.write(\'dev vhost0\')\\n                        # end if detination...\\n                    # end if flags \\u0026...\\n                # end if route.startswith...\\n            # end for route...\\n        # end with open...\\n        local(\\"sudo mv -f %s %s\\" % (tmp_file, cfg_file))\\n        # delete the route-dev file\\n        if os.path.isfile(\'/etc/sysconfig/network-scripts/route-%s\' % device):\\n            os.unlink(\'/etc/sysconfig/network-scripts/route-%s\' % device)\\n    # end def migrate_routes\\n\\n    def get_cfgfile_for_dev(self, iface, cfg_files):\\n        if not cfg_files:\\n            return None\\n        mapped_intf_cfgfile = None\\n        for file in cfg_files:\\n            with open(file, \'r\') as fd:\\n                contents = fd.read()\\n                regex = \'(?:^|\\\\n)\\\\s*iface\\\\s+%s\\\\s+\' % iface\\n                if re.search(regex, contents):\\n                    mapped_intf_cfgfile = file\\n        return mapped_intf_cfgfile\\n\\n    def get_sourced_files(self):\\n        \'\'\'Get all sourced config files\'\'\'\\n        files = self.get_valid_files(self.get_source_entries())\\n        files += self.get_source_directory_files()\\n        return list(set(files))\\n\\n    def get_source_directory_files(self):\\n        \'\'\'Get source-directory entry and make list of valid files\'\'\'\\n        regex = \'(?:^|\\\\n)\\\\s*source-directory\\\\s+(\\\\S+)\'\\n        files = list()\\n        with open(self.default_cfg_file, \'r\') as fd:\\n            entries = re.findall(regex, fd.read())\\n        dirs = [d for d in self.get_valid_files(entries) if os.path.isdir(d)]\\n        for dir in dirs:\\n            files.extend([os.path.join(dir, f) for f in os.listdir(dir)\\n                          if os.path.isfile(os.path.join(dir, f)) and\\n                          re.match(\'^[a-zA-Z0-9_-]+$\', f)])\\n        return files\\n\\n    def get_source_entries(self):\\n        \'\'\'\\n        Get entries matching source keyword from\\n        /etc/network/interfaces file.\\n        \'\'\'\\n        regex = \'(?:^|\\\\n)\\\\s*source\\\\s+(\\\\S+)\'\\n        with open(self.default_cfg_file, \'r\') as fd:\\n            return re.findall(regex, fd.read())\\n\\n    def get_valid_files(self, entries):\\n        \'\'\'Provided a list of glob\'d strings, return matching file names\'\'\'\\n        files = list()\\n        prepend = os.path.join(os.path.sep, \'etc\', \'network\') + os.path.sep\\n        for entry in entries:\\n            entry = entry.lstrip(\'./\') if entry.startswith(\'./\') else entry\\n            if entry.startswith(os.path.sep):\\n                entry = entry\\n            else:\\n                entry = prepend+entry\\n            files.extend(glob.glob(entry))\\n        return files\\n\\n    def _rewrite_net_interfaces_file(self, dev, mac, vhost_ip,\\n                                     netmask, gateway_ip, esxi_vm,\\n                                     vmpg_mtu, datapg_mtu):\\n        self.default_cfg_file = \'/etc/network/interfaces\'\\n        cfg_files = self.get_sourced_files()\\n        cfg_files.append(self.default_cfg_file)\\n        intf_cfgfile = self.get_cfgfile_for_dev(\'vhost0\', cfg_files)\\n        if intf_cfgfile:\\n            log.info(\\"Interface vhost0 is already present in\\" +\\n                     \\"/etc/network/interfaces\\")\\n            log.info(\\"Skipping rewrite of this file\\")\\n            return\\n        # endif\\n\\n        vlan = False\\n        if os.path.isfile(\'/proc/net/vlan/%s\' % dev):\\n            vlan_info = open(\'/proc/net/vlan/config\').readlines()\\n            match = re.search(\'^%s.*\\\\|\\\\s+(\\\\S+)$\' % dev, \\"\\\\n\\".join(vlan_info),\\n                              flags=re.M | re.I)\\n            if not match:\\n                raise RuntimeError(\'Configured vlan %s is not found in\',\\n                                   \'/proc/net/vlan/config\' % dev)\\n            phydev = match.group(1)\\n            vlan = True\\n\\n        # Replace strings matching dev to vhost0 in ifup and ifdown parts file\\n        # Any changes to the file/logic with static routes has to be\\n        # reflected in setup-vnc-static-routes.py too\\n        ifup_parts_file = os.path.join(os.path.sep, \'etc\', \'network\',\\n                                       \'if-up.d\', \'routes\')\\n        ifdown_parts_file = os.path.join(os.path.sep, \'etc\', \'network\',\\n                                         \'if-down.d\', \'routes\')\\n\\n        if (os.path.isfile(ifup_parts_file) and\\n                os.path.isfile(ifdown_parts_file)):\\n            local(\\"sudo sed -i \'s/%s/vhost0/g\' %s\\" % (dev, ifup_parts_file),\\n                  warn_only=True)\\n            local(\\"sudo sed -i \'s/%s/vhost0/g\' %s\\" % (dev, ifdown_parts_file),\\n                  warn_only=True)\\n\\n        dev_cfgfile = self.get_cfgfile_for_dev(dev, cfg_files)\\n        temp_intf_file = \'%s/interfaces\' % self._temp_dir_name\\n        local(\\"sudo cp %s %s\\" % (dev_cfgfile, temp_intf_file))\\n        with open(dev_cfgfile, \'r\') as fd:\\n            cfg_file = fd.read()\\n\\n        if not self._args.non_mgmt_ip:\\n            # remove entry from auto \\u003cdev\\u003e to auto excluding these pattern\\n            # then delete specifically auto \\u003cdev\\u003e\\n            local(\\"sudo sed -i \'/auto %s/,/auto/{/auto/!d}\' %s\\" %\\n                  (dev, temp_intf_file))\\n            local(\\"sudo sed -i \'/auto %s/d\' %s\\" % (dev, temp_intf_file))\\n            # add manual entry for dev\\n            local(\\"sudo echo \'auto %s\' \\u003e\\u003e %s\\" % (dev, temp_intf_file))\\n            local(\\"sudo echo \'iface %s inet manual\' \\u003e\\u003e %s\\" %\\n                  (dev, temp_intf_file))\\n            if vlan:\\n                local(\\"sudo echo \'    post-up ifconfig %s up\' \\u003e\\u003e %s\\" %\\n                      (dev, temp_intf_file))\\n                local(\\"sudo echo \'    pre-down ifconfig %s down\' \\u003e\\u003e %s\\" %\\n                      (dev, temp_intf_file))\\n            else:\\n                local(\\"sudo echo \'    pre-up ifconfig %s up\' \\u003e\\u003e %s\\" %\\n                      (dev, temp_intf_file))\\n                local(\\"sudo echo \'    post-down ifconfig %s down\' \\u003e\\u003e %s\\" %\\n                      (dev, temp_intf_file))\\n            if esxi_vm:\\n                    local(\\"sudo echo \'    pre-up ifconfig %s up mtu %s\' \\u003e\\u003e %s\\"\\n                          % (dev, datapg_mtu, temp_intf_file))\\n                    cmd = \\"sudo ethtool -i %s | grep driver | cut -f 2 -d \' \'\\"\\\\\\n                          % dev\\n                    device_driver = local(cmd, capture=True)\\n                    if (device_driver == \\"vmxnet3\\"):\\n                        cmd = \\"sudo echo \'    pre-up ethtool --offload \\"\\n                        rx_cmd = (cmd +\\n                                  \\"%s rx off\' \\u003e\\u003e %s\\" % (dev, temp_intf_file))\\n                        tx_cmd = (cmd +\\n                                  \\"%s tx off\' \\u003e\\u003e %s\\" % (dev, temp_intf_file))\\n                        local(rx_cmd)\\n                        local(tx_cmd)\\n            if vlan:\\n                local(\\"sudo echo \'    vlan-raw-device %s\' \\u003e\\u003e %s\\" %\\n                      (phydev, temp_intf_file))\\n            if \'bond\' in dev.lower():\\n                iters = re.finditer(\'^\\\\s*auto\\\\s\', cfg_file, re.M)\\n                indices = [pat_match.start() for pat_match in iters]\\n                matches = map(cfg_file.__getslice__, indices,\\n                              indices[1:] + [len(cfg_file)])\\n                for each in matches:\\n                    each = each.strip()\\n                    if re.match(\'^auto\\\\s+%s\' % dev, each):\\n                        string = \'\'\\n                        for lines in each.splitlines():\\n                            if \'bond-\' in lines:\\n                                string += lines+os.linesep\\n                        local(\\"sudo echo \'%s\' \\u003e\\u003e %s\\" % (string,\\n                                                        temp_intf_file))\\n                    else:\\n                        continue\\n            local(\\"sudo echo \'\' \\u003e\\u003e %s\\" % temp_intf_file)\\n        else:\\n            # remove ip address and gateway\\n            local(\\"sudo sed -i \'/iface %s inet static/, +2d\' %s\\" %\\n                  (dev, temp_intf_file), warn_only=True)\\n            if esxi_vm:\\n                local(\\"sudo echo \'    pre-up ifconfig %s up mtu %s\' \\u003e\\u003e %s\\" %\\n                      (dev, datapg_mtu, temp_intf_file), warn_only=True)\\n                cmd = \\"sudo ethtool -i %s | \\" % dev\\n                cmd += \\"sudo grep driver | sudo cut -f 2 -d \' \'\\"\\n                device_driver = local(cmd, capture=True, warn_only=True)\\n                if (device_driver == \\"vmxnet3\\"):\\n                    cmd = \\"sudo echo \'    pre-up ethtool --offload \\"\\n                    rx_cmd = cmd + \\"%s rx off\' \\u003e\\u003e %s\\" % (dev, temp_intf_file)\\n                    tx_cmd = cmd + \\"%s tx off\' \\u003e\\u003e %s\\" % (dev, temp_intf_file)\\n                    local(rx_cmd, warn_only=True)\\n                    local(tx_cmd, warn_only=True)\\n            if vlan:\\n                cmd = \\"sudo sed -i \'/auto %s/ a\\\\iface %s inet manual\\\\\\\\n    \\" %\\\\\\n                       (dev, dev)\\n                cmd += \\"post-up ifconfig %s up\\\\\\\\n    \\" % dev\\n                cmd += \\"pre-down ifconfig %s down\\\\\' %s\\" % (dev, temp_intf_file)\\n                local(cmd)\\n            else:\\n                cmd = \\"sudo sed -i \'/auto %s/ a\\\\iface %s inet manual\\\\\\\\n    \\" %\\\\\\n                       (dev, dev)\\n                cmd += \\"pre-up ifconfig %s up\\\\\\\\n    \\" % dev\\n                cmd += \\"post-down ifconfig %s down\\\\\' %s\\" %\\\\\\n                       (dev, temp_intf_file)\\n                local(cmd)\\n        if esxi_vm and vmpg_mtu:\\n            intf = self.get_secondary_device(self.dev)\\n            mac_addr = self.get_if_mac(intf)\\n            udev_net_file = \'/etc/udev/rules.d/70-persistent-net.rules\'\\n            temp_udev_net_file = \'%s/70-persistent-net.rules\' %\\\\\\n                                 (self._temp_dir_name)\\n            local(\\"sudo touch %s\\" % temp_udev_net_file)\\n            local(\\"sudo cp %s %s\\" % (udev_net_file, temp_udev_net_file))\\n            cmd = \\"sudo echo \'SUBSYSTEM==\\\\\\"net\\\\\\", ACTION==\\\\\\"add\\\\\\",\\"\\n            cmd += \\" DRIVERS==\\\\\\"?*\\\\\\",\\"\\n            cmd += \\" ATTR{address}==\\\\\\"%s\\\\\\", ATTR{dev_id}==\\\\\\"0x0\\\\\\", \\" % mac_addr\\n            cmd += \\"ATTR{type}==\\\\\\"1\\\\\\", KERNEL==\\\\\\"eth*\\\\\\", NAME=\\\\\\"%s\\\\\\"\' \\u003e\\u003e %s\\" %\\\\\\n                   (intf, temp_udev_net_file)\\n            local(cmd)\\n            local(\\"sudo mv -f %s %s\\" % (temp_udev_net_file, udev_net_file))\\n            local(\\"sudo sed -i \'/auto %s/,/down/d\' %s\\" % (intf,\\n                                                          temp_intf_file))\\n            local(\\"sudo echo \'\\\\nauto %s\' \\u003e\\u003e %s\\" % (intf, temp_intf_file))\\n            local(\\"sudo echo \'iface %s inet manual\' \\u003e\\u003e %s\\" % (intf,\\n                                                              temp_intf_file))\\n            local(\\"sudo echo \'    pre-up ifconfig %s up mtu %s\' \\u003e\\u003e %s\\" %\\n                  (intf, vmpg_mtu, temp_intf_file))\\n            local(\\"sudo echo \'    post-down ifconfig %s down\' \\u003e\\u003e %s\\" %\\n                  (intf, temp_intf_file))\\n            local(\\"sudo echo \'    pre-up ethtool --offload %s lro off\' \\u003e\\u003e %s\\" %\\n                  (intf, temp_intf_file))\\n\\n        # populte vhost0 as static\\n        local(\\"sudo echo \'\' \\u003e\\u003e %s\\" % (temp_intf_file))\\n        local(\\"sudo echo \'auto vhost0\' \\u003e\\u003e %s\\" % (temp_intf_file))\\n        local(\\"sudo echo \'iface vhost0 inet static\' \\u003e\\u003e %s\\" % (temp_intf_file))\\n        local(\\"sudo echo \'    pre-up %s/if-vhost0\' \\u003e\\u003e %s\\" %\\n              (self.contrail_bin_dir, temp_intf_file))\\n        local(\\"sudo echo \'    netmask %s\' \\u003e\\u003e %s\\" % (netmask, temp_intf_file))\\n        local(\\"sudo echo \'    network_name application\' \\u003e\\u003e %s\\" %\\n              temp_intf_file)\\n        if esxi_vm and datapg_mtu:\\n            local(\\"sudo echo \'    mtu %s\' \\u003e\\u003e %s\\" % (datapg_mtu,\\n                                                    temp_intf_file))\\n        if vhost_ip:\\n            local(\\"sudo echo \'    address %s\' \\u003e\\u003e %s\\" % (vhost_ip,\\n                                                        temp_intf_file))\\n        if (not self._args.non_mgmt_ip) and gateway_ip:\\n            local(\\"sudo echo \'    gateway %s\' \\u003e\\u003e %s\\" % (gateway_ip,\\n                                                        temp_intf_file))\\n\\n        domain = self.get_domain_search_list()\\n        if domain:\\n            local(\\"sudo echo \'    dns-search %s\' \\u003e\\u003e %s\\" % (domain,\\n                                                           temp_intf_file))\\n        dns_list = self.get_dns_servers(dev)\\n        if dns_list:\\n            local(\\"sudo echo -n \'    dns-nameservers\' \\u003e\\u003e %s\\" %\\n                  temp_intf_file)\\n            for dns in dns_list:\\n                local(\\"sudo echo -n \' %s\' \\u003e\\u003e %s\\" % (dns, temp_intf_file))\\n            local(\\"sudo echo \'\' \\u003e\\u003e %s\\" % temp_intf_file)\\n        local(\\"sudo echo \'    post-up ip link set vhost0 address %s\' \\u003e\\u003e %s\\" %\\n              (mac, temp_intf_file))\\n\\n        # move it to right place\\n        local(\\"sudo mv -f %s %s\\" % (temp_intf_file, dev_cfgfile))\\n"}\n'
line: b'{"repo_name":"mcrowson/django","ref":"refs/heads/master","path":"tests/choices/tests.py","content":"from django.test import TestCase\\n\\nfrom .models import Person\\n\\n\\nclass ChoicesTests(TestCase):\\n    def test_display(self):\\n        a = Person.objects.create(name=\'Adrian\', gender=\'M\')\\n        s = Person.objects.create(name=\'Sara\', gender=\'F\')\\n        self.assertEqual(a.gender, \'M\')\\n        self.assertEqual(s.gender, \'F\')\\n\\n        self.assertEqual(a.get_gender_display(), \'Male\')\\n        self.assertEqual(s.get_gender_display(), \'Female\')\\n\\n        # If the value for the field doesn\'t correspond to a valid choice,\\n        # the value itself is provided as a display value.\\n        a.gender = \'\'\\n        self.assertEqual(a.get_gender_display(), \'\')\\n\\n        a.gender = \'U\'\\n        self.assertEqual(a.get_gender_display(), \'U\')\\n"}\n'
line: b'{"repo_name":"RPi-Distro/python-gpiozero","ref":"refs/heads/master","path":"gpiozerocli/pinout.py","content":"# GPIO Zero: a library for controlling the Raspberry Pi\'s GPIO pins\\n# Copyright (c) 2017-2019 Dave Jones \\u003cdave@waveform.org.uk\\u003e\\n# Copyright (c) 2017 Ben Nuttall \\u003cben@bennuttall.com\\u003e\\n#\\n# Redistribution and use in source and binary forms, with or without\\n# modification, are permitted provided that the following conditions are met:\\n#\\n# * Redistributions of source code must retain the above copyright notice,\\n#   this list of conditions and the following disclaimer.\\n#\\n# * Redistributions in binary form must reproduce the above copyright notice,\\n#   this list of conditions and the following disclaimer in the documentation\\n#   and/or other materials provided with the distribution.\\n#\\n# * Neither the name of the copyright holder nor the names of its contributors\\n#   may be used to endorse or promote products derived from this software\\n#   without specific prior written permission.\\n#\\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \\"AS IS\\"\\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\\n# POSSIBILITY OF SUCH DAMAGE.\\n\\n\\"\\"\\"\\nA utility for querying Raspberry Pi GPIO pin-out information.\\n\\"\\"\\"\\n\\nfrom __future__ import (\\n    unicode_literals,\\n    absolute_import,\\n    print_function,\\n    division,\\n)\\n\\nimport argparse\\nimport sys\\nimport textwrap\\nimport warnings\\nimport webbrowser\\n\\nfrom gpiozero import pi_info\\n\\n\\nclass PinoutTool(object):\\n    def __init__(self):\\n        self.parser = argparse.ArgumentParser(\\n            description=__doc__\\n        )\\n        self.parser.add_argument(\\n            \'-r\', \'--revision\',\\n            dest=\'revision\',\\n            default=\'\',\\n            help=\'RPi revision. Default is to autodetect revision of current device\'\\n        )\\n        self.parser.add_argument(\\n            \'-c\', \'--color\',\\n            action=\\"store_true\\",\\n            default=None,\\n            help=\'Force colored output (by default, the output will include ANSI\'\\n            \'color codes if run in a color-capable terminal). See also --monochrome\'\\n        )\\n        self.parser.add_argument(\\n            \'-m\', \'--monochrome\',\\n            dest=\'color\',\\n            action=\'store_false\',\\n            help=\'Force monochrome output. See also --color\'\\n        )\\n        self.parser.add_argument(\\n            \'-x\', \'--xyz\',\\n            dest=\'xyz\',\\n            action=\'store_true\',\\n            help=\'Open pinout.xyz in the default web browser\'\\n        )\\n\\n    def __call__(self, args=None):\\n        if args is None:\\n            args = sys.argv[1:]\\n        try:\\n            return self.main(self.parser.parse_args(args)) or 0\\n        except argparse.ArgumentError as e:\\n            # argparse errors are already nicely formatted, print to stderr and\\n            # exit with code 2\\n            raise e\\n        except Exception as e:\\n            raise\\n            # Output anything else nicely formatted on stderr and exit code 1\\n            self.parser.exit(1, \'{prog}: error: {message}\\\\n\'.format(\\n                prog=self.parser.prog, message=e))\\n\\n    def main(self, args):\\n        warnings.simplefilter(\'ignore\')\\n        if args.xyz:\\n            webbrowser.open(\'https://pinout.xyz\')\\n        else:\\n            if args.revision == \'\':\\n                try:\\n                    pi_info().pprint(color=args.color)\\n                except ImportError:\\n                    formatter = self.parser._get_formatter()\\n                    formatter.add_text(\\n                        \\"Unable to initialize GPIO Zero. This usually means \\"\\n                        \\"that you are not running %(prog)s on a Raspberry Pi. \\"\\n                        \\"If you still wish to run %(prog)s, set the \\"\\n                        \\"GPIOZERO_PIN_FACTORY environment variable to \'mock\' \\"\\n                        \\"and retry, or refer to the Remote GPIO section of \\"\\n                        \\"the manual* to configure your environment to \\"\\n                        \\"remotely access your Pi.\\"\\n                    )\\n                    formatter.add_text(\\n                        \\"* https://gpiozero.readthedocs.io/en/stable/\\"\\n                        \\"remote_gpio.html\\"\\n                    )\\n                    sys.stderr.write(formatter.format_help())\\n                except IOError:\\n                    raise IOError(\'This device is not a Raspberry Pi\')\\n            else:\\n                pi_info(args.revision).pprint(color=args.color)\\n            formatter = self.parser._get_formatter()\\n            formatter.add_text(\\n                \\"For further information, please refer to \\"\\n                \\"https://pinout.xyz/\\"\\n            )\\n            sys.stdout.write(\'\\\\n\')\\n            sys.stdout.write(formatter.format_help())\\n\\n\\nmain = PinoutTool()\\n"}\n'
line: b'{"repo_name":"lordmos/blink","ref":"refs/heads/master","path":"Tools/TestResultServer/model/jsonresults_unittest.py","content":"# Copyright (C) 2010 Google Inc. All rights reserved.\\n#\\n# Redistribution and use in source and binary forms, with or without\\n# modification, are permitted provided that the following conditions are\\n# met:\\n#\\n#     * Redistributions of source code must retain the above copyright\\n# notice, this list of conditions and the following disclaimer.\\n#     * Redistributions in binary form must reproduce the above\\n# copyright notice, this list of conditions and the following disclaimer\\n# in the documentation and/or other materials provided with the\\n# distribution.\\n#     * Neither the name of Google Inc. nor the names of its\\n# contributors may be used to endorse or promote products derived from\\n# this software without specific prior written permission.\\n#\\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\\n# \\"AS IS\\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n\\ntry:\\n    import jsonresults\\n    from jsonresults import *\\nexcept ImportError:\\n    print \\"ERROR: Add the TestResultServer, google_appengine and yaml/lib directories to your PYTHONPATH\\"\\n    raise\\n\\nimport json\\nimport logging\\nimport unittest\\n\\nFULL_RESULT_EXAMPLE = \\"\\"\\"ADD_RESULTS({\\n    \\"seconds_since_epoch\\": 1368146629,\\n    \\"tests\\": {\\n        \\"media\\": {\\n            \\"encrypted-media\\": {\\n                \\"encrypted-media-v2-events.html\\": {\\n                    \\"bugs\\": [\\"crbug.com/1234\\"],\\n                    \\"expected\\": \\"TIMEOUT\\",\\n                    \\"actual\\": \\"TIMEOUT\\",\\n                    \\"time\\": 6.0\\n                },\\n                \\"encrypted-media-v2-syntax.html\\": {\\n                    \\"expected\\": \\"TIMEOUT\\",\\n                    \\"actual\\": \\"TIMEOUT\\"\\n                }\\n            },\\n            \\"progress-events-generated-correctly.html\\": {\\n                \\"expected\\": \\"PASS FAIL IMAGE TIMEOUT CRASH MISSING\\",\\n                \\"actual\\": \\"TIMEOUT\\",\\n                \\"time\\": 6.0\\n            },\\n            \\"W3C\\": {\\n                \\"audio\\": {\\n                    \\"src\\": {\\n                        \\"src_removal_does_not_trigger_loadstart.html\\": {\\n                            \\"expected\\": \\"PASS\\",\\n                            \\"actual\\": \\"PASS\\",\\n                            \\"time\\": 3.5\\n                        }\\n                    }\\n                },\\n                \\"video\\": {\\n                    \\"src\\": {\\n                        \\"src_removal_does_not_trigger_loadstart.html\\": {\\n                            \\"expected\\": \\"PASS\\",\\n                            \\"actual\\": \\"PASS\\",\\n                            \\"time\\": 1.1\\n                        },\\n                        \\"notrun.html\\": {\\n                            \\"expected\\": \\"NOTRUN\\",\\n                            \\"actual\\": \\"SKIP\\",\\n                            \\"time\\": 1.1\\n                        }\\n                    }\\n                }\\n            },\\n            \\"unexpected-skip.html\\": {\\n                \\"expected\\": \\"PASS\\",\\n                \\"actual\\": \\"SKIP\\"\\n            },\\n            \\"unexpected-fail.html\\": {\\n                \\"expected\\": \\"PASS\\",\\n                \\"actual\\": \\"FAIL\\"\\n            },\\n            \\"flaky-failed.html\\": {\\n                \\"expected\\": \\"PASS FAIL\\",\\n                \\"actual\\": \\"FAIL\\"\\n            },\\n            \\"media-document-audio-repaint.html\\": {\\n                \\"expected\\": \\"IMAGE\\",\\n                \\"actual\\": \\"IMAGE\\",\\n                \\"time\\": 0.1\\n            }\\n        }\\n    },\\n    \\"skipped\\": 2,\\n    \\"num_regressions\\": 0,\\n    \\"build_number\\": \\"3\\",\\n    \\"interrupted\\": false,\\n    \\"layout_tests_dir\\": \\"\\\\/tmp\\\\/cr\\\\/src\\\\/third_party\\\\/WebKit\\\\/LayoutTests\\",\\n    \\"version\\": 3,\\n    \\"builder_name\\": \\"Webkit\\",\\n    \\"num_passes\\": 10,\\n    \\"pixel_tests_enabled\\": true,\\n    \\"blink_revision\\": \\"1234\\",\\n    \\"has_pretty_patch\\": true,\\n    \\"fixable\\": 25,\\n    \\"num_flaky\\": 0,\\n    \\"num_failures_by_type\\": {\\n        \\"CRASH\\": 3,\\n        \\"MISSING\\": 0,\\n        \\"TEXT\\": 3,\\n        \\"IMAGE\\": 1,\\n        \\"PASS\\": 10,\\n        \\"SKIP\\": 2,\\n        \\"TIMEOUT\\": 16,\\n        \\"IMAGE+TEXT\\": 0,\\n        \\"FAIL\\": 2,\\n        \\"AUDIO\\": 0\\n    },\\n    \\"has_wdiff\\": true,\\n    \\"chromium_revision\\": \\"5678\\"\\n});\\"\\"\\"\\n\\nJSON_RESULTS_OLD_TEMPLATE = (\\n    \'{\\"[BUILDER_NAME]\\":{\'\\n    \'\\"allFixableCount\\":[[TESTDATA_COUNT]],\'\\n    \'\\"blinkRevision\\":[[TESTDATA_WEBKITREVISION]],\'\\n    \'\\"buildNumbers\\":[[TESTDATA_BUILDNUMBERS]],\'\\n    \'\\"chromeRevision\\":[[TESTDATA_CHROMEREVISION]],\'\\n    \'\\"failure_map\\": %s,\'\\n    \'\\"fixableCount\\":[[TESTDATA_COUNT]],\'\\n    \'\\"fixableCounts\\":[[TESTDATA_COUNTS]],\'\\n    \'\\"secondsSinceEpoch\\":[[TESTDATA_TIMES]],\'\\n    \'\\"tests\\":{[TESTDATA_TESTS]}\'\\n    \'},\'\\n    \'\\"version\\":[VERSION]\'\\n    \'}\') % json.dumps(CHAR_TO_FAILURE)\\n\\nJSON_RESULTS_COUNTS = \'{\\"\' + \'\\":[[TESTDATA_COUNT]],\\"\'.join([char for char in CHAR_TO_FAILURE.values()]) + \'\\":[[TESTDATA_COUNT]]}\'\\n\\nJSON_RESULTS_TEMPLATE = (\\n    \'{\\"[BUILDER_NAME]\\":{\'\\n    \'\\"blinkRevision\\":[[TESTDATA_WEBKITREVISION]],\'\\n    \'\\"buildNumbers\\":[[TESTDATA_BUILDNUMBERS]],\'\\n    \'\\"chromeRevision\\":[[TESTDATA_CHROMEREVISION]],\'\\n    \'\\"failure_map\\": %s,\'\\n    \'\\"num_failures_by_type\\":%s,\'\\n    \'\\"secondsSinceEpoch\\":[[TESTDATA_TIMES]],\'\\n    \'\\"tests\\":{[TESTDATA_TESTS]}\'\\n    \'},\'\\n    \'\\"version\\":[VERSION]\'\\n    \'}\') % (json.dumps(CHAR_TO_FAILURE), JSON_RESULTS_COUNTS)\\n\\nJSON_RESULTS_COUNTS_TEMPLATE = \'{\\"\' + \'\\":[TESTDATA],\\"\'.join([char for char in CHAR_TO_FAILURE]) + \'\\":[TESTDATA]}\'\\n\\nJSON_RESULTS_TEST_LIST_TEMPLATE = \'{\\"Webkit\\":{\\"tests\\":{[TESTDATA_TESTS]}}}\'\\n\\n\\nclass MockFile(object):\\n    def __init__(self, name=\'results.json\', data=\'\'):\\n        self.master = \'MockMasterName\'\\n        self.builder = \'MockBuilderName\'\\n        self.test_type = \'MockTestType\'\\n        self.name = name\\n        self.data = data\\n\\n    def save(self, data):\\n        self.data = data\\n        return True\\n\\n\\nclass JsonResultsTest(unittest.TestCase):\\n    def setUp(self):\\n        self._builder = \\"Webkit\\"\\n        self.old_log_level = logging.root.level\\n        logging.root.setLevel(logging.ERROR)\\n\\n    def tearDown(self):\\n        logging.root.setLevel(self.old_log_level)\\n\\n    # Use this to get better error messages than just string compare gives.\\n    def assert_json_equal(self, a, b):\\n        self.maxDiff = None\\n        a = json.loads(a) if isinstance(a, str) else a\\n        b = json.loads(b) if isinstance(b, str) else b\\n        self.assertEqual(a, b)\\n\\n    def test_strip_prefix_suffix(self):\\n        json = \\"[\'contents\']\\"\\n        self.assertEqual(JsonResults._strip_prefix_suffix(\\"ADD_RESULTS(\\" + json + \\");\\"), json)\\n        self.assertEqual(JsonResults._strip_prefix_suffix(json), json)\\n\\n    def _make_test_json(self, test_data, json_string=JSON_RESULTS_TEMPLATE, builder_name=\\"Webkit\\"):\\n        if not test_data:\\n            return \\"\\"\\n\\n        builds = test_data[\\"builds\\"]\\n        tests = test_data[\\"tests\\"]\\n        if not builds or not tests:\\n            return \\"\\"\\n\\n        counts = []\\n        build_numbers = []\\n        webkit_revision = []\\n        chrome_revision = []\\n        times = []\\n        for build in builds:\\n            counts.append(JSON_RESULTS_COUNTS_TEMPLATE.replace(\\"[TESTDATA]\\", build))\\n            build_numbers.append(\\"1000%s\\" % build)\\n            webkit_revision.append(\\"2000%s\\" % build)\\n            chrome_revision.append(\\"3000%s\\" % build)\\n            times.append(\\"100000%s000\\" % build)\\n\\n        json_string = json_string.replace(\\"[BUILDER_NAME]\\", builder_name)\\n        json_string = json_string.replace(\\"[TESTDATA_COUNTS]\\", \\",\\".join(counts))\\n        json_string = json_string.replace(\\"[TESTDATA_COUNT]\\", \\",\\".join(builds))\\n        json_string = json_string.replace(\\"[TESTDATA_BUILDNUMBERS]\\", \\",\\".join(build_numbers))\\n        json_string = json_string.replace(\\"[TESTDATA_WEBKITREVISION]\\", \\",\\".join(webkit_revision))\\n        json_string = json_string.replace(\\"[TESTDATA_CHROMEREVISION]\\", \\",\\".join(chrome_revision))\\n        json_string = json_string.replace(\\"[TESTDATA_TIMES]\\", \\",\\".join(times))\\n\\n        version = str(test_data[\\"version\\"]) if \\"version\\" in test_data else \\"4\\"\\n        json_string = json_string.replace(\\"[VERSION]\\", version)\\n        json_string = json_string.replace(\\"{[TESTDATA_TESTS]}\\", json.dumps(tests, separators=(\',\', \':\'), sort_keys=True))\\n        return json_string\\n\\n    def _test_merge(self, aggregated_data, incremental_data, expected_data, max_builds=jsonresults.JSON_RESULTS_MAX_BUILDS):\\n        aggregated_results = self._make_test_json(aggregated_data, builder_name=self._builder)\\n        incremental_json, _ = JsonResults._get_incremental_json(self._builder, self._make_test_json(incremental_data, builder_name=self._builder), is_full_results_format=False)\\n        merged_results, status_code = JsonResults.merge(self._builder, aggregated_results, incremental_json, num_runs=max_builds, sort_keys=True)\\n\\n        if expected_data:\\n            expected_results = self._make_test_json(expected_data, builder_name=self._builder)\\n            self.assert_json_equal(merged_results, expected_results)\\n            self.assertEqual(status_code, 200)\\n        else:\\n            self.assertTrue(status_code != 200)\\n\\n    def _test_get_test_list(self, input_data, expected_data):\\n        input_results = self._make_test_json(input_data)\\n        expected_results = JSON_RESULTS_TEST_LIST_TEMPLATE.replace(\\"{[TESTDATA_TESTS]}\\", json.dumps(expected_data, separators=(\',\', \':\')))\\n        actual_results = JsonResults.get_test_list(self._builder, input_results)\\n        self.assert_json_equal(actual_results, expected_results)\\n\\n    def test_update_files_empty_aggregate_data(self):\\n        small_file = MockFile(name=\'results-small.json\')\\n        large_file = MockFile(name=\'results.json\')\\n\\n        incremental_data = {\\n            \\"builds\\": [\\"2\\", \\"1\\"],\\n            \\"tests\\": {\\n                \\"001.html\\": {\\n                    \\"results\\": [[200, TEXT]],\\n                    \\"times\\": [[200, 0]],\\n                }\\n            }\\n        }\\n        incremental_string = self._make_test_json(incremental_data, builder_name=small_file.builder)\\n\\n        self.assertTrue(JsonResults.update_files(small_file.builder, incremental_string, small_file, large_file, is_full_results_format=False))\\n        self.assert_json_equal(small_file.data, incremental_string)\\n        self.assert_json_equal(large_file.data, incremental_string)\\n\\n    def test_update_files_null_incremental_data(self):\\n        small_file = MockFile(name=\'results-small.json\')\\n        large_file = MockFile(name=\'results.json\')\\n\\n        aggregated_data = {\\n            \\"builds\\": [\\"2\\", \\"1\\"],\\n            \\"tests\\": {\\n                \\"001.html\\": {\\n                    \\"results\\": [[200, TEXT]],\\n                    \\"times\\": [[200, 0]],\\n                }\\n            }\\n        }\\n        aggregated_string = self._make_test_json(aggregated_data, builder_name=small_file.builder)\\n\\n        small_file.data = large_file.data = aggregated_string\\n\\n        incremental_string = \\"\\"\\n\\n        self.assertEqual(JsonResults.update_files(small_file.builder, incremental_string, small_file, large_file, is_full_results_format=False),\\n            (\'No incremental JSON data to merge.\', 403))\\n        self.assert_json_equal(small_file.data, aggregated_string)\\n        self.assert_json_equal(large_file.data, aggregated_string)\\n\\n    def test_update_files_empty_incremental_data(self):\\n        small_file = MockFile(name=\'results-small.json\')\\n        large_file = MockFile(name=\'results.json\')\\n\\n        aggregated_data = {\\n            \\"builds\\": [\\"2\\", \\"1\\"],\\n            \\"tests\\": {\\n                \\"001.html\\": {\\n                    \\"results\\": [[200, TEXT]],\\n                    \\"times\\": [[200, 0]],\\n                }\\n            }\\n        }\\n        aggregated_string = self._make_test_json(aggregated_data, builder_name=small_file.builder)\\n\\n        small_file.data = large_file.data = aggregated_string\\n\\n        incremental_data = {\\n            \\"builds\\": [],\\n            \\"tests\\": {}\\n        }\\n        incremental_string = self._make_test_json(incremental_data, builder_name=small_file.builder)\\n\\n        self.assertEqual(JsonResults.update_files(small_file.builder, incremental_string, small_file, large_file, is_full_results_format=False),\\n            (\'No incremental JSON data to merge.\', 403))\\n        self.assert_json_equal(small_file.data, aggregated_string)\\n        self.assert_json_equal(large_file.data, aggregated_string)\\n\\n    def test_merge_with_empty_aggregated_results(self):\\n        incremental_data = {\\n            \\"builds\\": [\\"2\\", \\"1\\"],\\n            \\"tests\\": {\\n                \\"001.html\\": {\\n                    \\"results\\": [[200, TEXT]],\\n                    \\"times\\": [[200, 0]],\\n                }\\n            }\\n        }\\n        incremental_results, _ = JsonResults._get_incremental_json(self._builder, self._make_test_json(incremental_data), is_full_results_format=False)\\n        aggregated_results = \\"\\"\\n        merged_results, _ = JsonResults.merge(self._builder, aggregated_results, incremental_results, num_runs=jsonresults.JSON_RESULTS_MAX_BUILDS, sort_keys=True)\\n        self.assert_json_equal(merged_results, incremental_results)\\n\\n    def test_failures_by_type_added(self):\\n        aggregated_results = self._make_test_json({\\n            \\"builds\\": [\\"2\\", \\"1\\"],\\n            \\"tests\\": {\\n                \\"001.html\\": {\\n                    \\"results\\": [[100, TEXT], [100, FAIL]],\\n                    \\"times\\": [[200, 0]],\\n                }\\n            }\\n        }, json_string=JSON_RESULTS_OLD_TEMPLATE)\\n        incremental_results = self._make_test_json({\\n            \\"builds\\": [\\"3\\"],\\n            \\"tests\\": {\\n                \\"001.html\\": {\\n                    \\"results\\": [[1, TEXT]],\\n                    \\"times\\": [[1, 0]],\\n                }\\n            }\\n        }, json_string=JSON_RESULTS_OLD_TEMPLATE)\\n        incremental_json, _ = JsonResults._get_incremental_json(self._builder, incremental_results, is_full_results_format=False)\\n        merged_results, _ = JsonResults.merge(self._builder, aggregated_results, incremental_json, num_runs=201, sort_keys=True)\\n        self.assert_json_equal(merged_results, self._make_test_json({\\n            \\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n            \\"tests\\": {\\n                \\"001.html\\": {\\n                    \\"results\\": [[101, TEXT], [100, FAIL]],\\n                    \\"times\\": [[201, 0]],\\n                }\\n            }\\n        }))\\n\\n    def test_merge_full_results_format(self):\\n        expected_incremental_results = {\\n            \\"Webkit\\": {\\n                \\"blinkRevision\\": [\\"1234\\"],\\n                \\"buildNumbers\\": [\\"3\\"],\\n                \\"chromeRevision\\": [\\"5678\\"],\\n                \\"failure_map\\": CHAR_TO_FAILURE,\\n                \\"num_failures_by_type\\": {\\"AUDIO\\": [0], \\"CRASH\\": [3], \\"FAIL\\": [2], \\"IMAGE\\": [1], \\"IMAGE+TEXT\\": [0], \\"MISSING\\": [0], \\"PASS\\": [10], \\"SKIP\\": [2], \\"TEXT\\": [3], \\"TIMEOUT\\": [16]},\\n                \\"secondsSinceEpoch\\": [1368146629],\\n                \\"tests\\": {\\n                    \\"media\\": {\\n                        \\"W3C\\": {\\n                            \\"audio\\": {\\n                                \\"src\\": {\\n                                    \\"src_removal_does_not_trigger_loadstart.html\\": {\\n                                        \\"results\\": [[1, PASS]],\\n                                        \\"times\\": [[1, 4]],\\n                                    }\\n                                }\\n                            }\\n                        },\\n                        \\"encrypted-media\\": {\\n                            \\"encrypted-media-v2-events.html\\": {\\n                                \\"bugs\\": [\\"crbug.com/1234\\"],\\n                                \\"expected\\": \\"TIMEOUT\\",\\n                                \\"results\\": [[1, TIMEOUT]],\\n                                \\"times\\": [[1, 6]],\\n                            },\\n                            \\"encrypted-media-v2-syntax.html\\": {\\n                                \\"expected\\": \\"TIMEOUT\\",\\n                                \\"results\\": [[1, TIMEOUT]],\\n                                \\"times\\": [[1, 0]],\\n                            }\\n                        },\\n                        \\"media-document-audio-repaint.html\\": {\\n                            \\"expected\\": \\"IMAGE\\",\\n                            \\"results\\": [[1, IMAGE]],\\n                            \\"times\\": [[1, 0]],\\n                        },\\n                        \\"progress-events-generated-correctly.html\\": {\\n                            \\"expected\\": \\"PASS FAIL IMAGE TIMEOUT CRASH MISSING\\",\\n                            \\"results\\": [[1, TIMEOUT]],\\n                            \\"times\\": [[1, 6]],\\n                        },\\n                        \\"flaky-failed.html\\": {\\n                            \\"expected\\": \\"PASS FAIL\\",\\n                            \\"results\\": [[1, FAIL]],\\n                            \\"times\\": [[1, 0]],\\n                        },\\n                        \\"unexpected-fail.html\\": {\\n                            \\"results\\": [[1, FAIL]],\\n                            \\"times\\": [[1, 0]],\\n                        },\\n                    }\\n                }\\n            },\\n            \\"version\\": 4\\n        }\\n\\n        aggregated_results = \\"\\"\\n        incremental_json, _ = JsonResults._get_incremental_json(self._builder, FULL_RESULT_EXAMPLE, is_full_results_format=True)\\n        merged_results, _ = JsonResults.merge(\\"Webkit\\", aggregated_results, incremental_json, num_runs=jsonresults.JSON_RESULTS_MAX_BUILDS, sort_keys=True)\\n        self.assert_json_equal(merged_results, expected_incremental_results)\\n\\n    def test_merge_empty_aggregated_results(self):\\n        # No existing aggregated results.\\n        # Merged results == new incremental results.\\n        self._test_merge(\\n            # Aggregated results\\n            None,\\n            # Incremental results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT]],\\n                           \\"times\\": [[200, 0]]}}},\\n            # Expected result\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT]],\\n                           \\"times\\": [[200, 0]]}}})\\n\\n    def test_merge_duplicate_build_number(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[100, TEXT]],\\n                           \\"times\\": [[100, 0]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"2\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TEXT]],\\n                           \\"times\\": [[1, 0]]}}},\\n            # Expected results\\n            None)\\n\\n    def test_merge_incremental_single_test_single_run_same_result(self):\\n        # Incremental results has the latest build and same test results for\\n        # that run.\\n        # Insert the incremental results at the first place and sum number\\n        # of runs for TEXT (200 + 1) to get merged results.\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT]],\\n                           \\"times\\": [[200, 0]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TEXT]],\\n                           \\"times\\": [[1, 0]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[201, TEXT]],\\n                           \\"times\\": [[201, 0]]}}})\\n\\n    def test_merge_single_test_single_run_different_result(self):\\n        # Incremental results has the latest build but different test results\\n        # for that run.\\n        # Insert the incremental results at the first place.\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT]],\\n                           \\"times\\": [[200, 0]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, IMAGE]],\\n                           \\"times\\": [[1, 1]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, IMAGE], [200, TEXT]],\\n                           \\"times\\": [[1, 1], [200, 0]]}}})\\n\\n    def test_merge_single_test_single_run_result_changed(self):\\n        # Incremental results has the latest build but results which differ from\\n        # the latest result (but are the same as an older result).\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT], [10, IMAGE]],\\n                           \\"times\\": [[200, 0], [10, 1]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, IMAGE]],\\n                           \\"times\\": [[1, 1]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, IMAGE], [200, TEXT], [10, IMAGE]],\\n                           \\"times\\": [[1, 1], [200, 0], [10, 1]]}}})\\n\\n    def test_merge_multiple_tests_single_run(self):\\n        # All tests have incremental updates.\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT]],\\n                           \\"times\\": [[200, 0]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[100, IMAGE]],\\n                           \\"times\\": [[100, 1]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TEXT]],\\n                           \\"times\\": [[1, 0]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[1, IMAGE]],\\n                           \\"times\\": [[1, 1]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[201, TEXT]],\\n                           \\"times\\": [[201, 0]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[101, IMAGE]],\\n                           \\"times\\": [[101, 1]]}}})\\n\\n    def test_merge_multiple_tests_single_run_one_no_result(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT]],\\n                           \\"times\\": [[200, 0]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[100, IMAGE]],\\n                           \\"times\\": [[100, 1]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"002.html\\": {\\n                           \\"results\\": [[1, IMAGE]],\\n                           \\"times\\": [[1, 1]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, NO_DATA], [200, TEXT]],\\n                           \\"times\\": [[201, 0]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[101, IMAGE]],\\n                           \\"times\\": [[101, 1]]}}})\\n\\n    def test_merge_single_test_multiple_runs(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT]],\\n                           \\"times\\": [[200, 0]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"4\\", \\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[2, IMAGE], [1, FAIL]],\\n                           \\"times\\": [[3, 2]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"4\\", \\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, FAIL], [2, IMAGE], [200, TEXT]],\\n                           \\"times\\": [[3, 2], [200, 0]]}}})\\n\\n    def test_merge_multiple_tests_multiple_runs(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT]],\\n                           \\"times\\": [[200, 0]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[10, IMAGE_PLUS_TEXT]],\\n                           \\"times\\": [[10, 0]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"4\\", \\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[2, IMAGE]],\\n                           \\"times\\": [[2, 2]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[1, CRASH]],\\n                           \\"times\\": [[1, 1]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"4\\", \\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[2, IMAGE], [200, TEXT]],\\n                           \\"times\\": [[2, 2], [200, 0]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[1, CRASH], [10, IMAGE_PLUS_TEXT]],\\n                           \\"times\\": [[1, 1], [10, 0]]}}})\\n\\n    def test_merge_incremental_result_older_build(self):\\n        # Test the build in incremental results is older than the most recent\\n        # build in aggregated results.\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"3\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[5, TEXT]],\\n                           \\"times\\": [[5, 0]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"2\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TEXT]],\\n                           \\"times\\": [[1, 0]]}}},\\n            # Expected no merge happens.\\n            {\\"builds\\": [\\"2\\", \\"3\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[6, TEXT]],\\n                           \\"times\\": [[6, 0]]}}})\\n\\n    def test_merge_incremental_result_same_build(self):\\n        # Test the build in incremental results is same as the build in\\n        # aggregated results.\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[5, TEXT]],\\n                           \\"times\\": [[5, 0]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\", \\"2\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[2, TEXT]],\\n                           \\"times\\": [[2, 0]]}}},\\n            # Expected no merge happens.\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[7, TEXT]],\\n                           \\"times\\": [[7, 0]]}}})\\n\\n    def test_merge_remove_new_test(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[199, TEXT]],\\n                           \\"times\\": [[199, 0]]},\\n                       }},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TEXT]],\\n                           \\"times\\": [[1, 0]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[1, PASS]],\\n                           \\"times\\": [[1, 0]]},\\n                       \\"notrun.html\\": {\\n                           \\"results\\": [[1, NOTRUN]],\\n                           \\"times\\": [[1, 0]]},\\n                       \\"003.html\\": {\\n                           \\"results\\": [[1, NO_DATA]],\\n                           \\"times\\": [[1, 0]]},\\n                        }},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, TEXT]],\\n                           \\"times\\": [[200, 0]]},\\n                       }},\\n            max_builds=200)\\n\\n    def test_merge_remove_test(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\n                \\"builds\\": [\\"2\\", \\"1\\"],\\n                \\"tests\\": {\\n                    \\"directory\\": {\\n                        \\"directory\\": {\\n                            \\"001.html\\": {\\n                                \\"results\\": [[200, PASS]],\\n                                \\"times\\": [[200, 0]]\\n                            }\\n                        }\\n                    },\\n                    \\"002.html\\": {\\n                        \\"results\\": [[10, TEXT]],\\n                        \\"times\\": [[10, 0]]\\n                    },\\n                    \\"003.html\\": {\\n                        \\"results\\": [[190, PASS], [9, NO_DATA], [1, TEXT]],\\n                        \\"times\\": [[200, 0]]\\n                    },\\n                }\\n            },\\n            # Incremental results\\n            {\\n                \\"builds\\": [\\"3\\"],\\n                \\"tests\\": {\\n                    \\"directory\\": {\\n                        \\"directory\\": {\\n                            \\"001.html\\": {\\n                                \\"results\\": [[1, PASS]],\\n                                \\"times\\": [[1, 0]]\\n                            }\\n                        }\\n                    },\\n                    \\"002.html\\": {\\n                        \\"results\\": [[1, PASS]],\\n                        \\"times\\": [[1, 0]]\\n                    },\\n                    \\"003.html\\": {\\n                        \\"results\\": [[1, PASS]],\\n                        \\"times\\": [[1, 0]]\\n                    },\\n                }\\n            },\\n            # Expected results\\n            {\\n                \\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n                \\"tests\\": {\\n                    \\"002.html\\": {\\n                        \\"results\\": [[1, PASS], [10, TEXT]],\\n                        \\"times\\": [[11, 0]]\\n                    }\\n                }\\n            },\\n            max_builds=200)\\n\\n    def test_merge_updates_expected(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\n                \\"builds\\": [\\"2\\", \\"1\\"],\\n                \\"tests\\": {\\n                    \\"directory\\": {\\n                        \\"directory\\": {\\n                            \\"001.html\\": {\\n                                \\"expected\\": \\"FAIL\\",\\n                                \\"results\\": [[200, PASS]],\\n                                \\"times\\": [[200, 0]]\\n                            }\\n                        }\\n                    },\\n                    \\"002.html\\": {\\n                        \\"bugs\\": [\\"crbug.com/1234\\"],\\n                        \\"expected\\": \\"FAIL\\",\\n                        \\"results\\": [[10, TEXT]],\\n                        \\"times\\": [[10, 0]]\\n                    },\\n                    \\"003.html\\": {\\n                        \\"expected\\": \\"FAIL\\",\\n                        \\"results\\": [[190, PASS], [9, NO_DATA], [1, TEXT]],\\n                        \\"times\\": [[200, 0]]\\n                    },\\n                    \\"004.html\\": {\\n                        \\"results\\": [[199, PASS], [1, TEXT]],\\n                        \\"times\\": [[200, 0]]\\n                    },\\n                }\\n            },\\n            # Incremental results\\n            {\\n                \\"builds\\": [\\"3\\"],\\n                \\"tests\\": {\\n                    \\"002.html\\": {\\n                        \\"expected\\": \\"PASS\\",\\n                        \\"results\\": [[1, PASS]],\\n                        \\"times\\": [[1, 0]]\\n                    },\\n                    \\"003.html\\": {\\n                        \\"expected\\": \\"TIMEOUT\\",\\n                        \\"results\\": [[1, PASS]],\\n                        \\"times\\": [[1, 0]]\\n                    },\\n                    \\"004.html\\": {\\n                        \\"bugs\\": [\\"crbug.com/1234\\"],\\n                        \\"results\\": [[1, PASS]],\\n                        \\"times\\": [[1, 0]]\\n                    },\\n                }\\n            },\\n            # Expected results\\n            {\\n                \\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n                \\"tests\\": {\\n                    \\"002.html\\": {\\n                        \\"results\\": [[1, PASS], [10, TEXT]],\\n                        \\"times\\": [[11, 0]]\\n                    },\\n                    \\"003.html\\": {\\n                        \\"expected\\": \\"TIMEOUT\\",\\n                        \\"results\\": [[191, PASS], [9, NO_DATA]],\\n                        \\"times\\": [[200, 0]]\\n                    },\\n                    \\"004.html\\": {\\n                        \\"bugs\\": [\\"crbug.com/1234\\"],\\n                        \\"results\\": [[200, PASS]],\\n                        \\"times\\": [[200, 0]]\\n                    },\\n                }\\n            },\\n            max_builds=200)\\n\\n\\n    def test_merge_keep_test_with_all_pass_but_slow_time(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, PASS]],\\n                           \\"times\\": [[200, jsonresults.JSON_RESULTS_MIN_TIME]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[10, TEXT]],\\n                           \\"times\\": [[10, 0]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, PASS]],\\n                           \\"times\\": [[1, 1]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[1, PASS]],\\n                           \\"times\\": [[1, 0]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[201, PASS]],\\n                           \\"times\\": [[1, 1], [200, jsonresults.JSON_RESULTS_MIN_TIME]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[1, PASS], [10, TEXT]],\\n                           \\"times\\": [[11, 0]]}}})\\n\\n    def test_merge_pruning_slow_tests_for_debug_builders(self):\\n        self._builder = \\"MockBuilder(dbg)\\"\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[200, PASS]],\\n                           \\"times\\": [[200, 3 * jsonresults.JSON_RESULTS_MIN_TIME]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[10, TEXT]],\\n                           \\"times\\": [[10, 0]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, PASS]],\\n                           \\"times\\": [[1, 1]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[1, PASS]],\\n                           \\"times\\": [[1, 0]]},\\n                       \\"003.html\\": {\\n                           \\"results\\": [[1, PASS]],\\n                           \\"times\\": [[1, jsonresults.JSON_RESULTS_MIN_TIME]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[201, PASS]],\\n                           \\"times\\": [[1, 1], [200, 3 * jsonresults.JSON_RESULTS_MIN_TIME]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[1, PASS], [10, TEXT]],\\n                           \\"times\\": [[11, 0]]}}})\\n\\n    def test_merge_prune_extra_results(self):\\n        # Remove items from test results and times that exceed the max number\\n        # of builds to track.\\n        max_builds = jsonresults.JSON_RESULTS_MAX_BUILDS\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[max_builds, TEXT], [1, IMAGE]],\\n                           \\"times\\": [[max_builds, 0], [1, 1]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TIMEOUT]],\\n                           \\"times\\": [[1, 1]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TIMEOUT], [max_builds, TEXT]],\\n                           \\"times\\": [[1, 1], [max_builds, 0]]}}})\\n\\n    def test_merge_prune_extra_results_small(self):\\n        # Remove items from test results and times that exceed the max number\\n        # of builds to track, using smaller threshold.\\n        max_builds = jsonresults.JSON_RESULTS_MAX_BUILDS_SMALL\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[max_builds, TEXT], [1, IMAGE]],\\n                           \\"times\\": [[max_builds, 0], [1, 1]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TIMEOUT]],\\n                           \\"times\\": [[1, 1]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TIMEOUT], [max_builds, TEXT]],\\n                           \\"times\\": [[1, 1], [max_builds, 0]]}}},\\n            int(max_builds))\\n\\n    def test_merge_prune_extra_results_with_new_result_of_same_type(self):\\n        # Test that merging in a new result of the same type as the last result\\n        # causes old results to fall off.\\n        max_builds = jsonresults.JSON_RESULTS_MAX_BUILDS_SMALL\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[max_builds, TEXT], [1, NO_DATA]],\\n                           \\"times\\": [[max_builds, 0], [1, 1]]}}},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[1, TEXT]],\\n                           \\"times\\": [[1, 0]]}}},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"001.html\\": {\\n                           \\"results\\": [[max_builds, TEXT]],\\n                           \\"times\\": [[max_builds, 0]]}}},\\n            int(max_builds))\\n\\n    def test_merge_build_directory_hierarchy(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"bar\\": {\\"baz\\": {\\n                           \\"003.html\\": {\\n                                \\"results\\": [[25, TEXT]],\\n                                \\"times\\": [[25, 0]]}}},\\n                       \\"foo\\": {\\n                           \\"001.html\\": {\\n                                \\"results\\": [[50, TEXT]],\\n                                \\"times\\": [[50, 0]]},\\n                           \\"002.html\\": {\\n                                \\"results\\": [[100, IMAGE]],\\n                                \\"times\\": [[100, 0]]}}},\\n              \\"version\\": 4},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"baz\\": {\\n                           \\"004.html\\": {\\n                               \\"results\\": [[1, IMAGE]],\\n                               \\"times\\": [[1, 0]]}},\\n                       \\"foo\\": {\\n                           \\"001.html\\": {\\n                               \\"results\\": [[1, TEXT]],\\n                               \\"times\\": [[1, 0]]},\\n                           \\"002.html\\": {\\n                               \\"results\\": [[1, IMAGE]],\\n                               \\"times\\": [[1, 0]]}}},\\n             \\"version\\": 4},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"bar\\": {\\"baz\\": {\\n                           \\"003.html\\": {\\n                               \\"results\\": [[1, NO_DATA], [25, TEXT]],\\n                               \\"times\\": [[26, 0]]}}},\\n                       \\"baz\\": {\\n                           \\"004.html\\": {\\n                               \\"results\\": [[1, IMAGE]],\\n                               \\"times\\": [[1, 0]]}},\\n                       \\"foo\\": {\\n                           \\"001.html\\": {\\n                               \\"results\\": [[51, TEXT]],\\n                               \\"times\\": [[51, 0]]},\\n                           \\"002.html\\": {\\n                               \\"results\\": [[101, IMAGE]],\\n                               \\"times\\": [[101, 0]]}}},\\n             \\"version\\": 4})\\n\\n    # FIXME(aboxhall): Add some tests for xhtml/svg test results.\\n\\n    def test_get_test_name_list(self):\\n        # Get test name list only. Don\'t include non-test-list data and\\n        # of test result details.\\n        # FIXME: This also tests a temporary bug in the data where directory-level\\n        # results have a results and times values. Once that bug is fixed,\\n        # remove this test-case and assert we don\'t ever hit it.\\n        self._test_get_test_list(\\n            # Input results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"foo\\": {\\n                           \\"001.html\\": {\\n                               \\"results\\": [[200, PASS]],\\n                               \\"times\\": [[200, 0]]},\\n                           \\"results\\": [[1, NO_DATA]],\\n                           \\"times\\": [[1, 0]]},\\n                       \\"002.html\\": {\\n                           \\"results\\": [[10, TEXT]],\\n                           \\"times\\": [[10, 0]]}}},\\n            # Expected results\\n            {\\"foo\\": {\\"001.html\\": {}}, \\"002.html\\": {}})\\n\\n    def test_gtest(self):\\n        self._test_merge(\\n            # Aggregated results\\n            {\\"builds\\": [\\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"foo.bar\\": {\\n                           \\"results\\": [[50, TEXT]],\\n                           \\"times\\": [[50, 0]]},\\n                       \\"foo.bar2\\": {\\n                           \\"results\\": [[100, IMAGE]],\\n                           \\"times\\": [[100, 0]]},\\n                       \\"test.failed\\": {\\n                           \\"results\\": [[5, FAIL]],\\n                           \\"times\\": [[5, 0]]},\\n                       },\\n             \\"version\\": 3},\\n            # Incremental results\\n            {\\"builds\\": [\\"3\\"],\\n             \\"tests\\": {\\"foo.bar2\\": {\\n                           \\"results\\": [[1, IMAGE]],\\n                           \\"times\\": [[1, 0]]},\\n                       \\"foo.bar3\\": {\\n                           \\"results\\": [[1, TEXT]],\\n                           \\"times\\": [[1, 0]]},\\n                       \\"test.failed\\": {\\n                           \\"results\\": [[5, FAIL]],\\n                           \\"times\\": [[5, 0]]},\\n                       },\\n             \\"version\\": 4},\\n            # Expected results\\n            {\\"builds\\": [\\"3\\", \\"2\\", \\"1\\"],\\n             \\"tests\\": {\\"foo.bar\\": {\\n                           \\"results\\": [[1, NO_DATA], [50, TEXT]],\\n                           \\"times\\": [[51, 0]]},\\n                       \\"foo.bar2\\": {\\n                           \\"results\\": [[101, IMAGE]],\\n                           \\"times\\": [[101, 0]]},\\n                       \\"foo.bar3\\": {\\n                           \\"results\\": [[1, TEXT]],\\n                           \\"times\\": [[1, 0]]},\\n                       \\"test.failed\\": {\\n                           \\"results\\": [[10, FAIL]],\\n                           \\"times\\": [[10, 0]]},\\n                       },\\n             \\"version\\": 4})\\n\\nif __name__ == \'__main__\':\\n    unittest.main()\\n"}\n'
line: b'{"repo_name":"fabian4/trove","ref":"refs/heads/master","path":"trove/guestagent/datastore/experimental/redis/manager.py","content":"# Copyright (c) 2013 Rackspace\\n# All Rights Reserved.\\n#\\n#    Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may\\n#    not use this file except in compliance with the License. You may obtain\\n#    a copy of the License at\\n#\\n#         http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#    Unless required by applicable law or agreed to in writing, software\\n#    distributed under the License is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\\n#    License for the specific language governing permissions and limitations\\n#    under the License.\\n\\nfrom oslo_log import log as logging\\nfrom oslo_service import periodic_task\\n\\nfrom trove.common import cfg\\nfrom trove.common import exception\\nfrom trove.common.i18n import _\\nfrom trove.common import instance as rd_instance\\nfrom trove.common import utils\\nfrom trove.guestagent import backup\\nfrom trove.guestagent.common import operating_system\\nfrom trove.guestagent.datastore.experimental.redis import service\\nfrom trove.guestagent import dbaas\\nfrom trove.guestagent.strategies.replication import get_replication_strategy\\nfrom trove.guestagent import volume\\n\\n\\nLOG = logging.getLogger(__name__)\\nCONF = cfg.CONF\\nMANAGER = CONF.datastore_manager or \'redis\'\\nREPLICATION_STRATEGY = CONF.get(MANAGER).replication_strategy\\nREPLICATION_NAMESPACE = CONF.get(MANAGER).replication_namespace\\nREPLICATION_STRATEGY_CLASS = get_replication_strategy(REPLICATION_STRATEGY,\\n                                                      REPLICATION_NAMESPACE)\\n\\n\\nclass Manager(periodic_task.PeriodicTasks):\\n    \\"\\"\\"\\n    This is the Redis manager class. It is dynamically loaded\\n    based off of the service_type of the trove instance\\n    \\"\\"\\"\\n\\n    def __init__(self):\\n        super(Manager, self).__init__(CONF)\\n        self._app = service.RedisApp()\\n\\n    @periodic_task.periodic_task\\n    def update_status(self, context):\\n        \\"\\"\\"\\n        Updates the redis trove instance. It is decorated with\\n        perodic task so it is automatically called every 3 ticks.\\n        \\"\\"\\"\\n        LOG.debug(\\"Update status called.\\")\\n        self._app.status.update()\\n\\n    def rpc_ping(self, context):\\n        LOG.debug(\\"Responding to RPC ping.\\")\\n        return True\\n\\n    def change_passwords(self, context, users):\\n        \\"\\"\\"\\n        Changes the redis instance password,\\n        it is currently not not implemented.\\n        \\"\\"\\"\\n        LOG.debug(\\"Change passwords called.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'change_passwords\', datastore=MANAGER)\\n\\n    def reset_configuration(self, context, configuration):\\n        \\"\\"\\"\\n        Resets to the default configuration,\\n        currently this does nothing.\\n        \\"\\"\\"\\n        LOG.debug(\\"Reset configuration called.\\")\\n        self._app.reset_configuration(configuration)\\n\\n    def _perform_restore(self, backup_info, context, restore_location, app):\\n        \\"\\"\\"Perform a restore on this instance.\\"\\"\\"\\n        LOG.info(_(\\"Restoring database from backup %s.\\") % backup_info[\'id\'])\\n        try:\\n            backup.restore(context, backup_info, restore_location)\\n        except Exception:\\n            LOG.exception(_(\\"Error performing restore from backup %s.\\") %\\n                          backup_info[\'id\'])\\n            app.status.set_status(rd_instance.ServiceStatuses.FAILED)\\n            raise\\n        LOG.info(_(\\"Restored database successfully.\\"))\\n\\n    def prepare(self, context, packages, databases, memory_mb, users,\\n                device_path=None, mount_point=None, backup_info=None,\\n                config_contents=None, root_password=None, overrides=None,\\n                cluster_config=None, snapshot=None):\\n        \\"\\"\\"\\n        This is called when the trove instance first comes online.\\n        It is the first rpc message passed from the task manager.\\n        prepare handles all the base configuration of the redis instance.\\n        \\"\\"\\"\\n        try:\\n            self._app.status.begin_install()\\n            if device_path:\\n                device = volume.VolumeDevice(device_path)\\n                # unmount if device is already mounted\\n                device.unmount_device(device_path)\\n                device.format()\\n                device.mount(mount_point)\\n                operating_system.chown(mount_point, \'redis\', \'redis\',\\n                                       as_root=True)\\n                LOG.debug(\'Mounted the volume.\')\\n            self._app.install_if_needed(packages)\\n            LOG.info(_(\'Writing redis configuration.\'))\\n            if cluster_config:\\n                config_contents = (config_contents + \\"\\\\n\\"\\n                                   + \\"cluster-enabled yes\\\\n\\"\\n                                   + \\"cluster-config-file cluster.conf\\\\n\\")\\n            self._app.configuration_manager.save_configuration(config_contents)\\n            self._app.apply_initial_guestagent_configuration()\\n            if backup_info:\\n                persistence_dir = self._app.get_working_dir()\\n                self._perform_restore(backup_info, context, persistence_dir,\\n                                      self._app)\\n            if snapshot:\\n                self.attach_replica(context, snapshot, snapshot[\'config\'])\\n            self._app.restart()\\n            if cluster_config:\\n                self._app.status.set_status(\\n                    rd_instance.ServiceStatuses.BUILD_PENDING)\\n            else:\\n                self._app.complete_install_or_restart()\\n            LOG.info(_(\'Redis instance has been setup and configured.\'))\\n        except Exception:\\n            LOG.exception(_(\\"Error setting up Redis instance.\\"))\\n            self._app.status.set_status(rd_instance.ServiceStatuses.FAILED)\\n            raise\\n\\n    def restart(self, context):\\n        \\"\\"\\"\\n        Restart this redis instance.\\n        This method is called when the guest agent\\n        gets a restart message from the taskmanager.\\n        \\"\\"\\"\\n        LOG.debug(\\"Restart called.\\")\\n        self._app.restart()\\n\\n    def start_db_with_conf_changes(self, context, config_contents):\\n        \\"\\"\\"\\n        Start this redis instance with new conf changes.\\n        \\"\\"\\"\\n        LOG.debug(\\"Start DB with conf changes called.\\")\\n        self._app.start_db_with_conf_changes(config_contents)\\n\\n    def stop_db(self, context, do_not_start_on_reboot=False):\\n        \\"\\"\\"\\n        Stop this redis instance.\\n        This method is called when the guest agent\\n        gets a stop message from the taskmanager.\\n        \\"\\"\\"\\n        LOG.debug(\\"Stop DB called.\\")\\n        self._app.stop_db(do_not_start_on_reboot=do_not_start_on_reboot)\\n\\n    def get_filesystem_stats(self, context, fs_path):\\n        \\"\\"\\"Gets the filesystem stats for the path given.\\"\\"\\"\\n        LOG.debug(\\"Get Filesystem Stats.\\")\\n        mount_point = CONF.get(\\n            \'mysql\' if not MANAGER else MANAGER).mount_point\\n        return dbaas.get_filesystem_volume_stats(mount_point)\\n\\n    def create_backup(self, context, backup_info):\\n        \\"\\"\\"Create a backup of the database.\\"\\"\\"\\n        LOG.debug(\\"Creating backup.\\")\\n        backup.backup(context, backup_info)\\n\\n    def mount_volume(self, context, device_path=None, mount_point=None):\\n        device = volume.VolumeDevice(device_path)\\n        device.mount(mount_point, write_to_fstab=False)\\n        LOG.debug(\\"Mounted the device %s at the mount point %s.\\" %\\n                  (device_path, mount_point))\\n\\n    def unmount_volume(self, context, device_path=None, mount_point=None):\\n        device = volume.VolumeDevice(device_path)\\n        device.unmount(mount_point)\\n        LOG.debug(\\"Unmounted the device %s from the mount point %s.\\" %\\n                  (device_path, mount_point))\\n\\n    def resize_fs(self, context, device_path=None, mount_point=None):\\n        device = volume.VolumeDevice(device_path)\\n        device.resize_fs(mount_point)\\n        LOG.debug(\\"Resized the filesystem at %s.\\" % mount_point)\\n\\n    def update_overrides(self, context, overrides, remove=False):\\n        LOG.debug(\\"Updating overrides.\\")\\n        if remove:\\n            self._app.remove_overrides()\\n        else:\\n            self._app.update_overrides(context, overrides, remove)\\n\\n    def apply_overrides(self, context, overrides):\\n        LOG.debug(\\"Applying overrides.\\")\\n        self._app.apply_overrides(self._app.admin, overrides)\\n\\n    def update_attributes(self, context, username, hostname, user_attrs):\\n        LOG.debug(\\"Updating attributes.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'update_attributes\', datastore=MANAGER)\\n\\n    def create_database(self, context, databases):\\n        LOG.debug(\\"Creating database.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'create_database\', datastore=MANAGER)\\n\\n    def create_user(self, context, users):\\n        LOG.debug(\\"Creating user.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'create_user\', datastore=MANAGER)\\n\\n    def delete_database(self, context, database):\\n        LOG.debug(\\"Deleting database.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'delete_database\', datastore=MANAGER)\\n\\n    def delete_user(self, context, user):\\n        LOG.debug(\\"Deleting user.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'delete_user\', datastore=MANAGER)\\n\\n    def get_user(self, context, username, hostname):\\n        LOG.debug(\\"Getting user.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'get_user\', datastore=MANAGER)\\n\\n    def grant_access(self, context, username, hostname, databases):\\n        LOG.debug(\\"Granting access.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'grant_access\', datastore=MANAGER)\\n\\n    def revoke_access(self, context, username, hostname, database):\\n        LOG.debug(\\"Revoking access.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'revoke_access\', datastore=MANAGER)\\n\\n    def list_access(self, context, username, hostname):\\n        LOG.debug(\\"Listing access.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'list_access\', datastore=MANAGER)\\n\\n    def list_databases(self, context, limit=None, marker=None,\\n                       include_marker=False):\\n        LOG.debug(\\"Listing databases.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'list_databases\', datastore=MANAGER)\\n\\n    def list_users(self, context, limit=None, marker=None,\\n                   include_marker=False):\\n        LOG.debug(\\"Listing users.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'list_users\', datastore=MANAGER)\\n\\n    def enable_root(self, context):\\n        LOG.debug(\\"Enabling root.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'enable_root\', datastore=MANAGER)\\n\\n    def enable_root_with_password(self, context, root_password=None):\\n        LOG.debug(\\"Enabling root with password.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'enable_root_with_password\', datastore=MANAGER)\\n\\n    def is_root_enabled(self, context):\\n        LOG.debug(\\"Checking if root is enabled.\\")\\n        raise exception.DatastoreOperationNotSupported(\\n            operation=\'is_root_enabled\', datastore=MANAGER)\\n\\n    def backup_required_for_replication(self, context):\\n        replication = REPLICATION_STRATEGY_CLASS(context)\\n        return replication.backup_required_for_replication()\\n\\n    def get_replication_snapshot(self, context, snapshot_info,\\n                                 replica_source_config=None):\\n        LOG.debug(\\"Getting replication snapshot.\\")\\n        replication = REPLICATION_STRATEGY_CLASS(context)\\n        replication.enable_as_master(self._app, replica_source_config)\\n\\n        snapshot_id, log_position = (\\n            replication.snapshot_for_replication(context, self._app, None,\\n                                                 snapshot_info))\\n\\n        mount_point = CONF.get(MANAGER).mount_point\\n        volume_stats = dbaas.get_filesystem_volume_stats(mount_point)\\n\\n        replication_snapshot = {\\n            \'dataset\': {\\n                \'datastore_manager\': MANAGER,\\n                \'dataset_size\': volume_stats.get(\'used\', 0.0),\\n                \'volume_size\': volume_stats.get(\'total\', 0.0),\\n                \'snapshot_id\': snapshot_id\\n            },\\n            \'replication_strategy\': REPLICATION_STRATEGY,\\n            \'master\': replication.get_master_ref(self._app, snapshot_info),\\n            \'log_position\': log_position\\n        }\\n\\n        return replication_snapshot\\n\\n    def enable_as_master(self, context, replica_source_config):\\n        LOG.debug(\\"Calling enable_as_master.\\")\\n        replication = REPLICATION_STRATEGY_CLASS(context)\\n        replication.enable_as_master(self._app, replica_source_config)\\n\\n    def detach_replica(self, context, for_failover=False):\\n        LOG.debug(\\"Detaching replica.\\")\\n        replication = REPLICATION_STRATEGY_CLASS(context)\\n        replica_info = replication.detach_slave(self._app, for_failover)\\n        return replica_info\\n\\n    def get_replica_context(self, context):\\n        LOG.debug(\\"Getting replica context.\\")\\n        replication = REPLICATION_STRATEGY_CLASS(context)\\n        replica_info = replication.get_replica_context(self._app)\\n        return replica_info\\n\\n    def _validate_slave_for_replication(self, context, replica_info):\\n        if (replica_info[\'replication_strategy\'] != REPLICATION_STRATEGY):\\n            raise exception.IncompatibleReplicationStrategy(\\n                replica_info.update({\\n                    \'guest_strategy\': REPLICATION_STRATEGY\\n                }))\\n\\n    def attach_replica(self, context, replica_info, slave_config):\\n        LOG.debug(\\"Attaching replica.\\")\\n        try:\\n            if \'replication_strategy\' in replica_info:\\n                self._validate_slave_for_replication(context, replica_info)\\n            replication = REPLICATION_STRATEGY_CLASS(context)\\n            replication.enable_as_slave(self._app, replica_info,\\n                                        slave_config)\\n        except Exception:\\n            LOG.exception(\\"Error enabling replication.\\")\\n            self._app.status.set_status(rd_instance.ServiceStatuses.FAILED)\\n            raise\\n\\n    def make_read_only(self, context, read_only):\\n        LOG.debug(\\"Executing make_read_only(%s)\\" % read_only)\\n        self._app.make_read_only(read_only)\\n\\n    def _get_repl_info(self):\\n        return self._app.admin.get_info(\'replication\')\\n\\n    def _get_master_host(self):\\n        slave_info = self._get_repl_info()\\n        return slave_info and slave_info[\'master_host\'] or None\\n\\n    def _get_repl_offset(self):\\n        repl_info = self._get_repl_info()\\n        LOG.debug(\\"Got repl info: %s\\" % repl_info)\\n        offset_key = \'%s_repl_offset\' % repl_info[\'role\']\\n        offset = repl_info[offset_key]\\n        LOG.debug(\\"Found offset %s for key %s.\\" % (offset, offset_key))\\n        return int(offset)\\n\\n    def get_last_txn(self, context):\\n        master_host = self._get_master_host()\\n        repl_offset = self._get_repl_offset()\\n        return master_host, repl_offset\\n\\n    def get_latest_txn_id(self, context):\\n        LOG.info(_(\\"Retrieving latest repl offset.\\"))\\n        return self._get_repl_offset()\\n\\n    def wait_for_txn(self, context, txn):\\n        LOG.info(_(\\"Waiting on repl offset \'%s\'.\\") % txn)\\n\\n        def _wait_for_txn():\\n            current_offset = self._get_repl_offset()\\n            LOG.debug(\\"Current offset: %s.\\" % current_offset)\\n            return current_offset \\u003e= txn\\n\\n        try:\\n            utils.poll_until(_wait_for_txn, time_out=120)\\n        except exception.PollTimeOut:\\n            raise RuntimeError(_(\\"Timeout occurred waiting for Redis repl \\"\\n                                 \\"offset to change to \'%s\'.\\") % txn)\\n\\n    def cleanup_source_on_replica_detach(self, context, replica_info):\\n        LOG.debug(\\"Cleaning up the source on the detach of a replica.\\")\\n        replication = REPLICATION_STRATEGY_CLASS(context)\\n        replication.cleanup_source_on_replica_detach(self._app, replica_info)\\n\\n    def demote_replication_master(self, context):\\n        LOG.debug(\\"Demoting replica source.\\")\\n        replication = REPLICATION_STRATEGY_CLASS(context)\\n        replication.demote_master(self._app)\\n\\n    def cluster_meet(self, context, ip, port):\\n        LOG.debug(\\"Executing cluster_meet to join node to cluster.\\")\\n        self._app.cluster_meet(ip, port)\\n\\n    def get_node_ip(self, context):\\n        LOG.debug(\\"Retrieving cluster node ip address.\\")\\n        return self._app.get_node_ip()\\n\\n    def get_node_id_for_removal(self, context):\\n        LOG.debug(\\"Validating removal of node from cluster.\\")\\n        return self._app.get_node_id_for_removal()\\n\\n    def remove_nodes(self, context, node_ids):\\n        LOG.debug(\\"Removing nodes from cluster.\\")\\n        self._app.remove_nodes(node_ids)\\n\\n    def cluster_addslots(self, context, first_slot, last_slot):\\n        LOG.debug(\\"Executing cluster_addslots to assign hash slots %s-%s.\\",\\n                  first_slot, last_slot)\\n        self._app.cluster_addslots(first_slot, last_slot)\\n\\n    def cluster_complete(self, context):\\n        LOG.debug(\\"Cluster creation complete, starting status checks.\\")\\n        self._app.complete_install_or_restart()\\n"}\n'
line: b'{"repo_name":"Hernanarce/pelisalacarta","ref":"refs/heads/master","path":"python/version-mediaserver/platformcode/platformtools.py","content":"# -*- coding: utf-8 -*-\\r\\n# ------------------------------------------------------------\\r\\n# pelisalacarta 4\\r\\n# Copyright 2015 tvalacarta@gmail.com\\r\\n# http://blog.tvalacarta.info/plugin-xbmc/pelisalacarta/\\r\\n#\\r\\n# Distributed under the terms of GNU General Public License v3 (GPLv3)\\r\\n# http://www.gnu.org/licenses/gpl-3.0.html\\r\\n# ------------------------------------------------------------\\r\\n# This file is part of pelisalacarta 4.\\r\\n#\\r\\n# pelisalacarta 4 is free software: you can redistribute it and/or modify\\r\\n# it under the terms of the GNU General Public License as published by\\r\\n# the Free Software Foundation, either version 3 of the License, or\\r\\n# (at your option) any later version.\\r\\n#\\r\\n# pelisalacarta 4 is distributed in the hope that it will be useful,\\r\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\r\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\r\\n# GNU General Public License for more details.\\r\\n#\\r\\n# You should have received a copy of the GNU General Public License\\r\\n# along with pelisalacarta 4.  If not, see \\u003chttp://www.gnu.org/licenses/\\u003e.\\r\\n# ------------------------------------------------------------\\r\\n# platformtools\\r\\n# ------------------------------------------------------------\\r\\n# Herramientas responsables de adaptar los diferentes \\r\\n# cuadros de dialogo a una plataforma en concreto,\\r\\n# en este caso Mediserver.\\r\\n# version 1.3\\r\\n# ------------------------------------------------------------\\r\\nimport os\\r\\nimport sys\\r\\nfrom core import config\\r\\nfrom core import logger\\r\\nimport threading\\r\\ncontrollers = {}\\r\\n\\r\\n\\r\\ndef dialog_ok(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].dialog_ok(*args, **kwargs)\\r\\n    \\r\\ndef dialog_notification(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].dialog_notification(*args, **kwargs)\\r\\n\\r\\ndef dialog_yesno(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].dialog_yesno(*args, **kwargs)\\r\\n    \\r\\ndef dialog_select(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].dialog_select(*args, **kwargs)\\r\\n\\r\\ndef dialog_progress(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].dialog_progress(*args, **kwargs)\\r\\n \\r\\ndef dialog_progress_bg(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].dialog_progress_bg(*args, **kwargs)\\r\\n\\r\\ndef dialog_input(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].dialog_input(*args, **kwargs)\\r\\n    \\r\\ndef dialog_numeric(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].dialog_numeric(*args, **kwargs)\\r\\n\\r\\ndef itemlist_refresh(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].itemlist_refresh(*args, **kwargs)\\r\\n\\r\\ndef itemlist_update(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].itemlist_update(*args, **kwargs)\\r\\n\\r\\ndef render_items(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].render_items(*args,**kwargs)\\r\\n\\r\\ndef is_playing(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].is_playing(*args, **kwargs)\\r\\n\\r\\ndef play_video(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].play_video(*args, **kwargs)\\r\\n\\r\\ndef open_settings(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].open_settings(*args, **kwargs)\\r\\n\\r\\ndef show_channel_settings(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].show_channel_settings(*args, **kwargs)\\r\\n\\r\\ndef show_video_info(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].show_video_info(*args, **kwargs)\\r\\n\\r\\ndef show_recaptcha(*args, **kwargs):\\r\\n    id = threading.current_thread().name\\r\\n    return controllers[id].show_recaptcha(*args, **kwargs)"}\n'
line: b'{"repo_name":"hobson/pyexiv2","ref":"refs/heads/master","path":"test/usercomment.py","content":"# -*- coding: utf-8 -*-\\n\\n# ******************************************************************************\\n#\\n# Copyright (C) 2010 Olivier Tilloy \\u003colivier@tilloy.net\\u003e\\n#\\n# This file is part of the pyexiv2 distribution.\\n#\\n# pyexiv2 is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU General Public License\\n# as published by the Free Software Foundation; either version 2\\n# of the License, or (at your option) any later version.\\n#\\n# pyexiv2 is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n# GNU General Public License for more details.\\n#\\n# You should have received a copy of the GNU General Public License\\n# along with pyexiv2; if not, write to the Free Software\\n# Foundation, Inc., 51 Franklin Street, 5th Floor, Boston, MA 02110-1301 USA.\\n#\\n# Author: Olivier Tilloy \\u003colivier@tilloy.net\\u003e\\n#\\n# ******************************************************************************\\n\\nfrom pyexiv2.metadata import ImageMetadata\\n\\nimport unittest\\nimport testutils\\nimport os\\nimport tempfile\\nfrom testutils import EMPTY_JPG_DATA\\n\\n\\nclass TestUserCommentReadWrite(unittest.TestCase):\\n\\n    checksums = {\\n        \'usercomment-ascii.jpg\': \'ad29ac65fb6f63c8361aaed6cb02f8c7\',\\n        \'usercomment-unicode-ii.jpg\': \'13b7cc09129a8677f2cf18634f5abd3c\',\\n        \'usercomment-unicode-mm.jpg\': \'7addfed7823c556ba489cd4ab2037200\',\\n        }\\n\\n    def _read_image(self, filename):\\n        filepath = testutils.get_absolute_file_path(os.path.join(\'data\', filename))\\n        self.assert_(testutils.CheckFileSum(filepath, self.checksums[filename]))\\n        m = ImageMetadata(filepath)\\n        m.read()\\n        return m\\n\\n    def _expected_raw_value(self, endianness, value):\\n        from pyexiv2 import __exiv2_version__\\n        if __exiv2_version__ \\u003e= \'0.20\':\\n            return value\\n        else:\\n            encodings = {\'ii\': \'utf-16le\', \'mm\': \'utf-16be\'}\\n            return value.decode(\'utf-8\').encode(encodings[endianness])\\n\\n    def test_read_ascii(self):\\n        m = self._read_image(\'usercomment-ascii.jpg\')\\n        tag = m[\'Exif.Photo.UserComment\']\\n        self.assertEqual(tag.type, \'Comment\')\\n        self.assertEqual(tag.raw_value, \'charset=\\"Ascii\\" deja vu\')\\n        self.assertEqual(tag.value, u\'deja vu\')\\n\\n    def test_read_unicode_little_endian(self):\\n        m = self._read_image(\'usercomment-unicode-ii.jpg\')\\n        tag = m[\'Exif.Photo.UserComment\']\\n        self.assertEqual(tag.type, \'Comment\')\\n        self.assertEqual(tag.raw_value, \'charset=\\"Unicode\\" %s\' % self._expected_raw_value(\'ii\', \'d\xc3\xa9j\xc3\xa0 vu\'))\\n        self.assertEqual(tag.value, u\'d\xc3\xa9j\xc3\xa0 vu\')\\n\\n    def test_read_unicode_big_endian(self):\\n        m = self._read_image(\'usercomment-unicode-mm.jpg\')\\n        tag = m[\'Exif.Photo.UserComment\']\\n        self.assertEqual(tag.type, \'Comment\')\\n        self.assertEqual(tag.raw_value, \'charset=\\"Unicode\\" %s\' % self._expected_raw_value(\'mm\', \'d\xc3\xa9j\xc3\xa0 vu\'))\\n        self.assertEqual(tag.value, u\'d\xc3\xa9j\xc3\xa0 vu\')\\n\\n    def test_write_ascii(self):\\n        m = self._read_image(\'usercomment-ascii.jpg\')\\n        tag = m[\'Exif.Photo.UserComment\']\\n        self.assertEqual(tag.type, \'Comment\')\\n        tag.value = \'foo bar\'\\n        self.assertEqual(tag.raw_value, \'charset=\\"Ascii\\" foo bar\')\\n        self.assertEqual(tag.value, u\'foo bar\')\\n\\n    def test_write_unicode_over_ascii(self):\\n        m = self._read_image(\'usercomment-ascii.jpg\')\\n        tag = m[\'Exif.Photo.UserComment\']\\n        self.assertEqual(tag.type, \'Comment\')\\n        tag.value = u\'d\xc3\xa9j\xc3\xa0 vu\'\\n        self.assertEqual(tag.raw_value, \'d\xc3\xa9j\xc3\xa0 vu\')\\n        self.assertEqual(tag.value, u\'d\xc3\xa9j\xc3\xa0 vu\')\\n\\n    def test_write_unicode_little_endian(self):\\n        m = self._read_image(\'usercomment-unicode-ii.jpg\')\\n        tag = m[\'Exif.Photo.UserComment\']\\n        self.assertEqual(tag.type, \'Comment\')\\n        tag.value = u\'D\xc3\x89J\xc3\x80 VU\'\\n        self.assertEqual(tag.raw_value, \'charset=\\"Unicode\\" %s\' % self._expected_raw_value(\'ii\', \'D\xc3\x89J\xc3\x80 VU\'))\\n        self.assertEqual(tag.value, u\'D\xc3\x89J\xc3\x80 VU\')\\n\\n    def test_write_unicode_big_endian(self):\\n        m = self._read_image(\'usercomment-unicode-mm.jpg\')\\n        tag = m[\'Exif.Photo.UserComment\']\\n        self.assertEqual(tag.type, \'Comment\')\\n        tag.value = u\'D\xc3\x89J\xc3\x80 VU\'\\n        self.assertEqual(tag.raw_value, \'charset=\\"Unicode\\" %s\' % self._expected_raw_value(\'mm\', \'D\xc3\x89J\xc3\x80 VU\'))\\n        self.assertEqual(tag.value, u\'D\xc3\x89J\xc3\x80 VU\')\\n\\n\\nclass TestUserCommentAdd(unittest.TestCase):\\n\\n    def setUp(self):\\n        # Create an empty image file\\n        fd, self.pathname = tempfile.mkstemp(suffix=\'.jpg\')\\n        os.write(fd, EMPTY_JPG_DATA)\\n        os.close(fd)\\n\\n    def tearDown(self):\\n        os.remove(self.pathname)\\n\\n    def _test_add_comment(self, value):\\n        metadata = ImageMetadata(self.pathname)\\n        metadata.read()\\n        key = \'Exif.Photo.UserComment\'\\n        metadata[key] = value\\n        metadata.write()\\n\\n        metadata = ImageMetadata(self.pathname)\\n        metadata.read()\\n        self.assert_(key in metadata.exif_keys)\\n        tag = metadata[key]\\n        self.assertEqual(tag.type, \'Comment\')\\n        self.assertEqual(tag.value, value)\\n\\n    def test_add_comment_ascii(self):\\n        self._test_add_comment(\'deja vu\')\\n\\n    def test_add_comment_unicode(self):\\n        self._test_add_comment(u\'d\xc3\xa9j\xc3\xa0 vu\')\\n\\n"}\n'
line: b'{"repo_name":"andyzsf/edx","ref":"refs/heads/master","path":"common/djangoapps/student/migrations/0020_add_test_center_user.py","content":"# -*- coding: utf-8 -*-\\nimport datetime\\nfrom south.db import db\\nfrom south.v2 import SchemaMigration\\nfrom django.db import models\\n\\n\\nclass Migration(SchemaMigration):\\n\\n    def forwards(self, orm):\\n        # Adding model \'TestCenterUser\'\\n        db.create_table(\'student_testcenteruser\', (\\n            (\'id\', self.gf(\'django.db.models.fields.AutoField\')(primary_key=True)),\\n            (\'user\', self.gf(\'django.db.models.fields.related.ForeignKey\')(default=None, to=orm[\'auth.User\'], unique=True)),\\n            (\'created_at\', self.gf(\'django.db.models.fields.DateTimeField\')(auto_now_add=True, db_index=True, blank=True)),\\n            (\'updated_at\', self.gf(\'django.db.models.fields.DateTimeField\')(auto_now=True, db_index=True, blank=True)),\\n            (\'user_updated_at\', self.gf(\'django.db.models.fields.DateTimeField\')(db_index=True)),\\n            (\'candidate_id\', self.gf(\'django.db.models.fields.IntegerField\')(null=True, db_index=True)),\\n            (\'client_candidate_id\', self.gf(\'django.db.models.fields.CharField\')(max_length=50, db_index=True)),\\n            (\'first_name\', self.gf(\'django.db.models.fields.CharField\')(max_length=30, db_index=True)),\\n            (\'last_name\', self.gf(\'django.db.models.fields.CharField\')(max_length=50, db_index=True)),\\n            (\'middle_name\', self.gf(\'django.db.models.fields.CharField\')(max_length=30, blank=True)),\\n            (\'suffix\', self.gf(\'django.db.models.fields.CharField\')(max_length=255, blank=True)),\\n            (\'salutation\', self.gf(\'django.db.models.fields.CharField\')(max_length=50, blank=True)),\\n            (\'address_1\', self.gf(\'django.db.models.fields.CharField\')(max_length=40)),\\n            (\'address_2\', self.gf(\'django.db.models.fields.CharField\')(max_length=40, blank=True)),\\n            (\'address_3\', self.gf(\'django.db.models.fields.CharField\')(max_length=40, blank=True)),\\n            (\'city\', self.gf(\'django.db.models.fields.CharField\')(max_length=32, db_index=True)),\\n            (\'state\', self.gf(\'django.db.models.fields.CharField\')(db_index=True, max_length=20, blank=True)),\\n            (\'postal_code\', self.gf(\'django.db.models.fields.CharField\')(db_index=True, max_length=16, blank=True)),\\n            (\'country\', self.gf(\'django.db.models.fields.CharField\')(max_length=3, db_index=True)),\\n            (\'phone\', self.gf(\'django.db.models.fields.CharField\')(max_length=35)),\\n            (\'extension\', self.gf(\'django.db.models.fields.CharField\')(db_index=True, max_length=8, blank=True)),\\n            (\'phone_country_code\', self.gf(\'django.db.models.fields.CharField\')(max_length=3, db_index=True)),\\n            (\'fax\', self.gf(\'django.db.models.fields.CharField\')(max_length=35, blank=True)),\\n            (\'fax_country_code\', self.gf(\'django.db.models.fields.CharField\')(max_length=3, blank=True)),\\n            (\'company_name\', self.gf(\'django.db.models.fields.CharField\')(max_length=50, blank=True)),\\n        ))\\n        db.send_create_signal(\'student\', [\'TestCenterUser\'])\\n\\n\\n    def backwards(self, orm):\\n        # Deleting model \'TestCenterUser\'\\n        db.delete_table(\'student_testcenteruser\')\\n\\n\\n    models = {\\n        \'auth.group\': {\\n            \'Meta\': {\'object_name\': \'Group\'},\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'unique\': \'True\', \'max_length\': \'80\'}),\\n            \'permissions\': (\'django.db.models.fields.related.ManyToManyField\', [], {\'to\': \\"orm[\'auth.Permission\']\\", \'symmetrical\': \'False\', \'blank\': \'True\'})\\n        },\\n        \'auth.permission\': {\\n            \'Meta\': {\'ordering\': \\"(\'content_type__app_label\', \'content_type__model\', \'codename\')\\", \'unique_together\': \\"((\'content_type\', \'codename\'),)\\", \'object_name\': \'Permission\'},\\n            \'codename\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'content_type\': (\'django.db.models.fields.related.ForeignKey\', [], {\'to\': \\"orm[\'contenttypes.ContentType\']\\"}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\'})\\n        },\\n        \'auth.user\': {\\n            \'Meta\': {\'object_name\': \'User\'},\\n            \'about\': (\'django.db.models.fields.TextField\', [], {\'blank\': \'True\'}),\\n            \'avatar_type\': (\'django.db.models.fields.CharField\', [], {\'default\': \\"\'n\'\\", \'max_length\': \'1\'}),\\n            \'bronze\': (\'django.db.models.fields.SmallIntegerField\', [], {\'default\': \'0\'}),\\n            \'consecutive_days_visit_count\': (\'django.db.models.fields.IntegerField\', [], {\'default\': \'0\'}),\\n            \'country\': (\'django_countries.fields.CountryField\', [], {\'max_length\': \'2\', \'blank\': \'True\'}),\\n            \'date_joined\': (\'django.db.models.fields.DateTimeField\', [], {\'default\': \'datetime.datetime.now\'}),\\n            \'date_of_birth\': (\'django.db.models.fields.DateField\', [], {\'null\': \'True\', \'blank\': \'True\'}),\\n            \'display_tag_filter_strategy\': (\'django.db.models.fields.SmallIntegerField\', [], {\'default\': \'0\'}),\\n            \'email\': (\'django.db.models.fields.EmailField\', [], {\'max_length\': \'75\', \'blank\': \'True\'}),\\n            \'email_isvalid\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'email_key\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'32\', \'null\': \'True\'}),\\n            \'email_tag_filter_strategy\': (\'django.db.models.fields.SmallIntegerField\', [], {\'default\': \'1\'}),\\n            \'first_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'30\', \'blank\': \'True\'}),\\n            \'gold\': (\'django.db.models.fields.SmallIntegerField\', [], {\'default\': \'0\'}),\\n            \'gravatar\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'32\'}),\\n            \'groups\': (\'django.db.models.fields.related.ManyToManyField\', [], {\'to\': \\"orm[\'auth.Group\']\\", \'symmetrical\': \'False\', \'blank\': \'True\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'ignored_tags\': (\'django.db.models.fields.TextField\', [], {\'blank\': \'True\'}),\\n            \'interesting_tags\': (\'django.db.models.fields.TextField\', [], {\'blank\': \'True\'}),\\n            \'is_active\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'True\'}),\\n            \'is_staff\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'is_superuser\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'last_login\': (\'django.db.models.fields.DateTimeField\', [], {\'default\': \'datetime.datetime.now\'}),\\n            \'last_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'30\', \'blank\': \'True\'}),\\n            \'last_seen\': (\'django.db.models.fields.DateTimeField\', [], {\'default\': \'datetime.datetime.now\'}),\\n            \'location\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\', \'blank\': \'True\'}),\\n            \'new_response_count\': (\'django.db.models.fields.IntegerField\', [], {\'default\': \'0\'}),\\n            \'password\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'128\'}),\\n            \'questions_per_page\': (\'django.db.models.fields.SmallIntegerField\', [], {\'default\': \'10\'}),\\n            \'real_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\', \'blank\': \'True\'}),\\n            \'reputation\': (\'django.db.models.fields.PositiveIntegerField\', [], {\'default\': \'1\'}),\\n            \'seen_response_count\': (\'django.db.models.fields.IntegerField\', [], {\'default\': \'0\'}),\\n            \'show_country\': (\'django.db.models.fields.BooleanField\', [], {\'default\': \'False\'}),\\n            \'silver\': (\'django.db.models.fields.SmallIntegerField\', [], {\'default\': \'0\'}),\\n            \'status\': (\'django.db.models.fields.CharField\', [], {\'default\': \\"\'w\'\\", \'max_length\': \'2\'}),\\n            \'user_permissions\': (\'django.db.models.fields.related.ManyToManyField\', [], {\'to\': \\"orm[\'auth.Permission\']\\", \'symmetrical\': \'False\', \'blank\': \'True\'}),\\n            \'username\': (\'django.db.models.fields.CharField\', [], {\'unique\': \'True\', \'max_length\': \'30\'}),\\n            \'website\': (\'django.db.models.fields.URLField\', [], {\'max_length\': \'200\', \'blank\': \'True\'})\\n        },\\n        \'contenttypes.contenttype\': {\\n            \'Meta\': {\'ordering\': \\"(\'name\',)\\", \'unique_together\': \\"((\'app_label\', \'model\'),)\\", \'object_name\': \'ContentType\', \'db_table\': \\"\'django_content_type\'\\"},\\n            \'app_label\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'model\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'100\'})\\n        },\\n        \'student.courseenrollment\': {\\n            \'Meta\': {\'unique_together\': \\"((\'user\', \'course_id\'),)\\", \'object_name\': \'CourseEnrollment\'},\\n            \'course_id\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'255\', \'db_index\': \'True\'}),\\n            \'created\': (\'django.db.models.fields.DateTimeField\', [], {\'auto_now_add\': \'True\', \'null\': \'True\', \'db_index\': \'True\', \'blank\': \'True\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'user\': (\'django.db.models.fields.related.ForeignKey\', [], {\'to\': \\"orm[\'auth.User\']\\"})\\n        },\\n        \'student.pendingemailchange\': {\\n            \'Meta\': {\'object_name\': \'PendingEmailChange\'},\\n            \'activation_key\': (\'django.db.models.fields.CharField\', [], {\'unique\': \'True\', \'max_length\': \'32\', \'db_index\': \'True\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'new_email\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'255\', \'blank\': \'True\'}),\\n            \'user\': (\'django.db.models.fields.related.OneToOneField\', [], {\'to\': \\"orm[\'auth.User\']\\", \'unique\': \'True\'})\\n        },\\n        \'student.pendingnamechange\': {\\n            \'Meta\': {\'object_name\': \'PendingNameChange\'},\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'new_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'255\', \'blank\': \'True\'}),\\n            \'rationale\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'1024\', \'blank\': \'True\'}),\\n            \'user\': (\'django.db.models.fields.related.OneToOneField\', [], {\'to\': \\"orm[\'auth.User\']\\", \'unique\': \'True\'})\\n        },\\n        \'student.registration\': {\\n            \'Meta\': {\'object_name\': \'Registration\', \'db_table\': \\"\'auth_registration\'\\"},\\n            \'activation_key\': (\'django.db.models.fields.CharField\', [], {\'unique\': \'True\', \'max_length\': \'32\', \'db_index\': \'True\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'user\': (\'django.db.models.fields.related.ForeignKey\', [], {\'to\': \\"orm[\'auth.User\']\\", \'unique\': \'True\'})\\n        },\\n        \'student.testcenteruser\': {\\n            \'Meta\': {\'object_name\': \'TestCenterUser\'},\\n            \'address_1\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'40\'}),\\n            \'address_2\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'40\', \'blank\': \'True\'}),\\n            \'address_3\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'40\', \'blank\': \'True\'}),\\n            \'candidate_id\': (\'django.db.models.fields.IntegerField\', [], {\'null\': \'True\', \'db_index\': \'True\'}),\\n            \'city\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'32\', \'db_index\': \'True\'}),\\n            \'client_candidate_id\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\', \'db_index\': \'True\'}),\\n            \'company_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\', \'blank\': \'True\'}),\\n            \'country\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'3\', \'db_index\': \'True\'}),\\n            \'created_at\': (\'django.db.models.fields.DateTimeField\', [], {\'auto_now_add\': \'True\', \'db_index\': \'True\', \'blank\': \'True\'}),\\n            \'extension\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'8\', \'blank\': \'True\'}),\\n            \'fax\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'35\', \'blank\': \'True\'}),\\n            \'fax_country_code\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'3\', \'blank\': \'True\'}),\\n            \'first_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'30\', \'db_index\': \'True\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'last_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\', \'db_index\': \'True\'}),\\n            \'middle_name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'30\', \'blank\': \'True\'}),\\n            \'phone\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'35\'}),\\n            \'phone_country_code\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'3\', \'db_index\': \'True\'}),\\n            \'postal_code\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'16\', \'blank\': \'True\'}),\\n            \'salutation\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'50\', \'blank\': \'True\'}),\\n            \'state\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'20\', \'blank\': \'True\'}),\\n            \'suffix\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'255\', \'blank\': \'True\'}),\\n            \'updated_at\': (\'django.db.models.fields.DateTimeField\', [], {\'auto_now\': \'True\', \'db_index\': \'True\', \'blank\': \'True\'}),\\n            \'user\': (\'django.db.models.fields.related.ForeignKey\', [], {\'default\': \'None\', \'to\': \\"orm[\'auth.User\']\\", \'unique\': \'True\'}),\\n            \'user_updated_at\': (\'django.db.models.fields.DateTimeField\', [], {\'db_index\': \'True\'})\\n        },\\n        \'student.userprofile\': {\\n            \'Meta\': {\'object_name\': \'UserProfile\', \'db_table\': \\"\'auth_userprofile\'\\"},\\n            \'courseware\': (\'django.db.models.fields.CharField\', [], {\'default\': \\"\'course.xml\'\\", \'max_length\': \'255\', \'blank\': \'True\'}),\\n            \'gender\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'6\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'goals\': (\'django.db.models.fields.TextField\', [], {\'null\': \'True\', \'blank\': \'True\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'language\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'255\', \'blank\': \'True\'}),\\n            \'level_of_education\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'6\', \'null\': \'True\', \'blank\': \'True\'}),\\n            \'location\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'255\', \'blank\': \'True\'}),\\n            \'mailing_address\': (\'django.db.models.fields.TextField\', [], {\'null\': \'True\', \'blank\': \'True\'}),\\n            \'meta\': (\'django.db.models.fields.TextField\', [], {\'blank\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'db_index\': \'True\', \'max_length\': \'255\', \'blank\': \'True\'}),\\n            \'user\': (\'django.db.models.fields.related.OneToOneField\', [], {\'related_name\': \\"\'profile\'\\", \'unique\': \'True\', \'to\': \\"orm[\'auth.User\']\\"}),\\n            \'year_of_birth\': (\'django.db.models.fields.IntegerField\', [], {\'db_index\': \'True\', \'null\': \'True\', \'blank\': \'True\'})\\n        },\\n        \'student.usertestgroup\': {\\n            \'Meta\': {\'object_name\': \'UserTestGroup\'},\\n            \'description\': (\'django.db.models.fields.TextField\', [], {\'blank\': \'True\'}),\\n            \'id\': (\'django.db.models.fields.AutoField\', [], {\'primary_key\': \'True\'}),\\n            \'name\': (\'django.db.models.fields.CharField\', [], {\'max_length\': \'32\', \'db_index\': \'True\'}),\\n            \'users\': (\'django.db.models.fields.related.ManyToManyField\', [], {\'to\': \\"orm[\'auth.User\']\\", \'db_index\': \'True\', \'symmetrical\': \'False\'})\\n        }\\n    }\\n\\n    complete_apps = [\'student\']\\n"}\n'
line: b'{"repo_name":"Syrcon/servo","ref":"refs/heads/master","path":"tests/wpt/web-platform-tests/tools/serve/serve.py","content":"# -*- coding: utf-8 -*-\\nimport argparse\\nimport json\\nimport os\\nimport signal\\nimport socket\\nimport sys\\nimport threading\\nimport time\\nimport traceback\\nimport urllib2\\nimport uuid\\nfrom collections import defaultdict, OrderedDict\\nfrom multiprocessing import Process, Event\\n\\nfrom .. import localpaths\\n\\nimport sslutils\\nfrom wptserve import server as wptserve, handlers\\nfrom wptserve import stash\\nfrom wptserve.logger import set_logger\\nfrom mod_pywebsocket import standalone as pywebsocket\\n\\nrepo_root = localpaths.repo_root\\n\\nclass WorkersHandler(object):\\n    def __init__(self):\\n        self.handler = handlers.handler(self.handle_request)\\n\\n    def __call__(self, request, response):\\n        return self.handler(request, response)\\n\\n    def handle_request(self, request, response):\\n        worker_path = request.url_parts.path.replace(\\".worker\\", \\".worker.js\\")\\n        return \\"\\"\\"\\u003c!doctype html\\u003e\\n\\u003cmeta charset=utf-8\\u003e\\n\\u003cscript src=\\"/resources/testharness.js\\"\\u003e\\u003c/script\\u003e\\n\\u003cscript src=\\"/resources/testharnessreport.js\\"\\u003e\\u003c/script\\u003e\\n\\u003cdiv id=log\\u003e\\u003c/div\\u003e\\n\\u003cscript\\u003e\\nfetch_tests_from_worker(new Worker(\\"%s\\"));\\n\\u003c/script\\u003e\\n\\"\\"\\" % (worker_path,)\\n\\nrewrites = [(\\"GET\\", \\"/resources/WebIDLParser.js\\", \\"/resources/webidl2/lib/webidl2.js\\")]\\n\\nsubdomains = [u\\"www\\",\\n              u\\"www1\\",\\n              u\\"www2\\",\\n              u\\"\xe5\xa4\xa9\xe6\xb0\x97\xe3\x81\xae\xe8\x89\xaf\xe3\x81\x84\xe6\x97\xa5\\",\\n              u\\"\xc3\xa9l\xc3\xa8ve\\"]\\n\\nclass RoutesBuilder(object):\\n    def __init__(self):\\n        self.forbidden_override = [(\\"GET\\", \\"/tools/runner/*\\", handlers.file_handler),\\n                                   (\\"POST\\", \\"/tools/runner/update_manifest.py\\",\\n                                    handlers.python_script_handler)]\\n\\n        self.forbidden = [(\\"*\\", \\"/_certs/*\\", handlers.ErrorHandler(404)),\\n                          (\\"*\\", \\"/tools/*\\", handlers.ErrorHandler(404)),\\n                          (\\"*\\", \\"{spec}/tools/*\\", handlers.ErrorHandler(404)),\\n                          (\\"*\\", \\"/serve.py\\", handlers.ErrorHandler(404))]\\n\\n        self.static = [(\\"GET\\", \\"*.worker\\", WorkersHandler())]\\n\\n        self.mountpoint_routes = OrderedDict()\\n\\n        self.add_mount_point(\\"/\\", None)\\n\\n    def get_routes(self):\\n        routes = self.forbidden_override + self.forbidden + self.static\\n        # Using reversed here means that mount points that are added later\\n        # get higher priority. This makes sense since / is typically added\\n        # first.\\n        for item in reversed(self.mountpoint_routes.values()):\\n            routes.extend(item)\\n        return routes\\n\\n    def add_static(self, path, format_args, content_type, route):\\n        handler = handlers.StaticHandler(path, format_args, content_type)\\n        self.static.append((b\\"GET\\", str(route), handler))\\n\\n    def add_mount_point(self, url_base, path):\\n        url_base = \\"/%s/\\" % url_base.strip(\\"/\\") if url_base != \\"/\\" else \\"/\\"\\n\\n        self.mountpoint_routes[url_base] = []\\n\\n        routes = [(\\"GET\\", \\"*.asis\\", handlers.AsIsHandler),\\n                  (\\"*\\", \\"*.py\\", handlers.PythonScriptHandler),\\n                  (\\"GET\\", \\"*\\", handlers.FileHandler)]\\n\\n        for (method, suffix, handler_cls) in routes:\\n            self.mountpoint_routes[url_base].append(\\n                (method,\\n                 b\\"%s%s\\" % (str(url_base) if url_base != \\"/\\" else \\"\\", str(suffix)),\\n                 handler_cls(base_path=path, url_base=url_base)))\\n\\n\\ndef default_routes():\\n    return RoutesBuilder().get_routes()\\n\\n\\ndef setup_logger(level):\\n    import logging\\n    global logger\\n    logger = logging.getLogger(\\"web-platform-tests\\")\\n    logging.basicConfig(level=getattr(logging, level.upper()))\\n    set_logger(logger)\\n\\n\\ndef open_socket(port):\\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n    if port != 0:\\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\\n    sock.bind((\'127.0.0.1\', port))\\n    sock.listen(5)\\n    return sock\\n\\n\\ndef get_port():\\n    free_socket = open_socket(0)\\n    port = free_socket.getsockname()[1]\\n    logger.debug(\\"Going to use port %s\\" % port)\\n    free_socket.close()\\n    return port\\n\\n\\nclass ServerProc(object):\\n    def __init__(self):\\n        self.proc = None\\n        self.daemon = None\\n        self.stop = Event()\\n\\n    def start(self, init_func, host, port, paths, routes, bind_hostname, external_config,\\n              ssl_config, **kwargs):\\n        self.proc = Process(target=self.create_daemon,\\n                            args=(init_func, host, port, paths, routes, bind_hostname,\\n                                  external_config, ssl_config))\\n        self.proc.daemon = True\\n        self.proc.start()\\n\\n    def create_daemon(self, init_func, host, port, paths, routes, bind_hostname,\\n                      external_config, ssl_config, **kwargs):\\n        try:\\n            self.daemon = init_func(host, port, paths, routes, bind_hostname, external_config,\\n                                    ssl_config, **kwargs)\\n        except socket.error:\\n            print \\u003e\\u003e sys.stderr, \\"Socket error on port %s\\" % port\\n            raise\\n        except:\\n            print \\u003e\\u003e sys.stderr, traceback.format_exc()\\n            raise\\n\\n        if self.daemon:\\n            try:\\n                self.daemon.start(block=False)\\n                try:\\n                    self.stop.wait()\\n                except KeyboardInterrupt:\\n                    pass\\n            except:\\n                print \\u003e\\u003e sys.stderr, traceback.format_exc()\\n                raise\\n\\n    def wait(self):\\n        self.stop.set()\\n        self.proc.join()\\n\\n    def kill(self):\\n        self.stop.set()\\n        self.proc.terminate()\\n        self.proc.join()\\n\\n    def is_alive(self):\\n        return self.proc.is_alive()\\n\\n\\ndef check_subdomains(host, paths, bind_hostname, ssl_config):\\n    port = get_port()\\n    subdomains = get_subdomains(host)\\n\\n    wrapper = ServerProc()\\n    wrapper.start(start_http_server, host, port, paths, default_routes(), bind_hostname,\\n                  None, ssl_config)\\n\\n    connected = False\\n    for i in range(10):\\n        try:\\n            urllib2.urlopen(\\"http://%s:%d/\\" % (host, port))\\n            connected = True\\n            break\\n        except urllib2.URLError:\\n            time.sleep(1)\\n\\n    if not connected:\\n        logger.critical(\\"Failed to connect to test server on http://%s:%s You may need to edit /etc/hosts or similar\\" % (host, port))\\n        sys.exit(1)\\n\\n    for subdomain, (punycode, host) in subdomains.iteritems():\\n        domain = \\"%s.%s\\" % (punycode, host)\\n        try:\\n            urllib2.urlopen(\\"http://%s:%d/\\" % (domain, port))\\n        except Exception as e:\\n            logger.critical(\\"Failed probing domain %s. You may need to edit /etc/hosts or similar.\\" % domain)\\n            sys.exit(1)\\n\\n    wrapper.wait()\\n\\n\\ndef get_subdomains(host):\\n    #This assumes that the tld is ascii-only or already in punycode\\n    return {subdomain: (subdomain.encode(\\"idna\\"), host)\\n            for subdomain in subdomains}\\n\\n\\ndef start_servers(host, ports, paths, routes, bind_hostname, external_config, ssl_config,\\n                  **kwargs):\\n    servers = defaultdict(list)\\n    for scheme, ports in ports.iteritems():\\n        assert len(ports) == {\\"http\\":2}.get(scheme, 1)\\n\\n        for port in ports:\\n            if port is None:\\n                continue\\n            init_func = {\\"http\\":start_http_server,\\n                         \\"https\\":start_https_server,\\n                         \\"ws\\":start_ws_server,\\n                         \\"wss\\":start_wss_server}[scheme]\\n\\n            server_proc = ServerProc()\\n            server_proc.start(init_func, host, port, paths, routes, bind_hostname,\\n                              external_config, ssl_config, **kwargs)\\n            servers[scheme].append((port, server_proc))\\n\\n    return servers\\n\\n\\ndef start_http_server(host, port, paths, routes, bind_hostname, external_config, ssl_config,\\n                      **kwargs):\\n    return wptserve.WebTestHttpd(host=host,\\n                                 port=port,\\n                                 doc_root=paths[\\"doc_root\\"],\\n                                 routes=routes,\\n                                 rewrites=rewrites,\\n                                 bind_hostname=bind_hostname,\\n                                 config=external_config,\\n                                 use_ssl=False,\\n                                 key_file=None,\\n                                 certificate=None,\\n                                 latency=kwargs.get(\\"latency\\"))\\n\\n\\ndef start_https_server(host, port, paths, routes, bind_hostname, external_config, ssl_config,\\n                       **kwargs):\\n    return wptserve.WebTestHttpd(host=host,\\n                                 port=port,\\n                                 doc_root=paths[\\"doc_root\\"],\\n                                 routes=routes,\\n                                 rewrites=rewrites,\\n                                 bind_hostname=bind_hostname,\\n                                 config=external_config,\\n                                 use_ssl=True,\\n                                 key_file=ssl_config[\\"key_path\\"],\\n                                 certificate=ssl_config[\\"cert_path\\"],\\n                                 encrypt_after_connect=ssl_config[\\"encrypt_after_connect\\"],\\n                                 latency=kwargs.get(\\"latency\\"))\\n\\n\\nclass WebSocketDaemon(object):\\n    def __init__(self, host, port, doc_root, handlers_root, log_level, bind_hostname,\\n                 ssl_config):\\n        self.host = host\\n        cmd_args = [\\"-p\\", port,\\n                    \\"-d\\", doc_root,\\n                    \\"-w\\", handlers_root,\\n                    \\"--log-level\\", log_level]\\n\\n        if ssl_config is not None:\\n            # This is usually done through pywebsocket.main, however we\'re\\n            # working around that to get the server instance and manually\\n            # setup the wss server.\\n            if pywebsocket._import_ssl():\\n                tls_module = pywebsocket._TLS_BY_STANDARD_MODULE\\n            elif pywebsocket._import_pyopenssl():\\n                tls_module = pywebsocket._TLS_BY_PYOPENSSL\\n            else:\\n                print \\"No SSL module available\\"\\n                sys.exit(1)\\n\\n            cmd_args += [\\"--tls\\",\\n                         \\"--private-key\\", ssl_config[\\"key_path\\"],\\n                         \\"--certificate\\", ssl_config[\\"cert_path\\"],\\n                         \\"--tls-module\\", tls_module]\\n\\n        if (bind_hostname):\\n            cmd_args = [\\"-H\\", host] + cmd_args\\n        opts, args = pywebsocket._parse_args_and_config(cmd_args)\\n        opts.cgi_directories = []\\n        opts.is_executable_method = None\\n        self.server = pywebsocket.WebSocketServer(opts)\\n        ports = [item[0].getsockname()[1] for item in self.server._sockets]\\n        assert all(item == ports[0] for item in ports)\\n        self.port = ports[0]\\n        self.started = False\\n        self.server_thread = None\\n\\n    def start(self, block=False):\\n        self.started = True\\n        if block:\\n            self.server.serve_forever()\\n        else:\\n            self.server_thread = threading.Thread(target=self.server.serve_forever)\\n            self.server_thread.setDaemon(True)  # don\'t hang on exit\\n            self.server_thread.start()\\n\\n    def stop(self):\\n        \\"\\"\\"\\n        Stops the server.\\n\\n        If the server is not running, this method has no effect.\\n        \\"\\"\\"\\n        if self.started:\\n            try:\\n                self.server.shutdown()\\n                self.server.server_close()\\n                self.server_thread.join()\\n                self.server_thread = None\\n            except AttributeError:\\n                pass\\n            self.started = False\\n        self.server = None\\n\\n\\ndef start_ws_server(host, port, paths, routes, bind_hostname, external_config, ssl_config,\\n                    **kwargs):\\n    return WebSocketDaemon(host,\\n                           str(port),\\n                           repo_root,\\n                           paths[\\"ws_doc_root\\"],\\n                           \\"debug\\",\\n                           bind_hostname,\\n                           ssl_config = None)\\n\\n\\ndef start_wss_server(host, port, paths, routes, bind_hostname, external_config, ssl_config,\\n                     **kwargs):\\n    return WebSocketDaemon(host,\\n                           str(port),\\n                           repo_root,\\n                           paths[\\"ws_doc_root\\"],\\n                           \\"debug\\",\\n                           bind_hostname,\\n                           ssl_config)\\n\\n\\ndef get_ports(config, ssl_environment):\\n    rv = defaultdict(list)\\n    for scheme, ports in config[\\"ports\\"].iteritems():\\n        for i, port in enumerate(ports):\\n            if scheme in [\\"wss\\", \\"https\\"] and not ssl_environment.ssl_enabled:\\n                port = None\\n            if port == \\"auto\\":\\n                port = get_port()\\n            else:\\n                port = port\\n            rv[scheme].append(port)\\n    return rv\\n\\n\\n\\ndef normalise_config(config, ports):\\n    host = config[\\"external_host\\"] if config[\\"external_host\\"] else config[\\"host\\"]\\n    domains = get_subdomains(host)\\n    ports_ = {}\\n    for scheme, ports_used in ports.iteritems():\\n        ports_[scheme] = ports_used\\n\\n    for key, value in domains.iteritems():\\n        domains[key] = \\".\\".join(value)\\n\\n    domains[\\"\\"] = host\\n\\n    ports_ = {}\\n    for scheme, ports_used in ports.iteritems():\\n        ports_[scheme] = ports_used\\n\\n    return {\\"host\\": host,\\n            \\"domains\\": domains,\\n            \\"ports\\": ports_}\\n\\n\\ndef get_ssl_config(config, external_domains, ssl_environment):\\n    key_path, cert_path = ssl_environment.host_cert_path(external_domains)\\n    return {\\"key_path\\": key_path,\\n            \\"cert_path\\": cert_path,\\n            \\"encrypt_after_connect\\": config[\\"ssl\\"][\\"encrypt_after_connect\\"]}\\n\\ndef start(config, ssl_environment, routes, **kwargs):\\n    host = config[\\"host\\"]\\n    domains = get_subdomains(host)\\n    ports = get_ports(config, ssl_environment)\\n    bind_hostname = config[\\"bind_hostname\\"]\\n\\n    paths = {\\"doc_root\\": config[\\"doc_root\\"],\\n             \\"ws_doc_root\\": config[\\"ws_doc_root\\"]}\\n\\n    external_config = normalise_config(config, ports)\\n\\n    ssl_config = get_ssl_config(config, external_config[\\"domains\\"].values(), ssl_environment)\\n\\n    if config[\\"check_subdomains\\"]:\\n        check_subdomains(host, paths, bind_hostname, ssl_config)\\n\\n    servers = start_servers(host, ports, paths, routes, bind_hostname, external_config,\\n                            ssl_config, **kwargs)\\n\\n    return external_config, servers\\n\\n\\ndef iter_procs(servers):\\n    for servers in servers.values():\\n        for port, server in servers:\\n            yield server.proc\\n\\n\\ndef value_set(config, key):\\n    return key in config and config[key] is not None\\n\\n\\ndef get_value_or_default(config, key, default=None):\\n    return config[key] if value_set(config, key) else default\\n\\n\\ndef set_computed_defaults(config):\\n    if not value_set(config, \\"doc_root\\"):\\n        config[\\"doc_root\\"] = repo_root\\n\\n    if not value_set(config, \\"ws_doc_root\\"):\\n        root = get_value_or_default(config, \\"doc_root\\", default=repo_root)\\n        config[\\"ws_doc_root\\"] = os.path.join(root, \\"websockets\\", \\"handlers\\")\\n\\n\\ndef merge_json(base_obj, override_obj):\\n    rv = {}\\n    for key, value in base_obj.iteritems():\\n        if key not in override_obj:\\n            rv[key] = value\\n        else:\\n            if isinstance(value, dict):\\n                rv[key] = merge_json(value, override_obj[key])\\n            else:\\n                rv[key] = override_obj[key]\\n    return rv\\n\\n\\ndef get_ssl_environment(config):\\n    implementation_type = config[\\"ssl\\"][\\"type\\"]\\n    cls = sslutils.environments[implementation_type]\\n    try:\\n        kwargs = config[\\"ssl\\"][implementation_type].copy()\\n    except KeyError:\\n        raise ValueError(\\"%s is not a vaid ssl type.\\" % implementation_type)\\n    return cls(logger, **kwargs)\\n\\n\\ndef load_config(default_path, override_path=None, **kwargs):\\n    if os.path.exists(default_path):\\n        with open(default_path) as f:\\n            base_obj = json.load(f)\\n    else:\\n        raise ValueError(\\"Config path %s does not exist\\" % default_path)\\n\\n    if os.path.exists(override_path):\\n        with open(override_path) as f:\\n            override_obj = json.load(f)\\n    else:\\n        override_obj = {}\\n    rv = merge_json(base_obj, override_obj)\\n\\n    if kwargs.get(\\"config_path\\"):\\n        other_path = os.path.abspath(os.path.expanduser(kwargs.get(\\"config_path\\")))\\n        if os.path.exists(other_path):\\n            base_obj = rv\\n            with open(other_path) as f:\\n                override_obj = json.load(f)\\n            rv = merge_json(base_obj, override_obj)\\n        else:\\n            raise ValueError(\\"Config path %s does not exist\\" % other_path)\\n\\n    overriding_path_args = [(\\"doc_root\\", \\"Document root\\"),\\n                            (\\"ws_doc_root\\", \\"WebSockets document root\\")]\\n    for key, title in overriding_path_args:\\n        value = kwargs.get(key)\\n        if value is None:\\n            continue\\n        value = os.path.abspath(os.path.expanduser(value))\\n        if not os.path.exists(value):\\n            raise ValueError(\\"%s path %s does not exist\\" % (title, value))\\n        rv[key] = value\\n\\n    set_computed_defaults(rv)\\n    return rv\\n\\n\\ndef get_parser():\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\\"--latency\\", type=int,\\n                        help=\\"Artificial latency to add before sending http responses, in ms\\")\\n    parser.add_argument(\\"--config\\", action=\\"store\\", dest=\\"config_path\\",\\n                        help=\\"Path to external config file\\")\\n    parser.add_argument(\\"--doc_root\\", action=\\"store\\", dest=\\"doc_root\\",\\n                        help=\\"Path to document root. Overrides config.\\")\\n    parser.add_argument(\\"--ws_doc_root\\", action=\\"store\\", dest=\\"ws_doc_root\\",\\n                        help=\\"Path to WebSockets document root. Overrides config.\\")\\n    return parser\\n\\n\\ndef main():\\n    kwargs = vars(get_parser().parse_args())\\n    config = load_config(\\"config.default.json\\",\\n                         \\"config.json\\",\\n                         **kwargs)\\n\\n    setup_logger(config[\\"log_level\\"])\\n\\n    with stash.StashServer((config[\\"host\\"], get_port()), authkey=str(uuid.uuid4())):\\n        with get_ssl_environment(config) as ssl_env:\\n            config_, servers = start(config, ssl_env, default_routes(), **kwargs)\\n\\n            try:\\n                while any(item.is_alive() for item in iter_procs(servers)):\\n                    for item in iter_procs(servers):\\n                        item.join(1)\\n            except KeyboardInterrupt:\\n                logger.info(\\"Shutting down\\")\\n"}\n'
line: b'{"repo_name":"xadahiya/django","ref":"refs/heads/master","path":"django/contrib/admin/actions.py","content":"\\"\\"\\"\\nBuilt-in, globally-available admin actions.\\n\\"\\"\\"\\n\\nfrom django.contrib import messages\\nfrom django.contrib.admin import helpers\\nfrom django.contrib.admin.utils import get_deleted_objects, model_ngettext\\nfrom django.core.exceptions import PermissionDenied\\nfrom django.db import router\\nfrom django.template.response import TemplateResponse\\nfrom django.utils.encoding import force_text\\nfrom django.utils.translation import ugettext as _, ugettext_lazy\\n\\n\\ndef delete_selected(modeladmin, request, queryset):\\n    \\"\\"\\"\\n    Default action which deletes the selected objects.\\n\\n    This action first displays a confirmation page whichs shows all the\\n    deleteable objects, or, if the user has no permission one of the related\\n    childs (foreignkeys), a \\"permission denied\\" message.\\n\\n    Next, it deletes all selected objects and redirects back to the change list.\\n    \\"\\"\\"\\n    opts = modeladmin.model._meta\\n    app_label = opts.app_label\\n\\n    # Check that the user has delete permission for the actual model\\n    if not modeladmin.has_delete_permission(request):\\n        raise PermissionDenied\\n\\n    using = router.db_for_write(modeladmin.model)\\n\\n    # Populate deletable_objects, a data structure of all related objects that\\n    # will also be deleted.\\n    deletable_objects, model_count, perms_needed, protected = get_deleted_objects(\\n        queryset, opts, request.user, modeladmin.admin_site, using)\\n\\n    # The user has already confirmed the deletion.\\n    # Do the deletion and return a None to display the change list view again.\\n    if request.POST.get(\'post\'):\\n        if perms_needed:\\n            raise PermissionDenied\\n        n = queryset.count()\\n        if n:\\n            for obj in queryset:\\n                obj_display = force_text(obj)\\n                modeladmin.log_deletion(request, obj, obj_display)\\n            queryset.delete()\\n            modeladmin.message_user(request, _(\\"Successfully deleted %(count)d %(items)s.\\") % {\\n                \\"count\\": n, \\"items\\": model_ngettext(modeladmin.opts, n)\\n            }, messages.SUCCESS)\\n        # Return None to display the change list page again.\\n        return None\\n\\n    if len(queryset) == 1:\\n        objects_name = force_text(opts.verbose_name)\\n    else:\\n        objects_name = force_text(opts.verbose_name_plural)\\n\\n    if perms_needed or protected:\\n        title = _(\\"Cannot delete %(name)s\\") % {\\"name\\": objects_name}\\n    else:\\n        title = _(\\"Are you sure?\\")\\n\\n    context = dict(\\n        modeladmin.admin_site.each_context(request),\\n        title=title,\\n        objects_name=objects_name,\\n        deletable_objects=[deletable_objects],\\n        model_count=dict(model_count).items(),\\n        queryset=queryset,\\n        perms_lacking=perms_needed,\\n        protected=protected,\\n        opts=opts,\\n        action_checkbox_name=helpers.ACTION_CHECKBOX_NAME,\\n    )\\n\\n    request.current_app = modeladmin.admin_site.name\\n\\n    # Display the confirmation page\\n    return TemplateResponse(request, modeladmin.delete_selected_confirmation_template or [\\n        \\"admin/%s/%s/delete_selected_confirmation.html\\" % (app_label, opts.model_name),\\n        \\"admin/%s/delete_selected_confirmation.html\\" % app_label,\\n        \\"admin/delete_selected_confirmation.html\\"\\n    ], context)\\n\\ndelete_selected.short_description = ugettext_lazy(\\"Delete selected %(verbose_name_plural)s\\")\\n"}\n'
line: b'{"repo_name":"wsmith323/django","ref":"refs/heads/master","path":"tests/save_delete_hooks/tests.py","content":"from __future__ import unicode_literals\\n\\nfrom django.test import TestCase\\nfrom django.utils import six\\n\\nfrom .models import Person\\n\\n\\nclass SaveDeleteHookTests(TestCase):\\n    def test_basic(self):\\n        p = Person(first_name=\\"John\\", last_name=\\"Smith\\")\\n        self.assertEqual(p.data, [])\\n        p.save()\\n        self.assertEqual(p.data, [\\n            \\"Before save\\",\\n            \\"After save\\",\\n        ])\\n\\n        self.assertQuerysetEqual(\\n            Person.objects.all(), [\\n                \\"John Smith\\",\\n            ],\\n            six.text_type\\n        )\\n\\n        p.delete()\\n        self.assertEqual(p.data, [\\n            \\"Before save\\",\\n            \\"After save\\",\\n            \\"Before deletion\\",\\n            \\"After deletion\\",\\n        ])\\n        self.assertQuerysetEqual(Person.objects.all(), [])\\n"}\n'
line: b'{"repo_name":"sparkslabs/kamaelia","ref":"refs/heads/master","path":"Sketches/CL/Topology/src/RelationTopology/Util/RelationAttributeParsing.py","content":"#!/usr/bin/env python\\n# -*- coding: utf-8 -*-\\n\\n# Copyright 2010 British Broadcasting Corporation and Kamaelia Contributors(1)\\n#\\n# (1) Kamaelia Contributors are listed in the AUTHORS file and at\\n#     http://www.kamaelia.org/AUTHORS - please extend this file,\\n#     not this notice.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \\"License\\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\"\\"\\"\\\\\\n===============================================================\\nParse entities, attributes and relations definition received\\n===============================================================\\n\\nParse entities and relations definition received, one line one time.\\n\\n1. Definition format\\n1.) Empty line (including any number of white spaces)\\n2.) Line starting with # to comment\\n3.) Entity definition\\nExample:\\n--------\\nperson mum\\nperson dad gender=male,shape=rect,width=80,height=80\\nperson son gender=\\"male\\",photo=\\"../Files/son.gif,width=60,height=60\\"\\nperson daughter radius=100\\n4.) Relation definition\\nExample: \\n--------\\nchildof(mum, son)\\n\\n2. NOTE:\\n1.) Any number of spaces can exist before, after and between the above line\\nExample:\\n--------\\n  person    mum  \\n     childof  (  mum  , son  )  \\n2.) Parse one line one time and then send out\\n3.) Entity definition needs to come before relation definition \\nif the relations definition uses the entity\\n4.) When encountering repeated entity, it will update its attributes rather than\\ncreate a new one.       \\n\\"\\"\\"\\n\\ndef parseEntity(entityLine):\\n    \\"\\"\\" parse entity line \\"\\"\\"\\n    result = entityLine.split()\\n    #entity_ID = result[0]+\'_\'+result[1]\\n    entity_name = result[1]\\n    #particle = \'-\'\\n    particle = \'GenericParticle\'\\n    if len(result) == 3:\\n        attributes = result[2]\\n        #attributes = attributes.lower()\\n        attributes = attributes.replace(\'gender\',\'color\')\\n        attributes = attributes.replace(\'female\',\'pink\')\\n        attributes = attributes.replace(\'male\',\'blue\')\\n        attributes = attributes.replace(\'photo\',\'pic\')\\n        attributes = attributes + \',type=\' + result[0]\\n    else:\\n        attributes = \'type=\' + result[0]               \\n    return \\"ADD NODE %s %s auto %s %s\\" % (entity_name,entity_name,particle,attributes)\\n\\ndef parseUpdatedEntity(entityLine):\\n    \\"\\"\\" parse entity line \\"\\"\\"\\n    result = entityLine.split()\\n    #entity_ID = result[0]+\'_\'+result[1]\\n    entity_name = result[1]\\n    #particle = \'-\'\\n    #particle = \'GenericParticle\'\\n    if len(result) == 3:\\n        attributes = result[2]\\n        #attributes = attributes.lower()\\n        attributes = attributes.replace(\'gender\',\'color\')\\n        attributes = attributes.replace(\'female\',\'pink\')\\n        attributes = attributes.replace(\'male\',\'blue\')\\n        attributes = attributes.replace(\'photo\',\'pic\')\\n        attributes = attributes.replace(\'name\',\'label\')\\n    else:\\n        attributes = \'label=\' + entity_name              \\n    return \\"UPDATE NODE %s %s\\" % (entity_name,attributes)\\n\\ndef parseRelation(relationLine):\\n    \\"\\"\\" parse relation line \\"\\"\\"\\n    result = relationLine.split(\'(\')\\n    relation = result[0].strip()\\n    entities_str = result[1].rstrip(\')\')\\n    entities_list = entities_str.split(\',\')\\n    src = entities_list[0].strip()\\n    dst = entities_list[1].strip()\\n    return \\"ADD LINK %s %s %s\\" % (src,dst,relation)\\n\\n\\n        \\nimport re\\n\\nimport Axon\\nfrom Axon.Ipc import producerFinished, shutdownMicroprocess\\n\\nclass RelationAttributeParser(Axon.Component.component):\\n    \\"\\"\\"\\\\\\n======================================================================\\nA component to parse entities, attributes and relations definition\\n======================================================================\\n\\"\\"\\"\\n    def shutdown(self):\\n        \\"\\"\\" shutdown method: define when to shun down\\"\\"\\"\\n        while self.dataReady(\\"control\\"):\\n            data = self.recv(\\"control\\")\\n            if isinstance(data, producerFinished) or isinstance(data, shutdownMicroprocess):\\n                self.shutdown_mess = data\\n                return True\\n        return False\\n      \\n    def main(self):\\n        \\"\\"\\" main method: do stuff \\"\\"\\"\\n        \\n        previousNodes = []  \\n        \\n        # Put all codes within the loop, so that others can be run even it doesn\'t shut down\\n        while not self.shutdown():\\n            X = []\\n            links = []\\n            nodes = []\\n            updatedNodes = []\\n            while not self.anyReady():\\n                self.pause()\\n                yield 1\\n    \\n            while self.dataReady(\\"inbox\\"):\\n                L = self.recv(\\"inbox\\")\\n                if L.strip() == \\"\\": continue # empty line\\n                if L.lstrip()[0] == \\"#\\": continue # comment\\n                X.append(L.strip())\\n            #yield 1\\n\\n            for item in X:            \\n                if re.match(\'(.+)\\\\((.+),(.+)\\\\)\',item): # relation\\n                    command = parseRelation(item)\\n                    links.append(command)\\n                else:\\n                    isRepeated = False\\n                    for node in previousNodes:\\n                        if item.split()[1] == node.split()[2]:\\n                            isRepeated = True\\n                    if not isRepeated: # new entity\\n                        command = parseEntity(item)\\n                        nodes.append(command)        \\n                        previousNodes.append(command)\\n                    else: # old entity\\n                        command = parseUpdatedEntity(item)\\n                        updatedNodes.append(command)\\n            #yield 1\\n            for node in nodes:\\n                self.send(node, \\"outbox\\")\\n            for updatedNode in updatedNodes:\\n                self.send(updatedNode, \\"outbox\\")\\n            for link in links:\\n                self.send(link, \\"outbox\\")\\n            yield 1\\n            \\n        \\n        self.send(self.shutdown_mess,\\"signal\\")\\n        \\nif __name__ == \\"__main__\\":\\n    from Kamaelia.Util.DataSource import DataSource\\n    from Kamaelia.Visualisation.PhysicsGraph.lines_to_tokenlists import lines_to_tokenlists\\n    from Kamaelia.Util.Console import ConsoleReader,ConsoleEchoer\\n    from GenericTopologyViewer import GenericTopologyViewer\\n    from Kamaelia.Chassis.Graphline import Graphline\\n    \\n    # Data can be from both DataSource and console inputs\\n    Graphline(\\n        CONSOLEREADER = ConsoleReader(),\\n        DATASOURCE = DataSource([\\"  person  mum   gender=female,photo=../Files/mum.jpg,width=80,height=80 \\", \'  \', \\"\\"\\"   \\n                    \\"\\"\\", \'person dad gender=male,shape=rect,width=80,height=80\', \\n                    \'  person  son   gender=male,photo=../Files/son.gif,width=60,height=60\',\\n                    \'person son photo=../Files/son1.gif\',\\n                     \'person daughter radius=20\', \'person daughter radius=100\',\\n                     \' childof  (  mum  , son  ) \', \'childof(mum, daughter)\',\\n                     \'childof(dad, son)\', \'childof(dad, daughter)\']),\\n        PARSER = RelationAttributeParser(),\\n        TOKENS = lines_to_tokenlists(),\\n        VIEWER = GenericTopologyViewer(),\\n        CONSOLEECHOER = ConsoleEchoer(),\\n    linkages = {\\n        (\\"CONSOLEREADER\\",\\"outbox\\") : (\\"PARSER\\",\\"inbox\\"),\\n        (\\"DATASOURCE\\",\\"outbox\\") : (\\"PARSER\\",\\"inbox\\"),\\n        (\\"PARSER\\",\\"outbox\\") : (\\"TOKENS\\",\\"inbox\\"),\\n        (\\"TOKENS\\",\\"outbox\\")   : (\\"VIEWER\\",\\"inbox\\"),\\n        (\\"VIEWER\\",\\"outbox\\")  : (\\"CONSOLEECHOER\\",\\"inbox\\"),\\n        \\n    }\\n).run()"}\n'
line: b'{"repo_name":"dssg/wikienergy","ref":"refs/heads/master","path":"disaggregator/build/pandas/pandas/io/tests/__init__.py","content":"\\ndef setUp():\\n    import socket\\n    socket.setdefaulttimeout(5)\\n"}\n'
line: b'{"repo_name":"soulxu/libvirt-xuhj","ref":"refs/heads/master","path":"src/esx/esx_vi_generator.py","content":"#!/usr/bin/env python\\n\\n#\\n# esx_vi_generator.py: generates most of the SOAP type mapping code\\n#\\n# Copyright (C) 2010-2011 Matthias Bolte \\u003cmatthias.bolte@googlemail.com\\u003e\\n#\\n# This library is free software; you can redistribute it and/or\\n# modify it under the terms of the GNU Lesser General Public\\n# License as published by the Free Software Foundation; either\\n# version 2.1 of the License, or (at your option) any later version.\\n#\\n# This library is distributed in the hope that it will be useful,\\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\\n# Lesser General Public License for more details.\\n#\\n# You should have received a copy of the GNU Lesser General Public\\n# License along with this library; if not, write to the Free Software\\n# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307  USA\\n#\\n\\nimport sys\\nimport os\\nimport os.path\\n\\n\\n\\nOCCURRENCE__REQUIRED_ITEM = \\"r\\"\\nOCCURRENCE__REQUIRED_LIST = \\"rl\\"\\nOCCURRENCE__OPTIONAL_ITEM = \\"o\\"\\nOCCURRENCE__OPTIONAL_LIST = \\"ol\\"\\nOCCURRENCE__IGNORED = \\"i\\"\\n\\nvalid_occurrences = [OCCURRENCE__REQUIRED_ITEM,\\n                     OCCURRENCE__REQUIRED_LIST,\\n                     OCCURRENCE__OPTIONAL_ITEM,\\n                     OCCURRENCE__OPTIONAL_LIST,\\n                     OCCURRENCE__IGNORED]\\n\\nautobind_names = set()\\n\\nseparator = \\"/* \\" + (\\"* \\" * 37) + \\"*\\\\n\\"\\n\\n\\n\\ndef aligned(left, right, length=59):\\n    while len(left) \\u003c length:\\n        left += \\" \\"\\n\\n    return left + right\\n\\n\\n\\nclass Member:\\n    def __init__(self, type, occurrence):\\n        self.type = type\\n        self.occurrence = occurrence\\n\\n\\n    def is_enum(self):\\n        return self.type in predefined_enums or self.type in enums_by_name\\n\\n\\n    def is_object(self):\\n        return self.type in predefined_objects or self.type in objects_by_name\\n\\n\\n    def is_type_generated(self):\\n        return self.type in enums_by_name or self.type in objects_by_name\\n\\n\\n    def get_occurrence_comment(self):\\n        if self.occurrence == OCCURRENCE__REQUIRED_ITEM:\\n            return \\"/* required */\\"\\n        elif self.occurrence == OCCURRENCE__REQUIRED_LIST:\\n            return \\"/* required, list */\\"\\n        elif self.occurrence == OCCURRENCE__OPTIONAL_ITEM:\\n            return \\"/* optional */\\"\\n        elif self.occurrence == OCCURRENCE__OPTIONAL_LIST:\\n            return \\"/* optional, list */\\"\\n\\n        raise ValueError(\\"unknown occurrence value \'%s\'\\" % self.occurrence)\\n\\n\\n\\nclass Parameter(Member):\\n    def __init__(self, type, name, occurrence):\\n        Member.__init__(self, type, occurrence)\\n\\n        if \':\' in name and name.startswith(\\"_this\\"):\\n            self.name, self.autobind_name = name.split(\\":\\")\\n        else:\\n            self.name = name\\n            self.autobind_name = None\\n\\n\\n    def generate_parameter(self, is_last=False, is_header=True, offset=0):\\n        if self.occurrence == OCCURRENCE__IGNORED:\\n            raise ValueError(\\"invalid function parameter occurrence value \'%s\'\\"\\n                             % self.occurrence)\\n        elif self.autobind_name is not None:\\n            return \\"\\"\\n        else:\\n            string = \\"       \\"\\n            string += \\" \\" * offset\\n            string += \\"%s%s\\" % (self.get_type_string(), self.name)\\n\\n            if is_last:\\n                if is_header:\\n                    string += \\"); \\"\\n                else:\\n                    string += \\"), \\"\\n            else:\\n                string += \\", \\"\\n\\n            return aligned(string, self.get_occurrence_comment() + \\"\\\\n\\")\\n\\n\\n    def generate_return(self, offset = 0, end_of_line = \\";\\"):\\n        if self.occurrence == OCCURRENCE__IGNORED:\\n            raise ValueError(\\"invalid function parameter occurrence value \'%s\'\\"\\n                             % self.occurrence)\\n        else:\\n            string = \\"       \\"\\n            string += \\" \\" * offset\\n            string += \\"%s%s)%s\\" \\\\\\n                      % (self.get_type_string(True), self.name, end_of_line)\\n\\n            return aligned(string, self.get_occurrence_comment() + \\"\\\\n\\")\\n\\n\\n    def generate_require_code(self):\\n        if self.occurrence in [OCCURRENCE__REQUIRED_ITEM,\\n                               OCCURRENCE__REQUIRED_LIST]:\\n            return \\"    ESX_VI__METHOD__PARAMETER__REQUIRE(%s)\\\\n\\" % self.name\\n        else:\\n            return \\"\\"\\n\\n\\n    def generate_serialize_code(self):\\n        if self.occurrence in [OCCURRENCE__REQUIRED_LIST,\\n                               OCCURRENCE__OPTIONAL_LIST]:\\n            return \\"    ESX_VI__METHOD__PARAMETER__SERIALIZE_LIST(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n        elif self.type == \\"String\\":\\n            return \\"    ESX_VI__METHOD__PARAMETER__SERIALIZE_VALUE(String, %s)\\\\n\\" \\\\\\n                   % self.name\\n        else:\\n            return \\"    ESX_VI__METHOD__PARAMETER__SERIALIZE(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n\\n\\n    def get_type_string(self, as_return_value=False):\\n        string = \\"\\"\\n\\n        if self.type == \\"String\\" and \\\\\\n           self.occurrence not in [OCCURRENCE__REQUIRED_LIST,\\n                                   OCCURRENCE__OPTIONAL_LIST]:\\n            if as_return_value:\\n                string += \\"char *\\"\\n            else:\\n                string += \\"const char *\\"\\n        elif self.is_enum():\\n            string += \\"esxVI_%s \\" % self.type\\n        else:\\n            string += \\"esxVI_%s *\\" % self.type\\n\\n        if as_return_value:\\n            string += \\"*\\"\\n\\n        return string\\n\\n\\n    def get_occurrence_short_enum(self):\\n        if self.occurrence == OCCURRENCE__REQUIRED_ITEM:\\n            return \\"RequiredItem\\"\\n        elif self.occurrence == OCCURRENCE__REQUIRED_LIST:\\n            return \\"RequiredList\\"\\n        elif self.occurrence == OCCURRENCE__OPTIONAL_ITEM:\\n            return \\"OptionalItem\\"\\n        elif self.occurrence == OCCURRENCE__OPTIONAL_LIST:\\n            return \\"OptionalList\\"\\n\\n        raise ValueError(\\"unknown occurrence value \'%s\'\\" % self.occurrence)\\n\\n\\n\\nclass Method:\\n    def __init__(self, name, parameters, returns):\\n        self.name = name\\n        self.parameters = []\\n        self.autobind_parameter = None\\n        self.returns = returns\\n\\n        for parameter in parameters:\\n            if parameter.autobind_name is None:\\n                self.parameters.append(parameter)\\n            else:\\n                self.autobind_parameter = parameter\\n\\n\\n    def generate_header(self):\\n        header = \\"int esxVI_%s\\\\n\\" % self.name\\n        header += \\"      (esxVI_Context *ctx\\"\\n\\n        if len(self.parameters) \\u003e 0 or self.returns is not None:\\n            header += \\",\\\\n\\"\\n\\n            for parameter in self.parameters[:-1]:\\n                header += parameter.generate_parameter()\\n\\n            if self.returns is None:\\n                header += self.parameters[-1].generate_parameter(is_last=True)\\n            else:\\n                header += self.parameters[-1].generate_parameter()\\n                header += self.returns.generate_return()\\n        else:\\n            header += \\");\\\\n\\"\\n\\n        header += \\"\\\\n\\"\\n\\n        return header\\n\\n\\n    def generate_source(self):\\n        source = \\"/* esxVI_%s */\\\\n\\" % self.name\\n        source += \\"ESX_VI__METHOD(%s,\\" % self.name\\n\\n        if self.autobind_parameter is not None:\\n            autobind_names.add(self.autobind_parameter.autobind_name)\\n            source += \\" %s,\\\\n\\" % self.autobind_parameter.autobind_name\\n        else:\\n            source += \\" /* explicit _this */,\\\\n\\"\\n\\n        source += \\"               (esxVI_Context *ctx\\"\\n\\n        if len(self.parameters) \\u003e 0 or self.returns is not None:\\n            source += \\",\\\\n\\"\\n\\n            for parameter in self.parameters[:-1]:\\n                source += parameter.generate_parameter(is_header=False,\\n                                                       offset=9)\\n\\n            if self.returns is None:\\n                source += self.parameters[-1].generate_parameter(is_last=True,\\n                                                                 is_header=False,\\n                                                                 offset=9)\\n            else:\\n                source += self.parameters[-1].generate_parameter(is_header=False,\\n                                                                 offset=9)\\n                source += self.returns.generate_return(offset=9,\\n                                                       end_of_line=\\",\\")\\n        else:\\n            source += \\"),\\\\n\\"\\n\\n        if self.returns is None:\\n            source += \\"               void, /* nothing */, None,\\\\n\\"\\n        elif self.returns.type == \\"String\\":\\n            source += \\"               String, Value, %s,\\\\n\\" \\\\\\n                      % self.returns.get_occurrence_short_enum()\\n        else:\\n            source += \\"               %s, /* nothing */, %s,\\\\n\\" \\\\\\n                      % (self.returns.type,\\n                         self.returns.get_occurrence_short_enum())\\n\\n        source += \\"{\\\\n\\"\\n\\n        if self.autobind_parameter is not None:\\n            source += self.autobind_parameter.generate_require_code()\\n\\n        for parameter in self.parameters:\\n            source += parameter.generate_require_code()\\n\\n        source += \\"},\\\\n\\"\\n        source += \\"{\\\\n\\"\\n\\n        if self.autobind_parameter is not None:\\n            source += self.autobind_parameter.generate_serialize_code()\\n\\n        for parameter in self.parameters:\\n            source += parameter.generate_serialize_code()\\n\\n        source += \\"})\\\\n\\\\n\\\\n\\\\n\\"\\n\\n        return source\\n\\n\\n\\nclass Property(Member):\\n    def __init__(self, type, name, occurrence):\\n        Member.__init__(self, type, occurrence)\\n\\n        self.name = name\\n\\n\\n    def generate_struct_member(self):\\n        if self.occurrence == OCCURRENCE__IGNORED:\\n            return \\"    /* FIXME: %s is currently ignored */\\\\n\\" % self.name\\n        else:\\n            string = \\"    %s%s; \\" % (self.get_type_string(), self.name)\\n\\n            return aligned(string, self.get_occurrence_comment() + \\"\\\\n\\")\\n\\n\\n    def generate_free_code(self):\\n        if self.type == \\"String\\" and \\\\\\n           self.occurrence not in [OCCURRENCE__REQUIRED_LIST,\\n                                   OCCURRENCE__OPTIONAL_LIST,\\n                                   OCCURRENCE__IGNORED]:\\n            return \\"    VIR_FREE(item-\\u003e%s);\\\\n\\" % self.name\\n        elif self.is_enum():\\n            return \\"\\"\\n        else:\\n            if self.occurrence == OCCURRENCE__IGNORED:\\n                return \\"    /* FIXME: %s is currently ignored */\\\\n\\" % self.name\\n            else:\\n                return \\"    esxVI_%s_Free(\\u0026item-\\u003e%s);\\\\n\\" % (self.type, self.name)\\n\\n\\n    def generate_validate_code(self, managed=False):\\n        if managed:\\n            macro = \\"ESX_VI__TEMPLATE__PROPERTY__MANAGED_REQUIRE\\"\\n        else:\\n            macro = \\"ESX_VI__TEMPLATE__PROPERTY__REQUIRE\\"\\n\\n        if self.occurrence in [OCCURRENCE__REQUIRED_ITEM,\\n                               OCCURRENCE__REQUIRED_LIST]:\\n            return \\"    %s(%s)\\\\n\\" % (macro, self.name)\\n        elif self.occurrence == OCCURRENCE__IGNORED:\\n            return \\"    /* FIXME: %s is currently ignored */\\\\n\\" % self.name\\n        else:\\n            return \\"\\"\\n\\n\\n    def generate_deep_copy_code(self):\\n        if self.occurrence == OCCURRENCE__IGNORED:\\n            return \\"    /* FIXME: %s is currently ignored */\\\\n\\" % self.name\\n        elif self.occurrence in [OCCURRENCE__REQUIRED_LIST,\\n                                 OCCURRENCE__OPTIONAL_LIST]:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__DEEP_COPY_LIST(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n        elif self.type == \\"String\\":\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__DEEP_COPY_VALUE(String, %s)\\\\n\\" \\\\\\n                   % self.name\\n        elif self.is_enum():\\n            return \\"    (*dest)-\\u003e%s = src-\\u003e%s;\\\\n\\" % (self.name, self.name)\\n        else:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__DEEP_COPY(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n\\n\\n    def generate_serialize_code(self):\\n        if self.occurrence == OCCURRENCE__IGNORED:\\n            return \\"    /* FIXME: %s is currently ignored */\\\\n\\" % self.name\\n        elif self.occurrence in [OCCURRENCE__REQUIRED_LIST,\\n                                 OCCURRENCE__OPTIONAL_LIST]:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__SERIALIZE_LIST(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n        elif self.type == \\"String\\":\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__SERIALIZE_VALUE(String, %s)\\\\n\\" \\\\\\n                   % self.name\\n        else:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__SERIALIZE(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n\\n\\n    def generate_deserialize_code(self):\\n        if self.occurrence == OCCURRENCE__IGNORED:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__DESERIALIZE_IGNORE(%s) /* FIXME */\\\\n\\" \\\\\\n                   % self.name\\n        elif self.occurrence in [OCCURRENCE__REQUIRED_LIST,\\n                                 OCCURRENCE__OPTIONAL_LIST]:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__DESERIALIZE_LIST(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n        elif self.type == \\"String\\":\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__DESERIALIZE_VALUE(String, %s)\\\\n\\" \\\\\\n                   % self.name\\n        else:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__DESERIALIZE(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n\\n\\n    def generate_lookup_code(self):\\n        if self.occurrence == OCCURRENCE__IGNORED:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__CAST_FROM_ANY_TYPE_IGNORE(%s) /* FIXME */\\\\n\\" \\\\\\n                   % self.name\\n        elif self.occurrence in [OCCURRENCE__REQUIRED_LIST,\\n                                 OCCURRENCE__OPTIONAL_LIST]:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__CAST_LIST_FROM_ANY_TYPE(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n        elif self.type == \\"String\\":\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__CAST_VALUE_FROM_ANY_TYPE(String, %s)\\\\n\\" \\\\\\n                   % self.name\\n        else:\\n            return \\"    ESX_VI__TEMPLATE__PROPERTY__CAST_FROM_ANY_TYPE(%s, %s)\\\\n\\" \\\\\\n                   % (self.type, self.name)\\n\\n\\n    def get_type_string(self):\\n        if self.type == \\"String\\" and \\\\\\n           self.occurrence not in [OCCURRENCE__REQUIRED_LIST,\\n                                   OCCURRENCE__OPTIONAL_LIST]:\\n            return \\"char *\\"\\n        elif self.is_enum():\\n            return \\"esxVI_%s \\" % self.type\\n        else:\\n            return \\"esxVI_%s *\\" % self.type\\n\\n\\n\\nclass Type:\\n    def __init__(self, kind, name):\\n        self.kind = kind\\n        self.name = name\\n\\n\\n    def generate_typedef(self):\\n        return \\"typedef %s _esxVI_%s esxVI_%s;\\\\n\\" \\\\\\n               % (self.kind, self.name, self.name)\\n\\n\\n    def generate_typeenum(self):\\n        return \\"    esxVI_Type_%s,\\\\n\\" % self.name\\n\\n\\n    def generate_typetostring(self):\\n        string = \\"          case esxVI_Type_%s:\\\\n\\" % self.name\\n        string += \\"            return \\\\\\"%s\\\\\\";\\\\n\\\\n\\" % self.name\\n\\n        return string\\n\\n\\n    def generate_typefromstring(self):\\n        string =  \\"           else if (STREQ(type, \\\\\\"%s\\\\\\")) {\\\\n\\" % self.name\\n        string += \\"               return esxVI_Type_%s;\\\\n\\" % self.name\\n        string += \\"           }\\\\n\\"\\n\\n        return string\\n\\n\\n\\nclass Object(Type):\\n    FEATURE__DYNAMIC_CAST = (1 \\u003c\\u003c 1)\\n    FEATURE__LIST         = (1 \\u003c\\u003c 2)\\n    FEATURE__DEEP_COPY    = (1 \\u003c\\u003c 3)\\n    FEATURE__ANY_TYPE     = (1 \\u003c\\u003c 4)\\n    FEATURE__SERIALIZE    = (1 \\u003c\\u003c 5)\\n    FEATURE__DESERIALIZE  = (1 \\u003c\\u003c 6)\\n\\n\\n    def __init__(self, name, extends, properties, features=0, extended_by=None):\\n        Type.__init__(self, \\"struct\\", name)\\n        self.extends = extends\\n        self.features = features\\n        self.properties = properties\\n        self.extended_by = extended_by\\n        self.candidate_for_dynamic_cast = False\\n\\n        if self.extended_by is not None:\\n            self.extended_by.sort()\\n\\n\\n    def generate_struct_members(self, add_banner=False, struct_gap=False):\\n        members = \\"\\"\\n\\n        if struct_gap:\\n            members += \\"\\\\n\\"\\n\\n        if self.extends is not None:\\n            members += objects_by_name[self.extends] \\\\\\n                       .generate_struct_members(add_banner=True,\\n                                                struct_gap=False) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            members += \\"    /* %s */\\\\n\\" % self.name\\n\\n        for property in self.properties:\\n            members += property.generate_struct_member()\\n\\n        if len(self.properties) \\u003c 1:\\n            members += \\"    /* no properties */\\\\n\\"\\n\\n        return members\\n\\n\\n    def generate_free_code(self, add_banner=False):\\n        source = \\"\\"\\n\\n        if self.extends is not None:\\n            source += objects_by_name[self.extends] \\\\\\n                      .generate_free_code(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n        if len(self.properties) \\u003c 1:\\n            source += \\"    /* no properties */\\\\n\\"\\n        else:\\n            string = \\"\\"\\n\\n            for property in self.properties:\\n                string += property.generate_free_code()\\n\\n            if len(string) \\u003c 1:\\n                source += \\"    /* no properties to be freed */\\\\n\\"\\n            else:\\n                source += string\\n\\n        return source\\n\\n\\n    def generate_validate_code(self, add_banner=False):\\n        source = \\"\\"\\n\\n        if self.extends is not None:\\n            source += objects_by_name[self.extends] \\\\\\n                      .generate_validate_code(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n        if len(self.properties) \\u003c 1:\\n            source += \\"    /* no properties */\\\\n\\"\\n        else:\\n            string = \\"\\"\\n\\n            for property in self.properties:\\n                string += property.generate_validate_code()\\n\\n            if len(string) \\u003c 1:\\n                source += \\"    /* no required properties */\\\\n\\"\\n            else:\\n                source += string\\n\\n        return source\\n\\n\\n    def generate_dynamic_cast_code(self, is_first=True):\\n        source = \\"\\"\\n\\n        if self.extended_by is not None:\\n            if not is_first:\\n                source += \\"\\\\n\\"\\n\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n            for extended_by in self.extended_by:\\n                source += \\"    ESX_VI__TEMPLATE__DYNAMIC_CAST__ACCEPT(%s)\\\\n\\" \\\\\\n                          % extended_by\\n\\n            for extended_by in self.extended_by:\\n                source += objects_by_name[extended_by] \\\\\\n                          .generate_dynamic_cast_code(False)\\n\\n        return source\\n\\n\\n    def generate_deep_copy_code(self, add_banner = False):\\n        source = \\"\\"\\n\\n        if self.extends is not None:\\n            source += objects_by_name[self.extends] \\\\\\n                      .generate_deep_copy_code(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n        if len(self.properties) \\u003c 1:\\n            source += \\"    /* no properties */\\\\n\\"\\n        else:\\n            string = \\"\\"\\n\\n            for property in self.properties:\\n                string += property.generate_deep_copy_code()\\n\\n            if len(string) \\u003c 1:\\n                source += \\"    /* no properties to be deep copied */\\\\n\\"\\n            else:\\n                source += string\\n\\n        return source\\n\\n\\n    def generate_serialize_code(self, add_banner=False):\\n        source = \\"\\"\\n\\n        if self.extends is not None:\\n            source += objects_by_name[self.extends] \\\\\\n                      .generate_serialize_code(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n        if len(self.properties) \\u003c 1:\\n            source += \\"    /* no properties */\\\\n\\"\\n        else:\\n            for property in self.properties:\\n                source += property.generate_serialize_code()\\n\\n        return source\\n\\n\\n    def generate_deserialize_code(self, add_banner=False):\\n        source = \\"\\"\\n\\n        if self.extends is not None:\\n            source += objects_by_name[self.extends] \\\\\\n                      .generate_deserialize_code(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n        if len(self.properties) \\u003c 1:\\n            source += \\"    /* no properties */\\\\n\\"\\n        else:\\n            for property in self.properties:\\n                source += property.generate_deserialize_code()\\n\\n        return source\\n\\n\\n    def generate_header(self):\\n        header = separator\\n        header += \\" * VI Object: %s\\\\n\\" % self.name\\n\\n        if self.extends is not None:\\n            header += \\" *            extends %s\\\\n\\" % self.extends\\n\\n        first = True\\n\\n        if self.extended_by is not None:\\n            for extended_by in self.extended_by:\\n                if first:\\n                    header += \\" *            extended by %s\\\\n\\" % extended_by\\n                    first = False\\n                else:\\n                    header += \\" *                        %s\\\\n\\" % extended_by\\n\\n        header += \\" */\\\\n\\\\n\\"\\n\\n        # struct\\n        header += \\"struct _esxVI_%s {\\\\n\\" % self.name\\n\\n        if self.features \\u0026 Object.FEATURE__LIST:\\n            header += aligned(\\"    esxVI_%s *_next; \\" % self.name,\\n                              \\"/* optional */\\\\n\\")\\n        else:\\n            header += aligned(\\"    esxVI_%s *_unused; \\" % self.name,\\n                              \\"/* optional */\\\\n\\")\\n\\n        header += aligned(\\"    esxVI_Type _type; \\", \\"/* required */\\\\n\\")\\n        header += self.generate_struct_members(struct_gap=True)\\n        header += \\"};\\\\n\\\\n\\"\\n\\n        # functions\\n        header += \\"int esxVI_%s_Alloc(esxVI_%s **item);\\\\n\\" \\\\\\n                  % (self.name, self.name)\\n        header += \\"void esxVI_%s_Free(esxVI_%s **item);\\\\n\\" \\\\\\n                  % (self.name, self.name)\\n        header += \\"int esxVI_%s_Validate(esxVI_%s *item);\\\\n\\" \\\\\\n                  % (self.name, self.name)\\n\\n        if self.features \\u0026 Object.FEATURE__DYNAMIC_CAST:\\n            if self.extended_by is not None or self.extends is not None:\\n                header += \\"esxVI_%s *esxVI_%s_DynamicCast(void *item);\\\\n\\" \\\\\\n                          % (self.name, self.name)\\n            else:\\n                report_error(\\"cannot add dynamic cast support for an untyped object\\")\\n\\n        if self.features \\u0026 Object.FEATURE__LIST:\\n            header += \\"int esxVI_%s_AppendToList(esxVI_%s **list, esxVI_%s *item);\\\\n\\" \\\\\\n                      % (self.name, self.name, self.name)\\n\\n        if self.features \\u0026 Object.FEATURE__DEEP_COPY:\\n            header += \\"int esxVI_%s_DeepCopy(esxVI_%s **dst, esxVI_%s *src);\\\\n\\" \\\\\\n                      % (self.name, self.name, self.name)\\n\\n            if self.features \\u0026 Object.FEATURE__LIST:\\n                header += (\\"int esxVI_%s_DeepCopyList(esxVI_%s **dstList, \\"\\n                                                     \\"esxVI_%s *srcList);\\\\n\\") \\\\\\n                          % (self.name, self.name, self.name)\\n\\n        if self.features \\u0026 Object.FEATURE__ANY_TYPE:\\n            header += (\\"int esxVI_%s_CastFromAnyType(esxVI_AnyType *anyType, \\"\\n                                                    \\"esxVI_%s **item);\\\\n\\") \\\\\\n                      % (self.name, self.name)\\n\\n            if self.features \\u0026 Object.FEATURE__LIST:\\n                header += (\\"int esxVI_%s_CastListFromAnyType(esxVI_AnyType *anyType, \\"\\n                                                            \\"esxVI_%s **list);\\\\n\\") \\\\\\n                          % (self.name, self.name)\\n\\n        if self.features \\u0026 Object.FEATURE__SERIALIZE:\\n            header += (\\"int esxVI_%s_Serialize(esxVI_%s *item, \\"\\n                                              \\"const char *element, \\"\\n                                              \\"virBufferPtr output);\\\\n\\") \\\\\\n                      % (self.name, self.name)\\n\\n            if self.features \\u0026 Object.FEATURE__LIST:\\n                header += (\\"int esxVI_%s_SerializeList(esxVI_%s *list, \\"\\n                                                      \\"const char *element, \\"\\n                                                      \\"virBufferPtr output);\\\\n\\") \\\\\\n                          % (self.name, self.name)\\n\\n        if self.features \\u0026 Object.FEATURE__DESERIALIZE:\\n            header += \\"int esxVI_%s_Deserialize(xmlNodePtr node, esxVI_%s **item);\\\\n\\" \\\\\\n                      % (self.name, self.name)\\n\\n            if self.features \\u0026 Object.FEATURE__LIST:\\n                header += (\\"int esxVI_%s_DeserializeList(xmlNodePtr node, \\"\\n                                                        \\"esxVI_%s **list);\\\\n\\") \\\\\\n                          % (self.name, self.name)\\n\\n        header += \\"\\\\n\\\\n\\\\n\\"\\n\\n        return header\\n\\n\\n    def generate_source(self):\\n        source = separator\\n        source += \\" * VI Object: %s\\\\n\\" % self.name\\n\\n        if self.extends is not None:\\n            source += \\" *            extends %s\\\\n\\" % self.extends\\n\\n        first = True\\n\\n        if self.extended_by is not None:\\n            for extended_by in self.extended_by:\\n                if first:\\n                    source += \\" *            extended by %s\\\\n\\" % extended_by\\n                    first = False\\n                else:\\n                    source += \\" *                        %s\\\\n\\" % extended_by\\n\\n        source += \\" */\\\\n\\\\n\\"\\n\\n        # functions\\n        source += \\"/* esxVI_%s_Alloc */\\\\n\\" % self.name\\n        source += \\"ESX_VI__TEMPLATE__ALLOC(%s)\\\\n\\\\n\\" % self.name\\n\\n        # free\\n        if self.extended_by is None:\\n            source += \\"/* esxVI_%s_Free */\\\\n\\" % self.name\\n            source += \\"ESX_VI__TEMPLATE__FREE(%s,\\\\n\\" % self.name\\n            source += \\"{\\\\n\\"\\n\\n            if self.features \\u0026 Object.FEATURE__LIST:\\n                if self.extends is not None:\\n                    # avoid \\"dereferencing type-punned pointer will break\\n                    # strict-aliasing rules\\" warnings\\n                    source += \\"    esxVI_%s *next = (esxVI_%s *)item-\\u003e_next;\\\\n\\\\n\\" \\\\\\n                              % (self.extends, self.extends)\\n                    source += \\"    esxVI_%s_Free(\\u0026next);\\\\n\\" % self.extends\\n                    source += \\"    item-\\u003e_next = (esxVI_%s *)next;\\\\n\\\\n\\" % self.name\\n                else:\\n                    source += \\"    esxVI_%s_Free(\\u0026item-\\u003e_next);\\\\n\\\\n\\" % self.name\\n\\n            source += self.generate_free_code()\\n\\n            source += \\"})\\\\n\\\\n\\"\\n        else:\\n            source += \\"/* esxVI_%s_Free */\\\\n\\" % self.name\\n            source += \\"ESX_VI__TEMPLATE__DYNAMIC_FREE(%s,\\\\n\\" % self.name\\n            source += \\"{\\\\n\\"\\n\\n            for extended_by in self.extended_by:\\n                source += \\"    ESX_VI__TEMPLATE__DISPATCH__FREE(%s)\\\\n\\" \\\\\\n                          % extended_by\\n\\n            source += \\"},\\\\n\\"\\n            source += \\"{\\\\n\\"\\n\\n            if self.features \\u0026 Object.FEATURE__LIST:\\n                if self.extends is not None:\\n                    # avoid \\"dereferencing type-punned pointer will brea\\n                    # strict-aliasing rules\\" warnings\\n                    source += \\"    esxVI_%s *next = (esxVI_%s *)item-\\u003e_next;\\\\n\\\\n\\" \\\\\\n                              % (self.extends, self.extends)\\n                    source += \\"    esxVI_%s_Free(\\u0026next);\\\\n\\" % self.extends\\n                    source += \\"    item-\\u003e_next = (esxVI_%s *)next;\\\\n\\\\n\\" % self.name\\n                else:\\n                    source += \\"    esxVI_%s_Free(\\u0026item-\\u003e_next);\\\\n\\\\n\\" % self.name\\n\\n            source += self.generate_free_code()\\n\\n            source += \\"})\\\\n\\\\n\\"\\n\\n        # validate\\n        source += \\"/* esxVI_%s_Validate */\\\\n\\" % self.name\\n        source += \\"ESX_VI__TEMPLATE__VALIDATE(%s,\\\\n\\" % self.name\\n        source += \\"{\\\\n\\"\\n\\n        source += self.generate_validate_code()\\n\\n        source += \\"})\\\\n\\\\n\\"\\n\\n        # dynamic cast\\n        if self.features \\u0026 Object.FEATURE__DYNAMIC_CAST:\\n            if self.extended_by is not None or self.extends is not None:\\n                source += \\"/* esxVI_%s_DynamicCast */\\\\n\\" % self.name\\n                source += \\"ESX_VI__TEMPLATE__DYNAMIC_CAST(%s,\\\\n\\" % self.name\\n                source += \\"{\\\\n\\"\\n\\n                source += self.generate_dynamic_cast_code()\\n\\n                source += \\"})\\\\n\\\\n\\"\\n            else:\\n                report_error(\\"cannot add dynamic cast support for an untyped object\\")\\n\\n        # append to list\\n        if self.features \\u0026 Object.FEATURE__LIST:\\n            source += \\"/* esxVI_%s_AppendToList */\\\\n\\" % self.name\\n            source += \\"ESX_VI__TEMPLATE__LIST__APPEND(%s)\\\\n\\\\n\\" % self.name\\n\\n        # deep copy\\n        if self.extended_by is None:\\n            if self.features \\u0026 Object.FEATURE__DEEP_COPY:\\n                source += \\"/* esxVI_%s_DeepCopy */\\\\n\\" % self.name\\n                source += \\"ESX_VI__TEMPLATE__DEEP_COPY(%s,\\\\n\\" % self.name\\n                source += \\"{\\\\n\\"\\n\\n                source += self.generate_deep_copy_code()\\n\\n                source += \\"})\\\\n\\\\n\\"\\n\\n                if self.features \\u0026 Object.FEATURE__LIST:\\n                    source += \\"/* esxVI_%s_DeepCopyList */\\\\n\\" % self.name\\n                    source += \\"ESX_VI__TEMPLATE__LIST__DEEP_COPY(%s)\\\\n\\\\n\\" \\\\\\n                              % self.name\\n        else:\\n            if self.features \\u0026 Object.FEATURE__DEEP_COPY:\\n                source += \\"/* esxVI_%s_DeepCopy */\\\\n\\" % self.name\\n                source += \\"ESX_VI__TEMPLATE__DYNAMIC_DEEP_COPY(%s)\\\\n\\" % self.name\\n                source += \\"{\\\\n\\"\\n\\n                for extended_by in self.extended_by:\\n                    source += \\"    ESX_VI__TEMPLATE__DISPATCH__DEEP_COPY(%s)\\\\n\\" \\\\\\n                              % extended_by\\n\\n                source += \\"},\\\\n\\"\\n                source += \\"{\\\\n\\"\\n\\n                source += self.generate_deep_copy_code()\\n\\n                source += \\"})\\\\n\\\\n\\"\\n\\n                if self.features \\u0026 Object.FEATURE__LIST:\\n                    source += \\"/* esxVI_%s_DeepCopyList */\\\\n\\" % self.name\\n                    source += \\"ESX_VI__TEMPLATE__LIST__DEEP_COPY(%s)\\\\n\\\\n\\" \\\\\\n                              % self.name\\n\\n        # cast from any type\\n        if self.features \\u0026 Object.FEATURE__ANY_TYPE:\\n            source += \\"/* esxVI_%s_CastFromAnyType */\\\\n\\" % self.name\\n\\n            if self.extended_by is None:\\n                source += \\"ESX_VI__TEMPLATE__CAST_FROM_ANY_TYPE(%s)\\\\n\\\\n\\" \\\\\\n                          % self.name\\n            else:\\n                source += \\"ESX_VI__TEMPLATE__DYNAMIC_CAST_FROM_ANY_TYPE(%s,\\\\n\\" \\\\\\n                          % self.name\\n                source += \\"{\\\\n\\"\\n\\n                for extended_by in self.extended_by:\\n                    source += \\"    ESX_VI__TEMPLATE__DISPATCH__CAST_FROM_ANY_TYPE(%s)\\\\n\\" \\\\\\n                              % extended_by\\n\\n                source += \\"})\\\\n\\\\n\\"\\n\\n            if self.features \\u0026 Object.FEATURE__LIST:\\n                source += \\"/* esxVI_%s_CastListFromAnyType */\\\\n\\" % self.name\\n                source += \\"ESX_VI__TEMPLATE__LIST__CAST_FROM_ANY_TYPE(%s)\\\\n\\\\n\\" \\\\\\n                          % self.name\\n\\n        # serialize\\n        if self.extended_by is None:\\n            if self.features \\u0026 Object.FEATURE__SERIALIZE:\\n                source += \\"/* esxVI_%s_Serialize */\\\\n\\" % self.name\\n                source += \\"ESX_VI__TEMPLATE__SERIALIZE(%s,\\\\n\\" % self.name\\n                source += \\"{\\\\n\\"\\n\\n                source += self.generate_serialize_code()\\n\\n                source += \\"})\\\\n\\\\n\\"\\n\\n                if self.features \\u0026 Object.FEATURE__LIST:\\n                    source += \\"/* esxVI_%s_SerializeList */\\\\n\\" % self.name\\n                    source += \\"ESX_VI__TEMPLATE__LIST__SERIALIZE(%s)\\\\n\\\\n\\" \\\\\\n                              % self.name\\n        else:\\n            if self.features \\u0026 Object.FEATURE__SERIALIZE:\\n                source += \\"/* esxVI_%s_Serialize */\\\\n\\" % self.name\\n                source += \\"ESX_VI__TEMPLATE__DYNAMIC_SERIALIZE(%s,\\\\n\\" % self.name\\n                source += \\"{\\\\n\\"\\n\\n                for extended_by in self.extended_by:\\n                    source += \\"    ESX_VI__TEMPLATE__DISPATCH__SERIALIZE(%s)\\\\n\\" \\\\\\n                              % extended_by\\n\\n                source += \\"},\\\\n\\"\\n                source += \\"{\\\\n\\"\\n\\n                source += self.generate_serialize_code()\\n\\n                source += \\"})\\\\n\\\\n\\"\\n\\n                if self.features \\u0026 Object.FEATURE__LIST:\\n                    source += \\"/* esxVI_%s_SerializeList */\\\\n\\" % self.name\\n                    source += \\"ESX_VI__TEMPLATE__LIST__SERIALIZE(%s)\\\\n\\\\n\\" \\\\\\n                              % self.name\\n\\n        # deserialize\\n        if self.extended_by is None:\\n            if self.features \\u0026 Object.FEATURE__DESERIALIZE:\\n                source += \\"/* esxVI_%s_Deserialize */\\\\n\\" % self.name\\n                source += \\"ESX_VI__TEMPLATE__DESERIALIZE(%s,\\\\n\\" % self.name\\n                source += \\"{\\\\n\\"\\n\\n                source += self.generate_deserialize_code()\\n\\n                source += \\"})\\\\n\\\\n\\"\\n\\n                if self.features \\u0026 Object.FEATURE__LIST:\\n                    source += \\"/* esxVI_%s_DeserializeList */\\\\n\\" % self.name\\n                    source += \\"ESX_VI__TEMPLATE__LIST__DESERIALIZE(%s)\\\\n\\\\n\\" \\\\\\n                              % self.name\\n        else:\\n            if self.features \\u0026 Object.FEATURE__DESERIALIZE:\\n                source += \\"/* esxVI_%s_Deserialize */\\\\n\\" % self.name\\n                source += \\"ESX_VI__TEMPLATE__DYNAMIC_DESERIALIZE(%s,\\\\n\\" \\\\\\n                          % self.name\\n                source += \\"{\\\\n\\"\\n\\n                for extended_by in self.extended_by:\\n                    source += \\"    ESX_VI__TEMPLATE__DISPATCH__DESERIALIZE(%s)\\\\n\\" \\\\\\n                              % extended_by\\n\\n                source += \\"},\\\\n\\"\\n                source += \\"{\\\\n\\"\\n\\n                source += self.generate_deserialize_code()\\n\\n                source += \\"})\\\\n\\\\n\\"\\n\\n                if self.features \\u0026 Object.FEATURE__LIST:\\n                    source += \\"/* esxVI_%s_DeserializeList */\\\\n\\" % self.name\\n                    source += \\"ESX_VI__TEMPLATE__LIST__DESERIALIZE(%s)\\\\n\\\\n\\" \\\\\\n                              % self.name\\n\\n        source += \\"\\\\n\\\\n\\"\\n\\n        return source\\n\\n\\n\\nclass ManagedObject(Type):\\n    FEATURE__LIST = (1 \\u003c\\u003c 2)\\n\\n\\n    def __init__(self, name, extends, properties, features=0, extended_by=None):\\n        Type.__init__(self, \\"struct\\", name)\\n        self.extends = extends\\n        self.features = features\\n        self.properties = properties\\n        self.extended_by = extended_by\\n\\n        if self.extended_by is not None:\\n            self.extended_by.sort()\\n\\n\\n    def generate_struct_members(self, add_banner=False, struct_gap=False):\\n        members = \\"\\"\\n\\n        if struct_gap:\\n            members += \\"\\\\n\\"\\n\\n        if self.extends is not None:\\n            members += managed_objects_by_name[self.extends] \\\\\\n                       .generate_struct_members(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            members += \\"    /* %s */\\\\n\\" % self.name\\n\\n        for property in self.properties:\\n            members += property.generate_struct_member()\\n\\n        if len(self.properties) \\u003c 1:\\n            members += \\"    /* no properties */\\\\n\\"\\n\\n        return members\\n\\n\\n    def generate_free_code(self, add_banner=False):\\n        source = \\"\\"\\n\\n        if self.extends is not None:\\n            source += managed_objects_by_name[self.extends] \\\\\\n                      .generate_free_code(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n        if len(self.properties) \\u003c 1:\\n            source += \\"    /* no properties */\\\\n\\"\\n        else:\\n            string = \\"\\"\\n\\n            for property in self.properties:\\n                string += property.generate_free_code()\\n\\n            if len(string) \\u003c 1:\\n                source += \\"    /* no properties to be freed */\\\\n\\"\\n            else:\\n                source += string\\n\\n        return source\\n\\n\\n    def generate_validate_code(self, add_banner=False):\\n        source = \\"\\"\\n\\n        if self.extends is not None:\\n            source += managed_objects_by_name[self.extends] \\\\\\n                      .generate_validate_code(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n        if len(self.properties) \\u003c 1:\\n            source += \\"    /* no properties */\\\\n\\"\\n        else:\\n            string = \\"\\"\\n\\n            for property in self.properties:\\n                string += property.generate_validate_code(managed=True)\\n\\n            if len(string) \\u003c 1:\\n                source += \\"    /* no required properties */\\\\n\\"\\n            else:\\n                source += string\\n\\n        return source\\n\\n\\n    def generate_lookup_code1(self, add_banner=False):\\n        source = \\"\\"\\n\\n        if self.extends is not None:\\n            source += managed_objects_by_name[self.extends] \\\\\\n                      .generate_lookup_code1(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n        if len(self.properties) \\u003c 1:\\n            source += \\"    /* no properties */\\\\n\\"\\n        else:\\n            string = \\"\\"\\n\\n            for property in self.properties:\\n                string += \\"    \\\\\\"%s\\\\\\\\0\\\\\\"\\\\n\\" % property.name\\n\\n            if len(string) \\u003c 1:\\n                source += \\"    /* no properties */\\\\n\\"\\n            else:\\n                source += string\\n\\n        return source\\n\\n\\n    def generate_lookup_code2(self, add_banner=False):\\n        source = \\"\\"\\n\\n        if self.extends is not None:\\n            source += managed_objects_by_name[self.extends] \\\\\\n                      .generate_lookup_code2(add_banner=True) + \\"\\\\n\\"\\n\\n        if self.extends is not None or add_banner:\\n            source += \\"    /* %s */\\\\n\\" % self.name\\n\\n        if len(self.properties) \\u003c 1:\\n            source += \\"    /* no properties */\\\\n\\"\\n        else:\\n            string = \\"\\"\\n\\n            for property in self.properties:\\n                string += property.generate_lookup_code()\\n\\n            if len(string) \\u003c 1:\\n                source += \\"    /* no properties */\\\\n\\"\\n            else:\\n                source += string\\n\\n        return source\\n\\n\\n    def generate_comment(self):\\n        comment = separator\\n        comment += \\" * VI Managed Object: %s\\\\n\\" % self.name\\n\\n        if self.extends is not None:\\n            comment += \\" *                    extends %s\\\\n\\" % self.extends\\n\\n        first = True\\n\\n        if self.extended_by is not None:\\n            for extended_by in self.extended_by:\\n                if first:\\n                    comment += \\" *                    extended by %s\\\\n\\" \\\\\\n                               % extended_by\\n                    first = False\\n                else:\\n                    comment += \\" *                                %s\\\\n\\" \\\\\\n                               % extended_by\\n\\n        comment += \\" */\\\\n\\\\n\\"\\n\\n        return comment\\n\\n\\n    def generate_header(self):\\n        header = self.generate_comment()\\n\\n        # struct\\n        header += \\"struct _esxVI_%s {\\\\n\\" % self.name\\n\\n        if self.features \\u0026 Object.FEATURE__LIST:\\n            header += aligned(\\"    esxVI_%s *_next; \\" % self.name,\\n                              \\"/* optional */\\\\n\\")\\n        else:\\n            header += aligned(\\"    esxVI_%s *_unused; \\" % self.name,\\n                              \\"/* optional */\\\\n\\")\\n\\n        header += aligned(\\"    esxVI_Type _type; \\", \\"/* required */\\\\n\\")\\n        header += aligned(\\"    esxVI_ManagedObjectReference *_reference; \\",\\n                          \\"/* required */\\\\n\\")\\n        header += \\"\\\\n\\"\\n        header += self.generate_struct_members()\\n\\n        header += \\"};\\\\n\\\\n\\"\\n\\n        # functions\\n        header += \\"int esxVI_%s_Alloc(esxVI_%s **item);\\\\n\\" % (self.name, self.name)\\n        header += \\"void esxVI_%s_Free(esxVI_%s **item);\\\\n\\" % (self.name, self.name)\\n        header += (\\"int esxVI_%s_Validate(esxVI_%s *item, \\"\\n                                         \\"esxVI_String *selectedPropertyNameList);\\\\n\\") \\\\\\n                  % (self.name, self.name)\\n\\n        if self.features \\u0026 Object.FEATURE__LIST:\\n            header += \\"int esxVI_%s_AppendToList(esxVI_%s **list, esxVI_%s *item);\\\\n\\" \\\\\\n                      % (self.name, self.name, self.name)\\n\\n        header += \\"\\\\n\\\\n\\\\n\\"\\n\\n        return header\\n\\n\\n    def generate_helper_header(self):\\n        header = \\"\\"\\n\\n        # functions\\n        header += (\\"int esxVI_Lookup%s(esxVI_Context *ctx, \\"\\n                                      \\"const char *name, \\"\\n                                      \\"esxVI_ManagedObjectReference *root, \\"\\n                                      \\"esxVI_String *selectedPropertyNameList, \\"\\n                                      \\"esxVI_%s **item, \\"\\n                                      \\"esxVI_Occurrence occurrence);\\\\n\\") \\\\\\n                  % (self.name, self.name)\\n\\n        header += \\"\\\\n\\"\\n\\n        return header\\n\\n\\n    def generate_source(self):\\n        source = self.generate_comment()\\n\\n        # functions\\n        source += \\"/* esxVI_%s_Alloc */\\\\n\\" % self.name\\n        source += \\"ESX_VI__TEMPLATE__ALLOC(%s)\\\\n\\\\n\\" % self.name\\n\\n        # free\\n        if self.extended_by is None:\\n            source += \\"/* esxVI_%s_Free */\\\\n\\" % self.name\\n            source += \\"ESX_VI__TEMPLATE__FREE(%s,\\\\n\\" % self.name\\n            source += \\"{\\\\n\\"\\n\\n            if self.features \\u0026 ManagedObject.FEATURE__LIST:\\n                if self.extends is not None:\\n                    # avoid \\"dereferencing type-punned pointer will break\\n                    # strict-aliasing rules\\" warnings\\n                    source += \\"    esxVI_%s *next = (esxVI_%s *)item-\\u003e_next;\\\\n\\\\n\\" \\\\\\n                              % (self.extends, self.extends)\\n                    source += \\"    esxVI_%s_Free(\\u0026next);\\\\n\\" % self.extends\\n                    source += \\"    item-\\u003e_next = (esxVI_%s *)next;\\\\n\\\\n\\" % self.name\\n                else:\\n                    source += \\"    esxVI_%s_Free(\\u0026item-\\u003e_next);\\\\n\\" % self.name\\n\\n            source += \\"    esxVI_ManagedObjectReference_Free(\\u0026item-\\u003e_reference);\\\\n\\\\n\\"\\n\\n            source += self.generate_free_code()\\n\\n            source += \\"})\\\\n\\\\n\\"\\n        else:\\n            source += \\"/* esxVI_%s_Free */\\\\n\\" % self.name\\n            source += \\"ESX_VI__TEMPLATE__DYNAMIC_FREE(%s,\\\\n\\" % self.name\\n            source += \\"{\\\\n\\"\\n\\n            for extended_by in self.extended_by:\\n                source += \\"    ESX_VI__TEMPLATE__DISPATCH__FREE(%s)\\\\n\\" % extended_by\\n\\n            source += \\"},\\\\n\\"\\n            source += \\"{\\\\n\\"\\n\\n            if self.features \\u0026 Object.FEATURE__LIST:\\n                if self.extends is not None:\\n                    # avoid \\"dereferencing type-punned pointer will break\\n                    # strict-aliasing rules\\" warnings\\n                    source += \\"    esxVI_%s *next = (esxVI_%s *)item-\\u003e_next;\\\\n\\\\n\\" \\\\\\n                              % (self.extends, self.extends)\\n                    source += \\"    esxVI_%s_Free(\\u0026next);\\\\n\\" % self.extends\\n                    source += \\"    item-\\u003e_next = (esxVI_%s *)next;\\\\n\\\\n\\" % self.name\\n                else:\\n                    source += \\"    esxVI_%s_Free(\\u0026item-\\u003e_next);\\\\n\\" % self.name\\n\\n            source += \\"    esxVI_ManagedObjectReference_Free(\\u0026item-\\u003e_reference);\\\\n\\\\n\\"\\n\\n            source += self.generate_free_code()\\n\\n            source += \\"})\\\\n\\\\n\\"\\n\\n        # validate\\n        source += \\"/* esxVI_%s_Validate */\\\\n\\" % self.name\\n        source += \\"ESX_VI__TEMPLATE__MANAGED_VALIDATE(%s,\\\\n\\" % self.name\\n        source += \\"{\\\\n\\"\\n\\n        source += self.generate_validate_code()\\n\\n        source += \\"})\\\\n\\\\n\\"\\n\\n        # append to list\\n        if self.features \\u0026 ManagedObject.FEATURE__LIST:\\n            source += \\"/* esxVI_%s_AppendToList */\\\\n\\" % self.name\\n            source += \\"ESX_VI__TEMPLATE__LIST__APPEND(%s)\\\\n\\\\n\\" % self.name\\n\\n        source += \\"\\\\n\\\\n\\"\\n\\n        return source\\n\\n\\n    def generate_helper_source(self):\\n        source = \\"\\"\\n\\n        # lookup\\n        source += \\"/* esxVI_Lookup%s */\\\\n\\" % self.name\\n        source += \\"ESX_VI__TEMPLATE__LOOKUP(%s,\\\\n\\" % self.name\\n        source += \\"{\\\\n\\"\\n\\n        source += self.generate_lookup_code1()\\n\\n        source += \\"},\\\\n\\"\\n        source += \\"{\\\\n\\"\\n\\n        source += self.generate_lookup_code2()\\n\\n        source += \\"})\\\\n\\\\n\\"\\n\\n        source += \\"\\\\n\\\\n\\"\\n\\n        return source\\n\\n\\n\\nclass Enum(Type):\\n    FEATURE__ANY_TYPE = (1 \\u003c\\u003c 1)\\n    FEATURE__SERIALIZE = (1 \\u003c\\u003c 2)\\n    FEATURE__DESERIALIZE = (1 \\u003c\\u003c 3)\\n\\n\\n    def __init__(self, name, values, features=0):\\n        Type.__init__(self, \\"enum\\", name)\\n        self.values = values\\n        self.features = features\\n\\n\\n    def generate_header(self):\\n        header = separator\\n        header += \\" * VI Enum: %s\\\\n\\" % self.name\\n        header += \\" */\\\\n\\\\n\\"\\n\\n        # enum\\n        header += \\"enum _esxVI_%s {\\\\n\\" % self.name\\n        header += \\"    esxVI_%s_Undefined = 0,\\\\n\\" % self.name\\n\\n        for value in self.values:\\n            header += \\"    esxVI_%s_%s,\\\\n\\" % (self.name, capitalize_first(value))\\n\\n        header += \\"};\\\\n\\\\n\\"\\n\\n        # functions\\n        if self.features \\u0026 Enum.FEATURE__ANY_TYPE:\\n            header += (\\"int esxVI_%s_CastFromAnyType(esxVI_AnyType *anyType, \\"\\n                                                    \\"esxVI_%s *item);\\\\n\\") \\\\\\n                      % (self.name, self.name)\\n\\n        if self.features \\u0026 Enum.FEATURE__SERIALIZE:\\n            header += (\\"int esxVI_%s_Serialize(esxVI_%s item, const char *element, \\"\\n                                              \\"virBufferPtr output);\\\\n\\") \\\\\\n                      % (self.name, self.name)\\n\\n        if self.features \\u0026 Enum.FEATURE__DESERIALIZE:\\n            header += (\\"int esxVI_%s_Deserialize(xmlNodePtr node, \\"\\n                                                \\"esxVI_%s *item);\\\\n\\") \\\\\\n                      % (self.name, self.name)\\n\\n        header += \\"\\\\n\\\\n\\\\n\\"\\n\\n        return header\\n\\n\\n    def generate_source(self):\\n        source = separator\\n        source += \\" * VI Enum: %s\\\\n\\" % self.name\\n        source += \\" */\\\\n\\\\n\\"\\n\\n        source += \\"static const esxVI_Enumeration _esxVI_%s_Enumeration = {\\\\n\\" \\\\\\n                  % self.name\\n        source += \\"    esxVI_Type_%s, {\\\\n\\" % self.name\\n\\n        for value in self.values:\\n            source += \\"        { \\\\\\"%s\\\\\\", esxVI_%s_%s },\\\\n\\" \\\\\\n                      % (value, self.name, capitalize_first(value))\\n\\n        source += \\"        { NULL, -1 },\\\\n\\"\\n        source += \\"    },\\\\n\\"\\n        source += \\"};\\\\n\\\\n\\"\\n\\n        # functions\\n        if self.features \\u0026 Enum.FEATURE__ANY_TYPE:\\n            source += \\"/* esxVI_%s_CastFromAnyType */\\\\n\\" % self.name\\n            source += \\"ESX_VI__TEMPLATE__ENUMERATION__CAST_FROM_ANY_TYPE(%s)\\\\n\\\\n\\" \\\\\\n                      % self.name\\n\\n        if self.features \\u0026 Enum.FEATURE__SERIALIZE:\\n            source += \\"/* esxVI_%s_Serialize */\\\\n\\" % self.name\\n            source += \\"ESX_VI__TEMPLATE__ENUMERATION__SERIALIZE(%s)\\\\n\\\\n\\" \\\\\\n                      % self.name\\n\\n        if self.features \\u0026 Enum.FEATURE__DESERIALIZE:\\n            source += \\"/* esxVI_%s_Deserialize */\\\\n\\" % self.name\\n            source += \\"ESX_VI__TEMPLATE__ENUMERATION__DESERIALIZE(%s)\\\\n\\\\n\\" \\\\\\n                      % self.name\\n\\n        source += \\"\\\\n\\\\n\\"\\n\\n        return source\\n\\n\\n\\ndef report_error(message):\\n    print \\"error: \\" + message\\n    sys.exit(1)\\n\\n\\n\\ndef capitalize_first(string):\\n    return string[:1].upper() + string[1:]\\n\\n\\n\\ndef parse_object(block):\\n    # expected format: [managed] object \\u003cname\\u003e [extends \\u003cname\\u003e]\\n    header_items = block[0][1].split()\\n    managed = False\\n\\n    if header_items[0] == \\"managed\\":\\n        managed = True\\n        del header_items[0]\\n\\n    if len(header_items) \\u003c 2:\\n        report_error(\\"line %d: invalid block header\\" % (number))\\n\\n    assert header_items[0] == \\"object\\"\\n\\n    name = header_items[1]\\n    extends = None\\n\\n    if len(header_items) \\u003e 2:\\n        if header_items[2] != \\"extends\\":\\n            report_error(\\"line %d: invalid block header\\" % (number))\\n        else:\\n            extends = header_items[3]\\n\\n    properties = []\\n\\n    for line in block[1:]:\\n        # expected format: \\u003ctype\\u003e \\u003cname\\u003e \\u003coccurrence\\u003e\\n        items = line[1].split()\\n\\n        if len(items) != 3:\\n            report_error(\\"line %d: invalid property\\" % line[0])\\n\\n        if items[2] not in valid_occurrences:\\n            report_error(\\"line %d: invalid occurrence\\" % line[0])\\n\\n        properties.append(Property(type=items[0], name=items[1],\\n                                   occurrence=items[2]))\\n\\n    if managed:\\n        return ManagedObject(name=name, extends=extends, properties=properties)\\n    else:\\n        return Object(name=name, extends=extends, properties=properties)\\n\\n\\n\\ndef parse_enum(block):\\n    # expected format: enum \\u003cname\\u003e\\n    header_items = block[0][1].split()\\n\\n    if len(header_items) \\u003c 2:\\n        report_error(\\"line %d: invalid block header\\" % (number))\\n\\n    assert header_items[0] == \\"enum\\"\\n\\n    name = header_items[1]\\n\\n    values = []\\n\\n    for line in block[1:]:\\n        # expected format: \\u003cvalue\\u003e\\n        values.append(line[1])\\n\\n    return Enum(name=name, values=values)\\n\\n\\n\\ndef parse_method(block):\\n    # expected format: method \\u003cname\\u003e [returns \\u003ctype\\u003e \\u003coccurrence\\u003e]\\n    header_items = block[0][1].split()\\n\\n    if len(header_items) \\u003c 2:\\n        report_error(\\"line %d: invalid block header\\" % (number))\\n\\n    assert header_items[0] == \\"method\\"\\n\\n    name = header_items[1]\\n    returns = None\\n\\n    if len(header_items) \\u003e 2:\\n        if header_items[2] != \\"returns\\":\\n            report_error(\\"line %d: invalid block header\\" % (number))\\n        else:\\n            returns = Parameter(type=header_items[3], name=\\"output\\",\\n                                occurrence=header_items[4])\\n\\n    parameters = []\\n\\n    for line in block[1:]:\\n        # expected format: \\u003ctype\\u003e \\u003cname\\u003e \\u003coccurrence\\u003e\\n        items = line[1].split()\\n\\n        if len(items) != 3:\\n            report_error(\\"line %d: invalid property\\" % line[0])\\n\\n        if items[2] not in valid_occurrences:\\n            report_error(\\"line %d: invalid occurrence\\" % line[0])\\n\\n        parameters.append(Parameter(type=items[0], name=items[1],\\n                                    occurrence=items[2]))\\n\\n    return Method(name=name, parameters=parameters, returns=returns)\\n\\n\\n\\ndef is_known_type(type):\\n    return type in predefined_objects or \\\\\\n           type in predefined_enums or \\\\\\n           type in objects_by_name or \\\\\\n           type in managed_objects_by_name or \\\\\\n           type in enums_by_name\\n\\n\\n\\ndef open_and_print(filename):\\n    if filename.startswith(\\"./\\"):\\n        print \\"  GEN    \\" + filename[2:]\\n    else:\\n        print \\"  GEN    \\" + filename\\n\\n    return open(filename, \\"wb\\")\\n\\n\\n\\npredefined_enums = [\\"Boolean\\"]\\n\\npredefined_objects = [\\"AnyType\\",\\n                      \\"Int\\",\\n                      \\"Long\\",\\n                      \\"String\\",\\n                      \\"DateTime\\",\\n                      \\"MethodFault\\",\\n                      \\"ManagedObjectReference\\"]\\n\\nadditional_enum_features = { \\"ManagedEntityStatus\\"      : Enum.FEATURE__ANY_TYPE,\\n                             \\"TaskInfoState\\"            : Enum.FEATURE__ANY_TYPE,\\n                             \\"VirtualMachinePowerState\\" : Enum.FEATURE__ANY_TYPE }\\n\\nadditional_object_features = { \\"AutoStartDefaults\\"          : Object.FEATURE__ANY_TYPE,\\n                               \\"AutoStartPowerInfo\\"         : Object.FEATURE__ANY_TYPE,\\n                               \\"DatastoreHostMount\\"         : Object.FEATURE__DEEP_COPY |\\n                                                              Object.FEATURE__LIST |\\n                                                              Object.FEATURE__ANY_TYPE,\\n                               \\"DatastoreInfo\\"              : Object.FEATURE__ANY_TYPE |\\n                                                              Object.FEATURE__DYNAMIC_CAST,\\n                               \\"HostConfigManager\\"          : Object.FEATURE__ANY_TYPE,\\n                               \\"HostCpuIdInfo\\"              : Object.FEATURE__LIST |\\n                                                              Object.FEATURE__ANY_TYPE,\\n                               \\"HostDatastoreBrowserSearchResults\\" : Object.FEATURE__LIST |\\n                                                              Object.FEATURE__ANY_TYPE,\\n                               \\"ManagedObjectReference\\"     : Object.FEATURE__ANY_TYPE,\\n                               \\"ObjectContent\\"              : Object.FEATURE__DEEP_COPY,\\n                               \\"ResourcePoolResourceUsage\\"  : Object.FEATURE__ANY_TYPE,\\n                               \\"ServiceContent\\"             : Object.FEATURE__DESERIALIZE,\\n                               \\"SharesInfo\\"                 : Object.FEATURE__ANY_TYPE,\\n                               \\"TaskInfo\\"                   : Object.FEATURE__LIST |\\n                                                              Object.FEATURE__ANY_TYPE,\\n                               \\"UserSession\\"                : Object.FEATURE__ANY_TYPE,\\n                               \\"VirtualMachineQuestionInfo\\" : Object.FEATURE__ANY_TYPE,\\n                               \\"VirtualMachineSnapshotTree\\" : Object.FEATURE__DEEP_COPY |\\n                                                              Object.FEATURE__ANY_TYPE,\\n                               \\"VmEventArgument\\"            : Object.FEATURE__DESERIALIZE }\\n\\nremoved_object_features = {}\\n\\n\\n\\nif \\"srcdir\\" in os.environ:\\n    input_filename = os.path.join(os.environ[\\"srcdir\\"], \\"esx/esx_vi_generator.input\\")\\n    output_dirname = os.path.join(os.environ[\\"srcdir\\"], \\"esx\\")\\nelse:\\n    input_filename = os.path.join(os.getcwd(), \\"esx_vi_generator.input\\")\\n    output_dirname = os.getcwd()\\n\\n\\n\\ntypes_typedef = open_and_print(os.path.join(output_dirname, \\"esx_vi_types.generated.typedef\\"))\\ntypes_typeenum = open_and_print(os.path.join(output_dirname, \\"esx_vi_types.generated.typeenum\\"))\\ntypes_typetostring = open_and_print(os.path.join(output_dirname, \\"esx_vi_types.generated.typetostring\\"))\\ntypes_typefromstring = open_and_print(os.path.join(output_dirname, \\"esx_vi_types.generated.typefromstring\\"))\\ntypes_header = open_and_print(os.path.join(output_dirname, \\"esx_vi_types.generated.h\\"))\\ntypes_source = open_and_print(os.path.join(output_dirname, \\"esx_vi_types.generated.c\\"))\\nmethods_header = open_and_print(os.path.join(output_dirname, \\"esx_vi_methods.generated.h\\"))\\nmethods_source = open_and_print(os.path.join(output_dirname, \\"esx_vi_methods.generated.c\\"))\\nmethods_macro = open_and_print(os.path.join(output_dirname, \\"esx_vi_methods.generated.macro\\"))\\nhelpers_header = open_and_print(os.path.join(output_dirname, \\"esx_vi.generated.h\\"))\\nhelpers_source = open_and_print(os.path.join(output_dirname, \\"esx_vi.generated.c\\"))\\n\\n\\n\\nnumber = 0\\nobjects_by_name = {}\\nmanaged_objects_by_name = {}\\nenums_by_name = {}\\nmethods_by_name = {}\\nblock = None\\n\\n\\n\\n# parse input file\\nfor line in file(input_filename, \\"rb\\").readlines():\\n    number += 1\\n\\n    if \\"#\\" in line:\\n        line = line[:line.index(\\"#\\")]\\n\\n    line = line.lstrip().rstrip()\\n\\n    if len(line) \\u003c 1:\\n        continue\\n\\n    if line.startswith(\\"object\\") or line.startswith(\\"managed object\\") or \\\\\\n       line.startswith(\\"enum\\") or line.startswith(\\"method\\"):\\n        if block is not None:\\n            report_error(\\"line %d: nested block found\\" % (number))\\n        else:\\n            block = []\\n\\n    if block is not None:\\n        if line == \\"end\\":\\n            if block[0][1].startswith(\\"object\\"):\\n                obj = parse_object(block)\\n                objects_by_name[obj.name] = obj\\n            elif block[0][1].startswith(\\"managed object\\"):\\n                obj = parse_object(block)\\n                managed_objects_by_name[obj.name] = obj\\n            elif block[0][1].startswith(\\"enum\\"):\\n                enum = parse_enum(block)\\n                enums_by_name[enum.name] = enum\\n            else:\\n                method = parse_method(block)\\n                methods_by_name[method.name] = method\\n\\n            block = None\\n        else:\\n            block.append((number, line))\\n\\n\\n\\nfor method in methods_by_name.values():\\n    # method parameter types must be serializable\\n    for parameter in method.parameters:\\n        if not parameter.is_type_generated():\\n            continue\\n\\n        if parameter.is_enum():\\n            enums_by_name[parameter.type].features |= Enum.FEATURE__SERIALIZE\\n        else:\\n            objects_by_name[parameter.type].features |= Object.FEATURE__SERIALIZE\\n            objects_by_name[parameter.type].candidate_for_dynamic_cast = True\\n\\n        # detect list usage\\n        if parameter.occurrence == OCCURRENCE__REQUIRED_LIST or \\\\\\n           parameter.occurrence == OCCURRENCE__OPTIONAL_LIST:\\n            if parameter.is_enum():\\n                report_error(\\"unsupported usage of enum \'%s\' as list in \'%s\'\\"\\n                             % (parameter.type, method.name))\\n            else:\\n                objects_by_name[parameter.type].features |= Object.FEATURE__LIST\\n\\n    # method return types must be deserializable\\n    if method.returns and method.returns.is_type_generated():\\n        if method.returns.is_enum():\\n            enums_by_name[method.returns.type].features |= Enum.FEATURE__DESERIALIZE\\n        else:\\n            objects_by_name[method.returns.type].features |= Object.FEATURE__DESERIALIZE\\n            objects_by_name[method.returns.type].candidate_for_dynamic_cast = True\\n\\n        # detect list usage\\n        if method.returns.occurrence == OCCURRENCE__REQUIRED_LIST or \\\\\\n           method.returns.occurrence == OCCURRENCE__OPTIONAL_LIST:\\n            if method.returns.is_enum():\\n                report_error(\\"unsupported usage of enum \'%s\' as list in \'%s\'\\"\\n                             % (method.returns.type, method.name))\\n            else:\\n                objects_by_name[method.returns.type].features |= Object.FEATURE__LIST\\n\\n\\n\\nfor enum in enums_by_name.values():\\n    # apply additional features\\n    if enum.name in additional_enum_features:\\n        enum.features |= additional_enum_features[enum.name]\\n\\n        if additional_enum_features[enum.name] \\u0026 Enum.FEATURE__ANY_TYPE:\\n            enum.features |= Enum.FEATURE__DESERIALIZE\\n\\n\\n\\nfor obj in objects_by_name.values():\\n    for property in obj.properties:\\n        if property.occurrence != OCCURRENCE__IGNORED and \\\\\\n           not is_known_type(property.type):\\n            report_error(\\"object \'%s\' contains unknown property type \'%s\'\\"\\n                         % (obj.name, property.type))\\n\\n    if obj.extends is not None:\\n        if not is_known_type(obj.extends):\\n            report_error(\\"object \'%s\' extends unknown object \'%s\'\\"\\n                         % (obj.name, obj.extends))\\n\\n    for property in obj.properties:\\n        if not property.is_type_generated():\\n            continue\\n\\n        if property.is_enum():\\n            enums_by_name[property.type].candidate_for_dynamic_cast = True\\n        else:\\n            objects_by_name[property.type].candidate_for_dynamic_cast = True\\n\\n        # detect list usage\\n        if property.occurrence == OCCURRENCE__REQUIRED_LIST or \\\\\\n           property.occurrence == OCCURRENCE__OPTIONAL_LIST:\\n            if property.is_enum():\\n                report_error(\\"unsupported usage of enum \'%s\' as list in \'%s\'\\"\\n                             % (property.type, obj.type))\\n            else:\\n                objects_by_name[property.type].features |= Object.FEATURE__LIST\\n\\n    # apply/remove additional features\\n    if obj.name in additional_object_features:\\n        obj.features |= additional_object_features[obj.name]\\n\\n        if additional_object_features[obj.name] \\u0026 Object.FEATURE__ANY_TYPE:\\n            obj.features |= Object.FEATURE__DESERIALIZE\\n\\n    if obj.name in removed_object_features:\\n        obj.features \\u0026= ~removed_object_features[obj.name]\\n\\n    # detect extended_by relation\\n    if obj.extends is not None:\\n        extended_obj = objects_by_name[obj.extends]\\n\\n        if extended_obj.extended_by is None:\\n            extended_obj.extended_by = [obj.name]\\n        else:\\n            extended_obj.extended_by.append(obj.name)\\n            extended_obj.extended_by.sort()\\n\\n\\n\\nfor obj in objects_by_name.values():\\n    # if an object is a candidate (it is used directly as parameter or return\\n    # type or is a member of another object) and it is extended by another\\n    # object then this type needs the dynamic cast feature\\n    if obj.candidate_for_dynamic_cast and obj.extended_by:\\n        obj.features |= Object.FEATURE__DYNAMIC_CAST\\n\\n\\n\\ndef propagate_feature(obj, feature):\\n    global features_have_changed\\n\\n    if not (obj.features \\u0026 feature):\\n        return\\n\\n    for property in obj.properties:\\n        if property.occurrence == OCCURRENCE__IGNORED or \\\\\\n           not property.is_type_generated():\\n            continue\\n\\n        if property.is_enum():\\n            if feature == Object.FEATURE__SERIALIZE and \\\\\\n               not (enums_by_name[property.type].features \\u0026 Enum.FEATURE__SERIALIZE):\\n                enums_by_name[property.type].features |= Enum.FEATURE__SERIALIZE\\n                features_have_changed = True\\n            elif feature == Object.FEATURE__DESERIALIZE and \\\\\\n               not (enums_by_name[property.type].features \\u0026 Enum.FEATURE__DESERIALIZE):\\n                enums_by_name[property.type].features |= Enum.FEATURE__DESERIALIZE\\n                features_have_changed = True\\n        elif property.is_object():\\n            if not (objects_by_name[property.type].features \\u0026 feature):\\n                objects_by_name[property.type].features |= feature\\n                features_have_changed = True\\n\\n            if obj.name != property.type:\\n                propagate_feature(objects_by_name[property.type], feature)\\n\\n\\n\\ndef inherit_features(obj):\\n    global features_have_changed\\n\\n    if obj.extended_by is not None:\\n        for extended_by in obj.extended_by:\\n            previous = objects_by_name[extended_by].features\\n            objects_by_name[extended_by].features |= obj.features\\n\\n            if objects_by_name[extended_by].features != previous:\\n                features_have_changed = True\\n\\n    if obj.extends is not None:\\n        previous = objects_by_name[obj.extends].features\\n        objects_by_name[obj.extends].features |= obj.features\\n\\n        if objects_by_name[obj.extends].features != previous:\\n            features_have_changed = True\\n\\n    if obj.extended_by is not None:\\n        for extended_by in obj.extended_by:\\n            inherit_features(objects_by_name[extended_by])\\n\\n\\n\\n# there are two directions to spread features:\\n# 1) up and down the inheritance chain\\n# 2) from object types to their member property types\\n# spreading needs to be done alternating on both directions because they can\\n# affect each other\\nfeatures_have_changed = True\\n\\nwhile features_have_changed:\\n    features_have_changed = False\\n\\n    for obj in objects_by_name.values():\\n        propagate_feature(obj, Object.FEATURE__DEEP_COPY)\\n        propagate_feature(obj, Object.FEATURE__SERIALIZE)\\n        propagate_feature(obj, Object.FEATURE__DESERIALIZE)\\n\\n    for obj in objects_by_name.values():\\n        inherit_features(obj)\\n\\n\\n\\nfor obj in managed_objects_by_name.values():\\n    for property in obj.properties:\\n        if property.occurrence != OCCURRENCE__IGNORED and \\\\\\n           not is_known_type(property.type):\\n            report_error(\\"object \'%s\' contains unknown property type \'%s\'\\"\\n                         % (obj.name, property.type))\\n\\n    if obj.extends is not None:\\n        if not is_known_type(obj.extends):\\n            report_error(\\"object \'%s\' extends unknown object \'%s\'\\"\\n                         % (obj.name, obj.extends))\\n\\n    # detect extended_by relation\\n    if obj.extends is not None:\\n        extended_obj = managed_objects_by_name[obj.extends]\\n\\n        if extended_obj.extended_by is None:\\n            extended_obj.extended_by = [obj.name]\\n        else:\\n            extended_obj.extended_by.append(obj.name)\\n            extended_obj.extended_by.sort()\\n\\n\\n\\nnotice = \\"/* Generated by esx_vi_generator.py */\\\\n\\\\n\\\\n\\\\n\\"\\n\\ntypes_typedef.write(notice)\\ntypes_typeenum.write(notice)\\ntypes_typetostring.write(notice)\\ntypes_typefromstring.write(notice)\\ntypes_header.write(notice)\\ntypes_source.write(notice)\\nmethods_header.write(notice)\\nmethods_source.write(notice)\\nmethods_macro.write(notice)\\nhelpers_header.write(notice)\\nhelpers_source.write(notice)\\n\\n\\n\\n# output enums\\ntypes_typedef.write(separator +\\n                    \\" * VI Enums\\\\n\\" +\\n                    \\" */\\\\n\\\\n\\")\\n\\nnames = enums_by_name.keys()\\nnames.sort()\\n\\nfor name in names:\\n    types_typedef.write(enums_by_name[name].generate_typedef())\\n    types_typeenum.write(enums_by_name[name].generate_typeenum())\\n    types_typetostring.write(enums_by_name[name].generate_typetostring())\\n    types_typefromstring.write(enums_by_name[name].generate_typefromstring())\\n    types_header.write(enums_by_name[name].generate_header())\\n    types_source.write(enums_by_name[name].generate_source())\\n\\n\\n\\n# output objects\\ntypes_typedef.write(\\"\\\\n\\\\n\\\\n\\" +\\n                    separator +\\n                    \\" * VI Objects\\\\n\\" +\\n                    \\" */\\\\n\\\\n\\")\\ntypes_typeenum.write(\\"\\\\n\\")\\ntypes_typetostring.write(\\"\\\\n\\")\\ntypes_typefromstring.write(\\"\\\\n\\")\\n\\nnames = objects_by_name.keys()\\nnames.sort()\\n\\nfor name in names:\\n    types_typedef.write(objects_by_name[name].generate_typedef())\\n    types_typeenum.write(objects_by_name[name].generate_typeenum())\\n    types_typetostring.write(objects_by_name[name].generate_typetostring())\\n    types_typefromstring.write(objects_by_name[name].generate_typefromstring())\\n    types_header.write(objects_by_name[name].generate_header())\\n    types_source.write(objects_by_name[name].generate_source())\\n\\n\\n\\n# output managed objects\\ntypes_typedef.write(\\"\\\\n\\\\n\\\\n\\" +\\n                    separator +\\n                    \\" * VI Managed Objects\\\\n\\" +\\n                    \\" */\\\\n\\\\n\\")\\ntypes_typeenum.write(\\"\\\\n\\")\\ntypes_typetostring.write(\\"\\\\n\\")\\ntypes_typefromstring.write(\\"\\\\n\\")\\n\\nnames = managed_objects_by_name.keys()\\nnames.sort()\\n\\nfor name in names:\\n    types_typedef.write(managed_objects_by_name[name].generate_typedef())\\n    types_typeenum.write(managed_objects_by_name[name].generate_typeenum())\\n    types_typetostring.write(managed_objects_by_name[name].generate_typetostring())\\n    types_typefromstring.write(managed_objects_by_name[name].generate_typefromstring())\\n    types_header.write(managed_objects_by_name[name].generate_header())\\n    types_source.write(managed_objects_by_name[name].generate_source())\\n\\n\\n\\n# output methods\\nnames = methods_by_name.keys()\\nnames.sort()\\n\\nfor name in names:\\n    methods_header.write(methods_by_name[name].generate_header())\\n    methods_source.write(methods_by_name[name].generate_source())\\n\\nnames = list(autobind_names)\\nnames.sort()\\n\\nfor name in names:\\n    string = aligned(\\"#define ESX_VI__METHOD__PARAMETER__THIS__%s \\" % name, \\"\\\\\\\\\\\\n\\", 78)\\n    string += \\"    ESX_VI__METHOD__PARAMETER__THIS_FROM_SERVICE(ManagedObjectReference,      \\\\\\\\\\\\n\\"\\n    string += aligned(\\"\\", \\"%s)\\\\n\\\\n\\\\n\\\\n\\" % name, 49)\\n\\n    methods_macro.write(string)\\n\\n\\n\\n# output helpers\\nnames = managed_objects_by_name.keys()\\nnames.sort()\\n\\nfor name in names:\\n    helpers_header.write(managed_objects_by_name[name].generate_helper_header())\\n    helpers_source.write(managed_objects_by_name[name].generate_helper_source())\\n"}\n'
line: b'{"repo_name":"patrickstocklin/chattR","ref":"refs/heads/master","path":"lib/python2.7/site-packages/pip/index.py","content":"\\"\\"\\"Routines related to PyPI, indexes\\"\\"\\"\\nfrom __future__ import absolute_import\\n\\nimport logging\\nimport cgi\\nfrom collections import namedtuple\\nimport itertools\\nimport sys\\nimport os\\nimport re\\nimport mimetypes\\nimport posixpath\\nimport warnings\\n\\nfrom pip._vendor.six.moves.urllib import parse as urllib_parse\\nfrom pip._vendor.six.moves.urllib import request as urllib_request\\n\\nfrom pip.compat import ipaddress\\nfrom pip.utils import (\\n    Inf, cached_property, normalize_name, splitext, normalize_path,\\n    ARCHIVE_EXTENSIONS, SUPPORTED_EXTENSIONS)\\nfrom pip.utils.deprecation import RemovedInPip8Warning\\nfrom pip.utils.logging import indent_log\\nfrom pip.exceptions import (\\n    DistributionNotFound, BestVersionAlreadyInstalled, InvalidWheelFilename,\\n    UnsupportedWheel,\\n)\\nfrom pip.download import HAS_TLS, url_to_path, path_to_url\\nfrom pip.models import PyPI\\nfrom pip.wheel import Wheel, wheel_ext\\nfrom pip.pep425tags import supported_tags, supported_tags_noarch, get_platform\\nfrom pip._vendor import html5lib, requests, pkg_resources, six\\nfrom pip._vendor.packaging.version import parse as parse_version\\nfrom pip._vendor.requests.exceptions import SSLError\\n\\n\\n__all__ = [\'FormatControl\', \'fmt_ctl_handle_mutual_exclude\', \'PackageFinder\']\\n\\n\\n# Taken from Chrome\'s list of secure origins (See: http://bit.ly/1qrySKC)\\nSECURE_ORIGINS = [\\n    # protocol, hostname, port\\n    (\\"https\\", \\"*\\", \\"*\\"),\\n    (\\"*\\", \\"localhost\\", \\"*\\"),\\n    (\\"*\\", \\"127.0.0.0/8\\", \\"*\\"),\\n    (\\"*\\", \\"::1/128\\", \\"*\\"),\\n    (\\"file\\", \\"*\\", None),\\n]\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass InstallationCandidate(object):\\n\\n    def __init__(self, project, version, location):\\n        self.project = project\\n        self.version = parse_version(version)\\n        self.location = location\\n        self._key = (self.project, self.version, self.location)\\n\\n    def __repr__(self):\\n        return \\"\\u003cInstallationCandidate({0!r}, {1!r}, {2!r})\\u003e\\".format(\\n            self.project, self.version, self.location,\\n        )\\n\\n    def __hash__(self):\\n        return hash(self._key)\\n\\n    def __lt__(self, other):\\n        return self._compare(other, lambda s, o: s \\u003c o)\\n\\n    def __le__(self, other):\\n        return self._compare(other, lambda s, o: s \\u003c= o)\\n\\n    def __eq__(self, other):\\n        return self._compare(other, lambda s, o: s == o)\\n\\n    def __ge__(self, other):\\n        return self._compare(other, lambda s, o: s \\u003e= o)\\n\\n    def __gt__(self, other):\\n        return self._compare(other, lambda s, o: s \\u003e o)\\n\\n    def __ne__(self, other):\\n        return self._compare(other, lambda s, o: s != o)\\n\\n    def _compare(self, other, method):\\n        if not isinstance(other, InstallationCandidate):\\n            return NotImplemented\\n\\n        return method(self._key, other._key)\\n\\n\\nclass PackageFinder(object):\\n    \\"\\"\\"This finds packages.\\n\\n    This is meant to match easy_install\'s technique for looking for\\n    packages, by reading pages and looking for appropriate links.\\n    \\"\\"\\"\\n\\n    def __init__(self, find_links, index_urls,\\n                 allow_external=(), allow_unverified=(),\\n                 allow_all_external=False, allow_all_prereleases=False,\\n                 trusted_hosts=None, process_dependency_links=False,\\n                 session=None, format_control=None):\\n        \\"\\"\\"Create a PackageFinder.\\n\\n        :param format_control: A FormatControl object or None. Used to control\\n            the selection of source packages / binary packages when consulting\\n            the index and links.\\n        \\"\\"\\"\\n        if session is None:\\n            raise TypeError(\\n                \\"PackageFinder() missing 1 required keyword argument: \\"\\n                \\"\'session\'\\"\\n            )\\n\\n        # Build find_links. If an argument starts with ~, it may be\\n        # a local file relative to a home directory. So try normalizing\\n        # it and if it exists, use the normalized version.\\n        # This is deliberately conservative - it might be fine just to\\n        # blindly normalize anything starting with a ~...\\n        self.find_links = []\\n        for link in find_links:\\n            if link.startswith(\'~\'):\\n                new_link = normalize_path(link)\\n                if os.path.exists(new_link):\\n                    link = new_link\\n            self.find_links.append(link)\\n\\n        self.index_urls = index_urls\\n        self.dependency_links = []\\n\\n        # These are boring links that have already been logged somehow:\\n        self.logged_links = set()\\n\\n        self.format_control = format_control or FormatControl(set(), set())\\n\\n        # Do we allow (safe and verifiable) externally hosted files?\\n        self.allow_external = set(normalize_name(n) for n in allow_external)\\n\\n        # Which names are allowed to install insecure and unverifiable files?\\n        self.allow_unverified = set(\\n            normalize_name(n) for n in allow_unverified\\n        )\\n\\n        # Anything that is allowed unverified is also allowed external\\n        self.allow_external |= self.allow_unverified\\n\\n        # Do we allow all (safe and verifiable) externally hosted files?\\n        self.allow_all_external = allow_all_external\\n\\n        # Domains that we won\'t emit warnings for when not using HTTPS\\n        self.secure_origins = [\\n            (\\"*\\", host, \\"*\\")\\n            for host in (trusted_hosts if trusted_hosts else [])\\n        ]\\n\\n        # Stores if we ignored any external links so that we can instruct\\n        #   end users how to install them if no distributions are available\\n        self.need_warn_external = False\\n\\n        # Stores if we ignored any unsafe links so that we can instruct\\n        #   end users how to install them if no distributions are available\\n        self.need_warn_unverified = False\\n\\n        # Do we want to allow _all_ pre-releases?\\n        self.allow_all_prereleases = allow_all_prereleases\\n\\n        # Do we process dependency links?\\n        self.process_dependency_links = process_dependency_links\\n\\n        # The Session we\'ll use to make requests\\n        self.session = session\\n\\n        # If we don\'t have TLS enabled, then WARN if anyplace we\'re looking\\n        # relies on TLS.\\n        if not HAS_TLS:\\n            for link in itertools.chain(self.index_urls, self.find_links):\\n                parsed = urllib_parse.urlparse(link)\\n                if parsed.scheme == \\"https\\":\\n                    logger.warning(\\n                        \\"pip is configured with locations that require \\"\\n                        \\"TLS/SSL, however the ssl module in Python is not \\"\\n                        \\"available.\\"\\n                    )\\n                    break\\n\\n    def add_dependency_links(self, links):\\n        # # FIXME: this shouldn\'t be global list this, it should only\\n        # # apply to requirements of the package that specifies the\\n        # # dependency_links value\\n        # # FIXME: also, we should track comes_from (i.e., use Link)\\n        if self.process_dependency_links:\\n            warnings.warn(\\n                \\"Dependency Links processing has been deprecated and will be \\"\\n                \\"removed in a future release.\\",\\n                RemovedInPip8Warning,\\n            )\\n            self.dependency_links.extend(links)\\n\\n    @staticmethod\\n    def _sort_locations(locations, expand_dir=False):\\n        \\"\\"\\"\\n        Sort locations into \\"files\\" (archives) and \\"urls\\", and return\\n        a pair of lists (files,urls)\\n        \\"\\"\\"\\n        files = []\\n        urls = []\\n\\n        # puts the url for the given file path into the appropriate list\\n        def sort_path(path):\\n            url = path_to_url(path)\\n            if mimetypes.guess_type(url, strict=False)[0] == \'text/html\':\\n                urls.append(url)\\n            else:\\n                files.append(url)\\n\\n        for url in locations:\\n\\n            is_local_path = os.path.exists(url)\\n            is_file_url = url.startswith(\'file:\')\\n\\n            if is_local_path or is_file_url:\\n                if is_local_path:\\n                    path = url\\n                else:\\n                    path = url_to_path(url)\\n                if os.path.isdir(path):\\n                    if expand_dir:\\n                        path = os.path.realpath(path)\\n                        for item in os.listdir(path):\\n                            sort_path(os.path.join(path, item))\\n                    elif is_file_url:\\n                        urls.append(url)\\n                elif os.path.isfile(path):\\n                    sort_path(path)\\n            else:\\n                urls.append(url)\\n\\n        return files, urls\\n\\n    def _candidate_sort_key(self, candidate):\\n        \\"\\"\\"\\n        Function used to generate link sort key for link tuples.\\n        The greater the return value, the more preferred it is.\\n        If not finding wheels, then sorted by version only.\\n        If finding wheels, then the sort order is by version, then:\\n          1. existing installs\\n          2. wheels ordered via Wheel.support_index_min()\\n          3. source archives\\n        Note: it was considered to embed this logic into the Link\\n              comparison operators, but then different sdist links\\n              with the same version, would have to be considered equal\\n        \\"\\"\\"\\n        support_num = len(supported_tags)\\n        if candidate.location == INSTALLED_VERSION:\\n            pri = 1\\n        elif candidate.location.is_wheel:\\n            # can raise InvalidWheelFilename\\n            wheel = Wheel(candidate.location.filename)\\n            if not wheel.supported():\\n                raise UnsupportedWheel(\\n                    \\"%s is not a supported wheel for this platform. It \\"\\n                    \\"can\'t be sorted.\\" % wheel.filename\\n                )\\n            pri = -(wheel.support_index_min())\\n        else:  # sdist\\n            pri = -(support_num)\\n        return (candidate.version, pri)\\n\\n    def _sort_versions(self, applicable_versions):\\n        \\"\\"\\"\\n        Bring the latest version (and wheels) to the front, but maintain the\\n        existing ordering as secondary. See the docstring for `_link_sort_key`\\n        for details. This function is isolated for easier unit testing.\\n        \\"\\"\\"\\n        return sorted(\\n            applicable_versions,\\n            key=self._candidate_sort_key,\\n            reverse=True\\n        )\\n\\n    def _validate_secure_origin(self, logger, location):\\n        # Determine if this url used a secure transport mechanism\\n        parsed = urllib_parse.urlparse(str(location))\\n        origin = (parsed.scheme, parsed.hostname, parsed.port)\\n\\n        # Determine if our origin is a secure origin by looking through our\\n        # hardcoded list of secure origins, as well as any additional ones\\n        # configured on this PackageFinder instance.\\n        for secure_origin in (SECURE_ORIGINS + self.secure_origins):\\n            # Check to see if the protocol matches\\n            if origin[0] != secure_origin[0] and secure_origin[0] != \\"*\\":\\n                continue\\n\\n            try:\\n                # We need to do this decode dance to ensure that we have a\\n                # unicode object, even on Python 2.x.\\n                addr = ipaddress.ip_address(\\n                    origin[1]\\n                    if (\\n                        isinstance(origin[1], six.text_type) or\\n                        origin[1] is None\\n                    )\\n                    else origin[1].decode(\\"utf8\\")\\n                )\\n                network = ipaddress.ip_network(\\n                    secure_origin[1]\\n                    if isinstance(secure_origin[1], six.text_type)\\n                    else secure_origin[1].decode(\\"utf8\\")\\n                )\\n            except ValueError:\\n                # We don\'t have both a valid address or a valid network, so\\n                # we\'ll check this origin against hostnames.\\n                if origin[1] != secure_origin[1] and secure_origin[1] != \\"*\\":\\n                    continue\\n            else:\\n                # We have a valid address and network, so see if the address\\n                # is contained within the network.\\n                if addr not in network:\\n                    continue\\n\\n            # Check to see if the port patches\\n            if (origin[2] != secure_origin[2] and\\n                    secure_origin[2] != \\"*\\" and\\n                    secure_origin[2] is not None):\\n                continue\\n\\n            # If we\'ve gotten here, then this origin matches the current\\n            # secure origin and we should return True\\n            return True\\n\\n        # If we\'ve gotten to this point, then the origin isn\'t secure and we\\n        # will not accept it as a valid location to search. We will however\\n        # log a warning that we are ignoring it.\\n        logger.warning(\\n            \\"The repository located at %s is not a trusted or secure host and \\"\\n            \\"is being ignored. If this repository is available via HTTPS it \\"\\n            \\"is recommended to use HTTPS instead, otherwise you may silence \\"\\n            \\"this warning and allow it anyways with \'--trusted-host %s\'.\\",\\n            parsed.hostname,\\n            parsed.hostname,\\n        )\\n\\n        return False\\n\\n    def _get_index_urls_locations(self, project_name):\\n        \\"\\"\\"Returns the locations found via self.index_urls\\n\\n        Checks the url_name on the main (first in the list) index and\\n        use this url_name to produce all locations\\n        \\"\\"\\"\\n\\n        def mkurl_pypi_url(url):\\n            loc = posixpath.join(url, project_url_name)\\n            # For maximum compatibility with easy_install, ensure the path\\n            # ends in a trailing slash.  Although this isn\'t in the spec\\n            # (and PyPI can handle it without the slash) some other index\\n            # implementations might break if they relied on easy_install\'s\\n            # behavior.\\n            if not loc.endswith(\'/\'):\\n                loc = loc + \'/\'\\n            return loc\\n\\n        project_url_name = urllib_parse.quote(project_name.lower())\\n\\n        if self.index_urls:\\n            # Check that we have the url_name correctly spelled:\\n\\n            # Only check main index if index URL is given\\n            main_index_url = Link(\\n                mkurl_pypi_url(self.index_urls[0]),\\n                trusted=True,\\n            )\\n\\n            page = self._get_page(main_index_url)\\n            if page is None and PyPI.netloc not in str(main_index_url):\\n                warnings.warn(\\n                    \\"Failed to find %r at %s. It is suggested to upgrade \\"\\n                    \\"your index to support normalized names as the name in \\"\\n                    \\"/simple/{name}.\\" % (project_name, main_index_url),\\n                    RemovedInPip8Warning,\\n                )\\n\\n                project_url_name = self._find_url_name(\\n                    Link(self.index_urls[0], trusted=True),\\n                    project_url_name,\\n                ) or project_url_name\\n\\n        if project_url_name is not None:\\n            return [mkurl_pypi_url(url) for url in self.index_urls]\\n        return []\\n\\n    def _find_all_versions(self, project_name):\\n        \\"\\"\\"Find all available versions for project_name\\n\\n        This checks index_urls, find_links and dependency_links\\n        All versions found are returned\\n\\n        See _link_package_versions for details on which files are accepted\\n        \\"\\"\\"\\n        index_locations = self._get_index_urls_locations(project_name)\\n        index_file_loc, index_url_loc = self._sort_locations(index_locations)\\n        fl_file_loc, fl_url_loc = self._sort_locations(\\n            self.find_links, expand_dir=True)\\n        dep_file_loc, dep_url_loc = self._sort_locations(self.dependency_links)\\n\\n        file_locations = (\\n            Link(url) for url in itertools.chain(\\n                index_file_loc, fl_file_loc, dep_file_loc)\\n        )\\n\\n        # We trust every url that the user has given us whether it was given\\n        #   via --index-url or --find-links\\n        # We explicitly do not trust links that came from dependency_links\\n        # We want to filter out any thing which does not have a secure origin.\\n        url_locations = [\\n            link for link in itertools.chain(\\n                (Link(url, trusted=True) for url in index_url_loc),\\n                (Link(url, trusted=True) for url in fl_url_loc),\\n                (Link(url) for url in dep_url_loc),\\n            )\\n            if self._validate_secure_origin(logger, link)\\n        ]\\n\\n        logger.debug(\'%d location(s) to search for versions of %s:\',\\n                     len(url_locations), project_name)\\n\\n        for location in url_locations:\\n            logger.debug(\'* %s\', location)\\n\\n        canonical_name = pkg_resources.safe_name(project_name).lower()\\n        formats = fmt_ctl_formats(self.format_control, canonical_name)\\n        search = Search(project_name.lower(), canonical_name, formats)\\n        find_links_versions = self._package_versions(\\n            # We trust every directly linked archive in find_links\\n            (Link(url, \'-f\', trusted=True) for url in self.find_links),\\n            search\\n        )\\n\\n        page_versions = []\\n        for page in self._get_pages(url_locations, project_name):\\n            logger.debug(\'Analyzing links from page %s\', page.url)\\n            with indent_log():\\n                page_versions.extend(\\n                    self._package_versions(page.links, search)\\n                )\\n\\n        dependency_versions = self._package_versions(\\n            (Link(url) for url in self.dependency_links), search\\n        )\\n        if dependency_versions:\\n            logger.debug(\\n                \'dependency_links found: %s\',\\n                \', \'.join([\\n                    version.location.url for version in dependency_versions\\n                ])\\n            )\\n\\n        file_versions = self._package_versions(file_locations, search)\\n        if file_versions:\\n            file_versions.sort(reverse=True)\\n            logger.debug(\\n                \'Local files found: %s\',\\n                \', \'.join([\\n                    url_to_path(candidate.location.url)\\n                    for candidate in file_versions\\n                ])\\n            )\\n\\n        # This is an intentional priority ordering\\n        return (\\n            file_versions + find_links_versions + page_versions +\\n            dependency_versions\\n        )\\n\\n    def find_requirement(self, req, upgrade):\\n        \\"\\"\\"Try to find an InstallationCandidate for req\\n\\n        Expects req, an InstallRequirement and upgrade, a boolean\\n        Returns an InstallationCandidate or None\\n        May raise DistributionNotFound or BestVersionAlreadyInstalled\\n        \\"\\"\\"\\n        all_versions = self._find_all_versions(req.name)\\n\\n        # Filter out anything which doesn\'t match our specifier\\n        _versions = set(\\n            req.specifier.filter(\\n                # We turn the version object into a str here because otherwise\\n                # when we\'re debundled but setuptools isn\'t, Python will see\\n                # packaging.version.Version and\\n                # pkg_resources._vendor.packaging.version.Version as different\\n                # types. This way we\'ll use a str as a common data interchange\\n                # format. If we stop using the pkg_resources provided specifier\\n                # and start using our own, we can drop the cast to str().\\n                [str(x.version) for x in all_versions],\\n                prereleases=(\\n                    self.allow_all_prereleases\\n                    if self.allow_all_prereleases else None\\n                ),\\n            )\\n        )\\n        applicable_versions = [\\n            # Again, converting to str to deal with debundling.\\n            x for x in all_versions if str(x.version) in _versions\\n        ]\\n\\n        if req.satisfied_by is not None:\\n            # Finally add our existing versions to the front of our versions.\\n            applicable_versions.insert(\\n                0,\\n                InstallationCandidate(\\n                    req.name,\\n                    req.satisfied_by.version,\\n                    INSTALLED_VERSION,\\n                )\\n            )\\n            existing_applicable = True\\n        else:\\n            existing_applicable = False\\n\\n        applicable_versions = self._sort_versions(applicable_versions)\\n\\n        if not upgrade and existing_applicable:\\n            if applicable_versions[0].location is INSTALLED_VERSION:\\n                logger.debug(\\n                    \'Existing installed version (%s) is most up-to-date and \'\\n                    \'satisfies requirement\',\\n                    req.satisfied_by.version,\\n                )\\n            else:\\n                logger.debug(\\n                    \'Existing installed version (%s) satisfies requirement \'\\n                    \'(most up-to-date version is %s)\',\\n                    req.satisfied_by.version,\\n                    applicable_versions[0][2],\\n                )\\n            return None\\n\\n        if not applicable_versions:\\n            logger.critical(\\n                \'Could not find a version that satisfies the requirement %s \'\\n                \'(from versions: %s)\',\\n                req,\\n                \', \'.join(\\n                    sorted(\\n                        set(str(i.version) for i in all_versions),\\n                        key=parse_version,\\n                    )\\n                )\\n            )\\n\\n            if self.need_warn_external:\\n                logger.warning(\\n                    \\"Some externally hosted files were ignored as access to \\"\\n                    \\"them may be unreliable (use --allow-external %s to \\"\\n                    \\"allow).\\",\\n                    req.name,\\n                )\\n\\n            if self.need_warn_unverified:\\n                logger.warning(\\n                    \\"Some insecure and unverifiable files were ignored\\"\\n                    \\" (use --allow-unverified %s to allow).\\",\\n                    req.name,\\n                )\\n\\n            raise DistributionNotFound(\\n                \'No matching distribution found for %s\' % req\\n            )\\n\\n        if applicable_versions[0].location is INSTALLED_VERSION:\\n            # We have an existing version, and its the best version\\n            logger.debug(\\n                \'Installed version (%s) is most up-to-date (past versions: \'\\n                \'%s)\',\\n                req.satisfied_by.version,\\n                \', \'.join(str(i.version) for i in applicable_versions[1:]) or\\n                \\"none\\",\\n            )\\n            raise BestVersionAlreadyInstalled\\n\\n        if len(applicable_versions) \\u003e 1:\\n            logger.debug(\\n                \'Using version %s (newest of versions: %s)\',\\n                applicable_versions[0].version,\\n                \', \'.join(str(i.version) for i in applicable_versions)\\n            )\\n\\n        selected_version = applicable_versions[0].location\\n\\n        if (selected_version.verifiable is not None and not\\n                selected_version.verifiable):\\n            logger.warning(\\n                \\"%s is potentially insecure and unverifiable.\\", req.name,\\n            )\\n\\n        return selected_version\\n\\n    def _find_url_name(self, index_url, url_name):\\n        \\"\\"\\"\\n        Finds the true URL name of a package, when the given name isn\'t quite\\n        correct.\\n        This is usually used to implement case-insensitivity.\\n        \\"\\"\\"\\n        if not index_url.url.endswith(\'/\'):\\n            # Vaguely part of the PyPI API... weird but true.\\n            # FIXME: bad to modify this?\\n            index_url.url += \'/\'\\n        page = self._get_page(index_url)\\n        if page is None:\\n            logger.critical(\'Cannot fetch index base URL %s\', index_url)\\n            return\\n        norm_name = normalize_name(url_name)\\n        for link in page.links:\\n            base = posixpath.basename(link.path.rstrip(\'/\'))\\n            if norm_name == normalize_name(base):\\n                logger.debug(\\n                    \'Real name of requirement %s is %s\', url_name, base,\\n                )\\n                return base\\n        return None\\n\\n    def _get_pages(self, locations, project_name):\\n        \\"\\"\\"\\n        Yields (page, page_url) from the given locations, skipping\\n        locations that have errors, and adding download/homepage links\\n        \\"\\"\\"\\n        all_locations = list(locations)\\n        seen = set()\\n        normalized = normalize_name(project_name)\\n\\n        while all_locations:\\n            location = all_locations.pop(0)\\n            if location in seen:\\n                continue\\n            seen.add(location)\\n\\n            page = self._get_page(location)\\n            if page is None:\\n                continue\\n\\n            yield page\\n\\n            for link in page.rel_links():\\n\\n                if (normalized not in self.allow_external and not\\n                        self.allow_all_external):\\n                    self.need_warn_external = True\\n                    logger.debug(\\n                        \\"Not searching %s for files because external \\"\\n                        \\"urls are disallowed.\\",\\n                        link,\\n                    )\\n                    continue\\n\\n                if (link.trusted is not None and not\\n                        link.trusted and\\n                        normalized not in self.allow_unverified):\\n                    logger.debug(\\n                        \\"Not searching %s for urls, it is an \\"\\n                        \\"untrusted link and cannot produce safe or \\"\\n                        \\"verifiable files.\\",\\n                        link,\\n                    )\\n                    self.need_warn_unverified = True\\n                    continue\\n\\n                all_locations.append(link)\\n\\n    _py_version_re = re.compile(r\'-py([123]\\\\.?[0-9]?)$\')\\n\\n    def _sort_links(self, links):\\n        \\"\\"\\"\\n        Returns elements of links in order, non-egg links first, egg links\\n        second, while eliminating duplicates\\n        \\"\\"\\"\\n        eggs, no_eggs = [], []\\n        seen = set()\\n        for link in links:\\n            if link not in seen:\\n                seen.add(link)\\n                if link.egg_fragment:\\n                    eggs.append(link)\\n                else:\\n                    no_eggs.append(link)\\n        return no_eggs + eggs\\n\\n    def _package_versions(self, links, search):\\n        result = []\\n        for link in self._sort_links(links):\\n            v = self._link_package_versions(link, search)\\n            if v is not None:\\n                result.append(v)\\n        return result\\n\\n    def _log_skipped_link(self, link, reason):\\n        if link not in self.logged_links:\\n            logger.debug(\'Skipping link %s; %s\', link, reason)\\n            self.logged_links.add(link)\\n\\n    def _link_package_versions(self, link, search):\\n        \\"\\"\\"Return an InstallationCandidate or None\\"\\"\\"\\n        platform = get_platform()\\n\\n        version = None\\n        if link.egg_fragment:\\n            egg_info = link.egg_fragment\\n            ext = link.ext\\n        else:\\n            egg_info, ext = link.splitext()\\n            if not ext:\\n                self._log_skipped_link(link, \'not a file\')\\n                return\\n            if ext not in SUPPORTED_EXTENSIONS:\\n                self._log_skipped_link(\\n                    link, \'unsupported archive format: %s\' % ext)\\n                return\\n            if \\"binary\\" not in search.formats and ext == wheel_ext:\\n                self._log_skipped_link(\\n                    link, \'No binaries permitted for %s\' % search.supplied)\\n                return\\n            if \\"macosx10\\" in link.path and ext == \'.zip\':\\n                self._log_skipped_link(link, \'macosx10 one\')\\n                return\\n            if ext == wheel_ext:\\n                try:\\n                    wheel = Wheel(link.filename)\\n                except InvalidWheelFilename:\\n                    self._log_skipped_link(link, \'invalid wheel filename\')\\n                    return\\n                if (pkg_resources.safe_name(wheel.name).lower() !=\\n                        search.canonical):\\n                    self._log_skipped_link(\\n                        link, \'wrong project name (not %s)\' % search.supplied)\\n                    return\\n                if not wheel.supported():\\n                    self._log_skipped_link(\\n                        link, \'it is not compatible with this Python\')\\n                    return\\n                # This is a dirty hack to prevent installing Binary Wheels from\\n                # PyPI unless it is a Windows or Mac Binary Wheel. This is\\n                # paired with a change to PyPI disabling uploads for the\\n                # same. Once we have a mechanism for enabling support for\\n                # binary wheels on linux that deals with the inherent problems\\n                # of binary distribution this can be removed.\\n                comes_from = getattr(link, \\"comes_from\\", None)\\n                if (\\n                        (\\n                            not platform.startswith(\'win\') and not\\n                            platform.startswith(\'macosx\') and not\\n                            platform == \'cli\'\\n                        ) and\\n                        comes_from is not None and\\n                        urllib_parse.urlparse(\\n                            comes_from.url\\n                        ).netloc.endswith(PyPI.netloc)):\\n                    if not wheel.supported(tags=supported_tags_noarch):\\n                        self._log_skipped_link(\\n                            link,\\n                            \\"it is a pypi-hosted binary \\"\\n                            \\"Wheel on an unsupported platform\\",\\n                        )\\n                        return\\n                version = wheel.version\\n\\n        # This should be up by the search.ok_binary check, but see issue 2700.\\n        if \\"source\\" not in search.formats and ext != wheel_ext:\\n            self._log_skipped_link(\\n                link, \'No sources permitted for %s\' % search.supplied)\\n            return\\n\\n        if not version:\\n            version = egg_info_matches(egg_info, search.supplied, link)\\n        if version is None:\\n            self._log_skipped_link(\\n                link, \'wrong project name (not %s)\' % search.supplied)\\n            return\\n\\n        if (link.internal is not None and not\\n                link.internal and not\\n                normalize_name(search.supplied).lower()\\n                in self.allow_external and not\\n                self.allow_all_external):\\n            # We have a link that we are sure is external, so we should skip\\n            #   it unless we are allowing externals\\n            self._log_skipped_link(link, \'it is externally hosted\')\\n            self.need_warn_external = True\\n            return\\n\\n        if (link.verifiable is not None and not\\n                link.verifiable and not\\n                (normalize_name(search.supplied).lower()\\n                    in self.allow_unverified)):\\n            # We have a link that we are sure we cannot verify its integrity,\\n            #   so we should skip it unless we are allowing unsafe installs\\n            #   for this requirement.\\n            self._log_skipped_link(\\n                link, \'it is an insecure and unverifiable file\')\\n            self.need_warn_unverified = True\\n            return\\n\\n        match = self._py_version_re.search(version)\\n        if match:\\n            version = version[:match.start()]\\n            py_version = match.group(1)\\n            if py_version != sys.version[:3]:\\n                self._log_skipped_link(\\n                    link, \'Python version is incorrect\')\\n                return\\n        logger.debug(\'Found link %s, version: %s\', link, version)\\n\\n        return InstallationCandidate(search.supplied, version, link)\\n\\n    def _get_page(self, link):\\n        return HTMLPage.get_page(link, session=self.session)\\n\\n\\ndef egg_info_matches(\\n        egg_info, search_name, link,\\n        _egg_info_re=re.compile(r\'([a-z0-9_.]+)-([a-z0-9_.!+-]+)\', re.I)):\\n    \\"\\"\\"Pull the version part out of a string.\\n\\n    :param egg_info: The string to parse. E.g. foo-2.1\\n    :param search_name: The name of the package this belongs to. None to\\n        infer the name. Note that this cannot unambiguously parse strings\\n        like foo-2-2 which might be foo, 2-2 or foo-2, 2.\\n    :param link: The link the string came from, for logging on failure.\\n    \\"\\"\\"\\n    match = _egg_info_re.search(egg_info)\\n    if not match:\\n        logger.debug(\'Could not parse version from link: %s\', link)\\n        return None\\n    if search_name is None:\\n        full_match = match.group(0)\\n        return full_match[full_match.index(\'-\'):]\\n    name = match.group(0).lower()\\n    # To match the \\"safe\\" name that pkg_resources creates:\\n    name = name.replace(\'_\', \'-\')\\n    # project name and version must be separated by a dash\\n    look_for = search_name.lower() + \\"-\\"\\n    if name.startswith(look_for):\\n        return match.group(0)[len(look_for):]\\n    else:\\n        return None\\n\\n\\nclass HTMLPage(object):\\n    \\"\\"\\"Represents one page, along with its URL\\"\\"\\"\\n\\n    def __init__(self, content, url, headers=None, trusted=None):\\n        # Determine if we have any encoding information in our headers\\n        encoding = None\\n        if headers and \\"Content-Type\\" in headers:\\n            content_type, params = cgi.parse_header(headers[\\"Content-Type\\"])\\n\\n            if \\"charset\\" in params:\\n                encoding = params[\'charset\']\\n\\n        self.content = content\\n        self.parsed = html5lib.parse(\\n            self.content,\\n            encoding=encoding,\\n            namespaceHTMLElements=False,\\n        )\\n        self.url = url\\n        self.headers = headers\\n        self.trusted = trusted\\n\\n    def __str__(self):\\n        return self.url\\n\\n    @classmethod\\n    def get_page(cls, link, skip_archives=True, session=None):\\n        if session is None:\\n            raise TypeError(\\n                \\"get_page() missing 1 required keyword argument: \'session\'\\"\\n            )\\n\\n        url = link.url\\n        url = url.split(\'#\', 1)[0]\\n\\n        # Check for VCS schemes that do not support lookup as web pages.\\n        from pip.vcs import VcsSupport\\n        for scheme in VcsSupport.schemes:\\n            if url.lower().startswith(scheme) and url[len(scheme)] in \'+:\':\\n                logger.debug(\'Cannot look at %s URL %s\', scheme, link)\\n                return None\\n\\n        try:\\n            if skip_archives:\\n                filename = link.filename\\n                for bad_ext in ARCHIVE_EXTENSIONS:\\n                    if filename.endswith(bad_ext):\\n                        content_type = cls._get_content_type(\\n                            url, session=session,\\n                        )\\n                        if content_type.lower().startswith(\'text/html\'):\\n                            break\\n                        else:\\n                            logger.debug(\\n                                \'Skipping page %s because of Content-Type: %s\',\\n                                link,\\n                                content_type,\\n                            )\\n                            return\\n\\n            logger.debug(\'Getting page %s\', url)\\n\\n            # Tack index.html onto file:// URLs that point to directories\\n            (scheme, netloc, path, params, query, fragment) = \\\\\\n                urllib_parse.urlparse(url)\\n            if (scheme == \'file\' and\\n                    os.path.isdir(urllib_request.url2pathname(path))):\\n                # add trailing slash if not present so urljoin doesn\'t trim\\n                # final segment\\n                if not url.endswith(\'/\'):\\n                    url += \'/\'\\n                url = urllib_parse.urljoin(url, \'index.html\')\\n                logger.debug(\' file: URL is directory, getting %s\', url)\\n\\n            resp = session.get(\\n                url,\\n                headers={\\n                    \\"Accept\\": \\"text/html\\",\\n                    \\"Cache-Control\\": \\"max-age=600\\",\\n                },\\n            )\\n            resp.raise_for_status()\\n\\n            # The check for archives above only works if the url ends with\\n            #   something that looks like an archive. However that is not a\\n            #   requirement of an url. Unless we issue a HEAD request on every\\n            #   url we cannot know ahead of time for sure if something is HTML\\n            #   or not. However we can check after we\'ve downloaded it.\\n            content_type = resp.headers.get(\'Content-Type\', \'unknown\')\\n            if not content_type.lower().startswith(\\"text/html\\"):\\n                logger.debug(\\n                    \'Skipping page %s because of Content-Type: %s\',\\n                    link,\\n                    content_type,\\n                )\\n                return\\n\\n            inst = cls(\\n                resp.content, resp.url, resp.headers,\\n                trusted=link.trusted,\\n            )\\n        except requests.HTTPError as exc:\\n            level = 2 if exc.response.status_code == 404 else 1\\n            cls._handle_fail(link, exc, url, level=level)\\n        except requests.ConnectionError as exc:\\n            cls._handle_fail(link, \\"connection error: %s\\" % exc, url)\\n        except requests.Timeout:\\n            cls._handle_fail(link, \\"timed out\\", url)\\n        except SSLError as exc:\\n            reason = (\\"There was a problem confirming the ssl certificate: \\"\\n                      \\"%s\\" % exc)\\n            cls._handle_fail(link, reason, url, level=2, meth=logger.info)\\n        else:\\n            return inst\\n\\n    @staticmethod\\n    def _handle_fail(link, reason, url, level=1, meth=None):\\n        if meth is None:\\n            meth = logger.debug\\n\\n        meth(\\"Could not fetch URL %s: %s - skipping\\", link, reason)\\n\\n    @staticmethod\\n    def _get_content_type(url, session):\\n        \\"\\"\\"Get the Content-Type of the given url, using a HEAD request\\"\\"\\"\\n        scheme, netloc, path, query, fragment = urllib_parse.urlsplit(url)\\n        if scheme not in (\'http\', \'https\'):\\n            # FIXME: some warning or something?\\n            # assertion error?\\n            return \'\'\\n\\n        resp = session.head(url, allow_redirects=True)\\n        resp.raise_for_status()\\n\\n        return resp.headers.get(\\"Content-Type\\", \\"\\")\\n\\n    @cached_property\\n    def api_version(self):\\n        metas = [\\n            x for x in self.parsed.findall(\\".//meta\\")\\n            if x.get(\\"name\\", \\"\\").lower() == \\"api-version\\"\\n        ]\\n        if metas:\\n            try:\\n                return int(metas[0].get(\\"value\\", None))\\n            except (TypeError, ValueError):\\n                pass\\n\\n        return None\\n\\n    @cached_property\\n    def base_url(self):\\n        bases = [\\n            x for x in self.parsed.findall(\\".//base\\")\\n            if x.get(\\"href\\") is not None\\n        ]\\n        if bases and bases[0].get(\\"href\\"):\\n            return bases[0].get(\\"href\\")\\n        else:\\n            return self.url\\n\\n    @property\\n    def links(self):\\n        \\"\\"\\"Yields all links in the page\\"\\"\\"\\n        for anchor in self.parsed.findall(\\".//a\\"):\\n            if anchor.get(\\"href\\"):\\n                href = anchor.get(\\"href\\")\\n                url = self.clean_link(\\n                    urllib_parse.urljoin(self.base_url, href)\\n                )\\n\\n                # Determine if this link is internal. If that distinction\\n                #   doesn\'t make sense in this context, then we don\'t make\\n                #   any distinction.\\n                internal = None\\n                if self.api_version and self.api_version \\u003e= 2:\\n                    # Only api_versions \\u003e= 2 have a distinction between\\n                    #   external and internal links\\n                    internal = bool(\\n                        anchor.get(\\"rel\\") and\\n                        \\"internal\\" in anchor.get(\\"rel\\").split()\\n                    )\\n\\n                yield Link(url, self, internal=internal)\\n\\n    def rel_links(self, rels=(\'homepage\', \'download\')):\\n        \\"\\"\\"Yields all links with the given relations\\"\\"\\"\\n        rels = set(rels)\\n\\n        for anchor in self.parsed.findall(\\".//a\\"):\\n            if anchor.get(\\"rel\\") and anchor.get(\\"href\\"):\\n                found_rels = set(anchor.get(\\"rel\\").split())\\n                # Determine the intersection between what rels were found and\\n                #   what rels were being looked for\\n                if found_rels \\u0026 rels:\\n                    href = anchor.get(\\"href\\")\\n                    url = self.clean_link(\\n                        urllib_parse.urljoin(self.base_url, href)\\n                    )\\n                    yield Link(url, self, trusted=False)\\n\\n    _clean_re = re.compile(r\'[^a-z0-9$\\u0026+,/:;=?@.#%_\\\\\\\\|-]\', re.I)\\n\\n    def clean_link(self, url):\\n        \\"\\"\\"Makes sure a link is fully encoded.  That is, if a \' \' shows up in\\n        the link, it will be rewritten to %20 (while not over-quoting\\n        % or other characters).\\"\\"\\"\\n        return self._clean_re.sub(\\n            lambda match: \'%%%2x\' % ord(match.group(0)), url)\\n\\n\\nclass Link(object):\\n\\n    def __init__(self, url, comes_from=None, internal=None, trusted=None):\\n\\n        # url can be a UNC windows share\\n        if url != Inf and url.startswith(\'\\\\\\\\\\\\\\\\\'):\\n            url = path_to_url(url)\\n\\n        self.url = url\\n        self.comes_from = comes_from\\n        self.internal = internal\\n        self.trusted = trusted\\n\\n    def __str__(self):\\n        if self.comes_from:\\n            return \'%s (from %s)\' % (self.url, self.comes_from)\\n        else:\\n            return str(self.url)\\n\\n    def __repr__(self):\\n        return \'\\u003cLink %s\\u003e\' % self\\n\\n    def __eq__(self, other):\\n        if not isinstance(other, Link):\\n            return NotImplemented\\n        return self.url == other.url\\n\\n    def __ne__(self, other):\\n        if not isinstance(other, Link):\\n            return NotImplemented\\n        return self.url != other.url\\n\\n    def __lt__(self, other):\\n        if not isinstance(other, Link):\\n            return NotImplemented\\n        return self.url \\u003c other.url\\n\\n    def __le__(self, other):\\n        if not isinstance(other, Link):\\n            return NotImplemented\\n        return self.url \\u003c= other.url\\n\\n    def __gt__(self, other):\\n        if not isinstance(other, Link):\\n            return NotImplemented\\n        return self.url \\u003e other.url\\n\\n    def __ge__(self, other):\\n        if not isinstance(other, Link):\\n            return NotImplemented\\n        return self.url \\u003e= other.url\\n\\n    def __hash__(self):\\n        return hash(self.url)\\n\\n    @property\\n    def filename(self):\\n        _, netloc, path, _, _ = urllib_parse.urlsplit(self.url)\\n        name = posixpath.basename(path.rstrip(\'/\')) or netloc\\n        name = urllib_parse.unquote(name)\\n        assert name, (\'URL %r produced no filename\' % self.url)\\n        return name\\n\\n    @property\\n    def scheme(self):\\n        return urllib_parse.urlsplit(self.url)[0]\\n\\n    @property\\n    def netloc(self):\\n        return urllib_parse.urlsplit(self.url)[1]\\n\\n    @property\\n    def path(self):\\n        return urllib_parse.unquote(urllib_parse.urlsplit(self.url)[2])\\n\\n    def splitext(self):\\n        return splitext(posixpath.basename(self.path.rstrip(\'/\')))\\n\\n    @property\\n    def ext(self):\\n        return self.splitext()[1]\\n\\n    @property\\n    def url_without_fragment(self):\\n        scheme, netloc, path, query, fragment = urllib_parse.urlsplit(self.url)\\n        return urllib_parse.urlunsplit((scheme, netloc, path, query, None))\\n\\n    _egg_fragment_re = re.compile(r\'#egg=([^\\u0026]*)\')\\n\\n    @property\\n    def egg_fragment(self):\\n        match = self._egg_fragment_re.search(self.url)\\n        if not match:\\n            return None\\n        return match.group(1)\\n\\n    _hash_re = re.compile(\\n        r\'(sha1|sha224|sha384|sha256|sha512|md5)=([a-f0-9]+)\'\\n    )\\n\\n    @property\\n    def hash(self):\\n        match = self._hash_re.search(self.url)\\n        if match:\\n            return match.group(2)\\n        return None\\n\\n    @property\\n    def hash_name(self):\\n        match = self._hash_re.search(self.url)\\n        if match:\\n            return match.group(1)\\n        return None\\n\\n    @property\\n    def show_url(self):\\n        return posixpath.basename(self.url.split(\'#\', 1)[0].split(\'?\', 1)[0])\\n\\n    @property\\n    def verifiable(self):\\n        \\"\\"\\"\\n        Returns True if this link can be verified after download, False if it\\n        cannot, and None if we cannot determine.\\n        \\"\\"\\"\\n        trusted = self.trusted or getattr(self.comes_from, \\"trusted\\", None)\\n        if trusted is not None and trusted:\\n            # This link came from a trusted source. It *may* be verifiable but\\n            #   first we need to see if this page is operating under the new\\n            #   API version.\\n            try:\\n                api_version = getattr(self.comes_from, \\"api_version\\", None)\\n                api_version = int(api_version)\\n            except (ValueError, TypeError):\\n                api_version = None\\n\\n            if api_version is None or api_version \\u003c= 1:\\n                # This link is either trusted, or it came from a trusted,\\n                #   however it is not operating under the API version 2 so\\n                #   we can\'t make any claims about if it\'s safe or not\\n                return\\n\\n            if self.hash:\\n                # This link came from a trusted source and it has a hash, so we\\n                #   can consider it safe.\\n                return True\\n            else:\\n                # This link came from a trusted source, using the new API\\n                #   version, and it does not have a hash. It is NOT verifiable\\n                return False\\n        elif trusted is not None:\\n            # This link came from an untrusted source and we cannot trust it\\n            return False\\n\\n    @property\\n    def is_wheel(self):\\n        return self.ext == wheel_ext\\n\\n    @property\\n    def is_artifact(self):\\n        \\"\\"\\"\\n        Determines if this points to an actual artifact (e.g. a tarball) or if\\n        it points to an \\"abstract\\" thing like a path or a VCS location.\\n        \\"\\"\\"\\n        from pip.vcs import vcs\\n\\n        if self.scheme in vcs.all_schemes:\\n            return False\\n\\n        return True\\n\\n\\n# An object to represent the \\"link\\" for the installed version of a requirement.\\n# Using Inf as the url makes it sort higher.\\nINSTALLED_VERSION = Link(Inf)\\n\\n\\nFormatControl = namedtuple(\'FormatControl\', \'no_binary only_binary\')\\n\\"\\"\\"This object has two fields, no_binary and only_binary.\\n\\nIf a field is falsy, it isn\'t set. If it is {\':all:\'}, it should match all\\npackages except those listed in the other field. Only one field can be set\\nto {\':all:\'} at a time. The rest of the time exact package name matches\\nare listed, with any given package only showing up in one field at a time.\\n\\"\\"\\"\\n\\n\\ndef fmt_ctl_handle_mutual_exclude(value, target, other):\\n    new = value.split(\',\')\\n    while \':all:\' in new:\\n        other.clear()\\n        target.clear()\\n        target.add(\':all:\')\\n        del new[:new.index(\':all:\') + 1]\\n        if \':none:\' not in new:\\n            # Without a none, we want to discard everything as :all: covers it\\n            return\\n    for name in new:\\n        if name == \':none:\':\\n            target.clear()\\n            continue\\n        name = pkg_resources.safe_name(name).lower()\\n        other.discard(name)\\n        target.add(name)\\n\\n\\ndef fmt_ctl_formats(fmt_ctl, canonical_name):\\n    result = set([\\"binary\\", \\"source\\"])\\n    if canonical_name in fmt_ctl.only_binary:\\n        result.discard(\'source\')\\n    elif canonical_name in fmt_ctl.no_binary:\\n        result.discard(\'binary\')\\n    elif \':all:\' in fmt_ctl.only_binary:\\n        result.discard(\'source\')\\n    elif \':all:\' in fmt_ctl.no_binary:\\n        result.discard(\'binary\')\\n    return frozenset(result)\\n\\n\\ndef fmt_ctl_no_binary(fmt_ctl):\\n    fmt_ctl_handle_mutual_exclude(\\n        \':all:\', fmt_ctl.no_binary, fmt_ctl.only_binary)\\n\\n\\ndef fmt_ctl_no_use_wheel(fmt_ctl):\\n    fmt_ctl_no_binary(fmt_ctl)\\n    warnings.warn(\\n        \'--no-use-wheel is deprecated and will be removed in the future. \'\\n        \' Please use --no-binary :all: instead.\', DeprecationWarning,\\n        stacklevel=2)\\n\\n\\nSearch = namedtuple(\'Search\', \'supplied canonical formats\')\\n\\"\\"\\"Capture key aspects of a search.\\n\\n:attribute supplied: The user supplied package.\\n:attribute canonical: The canonical package name.\\n:attribute formats: The formats allowed for this package. Should be a set\\n    with \'binary\' or \'source\' or both in it.\\n\\"\\"\\"\\n"}\n'

line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/hcf.r","content":"\\n# Program to find the H.C.F of two input number\\n# define a function\\nhcf <- function(x, y) {\\n# choose the smaller number\\nif(x > y) {\\nsmaller = y\\n} else {\\nsmaller = x\\n}\\nfor(i in 1:smaller) {\\nif((x %% i == 0) && (y %% i == 0)) {\\nhcf = i\\n}\\n}\\nreturn(hcf)\\n}\\n# take input from the user\\nnum1 = 12\\nnum2 = 36\\nprint(paste(\'The H.C.F. of\', num1,\'and\', num2,\'is\', hcf(num1, num2)))\\n"        }\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/lcm.r","content":"\\n# Program to find the L.C.M. of two input number\\nlcm <- function(x, y) {\\n# choose the greater number\\nif(x > y) {\\ngreater = x\\n} else {\\ngreater = y\\n}\\nwhile(TRUE) {\\nif((greater %% x == 0) && (greater %% y == 0)) {\\nlcm = greater\\nbreak\\n}\\ngreater = greater + 1\\n}\\nreturn(lcm)\\n}\\n# take input from the user\\nnum1 = 48\\nnum2 = 72\\nprint(paste(\'The L.C.M. of\', num1,\'and\', num2,\'is\', lcm(num1, num2)))\\n"        }\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/matrixmul.r","content": "\\n#creating matrix\\nmatrixmul <- function(){\\nm <- matrix(1:8, nrow=2) \\nn <- matrix(8:15, nrow=2)   \\n\\n#Multiplying matrices \\nprint(m*n) \\n}\\n"       }\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/multiples.r","content": "\\n# R Program to find the multiplicationtable (from 1 to 10)\\n# take input from the user\\nmultiples <- function(){\\nnum = 12\\n# use for loop to iterate 10 times\\nfor(i in 1:10) {\\nprint(paste(num,\'x\', i, \'=\', num*i))\\n}\\n}\\n"       }\n'
line: b'{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/perfect.r","content": "\\nperfect <- function(){\\n    n <- as.integer(readline(prompt = \'Enter a number :\'))\\n    i = 1\\n    s = 0\\n\\n    while (i < n) {\\n      if (n %% i == 0) {\\n        s = s + i\\n      }\\n      i = i + 1\\n    }\\n\\n    if (s == n) {\\n      print(paste(\'The number is perfect :\', n))\\n    } else{\\n      print(paste(\'The number is not perfect :\', n))\\n    }\\n}\\n"       }\n'
line: b'{ "repo_name":"rakzeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/triangle.r","content": "\\n#RProgram to find the area of triangle\\ntriangle <- function(){\\na = 5\\nb = 6\\nc = 7\\n\\n# calculate the semi-perimeter\\ns = (a + b + c) / 2\\n\\n# calculate the area\\narea = (s*(s-a)*(s-b)*(s-c)) ** 0.5\\nprint(area)\\n}\\n" }      \n'
line: b'\n'
python: split train, test and valid ... 
python: train for is 635 lines and 0.0033187950029969215 Go. 
