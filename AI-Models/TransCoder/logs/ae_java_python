params.mono: dict_values([{'train': '/home/gcloud/TransCoder/data/test_dataset/java-python-.with_comments.XLM-syml/train.java_sa.pth', 'valid': '/home/gcloud/TransCoder/data/test_dataset/java-python-.with_comments.XLM-syml/valid.java_sa.pth', 'test': '/home/gcloud/TransCoder/data/test_dataset/java-python-.with_comments.XLM-syml/test.java_sa.pth'}, {'train': '/home/gcloud/TransCoder/data/test_dataset/java-python-.with_comments.XLM-syml/train.python_sa.pth', 'valid': '/home/gcloud/TransCoder/data/test_dataset/java-python-.with_comments.XLM-syml/valid.python_sa.pth', 'test': '/home/gcloud/TransCoder/data/test_dataset/java-python-.with_comments.XLM-syml/test.python_sa.pth'}])
SLURM job: False
0 - Number of nodes: 1
0 - Node ID        : 0
0 - Local rank     : 0
0 - Global rank    : 0
0 - World size     : 1
0 - GPUs per node  : 1
0 - Master         : True
0 - Multi-node     : False
0 - Multi-GPU      : False
0 - Hostname       : auto-code-transl-vm
debug_train: False
path: /home/gcloud/TransCoder/data/test_dataset/java-python-.with_comments.XLM-syml/train.java_sa.pth
single GPU
debug_train: False
path: /home/gcloud/TransCoder/data/test_dataset/java-python-.with_comments.XLM-syml/valid.java_sa.pth
single GPU
debug_train: False
path: /home/gcloud/TransCoder/data/test_dataset/java-python-.with_comments.XLM-syml/test.java_sa.pth
single GPU
debug_train: False
path: /home/gcloud/TransCoder/data/test_dataset/java-python-.with_comments.XLM-syml/train.python_sa.pth
single GPU
debug_train: False
path: /home/gcloud/TransCoder/data/test_dataset/java-python-.with_comments.XLM-syml/valid.python_sa.pth
single GPU
debug_train: False
path: /home/gcloud/TransCoder/data/test_dataset/java-python-.with_comments.XLM-syml/test.python_sa.pth
single GPU
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError("No module named 'amp_C'")
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
