{ "repo_name": "clbustos/dominanceAnalysis", "ref": "refs/heads/master", "path": "tests/testthat/test-daAverageContributionByLevellmWithCov.R", "content": "context(\"lmWithCovs method\")\ntest_that(\"Correct average contribution by level using lmWithCovs\", {\n  nam<-c(\"SES\",\"IQ\",\"nAch\",\"GPA\")\n  cor.m<-matrix(c(\n  1   ,.3  ,.41   ,.33,\n  .3  ,1   , .16  ,  .57 ,\n  .41 , .16, 1    ,  .50 ,\n  .33 , .57, .50  ,  1\n  ),4,4,byrow=T,\n  dimnames=list(nam,nam)\n  )\n  lwith<-lmWithCov(GPA~SES+IQ+nAch,cor.m)\n  rawR<-daRawResults(lwith)\n  daA<-daAverageContributionByLevel(rawR)$r2\n  expected<-matrix(c(\n  0, 1.089000e-01, 0.3249000, 0.2500000,\n  1, 2.328181e-02, 0.2450947, 0.1656952,\n  2, 6.548709e-05, 0.2276912, 0.1437922 ),\n  3,4,byrow=T,dimnames=list(\n  c(1,2,3),\n  c(\"level\",\"SES\",\"IQ\",\"nAch\")\n  ))\n  expect_equal(expected,as.matrix(daA),tolerance=0.001)\n  total<-sum(colMeans(daA[,-1]))\n  expect_equal(total,lwith$r.squared)\n})\n" }
{ "repo_name": "Dordt-Statistics-Research/LaByRInth", "ref": "refs/heads/master", "path": "inst/extdata/common-F1-probs/F5.R", "content": "## Copyright 2018 Jason Vander Woude\n##\n## Licensed under the Apache License, Version 2.0 (the \"License\");\n## you may not use this file except in compliance with the License.\n## You may obtain a copy of the License at\n##\n##     http://www.apache.org/licenses/LICENSE-2.0\n##\n## Unless required by applicable law or agreed to in writing, software\n## distributed under the License is distributed on an \"AS IS\" BASIS,\n## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n## See the License for the specific language governing permissions and\n## limitations under the License.\n\n\n## This file is auto-generated by multi-model-symbolics.sage for F5\n## generation plants. To generate a similar file for any generation of\n## recombinant inbred line (RIL) plants, load this SAGE file in a SAGE\n## interpreter and run the function 'parse_common_F1_probs_to_R' with the single\n## parameter being the generation of the plants where a value of 2 corresponds\n## to F2, 3 corresponds to F3, etc. Running the function will print the required\n## text for the R file to the console where it must be copied and pasted into a\n## file called 'F{generation}.R' and saved in the appropriate directory.\n\n## To download SAGE, visit https://www.sagemath.org/download.html\n\n\n## There is a seperate function for each file because at runtime, only one\n## function will be needed, so there is no need to source to code for all other\n## generations\n\nsite.pair.transition.probs <- list(\n    list(\n        function(r) {\n            matrix(c(\n                1, 0, 0, 0,\n                0, 0, 0, 0,\n                0, 0, 0, 0,\n                0, 0, 0, 0\n            ), nrow=4, ncol=4, byrow=T)\n        },\n\n        function(r) {\n            matrix(c(\n                15/32, 1/32, 1/32, 15/32,\n                0, 0, 0, 0,\n                0, 0, 0, 0,\n                0, 0, 0, 0\n            ), nrow=4, ncol=4, byrow=T)\n        },\n\n        function(r) {\n            matrix(c(\n                15/32, 1/32, 1/32, 15/32,\n                0, 0, 0, 0,\n                0, 0, 0, 0,\n                0, 0, 0, 0\n            ), nrow=4, ncol=4, byrow=T)\n        },\n\n        function(r) {\n            matrix(c(\n                0, 0, 0, 1,\n                0, 0, 0, 0,\n                0, 0, 0, 0,\n                0, 0, 0, 0\n            ), nrow=4, ncol=4, byrow=T)\n        }\n    ),\n\n    list(\n        function(r) {\n            matrix(c(\n                15/32, 0, 0, 0,\n                1/32, 0, 0, 0,\n                1/32, 0, 0, 0,\n                15/32, 0, 0, 0\n            ), nrow=4, ncol=4, byrow=T)\n        },\n\n        function(r) {\n            matrix(c(\n                1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 19/8*r^4 - 2*r^3 + 3/2*r^2 - 15/16*r + 15/32, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 15/8*r^4 - 1/2*r^3 - 1/2*r^2 + 11/16*r,\n                -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 19/8*r^4 - 7/4*r^3 + 7/8*r^2 - 1/4*r + 1/32, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 15/8*r^4 - 3/4*r^3 + 1/8*r^2, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r,\n                -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 15/8*r^4 - 3/4*r^3 + 1/8*r^2, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 19/8*r^4 - 7/4*r^3 + 7/8*r^2 - 1/4*r + 1/32, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r,\n                1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 15/8*r^4 - 1/2*r^3 - 1/2*r^2 + 11/16*r, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 19/8*r^4 - 2*r^3 + 3/2*r^2 - 15/16*r + 15/32\n            ), nrow=4, ncol=4, byrow=T)\n        },\n\n        function(r) {\n            matrix(c(\n                1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 15/8*r^4 - 1/2*r^3 - 1/2*r^2 + 11/16*r, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 19/8*r^4 - 2*r^3 + 3/2*r^2 - 15/16*r + 15/32,\n                -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 15/8*r^4 - 3/4*r^3 + 1/8*r^2, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 19/8*r^4 - 7/4*r^3 + 7/8*r^2 - 1/4*r + 1/32, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r,\n                -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 19/8*r^4 - 7/4*r^3 + 7/8*r^2 - 1/4*r + 1/32, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 15/8*r^4 - 3/4*r^3 + 1/8*r^2, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r,\n                1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 19/8*r^4 - 2*r^3 + 3/2*r^2 - 15/16*r + 15/32, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 15/8*r^4 - 1/2*r^3 - 1/2*r^2 + 11/16*r\n            ), nrow=4, ncol=4, byrow=T)\n        },\n\n        function(r) {\n            matrix(c(\n                0, 0, 0, 15/32,\n                0, 0, 0, 1/32,\n                0, 0, 0, 1/32,\n                0, 0, 0, 15/32\n            ), nrow=4, ncol=4, byrow=T)\n        }\n    ),\n\n    list(\n        function(r) {\n            matrix(c(\n                15/32, 0, 0, 0,\n                1/32, 0, 0, 0,\n                1/32, 0, 0, 0,\n                15/32, 0, 0, 0\n            ), nrow=4, ncol=4, byrow=T)\n        },\n\n        function(r) {\n            matrix(c(\n                1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 15/8*r^4 - 1/2*r^3 - 1/2*r^2 + 11/16*r, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 19/8*r^4 - 2*r^3 + 3/2*r^2 - 15/16*r + 15/32,\n                -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 15/8*r^4 - 3/4*r^3 + 1/8*r^2, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 19/8*r^4 - 7/4*r^3 + 7/8*r^2 - 1/4*r + 1/32, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r,\n                -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 19/8*r^4 - 7/4*r^3 + 7/8*r^2 - 1/4*r + 1/32, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 15/8*r^4 - 3/4*r^3 + 1/8*r^2, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r,\n                1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 19/8*r^4 - 2*r^3 + 3/2*r^2 - 15/16*r + 15/32, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 15/8*r^4 - 1/2*r^3 - 1/2*r^2 + 11/16*r\n            ), nrow=4, ncol=4, byrow=T)\n        },\n\n        function(r) {\n            matrix(c(\n                1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 19/8*r^4 - 2*r^3 + 3/2*r^2 - 15/16*r + 15/32, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 15/8*r^4 - 1/2*r^3 - 1/2*r^2 + 11/16*r,\n                -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 19/8*r^4 - 7/4*r^3 + 7/8*r^2 - 1/4*r + 1/32, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 15/8*r^4 - 3/4*r^3 + 1/8*r^2, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r,\n                -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 15/8*r^4 - 3/4*r^3 + 1/8*r^2, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 19/8*r^4 - 7/4*r^3 + 7/8*r^2 - 1/4*r + 1/32, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r,\n                1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 15/8*r^4 - 1/2*r^3 - 1/2*r^2 + 11/16*r, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, -1/4*r^8 + r^7 - 2*r^6 + 5/2*r^5 - 17/8*r^4 + 5/4*r^3 - 1/2*r^2 + 1/8*r, 1/4*r^8 - r^7 + 2*r^6 - 5/2*r^5 + 19/8*r^4 - 2*r^3 + 3/2*r^2 - 15/16*r + 15/32\n            ), nrow=4, ncol=4, byrow=T)\n        },\n\n        function(r) {\n            matrix(c(\n                0, 0, 0, 15/32,\n                0, 0, 0, 1/32,\n                0, 0, 0, 1/32,\n                0, 0, 0, 15/32\n            ), nrow=4, ncol=4, byrow=T)\n        }\n    ),\n\n    list(\n        function(r) {\n            matrix(c(\n                0, 0, 0, 0,\n                0, 0, 0, 0,\n                0, 0, 0, 0,\n                1, 0, 0, 0\n            ), nrow=4, ncol=4, byrow=T)\n        },\n\n        function(r) {\n            matrix(c(\n                0, 0, 0, 0,\n                0, 0, 0, 0,\n                0, 0, 0, 0,\n                15/32, 1/32, 1/32, 15/32\n            ), nrow=4, ncol=4, byrow=T)\n        },\n\n        function(r) {\n            matrix(c(\n                0, 0, 0, 0,\n                0, 0, 0, 0,\n                0, 0, 0, 0,\n                15/32, 1/32, 1/32, 15/32\n            ), nrow=4, ncol=4, byrow=T)\n        },\n\n        function(r) {\n            matrix(c(\n                0, 0, 0, 0,\n                0, 0, 0, 0,\n                0, 0, 0, 0,\n                0, 0, 0, 1\n            ), nrow=4, ncol=4, byrow=T)\n        }\n    )\n)\n" }
{ "repo_name": "richelbilderbeek/R", "ref": "refs/heads/master", "path": "old_notes/Plot/plot_surfaceplot.R", "content": "rm(list = ls())\nlibrary(fields)\nlibrary(testit)\nlibrary(rgl)\nlibrary(akima)\n\n# Use of x,y,z table\n\nx_res <- 3\ny_res <- 3\nn_rows <- x_res*y_res\nt <- matrix(0.0,n_rows,3)\nfor (x_i in c(1:x_res))\n{\n\tfor (y_i in c(1:y_res))\n\t{\n    i <- 1 + ((x_i-1) * y_res) + (y_i - 1)\n\t\tprint(i)\n\t\tassert(\"\",i >= 1 && i <= n_rows)\n\t\tx <- (x_i-1) / (x_res - 1)\n\t\ty <- (y_i-1) / (y_res - 1)\n\t\tt[i,1] <- x\n\t\tt[i,2] <- y\n\t\tt[i,3] <- sin(x) + cos(y)\n\t}\n}\n\nplot3d(t,xlab=\"f(A)\",ylab=\"f(B)\",zlab=\"Fitness\")\n\ns <- interp(t[,1],t[,2],t[,3])\nimage.plot(s)\n" }
{ "repo_name": "andrewdefries/andrewdefries.github.io", "ref": "refs/heads/master", "path": "FDA_Pesticide_Glossary/hydroxyhippuric_acid.R", "content": "library(\"knitr\")\nlibrary(\"rgl\")\n#knit(\"hydroxyhippuric_acid.Rmd\")\n#markdownToHTML('hydroxyhippuric_acid.md', 'hydroxyhippuric_acid.html', options=c(\"use_xhml\"))\n#system(\"pandoc -s hydroxyhippuric_acid.html -o hydroxyhippuric_acid.pdf\")\n\n\nknit2html('hydroxyhippuric_acid.Rmd')\n" }
{ "repo_name": "ColumbusCollaboratory/electron-quick-start", "ref": "refs/heads/master", "path": "R-Portable-Mac/library/miniCRAN/examples/example_pkgDep.R", "content": "\r\n\\dontrun{\r\npkgDep(pkg = c(\"ggplot2\", \"plyr\", \"reshape2\"),\r\n       repos = c(CRAN = getOption(\"minicran.mran\"))\r\n)\r\n}\r\n\r\npdb <- cranJuly2014\r\n\\dontrun{\r\npdb <- pkgAvail(repos = c(CRAN = getOption(\"minicran.mran\")))\r\n}\r\n\r\npkgDep(pkg = c(\"ggplot2\", \"plyr\", \"reshape2\"), pdb)\r\n\r\n" }
{ "repo_name": "sahilseth/packagedocs", "ref": "refs/heads/master", "path": "R/package_docs.R", "content": "#' Generate package docs\n#'\n#' @param toc should a table of contents be included?\n#' @param toc_depth depth of the table of contents (max is 2 for this template)\n#' @param \\ldots parameters passed to \\code{\\link[rmarkdown]{html_document}}\n#' @export\n#' @import rmarkdown\n#' @import htmltools\npackage_docs <- function(toc = TRUE, toc_depth = 2, toc_collapse = FALSE, extra_dependencies = NULL, ...) {\n\n  template <-  system.file(\"html_assets/template.html\", package = \"packagedocs\")\n  # header <- system.file(\"assets/header.html\", package = \"packagedocs\")\n\n   if(toc_depth > 2)\n     warning(\"toc_depth must be 2 or smaller\", call. = FALSE)\n\n  pddep <- html_dependency_packagedocs()\n  if(toc_collapse) {\n    pddep$script <- setdiff(pddep$script, \"pd-sticky-toc.js\")\n  } else {\n    pddep$script <- setdiff(pddep$script, \"pd-collapse-toc.js\")\n  }\n\n  extra_dependencies <- c(list(rmarkdown:::html_dependency_jquery(),\n    html_dependency_boot(), html_dependency_hglt(), html_dependency_fnta(),\n    html_dependency_sticky_kit(), html_dependency_jquery_easing(), pddep), extra_dependencies)\n\n  # call the base html_document function\n  rmarkdown::html_document(toc = toc,\n    toc_depth = toc_depth,\n    fig_width = 6.5,\n    fig_height = 4,\n    mathjax = NULL,\n    self_contained = FALSE,\n    template = template,\n    theme = NULL,\n    highlight = NULL,\n    extra_dependencies = extra_dependencies,\n    pandoc_args = c(\"--variable\", paste(\"current_year\", format(Sys.time(), \"%Y\"), sep = \"=\")), ...)\n    # includes = includes(before_body = header))\n}\n\nhtml_dependency_boot <- function() {\n  htmltools::htmlDependency(name = \"bootstrap\",\n    version = \"3.3.2\",\n    src = system.file(\"html_assets/bootstrap\", package = \"packagedocs\"),\n    script = c(\"js/bootstrap.min.js\", \"shim/html5shiv.min.js\", \"shim/respond.min.js\"),\n    stylesheet = c(\"css/bootstrap.min.css\"))\n}\n\nhtml_dependency_sticky_kit <- function() {\n  htmltools::htmlDependency(name = \"stickykit\",\n    version = \"1.1.1\",\n    src = system.file(\"html_assets/sticky-kit\", package = \"packagedocs\"),\n    script = c(\"sticky-kit.min.js\"))\n}\n\nhtml_dependency_jquery_easing <- function() {\n  htmltools::htmlDependency(name = \"jqueryeasing\",\n    version = \"1.3\",\n    src = system.file(\"html_assets/jquery-easing\", package = \"packagedocs\"),\n    script = c(\"jquery.easing.min.js\"))\n}\n\nhtml_dependency_packagedocs <- function() {\n  htmltools::htmlDependency(name = \"packagedocs\",\n    version = \"0.0.1\",\n    src = system.file(\"html_assets/packagedocs\", package = \"packagedocs\"),\n    script = c(\"pd.js\", \"pd-sticky-toc.js\", \"pd-collapse-toc.js\"),\n    stylesheet = \"pd.css\")\n}\n\nhtml_dependency_hglt <- function() {\n  htmltools::htmlDependency(name = \"highlight\",\n    version = \"8.4\",\n    src = system.file(\"html_assets/highlight\", package = \"packagedocs\"),\n    script = \"highlight.pack.js\",\n    stylesheet = \"tomorrow.css\")\n}\n\nhtml_dependency_fnta <- function() {\n  htmltools::htmlDependency(name = \"fontawesome\",\n    version = \"4.3.0\",\n    src = system.file(\"html_assets/fontawesome\", package = \"packagedocs\"),\n    stylesheet = \"css/font-awesome.min.css\")\n}\n" }
{ "repo_name": "cameronlloyd/HockeyMine", "ref": "refs/heads/master", "path": "WebScrape/scrapeMatchups.R", "content": "library('rvest')\nlibrary('XML')\n\nconfig <- config::get()\n\n# Import all teams from csv file\nimportTeams <- function(){\n  tryCatch({\n    temp = list.files(path=\"./WebScrape/TeamSchedules/CSV/.\",pattern=\"*.csv\")\n    for (i in 1:length(temp)) {\n      assign(substr(temp[i],1,nchar(temp[i])-4), \n             read.csv(paste(\"./WebScrape/TeamSchedules/CSV/\",temp[i],sep=\"\")),\n             envir=.GlobalEnv)\n    }\n  }, error = function(e){\n    cat(\"ERROR:\",conditionMessage(e),\"\\n\")\n  })\n}\n\n\n# Import single team from csv file\nimportTeam <- function(teamAbbr){\n  tryCatch({\n    assign(teamAbbr,\n           read.csv(paste(\"./WebScrape/TeamSchedules/CSV/\",teamAbbr,\".csv\",sep=\"\")),\n           envir=.GlobalEnv)\n  }, error = function(e){\n    cat(\"ERROR:\",conditionMessage(e),\"\\n\")\n  })  \n}\n\n\n\n\n\n# Update dataframes for each matchup between two teams.  This information will be\n#   more descriptive of standard schedule information\nUpdateTeams <- function(){\n  \n  tryCatch({\n    ## For each team, create dataframe of matchup info\n    team.Abbrs = config$teamAbbrs\n    for (abbr in names(team.Abbrs)){\n      print(paste(\"Updating: \",abbr,sep=\"\"))\n      teamInfo = UpdateSchedule(abbr, \"2016\")\n      print(\"   Saving.\")\n      saveRDS(teamInfo,file=paste(\"./WebScrape/TeamSchedules/RDA/\",abbr,\" Data.rda\",sep=\"\"))\n      write.csv(teamInfo,file=paste(\"./WebScrape/TeamSchedules/CSV/\",abbr,\".csv\",sep=\"\"))\n    } \n  }, error = function(e){\n    cat(\"ERROR:\",conditionMessage(e),\"\\n\")\n  })\n  \n  return (teamInfo)\n}\n\n# Update dataframes for a single team\nUpdateTeam <- function(abbr){\n  \n  tryCatch({\n    ## Create dataframe of matchup info\n    print(paste(\"Updating: \",abbr,sep=\"\"))\n    teamInfo = UpdateSchedule(abbr, \"2016\")\n    print(\"   Saving.\")\n    saveRDS(teamInfo,file=paste(\"./WebScrape/TeamSchedules/RDA/\",abbr,\" Data.rda\",sep=\"\"))\n    write.csv(teamInfo,file=paste(\"./WebScrape/TeamSchedules/CSV/\",abbr,\".csv\",sep=\"\"))\n  }, error = function(e){\n    cat(\"ERROR:\",conditionMessage(e),\"\\n\")\n  })\n  \n  return (teamInfo)\n}\n\n\n\n\n# Update schedule for current team with additional statistics.\nUpdateSchedule <- function(teamAbbr, year){\n  # Add new features to dataframe\n  df = get(eval(teamAbbr))\n  df$AvgCFPerClose = NA\n  df$AvgCFPer5v5 = NA\n  df$AvgCFPerEven = NA\n  df$SVPercentage = NA\n  df$AvgGoalieCount = NA\n  df$AvgShiftCount = NA\n  df$ATOI = NA\n  df$GID = NA\n  df$AvgAge = NA\n  df$PDO = NA\n  df$FOWPer = NA\n  \n  # Scrape event for each game at given date against given opponent\n  tryCatch({\n    # These stats will be the same for every record\n    fullTeam = AddFullTeamStats(teamAbbr, year)\n    \n    # Get matchup stats\n    dates = as.Date(df$Date)\n    count = 1\n    runningTot = c(0,0,0,0,0,0,0,0)\n    for (i in 1:length(dates)) {\n      # Get current event\n      event = df[i,]\n      \n      # Get current day\n      splitDate = strsplit(as.character(event$Date),\"-\")\n      year = unlist(splitDate)[1]\n      month = unlist(splitDate)[2]\n      day = unlist(splitDate)[3]\n      \n      # Get home team\n      homeTeam = teamAbbr\n      awayTeam = as.character(event$Opponent)\n      if (event$Location == \"Away\"){\n        homeTeam = awayTeam\n        awayTeam = teamAbbr\n      }\n  \n      # Create unique identifier (doubles as link game-reference)\n      id = paste(year,month,day,\"0\",homeTeam,sep=\"\")\n      \n      ## Update matchup record\n      result = UpdateMatchup(id, teamAbbr)\n      \n      ## Increment results\n      runningTot[1] = runningTot[1] + as.numeric(result[1])   #AvgCFPerClose\n      runningTot[2] = runningTot[2] + as.numeric(result[2])   #AvgCFPer5v5\n      runningTot[3] = runningTot[3] + as.numeric(result[3])   #AvgCFPerEven\n      runningTot[4] = runningTot[4] + as.numeric(result[4])   #AvgShiftCnt\n      runningTot[5] = runningTot[5] + as.numeric(result[6])   #SVPer\n      runningTot[6] = runningTot[6] + as.numeric(result[7])   #AvgGoalieCnt\n      AvgCFPerClose = round((runningTot[1]/count), digit=2)\n      AvgCFPer5v5 = round((runningTot[2]/count), digit=2)\n      AvgCFPerEven = round((runningTot[3]/count), digit=2)\n      AvgShiftCnt = round((runningTot[4]/count), digit=2)\n      SvPer = round((runningTot[5]/count), digit=2)\n      AvgGoalieCnt = round((runningTot[6]/count), digit=2)\n  \n      # ATOI\n      atoi = strsplit(as.character(result[5]),\"[.]\")\n      runningTot[7] = runningTot[7] + as.numeric(unlist(atoi)[1])\n      runningTot[8] = runningTot[8] + as.numeric(unlist(atoi)[2])\n      ATOI = convertATOI(runningTot[7], runningTot[8], count)\n      \n      # Add values to dataframe\n      df[count,'AvgCFPerClose'] = AvgCFPerClose\n      df[count,'AvgCFPer5v5'] = AvgCFPer5v5\n      df[count,'AvgCFPerEven'] = AvgCFPerEven\n      df[count,'SVPercentage'] = SvPer\n      df[count,'AvgGoalieCount'] = AvgGoalieCnt\n      df[count,'AvgShiftCount'] = AvgShiftCnt\n      df[count,'ATOI'] = ATOI\n      df[count,'GID'] = id\n      df[count,'AvgAge'] = fullTeam$AvAge\n      df[count,'PDO'] = fullTeam$PDO\n      df[count,'FOWPer'] = fullTeam$FOPer\n      \n      count = count + 1\n    }\n  }, error = function(e){\n    cat(\"ERROR:\",conditionMessage(e),\"\\n\")\n  })\n  \n  return (df)\n}\n\n# Update each matchup with team stats found from team homepage\nAddFullTeamStats <- function(team, year){\n  # Get HTML from page\n  link = paste(\"http://www.hockey-reference.com/teams/\",team,\"/\",year,\".html\",sep=\"\")\n  gameInfo = read_html(link)\n  \n  # Get team stats table\n  teamStats = gameInfo %>% html_nodes(xpath=\"//table[@id='team_stats']/tbody/tr[1]\")\n  \n  # Get AvAge, PDO\n  AvAge = teamStats %>% html_nodes(xpath=\"./td[1]\") %>% html_text()\n  PDO = teamStats %>% html_nodes(xpath=\"./td[25]\") %>% html_text()\n  \n  # Get FOW %\n  skaters = gameInfo %>% html_nodes(xpath=\"//table[@id='skaters']/tfoot/tr\")\n  FOPer = skaters %>% html_nodes(xpath=\"./td[28]\") %>% html_text()\n  \n  # Close web connection\n  closeAllConnections()\n  \n  results = list(\"AvAge\"=AvAge, \"PDO\"=PDO, \"FOPer\"=FOPer)\n  return (results)\n}\n\n# Update a single matchup with individual skater stats\nUpdateMatchup <- function(id, homeTeam, awayTeam){\n\n  # Get HTML from page\n  link = paste(\"http://www.hockey-reference.com/boxscores/\",id,\".html\",sep=\"\")\n  gameInfo = read_html(link)\n  \n  # Get goalie info\n  homeGoalie = getGoalieStats(gameInfo, homeTeam)\n  \n  # Get Skater info\n  homeSkater = getSkaterStats(gameInfo, homeTeam)\n  \n  # Add stats to data frame\n  AvgCFPerClose = homeSkater$AvgCFPerClose\n  AvgCFPer5v5 = homeSkater$AvgCFPer5v5\n  AvgCFPerEven = homeSkater$AvgCFPerEven\n  AvgShiftCnt = homeSkater$avgShift\n  ATOI = homeSkater$atoi\n  SVPer = homeGoalie$svPer\n  GoalieCnt = homeGoalie$count\n  result = cbind(AvgCFPerClose, AvgCFPer5v5, AvgCFPerEven,\n                 AvgShiftCnt, ATOI, SVPer, GoalieCnt)\n  \n  # Close web connection\n  closeAllConnections()\n\n  return (result)\n}\n\n\n\n\n# Retrieve avg save percentage of goalies and the amount of goalies played using matchup page\ngetGoalieStats <- function(html, team){\n  # Find table\n  goalies = html_nodes(html, xpath=paste(\"//table[@id='\",team,\"_goalies']/tbody/tr\",sep=\"\"))\n  \n  # Get avg save percentage and count of goalies playing\n  svPer = 0\n  count = 0\n  for (goalie in goalies){\n    curr = as.numeric(html_nodes(goalie, xpath=\"./td[6]\") %>% html_text())\n    if (!is.na(curr)){\n      svPer = svPer + curr\n      count = count + 1\n    }\n  }\n  svPer = round(svPer / count, digits=3)\n  \n  results = list(\"svPer\"=svPer, \"count\"=count)\n  return (results)\n}\n\n# Retrieve avg time on ice, average shift count and count of players for given team\ngetSkaterStats <- function(html, team){\n  # Find table\n  skaters = html_nodes(html, xpath=paste(\"//table[@id='\",team,\"_skaters']/tbody/tr\",sep=\"\"))\n  \n  # Get ATOI, Avg Shifts and count of skaters\n  atoi_m = 0\n  atoi_s = 0\n  avgShift = 0\n  count = 0\n  for (skater in skaters){\n    avgShift = avgShift + as.numeric(html_nodes(skater, xpath=\"./td[16]\") %>% html_text())\n    \n    curr_atoi = html_nodes(skater, xpath=\"./td[17]\") %>% html_text()\n    curr_atoi = strsplit(as.character(curr_atoi),\":\")\n    atoi_m = atoi_m + as.numeric(unlist(curr_atoi)[1])\n    atoi_s = atoi_s + as.numeric(unlist(curr_atoi)[2])\n    \n    count = count + 1\n  }\n  avgShift = round(avgShift / count, digits=2)\n  \n  # Update ATOI\n  atoi = convertATOI(atoi_m, atoi_s, count)\n  \n  # Get Corsi-For (Close), (5v5) and (even)\n  skatersAdv = html_nodes(html, xpath=paste(\"//div[@id='all_\",team,\"_adv']\",sep=\"\")) %>%\n    html_nodes(xpath='comment()') %>%\n    html_text() %>%\n    read_html() %>%\n    html_node('table') %>%\n    html_node('tfoot')\n  AvgCFPerClose = as.numeric(skatersAdv %>% html_nodes(xpath=\"./tr[@class='CLAll hidden']/td[4]\") %>% html_text())\n  AvgCFPer5v5 = as.numeric(skatersAdv %>% html_nodes(xpath=\"./tr[@class='ALL5v5 hidden']/td[4]\") %>% html_text())\n  AvgCFPerEven = as.numeric(skatersAdv %>% html_nodes(xpath=\"./tr[@class='ALLEV hidden']/td[4]\") %>% html_text())\n  \n  results = list(\"avgShift\"=avgShift, \"atoi\"=atoi, \"count\"=count, \"AvgCFPerClose\"=AvgCFPerClose, \n                 \"AvgCFPer5v5\"=AvgCFPer5v5, \"AvgCFPerEven\"=AvgCFPerEven)\n  return (results)\n}\n\n\n\n\n# Deletes a column from each team df in workspace.  Assuming that it is there\n# NOTE: Need to hard code column to delete in function\nDeleteColumn <- function(){\n  for (team in names(config$teamAbbrs)){\n    df = get(team)\n    df$X.1 = NULL\n    df$X.2 = NULL\n    assign(team, df, envir=.GlobalEnv)\n    Write2Files(team, df)\n  }\n}\n\n# Converts int totals of minutes and second to a string notation averaged by count\nconvertATOI <- function(m,s,count){\n  m = m / count\n  s = s / count\n  h = as.integer(m / 60)\n  m = as.integer((m %% 60) + as.integer(s / 60))\n  s = as.integer(s %% 60)\n  atoi = paste(m, s, sep=\".\")\n  \n  return (atoi)\n}\n\n# Writes team df to both CSV and RDA files\nWrite2Files <- function(team, df){\n  saveRDS(df,file=paste(\"./WebScrape/TeamSchedules/RDA/\",team,\" Data.rda\",sep=\"\"))\n  write.csv(df,file=paste(\"./WebScrape/TeamSchedules/CSV/\",team,\".csv\",sep=\"\"))\n}\n\n#team = \"VAN\"\n#data = UpdateTeam(team)\n#Write2Files(team,data)\n#importTeam(team)\n#DeleteColumn()\n\nimportTeams()\ndata = UpdateTeams()\nDeleteColumn()\n" }
{ "repo_name": "AlexanderVR/laggedLGCP", "ref": "refs/heads/master", "path": "R/lag_poisson_fit.R", "content": "#' Fitting 1-D Log-Gaussian Cox Process with lag.\n#' \n#' @export\n#' \n#' @param events A list of vectors of timestamps, one vector for each observed process.\n#' @param max_lag Lag parameter constrained to lie within [-max_lag, max_lag].\n#' @param time_unit Unit of measurement for the timestamps.\n#' @param likelihood Either 'poisson' or 'coalescent'. If 'coalescent', param EVENTS must contain\n#' vectors named 'coal_times', 'samp_times', and 'n_sampled'.\n#' @param n_bins Number of bins to use for binning the events.\n#' @param max_n_frequencies Maximum number of cosine and sine basis functions to use in spectral representation.\n#' @param min_percent_padding How far to extend the domain of the latent (periodic) Gaussian process.\n#' @param prob_quantiles Which quantiles of the latent parameters should be returned?\n#' @param return_stanfit If TRUE, the returned list includes the stanfit object containing all MCMC samples.\n#' @param fitting_args Additional list of arguments that affect the marginal GMO fitting and MCMC simulation code.\n#' \n#' @useDynLib laggedLGCP, .registration = TRUE\n#'\n#' @examples \n#' # To simulate a coalescent process with lagged sampling times\n#' sim <- sim_lag_coalescent(lag=-1, c=1, beta=2, scaling=0.05)\n#' fit <- fit_LGCP_lag(sim$events, max_lag=3, likelihood='coalescent')\n#' # Plot inferred and 'true' effective population size trajectory\n#' par(mfrow=c(3,1))\n#' plot_coal_result(fit, traj=sim$rate_functions$coal_fun, main=\"Lagged Preferential Sampling\", ylim=c(0.5,15))\n#' # Plot non-Pref. sampling and PS without lag inferences``\n#' plot_BNPR(BNPR(sim$events, lengthout = 100), traj=sim$rate_functions$coal_fun, main=\"No PS\", ylim=c(0.5,15))\n#' plot_BNPR(BNPR_PS(sim$events, lengthout = 100), traj=sim$rate_functions$coal_fun, main=\"PS without lag\", ylim=c(0.5,15))\n\nfit_LGCP_lag <- function(events,\n                         max_lag,\n                         time_unit = 'Weeks',\n                         likelihood = 'poisson',\n                         n_bins = 100, \n                         max_n_frequencies = 1023, \n                         min_percent_padding = 10.0, \n                         prob_quantiles = c(.025, .25),\n                         return_stanfit = FALSE,\n                         fitting_args = list()) {\n\n  if (all(sort(names(events)) == c('coal_times', 'n_sampled', 'samp_times')))\n    likelihood <- 'coalescent'\n  stopifnot(any(likelihood == c('poisson', 'coalescent')))\n  stopifnot(max_lag > 0)\n  stopifnot(max(prob_quantiles) < 0.5)\n  \n  # set defaults for GMO and stan fitting\n  fitting_defaults <- list(n_chains = 1, \n                           n_samples = 1000,\n                           refresh = 10, \n                           gmo_iter = 100, \n                           gmo_draws = 15, \n                           gmo_tol = 1e-4,\n                           gmo_eta = 1.,\n                           gmo_init_lengthscale = -1,\n                           gmo_max_block_size = 256)\n\n  # Override defaults if specified\n  for (arg_name in names(fitting_args)) {\n    stopifnot(any(arg_name == names(fitting_defaults)))\n    fitting_defaults[[arg_name]] <- fitting_args[[arg_name]]\n  }\n  \n  # adjust starting lengthscale value if unspecified\n  if (fitting_defaults$gmo_init_lengthscale <= 0) {\n    fitting_defaults$gmo_init_lengthscale <- 2 * max_lag\n  }\n  \n  # fill out probability quantiles with median and symmetric values\n  prob_quantiles <- c(prob_quantiles, 0.5, rev(1 - prob_quantiles))\n  \n  # Make sure event data is a list of numeric vectors\n  stopifnot(all(vapply(events, function(x) is.numeric(x) & is.vector(x), logical(1))))\n\n  # Bin the event data\n  if (likelihood == 'poisson') {\n    pars <- bin_poisson(dat$events, n_bins=n_bins)\n  } else {  # have coalescent data \n    pars <- bin_coal(events$coal_times, events$samp_times, events$n_sampled, n_bins)\n  }\n      \n  # Set the prior on lag parameter to N(mu = 0, sigma = max_lag / 2)\n  pars$max_lag <- max_lag\n\n  # number of bins WITH padding -- must be a power of 2.\n  pars$Ntot <- nextpow2(pars$N, min_percent_padding)\n\n  # how many hyperparameters to optimize\n  pars$npars <- 4  \n\n  # number of frequencies to use\n  pars$n_nonzero_freq <- min(max_n_frequencies, floor(pars$Ntot/2 - 1))\n\n  # marginal process fitting function\n  marginal_optimize <- function(k) {\n    ind_pars <- split_data(pars, k)\n    if (k==2 && pars$coalescent) {\n      # second process (sampling) does not have the coalescent likelihood. \n      ind_pars$coalescent = 0\n    }\n    print(ind_pars$coalescent)\n    sfit=sampling(stanmodels$single_gp_fixed_gmo, data = c(ind_pars, list(GMO_FLAG = FALSE, fixed_phi = double())),\n         chains = 0, iter = 1)\n    opt_res <- gmo(full_model=sfit, \n                 data = ind_pars, \n                 iter = as.integer(fitting_defaults$gmo_iter),\n                 tol = fitting_defaults$gmo_tol, \n                 eta = fitting_defaults$gmo_eta, \n                 draws = as.integer(fitting_defaults$gmo_draws), \n                 init = get_init(ind_pars, lengthscale=fitting_defaults$gmo_init_lengthscale),\n                 max_block_size = fitting_defaults$gmo_max_block_size)\n\n    u_opt <- opt_res$par\n    return(c(mu=u_opt[1], sigma=exp(u_opt[2]), lengthscale=exp(u_opt[3]), nu=exp(u_opt[4])))\n  }\n  # optimize hyperparameters of each marginal latent GP\n  hyp_opt <- vapply(1:pars$n_series, FUN=marginal_optimize, FUN.VALUE=numeric(pars$npars))\n\n  # Package the hyperparameters together\n  pars <- c(pars, as.list(data.frame(t(hyp_opt))))\n\n  # Now train joint GP stan model, with the above fixed hyperparameters\n  fit <- sampling(stanmodels$multiple_gp_nonsep_fixed, \n                  data=pars, \n                  chains=fitting_defaults$n_chains, \n                  iter=fitting_defaults$n_samples, \n                  control=list(adapt_delta=0.95), \n                  refresh=fitting_defaults$refresh,\n                  pars=c(\"re\", \"im\", \"x\", \"cor\"), include=FALSE) # ignore high-dimensional latent variables\n\n  # extract quantiles for all posterior samples\n  samples <- rstan::extract(fit, permuted=TRUE)\n  rates <- samples$rates\n  quantiles <- list()\n  for (param in names(samples)) {\n    samp <- samples[[param]]\n    if (is.vector(samp) || length(samp) == dim(samp)[1])\n      quantiles[[param]] <- quantile(samp, probs = prob_quantiles)\n    else {\n      n_dim <- length(dim(samp))\n      quantiles[[param]] <- apply(samp, MARGIN=2:n_dim,\n                    FUN=function(x) quantile(x, probs=prob_quantiles))\n    }\n  }\n  print(quantiles$shifts)\n  \n  # Naive cross-correlation estimate of lag\n  shifts_ccf <- rep(0, pars$n_series - 1)\n  for (k in 1:(pars$n_series - 1)) {\n    crosscor <- ccf(pars$counts[, pars$n_series], pars$counts[,k], plot=FALSE)\n    shifts_ccf[k] <- crosscor$lag[which.max(crosscor$acf)] * pars$delta\n  }\n\n  # Data to return\n  res <- list(quantiles = quantiles, shifts_ccf = shifts_ccf, data = pars, \n              events=events)\n  if (return_stanfit)\n    res$stanfit <- fit\n  return(res)\n}\n\n#' Return marginal GP hyperparameters fitted by optimizing the marginal likelihood.\n#' \n#' @export\n#' @param res Result from fit_LGCP_lag() function.\n#' \nget_hypers <- function(res)\n  return(res$data[c('mu', 'sigma', 'lengthscale', 'nu')])\n" }
{ "repo_name": "yu-iskw/R-OO", "ref": "refs/heads/master", "path": "src/set-method-with-override.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nlibrary(methods)\n\nnot <- function(x) { -1 * x }\n\nsetClass(\"MyLogical\", representation(value = \"logical\"))\n\nsetGeneric(\"not\", function(x) {standardGeneric(\"not\")})\n\nsetMethod(\"not\", signature(x = \"MyLogical\"), function(x) { ! x@value })\n\na <- new(\"MyLogical\", value = FALSE)\nnot(1)\nnot(a)\n" }
{ "repo_name": "NovaInstitute/Rpackages", "ref": "refs/heads/master", "path": "novaSurvey/tests/testthat/testPopsurvey.R", "content": "context(\"Testing popProjek in package popSurvey\")\r\n\r\norig = eval(parse(text = 'structure(list(place = c(\"embalenhle\", \"\", \"\", \"emzinoni\", \r\n\"\", \"\", \"kwadela\", \"\", \"\", \"lebohang\", \"\", \"\"), `sol energy coal ignition` = structure(c(1L, \r\n                  2L, NA, 1L, 2L, NA, 1L, 2L, NA, 1L, 2L, NA), .Label = c(\"N\", \r\n                  \"Y\"), class = \"factor\"), PointEst = c(18545, 16859, 0, 3544, \r\n                  6766, 0, 625, 357, 0, 4276, 4632, 0), Lower = c(11460, 10035, \r\n                  0, 2104, 4981, 0, 422, 194, 0, 2675, 2984, 0), Upper = c(25369, \r\n                  23944, 5475, 5329, 8206, 1105, 788, 560, 146, 5924, 6233, 1186\r\n                  )), .Names = c(\"place\", \"sol energy coal ignition\", \"PointEst\", \r\n                  \"Lower\", \"Upper\"), row.names = c(NA, -12L), class = \"data.frame\")'))\r\n\r\nx <- do.call(\"popProjek\", toetsdata.popProjek())\r\n\r\nd <- all.equal(orig, x)\r\n\r\nstate <- d\r\n\r\nif(is.logical(state)) {\r\n        message(\"test passed\")\r\n}else {\r\n                message(\"test not passed\")\r\n                print(d)\r\n                \r\n}\r\n\r\n\r\n\r\n\r\n" }
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Decimal-to-binary.R","content":"# Program to convert decimal number into binary number using recursive function\nconvert_to_binary <- function(n) {\n  if(n > 1) {\n    convert_to_binary(as.integer(n/2))\n  }\n  cat(n %% 2)\n}\n\nconvert_to_binary(52)"}
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Factorial.R","content":"# take input from the user\nfactorial <- function(n) {\nfactorial = 1\n# check is the number is negative, positive or zero\nif(num < 0) {\n  print('Sorry factorial does not exist for negative numbers')\n} else if(num == 0) {\n  print('The factorial of 0 is 1')\n} else {\n  for(i in 1:num) {\n    factorial = factorial * i\n  }\n  print(paste('The factorial of', num ,'is',factorial))\n}\n}\n\nfactorial(4)"}
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/First-10-Fibonacci.R","content":"fibonacci <- function(n) {\nFibonacci <- numeric(n)\nFibonacci[1] <- Fibonacci[2] <- 1\nfor (i in 3:10) Fibonacci[i] <- Fibonacci[i - 2] + Fibonacci[i - 1]\nprint('First 10 Fibonacci numbers:'')\nprint(Fibonacci)\n}\n\nfibonacci(10)"}
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Max-Min-of-Vector.R","content":"max_min_vector <- function() {\nnums = c(10, 20, 30, 40, 50, 60)\nprint('Original vector:')\nprint(nums)   \nprint(paste('Maximum value of the said vector:',max(nums)))\nprint(paste('Minimum value of the said vector:',min(nums)))\n}\n\nmax_min_vector()"}
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Odd-or-Even.R","content":"# Program to check if the input number is odd or even.\n# A number is even if division by 2 give a remainder of 0.\n# If remainder is 1, it is odd.\nodd_or_even <- function(n){\nnum = n\nif((num %% 2) == 0) {\n  print(paste(num,'is Even'))\n} else {\n  print(paste(num,'is Odd'))\n}\n}\n\nodd_or_even(4)"}
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Prime-Numbers.R","content":"prime_numbers <- function(n) {\n  if (n >= 2) {\n    x = seq(2, n)\n    prime_nums = c()\n    for (i in seq(2, n)) {\n      if (any(x == i)) {\n        prime_nums = c(prime_nums, i)\n        x = c(x[(x %% i) != 0], i)\n      }\n    }\n    return(prime_nums)\n  }\n  else \n  {\n    stop('Input number should be at least 2.'')\n  }\n} \nprime_numbers(12)"}
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/Read-csv-file.R","content":"read_csv_file <- function() {\nmovie_data = read.csv(file=movies.csv, header=TRUE, sep=',')\nprint('Content of the .csv file:'')\nprint(movie_data)\n}\n\nread_csv_file()"}
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/add1.r","content":"add1 <- function(n) {\n   print( n+1)\n}\n\nadd1(90)\n"}
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/add2numbers.r","content":"add2nums <- function(num1, num2) {\n   print( num1+num2)\n}\n\nadd2nums(34, 6)\n"}
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/areaofsquare.r","content":"areaSquare <- function(num) {\n   msg1 <- 'Invalid measurement'\n   msg2 <- 'Area of the Square is: '\n   if  (num <= 0){\n        print(msg1)\n   }else {\n       print (msg2)\n       print(num*num)\n   }\n  \n}\n\nareaSquare(6)"}
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/divisibleby10.r","content":"divisibleby10 <- function(num) {\nif(num %% 10 == 0){\n    print('True')\n}else{\n    print('False')\n}\n\n}\n\ndivisibleby10(60)\n"}
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/integertype.r","content":"integerType <- function(num){\nif(num > 0) {\nprint('Positive number')\n} else {\nif(num == 0) {\nprint('Zero')\n} else {\nprint('Negative number')\n}\n}\n}\n\nintegerType(-90)\n"}
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/lengthoflist.r","content":"\nlengthofVector <- function(vector){\ncount <- 0\nfor (i in vector){\n    count <- count + 1\n}\nprint(count)\n}\n\narray <- c(3,4,5,1,6)\nlengthofVector(array)\n"}
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/palindrome.r","content":"\npalindrome <- function(n){\nrev = 0\n    num = n\n\n    while (n > 0) {\n      r = n %% 10\n      rev = rev * 10 + r\n      n = n %/% 10\n    }\n\n    if (rev == num)\n    {\n      print(paste('Number is palindrome :', rev))\n    }\n    else{\n      print(paste('Number is not palindrome :', rev))\n    }\n}\npalindrome(121)\n"}
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/sort.r","content":"integerType <- function(num){\nif(num > 0) {\nsortvector <- function(vector){\nprint(sort(vector))\n}\n\narray <- c(23,12,11,34,21)\nsortvector(array)\n"}
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/spmv.r","content":"\nsum <- function(vector){\nresult <- 0\nfor(i in vector){\n    result = result + i\n}\nprint(result)\n}\n\nproduct <- function(vector){\n    result <- 1\n    for(i in vector){\n        result = result*i\n    }\n    print(result)\n}\n\narray <- c(1,2,3,4)\nsum(array)\nproduct(array)\nmean(array, na.rm=TRUE)\n"}
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/upperlower.r","content":"\nupperCase <- function(word){\nresult <- toupper(word)\nprint(result)\n}\n\nlowerCase <- function(word){\nresult <- tolower(word)\nprint(result)\n}\n\nupperCase('Function')\nlowerCase('FUNCTION')\n"}
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/armstrong.r","content":"\n# take input from the user\narmstrong <- function(){\nnum = as.integer(readline(prompt='Enter a number: '))\n# initialize sum\nsum = 0\n# find the sum of the cube of each digit\ntemp = num\nwhile(temp > 0) {\ndigit = temp %% 10\nsum = sum + (digit ^ 3)\ntemp = floor(temp / 10)\n}\n# display the result\nif(num == sum) {\nprint(paste(num, 'is an Armstrong number'))\n} else {\nprint(paste(num, 'is not an Armstrong number'))\n}\n}\n" }
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/divisibleby10.r","content":"\ndivisibleby10 <- function(num) {\nif(num %% 10 == 0){\n    print('True')\n}else{\n    print('False')\n}\n\n}\n\ndivisibleby10(60)\n" }
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/factors.r","content":"\n#R program to print factors\nfactors <- function()\nn=14\nfor(i in 1:n)\n{\nif((n%%i)==0)\n{\n  print(i)\n}\n}\n}\n" }
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/greatest3.r","content":"\ngreatest3 <- function(){\n    x <- 71\n    y <- 87\n    z <- 24\n\n    if (x > y && x > z) {\n      print(paste('Greatest is :', x))\n    } else if (y > z) {\n      print(paste('Greatest is :', y))\n    } else{\n      print(paste('Greatest is :', z))\n    }\n\n}\n"        }
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/hcf.r","content":"\n# Program to find the H.C.F of two input number\n# define a function\nhcf <- function(x, y) {\n# choose the smaller number\nif(x > y) {\nsmaller = y\n} else {\nsmaller = x\n}\nfor(i in 1:smaller) {\nif((x %% i == 0) && (y %% i == 0)) {\nhcf = i\n}\n}\nreturn(hcf)\n}\n# take input from the user\nnum1 = 12\nnum2 = 36\nprint(paste('The H.C.F. of', num1,'and', num2,'is', hcf(num1, num2)))\n"        }
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/lcm.r","content":"\n# Program to find the L.C.M. of two input number\nlcm <- function(x, y) {\n# choose the greater number\nif(x > y) {\ngreater = x\n} else {\ngreater = y\n}\nwhile(TRUE) {\nif((greater %% x == 0) && (greater %% y == 0)) {\nlcm = greater\nbreak\n}\ngreater = greater + 1\n}\nreturn(lcm)\n}\n# take input from the user\nnum1 = 48\nnum2 = 72\nprint(paste('The L.C.M. of', num1,'and', num2,'is', lcm(num1, num2)))\n"        }
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/matrixmul.r","content": "\n#creating matrix\nmatrixmul <- function(){\nm <- matrix(1:8, nrow=2) \nn <- matrix(8:15, nrow=2)   \n\n#Multiplying matrices \nprint(m*n) \n}\n"       }
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/multiples.r","content": "\n# R Program to find the multiplicationtable (from 1 to 10)\n# take input from the user\nmultiples <- function(){\nnum = 12\n# use for loop to iterate 10 times\nfor(i in 1:10) {\nprint(paste(num,'x', i, '=', num*i))\n}\n}\n"       }
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/perfect.r","content": "\nperfect <- function(){\n    n <- as.integer(readline(prompt = 'Enter a number :'))\n    i = 1\n    s = 0\n\n    while (i < n) {\n      if (n %% i == 0) {\n        s = s + i\n      }\n      i = i + 1\n    }\n\n    if (s == n) {\n      print(paste('The number is perfect :', n))\n    } else{\n      print(paste('The number is not perfect :', n))\n    }\n}\n"       }
{ "repo_name":"rakeshamireddy/Automatic-Code-Translation","ref":"refs/heads/master","path":"data/test_dataset/R/triangle.r","content": "\n#RProgram to find the area of triangle\ntriangle <- function(){\na = 5\nb = 6\nc = 7\n\n# calculate the semi-perimeter\ns = (a + b + c) / 2\n\n# calculate the area\narea = (s*(s-a)*(s-b)*(s-c)) ** 0.5\nprint(area)\n}\n" }
{ "repo_name": "uncsurveysCF/uncsurveys", "ref": "refs/heads/master", "path": "RDatos/grafGroupBarPeriodo.R", "content": "# grafGroupBar.R : Grafico de barra para escalas likerts\r\n\r\nlibrary(DBI)\r\nlibrary(RMySQL)\r\n\r\nargs <- commandArgs(TRUE) \r\nidP <- args[1]\r\nidPeriodo <- args[2]\r\ncon <- dbConnect(RMySQL::MySQL(), username = \"root\", password = \"\", host = \"localhost\", dbname=\"uncsurveys\")\r\nsql <- paste('SELECT (CASE WHEN LENGTH(O.Texto) > 9 THEN O.idOpcion ELSE O.Texto END) AS opcion, C.Texto as columna, (SELECT COUNT(*) FROM respuestaspreguntas R INNER JOIN respuestas E ON R.idRespuesta = E.idRespuesta WHERE E.idPeriodo = ',idPeriodo,' AND R.idOpcion = O.idOpcion AND R.idColumna = C.idColumna) AS cantidad FROM columnaspreguntas C LEFT JOIN opcionespreguntas O ON O.idPregunta = C.idPregunta WHERE O.idPregunta = ',idP,' ORDER BY C.Ponderacion',sep='')\r\ndatos <- dbGetQuery(con, sql)\r\ndata_ordered = datos[with(datos, order(opcion)), ]\r\ndata = data_ordered$cantidad\r\ncols <- unique(data_ordered[2])\r\nrows <- unique(data_ordered[1])\r\ndata = matrix(data, ncol = length(rows$opcion), nrow=length(cols$columna)) \r\ncolnames(data) = rows$opcion\r\nrownames(data) = cols$columna\r\nnom <- paste('../Datos/img/graBarGroup',idP,idPeriodo,'.png',sep='')\r\npng(file=nom)\r\nprop = prop.table(data, margin = 1)\r\nd <- length(rows$opcion)\r\nif( d > 6){\r\nif(length(rows) > 6){\r\n    barplot(data , col = rainbow(length(cols$columna)),cex.names=0.65,srt=45, width = 3,horiz=TRUE, beside = TRUE)\r\n}else\r\n{\r\n   barplot(data , col = rainbow(length(cols$columna)),cex.names=0.65,srt=45, width = 3,beside = TRUE)\r\n}\r\nlegend(\"topright\", cols$columna, cex=0.6,bty=\"n\", fill=rainbow(length(cols$columna)));\r\ndev.off()\r\ndbDisconnect(con)" }
{ "repo_name": "jrboyd/jrb_R_scripts", "ref": "refs/heads/master", "path": "heatmap.res_lists.R", "content": "plot.hmap_res_lists = function(hmap_res, nper_column = 50, nper_row = 10, cex = 1, col_spacing = 1, row_spacing = 1){\n  layout(1)\n  res = hmap_res\n  class_members = res$cluster_members\n  class_colors = res$colors\n  class_sizes = sapply(class_members, length)\n  #   classSizes = res[[1]]\n  #   asPlotted = res[[3]]\n  #   class_colors = res[[4]]\n  data_out = matrix(unlist(class_members), ncol = 1)\n  colors_out = matrix('black', nrow = sum(class_sizes), ncol = 1)\n  rownames(colors_out) = unlist(class_members)\n  for(i in 1:length(class_members)){\n    colors_out[class_members[[i]],] = class_colors[i]\n  }  \n  #   soFar = 0\n  #   for(j in 1:length(classSizes)){\n  #     size = classSizes[j]\n  #     start = soFar + 1\n  #     end = soFar + size\n  #     soFar = soFar + size\n  #     colors_out[start:end,] = class_colors[j]\n  #   }\n  #grow nrow of matrices to nper_column\n  while(nrow(data_out) %% nper_column != 0){\n    data_out = rbind(data_out, '')\n    colors_out = rbind(colors_out, 'black')\n  }\n  \n  color_data = matrix(colors_out, nrow = nper_column)\n  names_data = matrix(data_out, nrow = nper_column)\n  #grow ncol to nper_row\n  while(ncol(color_data) %% nper_row != 0){ #grow matrices to nper_row\n    color_data = cbind(color_data, rep('black', nrow(color_data)))\n    names_data = cbind(names_data, rep('', nrow(color_data)))\n  }\n  #rev rows for plotting\n#   color_data = color_data[nrow(color_data):1,]\n#   names_data = names_data[nrow(names_data):1,]\n  \n  for(i in 1:(ncol(color_data)/nper_row)){\n    \n    start = (i - 1) * nper_row + 1\n    end = i * nper_row\n    #     print(start)\n    #     print(end)\n    names_tmp = names_data[,start:end, drop = F]\n    color_tmp = color_data[,start:end, drop = F]\n#     print(names_tmp)\n#     print(color_tmp)\n    plot(1, xlim = c(0,(ncol(names_tmp) + 1)), ylim = c((nrow(names_tmp) + 1), 0), type = 'n', xlab = '', ylab = '', axes = F)\n    for(x in 1:ncol(names_tmp)) for(y in 1:nrow(names_tmp)){\n      if(color_tmp[y,x] == 'white'){\n        rect((x-.5) * col_spacing,\n             (y-.5) * row_spacing,\n             (x+.5) * col_spacing,\n             (y+.5) * row_spacing,\n             bty = 'n', col = 'black',border = \"transparent\")\n      }\n    }\n    for(x in 1:ncol(names_tmp)) for(y in 1:nrow(names_tmp)){\n      \n      text(x * col_spacing,y * row_spacing,names_tmp[y,x], col = color_tmp[y,x], cex = cex, adj = c(.5,.5))\n    }\n    \n    \n  }\n}" }
{ "repo_name": "jonnaro/5C-Data", "ref": "refs/heads/master", "path": "scripts/calls-by-date.R", "content": "# Total Call Volume by Date\n\nlibrary(tidyverse)\nlibrary(scales)\n\n# summarize calls by date\ndate_trend <- calls %>%\n  group_by(date, result) %>%\n  summarize(calls = sum(calls)) %>%\n  filter(date > \"2017-01-24\")\n\ndate_trend <- spread(date_trend, result, calls)\n\ndate_trend <- date_trend %>%\n  mutate(SuccessCalls = contacted + vm,\n         FailedCalls  = unavailable,\n         TotalCalls   = contacted + vm + unavailable,\n         AvailPct     = 100 * (contacted + vm) / TotalCalls) %>%\n  select(date, SuccessCalls, FailedCalls, TotalCalls, AvailPct)\n\ndate_trend <- within(date_trend, CumSuccess <- cumsum(SuccessCalls))\ndate_trend <- within(date_trend, CumTotal   <- cumsum(TotalCalls))\n\n# output: daily call volume\nggplot(date_trend, aes(x = date)) + \n  geom_bar(aes(y = TotalCalls, group = 1), stat = \"identity\", fill = \"orange\") +\n  geom_bar(aes(y = SuccessCalls, group = 1), stat = \"identity\", fill = \"blue\") +\n  labs(title   = \"Daily Call Volume\",\n       caption = paste(\"Last updated:\", updated)) +\n  scale_x_date(date_breaks = \"1 day\", labels = date_format(\"%b %d\")) +\n  scale_y_continuous(expand = c(0, 0), labels = comma) +\n  theme(axis.text.x        = element_text(angle = 45, hjust = 1),\n        axis.text.y        = element_text(size = rel(1.1)),\n        axis.ticks         = element_blank(),\n        axis.title.x       = element_blank(),\n        axis.title.y       = element_text(size = rel(0.8), face = \"italic\"),\n        legend.text        = element_text(size = rel(0.8)),\n        legend.title       = element_blank(),\n        panel.background   = element_blank(),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.major.y = element_line(colour = \"grey80\", size = 0.2),\n        panel.grid.minor.y = element_line(colour = \"grey80\", size = 0.2),\n        plot.title         = element_text(size = rel(1.5), hjust = 0))\n\nggsave(\"./output/daily-call-volume.png\")\n\n# output: cumulative calls per day\nggplot(date_trend, aes(x = date, y = CumSuccess, group = 1)) + \n  geom_line(colour = \"blue\", size = 1.0) +\n  geom_point(colour = \"black\") +\n  labs(title   = \"Total Successful Calls Made Over Time\",\n       caption = paste(\"Last updated:\", updated)) +\n  expand_limits(y = 0) + \n  scale_x_date(date_breaks = \"1 day\", labels = date_format(\"%b %d\")) +\n  scale_y_continuous(labels = comma) +\n  theme(axis.text.x        = element_text(angle = 90, hjust = 1),\n        axis.text.y        = element_text(size = rel(1.0)),\n        axis.ticks         = element_blank(),\n        axis.title.x       = element_blank(),\n        axis.title.y       = element_text(size = rel(0.8), face = \"italic\"),\n        legend.text        = element_text(size = rel(0.9)),\n        legend.title       = element_blank(),\n        panel.background   = element_blank(),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.major.y = element_line(colour = \"grey80\", size = 0.2),\n        panel.grid.minor.y = element_line(colour = \"grey80\", size = 0.2),\n        plot.title         = element_text(size = rel(1.5), hjust = 0))\n\nggsave(\"./output/cumulative-call-volume.png\")\n  " }
{ "repo_name": "cities-lab/VisionEval", "ref": "refs/heads/master", "path": "sources/modules/VESimHouseholds/tests/scripts/test.R", "content": "library(rhdf5)\n\n#Test CreateHouseholds module\nsource(\"R/CreateHouseholds.R\")\ntestModule(\n  ModuleName = \"CreateHouseholds\",\n  LoadDatastore = FALSE,\n  SaveDatastore = TRUE,\n  DoRun = TRUE\n)\n\n#Test PredictWorkers module\nsource(\"R/PredictWorkers.R\")\ntestModule(\n  ModuleName = \"PredictWorkers\",\n  LoadDatastore = TRUE,\n  SaveDatastore = TRUE,\n  DoRun = TRUE\n)\n\n#Test AssignLifeCycle module\nsource(\"R/AssignLifeCycle.R\")\ntestModule(\n  ModuleName = \"AssignLifeCycle\",\n  LoadDatastore = TRUE,\n  SaveDatastore = TRUE,\n  DoRun = TRUE\n)\n\n#Test PredictIncome module\nsource(\"R/PredictIncome.R\")\ntestModule(\n  ModuleName = \"PredictIncome\",\n  LoadDatastore = TRUE,\n  SaveDatastore = TRUE,\n  DoRun = TRUE\n)\n\n#Test PredictHousing module\nsource(\"R/PredictHousing.R\")\ntestModule(\n  ModuleName = \"PredictHousing\",\n  LoadDatastore = TRUE,\n  SaveDatastore = TRUE,\n  DoRun = TRUE\n)\n" }
{ "repo_name": "Myasuka/systemml", "ref": "refs/heads/master", "path": "src/test/scripts/functions/misc/IPAUnknownRecursion.R", "content": "#-------------------------------------------------------------\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n# \n#   http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n#-------------------------------------------------------------\n\n\r\nargs <- commandArgs(TRUE)\r\noptions(digits=22)\r\n\r\nlibrary(\"Matrix\")\r\n\r\nfactorial <- function(arr, pos){\r\n\tif(pos == 1){\r\n     arr[1, pos] = 1\r\n\t} else {\r\n\t\tarr = factorial(arr, pos-1)\r\n\t\tarr[1, pos] = pos * arr[1, pos-1]\r\n\t}\r\n\r\n  return(arr);\t\r\n}\r\n\r\nn = as.integer(args[1])\r\narr = matrix(0, 1, n)\r\narr = factorial(arr, n)\r\n\r\nR = matrix(0, 1, n);\r\nfor(i in 1:n)\r\n{\r\n   R[1,i] = arr[1, i];\r\n}\r\n\r\nwriteMM(as(R, \"CsparseMatrix\"), paste(args[2], \"R\", sep=\"\")); \r\n\r\n\r\n" }
{ "repo_name": "swarm-lab/Rvision", "ref": "refs/heads/master", "path": "vignettes/z2_io.R", "content": "## ----eval=FALSE, echo=TRUE----------------------------------------------------\n#  # Find the path to the Balloon.mp4 video provided with Rvision\n#  path_to_video <- system.file(\"sample_vid\", \"Balloon.mp4\", package = \"Rvision\")\n#  \n#  # Open the video file stream\n#  my_video <- video(filename = path_to_video)\n\n## ----eval=FALSE, echo=TRUE----------------------------------------------------\n#  release(my_video)\n\n## ----eval=FALSE, echo=TRUE----------------------------------------------------\n#  # Start the default camera stream\n#  my_stream <- stream(index = 0)\n\n## ----eval=FALSE, echo=TRUE----------------------------------------------------\n#  release(my_stream)\n\n## ----eval=FALSE, echo=TRUE----------------------------------------------------\n#  # Find the path to the bunny.png image provided with Rvision\n#  path_to_image <- system.file(\"sample_img\", \"bunny.png\", package = \"Rvision\")\n#  \n#  # Load the image in memory\n#  my_image <- image(filename = path_to_image)\n\n## ----eval=FALSE, echo=TRUE----------------------------------------------------\n#  # Capture the next available frame from the my_video object created earlier\n#  my_image <- readNext(my_video)\n#  \n#  # Capture frame 100 from the my_video object created earlier\n#  my_image <- readFrame(my_video, 100)\n\n## ----eval=FALSE, echo=TRUE----------------------------------------------------\n#  # Capture the next available frame from the my_stream object created earlier\n#  my_image <- readNext(my_stream)\n\n## ----eval=FALSE, echo=TRUE----------------------------------------------------\n#  # Create a 100 x 100 x 3 random array\n#  my_array <- array(rnorm(30000), dim = c(100, 100, 3))\n#  \n#  # Load the image in memory\n#  my_image <- image(my_array)\n\n## ----eval=FALSE, echo=TRUE----------------------------------------------------\n#  # Create a 1280x720 video file called file.mp4 on the desktop, using the x264\n#  # codec at 30 frames per second\n#  my_writer <- videoWriter(\"~/Desktop/file.mp4\", fourcc = \"x264\", fps = 30,\n#                           height = 720, width = 1280)\n\n## ----eval=FALSE, echo=TRUE----------------------------------------------------\n#  # Capture the next 30 frames from the my_stream camera stream created earlier\n#  # and write them to file.mp4\n#  for (i in seq_len(30)) {\n#    writeFrame(my_writer, readNext(my_stream))\n#  }\n\n## ----eval=FALSE, echo=TRUE----------------------------------------------------\n#  release(my_writer)\n\n## ----eval=FALSE, echo=TRUE----------------------------------------------------\n#  # Write the my_image object created earlier to a PNG file called file.png on the\n#  # desktop\n#  write.Image(my_image, \"~/Desktop/file.png\")\n\n" }
{ "repo_name": "LanceFiondella/srt.core", "ref": "refs/heads/master", "path": "utility/plots/PlotModelResults.R", "content": "# Plot model results (and raw data, if specified)\n\nplot_model_results <- function(ModResults, DataModeled, DataSetName, DisplayModels, DataView, PlotView, PlotData, PlotDataEnd, RelMissionTime, plotWidthRange, plotHeightRange, plotPixels, AdditionalCurveLength) {\n  \n  require(ggplot2)\n  \n  PlotFault <- FALSE\n  \n  # Initialize the model results plot\n  \n  localResultsPlot <- ggplot()\n  \n  # Set up the values that we'll need to create a plot legend\n  \n  scaleManBreaks <- c()\n  scaleManColors <- c()\n  \n  # Create plot axes\n  \n  if(DataView == \"IF\") {\n    localResultsPlot <- localResultsPlot + ggtitle(paste0(\"Interfailure Times vs. Cumulative Test Time for \", DataSetName))\n    localResultsPlot <- localResultsPlot + xlab(\"Cumulative Test Time\")+ylab(\"Times Between Successive Failures\")\n  } else if(DataView == \"MVF\") {\n    localResultsPlot <- localResultsPlot + ggtitle(paste0(\"Cumulative Failures vs. Cumulative Test Time for \", DataSetName))\n    localResultsPlot <- localResultsPlot + xlab(\"Cumulative Test Time\")+ylab(\"Cumulative Failures\")\n  } else if(DataView == \"FI\") {\n    localResultsPlot <- localResultsPlot + ggtitle(paste0(\"Failure Intensity vs. Cumulative Test Time for \", DataSetName))\n    localResultsPlot <- localResultsPlot + xlab(\"Cumulative Test Time\")+ylab(\"Failure Intensity\")\n  } else if(DataView == \"R\") {\n    localResultsPlot <- localResultsPlot + ggtitle(paste0(\"Reliability vs. Cumulative Test Time for \", DataSetName))\n    localResultsPlot <- localResultsPlot + xlab(\"Cumulative Test Time\")+ylab(\"Reliability\")\n  } else if(DataView == \"R_growth\") {\n    localResultsPlot <- localResultsPlot + ggtitle(paste0(\"Reliability Growth vs. Cumulative Test Time for \", DataSetName, \": Operational Time of \", as.character(RelMissionTime)))\n    localResultsPlot <- localResultsPlot + xlab(\"Cumulative Test Time\")+ylab(\"Reliability Growth\")\n  } else if (DataView == \"FC\") {\n    localResultsPlot <- localResultsPlot + ggtitle(paste0(\"Failure Counts vs. Cumulative Test Time for \", DataSetName))\n    localResultsPlot <- localResultsPlot + xlab(\"Cumulative Test Time\")+ylab(\"Failures per Test Interval\")\n  } else {\n    \n    # Couldn't identify view of data to display.\n    # #print an error message.\n    \n    #print(msgModelDataViewUnknown)\n    PlotFault <- TRUE\n  }\n  \n  # Set up the time offset for the data that's going to be input to the models.\n  # Since the model lines get updated each time the plot is redrawn, the models\n  # need to be run each time the plot is redrawn.  We also need to set up an\n  # offset for the starting failure number in case we're dealing with a subset\n  # of the data.\n  \n  timeOffset <- DataModeled$FT[1]-DataModeled$IF[1]\n  failureOffset <- DataModeled$FN[1]-1\n  \n  # Set up the vectors for the x-axis when drawing curves on the plot.\n  \n  if(!is.null(plotWidthRange) && !is.null(plotHeightRange)) {\n    # We've zoomed in to a subset of the plot.  We don't need to specify the\n    # x values for each model.\n    \n    startPoint <- max(plotWidthRange[1], DataModeled$FT[1])\n    timeAxisLinePlotVals <- seq(from=startPoint, to=plotWidthRange[2], by=(plotWidthRange[2]-startPoint)/(plotPixels-1))\n  }\n  \n  for (modelIndex in DisplayModels) {\n    # Get the model parameters.\n    \n    model_params <- c()\n    for (parmIndex in 1:length(get(paste0(modelIndex, \"_params\")))) {\n      model_params <- c(model_params, ModResults[[paste0(modelIndex, \"_parm_\", parmIndex)]][length(DataModeled[[1]])])\n    }\n    names(model_params) <- paste(modelIndex, get(paste0(modelIndex, \"_params\")), sep=\"_\")\n    \n    # Create plot data, axes, and titles based on the view\n    # of the data selected by the user (e.g., MTTFs)\n\n    # Set up the vectors for the x-axis when drawing curves on the plot.\n    # If we haven't already done this for a section of the plot that we're\n    # zooming in to, we do it for the whole plot here.\n    \n    if(is.null(plotWidthRange) && is.null(plotHeightRange)) {\n      # We're looking at the entire plot.\n      \n      xAxisVals <- unlist(subset(ModResults, !is.infinite(get(paste0(modelIndex, \"_CumTime\"))), select=get(paste0(modelIndex, \"_CumTime\"))), use.names=FALSE)\n      IFVals <- unlist(DataModeled$IF, use.names=FALSE)\n      timeAxisLinePlotVals <- seq(from=xAxisVals[1], to=xAxisVals[length(xAxisVals)]+AdditionalCurveLength, by=(xAxisVals[length(xAxisVals)]+AdditionalCurveLength-(xAxisVals[1]-IFVals[1]))/(plotPixels-1))\n    }\n    \n    model_input_data <- data.frame(\"FT\" = timeAxisLinePlotVals-timeOffset)\n    \n    if(DataView == \"IF\") {\n      model_plot_data <- data.frame(\"Time\" = ModResults[[paste(modelIndex, \"CumTime\", sep=\"_\")]], \"Failure\" = ModResults[[paste(modelIndex, \"IF\", sep=\"_\")]], \"Model\" = rep(get(paste(modelIndex, \"fullname\", sep=\"_\")), length(ModResults[[\"Failure\"]])))\n      local_estimate <- get(paste(modelIndex,\"MTTF\",sep=\"_\"))(model_params, model_input_data)[[\"MTTF\"]]\n      model_line_data <- model_plot_data\n      # model_line_data <- data.frame(\"Time\"= unlist(model_input_data+timeOffset, use.names=FALSE), \"Failure\"=local_estimate, \"Model\" = rep(get(paste(modelIndex, \"fullname\", sep=\"_\")), length(local_estimate)))\n    } else if(DataView == \"MVF\") {\n      model_plot_data <- data.frame(\"Time\" = ModResults[[paste(modelIndex, \"CumTime\", sep=\"_\")]], \"Failure\" = ModResults[[paste(modelIndex, \"MVF\", sep=\"_\")]], \"Model\" = rep(get(paste(modelIndex, \"fullname\", sep=\"_\")), length(ModResults[[\"Failure\"]])))\n      local_estimate <- get(paste(modelIndex,\"MVF\",sep=\"_\"))(model_params, model_input_data)[[\"Failure\"]] + failureOffset\n      model_line_data <- data.frame(\"Time\"= timeAxisLinePlotVals, \"Failure\"=local_estimate, \"Model\" = rep(get(paste(modelIndex, \"fullname\", sep=\"_\")), length(local_estimate)))\n      \n      # Now we see if this is a case in which the model assumes a finite number of failures, and if we've\n      # asked the model to predict ahead for more failures than the model thinks are left.\n      \n      if (is.infinite(ModResults[[paste(modelIndex, \"CumTime\", sep=\"_\")]][length(ModResults[[paste(modelIndex, \"CumTime\", sep=\"_\")]])])) {\n        # model_line_data[length(model_line_data[,1])+1,] <- unlist(c(Inf, model_params[get(paste0(modelIndex, \"_numfailsparm\")[1])], get(paste(modelIndex, \"fullname\", sep=\"_\"))), use.names=FALSE)\n      }\n    } else if(DataView == \"FI\") {\n      model_plot_data <- data.frame(\"Time\" = ModResults[[paste(modelIndex, \"CumTime\", sep=\"_\")]], \"Failure\" = ModResults[[paste(modelIndex, \"FI\", sep=\"_\")]], \"Model\" = rep(get(paste(modelIndex, \"fullname\", sep=\"_\")), length(ModResults[[\"Failure\"]])))\n      local_estimate <- get(paste(modelIndex,\"FI\",sep=\"_\"))(model_params, model_input_data)[[\"Failure_Rate\"]]\n      model_line_data <- data.frame(\"Time\"= timeAxisLinePlotVals, \"Failure\"=local_estimate, \"Model\" = rep(get(paste(modelIndex, \"fullname\", sep=\"_\")), length(local_estimate)))\n    } else if(DataView == \"R\") {\n      model_plot_data <- data.frame(\"Time\" = ModResults[[paste(modelIndex, \"CumTime\", sep=\"_\")]], \"Failure\" = ModResults[[paste(modelIndex, \"Rel\", sep=\"_\")]], \"Model\" = rep(get(paste(modelIndex, \"fullname\", sep=\"_\")), length(ModResults[[\"Failure\"]])))\n    } else if(DataView == \"R_growth\") {\n      \n      # This is an interactive plot - users can change the mission time for\n      # which reliability will be computed.  The plot will then be redrawn.\n      \n      rg_input_data <- data.frame(\"FT\" = subset(ModResults, !is.infinite(get(paste0(modelIndex, \"_CumTime\"))), select=get(paste0(modelIndex, \"_CumTime\")))-timeOffset)\n      names(rg_input_data) <- c(\"FT\")\n      temp_R_growth <- data.frame(\"Reliability_Growth\"=c(get(paste(modelIndex,\"R_growth\",sep=\"_\"))(model_params, rg_input_data, RelMissionTime)[[\"Reliability_Growth\"]], rep(1, length(ModResults[[paste(modelIndex, \"CumTime\", sep=\"_\")]])-length(rg_input_data[[1]]))))\n      model_plot_data <- data.frame(\"Time\" = ModResults[[paste(modelIndex, \"CumTime\", sep=\"_\")]], \"Failure\" = temp_R_growth[[\"Reliability_Growth\"]], \"Model\" = rep(get(paste(modelIndex, \"fullname\", sep=\"_\")), length(ModResults[[\"Failure\"]])))\n      local_estimate <- get(paste(modelIndex,\"R_growth\",sep=\"_\"))(model_params, model_input_data, RelMissionTime)[[\"Reliability_Growth\"]]\n      model_line_data <- data.frame(\"Time\"= timeAxisLinePlotVals, \"Failure\"=local_estimate, \"Model\" = rep(get(paste(modelIndex, \"fullname\", sep=\"_\")), length(local_estimate)))\n      \n    } else if (DataView == \"FC\") {\n      model_plot_data <- data.frame(\"Time\" = ModResults[[paste(modelIndex, \"CumTime\", sep=\"_\")]], \"Failure\" = ModResults[[paste(modelIndex, \"FC\", sep=\"_\")]], \"Model\" = rep(get(paste(modelIndex, \"fullname\", sep=\"_\")), length(ModResults[[\"Failure\"]])))\n    } else {\n      \n      # Couldn't identify view of data to display.\n      # #print an error message.\n      \n      #print(msgModelDataViewUnknown)\n      PlotFault <- TRUE\n    }\n    \n    scaleManBreaks <- c(scaleManBreaks, get(paste(modelIndex, \"fullname\", sep=\"_\")))\n    scaleManColors <- c(scaleManColors, get(paste(modelIndex, \"plotcolor\", sep=\"_\")))\n    \n    if (PlotView == \"points_and_lines\") {\n      localResultsPlot <- localResultsPlot + geom_point(data=model_plot_data,aes(Time,Failure,color=Model)) + geom_line(data=model_line_data, aes(Time,Failure,color=Model,linetype=Model))\n    } else if (PlotView == \"points\") {\n      localResultsPlot <- localResultsPlot + geom_point(data=model_plot_data,aes(Time,Failure,color=Model))\n    } else if (PlotView == \"lines\") {\n      localResultsPlot <- localResultsPlot + geom_line(data=model_line_data, aes(Time,Failure,color=Model,linetype=Model))\n    } else {\n      \n      # Couldn't identify the plot type.\n      # #print an error message.\n      \n      #print(paste0(\"plot_model_results: \", msgPlotTypeUnknown))\n      PlotFault <- TRUE\n    }\n  }\n  \n  \n  if(PlotData) {\n    \n    # There aren't any sensible plots to be drawn if we're showing\n    # reliability or reliability growth.\n    \n    if((DataView != \"R\") && (DataView != \"R_growth\")) {\n      scaleManBreaks <- c(scaleManBreaks, \"Data\")\n      scaleManColors <- c(scaleManColors, \"black\")\n      \n      if (dataType(names(DataModeled)) == \"FR\") {\n        FN <- DataModeled$FN\n        FT <- DataModeled$FT\n        IF <- DataModeled$IF\n      } else if (dataType(names(DataModeled)) == \"FC\") {\n        \n        # We need to complete the failure counts models.\n        \n      } else {\n        # The type of the input data couldn't be determined.\n        # #print an error message.\n        \n        #print(msgInputDataTypeUnknown)\n        PlotFault <- TRUE\n      }\n      \n      # Now plot data depending on the view of the data.\n      \n      if(DataView == \"IF\") {\n        model_plot_data <- data.frame(\"Time\"=FT, \"Failure\"=IF, \"Model\"=rep(\"Data\", length(FT)))\n        localResultsPlot <- localResultsPlot\n      } else if(DataView == \"MVF\") {\n        model_plot_data <- data.frame(\"Time\"=FT, \"Failure\"=FN, \"Model\"=rep(\"Data\", length(FT)))\n      } else if(DataView == \"FI\") {\n        model_plot_data <- data.frame(\"Time\"=FT, \"Failure\"=c(1/IF), \"Model\"=rep(\"Data\", length(FT)))\n      } else if (DataView == \"FC\") {\n        model_plot_data <- data.frame(\"Time\"=FT, \"Failure\"=FC, \"Model\"=rep(\"Data\", length(FT)))\n      } else if (!((DataView == \"R\") || (DataView == \"R_growth\"))) {\n        \n        # Couldn't identify view of data to display.\n        # #print an error message.\n        \n        #print(msgModelDataViewUnknown)\n        PlotFault <- TRUE\n      }\n      \n      if (PlotView == \"points_and_lines\") {\n        localResultsPlot <- localResultsPlot + geom_point(data=model_plot_data,aes(Time,Failure,color=Model)) + geom_step(data=model_plot_data, aes(Time,Failure,color=Model,linetype=Model))\n      } else if (PlotView == \"points\") {\n        localResultsPlot <- localResultsPlot + geom_point(data=model_plot_data,aes(Time,Failure,color=Model))\n      } else if (PlotView == \"lines\") {\n        localResultsPlot <- localResultsPlot + geom_step(data=model_plot_data, aes(Time,Failure,color=Model,linetype=Model))\n      } else {\n        \n        # Couldn't identify the plot type.\n        # #print an error message.\n        \n        #print(paste0(\"plot_model_results: \", msgPlotTypeUnknown))\n        PlotFault <- TRUE\n      }\n    }\n  }\n  \n  if(PlotDataEnd) {\n    localResultsPlot <- localResultsPlot + geom_vline(xintercept=DataModeled$FT[length(DataModeled$FT)], linetype='longdash', alpha = 0.8)\n  }\n    \n  \n  #localResultsPlot <- localResultsPlot + scale_color_manual(\"\", breaks=scaleManBreaks, values=scaleManColors)\n  localResultsPlot <- localResultsPlot + theme(legend.position = \"bottom\", text = element_text(size=14))\n\n  if(PlotFault) {\n    localResultsPlot = NULL\n  }\n  return(localResultsPlot)\n}\n" }
{ "repo_name": "brunocontrino/DOSCHEDA", "ref": "refs/heads/master", "path": "R/plot.ChemoProtSet.R", "content": "#' Default plot for objects of class ChemoProtSet\n#'\n#'Description\n#'\n#' @param x object of class 'ChemoProtSet'\n#' @param sigmoidCoef the sigmoidal coeffcient, one of ('difference', 'slope', 'rb50'). Obselete if modelType is 'linear'\n#' @param ... other plotting options\n#' @return plot for objects of class ChemoProtSet\n#' @import ggplot2\n#' @import gridExtra\n#' @import reshape2\n#' @export\n\nplot.ChemoProtSet <- function(x, sigmoidCoef = \"rb50\", ...) {\n    \n    inherits(x, \"ChemoProtSet\")\n    modParams <- getParameters(x)\n    if (modParams$modelType == \"linear\") {\n        data.merged <- getFinal(x)\n        ..count.. <- NULL\n        m0 <- ggplot2::ggplot(data.merged, ggplot2::aes(x = data.merged$P.Value_slope))\n        m0 <- m0 + geom_histogram(ggplot2::aes(fill = ..count..), binwidth = 0.01) + ggplot2::scale_fill_gradient(\"Count\", \n            low = \"green\", high = \"red\") + ggplot2::xlab(\"P.val slope\")\n        \n        ..count.. <- NULL\n        m1 <- ggplot2::ggplot(data.merged, ggplot2::aes(x = data.merged$P.Value_intercept))\n        m1 <- m1 + geom_histogram(ggplot2::aes(fill = ..count..), binwidth = 0.01) + ggplot2::scale_fill_gradient(\"Count\", \n            low = \"green\", high = \"red\") + ggplot2::xlab(\"Pval intercept\")\n        \n        m2 <- ggplot2::ggplot(data.merged, ggplot2::aes(x = data.merged$P.Value_quadratic))\n        m2 <- m2 + geom_histogram(ggplot2::aes(fill = ..count..), binwidth = 0.01) + ggplot2::scale_fill_gradient(\"Count\", \n            low = \"green\", high = \"red\") + ggplot2::xlab(\"Pval quadratic\")\n        \n        gridExtra::grid.arrange(m0, m1, m2)\n        \n    } else {\n        \n        data_merged_2 <- getFinal(x)\n        conc <- modParams$sigmoidConc\n        topperc <- 30\n        # difference in % between top and bottom\n        \n        if (modParams$dataType == \"intensity\") {\n            \n            pred.names <- paste0(\"predX\", 1:(modParams$chans - 1))\n            final.Names <- paste0(\"rep1_C\", 0:(modParams$chans - 2))\n            diffinter <- data_merged_2[(data_merged_2$predX1 - data_merged_2[, paste(\"predX\", (modParams$chans - \n                1), sep = \"\")]) > topperc & data_merged_2$predX1 <= 100, ]\n            \n        } else {\n            pred.names <- paste0(\"predX\", 1:(modParams$chans))\n            final.Names <- paste0(\"rep1_C\", 0:(modParams$chans - 1))\n            diffinter <- data_merged_2[(data_merged_2$predX1 - data_merged_2[, paste(\"predX\", (modParams$chans), \n                sep = \"\")]) > topperc & data_merged_2$predX1 <= 100, ]\n            \n        }\n        \n        \n        if (sigmoidCoef == \"difference\") {\n            \n            if (nrow(diffinter) > 0) {\n                Diff_Top_bottom_pred <- shape_for_ggplot_pred(diffinter, log2(conc), pred.names)\n                Diff_Top_bottom_perc <- shape_for_ggplot_perc(diffinter, log2(conc), final.Names)\n                what <- c(\"(Top - Bottom) >\")\n                GeneID <- factor(Diff_Top_bottom_pred$GeneID)\n                value <- NULL\n                Diff_Top_bottom <- ggplot2::ggplot() + ggplot2::geom_line(data = Diff_Top_bottom_pred, \n                  ggplot2::aes(x = x, y = value, colour = GeneID), size = 1) + ggplot2::geom_point(data = Diff_Top_bottom_perc, \n                  ggplot2::aes(x = x, y = value, colour = Diff_Top_bottom_perc$GeneID)) + ggplot2::labs(title = paste(what, \n                  topperc, sep = \"\"))\n                \n                Diff_Top_bottom\n            } else {\n                Diff_Top_bottom <- ggplot2::ggplot() + ggplot2::labs(title = paste(\"No significant Top-Bottom >\", \n                  topperc, \"%\", \"\\n\", \"has been found\", sep = \"\"))\n            }\n        } else if (sigmoidCoef == \"slope\") {\n            \n            ## next plot (SLOPE)\n            \n            top <- 15\n            \n            if (modParams$dataType == \"intensity\") {\n                pred.names <- paste0(\"predX\", 1:(modParams$chans - 1))\n                final.Names <- paste0(\"rep1_C\", 0:(modParams$chans - 2))\n            } else {\n                pred.names <- paste0(\"predX\", 1:(modParams$chans))\n                final.Names <- paste0(\"rep1_C\", 0:(modParams$chans - 1))\n            }\n            \n            data_merged_2 <- getFinal(x)\n            \n            # Here make the subselections for using the ggplot functions SLOPE\n            slope <- stats::na.omit(data_merged_2[data_merged_2$SlopePval < 0.05, ])\n            slope_ordered <- stats::na.omit(slope[order(slope$SlopePval, decreasing = FALSE), ][1:top, \n                ])\n            if (nrow(slope_ordered) > 0) {\n                slope_pred <- shape_for_ggplot_pred(slope_ordered, log10(conc), pred.names)\n                slope_perc <- shape_for_ggplot_perc(slope_ordered, log10(conc), final.Names)\n                what <- c(\"Slope (p.val) \")\n                GeneID <- factor(slope_pred$GeneID)\n                Slope_pl <- ggplot2::ggplot() + ggplot2::geom_line(data = slope_pred, ggplot2::aes(x = x, \n                  y = value, colour = GeneID), size = 1) + ggplot2::geom_point(data = slope_perc, \n                  ggplot2::aes(x = x, y = value, colour = slope_perc$GeneID)) + ggplot2::labs(title = paste(what, \n                  \"Top\", top, sep = \"\"))\n                Slope_pl\n            } else {\n                Slope_pl <- ggplot2::ggplot() + ggplot2::labs(title = \"No significant Sigmoidal Slope has been found\")\n            }\n            \n        } else if (sigmoidCoef == \"rb50\") {\n            \n            ##### next plot (RB50)\n            \n            \n            top <- 15\n            \n            RB50 <- data.frame(stats::na.omit(data_merged_2[data_merged_2$RB50Err < as.numeric(summary(data_merged_2$RB50Err)[5]) & \n                data_merged_2$RB50Pval < 0.05 & data_merged_2$predX1 - data_merged_2[, paste0(\"predX\", \n                (modParams$chans - 1))] > 0 & data_merged_2$predX1 <= 100, ]))\n            \n            \n            RB50_ordered <- stats::na.omit(RB50[order(RB50$RB50Pval, decreasing = FALSE), ][1:top, \n                ])\n            \n            if (nrow(RB50_ordered) > 0) {\n                RB50_pred <- shape_for_ggplot_pred(RB50_ordered, log10(conc), pred.names)\n                RB50_perc <- shape_for_ggplot_perc(RB50_ordered, log10(conc), final.Names)\n                what <- c(\"RB50 (p.val) \")\n                GeneID <- factor(RB50_pred$GeneID)\n                RB50_pl <- ggplot2::ggplot() + ggplot2::geom_line(data = RB50_pred, ggplot2::aes(x = x, \n                  y = value, colour = GeneID), size = 1) + ggplot2::geom_point(data = RB50_perc, \n                  ggplot2::aes(x = x, y = value, colour = RB50_perc$GeneID)) + ggplot2::labs(title = paste(what, \n                  \"Top\", top, sep = \"\"))\n                RB50_pl\n            } else {\n                RB50_pl <- ggplot2::ggplot() + ggplot2::labs(title = \"No significant RB50 has been found\")\n                print(RB50_pl)\n            }\n        } else {\n            message(\"sigmoidCoef not accepted please enter one of: \\\"difference\\\", \\\"slope\\\" or \\\"rb50\\\"\")\n        }\n    }\n}\n" }
{ "repo_name": "KellyBlack/R-Object-Oriented-Programming", "ref": "refs/heads/master", "path": "chapter1/chapter_1_ex19.R", "content": "#\r\n# Author: Kelly Black\r\n# Description: These are examples intended to be typed in from the command line.\r\n#              From chapter 1 of Object Oriented Programming in R\r\n#\r\n# The MIT License (MIT)\r\n# \r\n# Copyright (c) 2014, Kelly Black\r\n#\r\n# Permission is hereby granted, free of charge, to any person obtaining a copy\r\n# of this software and associated documentation files (the \"Software\"), to deal\r\n# in the Software without restriction, including without limitation the rights\r\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\n# copies of the Software, and to permit persons to whom the Software is\r\n# furnished to do so, subject to the following conditions:\r\n# \r\n# The above copyright notice and this permission notice shall be included in\r\n# all copies or substantial portions of the Software.\r\n# \r\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\r\n# THE SOFTWARE.\r\n#\r\n#\r\n\r\n\r\na <- as.character(4.5)\r\na\r\nis.character(a)\r\na <- 4.5\r\nis.character(a)\r\n" }
{ "repo_name": "quommit/settle", "ref": "refs/heads/master", "path": "tests/testthat/test_survey_data.R", "content": "testthat::context(\"Survey data retrieval tests\")\n\ntestthat::test_that(\"Method get_records retrieves hypertable data frame\", {\n  records <- get_records(tagarina_croplands)\n  testthat::expect_is(records, \"data.frame\")\n  testthat::expect_equal(records, tagarina_croplands$body$hypertable)\n})\n\ntestthat::test_that(\"Method get_recordcount retrieves the right length\", {\n  count <- get_recordcount(tagarina_croplands)\n  testthat::expect_equal(count, 460)\n})\n\ntestthat::test_that(\"Method get_mapcatalogue retrieves data frame\", {\n  records <- get_mapcatalogue(tagarina_croplands)\n  testthat::expect_is(records, \"data.frame\")\n})\n\ntestthat::test_that(\"Method get_mapcatalogue retrieves consistent data\", {\n  first_geom_name <- names(tagarina_croplands$header$geometries)[1]\n  first_geom_type <- class(tagarina_croplands$header$geometries[[1]])[1]\n  records <- get_mapcatalogue(tagarina_croplands)\n  testthat::expect_equal(grep(first_geom_name, records$mapname), 1)\n  testthat::expect_true(1 %in% grep(first_geom_type, records$featuretype))\n})\n\ntestthat::test_that(\"Method get_landholders retrieves character vector\", {\n  landholders <- get_landholders(tagarina_croplands)\n  testthat::expect_is(landholders, \"character\")\n})\n\ntestthat::test_that(\"Method get_landholders returns no duplicates\", {\n  landholders <- get_landholders(tagarina_croplands)\n  testthat::expect_equal(anyDuplicated(landholders), 0)\n})\n\ntestthat::test_that(\"Method nlandholders retrieves the rigth length\", {\n  count <- nlandholders(tagarina_croplands)\n  testthat::expect_equal(count, 42)\n})\n\ntestthat::test_that(\"Method get_agglevel_names retrieves character vector\", {\n  placenames <- get_agglevel_names(tagarina_croplands, 1)\n  testthat::expect_is(placenames, \"character\")\n})\n\ntestthat::test_that(\"Method get_agglevel_names returns no duplicates\", {\n  placenames <- get_agglevel_names(tagarina_croplands, 1)\n  testthat::expect_equal(anyDuplicated(placenames), 0)\n})\n\ntestthat::test_that(\"Method nagglevels retrieves the rigth length\", {\n  count <- nagglevels(tagarina_croplands)\n  testthat::expect_equal(count, 2)\n})\n\ntestthat::test_that(\"Method nplots retrieves the rigth length\", {\n  count <- nplots(tagarina_croplands)\n  testthat::expect_equal(count, 64)\n})" }
{ "repo_name": "anjalimbhatt/sicss-culturalvariation", "ref": "refs/heads/master", "path": "analyses/misc.R", "content": "library(stringr)\n\ntw <- lapply(list.files(), \n             function(x) {print(x) \n               as.data.frame(parse_stream(x))}) %>%\n        bind_rows()\n\ncoord = tw %>%\n  filter(!is.na(coordinates) | place_type == \"city\") %>%\n  separate(place_full_name, c(\"town\", \"state\"), \", \") %>%\n  filter(!is.na(coordinates) | state == \"IA\") %>%\n  filter(source != \"TweetMyJOBS\")\n\n\n\n###################\n########\n\ngt = read.delim(\"GeoText.2010-10-12/full_text.txt\", sep =\"\\t\", header = F)\n\ngt.clean = gt %>%\n  select(-V2, -V3)\n\nnames(gt.clean) = c(\"userid\", \"latitude\", \"longitude\", \"tweet\")\n\nd.iowa  = gt.clean %>%\n  filter(longitude > -96.6397171020508 & longitude < -90.1400604248047) %>%\n  filter(latitude > 40.3755989074707 & latitude < 43.5011367797852)\n\n  \nlength(unique(d.iowa$userid))\n\nlength(unique(gt.clean$userid))\n# Iowa bounding box\niowa_bounding_box = c(-96.6397171020508, 40.3755989074707, # southwest coordinates\n                      -90.1400604248047, 43.5011367797852) # northeast coordinates\n\n#######\ncoord2 = coord %>%\n  separate(coordinates, c(\"lat\", \"lon\"), \" \") %>%\n  mutate(lon = as.numeric(lon),\n         lat = as.numeric(lat)) %>%\n  mutate_geocode(value.place_name)\n\nmap <- get_stamenmap(iowa_bounding_box, zoom = 5, maptype = \"toner-lite\")\nggmap(map)\nqmplot(lon, lat, \n       data = coord2, maptype = \"toner-lite\", color = I(\"red\")) \n\nggmap(map) +\n  geom_point(aes(x = lon, y = lat), data = coord2, alpha = .5, color = \"red\") +\n  theme_bw()\n" }
{ "repo_name": "inbo/reporting-rshiny-grofwildjacht", "ref": "refs/heads/master", "path": "reporting-grofwild/R/countYearSchade.R", "content": "# Interactive barplot for wildschade data in function of year\n# \n# Author: mvarewyck\n###############################################################################\n\n\n\n#' Create interactive barplot for wildschade data variable of interest ifo year\n#' @inheritParams countYearAge\n#' @param type character, variable name in \\code{data} of interest \n#' @return list with\n#' \\itemize{\n#' \\item{'plot': }{plotly object}\n#' \\item{'data': }{data.fram used for plot}\n#' } \n#' @author mvarewyck\n#' @import plotly\n#' @importFrom RColorBrewer brewer.pal\n#' @export\ncountYearSchade <- function(data, jaartallen = NULL, type = NULL,\n        summarizeBy = c(\"count\", \"percent\"),\n        width = NULL, height = NULL) {\n    \n    # For R CMD check\n    freq <- NULL\n    \n    typeNaam <- switch(type,\n            \"wildsoort\" = \"Wildsoort\",\n            \"SoortNaam\" = \"Gewas\", \n            \"schadeCode\" = \"Type Schade\",\n            type\n    )\n    \n    summarizeBy <- match.arg(summarizeBy)\n    \n    if (is.null(jaartallen))\n        jaartallen <- unique(data$afschotjaar)\n    \n    # Select data\n    plotData <- data[data$afschotjaar %in% jaartallen, \n            c(\"afschotjaar\", type)]\n    names(plotData) <- c(\"jaar\", \"variabele\")\n    \n    # Percentage collected\n    nRecords <- nrow(plotData)\n    \n    # Remove some categories\n    plotData[is.na(plotData$variabele), \"variabele\"] <- \"Onbekend\"\n    plotData <- plotData[!is.na(plotData$jaar) & !is.na(plotData$variabele), ]\n    \n    # Summarize data per year and age category\n    summaryData <- count(df = plotData, vars = names(plotData))\n    \n    \n    # Add line for records with 0 observations\n    fullData <- cbind(expand.grid(\n                    jaar = min(summaryData$jaar):max(summaryData$jaar),\n                    variabele = unique(summaryData$variabele)))\n    summaryData <- merge(summaryData, fullData, all.x = TRUE, all.y = TRUE)\n    summaryData$freq[is.na(summaryData$freq)] <- 0\n    \n    \n    # Calculate percentages \n    summaryData <- ddply(summaryData, \"jaar\", transform, \n            percent = freq / sum(freq) * 100)\n    \n    # Summarize data per year\n    totalCount <- count(df = plotData, vars = \"jaar\")\n    totalCount$totaal <- totalCount$freq\n    totalCount$freq <- NULL\n    \n    summaryData <- merge(summaryData, totalCount)\n    \n    \n    # Make full schade names\n    if (type == \"schadeCode\") {\n      summaryData$variabele <- names(fullNames(summaryData$variabele))\n    }\n    \n    # For optimal displaying in the plot\n    summaryData$jaar <- as.factor(summaryData$jaar)\n    \n    if (summarizeBy == \"count\") {\n        \n        summaryData$text <- paste0(\"<b>\", summaryData$variabele, \" in \", summaryData$jaar, \"</b>\",\n                \"<br>Aantal: \", summaryData$freq, \n                \"<br>Totaal: \", summaryData$totaal)\n        \n    } else {\n        \n        summaryData$text <- paste0(\"<b>\", summaryData$variabele, \" in \", summaryData$jaar, \"</b>\",\n                \"<br>Percent: \", round(summaryData$percent), \"%\")\n        \n    }\n    \n    # Max. 40 colors\n    paletteNames <- c(\"Set3\", \"Paired\", \"Dark2\", \"Pastel2\")\n    colors <- unlist(sapply(paletteNames, function(x)\n        suppressWarnings(brewer.pal(n = 12, name = x))))[1:length(unique(summaryData$variabele))]\n    names(colors) <- unique(summaryData$variabele)\n    if (\"onbekend\" %in% tolower(unique(summaryData$variabele)))\n      colors[tolower(names(colors)) == \"onbekend\"] <- \"gray\"\n    \n    title <- paste0(typeNaam, \" \",\n            ifelse(length(jaartallen) > 1, paste(\"van\", min(jaartallen), \"tot\", max(jaartallen)),\n                    paste(\"in\", jaartallen))\n    )\n    \n    \n    \n    # Create plot\n    toPlot <- plot_ly(data = summaryData, x = ~jaar,\n                    y = if (summarizeBy == \"count\") ~freq else ~percent, \n                    color = ~variabele, text = ~text, hoverinfo = \"text+name\",\n                    colors = colors, type = \"bar\",\n                    width = width, height = height) %>%\n            layout(title = title,\n                    xaxis = list(title = \"Jaar\"), \n                    yaxis = list(title = if (summarizeBy == \"count\") \"Aantal\" else \"Percentage\"),\n                    barmode = if (nlevels(summaryData$jaar) == 1) \"group\" else \"stack\",\n                    # hardcode graph size to prevent legend overlapping plot\n                    autosize = FALSE,\n                    width = 800, \n                    height = 600,\n                    margin = list(b = 120, t = 100),\n                    legend = list(y = 0.1),\n                    annotations = list(\n                            x = totalCount$jaar, \n                            y = if (summarizeBy == \"count\") totalCount$totaal else 100, \n                            text = paste(if (length(unique(totalCount$jaar)) == 1) \"totaal:\" else \"\", \n                                    totalCount$totaal),\n                            xanchor = 'center', yanchor = 'bottom',\n                            showarrow = FALSE),\n                    showlegend = TRUE\n            )\n    \n    \n    \n    colsFinal <- colnames(summaryData)[\n            !colnames(summaryData) %in% c(\"text\", \n                    if(summarizeBy == \"count\")\t\"percent\"\telse\tc(\"freq\", \"totaal\")\n            )\n    ]\n    \n\n    summaryDataFinal <- summaryData[, colsFinal]\n    \n    # Change variable name\n    if (\"freq\" %in% names(summaryDataFinal)) {\n      names(summaryDataFinal)[names(summaryDataFinal) == \"freq\"] <- \"aantal\"\n      \n    }\n    \n    # To prevent warnings in UI\n    toPlot$elementId <- NULL\n    \n    \n    return(list(plot = toPlot, data = summaryDataFinal))\n}" }
{ "repo_name": "jukiewiczm/renjin", "ref": "refs/heads/master", "path": "tests/src/test/R/test.log10.R", "content": "# Generated by gen-math-unary-tests.R using GNU R version 3.2.0 (2015-04-16)\nlibrary(hamcrest)\nlog10.foo <- function(...) 41\nMath.bar <- function(...) 44\ntest.log10.1 <- function() assertThat(log10(-0.01), identicalTo(NaN))\ntest.log10.2 <- function() assertThat(log10(-0.1), identicalTo(NaN))\ntest.log10.3 <- function() assertThat(log10(-1), identicalTo(NaN))\ntest.log10.4 <- function() assertThat(log10(-1.5), identicalTo(NaN))\ntest.log10.5 <- function() assertThat(log10(-2), identicalTo(NaN))\ntest.log10.6 <- function() assertThat(log10(-2.5), identicalTo(NaN))\ntest.log10.7 <- function() assertThat(log10(-4), identicalTo(NaN))\ntest.log10.8 <- function() assertThat(log10(-10), identicalTo(NaN))\ntest.log10.9 <- function() assertThat(log10(-100), identicalTo(NaN))\ntest.log10.10 <- function() assertThat(log10(-0.785398), identicalTo(NaN))\ntest.log10.11 <- function() assertThat(log10(-1.5708), identicalTo(NaN))\ntest.log10.12 <- function() assertThat(log10(-3.14159), identicalTo(NaN))\ntest.log10.13 <- function() assertThat(log10(-6.28319), identicalTo(NaN))\ntest.log10.14 <- function() assertThat(log10(0.01), identicalTo(-2))\ntest.log10.15 <- function() assertThat(log10(0.1), identicalTo(-1))\ntest.log10.16 <- function() assertThat(log10(1), identicalTo(0))\ntest.log10.17 <- function() assertThat(log10(1.5), identicalTo(0.176091259055681, tol = 0.000100))\ntest.log10.18 <- function() assertThat(log10(2), identicalTo(0.301029995663981, tol = 0.000100))\ntest.log10.19 <- function() assertThat(log10(2.5), identicalTo(0.397940008672038, tol = 0.000100))\ntest.log10.20 <- function() assertThat(log10(4), identicalTo(0.602059991327962, tol = 0.000100))\ntest.log10.21 <- function() assertThat(log10(10), identicalTo(1))\ntest.log10.22 <- function() assertThat(log10(100), identicalTo(2))\ntest.log10.23 <- function() assertThat(log10(0.785398), identicalTo(-0.104910208986239, tol = 0.000100))\ntest.log10.24 <- function() assertThat(log10(1.5708), identicalTo(0.196120892598381, tol = 0.000100))\ntest.log10.25 <- function() assertThat(log10(3.14159), identicalTo(0.497149505861123, tol = 0.000100))\ntest.log10.26 <- function() assertThat(log10(6.28319), identicalTo(0.798180192726274, tol = 0.000100))\ntest.log10.27 <- function() assertThat(log10(NULL), throwsError())\ntest.log10.28 <- function() assertThat(log10(c(0.01, 0.1, 1, 1.5)), identicalTo(c(-2, -1, 0, 0.176091259055681), tol = 0.000100))\ntest.log10.29 <- function() assertThat(log10(integer(0)), identicalTo(numeric(0)))\ntest.log10.30 <- function() assertThat(log10(numeric(0)), identicalTo(numeric(0)))\ntest.log10.31 <- function() assertThat(log10(NaN), identicalTo(NaN))\ntest.log10.32 <- function() assertThat(log10(NA_real_), identicalTo(NA_real_))\ntest.log10.33 <- function() assertThat(log10(Inf), identicalTo(Inf))\ntest.log10.34 <- function() assertThat(log10(-Inf), identicalTo(NaN))\ntest.log10.35 <- function() assertThat(log10(c(1L, 4L)), identicalTo(c(0, 0.602059991327962), tol = 0.000100))\ntest.log10.36 <- function() assertThat(log10(structure(1, class = \"foo\")), identicalTo(41))\ntest.log10.37 <- function() assertThat(log10(structure(1, class = \"bar\")), identicalTo(44))\ntest.log10.38 <- function() assertThat(log10(structure(list(\"a\"), class = \"foo\")), identicalTo(41))\ntest.log10.39 <- function() assertThat(log10(structure(list(\"b\"), class = \"bar\")), identicalTo(44))\ntest.log10.40 <- function() assertThat(log10(structure(c(1, 2, 3), .Names = c(\"a\", \"b\", \"c\"))), identicalTo(structure(c(0, 0.301029995663981, 0.477121254719662), .Names = c(\"a\", \"b\", \"c\")), tol = 0.000100))\ntest.log10.41 <- function() assertThat(log10(structure(c(1, 2), .Names = c(\"x\", \"\"))), identicalTo(structure(c(0, 0.301029995663981), .Names = c(\"x\", \"\")), tol = 0.000100))\ntest.log10.42 <- function() assertThat(log10(structure(1:12, .Dim = 3:4)), identicalTo(structure(c(0, 0.301029995663981, 0.477121254719662, 0.602059991327962, 0.698970004336019, 0.778151250383644, 0.845098040014257, 0.903089986991944, 0.954242509439325, 1, 1.04139268515822, 1.07918124604762), .Dim = 3:4), tol = 0.000100))\ntest.log10.43 <- function() assertThat(log10(structure(0, rando.attr = 4L)), identicalTo(structure(-Inf, rando.attr = 4L)))\ntest.log10.44 <- function() assertThat(log10(structure(0, class = \"zinga\")), identicalTo(structure(-Inf, class = \"zinga\")))\n" }
{ "repo_name": "HumanNeuroscienceLab/causality", "ref": "refs/heads/master", "path": "lib/lofs.R", "content": "# For each node, we look at all elements that are connected to that node. we ask for all combinations of those elements and regress them on our node, and ask what combination gives a residual with the most non-gaussian distribution\nlibrary(nortest)\nlibrary(plyr)\n\nNG <- function(x) as.numeric(ad.test(x)$statistic)\n\ncalc_residuals <- function(y, X) {\n  if (!is.matrix(y))\n    y <- as.matrix(y)\n  if (!is.matrix(X))\n    X <- as.matrix(X)\n  \n  # beta values\n  iXtX <- solve(t(X) %*% X)\n  b <- iXtX %*% t(X) %*% y\n  \n  # residuals\n  res <- y - X %*% b\n  \n  return(res)\n}\n\n#' Rule 1 for LOFS\n#'\n#' TODO\n#'\n#' @param S adjacency matrix of all connections between nodes (nnodes x nnodes)\n#' @param dat matrix of time-series (ntpts x nnodes)\n#' @return G matrix with connections in S oriented\n#'\n#' @references\n#' TODO\n#'\nlofs.r1 <- function(S, dat, .NG=NG) {  \n  nnodes  <- ncol(dat)\n  \n  # standardize\n  dat     <- scale(dat)\n  \n  # output matrix\n  G       <- matrix(0, nnodes, nnodes)\n  \n  # no self-nodes\n  diag(S) <- 0\n  \n  for (i in 1:nnodes) {\n    possible_parents <- which(S[,i]>0)\n    \n    # want to get every possible combination of parent nodes\n    combos <- llply(1:length(possible_parents), function(j) {\n      combn(possible_parents, j, simplify=F)\n    })\n    combos <- unlist(combos, recursive=F)\n    \n    # regress on node with possible parent combos\n    # and get the non-gaussianity of the residuals\n    ngs <- sapply(combos, function(inds) {\n      resids <- calc_residuals(dat[,i,drop=F], dat[,inds,drop=F])\n      .NG(resids)\n    })\n    \n    # get non-gaussianity without anything\n    self_ng <- .NG(dat[,i])\n    \n    # which node has the maximum non-gaussianity?\n    # and greater than empty set\n    max_ind <- which.max(ngs)\n    if (ngs[max_ind] > self_ng) {\n      G[combos[[max_ind]],i] <- S[combos[[max_ind]],i]\n    }\n  }\n  \n  return(G)\n}\n\n#' Rule 2 for LOFS\n#' \n#' Basically for this one, I want to take the combo approach from lofs1\n#' except that this time would want to do both directions X<-Y and Y->X\n#' save each of those scores.... calculation for the score shoud be looked up\nlofs.r2 <- function(S, dat, .NG=NG) {\n  nnodes  <- ncol(dat)\n  \n  # standardize\n  dat     <- scale(dat)\n  \n  # no self-nodes in input adjacency\n  diag(S) <- 0\n  \n  \n}\n\n#' Rule 3 for LOFS\nlofs.r3 <- function(S, dat, .NG=NG) {\n  nnodes  <- ncol(dat)\n  \n  # standardize\n  dat     <- scale(dat)\n  \n  # no self-nodes in input adjacency\n  diag(S) <- 0\n  \n  # output matrix\n  G       <- matrix(0, nnodes, nnodes)\n  \n  # get i,j coordinates for edges\n  inds    <- which(upper.tri(G) & S==1)\n  coords  <- expand.grid(list(i=1:nnodes, j=1:nnodes))\n  coords  <- coords[inds,]\n  \n  # Loop through each edge\n  for (ii in seq_along(inds)) {\n    i <- coords$i[ii]; j <- coords$j[ii]\n    X <- dat[,i]; Y <- dat[,j]\n    \n    # X <- Y\n    # if NG(X,Y) + NG(Y) > NG(Y,X) + NG(X)\n    # X -> Y\n    # if NG(X,Y) + NG(Y) < NG(Y,X) + NG(X)\n    toX <- .NG(calc_residuals(X,Y)) + .NG(Y)\n    toY <- .NG(calc_residuals(Y,X)) + .NG(X)\n    if (toX > toY) {\n      G[i,j] <- 1\n    } else {\n      G[j,i] <- 1\n    }\n  }\n  \n  return(G)\n}\n\n\n#' Rule 4 for LOFS\n#'\n#' TODO\n#'\n#' @param S adjacency matrix of all connections between nodes (nnodes x nnodes)\n#' @param dat matrix of time-series (ntpts x nnodes)\n#' @param epsilon threshold for weighted matrix (default=0.1)\n#' @param zeta range for free parameter, -zeta to zeta (default=1)\n#' @param .NG function that measures the non-gaussianity of the data (default=NG aka the anderson-darling test)\n#' @param add.self whether to have 1s in the diagonal of the weighted matrix (default=TRUE)\n#' @return list(W,G) W weighted and G unweighted matrix with connections in S oriented\n#'\n#' @references\n#' TODO\n#'\nlofs.r4 <- function(S, dat, epsilon=0.1, zeta=1, .NG=NG, add.self=TRUE) {\n  nnodes  <- ncol(dat)\n  \n  # standardize\n  dat     <- scale(dat)\n  \n  # get correlation for initializing parameter search\n  cordat  <- crossprod(dat)/(nrow(dat)-1)\n  \n  # no self-nodes in input adjacency\n  diag(S) <- 0\n  \n  # construct W - output matrix\n  W       <- matrix(0, nnodes, nnodes)\n  if (add.self) diag(W) <- 1\n  W[S!=0] <- NA\n  \n  # function to be applied to each row\n  # @param wi = free parameters\n  # @param X = time-series\n  # @param Wi = row of matrix W\n  fun <- function(wi, X, Wi) {\n    Wi[is.na(Wi)] <- wi\n    .NG(Wi %*% t(X))\n    sum(wi)\n  }\n  \n  # Loop through each row\n  for (i in 1:nnodes) {\n    inds  <- which(S[i,] > 0)\n    # determine weights for free parameters while maximizing NG\n    res   <- optim(cordat[i,inds], fun, \n                   X=dat, Wi=W[i,], \n                   method=\"L-BFGS-B\", control=list(fnscale=-1), \n                   lower=-zeta, upper=zeta)\n    # assign optimized weights\n    W[i,inds] <- res$par\n  }\n  \n  # Threshold W to get G\n  G <- (W > epsilon) * 1\n  \n  list(W=W, G=G)\n}\n\n" }
{ "repo_name": "SoftFx/FDK", "ref": "refs/heads/master", "path": "FDK2R/FdkRLib/examples/sample_json.R", "content": "\ngetTickData <- function(symbolName){\n  basicUrl = \"http://tp.dev.soft-fx.eu:5021/api/v1/public/tick/\"\n  fullUrl = paste(basicUrl, symbolName, sep = \"\")\n  fromJSON(fullUrl)\n}\nsymbol = \"EURUSD\"\ntickData <- getTickData(symbol)\nbestBid <- tickData$BestBid$Price\noldTimeStamp <-tickData$Timestamp\n\ntickData <- getTickData(symbol)\n\ntimeStamp <- tickData$Timestamp\nif(timeStamp!=oldTimeStamp)\n{\n  bestBid <- c(bestBid, tickData$BestBid$Price)\n  plot(bestBid)\n  oldTimeStamp = timeStamp\n}\n" }
{ "repo_name": "jovacich/lyrics-genre-classification", "ref": "refs/heads/master", "path": "considerations.R", "content": "#try to remove stopwords\nlyrics2 = lyrics\nlyrics2 = lyrics2[,-(stopWordsIndex+6)]\n\ntrainDat = lyrics2[train,]\n\n#randomforest.. results are very poor\nlyrics.forest = randomForest(trainDat[,7:5006],trainDat[,2],ntree = 1,mtry = 50)\npred = predict(lyrics.forest,lyrics2[-train,5:5006], ntrees=1)\nmse = rep(1,5958)\nmse[which(pred[1]==lyrics2[-train,2])]=0\nmean(mse)\nvarImpPlot(lyrics.forest)\nplot(lyrics.forest)\nplot(lyrics.forest, legend(\"topright\", legend=unique(lyrics2$Genre), col=unique(lyrics2$Genre), pch=19))\n\n#weight by tf-idf\nlyrics2 = lyrics\nwords = as.DocumentTermMatrix(lyrics2[,7:5006],weighting=weightTfIdf)\nwords = as.matrix(words)\nwords = as.data.frame(words)\nlyrics2[,7:5006] = words\nlyrics2[,5:5006] = scale(lyrics2[,5:5006])\n" }
{ "repo_name": "michaellindon/michaellindon.github.io", "ref": "refs/heads/master", "path": "lindonslog/example_code/multioptim.R", "content": "library(Rmpi)\nmpi.spawn.Rslaves(nslaves=6)\nmpi.remote.exec(rm(list=ls()))\n\nenergy<-function(theta){\n  E=3*sin(theta)+(0.1*theta-3)^2\n  return(E);\n}\n\nlogdensity<-function(theta){\n  #Distribution one wishes to sample from here.\n  #It may be more convinient to pass a theta as a list\n  rate=20;\n  ldensity=-rate*energy(theta);\n  return(ldensity)\n}\n\n\n\n\ntemper<-function(niter,Bmin,swap.interval){\n  rank=mpi.comm.rank();\n  size=mpi.comm.size();\n  swap=0;\n  swaps.attempted=0;\n  swaps.accepted=0;\n  \n  #Higher ranks run the higher \"temperatures\" (~smaller fractional powers)\n  B=rep(0,size-1);\n  for(r in 1:size-1){\n    temp=(r-1)/(size-2);\n    B[r]=Bmin^temp;\n  }\n  \n  \n  #Create a list for proposal moves\n  theta=rep(0,niter)\n  \n  for(t in 2:niter){\n    \n    prop=theta[t-1]+rnorm(1,0,1);\n    \n    #Calculate Log-Density at proposed and current position\n    logdensity.current=logdensity(theta[t-1])\n    logdensity.prop=logdensity(prop);\n    \n    #Calculate log acceptance probability\n    lalpha=B[rank]*(logdensity.prop-logdensity.current)\n    \n    if(log(runif(1))<lalpha){\n      #Accept proposed move\n      theta[t]=prop;\n      logdensity.current=logdensity.prop;\n    }else{\n      #Otherwise do not move\n      theta[t]=theta[t-1];\n    } \n    \n    if(t%%swap.interval ==0){\n      for(evenodd in 0:1){\n        swap=0;\n        logdensity.partner=0;\n        #if rank even\n        if(rank%%2 == evenodd%%2){\n          rank.partner=rank + 1;\n          #ranks range from 1:size-1. Cannot have a partner rank == size\n          if(0<rank.partner && rank.partner<size){\n            #On first iteration, evens receive from above odd\n            #On second iteration, odds receive from above evens\n            logdensity.partner<-mpi.recv.Robj(rank.partner,rank.partner);\n            lalpha = (B[rank]-B[rank.partner])*(logdensity.partner-logdensity.current);\n            swaps.attempted=swaps.attempted+1;\n            if(log(runif(1))<lalpha){\n              swap=1;\n              swaps.accepted=swaps.accepted+1;\n            }\n            mpi.send.Robj(swap,dest=rank.partner,tag=rank)\n          }\n          if(swap==1){\n            thetaswap=theta[t];\n            mpi.send.Robj(thetaswap,dest=rank.partner,tag=rank)\n            theta[t]=mpi.recv.Robj(rank.partner,rank.partner)\n          }\n        }else{\n          rank.partner=rank-1;\n          #ranks range from 1:size-1. Cannot have a partner rank ==0\n          if(0<rank.partner && rank.partner<size){\n            #On first iteration, odds send to evens below\n            #On second iteration, evens sent to odds below\n            mpi.send.Robj(logdensity.current,dest=rank.partner,tag=rank);\n            swap=mpi.recv.Robj(rank.partner,rank.partner);\n          }\n          if(swap==1){\n            thetaswap=theta[t];\n            theta[t]=mpi.recv.Robj(rank.partner,rank.partner);\n            mpi.send.Robj(thetaswap,dest=rank.partner,tag=rank);\n          }\n        }\n      }\n    }\n  }\n  return(theta)\n}\n\n\n\n#Send to slaves any initial variables that they require\nniter=4000\nBmin=0.005\nswap.interval=100\nmpi.bcast.Robj2slave(niter)\nmpi.bcast.Robj2slave(Bmin)\nmpi.bcast.Robj2slave(swap.interval)\nmpi.bcast.Robj2slave(energy)\nmpi.bcast.Robj2slave(logdensity)\n#Send to slaves and functions that they require\nmpi.bcast.Robj2slave(temper)\n#Check to see that the slaves have the correct function\nmpi.remote.exec(temper)\n#Run the tempering algorithm\nmcmc=mpi.remote.exec(temper(niter,Bmin,swap.interval))\n\npar(mfrow=c(3,2))\nplot(mcmc[[1]],xlab=\"Iteration\",ylab=~theta,type=\"l\")\nplot(mcmc[[2]],xlab=\"Iteration\",ylab=~theta,type=\"l\")\nplot(mcmc[[3]],xlab=\"Iteration\",ylab=~theta,type=\"l\")\nplot(mcmc[[4]],xlab=\"Iteration\",ylab=~theta,type=\"l\")\nplot(mcmc[[5]],xlab=\"Iteration\",ylab=~theta,type=\"l\")\nplot(mcmc[[6]],xlab=\"Iteration\",ylab=~theta,type=\"l\")\n\nmpi.close.Rslaves()\nmpi.quit()\n\n" }
{ "repo_name": "burrm/lolcat", "ref": "refs/heads/master", "path": "R/z.test.onesample.simple.R", "content": "z.test.onesample.simple<-function(sample.mean\n                                  ,known.population.variance = 1\n                                  ,sample.size\n                                  ,null.hypothesis.mean = 0\n                                  ,alternative = c(\"two.sided\",\"less\",\"greater\")\n                                  ,conf.level = 0.95\n                                  ,finite.population.N = NA\n) {\n  validate.htest.alternative(alternative = alternative)\n  se.est  <- sqrt(known.population.variance/sample.size)\n  \n  if (!is.na(finite.population.N)) {\n    se.est <- se.est * sqrt((finite.population.N-sample.size)/(finite.population.N-1))\n  }\n  \n  z       <- (sample.mean-null.hypothesis.mean)/se.est\n  cv      <- qnorm(conf.level+(1-conf.level)/2)\n  \n  z.upper <- sample.mean + cv*se.est\n  z.lower <- sample.mean - cv*se.est\n  \n  p.value <- if (alternative[1] == \"two.sided\") {\n    tmp<-pnorm(z)\n    min(tmp,1-tmp)*2\n  } else if (alternative[1] == \"greater\") {\n    pnorm(z,lower.tail = FALSE)\n  } else if (alternative[1] == \"less\") {\n    pnorm(z,lower.tail = TRUE)\n  } else {\n    NA\n  }\n  \n  pow <- power.mean.z.onesample(sample.size = sample.size\n                               ,effect.size = sample.mean-null.hypothesis.mean\n                               ,variance = known.population.variance\n                               ,alpha = 1-conf.level\n                               ,alternative = alternative\n                               ,details = F)\n  \n  retval<-list(data.name   = \"sample mean, population variance, sample size\",\n               statistic   = z, \n               estimate    = c(sample.mean = sample.mean \n                               ,sample.size = sample.size\n                               ,se.est = se.est\n                               ,power = pow\n                               ),\n               parameter   = null.hypothesis.mean,\n               p.value     = p.value,\n               null.value  = null.hypothesis.mean,\n               alternative = alternative[1],\n               method      = \"One-Sample Z Test For Means\",\n               conf.int    = c(z.lower,z.upper)\n  )\n  \n  #names(retval$estimate) <- c(\"sample mean\")\n  names(retval$statistic) <- \"z statistic\"\n  names(retval$null.value) <- \"mean\"\n  names(retval$parameter) <- \"null hypothesis mean\"\n  attr(retval$conf.int, \"conf.level\")  <- conf.level\n  \n  class(retval)<-\"htest\"\n  retval\n  \n}\n\n\n#z.test.onesample.simple(sample.mean = .5, known.population.variance = 1, sample.size = 4,null.hypothesis.mean = 0, alternative = \"less\")\n" }
{ "repo_name": "PF-BB/transcriptome", "ref": "refs/heads/master", "path": "SIMA/R/03_VolcanoPlot.R", "content": "#' Volcano plot performed after a differential analysis. It represents the p-value obtained after a two sample (limma) test as a function of the Fold Change.\n#' @param fit an an ‘MArrayLM’ fitted linear model object.\n#' @param coef, an Integer value corresponding to the desired contrast. Default to NULL.\n#' @param FC, numeric, Fold Change threshold.\n#' @param PV, numeric, p-value threshold.\n#' @param p.val, character string, compute raw p-value (\"raw\") or adjusted p-value (adj\"). Default to \"adj\". The p-values are adjusted with the procedure defined by Benjamini and Hochberg.\n#' @param name, character string, title for the graphic. Default is an empty title (\"\").\n#' @title Volcano plot\n#' @export VolcanoPlot\n\nVolcanoPlot <- function(fit, coef=NULL, FC=2, PV=0.05, p.val = c(\"adj\",\"raw\"), name = \"\"){  # , plotInFile=TRUE\n\t\n\ttitre = NULL\n\tpval  = NULL\n\tylab = \"\"\n\tp.val <- match.arg(p.val)\n  \n\tif (p.val == \"adj\"){\n\t  topT = topTable(fit, coef=coef, number=Inf)\n\t\tif (is.element(\"adj.P.Val\", colnames(topT))) {\n\t\t\tpval  = -log10(topT$adj.P.Val)\n\t\t\tif (name == \"\"){\n\t\t\t\ttitre = paste(\"Volcanoplot\",\"(FDR)\", sep=\" - \")\n\t\t\t\t\n\t\t\t}\n\t\t\telse {\n\t\t\t\ttitre = paste(\"Volcanoplot\",name,\"(FDR)\", sep=\" - \")\n\t\t\t}\n\t\t\tylab  = \"-log10(Adj.P.Val)\"\n\t\t}\n\t\telse{\n\t\t\twarning(\"The topTable object does not contain column 'adj.P.Val'\")\n\t\t\treturn()\n\t\t}\n\t}\n\telse if (p.val == \"raw\"){\n\t  topT = topTable(fit, coef=coef, adjust.method=\"none\",number=Inf)\n\t\tif (is.element(\"P.Value\", colnames(topT))) {\n\t\t\tpval = -log10(topT$P.Value)\n\t\t\tif (name == \"\"){\n\t\t\t\ttitre = paste(\"Volcanoplot\",\"(noFDR)\", sep=\" - \")\n\t\t\t}\n\t\t\telse {\n\t\t\t\ttitre = paste(\"Volcanoplot\",name,\"(noFDR)\", sep=\" - \")\n\t\t\t}\n\t\t\tylab  = \"-log10(P.Value)\"\t\t\n\t\t}\n\t\telse{\n\t\t\twarning(\"The topTable object does not contain column 'P.Value'\")\n\t\t\treturn()\n\t\t}\n\t}\n\telse{\n\t\twarning(\"The option 'p.val' should be either 'adj' or 'raw'\")\n\t\treturn()\n\t}\t\n\t\n\t\n\tgauche = intersect( which(pval >= -log10(PV))  , which(topT$logFC <= -log2(FC)))\n\tdroite = intersect( which(pval >= -log10(PV))  , which(topT$logFC >= log2(FC)))\n\treste  = union( which(pval< -log10(PV)), which(abs(topT$logFC)< log2(FC) ))\n\t\n\t# --- definition de l'echelle des abscisses\n\tmin.log = min(topT$logFC,na.rm=T)\n\tmax.log = max(topT$logFC,na.rm=T)\n\tmax.xax = floor(max(abs(min.log), abs(max.log)))+1\n\txaxis = c(-max.xax, max.xax)\n\t\n\t# --- definition de l'echelle des ordonnÈe\n\tmax.pv = max(pval, na.rm=TRUE)\n\tmax.yax = 10*(max.pv%/%10) + 10\n\tyaxis = c(0,max.yax)\n\t\n\t# --- Plot\n\tlayout( matrix(c(1,1,2,3), ncol=2,nrow=2, byrow=T), heights=c(0.9,0.1) )\n\t#layout.show(3)\n\t\n\tpar(mar = c(5,5,5,5))\n\txlab = \"log2(FC)\"\n\tplot  (topT$logFC[reste] , pval[reste] , col=\"grey\" , pch=16, cex=0.7,main=titre, xlab=xlab,ylab=ylab,xlim=xaxis, ylim=yaxis)\n\tpoints(topT$logFC[gauche], pval[gauche], col=\"chartreuse4\", pch=16, cex=1)\n\tpoints(topT$logFC[droite], pval[droite], col=\"red\", pch=16, cex=1)\n\t\n\tabline(v=c(-log2(FC), 0, log2(FC)), col=c(\"blue\",\"black\",\"blue\"), lty=2)\n\tabline(h= -log10(PV), col=\"blue\", lty=2)\n\t\n\t\n\tlabs1 = c(\"Threshold:\",paste(\" - FC:\", FC, sep=\" \"), paste(\" - PV:\", PV))\n\tpar(mar = c(0.1,0.1,0.1, 1.1))\n\tplot( c(0,1), c(0,1), type=\"n\", ylab=\"\", xlab = \"\", axes = F)\n\ttext( x = c(0,0,0), y=c(0.8,0.5,0.1), labels = labs1 , adj = c(0,0),font=2)\n\t\n\tlabs2 = c(paste(\"Underexpressed:\", length(gauche), sep=\" \"),\n\t\t\tpaste(\"Overexpressed :\" , length(droite), sep=\" \"))\n\tpar(mar = c(0.1,0.1,0.1, 1.1))\n\tplot( c(0,1), c(0,1), type=\"n\", ylab=\"\", xlab = \"\", axes = F)\n\ttext( x = c(0,0), y=c(0.8,0.5), labels = labs2 , adj = c(0,0),col=c(\"chartreuse4\",\"red\"),font=2)\n}\n\n\n\n" }
{ "repo_name": "tom-jin/LazyABC", "ref": "refs/heads/master", "path": "R/RejectionABC.R", "content": "#' Rejection Approximate Bayesian Computation\n#'\n#' Perform rejection ABC sampling on user specified distributions.\n#'\n#' This is a generic function: methods can be defined for it directly\n#' or via the \\code{\\link{Summary}} group generic. For this to work properly,\n#' the arguments \\code{...} should be unnamed, and dispatch is on the\n#' first argument.\n#'\n#' @param data an observation\n#' @param prior a function taking theta and returning its density from the prior\n#' @param simulator a function taking theta performing simulation\n#' @param tolerance the epsilon tolerance to accept simulations\n#' @param n number of iterations to run\n#' @return n weighted samples of the parameter from the posterior distribution.\n#' @examples\n#' posterior <- \n#' RejectionABC(data = 2, prior = function(){runif(1, -10, 10)}, \n#'              simulator = function(theta){rnorm(1, 2*(theta+2)*theta*(theta-2), 0.1+theta^2)}, \n#'              n = 3000, tolerance = 1)\n\nRejectionABC <- function(data, prior, simulator, tolerance = 0.1, n = 10000) {\n  posterior <- rep(NA, n)\n  \n  for(i in 1:n) {\n    theta <- NA\n    repeat {\n      theta <- prior()\n      X <- simulator(theta)\n      if(abs(X - data) < tolerance)\n        break\n    }\n    posterior[i] <- theta\n  }\n  return(posterior)\n}" }
{ "repo_name": "personality-project/psych", "ref": "refs/heads/master", "path": "R/test.all.R", "content": "\ntest.all <- function(pl,package=\"psych\",dependencies = c(\"Depends\", \"Imports\", \"LinkingTo\"),find=FALSE,skip=NULL) {\n if (find) {\n     pl <-tools::dependsOnPkgs(package,dependencies=dependencies) \n     if(!is.null(skip) && skip %in% pl) {pl <- pl[-which(skip ==pl)]}\n     }\n np <- length(pl)\n if(np > 0 ) {\n for(i in 1:np) {\n   p <- pl[i]\n test <- require(p,character.only=TRUE)\n if(!test) {cat(\"\\nCould not find package \",p, \"\\n\")\n      next  \n      }\n cat(paste(\"\\nNow testing package \" ,p ))\n  ob <- paste(\"package\",p,sep=\":\")\n  ol <- objects(ob)\n  nf <- length(ol)\n  options(\"example.ask\"=FALSE)\n  for(i in 1:nf) {\n    fn <- as.character(ol[[i]])\n    example(topic=fn,package=p,character.only=TRUE)\n    }\n detach(ob,character.only=TRUE)\n} } else {cat(\"\\nNo dependencies for package \", package) }\n}\n\n\n#tools::package_dependencies(reverse = TRUE)   #lists all the reverse dependencies\n#tools::check_packages_in_dir(dir,reverse = list())   #might check them,  unclear\n#rd <-reverse_dependencies_with_maintainers(\"psych\")" }
{ "repo_name": "swager/grf", "ref": "refs/heads/master", "path": "experiments/quantile_examples/quantile_simu_2.R", "content": "set.seed(1234)\n\nrm(list = ls())\n\nsetwd(\"~/git/grf/experiments/quantile_examples\")\n\nlibrary(quantregForest)\nlibrary(grf)\n\np = 40\nn = 2000\nJMP = 0.8\n\nticks = 1001\nX.test = matrix(0, ticks, p)\nX.test[,1] = seq(-1, 1, length.out = ticks)\nX.test.df = data.frame(X=X.test)\n\nX = matrix(2 * runif(n * p) - 1, n, p)\nY = rnorm(n) + JMP * (X[,1] > 0)\nD = data.frame(X=X, Y=Y)\n\nqrf.meinshausen = quantregForest(X, Y, mtry=p, nodesize=10, replace=FALSE, sampsize=ceiling(0.25*n))\npreds.meinshausen = predict(qrf.meinshausen, X.test, quantiles = c(0.1, 0.5, 0.9))\n\nqrf.grad = quantile_forest(X, Y, quantiles = c(0.1, 0.5, 0.9), mtry=p, min.node.size = 10, sample.fraction=0.5, num.trees=500)\npreds.grad = predict(qrf.grad, X.test.df, quantiles = c(0.1, 0.5, 0.9))\n\npreds.truth = cbind(-qnorm(0.9) + JMP * (X.test[,1] > 0),\n                    JMP * (X.test[,1] > 0),\n                    qnorm(0.9) + JMP * (X.test[,1] > 0)) \n\npdf(\"quantile_plot_shift_n2k_p40.pdf\")\n\npardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab = 1.5, cex.axis = 1.5, cex.sub = 1.5)\nplot(NA, NA, xlim = c(-1, 1), ylim = range(preds.meinshausen, preds.grad, preds.truth), xlab = \"X\", ylab = \"Y\")\n\nlines(X.test[,1], preds.meinshausen[,1], col = 4, lwd = 2, lty = 2)\nlines(X.test[,1], preds.meinshausen[,2], col = 4, lwd = 2, lty = 1)\nlines(X.test[,1], preds.meinshausen[,3], col = 4, lwd = 2, lty = 2)\n\nlines(X.test[,1], preds.grad[,1], col = 2, lwd = 2, lty = 2)\nlines(X.test[,1], preds.grad[,2], col = 2, lwd = 2, lty = 1)\nlines(X.test[,1], preds.grad[,3], col = 2, lwd = 2, lty = 2)\n\nlines(X.test[,1], preds.truth[,1], col = 1, lwd = 2, lty = 2)\nlines(X.test[,1], preds.truth[,2], col = 1, lwd = 2, lty = 1)\nlines(X.test[,1], preds.truth[,3], col = 1, lwd = 2, lty = 2)\n\nlegend(\"topleft\", c(\"truth\", \"quantregForest\", \"GRF\"), lwd = 2, col = c(1, 4, 2), cex=1.5)\n\npar=pardef\ndev.off()\n" }
{ "repo_name": "swager/gradient-forest", "ref": "refs/heads/master", "path": "experiments/aos19/quantile_examples/quantile_simu_2.R", "content": "set.seed(1234)\n\nrm(list = ls())\n\nsetwd(\"~/git/grf/experiments/quantile_examples\")\n\nlibrary(quantregForest)\nlibrary(grf)\n\np = 40\nn = 2000\nJMP = 0.8\n\nticks = 1001\nX.test = matrix(0, ticks, p)\nX.test[,1] = seq(-1, 1, length.out = ticks)\nX.test.df = data.frame(X=X.test)\n\nX = matrix(2 * runif(n * p) - 1, n, p)\nY = rnorm(n) + JMP * (X[,1] > 0)\nD = data.frame(X=X, Y=Y)\n\nqrf.meinshausen = quantregForest(X, Y, mtry=p, nodesize=10, replace=FALSE, sampsize=ceiling(0.25*n))\npreds.meinshausen = predict(qrf.meinshausen, X.test, quantiles = c(0.1, 0.5, 0.9))\n\nqrf.grad = quantile_forest(X, Y, quantiles = c(0.1, 0.5, 0.9), mtry=p, min.node.size = 10, sample.fraction=0.5, num.trees=500)\npreds.grad = predict(qrf.grad, X.test.df, quantiles = c(0.1, 0.5, 0.9))\n\npreds.truth = cbind(-qnorm(0.9) + JMP * (X.test[,1] > 0),\n                    JMP * (X.test[,1] > 0),\n                    qnorm(0.9) + JMP * (X.test[,1] > 0)) \n\npdf(\"quantile_plot_shift_n2k_p40.pdf\")\n\npardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab = 1.5, cex.axis = 1.5, cex.sub = 1.5)\nplot(NA, NA, xlim = c(-1, 1), ylim = range(preds.meinshausen, preds.grad, preds.truth), xlab = \"X\", ylab = \"Y\")\n\nlines(X.test[,1], preds.meinshausen[,1], col = 4, lwd = 2, lty = 2)\nlines(X.test[,1], preds.meinshausen[,2], col = 4, lwd = 2, lty = 1)\nlines(X.test[,1], preds.meinshausen[,3], col = 4, lwd = 2, lty = 2)\n\nlines(X.test[,1], preds.grad[,1], col = 2, lwd = 2, lty = 2)\nlines(X.test[,1], preds.grad[,2], col = 2, lwd = 2, lty = 1)\nlines(X.test[,1], preds.grad[,3], col = 2, lwd = 2, lty = 2)\n\nlines(X.test[,1], preds.truth[,1], col = 1, lwd = 2, lty = 2)\nlines(X.test[,1], preds.truth[,2], col = 1, lwd = 2, lty = 1)\nlines(X.test[,1], preds.truth[,3], col = 1, lwd = 2, lty = 2)\n\nlegend(\"topleft\", c(\"truth\", \"quantregForest\", \"GRF\"), lwd = 2, col = c(1, 4, 2), cex=1.5)\n\npar=pardef\ndev.off()\n" }
{ "repo_name": "steinbaugh/basejump", "ref": "refs/heads/master", "path": "R/matchEnsemblReleaseToURL.R", "content": "#' Match Ensembl release to archive URL.\n#'\n#' @note Updated 2020-03-16.\n#' @export\n#'\n#' @param release `integer(1)` or `character(1)`.\n#'   Ensembl release (e.g. 99).\n#'\n#' @return `character(1)`.\n#'   URL.\n#'\n#' @seealso\n#' - `biomaRt::listEnsemblArchives()`.\n#'\n#' @examples\n#' matchEnsemblReleaseToURL(96L)\nmatchEnsemblReleaseToURL <- function(release) {\n    currentURL <- \"http://useast.ensembl.org\"\n    if (is.null(release)) {\n        return(currentURL)\n    }\n    release <- as.character(release)\n    assert(isString(release))\n    map <- tryCatch(\n        expr = listEnsemblArchives(),\n        error = function(e) {\n            stop(\"'biomaRt::listEnsemblArchives()' error: \", e)\n        }\n    )\n    assert(\n        is.data.frame(map),\n        isSubset(c(\"url\", \"version\"), colnames(map))\n    )\n    if (!release %in% map[[\"version\"]]) {\n        stop(sprintf(\n            \"Supported Ensembl releases: %s.\",\n            toString(map[[\"version\"]])\n        ))\n    }\n    ## Extract the matching row, so we can check if releast is current.\n    which <- match(x = release, table = map[[\"version\"]])\n    x <- map[which, , drop = FALSE]\n    isCurrent <- identical(x[1L, \"current_release\"], \"*\")\n    if (isTRUE(isCurrent)) {\n        return(currentURL)\n    }\n    url <- x[1L, \"url\"]\n    assert(isTRUE(grepl(\"ensembl\\\\.org\", url)))\n    url\n}\n" }
{ "repo_name": "seqcloud/seqcloudR", "ref": "refs/heads/master", "path": "R/matchEnsemblReleaseToURL.R", "content": "#' Match Ensembl release to archive URL.\n#'\n#' @note Updated 2020-03-16.\n#' @export\n#'\n#' @param release `integer(1)` or `character(1)`.\n#'   Ensembl release (e.g. 99).\n#'\n#' @return `character(1)`.\n#'   URL.\n#'\n#' @seealso\n#' - `biomaRt::listEnsemblArchives()`.\n#'\n#' @examples\n#' matchEnsemblReleaseToURL(96L)\nmatchEnsemblReleaseToURL <- function(release) {\n    currentURL <- \"http://useast.ensembl.org\"\n    if (is.null(release)) {\n        return(currentURL)\n    }\n    release <- as.character(release)\n    assert(isString(release))\n    map <- tryCatch(\n        expr = listEnsemblArchives(),\n        error = function(e) {\n            stop(\"'biomaRt::listEnsemblArchives()' error: \", e)\n        }\n    )\n    assert(\n        is.data.frame(map),\n        isSubset(c(\"url\", \"version\"), colnames(map))\n    )\n    if (!release %in% map[[\"version\"]]) {\n        stop(sprintf(\n            \"Supported Ensembl releases: %s.\",\n            toString(map[[\"version\"]])\n        ))\n    }\n    ## Extract the matching row, so we can check if releast is current.\n    which <- match(x = release, table = map[[\"version\"]])\n    x <- map[which, , drop = FALSE]\n    isCurrent <- identical(x[1L, \"current_release\"], \"*\")\n    if (isTRUE(isCurrent)) {\n        return(currentURL)\n    }\n    url <- x[1L, \"url\"]\n    assert(isTRUE(grepl(\"ensembl\\\\.org\", url)))\n    url\n}\n" }
{ "repo_name": "WLOGSolutions/telco-customer-churn-in-r-and-h2o", "ref": "refs/heads/master", "path": "telco-customer-churn-in-r-and-h2o/deployment/03_zip_project.R", "content": "#----------------------------------------------------------------------------\n# RSuite\n# Copyright (c) 2017, WLOG Solutions\n#\n# This script prepares deployment zip tagged with version (from SVN or enforced).\n#\n# This script was generated authomaticaly by RSuite. Do not change it please.\n#----------------------------------------------------------------------------\n\nargs <- commandArgs()\n\nif (any(grepl(\"--usage\", args))) {\n  cat(\"This script collects everything installed in project lib folder together with project master scripts and\\n\")\n  cat(\"    specified in PROPERTIES artifacts, adds configuration template (config_templ.txt) and packs everything\\n\")\n  cat(\"    into zip package stamped with a version. It can be therefore deployed on target environment as contains\\n\")\n  cat(\"    everything to run project functionalities.\\n\")\n  cat(\"Call: Rscript 03_zip_project.R <args>\\n\")\n  cat(\"\\t-v<ver>            specifies zip version to be used to stamp zip package.\\n\")\n  cat(\"\\t--verbose          if passed print lots of messages\\n\")\n  cat(\"\\t--usage            print this message and exit\\n\")\n  stop(\"Noithing else to be done\")\n}\n\nif (any(grepl(\"--verbose\", args))) {\n  logging::setLevel(\"DEBUG\")\n}\n\n# Getting version\nverArgs <- grepl(\"^-v([0-9]+)([\\\\._-][0-9]+)*$\", args)\nif (any(verArgs)) {\n  zip_ver <- sub(\"^-v\", \"\", args[verArgs][[1]])\n} else {\n  zip_ver <- NULL\n}\nRSuite::prj_zip(zip_ver = zip_ver)\n" }
{ "repo_name": "WLOGSolutions/microsoft_cntk2.0_from_r", "ref": "refs/heads/master", "path": "deployment/03_zip_project.R", "content": "#----------------------------------------------------------------------------\n# RSuite\n# Copyright (c) 2017, WLOG Solutions\n#\n# This script prepares deployment zip tagged with version (from SVN or enforced).\n#\n# This script was generated authomaticaly by RSuite. Do not change it please.\n#----------------------------------------------------------------------------\n\nargs <- commandArgs()\n\nif (any(grepl(\"--usage\", args))) {\n  cat(\"This script collects everything installed in project lib folder together with project master scripts and\\n\")\n  cat(\"    specified in PROPERTIES artifacts, adds configuration template (config_templ.txt) and packs everything\\n\")\n  cat(\"    into zip package stamped with a version. It can be therefore deployed on target environment as contains\\n\")\n  cat(\"    everything to run project functionalities.\\n\")\n  cat(\"Call: Rscript 03_zip_project.R <args>\\n\")\n  cat(\"\\t-v<ver>            specifies zip version to be used to stamp zip package.\\n\")\n  cat(\"\\t--verbose          if passed print lots of messages\\n\")\n  cat(\"\\t--usage            print this message and exit\\n\")\n  stop(\"Noithing else to be done\")\n}\n\nif (any(grepl(\"--verbose\", args))) {\n  logging::setLevel(\"DEBUG\")\n}\n\n# Getting version\nverArgs <- grepl(\"^-v([0-9]+)([\\\\._-][0-9]+)*$\", args)\nif (any(verArgs)) {\n  zip_ver <- sub(\"^-v\", \"\", args[verArgs][[1]])\n} else {\n  zip_ver <- NULL\n}\nRSuite::prj_zip(zip_ver = zip_ver)\n" }
{ "repo_name": "pchmieli/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_jira/runit_pub_487_binop_frame_or_row.R", "content": "\n\n\n\n\n# use this for interactive setup\n#      library(h2o)\n#      library(testthat)\n#      h2o.startLogging()\n#      conn = h2o.init()\n\n\ntest.frame_add <- function() {\n\n    a_initial = cbind(\n    c(0,0,1,0,0,1,0,0,0,0),\n    c(1,1,1,0,1,0,1,0,1,0),\n    c(1,0,1,0,1,0,1,0,0,1),\n    c(1,1,0,0,0,1,0,0,0,1),\n    c(1,1,1,0,1,0,0,0,1,1),\n    c(1,0,1,0,0,0,0,0,1,1),\n    c(1,1,1,0,0,0,1,1,1,0),\n    c(0,0,1,1,1,0,0,1,1,0),\n    c(0,1,1,1,1,0,0,1,1,0),\n    c(0,0,0,0,0,1,1,0,0,0)\n    )\n    \n    a.h2o <- as.h2o(a_initial, destination_frame=\"cA_0\")\n    b.h2o <- as.h2o(a_initial, destination_frame=\"cA_1\")\n\n    Log.info(\"Try a.h2o[1,] + b.h2o[1,]\")\n    res <- a.h2o[1,] + b.h2o[1,]\n\n    Log.info(\"Try a.h2o[,1] + b.h2o[,1]\")\n    res2 <- a.h2o[,1] + b.h2o[,1]\n  \n    Log.info(\"Try a.h2o + b.h2o\")\n    res3 <- a.h2o + b.h2o\n\n    Log.info(\"Try a.h2o == b.h2o\")\n    res4 <- a.h2o == b.h2o\n      \n  \n    \n}\n\ndoTest(\"Test frame add.\", test.frame_add)\n\n\n" }
{ "repo_name": "madmax983/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_jira/runit_pub_487_binop_frame_or_row.R", "content": "\n\n\n\n\n# use this for interactive setup\n#      library(h2o)\n#      library(testthat)\n#      h2o.startLogging()\n#      conn = h2o.init()\n\n\ntest.frame_add <- function() {\n\n    a_initial = cbind(\n    c(0,0,1,0,0,1,0,0,0,0),\n    c(1,1,1,0,1,0,1,0,1,0),\n    c(1,0,1,0,1,0,1,0,0,1),\n    c(1,1,0,0,0,1,0,0,0,1),\n    c(1,1,1,0,1,0,0,0,1,1),\n    c(1,0,1,0,0,0,0,0,1,1),\n    c(1,1,1,0,0,0,1,1,1,0),\n    c(0,0,1,1,1,0,0,1,1,0),\n    c(0,1,1,1,1,0,0,1,1,0),\n    c(0,0,0,0,0,1,1,0,0,0)\n    )\n    \n    a.h2o <- as.h2o(a_initial, destination_frame=\"cA_0\")\n    b.h2o <- as.h2o(a_initial, destination_frame=\"cA_1\")\n\n    Log.info(\"Try a.h2o[1,] + b.h2o[1,]\")\n    res <- a.h2o[1,] + b.h2o[1,]\n\n    Log.info(\"Try a.h2o[,1] + b.h2o[,1]\")\n    res2 <- a.h2o[,1] + b.h2o[,1]\n  \n    Log.info(\"Try a.h2o + b.h2o\")\n    res3 <- a.h2o + b.h2o\n\n    Log.info(\"Try a.h2o == b.h2o\")\n    res4 <- a.h2o == b.h2o\n      \n  \n    \n}\n\ndoTest(\"Test frame add.\", test.frame_add)\n\n\n" }
{ "repo_name": "tactical-Data/CandidateSpeechHeatMap", "ref": "refs/heads/master", "path": "candidate_text_tc.R", "content": "\n\n\ncandidate_text_tc <- function(candidate, mydata, word.filter = \"\") {\n    ## \n    ## PUT CANDIDATE TEXT INTO A TEXT CORPUS\n    ## \n    ## INPUTS:\n    ##      mydata: a candidate data frame with candidate name and unprocessed text\n    ##          n        name     text\n    ##      50 50       RUBIO     Our greatest days lie ahead ....\n    ##      51 51 QUINTANILLA     Mr. Trump?\n    ##      52 52       TRUMP     I think maybe my greatest weakness ... [laughter]\n    ##\n    ##      candidate: a name (e.g. \"trump\")\n    ##    \n    ##      word.filter - a word that can be selected to filter text for content\n    ## OUTPUTS:\n    ##      t_c: A Text Corpus of candidate speech\n    \n    ## filter for candidate and specific words\n    \n    ## error handling in case candidate invalid\n    if (candidate %in% unique(mydata$name)) {\n        candidate <- candidate\n    } else {\n        candidate <- \"\"\n    }\n    \n    ## select text\n    text<-mydata$text[mydata$name==candidate & grepl(word.filter, mydata$text)]\n    text<-paste(text, collapse = \" \")\n    \n    t_c <- Corpus(VectorSource(text))\n    t_c <- tm_map(t_c, content_transformer(tolower))\n    t_c <- tm_map(t_c, removePunctuation)\n    t_c <- tm_map(t_c, removeNumbers)\n    t_c <- tm_map(t_c, removeWords, stopwords(\"english\"))\n    \n    ## filter for some specific words \n    t_c <- tm_map(t_c, removeWords, c(\"applause\", \"thats\", \"laughter\", \"dont\", \"back\", \"can\", \"get\", \"cant\", \"come\", \"big\", \"inaudible\", \"dont\", \"back\", \"can\", \"get\"))\n    \n    return(t_c)   \n    \n} \n" }
{ "repo_name": "ww44ss/presidential_debates_clinton_saunders", "ref": "refs/heads/master", "path": "candidate_text_tc.R", "content": "\n\n\ncandidate_text_tc <- function(candidate, mydata, word.filter = \"\") {\n    ## \n    ## PUT CANDIDATE TEXT INTO A TEXT CORPUS\n    ## \n    ## INPUTS:\n    ##      mydata: a candidate data frame with candidate name and unprocessed text\n    ##          n        name     text\n    ##      50 50       RUBIO     Our greatest days lie ahead ....\n    ##      51 51 QUINTANILLA     Mr. Trump?\n    ##      52 52       TRUMP     I think maybe my greatest weakness ... [laughter]\n    ##\n    ##      candidate: a name (e.g. \"trump\")\n    ##    \n    ##      word.filter - a word that can be selected to filter text for content\n    ## OUTPUTS:\n    ##      t_c: A Text Corpus of candidate speech\n    \n    ## filter for candidate and specific words\n    \n    ## error handling in case candidate invalid\n    if (candidate %in% unique(mydata$name)) {\n        candidate <- candidate\n    } else {\n        candidate <- \"\"\n    }\n    \n    ## select text\n    text<-mydata$text[mydata$name==candidate & grepl(word.filter, mydata$text)]\n    text<-paste(text, collapse = \" \")\n    \n    t_c <- Corpus(VectorSource(text))\n    t_c <- tm_map(t_c, content_transformer(tolower))\n    t_c <- tm_map(t_c, removePunctuation)\n    t_c <- tm_map(t_c, removeNumbers)\n    t_c <- tm_map(t_c, removeWords, stopwords(\"english\"))\n    \n    ## filter for some specific words \n    t_c <- tm_map(t_c, removeWords, c(\"applause\", \"thats\", \"laughter\", \"dont\", \"back\", \"can\", \"get\", \"cant\", \"come\", \"big\", \"inaudible\", \"dont\", \"back\", \"can\", \"get\"))\n    \n    return(t_c)   \n    \n} \n" }
{ "repo_name": "amyecampbell/hgsc_subtypes", "ref": "refs/heads/master", "path": "4.Survival/Scripts/Functions/Survival_Functions.R", "content": "############################################\n# Cross-population analysis of high-grade serous ovarian cancer reveals only two robust subtypes\n# \n# Way, G.P., Rudd, J., Wang, C., Hamidi, H., Fridley, L.B,  \n# Konecny, G., Goode, E., Greene, C.S., Doherty, J.A.\n# ~~~~~~~~~~~~~~~~~~~~~\n# This script stores several functions that are required for survival analyses\n\n############################################\n#Load Functions\n############################################\n#pData is a list of full phenodata for each dataset\n#clusterMemb is a list of cluster memberships for each sample in each dataset\n\nGetCoxPHready <- function (pData, clusterMemb, recodeAge = F) {\n  # ~~~~~~~~~~~~~~\n  # Retrieves and subsets pertinent covariates and samples to be used in building\n  # the survival models. This function will output dataframes to be used in coxph.\n  # Note: the pData list and clusterMemb list are required to be in the same order.\n  #\n  # Args: \n  # pData: phenotype data for a list of datasets\n  # clusterMemb: cluster membership information for a list of datasets\n  # recodeAge: a boolean expression indicating whether or not age should be binned \n  #\n  # Returns:\n  # A list object holding the important covariate data for all samples with full data\n  # ~~~~~~~~~~~~~~\n  \n  # Load PhenoData it is in a list; first step is to subset to only the covariates we need\n  PhenoDataSubset <- list()\n  for (pheno in 1:length(pData)) {\n    # Covariates need to be recoded for the Mayo clinic data\n    if(names(pData)[pheno] == \"Mayo\") {\n      phenoData <- pData[[pheno]]\n      \n      # Compile columns of use, combine them, and then rename them\n      # Age column\n      age <- phenoData$age_at_initial_pathologic_diagnosis\n      \n      # Bin the ages into 10 bins if recodAge is True\n      if (recodeAge == T) {\n        age <- recode(age, \"0:14=1; 15:39=2; 40:44=3; 45:49=4; 50:54=5; 55:59=6; 60:64=7; \n                      65:69=8; 70:74=9; 75:150=10\")\n      }\n      \n      # Rename other important variables\n      stage <- phenoData$tumorstage\n      grade <- phenoData$grade\n      debulking <- phenoData$debulking\n      vital <- phenoData$vital_status\n      days <- phenoData$days_to_death\n      \n      # Convert days into months \n      days <- as.numeric(paste(days)) / 30.4375\n      \n      # Combine important covariate data\n      usePheno <- cbind(age, stage, grade, debulking, vital, days)\n      colnames(usePheno) <- c(\"age_at_initial_pathologic_diagnosis\", \"tumorstage\", \"grade\", \n                              \"debulking\", \"vital_status\", \"days_to_death\")\n      rownames(usePheno) <- phenoData$ID\n      \n      # Prepare the vital status codes in advance (0 = alive; 1 = deceased)    \n      usePheno <- as.data.frame(usePheno)\n      usePheno$vital_status <- gsub(1, 0, usePheno$vital_status)\n      usePheno$vital_status <- gsub(2, 1, usePheno$vital_status)\n      usePheno$vital_status <- as.numeric(paste(usePheno$vital_status))\n      PhenoDataSubset[[names(pData)[pheno]]] <- usePheno\n \n    } else {\n      phenoData <- pData[[pheno]]\n      usePheno <- phenoData[ ,c(\"age_at_initial_pathologic_diagnosis\", \"tumorstage\", \"grade\", \n                               \"debulking\", \"vital_status\", \"days_to_death\")]\n      colnames(usePheno) <- c(\"age_at_initial_pathologic_diagnosis\", \"tumorstage\", \"grade\", \n                              \"debulking\", \"vital_status\", \"days_to_death\")\n      if(recodeAge == T) {\n        recoding <- recode(usePheno[ ,\"age_at_initial_pathologic_diagnosis\"], \"0:14=1; 15:39=2; \n                           40:44=3; 45:49=4; 50:54=5; 55:59=6; 60:64=7; 65:69=8; 70:74=9; 75:150=10\")\n        usePheno[ ,\"age_at_initial_pathologic_diagnosis\"] <- recoding\n      }\n      \n      # Recode vital status\n      usePheno$vital_status <- gsub(\"living\", 0, usePheno$vital_status)\n      usePheno$vital_status <- gsub(\"deceased\", 1, usePheno$vital_status)\n      usePheno$vital_status <- as.numeric(paste(usePheno$vital_status))\n      rownames(usePheno) <- rownames(phenoData)\n      \n      # Convert days into months \n      usePheno$days_to_death <- as.numeric(paste(usePheno$days_to_death)) / 30.4375\n      \n      # Store to list\n      PhenoDataSubset[[names(pData)[pheno]]] <- usePheno\n    }\n  }\n  \n  # Create a list of dataframes that includes phenodata and cluster membership data\n  CoxDataFrameList <- list()\n  for (clus in 1:length(clusterMemb)) {\n    \n    # Get cluster membership and PhenoData for each dataset\n    Clusters <- clusterMemb[[clus]]\n    PhenoData <- PhenoDataSubset[[clus]]\n    \n    # Combine and store in new dataframe\n    coxUse <- cbind(PhenoData, Clusters)\n    CoxDataFrameList[[names(PhenoDataSubset)[clus]]] <- coxUse\n  }\n  \n  # Run the cox proportional hazards model correcting for covariates, when they are available\n  CoxPH <- list()\n  for (coxmodel in 1:length(CoxDataFrameList)) {\n    \n    # compile a list of useable covariates for each dataset in CoxDataFrameList\n    covariates <- c()\n    for(covar in 1:ncol(CoxDataFrameList[[coxmodel]]))\n    {\n      \n      # Covariate information for the particular cox model\n      tmp <- CoxDataFrameList[[coxmodel]][ ,covar]\n      if (sum(is.na(tmp)) == length(tmp)) {\n        \n        # If there is no information for all samples for the given factor, print info to screen\n        cat(names(CoxDataFrameList)[coxmodel], \"do not use this factor:\", \n            colnames(CoxDataFrameList[[coxmodel]])[covar], \"\\n\")\n      } else {\n        tmpfactor <- as.character(paste(colnames(CoxDataFrameList[[coxmodel]])[covar]))\n        covariates <- c(covariates, tmpfactor)\n      }\n    }\n    \n    # Subset the given model to useable covariates and print to screen which ones are being used\n    CoxDataFrameList[[coxmodel]] <- CoxDataFrameList[[coxmodel]][ ,covariates]\n    cat(names(PhenoDataSubset)[coxmodel], \"use these covariates:\", covariates, \"\\n\")\n    \n    # Here, you must subset usecox to only the complete.cases (subset samples)\n    usecox <- CoxDataFrameList[[coxmodel]]\n    usecox <- usecox[complete.cases(usecox), ] \n    \n    # Correct for \"unknown\" coding for debulking status in Mayo\n    if (names(CoxDataFrameList)[coxmodel] == \"Mayo\") {\n      usecox <- usecox[usecox$debulking == 1 | usecox$debulking == 2, ]\n    }\n    \n    # Store to cox model list and prepare for output\n    CoxPH[[names(CoxDataFrameList)[coxmodel]]] <- usecox\n  }\n  return(CoxPH)\n}\n\n\ndoCoxPH_KM <- function (coxphdata, fname) {\n  # ~~~~~~~~~~~~~~\n  # This function will write the KM plots to file\n  #\n  # Args: \n  # coxphdata: a sample by covariate dataframe\n  # fname: the name of the dataset\n  #\n  # Returns:\n  # A Kaplan Meier curve representing survival for each cluster\n  # ~~~~~~~~~~~~~~\n  \n  # Get a survfit object and save it to a list\n  tmpsurvfit2 <- survfit(Surv(days_to_death, vital_status) ~ ClusterK2, data = coxphdata)\n  tmpsurvfit3 <- survfit(Surv(days_to_death, vital_status) ~ ClusterK3, data = coxphdata)\n  tmpsurvfit4 <- survfit(Surv(days_to_death, vital_status) ~ ClusterK4, data = coxphdata)\n  survlist <- list(K2 = tmpsurvfit2, K3 = tmpsurvfit3, K4 = tmpsurvfit4)\n  \n  # Store sample size variable\n  n <- nrow(coxphdata)\n  \n  # Initialize graphics device\n  png(paste(\"4.Survival/Figures/\", fname, \"KM_survival.png\", sep = \"\"), width = 280, height = 600)\n  \n  # Obtain appropriate plotting margins\n  par(mfrow=c(3,1))\n  par(mar = rep(2.5, 4))\n\n  # K = 2 plot and legend\n  plot(survlist[[1]], main = \"\", col = c('skyblue1', 'tomato'), lwd = 3, cex = 2, cex.lab = 2, \n       cex.axis = 1.75, lty = c(4, 5), xlab = \"\", ylab = \"\") \n  legend(\"topright\", legend = paste(\"Survival\\nn = \", n), bty = \"n\", cex = 2)\n  \n  # K = 3 plot and legend\n  plot(survlist[[2]], main = \"\", col = c('skyblue1', 'tomato', 'springgreen'), lwd = 3, cex = 2, cex.lab = 2, \n       cex.axis = 1.75, lty = c(4, 5), xlab = \"\", ylab = \"\") \n  #legend(\"topright\", legend = paste(\"Survival\\nn = \", n), bty = \"n\", cex = 2)\n  \n  # K = 4 plot and legend\n  plot(survlist[[3]], main = \"\", col = c('skyblue1', 'tomato', 'springgreen', 'violet'), lwd = 3, cex = 2, \n       cex.lab = 2, cex.axis = 1.75, lty = c(4, 5), xlab = \"\", ylab = \"\") \n  #legend(\"topright\", legend = paste(\"Survival\\nn = \", n), bty = \"n\", cex = 2)\n  dev.off()\n}\n\n\ncustomCoxPH <- function (data, type = \"multi\") {\n  # ~~~~~~~~~~~~~~\n  # This function will prepare the cox models and evaluate them\n  #\n  # Args: \n  # data: a sample by covariate dataframe\n  # type: the completeness by which the cox model is built. Can be either\n  # \"multi\", \"uni\", or \"removeAge\"\n  #\n  # Returns:\n  # The results of a Cox proportional hazards model separated by clustering methods\n  # ~~~~~~~~~~~~~~\n  \n  # Get the covariate variables\n  variables <- colnames(data)\n  \n  # Get the important days to death and vital status variables\n  dtd <- variables[grepl(\"days_to_death\", variables)]\n  vs <- variables[grepl(\"vital_status\", variables)]\n  \n  # Get cluster information\n  clus <- variables[grepl(\"Cluster\", variables)]\n  \n  # Separate the variables into covariates\n  covariates <- setdiff(variables, c(dtd, vs, clus))\n  \n  # Build the cox models for each cluster method\n  coxPH_list <- list()\n  for (centroid in 1:length(clus)) {\n    if(type == \"multi\") {\n      \n      # Initialize the model; this will be different according to the covariates available\n      model <- c()\n      for (covar in 1:(length(covariates) + 1)) {\n        if (covar == 1) {\n          model <- paste(\"coxph(Surv(\", dtd, \", \", vs, \")~ as.factor(\", clus[centroid], \") + \", \n                         covariates[covar], sep = \"\")\n        } else if (covar <= length(covariates)) {\n          model <- paste(model, covariates[covar], sep = \" + \")\n        } else {\n          model <- paste(model, \", data = data)\", sep = \"\")\n        }     \n      }\n      \n      # Evaluate the model and store into list\n      coxPH_list[[clus[centroid]]] <- eval(parse(text = model))\n\n    # Get the univariate model information\n    } else if (type == \"uni\") {\n      model <- paste(\"coxph(Surv(\", dtd, \", \", vs, \")~ as.factor(\", clus[centroid], \"), data = data)\")\n      coxPH_list[[clus[centroid]]]  <- eval(parse(text = model))\n\n    # Remove age adjustments for all models\n    } else if (type == \"removeAge\") {\n      model <- c()\n      if (\"age_at_initial_pathologic_diagnosis\" %in% covariates) {\n        covariates <- covariates[-grep(\"age_at_initial_pathologic_diagnosis\", covariates)]\n      }\n      for (covar in 1:(length(covariates) + 1)) {\n        \n        if (covar == 1) {\n          model <- paste(\"coxph(Surv(\", dtd, \", \", vs, \")~ as.factor(\", clus[centroid], \") + \", \n                         covariates[covar], sep = \"\")\n        } else if (covar <= length(covariates)) {\n          model <- paste(model, covariates[covar], sep = \" + \")\n        } else {\n          model <- paste(model, \", data = data)\", sep = \"\")\n        }   \n      }\n      \n      # Evaluate the model\n      coxPH_list[[clus[centroid]]] <- eval(parse(text = model))\n    }\n  }\n  return(coxPH_list)\n}\n\n\ncoxSum <- function (coxData, name) {\n  # ~~~~~~~~~~~~~~\n  # This function will write out the hazards ratios, confidence intervals, pvalues,\n  # and Wald's P for each cox proportional hazards model\n  #\n  # Args: \n  # coxData: a coxph object\n  # name: the name of the dataset\n  #\n  # Returns:\n  # a dataframe of pertinent information regarding the cox model summary\n  # ~~~~~~~~~~~~~~\n  \n  cox.sum <- summary(coxData)\n  \n  # 95% Confidence Intervals\n  conf.int <- cox.sum$conf.int[ ,3:4]\n  \n  # Hazard ratio\n  hazard <- exp(coxData$coefficients)\n  \n  # p value\n  Pvalues <- cox.sum$coefficients[ ,5]\n  \n  # Wald's P\n  WaldsP <- cox.sum$waldtest[\"pvalue\"]\n  \n  # Combine together and write to file\n  CoxPHtmp <- cbind(conf.int, hazard, Pvalues, WaldsP)\n  write.csv(CoxPHtmp, paste(\"4.Survival/Tables/\", name, \".csv\", sep = \"\"), row.names = T)\n}\n" }
{ "repo_name": "greenelab/hgsc_subtypes", "ref": "refs/heads/master", "path": "4.Survival/Scripts/Functions/Survival_Functions.R", "content": "############################################\n# Cross-population analysis of high-grade serous ovarian cancer reveals only two robust subtypes\n# \n# Way, G.P., Rudd, J., Wang, C., Hamidi, H., Fridley, L.B,  \n# Konecny, G., Goode, E., Greene, C.S., Doherty, J.A.\n# ~~~~~~~~~~~~~~~~~~~~~\n# This script stores several functions that are required for survival analyses\n\n############################################\n#Load Functions\n############################################\n#pData is a list of full phenodata for each dataset\n#clusterMemb is a list of cluster memberships for each sample in each dataset\n\nGetCoxPHready <- function (pData, clusterMemb, recodeAge = F) {\n  # ~~~~~~~~~~~~~~\n  # Retrieves and subsets pertinent covariates and samples to be used in building\n  # the survival models. This function will output dataframes to be used in coxph.\n  # Note: the pData list and clusterMemb list are required to be in the same order.\n  #\n  # Args: \n  # pData: phenotype data for a list of datasets\n  # clusterMemb: cluster membership information for a list of datasets\n  # recodeAge: a boolean expression indicating whether or not age should be binned \n  #\n  # Returns:\n  # A list object holding the important covariate data for all samples with full data\n  # ~~~~~~~~~~~~~~\n  \n  # Load PhenoData it is in a list; first step is to subset to only the covariates we need\n  PhenoDataSubset <- list()\n  for (pheno in 1:length(pData)) {\n    # Covariates need to be recoded for the Mayo clinic data\n    if(names(pData)[pheno] == \"Mayo\") {\n      phenoData <- pData[[pheno]]\n      \n      # Compile columns of use, combine them, and then rename them\n      # Age column\n      age <- phenoData$age_at_initial_pathologic_diagnosis\n      \n      # Bin the ages into 10 bins if recodAge is True\n      if (recodeAge == T) {\n        age <- recode(age, \"0:14=1; 15:39=2; 40:44=3; 45:49=4; 50:54=5; 55:59=6; 60:64=7; \n                      65:69=8; 70:74=9; 75:150=10\")\n      }\n      \n      # Rename other important variables\n      stage <- phenoData$tumorstage\n      grade <- phenoData$grade\n      debulking <- phenoData$debulking\n      vital <- phenoData$vital_status\n      days <- phenoData$days_to_death\n      \n      # Convert days into months \n      days <- as.numeric(paste(days)) / 30.4375\n      \n      # Combine important covariate data\n      usePheno <- cbind(age, stage, grade, debulking, vital, days)\n      colnames(usePheno) <- c(\"age_at_initial_pathologic_diagnosis\", \"tumorstage\", \"grade\", \n                              \"debulking\", \"vital_status\", \"days_to_death\")\n      rownames(usePheno) <- phenoData$ID\n      \n      # Prepare the vital status codes in advance (0 = alive; 1 = deceased)    \n      usePheno <- as.data.frame(usePheno)\n      usePheno$vital_status <- gsub(1, 0, usePheno$vital_status)\n      usePheno$vital_status <- gsub(2, 1, usePheno$vital_status)\n      usePheno$vital_status <- as.numeric(paste(usePheno$vital_status))\n      PhenoDataSubset[[names(pData)[pheno]]] <- usePheno\n \n    } else {\n      phenoData <- pData[[pheno]]\n      usePheno <- phenoData[ ,c(\"age_at_initial_pathologic_diagnosis\", \"tumorstage\", \"grade\", \n                               \"debulking\", \"vital_status\", \"days_to_death\")]\n      colnames(usePheno) <- c(\"age_at_initial_pathologic_diagnosis\", \"tumorstage\", \"grade\", \n                              \"debulking\", \"vital_status\", \"days_to_death\")\n      if(recodeAge == T) {\n        recoding <- recode(usePheno[ ,\"age_at_initial_pathologic_diagnosis\"], \"0:14=1; 15:39=2; \n                           40:44=3; 45:49=4; 50:54=5; 55:59=6; 60:64=7; 65:69=8; 70:74=9; 75:150=10\")\n        usePheno[ ,\"age_at_initial_pathologic_diagnosis\"] <- recoding\n      }\n      \n      # Recode vital status\n      usePheno$vital_status <- gsub(\"living\", 0, usePheno$vital_status)\n      usePheno$vital_status <- gsub(\"deceased\", 1, usePheno$vital_status)\n      usePheno$vital_status <- as.numeric(paste(usePheno$vital_status))\n      rownames(usePheno) <- rownames(phenoData)\n      \n      # Convert days into months \n      usePheno$days_to_death <- as.numeric(paste(usePheno$days_to_death)) / 30.4375\n      \n      # Store to list\n      PhenoDataSubset[[names(pData)[pheno]]] <- usePheno\n    }\n  }\n  \n  # Create a list of dataframes that includes phenodata and cluster membership data\n  CoxDataFrameList <- list()\n  for (clus in 1:length(clusterMemb)) {\n    \n    # Get cluster membership and PhenoData for each dataset\n    Clusters <- clusterMemb[[clus]]\n    PhenoData <- PhenoDataSubset[[clus]]\n    \n    # Combine and store in new dataframe\n    coxUse <- cbind(PhenoData, Clusters)\n    CoxDataFrameList[[names(PhenoDataSubset)[clus]]] <- coxUse\n  }\n  \n  # Run the cox proportional hazards model correcting for covariates, when they are available\n  CoxPH <- list()\n  for (coxmodel in 1:length(CoxDataFrameList)) {\n    \n    # compile a list of useable covariates for each dataset in CoxDataFrameList\n    covariates <- c()\n    for(covar in 1:ncol(CoxDataFrameList[[coxmodel]]))\n    {\n      \n      # Covariate information for the particular cox model\n      tmp <- CoxDataFrameList[[coxmodel]][ ,covar]\n      if (sum(is.na(tmp)) == length(tmp)) {\n        \n        # If there is no information for all samples for the given factor, print info to screen\n        cat(names(CoxDataFrameList)[coxmodel], \"do not use this factor:\", \n            colnames(CoxDataFrameList[[coxmodel]])[covar], \"\\n\")\n      } else {\n        tmpfactor <- as.character(paste(colnames(CoxDataFrameList[[coxmodel]])[covar]))\n        covariates <- c(covariates, tmpfactor)\n      }\n    }\n    \n    # Subset the given model to useable covariates and print to screen which ones are being used\n    CoxDataFrameList[[coxmodel]] <- CoxDataFrameList[[coxmodel]][ ,covariates]\n    cat(names(PhenoDataSubset)[coxmodel], \"use these covariates:\", covariates, \"\\n\")\n    \n    # Here, you must subset usecox to only the complete.cases (subset samples)\n    usecox <- CoxDataFrameList[[coxmodel]]\n    usecox <- usecox[complete.cases(usecox), ] \n    \n    # Correct for \"unknown\" coding for debulking status in Mayo\n    if (names(CoxDataFrameList)[coxmodel] == \"Mayo\") {\n      usecox <- usecox[usecox$debulking == 1 | usecox$debulking == 2, ]\n    }\n    \n    # Store to cox model list and prepare for output\n    CoxPH[[names(CoxDataFrameList)[coxmodel]]] <- usecox\n  }\n  return(CoxPH)\n}\n\n\ndoCoxPH_KM <- function (coxphdata, fname) {\n  # ~~~~~~~~~~~~~~\n  # This function will write the KM plots to file\n  #\n  # Args: \n  # coxphdata: a sample by covariate dataframe\n  # fname: the name of the dataset\n  #\n  # Returns:\n  # A Kaplan Meier curve representing survival for each cluster\n  # ~~~~~~~~~~~~~~\n  \n  # Get a survfit object and save it to a list\n  tmpsurvfit2 <- survfit(Surv(days_to_death, vital_status) ~ ClusterK2, data = coxphdata)\n  tmpsurvfit3 <- survfit(Surv(days_to_death, vital_status) ~ ClusterK3, data = coxphdata)\n  tmpsurvfit4 <- survfit(Surv(days_to_death, vital_status) ~ ClusterK4, data = coxphdata)\n  survlist <- list(K2 = tmpsurvfit2, K3 = tmpsurvfit3, K4 = tmpsurvfit4)\n  \n  # Store sample size variable\n  n <- nrow(coxphdata)\n  \n  # Initialize graphics device\n  png(paste(\"4.Survival/Figures/\", fname, \"KM_survival.png\", sep = \"\"), width = 280, height = 600)\n  \n  # Obtain appropriate plotting margins\n  par(mfrow=c(3,1))\n  par(mar = rep(2.5, 4))\n\n  # K = 2 plot and legend\n  plot(survlist[[1]], main = \"\", col = c('skyblue1', 'tomato'), lwd = 3, cex = 2, cex.lab = 2, \n       cex.axis = 1.75, lty = c(4, 5), xlab = \"\", ylab = \"\") \n  legend(\"topright\", legend = paste(\"Survival\\nn = \", n), bty = \"n\", cex = 2)\n  \n  # K = 3 plot and legend\n  plot(survlist[[2]], main = \"\", col = c('skyblue1', 'tomato', 'springgreen'), lwd = 3, cex = 2, cex.lab = 2, \n       cex.axis = 1.75, lty = c(4, 5), xlab = \"\", ylab = \"\") \n  #legend(\"topright\", legend = paste(\"Survival\\nn = \", n), bty = \"n\", cex = 2)\n  \n  # K = 4 plot and legend\n  plot(survlist[[3]], main = \"\", col = c('skyblue1', 'tomato', 'springgreen', 'violet'), lwd = 3, cex = 2, \n       cex.lab = 2, cex.axis = 1.75, lty = c(4, 5), xlab = \"\", ylab = \"\") \n  #legend(\"topright\", legend = paste(\"Survival\\nn = \", n), bty = \"n\", cex = 2)\n  dev.off()\n}\n\n\ncustomCoxPH <- function (data, type = \"multi\") {\n  # ~~~~~~~~~~~~~~\n  # This function will prepare the cox models and evaluate them\n  #\n  # Args: \n  # data: a sample by covariate dataframe\n  # type: the completeness by which the cox model is built. Can be either\n  # \"multi\", \"uni\", or \"removeAge\"\n  #\n  # Returns:\n  # The results of a Cox proportional hazards model separated by clustering methods\n  # ~~~~~~~~~~~~~~\n  \n  # Get the covariate variables\n  variables <- colnames(data)\n  \n  # Get the important days to death and vital status variables\n  dtd <- variables[grepl(\"days_to_death\", variables)]\n  vs <- variables[grepl(\"vital_status\", variables)]\n  \n  # Get cluster information\n  clus <- variables[grepl(\"Cluster\", variables)]\n  \n  # Separate the variables into covariates\n  covariates <- setdiff(variables, c(dtd, vs, clus))\n  \n  # Build the cox models for each cluster method\n  coxPH_list <- list()\n  for (centroid in 1:length(clus)) {\n    if(type == \"multi\") {\n      \n      # Initialize the model; this will be different according to the covariates available\n      model <- c()\n      for (covar in 1:(length(covariates) + 1)) {\n        if (covar == 1) {\n          model <- paste(\"coxph(Surv(\", dtd, \", \", vs, \")~ as.factor(\", clus[centroid], \") + \", \n                         covariates[covar], sep = \"\")\n        } else if (covar <= length(covariates)) {\n          model <- paste(model, covariates[covar], sep = \" + \")\n        } else {\n          model <- paste(model, \", data = data)\", sep = \"\")\n        }     \n      }\n      \n      # Evaluate the model and store into list\n      coxPH_list[[clus[centroid]]] <- eval(parse(text = model))\n\n    # Get the univariate model information\n    } else if (type == \"uni\") {\n      model <- paste(\"coxph(Surv(\", dtd, \", \", vs, \")~ as.factor(\", clus[centroid], \"), data = data)\")\n      coxPH_list[[clus[centroid]]]  <- eval(parse(text = model))\n\n    # Remove age adjustments for all models\n    } else if (type == \"removeAge\") {\n      model <- c()\n      if (\"age_at_initial_pathologic_diagnosis\" %in% covariates) {\n        covariates <- covariates[-grep(\"age_at_initial_pathologic_diagnosis\", covariates)]\n      }\n      for (covar in 1:(length(covariates) + 1)) {\n        \n        if (covar == 1) {\n          model <- paste(\"coxph(Surv(\", dtd, \", \", vs, \")~ as.factor(\", clus[centroid], \") + \", \n                         covariates[covar], sep = \"\")\n        } else if (covar <= length(covariates)) {\n          model <- paste(model, covariates[covar], sep = \" + \")\n        } else {\n          model <- paste(model, \", data = data)\", sep = \"\")\n        }   \n      }\n      \n      # Evaluate the model\n      coxPH_list[[clus[centroid]]] <- eval(parse(text = model))\n    }\n  }\n  return(coxPH_list)\n}\n\n\ncoxSum <- function (coxData, name) {\n  # ~~~~~~~~~~~~~~\n  # This function will write out the hazards ratios, confidence intervals, pvalues,\n  # and Wald's P for each cox proportional hazards model\n  #\n  # Args: \n  # coxData: a coxph object\n  # name: the name of the dataset\n  #\n  # Returns:\n  # a dataframe of pertinent information regarding the cox model summary\n  # ~~~~~~~~~~~~~~\n  \n  cox.sum <- summary(coxData)\n  \n  # 95% Confidence Intervals\n  conf.int <- cox.sum$conf.int[ ,3:4]\n  \n  # Hazard ratio\n  hazard <- exp(coxData$coefficients)\n  \n  # p value\n  Pvalues <- cox.sum$coefficients[ ,5]\n  \n  # Wald's P\n  WaldsP <- cox.sum$waldtest[\"pvalue\"]\n  \n  # Combine together and write to file\n  CoxPHtmp <- cbind(conf.int, hazard, Pvalues, WaldsP)\n  write.csv(CoxPHtmp, paste(\"4.Survival/Tables/\", name, \".csv\", sep = \"\"), row.names = T)\n}\n" }
{ "repo_name": "gwaygenomics/hgsc_subtypes", "ref": "refs/heads/master", "path": "4.Survival/Scripts/Functions/Survival_Functions.R", "content": "############################################\n# Cross-population analysis of high-grade serous ovarian cancer reveals only two robust subtypes\n# \n# Way, G.P., Rudd, J., Wang, C., Hamidi, H., Fridley, L.B,  \n# Konecny, G., Goode, E., Greene, C.S., Doherty, J.A.\n# ~~~~~~~~~~~~~~~~~~~~~\n# This script stores several functions that are required for survival analyses\n\n############################################\n#Load Functions\n############################################\n#pData is a list of full phenodata for each dataset\n#clusterMemb is a list of cluster memberships for each sample in each dataset\n\nGetCoxPHready <- function (pData, clusterMemb, recodeAge = F) {\n  # ~~~~~~~~~~~~~~\n  # Retrieves and subsets pertinent covariates and samples to be used in building\n  # the survival models. This function will output dataframes to be used in coxph.\n  # Note: the pData list and clusterMemb list are required to be in the same order.\n  #\n  # Args: \n  # pData: phenotype data for a list of datasets\n  # clusterMemb: cluster membership information for a list of datasets\n  # recodeAge: a boolean expression indicating whether or not age should be binned \n  #\n  # Returns:\n  # A list object holding the important covariate data for all samples with full data\n  # ~~~~~~~~~~~~~~\n  \n  # Load PhenoData it is in a list; first step is to subset to only the covariates we need\n  PhenoDataSubset <- list()\n  for (pheno in 1:length(pData)) {\n    # Covariates need to be recoded for the Mayo clinic data\n    if(names(pData)[pheno] == \"Mayo\") {\n      phenoData <- pData[[pheno]]\n      \n      # Compile columns of use, combine them, and then rename them\n      # Age column\n      age <- phenoData$age_at_initial_pathologic_diagnosis\n      \n      # Bin the ages into 10 bins if recodAge is True\n      if (recodeAge == T) {\n        age <- recode(age, \"0:14=1; 15:39=2; 40:44=3; 45:49=4; 50:54=5; 55:59=6; 60:64=7; \n                      65:69=8; 70:74=9; 75:150=10\")\n      }\n      \n      # Rename other important variables\n      stage <- phenoData$tumorstage\n      grade <- phenoData$grade\n      debulking <- phenoData$debulking\n      vital <- phenoData$vital_status\n      days <- phenoData$days_to_death\n      \n      # Convert days into months \n      days <- as.numeric(paste(days)) / 30.4375\n      \n      # Combine important covariate data\n      usePheno <- cbind(age, stage, grade, debulking, vital, days)\n      colnames(usePheno) <- c(\"age_at_initial_pathologic_diagnosis\", \"tumorstage\", \"grade\", \n                              \"debulking\", \"vital_status\", \"days_to_death\")\n      rownames(usePheno) <- phenoData$ID\n      \n      # Prepare the vital status codes in advance (0 = alive; 1 = deceased)    \n      usePheno <- as.data.frame(usePheno)\n      usePheno$vital_status <- gsub(1, 0, usePheno$vital_status)\n      usePheno$vital_status <- gsub(2, 1, usePheno$vital_status)\n      usePheno$vital_status <- as.numeric(paste(usePheno$vital_status))\n      PhenoDataSubset[[names(pData)[pheno]]] <- usePheno\n \n    } else {\n      phenoData <- pData[[pheno]]\n      usePheno <- phenoData[ ,c(\"age_at_initial_pathologic_diagnosis\", \"tumorstage\", \"grade\", \n                               \"debulking\", \"vital_status\", \"days_to_death\")]\n      colnames(usePheno) <- c(\"age_at_initial_pathologic_diagnosis\", \"tumorstage\", \"grade\", \n                              \"debulking\", \"vital_status\", \"days_to_death\")\n      if(recodeAge == T) {\n        recoding <- recode(usePheno[ ,\"age_at_initial_pathologic_diagnosis\"], \"0:14=1; 15:39=2; \n                           40:44=3; 45:49=4; 50:54=5; 55:59=6; 60:64=7; 65:69=8; 70:74=9; 75:150=10\")\n        usePheno[ ,\"age_at_initial_pathologic_diagnosis\"] <- recoding\n      }\n      \n      # Recode vital status\n      usePheno$vital_status <- gsub(\"living\", 0, usePheno$vital_status)\n      usePheno$vital_status <- gsub(\"deceased\", 1, usePheno$vital_status)\n      usePheno$vital_status <- as.numeric(paste(usePheno$vital_status))\n      rownames(usePheno) <- rownames(phenoData)\n      \n      # Convert days into months \n      usePheno$days_to_death <- as.numeric(paste(usePheno$days_to_death)) / 30.4375\n      \n      # Store to list\n      PhenoDataSubset[[names(pData)[pheno]]] <- usePheno\n    }\n  }\n  \n  # Create a list of dataframes that includes phenodata and cluster membership data\n  CoxDataFrameList <- list()\n  for (clus in 1:length(clusterMemb)) {\n    \n    # Get cluster membership and PhenoData for each dataset\n    Clusters <- clusterMemb[[clus]]\n    PhenoData <- PhenoDataSubset[[clus]]\n    \n    # Combine and store in new dataframe\n    coxUse <- cbind(PhenoData, Clusters)\n    CoxDataFrameList[[names(PhenoDataSubset)[clus]]] <- coxUse\n  }\n  \n  # Run the cox proportional hazards model correcting for covariates, when they are available\n  CoxPH <- list()\n  for (coxmodel in 1:length(CoxDataFrameList)) {\n    \n    # compile a list of useable covariates for each dataset in CoxDataFrameList\n    covariates <- c()\n    for(covar in 1:ncol(CoxDataFrameList[[coxmodel]]))\n    {\n      \n      # Covariate information for the particular cox model\n      tmp <- CoxDataFrameList[[coxmodel]][ ,covar]\n      if (sum(is.na(tmp)) == length(tmp)) {\n        \n        # If there is no information for all samples for the given factor, print info to screen\n        cat(names(CoxDataFrameList)[coxmodel], \"do not use this factor:\", \n            colnames(CoxDataFrameList[[coxmodel]])[covar], \"\\n\")\n      } else {\n        tmpfactor <- as.character(paste(colnames(CoxDataFrameList[[coxmodel]])[covar]))\n        covariates <- c(covariates, tmpfactor)\n      }\n    }\n    \n    # Subset the given model to useable covariates and print to screen which ones are being used\n    CoxDataFrameList[[coxmodel]] <- CoxDataFrameList[[coxmodel]][ ,covariates]\n    cat(names(PhenoDataSubset)[coxmodel], \"use these covariates:\", covariates, \"\\n\")\n    \n    # Here, you must subset usecox to only the complete.cases (subset samples)\n    usecox <- CoxDataFrameList[[coxmodel]]\n    usecox <- usecox[complete.cases(usecox), ] \n    \n    # Correct for \"unknown\" coding for debulking status in Mayo\n    if (names(CoxDataFrameList)[coxmodel] == \"Mayo\") {\n      usecox <- usecox[usecox$debulking == 1 | usecox$debulking == 2, ]\n    }\n    \n    # Store to cox model list and prepare for output\n    CoxPH[[names(CoxDataFrameList)[coxmodel]]] <- usecox\n  }\n  return(CoxPH)\n}\n\n\ndoCoxPH_KM <- function (coxphdata, fname) {\n  # ~~~~~~~~~~~~~~\n  # This function will write the KM plots to file\n  #\n  # Args: \n  # coxphdata: a sample by covariate dataframe\n  # fname: the name of the dataset\n  #\n  # Returns:\n  # A Kaplan Meier curve representing survival for each cluster\n  # ~~~~~~~~~~~~~~\n  \n  # Get a survfit object and save it to a list\n  tmpsurvfit2 <- survfit(Surv(days_to_death, vital_status) ~ ClusterK2, data = coxphdata)\n  tmpsurvfit3 <- survfit(Surv(days_to_death, vital_status) ~ ClusterK3, data = coxphdata)\n  tmpsurvfit4 <- survfit(Surv(days_to_death, vital_status) ~ ClusterK4, data = coxphdata)\n  survlist <- list(K2 = tmpsurvfit2, K3 = tmpsurvfit3, K4 = tmpsurvfit4)\n  \n  # Store sample size variable\n  n <- nrow(coxphdata)\n  \n  # Initialize graphics device\n  png(paste(\"4.Survival/Figures/\", fname, \"KM_survival.png\", sep = \"\"), width = 280, height = 600)\n  \n  # Obtain appropriate plotting margins\n  par(mfrow=c(3,1))\n  par(mar = rep(2.5, 4))\n\n  # K = 2 plot and legend\n  plot(survlist[[1]], main = \"\", col = c('skyblue1', 'tomato'), lwd = 3, cex = 2, cex.lab = 2, \n       cex.axis = 1.75, lty = c(4, 5), xlab = \"\", ylab = \"\") \n  legend(\"topright\", legend = paste(\"Survival\\nn = \", n), bty = \"n\", cex = 2)\n  \n  # K = 3 plot and legend\n  plot(survlist[[2]], main = \"\", col = c('skyblue1', 'tomato', 'springgreen'), lwd = 3, cex = 2, cex.lab = 2, \n       cex.axis = 1.75, lty = c(4, 5), xlab = \"\", ylab = \"\") \n  #legend(\"topright\", legend = paste(\"Survival\\nn = \", n), bty = \"n\", cex = 2)\n  \n  # K = 4 plot and legend\n  plot(survlist[[3]], main = \"\", col = c('skyblue1', 'tomato', 'springgreen', 'violet'), lwd = 3, cex = 2, \n       cex.lab = 2, cex.axis = 1.75, lty = c(4, 5), xlab = \"\", ylab = \"\") \n  #legend(\"topright\", legend = paste(\"Survival\\nn = \", n), bty = \"n\", cex = 2)\n  dev.off()\n}\n\n\ncustomCoxPH <- function (data, type = \"multi\") {\n  # ~~~~~~~~~~~~~~\n  # This function will prepare the cox models and evaluate them\n  #\n  # Args: \n  # data: a sample by covariate dataframe\n  # type: the completeness by which the cox model is built. Can be either\n  # \"multi\", \"uni\", or \"removeAge\"\n  #\n  # Returns:\n  # The results of a Cox proportional hazards model separated by clustering methods\n  # ~~~~~~~~~~~~~~\n  \n  # Get the covariate variables\n  variables <- colnames(data)\n  \n  # Get the important days to death and vital status variables\n  dtd <- variables[grepl(\"days_to_death\", variables)]\n  vs <- variables[grepl(\"vital_status\", variables)]\n  \n  # Get cluster information\n  clus <- variables[grepl(\"Cluster\", variables)]\n  \n  # Separate the variables into covariates\n  covariates <- setdiff(variables, c(dtd, vs, clus))\n  \n  # Build the cox models for each cluster method\n  coxPH_list <- list()\n  for (centroid in 1:length(clus)) {\n    if(type == \"multi\") {\n      \n      # Initialize the model; this will be different according to the covariates available\n      model <- c()\n      for (covar in 1:(length(covariates) + 1)) {\n        if (covar == 1) {\n          model <- paste(\"coxph(Surv(\", dtd, \", \", vs, \")~ as.factor(\", clus[centroid], \") + \", \n                         covariates[covar], sep = \"\")\n        } else if (covar <= length(covariates)) {\n          model <- paste(model, covariates[covar], sep = \" + \")\n        } else {\n          model <- paste(model, \", data = data)\", sep = \"\")\n        }     \n      }\n      \n      # Evaluate the model and store into list\n      coxPH_list[[clus[centroid]]] <- eval(parse(text = model))\n\n    # Get the univariate model information\n    } else if (type == \"uni\") {\n      model <- paste(\"coxph(Surv(\", dtd, \", \", vs, \")~ as.factor(\", clus[centroid], \"), data = data)\")\n      coxPH_list[[clus[centroid]]]  <- eval(parse(text = model))\n\n    # Remove age adjustments for all models\n    } else if (type == \"removeAge\") {\n      model <- c()\n      if (\"age_at_initial_pathologic_diagnosis\" %in% covariates) {\n        covariates <- covariates[-grep(\"age_at_initial_pathologic_diagnosis\", covariates)]\n      }\n      for (covar in 1:(length(covariates) + 1)) {\n        \n        if (covar == 1) {\n          model <- paste(\"coxph(Surv(\", dtd, \", \", vs, \")~ as.factor(\", clus[centroid], \") + \", \n                         covariates[covar], sep = \"\")\n        } else if (covar <= length(covariates)) {\n          model <- paste(model, covariates[covar], sep = \" + \")\n        } else {\n          model <- paste(model, \", data = data)\", sep = \"\")\n        }   \n      }\n      \n      # Evaluate the model\n      coxPH_list[[clus[centroid]]] <- eval(parse(text = model))\n    }\n  }\n  return(coxPH_list)\n}\n\n\ncoxSum <- function (coxData, name) {\n  # ~~~~~~~~~~~~~~\n  # This function will write out the hazards ratios, confidence intervals, pvalues,\n  # and Wald's P for each cox proportional hazards model\n  #\n  # Args: \n  # coxData: a coxph object\n  # name: the name of the dataset\n  #\n  # Returns:\n  # a dataframe of pertinent information regarding the cox model summary\n  # ~~~~~~~~~~~~~~\n  \n  cox.sum <- summary(coxData)\n  \n  # 95% Confidence Intervals\n  conf.int <- cox.sum$conf.int[ ,3:4]\n  \n  # Hazard ratio\n  hazard <- exp(coxData$coefficients)\n  \n  # p value\n  Pvalues <- cox.sum$coefficients[ ,5]\n  \n  # Wald's P\n  WaldsP <- cox.sum$waldtest[\"pvalue\"]\n  \n  # Combine together and write to file\n  CoxPHtmp <- cbind(conf.int, hazard, Pvalues, WaldsP)\n  write.csv(CoxPHtmp, paste(\"4.Survival/Tables/\", name, \".csv\", sep = \"\"), row.names = T)\n}\n" }
{ "repo_name": "junwucs/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_demos/runit_demo_NOFEATURE_sciworkFlow.R", "content": "#Split data into test/train.\n#Do Grid search over lambda and Score all the models on a test set. Choose the best model by AUC on the test set.\n#Do grid search on gbm and predict on test set. Print the AUCs and model params \"\n\nsetwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource('../h2o-runit.R')\n\n\ntest <- function() {\nprint(\"Reading in prostate dataset\")\npros.hex <- h2o.importFile(normalizePath(locate(\"smalldata/logreg/prostate.csv\")), destination_frame=\"pros.hex\")\nprint (\"Run summary\")\nsummary(pros.hex)\nprint(\"Summary of a column\")\nprint(summary(pros.hex$CAPSULE))\nprint(\"Convert a column to factor\")\npros.hex$CAPSULE <- as.factor(pros.hex$CAPSULE)\nprint(\"print the summary again\")\nprint(summary(pros.hex$CAPSULE))\nprint(\"Print quantile of a column\")\nprint(quantile(pros.hex$AGE,probs=seq(0,1,.1)))\n\nprint(\"split frame into test train\")\n#  a <- h2o.splitFrame(pros.hex,ratios=c(.2),shuffle=T)\n# print(\"print dimension and assign to test and train\")\n# print(dim(a[[1]]))\n# print(dim(a[[2]]))\n# pros.train <- a[[2]]\n# pros.test <- a[[1]]\nsid <- h2o.runif(pros.hex)\npros.train <- pros.hex[sid > 0.2, ]\npros.test <- pros.hex[sid <= 0.2, ]\n\nmyX <- c(\"AGE\",\"RACE\",\"DPROS\",\"DCAPS\",\"PSA\",\"VOL\",\"GLEASON\")\nmyY <- \"CAPSULE\"\n\n#GLM\nprint(\"Build GLM model\")\nmy.glm <- h2o.glm(x=myX, y=myY, training_frame=pros.train, family=\"binomial\",standardize=T,\n  lambda_search=T) #TODO: something return_all_lambdas = T?\nprint(my.glm)\n\nprint(\"This is the best model\")\nbest_model <- my.glm@best_model\nprint(best_model)\n\nprint(\"predict on best lambda model\")\npred <- predict(my.glm@models[[best_model]],pros.test)\nprint(head(pred))\n\nprint(\"print performance and AUC\")\nperf <- h2o.performance(pred$'1',pros.test$CAPSULE )\nprint(perf)\nprint(perf@model$AUC)\nplot(perf,type=\"roc\")\n\nresult_frame <- data.frame(id = 0,auc = 0 , key = 0)\n\nprint(\"print performance for all models on test set\")\nfor(i in 1:100){\n  pred <- predict(my.glm@models[[i]],pros.test)\n  perf <- h2o.performance(pred$'1',pros.test$CAPSULE )\n  print ( paste (\"  model number:\", i, \"  AUC on test set: \", round(perf@model$AUC, digits=4),  sep=''), quote=F)\n  result_frame <- rbind(result_frame, c(i,round(perf@model$AUC, digits=4),my.glm@models[[i]]@key))\n}\n\nresult_frame <- result_frame[-1,]\nresult_frame\nprint(\"order the results by AUC on test set\")\nordered_results <- result_frame[order(result_frame$AUC,decreasing=T),]\nordered_results\nprint(\"get the model that gives the best prediction using the AUC score\")\nglm_best_model <- h2o.getModel(model_id= ordered_results[1,\"key\"])\nprint(glm_best_model)\n\n#GBM\nprint(\"Grid search gbm\")\npros.gbm <- h2o.gbm(x = myX, y = myY, loss = \"bernoulli\", data = pros.train, n.trees = c(50,100),n.minobsinnode=1,\n                    interaction.depth = c(2,3), shrinkage = c(0.01,.001), n.bins = c(20), importance = F)\npros.rf <- h2o.randomForest(x=myX,y=myY,data=pros.train,classification=T,ntree=c(5,10),depth=10,mtries=c(2,5),importance=F, type = \"BigData\")\nprint(pros.gbm)\npros.gbm@sumtable\nprint(\"number of models built\")\nnum_models <- length(pros.gbm@sumtable)\nprint(num_models)\n\nprint(\"Scoring\")\nfor ( i in 1:num_models ) {\n  #i=1\n  model <- pros.gbm@model[[i]]\n  pred <- predict( model, pros.test )\n  perf <- h2o.performance ( pred$'1', pros.test$CAPSULE, measure=\"F1\" )\n\n  print ( paste ( pros.gbm@sumtable[[i]]$model_key, \" trees:\", pros.gbm@sumtable[[i]]$n.trees,\n                  \" depth:\", pros.gbm@sumtable[[i]]$interaction.depth,\n                  \" shrinkage:\", pros.gbm@sumtable[[i]]$shrinkage,\n                  \" min row: \", pros.gbm@sumtable[[i]]$n.minobsinnode,\n                  \" bins:\", pros.gbm@sumtable[[i]]$nbins,\n                  \" AUC:\", round(perf@model$AUC, digits=4), sep=''), quote=F)\n}\n\nprint(\" Performance measure on a test set \")\nmodel <- pros.gbm@model[[1]] #  my.glm@models[[80]], pros.rf@model[[1]]\npred <- predict ( model, pros.test )\nperf <- h2o.performance ( pred$'1', pros.test$CAPSULE, measure=\"F1\" )\nprint(perf)\n\ntestEnd()\n}\n\ndoTest(\"Split data into test/train, do Grid search over lambda and Score all the models on a test set and choose the best model by AUC on the test set. Do grid search on gbm and predict on test set, print the AUCs and model params \", test)\n" }
{ "repo_name": "brightchen/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_demos/runit_demo_NOFEATURE_sciworkFlow.R", "content": "#Split data into test/train.\n#Do Grid search over lambda and Score all the models on a test set. Choose the best model by AUC on the test set.\n#Do grid search on gbm and predict on test set. Print the AUCs and model params \"\n\nsetwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource('../h2o-runit.R')\n\n\ntest <- function() {\nprint(\"Reading in prostate dataset\")\npros.hex <- h2o.importFile(normalizePath(locate(\"smalldata/logreg/prostate.csv\")), destination_frame=\"pros.hex\")\nprint (\"Run summary\")\nsummary(pros.hex)\nprint(\"Summary of a column\")\nprint(summary(pros.hex$CAPSULE))\nprint(\"Convert a column to factor\")\npros.hex$CAPSULE <- as.factor(pros.hex$CAPSULE)\nprint(\"print the summary again\")\nprint(summary(pros.hex$CAPSULE))\nprint(\"Print quantile of a column\")\nprint(quantile(pros.hex$AGE,probs=seq(0,1,.1)))\n\nprint(\"split frame into test train\")\n#  a <- h2o.splitFrame(pros.hex,ratios=c(.2),shuffle=T)\n# print(\"print dimension and assign to test and train\")\n# print(dim(a[[1]]))\n# print(dim(a[[2]]))\n# pros.train <- a[[2]]\n# pros.test <- a[[1]]\nsid <- h2o.runif(pros.hex)\npros.train <- pros.hex[sid > 0.2, ]\npros.test <- pros.hex[sid <= 0.2, ]\n\nmyX <- c(\"AGE\",\"RACE\",\"DPROS\",\"DCAPS\",\"PSA\",\"VOL\",\"GLEASON\")\nmyY <- \"CAPSULE\"\n\n#GLM\nprint(\"Build GLM model\")\nmy.glm <- h2o.glm(x=myX, y=myY, training_frame=pros.train, family=\"binomial\",standardize=T,\n  lambda_search=T) #TODO: something return_all_lambdas = T?\nprint(my.glm)\n\nprint(\"This is the best model\")\nbest_model <- my.glm@best_model\nprint(best_model)\n\nprint(\"predict on best lambda model\")\npred <- predict(my.glm@models[[best_model]],pros.test)\nprint(head(pred))\n\nprint(\"print performance and AUC\")\nperf <- h2o.performance(pred$'1',pros.test$CAPSULE )\nprint(perf)\nprint(perf@model$AUC)\nplot(perf,type=\"roc\")\n\nresult_frame <- data.frame(id = 0,auc = 0 , key = 0)\n\nprint(\"print performance for all models on test set\")\nfor(i in 1:100){\n  pred <- predict(my.glm@models[[i]],pros.test)\n  perf <- h2o.performance(pred$'1',pros.test$CAPSULE )\n  print ( paste (\"  model number:\", i, \"  AUC on test set: \", round(perf@model$AUC, digits=4),  sep=''), quote=F)\n  result_frame <- rbind(result_frame, c(i,round(perf@model$AUC, digits=4),my.glm@models[[i]]@key))\n}\n\nresult_frame <- result_frame[-1,]\nresult_frame\nprint(\"order the results by AUC on test set\")\nordered_results <- result_frame[order(result_frame$AUC,decreasing=T),]\nordered_results\nprint(\"get the model that gives the best prediction using the AUC score\")\nglm_best_model <- h2o.getModel(model_id= ordered_results[1,\"key\"])\nprint(glm_best_model)\n\n#GBM\nprint(\"Grid search gbm\")\npros.gbm <- h2o.gbm(x = myX, y = myY, loss = \"bernoulli\", data = pros.train, n.trees = c(50,100),n.minobsinnode=1,\n                    interaction.depth = c(2,3), shrinkage = c(0.01,.001), n.bins = c(20), importance = F)\npros.rf <- h2o.randomForest(x=myX,y=myY,data=pros.train,classification=T,ntree=c(5,10),depth=10,mtries=c(2,5),importance=F, type = \"BigData\")\nprint(pros.gbm)\npros.gbm@sumtable\nprint(\"number of models built\")\nnum_models <- length(pros.gbm@sumtable)\nprint(num_models)\n\nprint(\"Scoring\")\nfor ( i in 1:num_models ) {\n  #i=1\n  model <- pros.gbm@model[[i]]\n  pred <- predict( model, pros.test )\n  perf <- h2o.performance ( pred$'1', pros.test$CAPSULE, measure=\"F1\" )\n\n  print ( paste ( pros.gbm@sumtable[[i]]$model_key, \" trees:\", pros.gbm@sumtable[[i]]$n.trees,\n                  \" depth:\", pros.gbm@sumtable[[i]]$interaction.depth,\n                  \" shrinkage:\", pros.gbm@sumtable[[i]]$shrinkage,\n                  \" min row: \", pros.gbm@sumtable[[i]]$n.minobsinnode,\n                  \" bins:\", pros.gbm@sumtable[[i]]$nbins,\n                  \" AUC:\", round(perf@model$AUC, digits=4), sep=''), quote=F)\n}\n\nprint(\" Performance measure on a test set \")\nmodel <- pros.gbm@model[[1]] #  my.glm@models[[80]], pros.rf@model[[1]]\npred <- predict ( model, pros.test )\nperf <- h2o.performance ( pred$'1', pros.test$CAPSULE, measure=\"F1\" )\nprint(perf)\n\ntestEnd()\n}\n\ndoTest(\"Split data into test/train, do Grid search over lambda and Score all the models on a test set and choose the best model by AUC on the test set. Do grid search on gbm and predict on test set, print the AUCs and model params \", test)\n" }
{ "repo_name": "printedheart/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_demos/runit_demo_NOFEATURE_sciworkFlow.R", "content": "#Split data into test/train.\n#Do Grid search over lambda and Score all the models on a test set. Choose the best model by AUC on the test set.\n#Do grid search on gbm and predict on test set. Print the AUCs and model params \"\n\nsetwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource('../h2o-runit.R')\n\n\ntest <- function() {\nprint(\"Reading in prostate dataset\")\npros.hex <- h2o.importFile(normalizePath(locate(\"smalldata/logreg/prostate.csv\")), destination_frame=\"pros.hex\")\nprint (\"Run summary\")\nsummary(pros.hex)\nprint(\"Summary of a column\")\nprint(summary(pros.hex$CAPSULE))\nprint(\"Convert a column to factor\")\npros.hex$CAPSULE <- as.factor(pros.hex$CAPSULE)\nprint(\"print the summary again\")\nprint(summary(pros.hex$CAPSULE))\nprint(\"Print quantile of a column\")\nprint(quantile(pros.hex$AGE,probs=seq(0,1,.1)))\n\nprint(\"split frame into test train\")\n#  a <- h2o.splitFrame(pros.hex,ratios=c(.2),shuffle=T)\n# print(\"print dimension and assign to test and train\")\n# print(dim(a[[1]]))\n# print(dim(a[[2]]))\n# pros.train <- a[[2]]\n# pros.test <- a[[1]]\nsid <- h2o.runif(pros.hex)\npros.train <- pros.hex[sid > 0.2, ]\npros.test <- pros.hex[sid <= 0.2, ]\n\nmyX <- c(\"AGE\",\"RACE\",\"DPROS\",\"DCAPS\",\"PSA\",\"VOL\",\"GLEASON\")\nmyY <- \"CAPSULE\"\n\n#GLM\nprint(\"Build GLM model\")\nmy.glm <- h2o.glm(x=myX, y=myY, training_frame=pros.train, family=\"binomial\",standardize=T,\n  lambda_search=T) #TODO: something return_all_lambdas = T?\nprint(my.glm)\n\nprint(\"This is the best model\")\nbest_model <- my.glm@best_model\nprint(best_model)\n\nprint(\"predict on best lambda model\")\npred <- predict(my.glm@models[[best_model]],pros.test)\nprint(head(pred))\n\nprint(\"print performance and AUC\")\nperf <- h2o.performance(pred$'1',pros.test$CAPSULE )\nprint(perf)\nprint(perf@model$AUC)\nplot(perf,type=\"roc\")\n\nresult_frame <- data.frame(id = 0,auc = 0 , key = 0)\n\nprint(\"print performance for all models on test set\")\nfor(i in 1:100){\n  pred <- predict(my.glm@models[[i]],pros.test)\n  perf <- h2o.performance(pred$'1',pros.test$CAPSULE )\n  print ( paste (\"  model number:\", i, \"  AUC on test set: \", round(perf@model$AUC, digits=4),  sep=''), quote=F)\n  result_frame <- rbind(result_frame, c(i,round(perf@model$AUC, digits=4),my.glm@models[[i]]@key))\n}\n\nresult_frame <- result_frame[-1,]\nresult_frame\nprint(\"order the results by AUC on test set\")\nordered_results <- result_frame[order(result_frame$AUC,decreasing=T),]\nordered_results\nprint(\"get the model that gives the best prediction using the AUC score\")\nglm_best_model <- h2o.getModel(model_id= ordered_results[1,\"key\"])\nprint(glm_best_model)\n\n#GBM\nprint(\"Grid search gbm\")\npros.gbm <- h2o.gbm(x = myX, y = myY, loss = \"bernoulli\", data = pros.train, n.trees = c(50,100),n.minobsinnode=1,\n                    interaction.depth = c(2,3), shrinkage = c(0.01,.001), n.bins = c(20), importance = F)\npros.rf <- h2o.randomForest(x=myX,y=myY,data=pros.train,classification=T,ntree=c(5,10),depth=10,mtries=c(2,5),importance=F, type = \"BigData\")\nprint(pros.gbm)\npros.gbm@sumtable\nprint(\"number of models built\")\nnum_models <- length(pros.gbm@sumtable)\nprint(num_models)\n\nprint(\"Scoring\")\nfor ( i in 1:num_models ) {\n  #i=1\n  model <- pros.gbm@model[[i]]\n  pred <- predict( model, pros.test )\n  perf <- h2o.performance ( pred$'1', pros.test$CAPSULE, measure=\"F1\" )\n\n  print ( paste ( pros.gbm@sumtable[[i]]$model_key, \" trees:\", pros.gbm@sumtable[[i]]$n.trees,\n                  \" depth:\", pros.gbm@sumtable[[i]]$interaction.depth,\n                  \" shrinkage:\", pros.gbm@sumtable[[i]]$shrinkage,\n                  \" min row: \", pros.gbm@sumtable[[i]]$n.minobsinnode,\n                  \" bins:\", pros.gbm@sumtable[[i]]$nbins,\n                  \" AUC:\", round(perf@model$AUC, digits=4), sep=''), quote=F)\n}\n\nprint(\" Performance measure on a test set \")\nmodel <- pros.gbm@model[[1]] #  my.glm@models[[80]], pros.rf@model[[1]]\npred <- predict ( model, pros.test )\nperf <- h2o.performance ( pred$'1', pros.test$CAPSULE, measure=\"F1\" )\nprint(perf)\n\ntestEnd()\n}\n\ndoTest(\"Split data into test/train, do Grid search over lambda and Score all the models on a test set and choose the best model by AUC on the test set. Do grid search on gbm and predict on test set, print the AUCs and model params \", test)\n" }
{ "repo_name": "datachand/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_demos/runit_demo_NOFEATURE_sciworkFlow.R", "content": "#Split data into test/train.\n#Do Grid search over lambda and Score all the models on a test set. Choose the best model by AUC on the test set.\n#Do grid search on gbm and predict on test set. Print the AUCs and model params \"\n\nsetwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource('../h2o-runit.R')\n\n\ntest <- function() {\nprint(\"Reading in prostate dataset\")\npros.hex <- h2o.importFile(normalizePath(locate(\"smalldata/logreg/prostate.csv\")), destination_frame=\"pros.hex\")\nprint (\"Run summary\")\nsummary(pros.hex)\nprint(\"Summary of a column\")\nprint(summary(pros.hex$CAPSULE))\nprint(\"Convert a column to factor\")\npros.hex$CAPSULE <- as.factor(pros.hex$CAPSULE)\nprint(\"print the summary again\")\nprint(summary(pros.hex$CAPSULE))\nprint(\"Print quantile of a column\")\nprint(quantile(pros.hex$AGE,probs=seq(0,1,.1)))\n\nprint(\"split frame into test train\")\n#  a <- h2o.splitFrame(pros.hex,ratios=c(.2),shuffle=T)\n# print(\"print dimension and assign to test and train\")\n# print(dim(a[[1]]))\n# print(dim(a[[2]]))\n# pros.train <- a[[2]]\n# pros.test <- a[[1]]\nsid <- h2o.runif(pros.hex)\npros.train <- pros.hex[sid > 0.2, ]\npros.test <- pros.hex[sid <= 0.2, ]\n\nmyX <- c(\"AGE\",\"RACE\",\"DPROS\",\"DCAPS\",\"PSA\",\"VOL\",\"GLEASON\")\nmyY <- \"CAPSULE\"\n\n#GLM\nprint(\"Build GLM model\")\nmy.glm <- h2o.glm(x=myX, y=myY, training_frame=pros.train, family=\"binomial\",standardize=T,\n  lambda_search=T) #TODO: something return_all_lambdas = T?\nprint(my.glm)\n\nprint(\"This is the best model\")\nbest_model <- my.glm@best_model\nprint(best_model)\n\nprint(\"predict on best lambda model\")\npred <- predict(my.glm@models[[best_model]],pros.test)\nprint(head(pred))\n\nprint(\"print performance and AUC\")\nperf <- h2o.performance(pred$'1',pros.test$CAPSULE )\nprint(perf)\nprint(perf@model$AUC)\nplot(perf,type=\"roc\")\n\nresult_frame <- data.frame(id = 0,auc = 0 , key = 0)\n\nprint(\"print performance for all models on test set\")\nfor(i in 1:100){\n  pred <- predict(my.glm@models[[i]],pros.test)\n  perf <- h2o.performance(pred$'1',pros.test$CAPSULE )\n  print ( paste (\"  model number:\", i, \"  AUC on test set: \", round(perf@model$AUC, digits=4),  sep=''), quote=F)\n  result_frame <- rbind(result_frame, c(i,round(perf@model$AUC, digits=4),my.glm@models[[i]]@key))\n}\n\nresult_frame <- result_frame[-1,]\nresult_frame\nprint(\"order the results by AUC on test set\")\nordered_results <- result_frame[order(result_frame$AUC,decreasing=T),]\nordered_results\nprint(\"get the model that gives the best prediction using the AUC score\")\nglm_best_model <- h2o.getModel(model_id= ordered_results[1,\"key\"])\nprint(glm_best_model)\n\n#GBM\nprint(\"Grid search gbm\")\npros.gbm <- h2o.gbm(x = myX, y = myY, loss = \"bernoulli\", data = pros.train, n.trees = c(50,100),n.minobsinnode=1,\n                    interaction.depth = c(2,3), shrinkage = c(0.01,.001), n.bins = c(20), importance = F)\npros.rf <- h2o.randomForest(x=myX,y=myY,data=pros.train,classification=T,ntree=c(5,10),depth=10,mtries=c(2,5),importance=F, type = \"BigData\")\nprint(pros.gbm)\npros.gbm@sumtable\nprint(\"number of models built\")\nnum_models <- length(pros.gbm@sumtable)\nprint(num_models)\n\nprint(\"Scoring\")\nfor ( i in 1:num_models ) {\n  #i=1\n  model <- pros.gbm@model[[i]]\n  pred <- predict( model, pros.test )\n  perf <- h2o.performance ( pred$'1', pros.test$CAPSULE, measure=\"F1\" )\n\n  print ( paste ( pros.gbm@sumtable[[i]]$model_key, \" trees:\", pros.gbm@sumtable[[i]]$n.trees,\n                  \" depth:\", pros.gbm@sumtable[[i]]$interaction.depth,\n                  \" shrinkage:\", pros.gbm@sumtable[[i]]$shrinkage,\n                  \" min row: \", pros.gbm@sumtable[[i]]$n.minobsinnode,\n                  \" bins:\", pros.gbm@sumtable[[i]]$nbins,\n                  \" AUC:\", round(perf@model$AUC, digits=4), sep=''), quote=F)\n}\n\nprint(\" Performance measure on a test set \")\nmodel <- pros.gbm@model[[1]] #  my.glm@models[[80]], pros.rf@model[[1]]\npred <- predict ( model, pros.test )\nperf <- h2o.performance ( pred$'1', pros.test$CAPSULE, measure=\"F1\" )\nprint(perf)\n\ntestEnd()\n}\n\ndoTest(\"Split data into test/train, do Grid search over lambda and Score all the models on a test set and choose the best model by AUC on the test set. Do grid search on gbm and predict on test set, print the AUCs and model params \", test)\n" }
{ "repo_name": "mgdlxd/GEO-AWS", "ref": "refs/heads/master", "path": "inst/shiny-apps/shinyGEO/ui/ui.tab.expression.R", "content": "############################################################\n# Expression Profiles\n############################################################\n              ### add a contional panel that only show this once loaded\ntab.expression = tabItem(tabName = \"Home\",\n\t\t conditionalPanel(condition = \"output.sidebarDisplay=='ALL'\",\n                     #  h4(\"Determine if these samples are fair to compare by observing the graphical appearance & profiles values.\"),\n\n#         <h3 class='panel-title' style = \\'font-weight: bold\\'>Primary Contributors</h3>\n\n\n\t\tdiv(class = 'panel panel-default', #style = \"display: inline-block;\",\n\t\t\tdiv(class = 'panel-heading',\n\t\t\t\th4(class = 'panel-title', style = 'font-weight: bold',\"Dataset Summary\")\n\t\t\t),\n\t\t\tdiv(class = 'panel-body',\n\t\t  \t\tuiOutput(\"summary\"),\n\t\t \t  \ta(id = \"normLink\", \"(View expression profiles)\",style=\"cursor:pointer\")\n\t\t\t)\n\t\t)\n\n),\n                # possibly use formatBSModal with applyID = \"saveExp\"\n\t\tbsModal(\"normalizationModal\", \"Expression Profiles\", \"normLink\", size = \"large\",\n                      # radioButtons(\"radio\", label = \"Select a method of log transformation to apply to the data\", \n                       #             choices = list(\"Auto-Detect\" = 1, \"Yes\" = 2, \"No\" = 3), \n                        #            selected = 1, inline = TRUE),\n\t\t\tbsAlert(\"expAlert\"),\n                       plotOutput(\"exProfiles\") \n\t\t),\n\n\t\tDT::dataTableOutput(\"clinicalDataSummarySummary\")\n)\n\n" }
{ "repo_name": "gdancik/GEO-AWS", "ref": "refs/heads/master", "path": "ui/ui.tab.expression.R", "content": "############################################################\n# Expression Profiles\n############################################################\n              ### add a contional panel that only show this once loaded\ntab.expression = tabItem(tabName = \"Home\",\n\t\t conditionalPanel(condition = \"output.sidebarDisplay=='ALL'\",\n                     #  h4(\"Determine if these samples are fair to compare by observing the graphical appearance & profiles values.\"),\n\n#         <h3 class='panel-title' style = \\'font-weight: bold\\'>Primary Contributors</h3>\n\n\n\t\tdiv(class = 'panel panel-default', #style = \"display: inline-block;\",\n\t\t\tdiv(class = 'panel-heading',\n\t\t\t\th4(class = 'panel-title', style = 'font-weight: bold',\"Dataset Summary\")\n\t\t\t),\n\t\t\tdiv(class = 'panel-body',\n\t\t  \t\tuiOutput(\"summary\"),\n\t\t \t  \ta(id = \"normLink\", \"(View expression profiles)\",style=\"cursor:pointer\")\n\t\t\t)\n\t\t)\n\n),\n                # possibly use formatBSModal with applyID = \"saveExp\"\n\t\tbsModal(\"normalizationModal\", \"Expression Profiles\", \"normLink\", size = \"large\",\n                      # radioButtons(\"radio\", label = \"Select a method of log transformation to apply to the data\", \n                       #             choices = list(\"Auto-Detect\" = 1, \"Yes\" = 2, \"No\" = 3), \n                        #            selected = 1, inline = TRUE),\n\t\t\tbsAlert(\"expAlert\"),\n                       plotOutput(\"exProfiles\") \n\t\t),\n\n\t\tDT::dataTableOutput(\"clinicalDataSummarySummary\")\n)\n\n" }
{ "repo_name": "jasdumas/GEO-AWS", "ref": "refs/heads/master", "path": "inst/shiny-apps/shinyGEO/ui/ui.tab.expression.R", "content": "############################################################\n# Expression Profiles\n############################################################\n              ### add a contional panel that only show this once loaded\ntab.expression = tabItem(tabName = \"Home\",\n\t\t conditionalPanel(condition = \"output.sidebarDisplay=='ALL'\",\n                     #  h4(\"Determine if these samples are fair to compare by observing the graphical appearance & profiles values.\"),\n\n#         <h3 class='panel-title' style = \\'font-weight: bold\\'>Primary Contributors</h3>\n\n\n\t\tdiv(class = 'panel panel-default', #style = \"display: inline-block;\",\n\t\t\tdiv(class = 'panel-heading',\n\t\t\t\th4(class = 'panel-title', style = 'font-weight: bold',\"Dataset Summary\")\n\t\t\t),\n\t\t\tdiv(class = 'panel-body',\n\t\t  \t\tuiOutput(\"summary\"),\n\t\t \t  \ta(id = \"normLink\", \"(View expression profiles)\",style=\"cursor:pointer\")\n\t\t\t)\n\t\t)\n\n),\n                # possibly use formatBSModal with applyID = \"saveExp\"\n\t\tbsModal(\"normalizationModal\", \"Expression Profiles\", \"normLink\", size = \"large\",\n                      # radioButtons(\"radio\", label = \"Select a method of log transformation to apply to the data\", \n                       #             choices = list(\"Auto-Detect\" = 1, \"Yes\" = 2, \"No\" = 3), \n                        #            selected = 1, inline = TRUE),\n\t\t\tbsAlert(\"expAlert\"),\n                       plotOutput(\"exProfiles\") \n\t\t),\n\n\t\tDT::dataTableOutput(\"clinicalDataSummarySummary\")\n)\n\n" }
{ "repo_name": "cxxr-devel/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/subset2/tc_subset2_16.R", "content": "expected <- eval(parse(text=\"c(234.289, 259.426, 258.054, 284.599, 328.975, 346.999, 365.385, 363.112, 397.469, 419.18, 442.769, 444.546, 482.704, 502.601, 518.173, 554.894)\"));       \ntest(id=0, code={       \nargv <- eval(parse(text=\"list(structure(list(Employed = c(60.323, 61.122, 60.171, 61.187, 63.221, 63.639, 64.989, 63.761, 66.019, 67.857, 68.169, 66.513, 68.655, 69.564, 69.331, 70.551), GNP.deflator = c(83, 88.5, 88.2, 89.5, 96.2, 98.1, 99, 100, 101.2, 104.6, 108.4, 110.8, 112.6, 114.2, 115.7, 116.9), GNP = c(234.289, 259.426, 258.054, 284.599, 328.975, 346.999, 365.385, 363.112, 397.469, 419.18, 442.769, 444.546, 482.704, 502.601, 518.173, 554.894), Unemployed = c(235.6, 232.5, 368.2, 335.1, 209.9, 193.2, 187, 357.8, 290.4, 282.2, 293.6, 468.1, 381.3, 393.1, 480.6, 400.7), Armed.Forces = c(159, 145.6, 161.6, 165, 309.9, 359.4, 354.7, 335, 304.8, 285.7, 279.8, 263.7, 255.2, 251.4, 257.2, 282.7), Population = c(107.608, 108.632, 109.773, 110.929, 112.075, 113.27, 115.094, 116.219, 117.388, 118.734, 120.445, 121.95, 123.366, 125.368, 127.852, 130.081), Year = 1947:1962), .Names = c(\\\"Employed\\\", \\\"GNP.deflator\\\", \\\"GNP\\\", \\\"Unemployed\\\", \\\"Armed.Forces\\\", \\\"Population\\\", \\\"Year\\\"), class = \\\"data.frame\\\", row.names = 1947:1962, terms = quote(Employed ~     GNP.deflator + GNP + Unemployed + Armed.Forces + Population + Year)), 3L)\"));       \ndo.call(`.subset2`, argv);       \n}, o=expected);       \n\n" }
{ "repo_name": "krlmlr/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/subset2/tc_subset2_16.R", "content": "expected <- eval(parse(text=\"c(234.289, 259.426, 258.054, 284.599, 328.975, 346.999, 365.385, 363.112, 397.469, 419.18, 442.769, 444.546, 482.704, 502.601, 518.173, 554.894)\"));       \ntest(id=0, code={       \nargv <- eval(parse(text=\"list(structure(list(Employed = c(60.323, 61.122, 60.171, 61.187, 63.221, 63.639, 64.989, 63.761, 66.019, 67.857, 68.169, 66.513, 68.655, 69.564, 69.331, 70.551), GNP.deflator = c(83, 88.5, 88.2, 89.5, 96.2, 98.1, 99, 100, 101.2, 104.6, 108.4, 110.8, 112.6, 114.2, 115.7, 116.9), GNP = c(234.289, 259.426, 258.054, 284.599, 328.975, 346.999, 365.385, 363.112, 397.469, 419.18, 442.769, 444.546, 482.704, 502.601, 518.173, 554.894), Unemployed = c(235.6, 232.5, 368.2, 335.1, 209.9, 193.2, 187, 357.8, 290.4, 282.2, 293.6, 468.1, 381.3, 393.1, 480.6, 400.7), Armed.Forces = c(159, 145.6, 161.6, 165, 309.9, 359.4, 354.7, 335, 304.8, 285.7, 279.8, 263.7, 255.2, 251.4, 257.2, 282.7), Population = c(107.608, 108.632, 109.773, 110.929, 112.075, 113.27, 115.094, 116.219, 117.388, 118.734, 120.445, 121.95, 123.366, 125.368, 127.852, 130.081), Year = 1947:1962), .Names = c(\\\"Employed\\\", \\\"GNP.deflator\\\", \\\"GNP\\\", \\\"Unemployed\\\", \\\"Armed.Forces\\\", \\\"Population\\\", \\\"Year\\\"), class = \\\"data.frame\\\", row.names = 1947:1962, terms = quote(Employed ~     GNP.deflator + GNP + Unemployed + Armed.Forces + Population + Year)), 3L)\"));       \ndo.call(`.subset2`, argv);       \n}, o=expected);       \n\n" }
{ "repo_name": "kmillar/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/subset2/tc_subset2_16.R", "content": "expected <- eval(parse(text=\"c(234.289, 259.426, 258.054, 284.599, 328.975, 346.999, 365.385, 363.112, 397.469, 419.18, 442.769, 444.546, 482.704, 502.601, 518.173, 554.894)\"));       \ntest(id=0, code={       \nargv <- eval(parse(text=\"list(structure(list(Employed = c(60.323, 61.122, 60.171, 61.187, 63.221, 63.639, 64.989, 63.761, 66.019, 67.857, 68.169, 66.513, 68.655, 69.564, 69.331, 70.551), GNP.deflator = c(83, 88.5, 88.2, 89.5, 96.2, 98.1, 99, 100, 101.2, 104.6, 108.4, 110.8, 112.6, 114.2, 115.7, 116.9), GNP = c(234.289, 259.426, 258.054, 284.599, 328.975, 346.999, 365.385, 363.112, 397.469, 419.18, 442.769, 444.546, 482.704, 502.601, 518.173, 554.894), Unemployed = c(235.6, 232.5, 368.2, 335.1, 209.9, 193.2, 187, 357.8, 290.4, 282.2, 293.6, 468.1, 381.3, 393.1, 480.6, 400.7), Armed.Forces = c(159, 145.6, 161.6, 165, 309.9, 359.4, 354.7, 335, 304.8, 285.7, 279.8, 263.7, 255.2, 251.4, 257.2, 282.7), Population = c(107.608, 108.632, 109.773, 110.929, 112.075, 113.27, 115.094, 116.219, 117.388, 118.734, 120.445, 121.95, 123.366, 125.368, 127.852, 130.081), Year = 1947:1962), .Names = c(\\\"Employed\\\", \\\"GNP.deflator\\\", \\\"GNP\\\", \\\"Unemployed\\\", \\\"Armed.Forces\\\", \\\"Population\\\", \\\"Year\\\"), class = \\\"data.frame\\\", row.names = 1947:1962, terms = quote(Employed ~     GNP.deflator + GNP + Unemployed + Armed.Forces + Population + Year)), 3L)\"));       \ndo.call(`.subset2`, argv);       \n}, o=expected);       \n\n" }
{ "repo_name": "kmillar/rho", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/subset2/tc_subset2_16.R", "content": "expected <- eval(parse(text=\"c(234.289, 259.426, 258.054, 284.599, 328.975, 346.999, 365.385, 363.112, 397.469, 419.18, 442.769, 444.546, 482.704, 502.601, 518.173, 554.894)\"));       \ntest(id=0, code={       \nargv <- eval(parse(text=\"list(structure(list(Employed = c(60.323, 61.122, 60.171, 61.187, 63.221, 63.639, 64.989, 63.761, 66.019, 67.857, 68.169, 66.513, 68.655, 69.564, 69.331, 70.551), GNP.deflator = c(83, 88.5, 88.2, 89.5, 96.2, 98.1, 99, 100, 101.2, 104.6, 108.4, 110.8, 112.6, 114.2, 115.7, 116.9), GNP = c(234.289, 259.426, 258.054, 284.599, 328.975, 346.999, 365.385, 363.112, 397.469, 419.18, 442.769, 444.546, 482.704, 502.601, 518.173, 554.894), Unemployed = c(235.6, 232.5, 368.2, 335.1, 209.9, 193.2, 187, 357.8, 290.4, 282.2, 293.6, 468.1, 381.3, 393.1, 480.6, 400.7), Armed.Forces = c(159, 145.6, 161.6, 165, 309.9, 359.4, 354.7, 335, 304.8, 285.7, 279.8, 263.7, 255.2, 251.4, 257.2, 282.7), Population = c(107.608, 108.632, 109.773, 110.929, 112.075, 113.27, 115.094, 116.219, 117.388, 118.734, 120.445, 121.95, 123.366, 125.368, 127.852, 130.081), Year = 1947:1962), .Names = c(\\\"Employed\\\", \\\"GNP.deflator\\\", \\\"GNP\\\", \\\"Unemployed\\\", \\\"Armed.Forces\\\", \\\"Population\\\", \\\"Year\\\"), class = \\\"data.frame\\\", row.names = 1947:1962, terms = quote(Employed ~     GNP.deflator + GNP + Unemployed + Armed.Forces + Population + Year)), 3L)\"));       \ndo.call(`.subset2`, argv);       \n}, o=expected);       \n\n" }
{ "repo_name": "ArunChauhan/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/subset2/tc_subset2_16.R", "content": "expected <- eval(parse(text=\"c(234.289, 259.426, 258.054, 284.599, 328.975, 346.999, 365.385, 363.112, 397.469, 419.18, 442.769, 444.546, 482.704, 502.601, 518.173, 554.894)\"));       \ntest(id=0, code={       \nargv <- eval(parse(text=\"list(structure(list(Employed = c(60.323, 61.122, 60.171, 61.187, 63.221, 63.639, 64.989, 63.761, 66.019, 67.857, 68.169, 66.513, 68.655, 69.564, 69.331, 70.551), GNP.deflator = c(83, 88.5, 88.2, 89.5, 96.2, 98.1, 99, 100, 101.2, 104.6, 108.4, 110.8, 112.6, 114.2, 115.7, 116.9), GNP = c(234.289, 259.426, 258.054, 284.599, 328.975, 346.999, 365.385, 363.112, 397.469, 419.18, 442.769, 444.546, 482.704, 502.601, 518.173, 554.894), Unemployed = c(235.6, 232.5, 368.2, 335.1, 209.9, 193.2, 187, 357.8, 290.4, 282.2, 293.6, 468.1, 381.3, 393.1, 480.6, 400.7), Armed.Forces = c(159, 145.6, 161.6, 165, 309.9, 359.4, 354.7, 335, 304.8, 285.7, 279.8, 263.7, 255.2, 251.4, 257.2, 282.7), Population = c(107.608, 108.632, 109.773, 110.929, 112.075, 113.27, 115.094, 116.219, 117.388, 118.734, 120.445, 121.95, 123.366, 125.368, 127.852, 130.081), Year = 1947:1962), .Names = c(\\\"Employed\\\", \\\"GNP.deflator\\\", \\\"GNP\\\", \\\"Unemployed\\\", \\\"Armed.Forces\\\", \\\"Population\\\", \\\"Year\\\"), class = \\\"data.frame\\\", row.names = 1947:1962, terms = quote(Employed ~     GNP.deflator + GNP + Unemployed + Armed.Forces + Population + Year)), 3L)\"));       \ndo.call(`.subset2`, argv);       \n}, o=expected);       \n\n" }
{ "repo_name": "rho-devel/rho", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/subset2/tc_subset2_16.R", "content": "expected <- eval(parse(text=\"c(234.289, 259.426, 258.054, 284.599, 328.975, 346.999, 365.385, 363.112, 397.469, 419.18, 442.769, 444.546, 482.704, 502.601, 518.173, 554.894)\"));       \ntest(id=0, code={       \nargv <- eval(parse(text=\"list(structure(list(Employed = c(60.323, 61.122, 60.171, 61.187, 63.221, 63.639, 64.989, 63.761, 66.019, 67.857, 68.169, 66.513, 68.655, 69.564, 69.331, 70.551), GNP.deflator = c(83, 88.5, 88.2, 89.5, 96.2, 98.1, 99, 100, 101.2, 104.6, 108.4, 110.8, 112.6, 114.2, 115.7, 116.9), GNP = c(234.289, 259.426, 258.054, 284.599, 328.975, 346.999, 365.385, 363.112, 397.469, 419.18, 442.769, 444.546, 482.704, 502.601, 518.173, 554.894), Unemployed = c(235.6, 232.5, 368.2, 335.1, 209.9, 193.2, 187, 357.8, 290.4, 282.2, 293.6, 468.1, 381.3, 393.1, 480.6, 400.7), Armed.Forces = c(159, 145.6, 161.6, 165, 309.9, 359.4, 354.7, 335, 304.8, 285.7, 279.8, 263.7, 255.2, 251.4, 257.2, 282.7), Population = c(107.608, 108.632, 109.773, 110.929, 112.075, 113.27, 115.094, 116.219, 117.388, 118.734, 120.445, 121.95, 123.366, 125.368, 127.852, 130.081), Year = 1947:1962), .Names = c(\\\"Employed\\\", \\\"GNP.deflator\\\", \\\"GNP\\\", \\\"Unemployed\\\", \\\"Armed.Forces\\\", \\\"Population\\\", \\\"Year\\\"), class = \\\"data.frame\\\", row.names = 1947:1962, terms = quote(Employed ~     GNP.deflator + GNP + Unemployed + Armed.Forces + Population + Year)), 3L)\"));       \ndo.call(`.subset2`, argv);       \n}, o=expected);       \n\n" }
{ "repo_name": "nkhuyu/kaggle-avito-1", "ref": "refs/heads/master", "path": "_full_100pct_run.R", "content": "# run entire script with 100% data \n\nsetwd('/home/fast/2015_avito') # modify this line to point the right folder\n\nrequire(inline)\nrequire(sqldf)\nrequire(data.table)\nrequire(xgboost)\nrequire(tau)\n\nsource('kaggle-avito/avito_utils.R')\n\nfull_data <- T\nsource('kaggle-avito/avito_data1.R')\n\nsource('kaggle-avito/avito_phone.R')\n\nsource('kaggle-avito/avito_search.R')\n\nsource('kaggle-avito/avito_visit.R')\n\nsource('kaggle-avito/avito_cat_cnt.R')\n\nsource('kaggle-avito/avito_data_merge.R')\n\nn_repeat <- 8\n\n#one model with shorter feature list and all the data points\nrseq_limit <- 1e6\nxt <- 0\ngc()\nrseed_offset <- 200\nfeature_list <- feature_list1\nsource('kaggle-avito/avito_train_xgb.R')\npredv_list1 <- predv / ctr\npredh_list1 <- predh / ctr\n\n#another model with longer feature_list but only last 200 impressions\nrseq_limit <- 200\nxt <- 0\ngc()\nrseed_offset <- 300\nfeature_list <- c(feature_list1, feature_list2)\nsource('kaggle-avito/avito_train_xgb.R')\npredv_list2_r200 <- predv / ctr\npredh_list2_r200 <- predh / ctr\n\n#test on 2% validation data\npredv_avg2 <- (predv_list1 + predv_list2_r200) / 2\nprint(paste(\"logloss of avg of two xgbs (should be ~.04252): \", logloss(yv, predv_avg2)))\n\n#create final submission\npredh_avg2 <- (predh_list1 + predh_list2_r200) / 2\npredh.dt <- data.frame(ID=dt1$ID[fh], IsClick=round(predh_avg2,6))\nmy_write_csv(predh.dt, file='avito_sub_finalx8_final', row.names=F) #private LB ~.04029\n" }
{ "repo_name": "owenzhang/kaggle-avito", "ref": "refs/heads/master", "path": "_full_100pct_run.R", "content": "# run entire script with 100% data \n\nsetwd('/home/fast/2015_avito') # modify this line to point the right folder\n\nrequire(inline)\nrequire(sqldf)\nrequire(data.table)\nrequire(xgboost)\nrequire(tau)\n\nsource('kaggle-avito/avito_utils.R')\n\nfull_data <- T\nsource('kaggle-avito/avito_data1.R')\n\nsource('kaggle-avito/avito_phone.R')\n\nsource('kaggle-avito/avito_search.R')\n\nsource('kaggle-avito/avito_visit.R')\n\nsource('kaggle-avito/avito_cat_cnt.R')\n\nsource('kaggle-avito/avito_data_merge.R')\n\nn_repeat <- 8\n\n#one model with shorter feature list and all the data points\nrseq_limit <- 1e6\nxt <- 0\ngc()\nrseed_offset <- 200\nfeature_list <- feature_list1\nsource('kaggle-avito/avito_train_xgb.R')\npredv_list1 <- predv / ctr\npredh_list1 <- predh / ctr\n\n#another model with longer feature_list but only last 200 impressions\nrseq_limit <- 200\nxt <- 0\ngc()\nrseed_offset <- 300\nfeature_list <- c(feature_list1, feature_list2)\nsource('kaggle-avito/avito_train_xgb.R')\npredv_list2_r200 <- predv / ctr\npredh_list2_r200 <- predh / ctr\n\n#test on 2% validation data\npredv_avg2 <- (predv_list1 + predv_list2_r200) / 2\nprint(paste(\"logloss of avg of two xgbs (should be ~.04252): \", logloss(yv, predv_avg2)))\n\n#create final submission\npredh_avg2 <- (predh_list1 + predh_list2_r200) / 2\npredh.dt <- data.frame(ID=dt1$ID[fh], IsClick=round(predh_avg2,6))\nmy_write_csv(predh.dt, file='avito_sub_finalx8_final', row.names=F) #private LB ~.04029\n" }
{ "repo_name": "minhpqn/kaggle-avito", "ref": "refs/heads/master", "path": "_full_100pct_run.R", "content": "# run entire script with 100% data \n\nsetwd('/home/fast/2015_avito') # modify this line to point the right folder\n\nrequire(inline)\nrequire(sqldf)\nrequire(data.table)\nrequire(xgboost)\nrequire(tau)\n\nsource('kaggle-avito/avito_utils.R')\n\nfull_data <- T\nsource('kaggle-avito/avito_data1.R')\n\nsource('kaggle-avito/avito_phone.R')\n\nsource('kaggle-avito/avito_search.R')\n\nsource('kaggle-avito/avito_visit.R')\n\nsource('kaggle-avito/avito_cat_cnt.R')\n\nsource('kaggle-avito/avito_data_merge.R')\n\nn_repeat <- 8\n\n#one model with shorter feature list and all the data points\nrseq_limit <- 1e6\nxt <- 0\ngc()\nrseed_offset <- 200\nfeature_list <- feature_list1\nsource('kaggle-avito/avito_train_xgb.R')\npredv_list1 <- predv / ctr\npredh_list1 <- predh / ctr\n\n#another model with longer feature_list but only last 200 impressions\nrseq_limit <- 200\nxt <- 0\ngc()\nrseed_offset <- 300\nfeature_list <- c(feature_list1, feature_list2)\nsource('kaggle-avito/avito_train_xgb.R')\npredv_list2_r200 <- predv / ctr\npredh_list2_r200 <- predh / ctr\n\n#test on 2% validation data\npredv_avg2 <- (predv_list1 + predv_list2_r200) / 2\nprint(paste(\"logloss of avg of two xgbs (should be ~.04252): \", logloss(yv, predv_avg2)))\n\n#create final submission\npredh_avg2 <- (predh_list1 + predh_list2_r200) / 2\npredh.dt <- data.frame(ID=dt1$ID[fh], IsClick=round(predh_avg2,6))\nmy_write_csv(predh.dt, file='avito_sub_finalx8_final', row.names=F) #private LB ~.04029\n" }
{ "repo_name": "ChenglongChen/kaggle-avito", "ref": "refs/heads/master", "path": "_full_100pct_run.R", "content": "# run entire script with 100% data \n\nsetwd('/home/fast/2015_avito') # modify this line to point the right folder\n\nrequire(inline)\nrequire(sqldf)\nrequire(data.table)\nrequire(xgboost)\nrequire(tau)\n\nsource('kaggle-avito/avito_utils.R')\n\nfull_data <- T\nsource('kaggle-avito/avito_data1.R')\n\nsource('kaggle-avito/avito_phone.R')\n\nsource('kaggle-avito/avito_search.R')\n\nsource('kaggle-avito/avito_visit.R')\n\nsource('kaggle-avito/avito_cat_cnt.R')\n\nsource('kaggle-avito/avito_data_merge.R')\n\nn_repeat <- 8\n\n#one model with shorter feature list and all the data points\nrseq_limit <- 1e6\nxt <- 0\ngc()\nrseed_offset <- 200\nfeature_list <- feature_list1\nsource('kaggle-avito/avito_train_xgb.R')\npredv_list1 <- predv / ctr\npredh_list1 <- predh / ctr\n\n#another model with longer feature_list but only last 200 impressions\nrseq_limit <- 200\nxt <- 0\ngc()\nrseed_offset <- 300\nfeature_list <- c(feature_list1, feature_list2)\nsource('kaggle-avito/avito_train_xgb.R')\npredv_list2_r200 <- predv / ctr\npredh_list2_r200 <- predh / ctr\n\n#test on 2% validation data\npredv_avg2 <- (predv_list1 + predv_list2_r200) / 2\nprint(paste(\"logloss of avg of two xgbs (should be ~.04252): \", logloss(yv, predv_avg2)))\n\n#create final submission\npredh_avg2 <- (predh_list1 + predh_list2_r200) / 2\npredh.dt <- data.frame(ID=dt1$ID[fh], IsClick=round(predh_avg2,6))\nmy_write_csv(predh.dt, file='avito_sub_finalx8_final', row.names=F) #private LB ~.04029\n" }
{ "repo_name": "followyourheart/kaggle-avito", "ref": "refs/heads/master", "path": "_full_100pct_run.R", "content": "# run entire script with 100% data \n\nsetwd('/home/fast/2015_avito') # modify this line to point the right folder\n\nrequire(inline)\nrequire(sqldf)\nrequire(data.table)\nrequire(xgboost)\nrequire(tau)\n\nsource('kaggle-avito/avito_utils.R')\n\nfull_data <- T\nsource('kaggle-avito/avito_data1.R')\n\nsource('kaggle-avito/avito_phone.R')\n\nsource('kaggle-avito/avito_search.R')\n\nsource('kaggle-avito/avito_visit.R')\n\nsource('kaggle-avito/avito_cat_cnt.R')\n\nsource('kaggle-avito/avito_data_merge.R')\n\nn_repeat <- 8\n\n#one model with shorter feature list and all the data points\nrseq_limit <- 1e6\nxt <- 0\ngc()\nrseed_offset <- 200\nfeature_list <- feature_list1\nsource('kaggle-avito/avito_train_xgb.R')\npredv_list1 <- predv / ctr\npredh_list1 <- predh / ctr\n\n#another model with longer feature_list but only last 200 impressions\nrseq_limit <- 200\nxt <- 0\ngc()\nrseed_offset <- 300\nfeature_list <- c(feature_list1, feature_list2)\nsource('kaggle-avito/avito_train_xgb.R')\npredv_list2_r200 <- predv / ctr\npredh_list2_r200 <- predh / ctr\n\n#test on 2% validation data\npredv_avg2 <- (predv_list1 + predv_list2_r200) / 2\nprint(paste(\"logloss of avg of two xgbs (should be ~.04252): \", logloss(yv, predv_avg2)))\n\n#create final submission\npredh_avg2 <- (predh_list1 + predh_list2_r200) / 2\npredh.dt <- data.frame(ID=dt1$ID[fh], IsClick=round(predh_avg2,6))\nmy_write_csv(predh.dt, file='avito_sub_finalx8_final', row.names=F) #private LB ~.04029\n" }
{ "repo_name": "thanhleviet/kaggle-avito", "ref": "refs/heads/master", "path": "_full_100pct_run.R", "content": "# run entire script with 100% data \n\nsetwd('/home/fast/2015_avito') # modify this line to point the right folder\n\nrequire(inline)\nrequire(sqldf)\nrequire(data.table)\nrequire(xgboost)\nrequire(tau)\n\nsource('kaggle-avito/avito_utils.R')\n\nfull_data <- T\nsource('kaggle-avito/avito_data1.R')\n\nsource('kaggle-avito/avito_phone.R')\n\nsource('kaggle-avito/avito_search.R')\n\nsource('kaggle-avito/avito_visit.R')\n\nsource('kaggle-avito/avito_cat_cnt.R')\n\nsource('kaggle-avito/avito_data_merge.R')\n\nn_repeat <- 8\n\n#one model with shorter feature list and all the data points\nrseq_limit <- 1e6\nxt <- 0\ngc()\nrseed_offset <- 200\nfeature_list <- feature_list1\nsource('kaggle-avito/avito_train_xgb.R')\npredv_list1 <- predv / ctr\npredh_list1 <- predh / ctr\n\n#another model with longer feature_list but only last 200 impressions\nrseq_limit <- 200\nxt <- 0\ngc()\nrseed_offset <- 300\nfeature_list <- c(feature_list1, feature_list2)\nsource('kaggle-avito/avito_train_xgb.R')\npredv_list2_r200 <- predv / ctr\npredh_list2_r200 <- predh / ctr\n\n#test on 2% validation data\npredv_avg2 <- (predv_list1 + predv_list2_r200) / 2\nprint(paste(\"logloss of avg of two xgbs (should be ~.04252): \", logloss(yv, predv_avg2)))\n\n#create final submission\npredh_avg2 <- (predh_list1 + predh_list2_r200) / 2\npredh.dt <- data.frame(ID=dt1$ID[fh], IsClick=round(predh_avg2,6))\nmy_write_csv(predh.dt, file='avito_sub_finalx8_final', row.names=F) #private LB ~.04029\n" }
{ "repo_name": "wavelets/kaggle-avito", "ref": "refs/heads/master", "path": "_full_100pct_run.R", "content": "# run entire script with 100% data \n\nsetwd('/home/fast/2015_avito') # modify this line to point the right folder\n\nrequire(inline)\nrequire(sqldf)\nrequire(data.table)\nrequire(xgboost)\nrequire(tau)\n\nsource('kaggle-avito/avito_utils.R')\n\nfull_data <- T\nsource('kaggle-avito/avito_data1.R')\n\nsource('kaggle-avito/avito_phone.R')\n\nsource('kaggle-avito/avito_search.R')\n\nsource('kaggle-avito/avito_visit.R')\n\nsource('kaggle-avito/avito_cat_cnt.R')\n\nsource('kaggle-avito/avito_data_merge.R')\n\nn_repeat <- 8\n\n#one model with shorter feature list and all the data points\nrseq_limit <- 1e6\nxt <- 0\ngc()\nrseed_offset <- 200\nfeature_list <- feature_list1\nsource('kaggle-avito/avito_train_xgb.R')\npredv_list1 <- predv / ctr\npredh_list1 <- predh / ctr\n\n#another model with longer feature_list but only last 200 impressions\nrseq_limit <- 200\nxt <- 0\ngc()\nrseed_offset <- 300\nfeature_list <- c(feature_list1, feature_list2)\nsource('kaggle-avito/avito_train_xgb.R')\npredv_list2_r200 <- predv / ctr\npredh_list2_r200 <- predh / ctr\n\n#test on 2% validation data\npredv_avg2 <- (predv_list1 + predv_list2_r200) / 2\nprint(paste(\"logloss of avg of two xgbs (should be ~.04252): \", logloss(yv, predv_avg2)))\n\n#create final submission\npredh_avg2 <- (predh_list1 + predh_list2_r200) / 2\npredh.dt <- data.frame(ID=dt1$ID[fh], IsClick=round(predh_avg2,6))\nmy_write_csv(predh.dt, file='avito_sub_finalx8_final', row.names=F) #private LB ~.04029\n" }
{ "repo_name": "ibl/vcf", "ref": "refs/heads/gh-pages", "path": "R/lala.R", "content": "lala" }
{ "repo_name": "sammorris81/rare-binary", "ref": "refs/heads/master", "path": "code/analysis/birds/western_bluebird_15.R", "content": "rm(list=ls())\nsource(file = \"./package_load.R\", chdir = T)\nspecies <- \"western_bluebird\"\ncv <- 1\nknot.percent <- 15\nsource(file = \"./fitmodel.R\")\n\nrm(list = ls())\nsource(file = \"./package_load.R\", chdir = T)\nspecies <- \"western_bluebird\"\ncv <- 2\nknot.percent <- 15\nsource(file = \"./fitmodel.R\")" }
{ "repo_name": "scollinspt/R_code", "ref": "refs/heads/master", "path": "Book_Codes/Ch.10/10.3_Matching.R", "content": "## Read in the data\r\n# Data are at http://www.stat.columbia.edu/~gelman/arm/examples/?\r\n?data\r\n\r\nlibrary (\"arm\")\r\n\r\nattach.all ()\r\n\r\n## Computation of propensity score matches\r\n\r\n # starting point\r\n\r\nps.fit.1 <- glm (treat ~ as.factor(educ) + as.factor(ethnic) + b.marr +\r\n   work.dur + prenatal + mom.age + sex + first + preterm + age + \r\n   dayskidh + bw + unemp.rt, data=cc2, family=binomial(link=\"logit\"))\r\n\r\n # second model\r\n\r\nps.fit.2 <- glm (treat ~ bwg + as.factor(educ) + bwg:as.factor(educ) +\r\n   as.factor(ethnic) + b.marr + as.factor(ethnic):b.marr + work.dur + \r\n   prenatal + preterm + age + mom.age + sex + first, data=cc2,\r\n   family=binomial(link=\"logit\"))\r\n\r\n # predicted values\r\n\r\npscores <- matching (z=cc2$treat, score=pscores)\r\nmatched <- cc2[matches$matched,]\r\n\r\n # regression on matched data\r\n\r\nreg.ps <- lm (ppvtr.36 ~ treat + hispanic + black + b.marr + lths + hs +\r\n   ltcoll + work.dur + prenatal + mom.age + sex + first + preterm + age +\r\n   dayskidh + bw, data=matched)\r\n\r\n## Propensity score as a one-number summary (Figure 10.7)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n" }
{ "repo_name": "rvladimiro/dafR", "ref": "refs/heads/master", "path": "R/helperGetS3Config.R", "content": "#' @export\n#' @name GetS3Config\n#' @author Ricardo Vladimiro\n#' @title Get S3 server configuration from a given a yaml configuration file\n#' @param id The server name in the yaml config file\n#' @param yamlFile The yaml configuration file with the S3 information\n#' @return A list of configuration parameters\nGetS3Config <- function(id, yamlFile) {\n    \n    # Load the configuration\n    config <- GetYAMLConfig(id, yamlFile)\n    \n    # Check if all server information needed is present\n    nOfErrors = 0\n    \n    if(is.null(config$accessKey)) {\n        Shout('Access Key not present for ', id)\n        nOfErrors = nOfErrors + 1\n    }\n    \n    if(is.null(config$secretKey)) {\n        Shout('Secret Key not present for ', id)\n        nOfErrors = nOfErrors + 1\n    }\n    \n    if(is.null(config$folder)) {\n        Shout('Folder not present for ', id)\n        nOfErrors = nOfErrors + 1\n    }\n    \n    # If any error was found stop execution\n    if(nOfErrors > 0) {\n        stop(paste(nOfErrors, 'error(s) found. Review yaml config file.'))\n    }\n    \n    # Return the config\n    return(config)\n    \n}" }
{ "repo_name": "PietaSchofield/pietalib", "ref": "refs/heads/master", "path": "R/pietalib.R", "content": "#' pietalib\n#'\n#' my useful functions\n#'\n#'\n#' @docType package\n#' @name pietalib\n#'\nNULL\n" }
{ "repo_name": "hmorzaria/Biodiversity", "ref": "refs/heads/master", "path": "vertices_poligonos_pacifico.R", "content": "#C:\\Users\\Gaby Cruz\\Desktop\\SIG\\vertices_pacific_9km.txt\nsetwd(\"C:/Users/Gaby Cruz/Desktop/SIG\")\nfile=read.csv(\"vertices_pacific_9km.txt\");\n#4long #5lat\n1273665/5\ndatos =matrix(1:1273665, ncol=1)\n#1273665  #482795\ncontador=0\nfor (i in 1:1273665)\n{\n  coords=paste(file[contador*5+1,4],file[contador*5+1,5],file[contador*5+2,4],file[contador*5+2,5],file[contador*5+3,4],file[contador*5+3,5],file[contador*5+4,4],file[contador*5+4,5],file[contador*5+5,4],file[contador*5+5,5], sep=\",\")\n  datos[i]=paste(\"POLYGON((\",coords,\"))\", sep=\"\")\n  contador=contador+1\n  print(contador/1273665)\n  \n}\nwrite.csv(datos,\"vertices_pacific.csv\")" }
{ "repo_name": "stevekogo/Climsoft", "ref": "refs/heads/master", "path": "ClimsoftVer4/ClimsoftVer4/static/ClimateObject/R/ClimateMethods/DataManipulation/display_spell_length_method.R", "content": "#==================================================================================================\n# Climatic Extremes\n#' @title Spell Length Table.\n#' @name display_spell_length\n#' @author Fanuel and Steve 2015 (AMI)\n\n#' @description  \n#' Displays the spell length table per year per month.\n#' @param col_name  The name of the spell length column.  \n#' @param months_list The names of the months.\n#' @param day_display The name of the first column in th table.\n#' @param na.rm  A logical indicating whether missing values should be removed.\n#' @examples\n#' ClimateObj <- climate( data_tables = list( dataframe=dataframe ), date_formats = list( \"%m/%d/%Y\" ) )\n#' Default dateformats: \"%Y/%m/%d\"\n#' where \"data\" is a data.frame containing the desired data to be computed.\n#' climateObj$display_spell_length().\n#' @return return yearly tables of spell lengths.\nclimate$methods(display_spell_length = function(data_list = list(), col_name = \"spell length\", na.rm=TRUE, threshold=0.85, \n                                                months_list = month.abb, day_display = \"Day\"){\n  \n  data_list=add_to_data_info_required_variable_list(data_list, list(rain_label))\n  \n  data_list=add_to_data_info_time_period(data_list, daily_label)\n  \n  rettables = list()\n  \n  climate_data_objs = get_climate_data_objects(data_list)\n  \n  for(data_obj in climate_data_objs) {\n    \n    if( !(data_obj$is_present(spell_length_label)) ) {\n      data_obj$add_spell_length_col(col_name=col_name, threshold = threshold)\n    }\n    spell_length_col = data_obj$getvname(spell_length_label)\n\n    if( !(data_obj$is_present(year_label) && data_obj$is_present(month_label) && data_obj$is_present(day_label)) ) {\n      data_obj$add_year_month_day_cols()\n    }\n    \n    year_col = data_obj$getvname(year_label)\n    month_col = data_obj$getvname(month_label)\n    day_col = data_obj$getvname(day_label)\n    \n    curr_data_list = data_obj$get_data_for_analysis(data_list)\n    \n    for( curr_data in curr_data_list ) {\n      \n      for (k in 1:length(curr_data[[spell_length_col]])){\n        if (is.na(curr_data[[spell_length_col]][[k]])){\n          curr_data[[spell_length_col]][[k]]=\"m\" \n        }        \n      }\n\n      tables = list()\n      years_split <- split(curr_data, list(as.factor(curr_data[[year_col]])))\n      \n      # initialize the counter\n      i = 1\n      for ( single_year in years_split ) {\n        tables[[i]] <- dcast(single_year, single_year[[ day_col ]]~single_year[[ month_col ]], value.var = spell_length_col)\n        \n        end = length(colnames(tables[[i]]))\n        names(tables[[i]])[ 1 ] <- day_display\n        colnames(tables[[i]])[2:end] <- months_list[1:end-1]\n        \n        Day = \" \"\n        dat1 = tables[[i]][,2:13]\n        dat2 = sapply(dat1[,1:12], as.numeric)\n        summ =suppressWarnings(as.data.frame(lapply(as.data.frame(dat2), max, na.rm = na.rm)))\n        overall_max = c(\"Maximum\",rep(NA,10),\"(Overall:\",paste(max(summ, na.rm=na.rm),\")\",sep = \"\"))\n        tables[[i]] = rbind(tables[[i]], overall_max)\n        tables[[i]] = join(tables[[i]], cbind(Day,summ), by = c(\"Day\",month.abb), type = \"full\", match = \"all\")\n        missing = c(\" \",rep(NA,13))\n        \n        for(j in 2:13){\n          sa = which(tables[[i]][,j] %in% 0)\n          sb = which(tables[[i]][,j] %in% NA)\n          sc = which(tables[[i]][,j] %in% -Inf)\n          sf = which(tables[[i]][,j] %in% \"m\")\n          tables[[i]][sa,j]<-\"--\"\n          tables[[i]][sb,j]<-\" \"\n          tables[[i]][sc,j]<-\"m\"\n          missing[j] <- length(sf)\n        }\n        overall_miss_max = c(\"Missing\",rep(\" \",10),\"(Overall:\",paste(sum(as.numeric(missing[2:13]), na.rm=na.rm),\")\",sep = \"\"))\n        if(max(as.numeric(missing[2:13]), na.rm=na.rm)>0){\n          tables[[i]] = rbind(tables[[i]], overall_miss_max, missing)\n        }\n        i = i + 1\n      }\n\n      names(tables) <- names(years_split)\n      rettables[[data_obj$get_station_data( curr_data, station_label )]] = tables\n    }\n    \n    return(tables)\n    \n  }\n  \n}\n)" }
{ "repo_name": "Ahmadou2015/Climsoft", "ref": "refs/heads/master", "path": "ClimsoftVer4/ClimsoftVer4/static/ClimateObject/R/ClimateMethods/DataManipulation/display_spell_length_method.R", "content": "#==================================================================================================\n# Climatic Extremes\n#' @title Spell Length Table.\n#' @name display_spell_length\n#' @author Fanuel and Steve 2015 (AMI)\n\n#' @description  \n#' Displays the spell length table per year per month.\n#' @param col_name  The name of the spell length column.  \n#' @param months_list The names of the months.\n#' @param day_display The name of the first column in th table.\n#' @param na.rm  A logical indicating whether missing values should be removed.\n#' @examples\n#' ClimateObj <- climate( data_tables = list( dataframe=dataframe ), date_formats = list( \"%m/%d/%Y\" ) )\n#' Default dateformats: \"%Y/%m/%d\"\n#' where \"data\" is a data.frame containing the desired data to be computed.\n#' climateObj$display_spell_length().\n#' @return return yearly tables of spell lengths.\nclimate$methods(display_spell_length = function(data_list = list(), col_name = \"spell length\", na.rm=TRUE, threshold=0.85, \n                                                months_list = month.abb, day_display = \"Day\"){\n  \n  data_list=add_to_data_info_required_variable_list(data_list, list(rain_label))\n  \n  data_list=add_to_data_info_time_period(data_list, daily_label)\n  \n  rettables = list()\n  \n  climate_data_objs = get_climate_data_objects(data_list)\n  \n  for(data_obj in climate_data_objs) {\n    \n    if( !(data_obj$is_present(spell_length_label)) ) {\n      data_obj$add_spell_length_col(col_name=col_name, threshold = threshold)\n    }\n    spell_length_col = data_obj$getvname(spell_length_label)\n\n    if( !(data_obj$is_present(year_label) && data_obj$is_present(month_label) && data_obj$is_present(day_label)) ) {\n      data_obj$add_year_month_day_cols()\n    }\n    \n    year_col = data_obj$getvname(year_label)\n    month_col = data_obj$getvname(month_label)\n    day_col = data_obj$getvname(day_label)\n    \n    curr_data_list = data_obj$get_data_for_analysis(data_list)\n    \n    for( curr_data in curr_data_list ) {\n      \n      for (k in 1:length(curr_data[[spell_length_col]])){\n        if (is.na(curr_data[[spell_length_col]][[k]])){\n          curr_data[[spell_length_col]][[k]]=\"m\" \n        }        \n      }\n\n      tables = list()\n      years_split <- split(curr_data, list(as.factor(curr_data[[year_col]])))\n      \n      # initialize the counter\n      i = 1\n      for ( single_year in years_split ) {\n        tables[[i]] <- dcast(single_year, single_year[[ day_col ]]~single_year[[ month_col ]], value.var = spell_length_col)\n        \n        end = length(colnames(tables[[i]]))\n        names(tables[[i]])[ 1 ] <- day_display\n        colnames(tables[[i]])[2:end] <- months_list[1:end-1]\n        \n        Day = \" \"\n        dat1 = tables[[i]][,2:13]\n        dat2 = sapply(dat1[,1:12], as.numeric)\n        summ =suppressWarnings(as.data.frame(lapply(as.data.frame(dat2), max, na.rm = na.rm)))\n        overall_max = c(\"Maximum\",rep(NA,10),\"(Overall:\",paste(max(summ, na.rm=na.rm),\")\",sep = \"\"))\n        tables[[i]] = rbind(tables[[i]], overall_max)\n        tables[[i]] = join(tables[[i]], cbind(Day,summ), by = c(\"Day\",month.abb), type = \"full\", match = \"all\")\n        missing = c(\" \",rep(NA,13))\n        \n        for(j in 2:13){\n          sa = which(tables[[i]][,j] %in% 0)\n          sb = which(tables[[i]][,j] %in% NA)\n          sc = which(tables[[i]][,j] %in% -Inf)\n          sf = which(tables[[i]][,j] %in% \"m\")\n          tables[[i]][sa,j]<-\"--\"\n          tables[[i]][sb,j]<-\" \"\n          tables[[i]][sc,j]<-\"m\"\n          missing[j] <- length(sf)\n        }\n        overall_miss_max = c(\"Missing\",rep(\" \",10),\"(Overall:\",paste(sum(as.numeric(missing[2:13]), na.rm=na.rm),\")\",sep = \"\"))\n        if(max(as.numeric(missing[2:13]), na.rm=na.rm)>0){\n          tables[[i]] = rbind(tables[[i]], overall_miss_max, missing)\n        }\n        i = i + 1\n      }\n\n      names(tables) <- names(years_split)\n      rettables[[data_obj$get_station_data( curr_data, station_label )]] = tables\n    }\n    \n    return(tables)\n    \n  }\n  \n}\n)" }
{ "repo_name": "mhabimana/Climsoft", "ref": "refs/heads/master", "path": "ClimsoftVer4/ClimsoftVer4/static/ClimateObject/R/ClimateMethods/DataManipulation/display_spell_length_method.R", "content": "#==================================================================================================\n# Climatic Extremes\n#' @title Spell Length Table.\n#' @name display_spell_length\n#' @author Fanuel and Steve 2015 (AMI)\n\n#' @description  \n#' Displays the spell length table per year per month.\n#' @param col_name  The name of the spell length column.  \n#' @param months_list The names of the months.\n#' @param day_display The name of the first column in th table.\n#' @param na.rm  A logical indicating whether missing values should be removed.\n#' @examples\n#' ClimateObj <- climate( data_tables = list( dataframe=dataframe ), date_formats = list( \"%m/%d/%Y\" ) )\n#' Default dateformats: \"%Y/%m/%d\"\n#' where \"data\" is a data.frame containing the desired data to be computed.\n#' climateObj$display_spell_length().\n#' @return return yearly tables of spell lengths.\nclimate$methods(display_spell_length = function(data_list = list(), col_name = \"spell length\", na.rm=TRUE, threshold=0.85, \n                                                months_list = month.abb, day_display = \"Day\"){\n  \n  data_list=add_to_data_info_required_variable_list(data_list, list(rain_label))\n  \n  data_list=add_to_data_info_time_period(data_list, daily_label)\n  \n  rettables = list()\n  \n  climate_data_objs = get_climate_data_objects(data_list)\n  \n  for(data_obj in climate_data_objs) {\n    \n    if( !(data_obj$is_present(spell_length_label)) ) {\n      data_obj$add_spell_length_col(col_name=col_name, threshold = threshold)\n    }\n    spell_length_col = data_obj$getvname(spell_length_label)\n\n    if( !(data_obj$is_present(year_label) && data_obj$is_present(month_label) && data_obj$is_present(day_label)) ) {\n      data_obj$add_year_month_day_cols()\n    }\n    \n    year_col = data_obj$getvname(year_label)\n    month_col = data_obj$getvname(month_label)\n    day_col = data_obj$getvname(day_label)\n    \n    curr_data_list = data_obj$get_data_for_analysis(data_list)\n    \n    for( curr_data in curr_data_list ) {\n      \n      for (k in 1:length(curr_data[[spell_length_col]])){\n        if (is.na(curr_data[[spell_length_col]][[k]])){\n          curr_data[[spell_length_col]][[k]]=\"m\" \n        }        \n      }\n\n      tables = list()\n      years_split <- split(curr_data, list(as.factor(curr_data[[year_col]])))\n      \n      # initialize the counter\n      i = 1\n      for ( single_year in years_split ) {\n        tables[[i]] <- dcast(single_year, single_year[[ day_col ]]~single_year[[ month_col ]], value.var = spell_length_col)\n        \n        end = length(colnames(tables[[i]]))\n        names(tables[[i]])[ 1 ] <- day_display\n        colnames(tables[[i]])[2:end] <- months_list[1:end-1]\n        \n        Day = \" \"\n        dat1 = tables[[i]][,2:13]\n        dat2 = sapply(dat1[,1:12], as.numeric)\n        summ =suppressWarnings(as.data.frame(lapply(as.data.frame(dat2), max, na.rm = na.rm)))\n        overall_max = c(\"Maximum\",rep(NA,10),\"(Overall:\",paste(max(summ, na.rm=na.rm),\")\",sep = \"\"))\n        tables[[i]] = rbind(tables[[i]], overall_max)\n        tables[[i]] = join(tables[[i]], cbind(Day,summ), by = c(\"Day\",month.abb), type = \"full\", match = \"all\")\n        missing = c(\" \",rep(NA,13))\n        \n        for(j in 2:13){\n          sa = which(tables[[i]][,j] %in% 0)\n          sb = which(tables[[i]][,j] %in% NA)\n          sc = which(tables[[i]][,j] %in% -Inf)\n          sf = which(tables[[i]][,j] %in% \"m\")\n          tables[[i]][sa,j]<-\"--\"\n          tables[[i]][sb,j]<-\" \"\n          tables[[i]][sc,j]<-\"m\"\n          missing[j] <- length(sf)\n        }\n        overall_miss_max = c(\"Missing\",rep(\" \",10),\"(Overall:\",paste(sum(as.numeric(missing[2:13]), na.rm=na.rm),\")\",sep = \"\"))\n        if(max(as.numeric(missing[2:13]), na.rm=na.rm)>0){\n          tables[[i]] = rbind(tables[[i]], overall_miss_max, missing)\n        }\n        i = i + 1\n      }\n\n      names(tables) <- names(years_split)\n      rettables[[data_obj$get_station_data( curr_data, station_label )]] = tables\n    }\n    \n    return(tables)\n    \n  }\n  \n}\n)" }
{ "repo_name": "cxxr-devel/cxxr", "ref": "refs/heads/master", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "SurajGupta/r-source", "ref": "refs/heads/master", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "aviralg/R-dyntrace", "ref": "refs/heads/master", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "mathematicalcoffee/r-source", "ref": "refs/heads/trunk", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "krlmlr/cxxr", "ref": "refs/heads/master", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "nathan-russell/r-source", "ref": "refs/heads/trunk", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "minux/R", "ref": "refs/heads/trunk", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "allr/timeR", "ref": "refs/heads/master", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "kmillar/cxxr", "ref": "refs/heads/master", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "krlmlr/r-source", "ref": "refs/heads/master", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "WelkinGuan/r-source", "ref": "refs/heads/trunk", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "kmillar/rho", "ref": "refs/heads/master", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "Mouseomics/R", "ref": "refs/heads/master", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "ArunChauhan/cxxr", "ref": "refs/heads/master", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "allr/r-instrumented", "ref": "refs/heads/master", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "rho-devel/rho", "ref": "refs/heads/master", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "MouseGenomics/R", "ref": "refs/heads/master", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "abiyug/r-source", "ref": "refs/heads/trunk", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "andy-thomason/r-source", "ref": "refs/heads/trunk", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "jimhester/r-source", "ref": "refs/heads/trunk", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "LeifAndersen/R", "ref": "refs/heads/master", "path": "src/library/grDevices/R/raster.R", "content": "#  File src/library/grDevices/R/raster.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2015 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\n\n# A raster object is a character vector\n# of colour strings\n# plus a number of rows and columns\n# The vector gives colours in ROW ORDER,\n# starting from the TOP row\n#\n# due to the inherent inefficiency of\n# raster implementation the graphics\n# routines also support \"nativeRaster\"\n# which is the native R representation\n# (integer matrix) of colors in the same\n# order as raster, suitable for practical\n# use with images\n\nis.raster <- function(x)\n    inherits(x, \"raster\")\n\nas.raster <- function(x, ...)\n    UseMethod(\"as.raster\")\n\nas.raster.raster <- function(x, ...)  x\n\nas.raster.logical <- function(x, max = 1, ...)\n    as.raster(matrix(x, ...), max)\n\nas.raster.raw <- function(x, max = 255L, ...)\n    as.raster(matrix(x, ...), max=max)\n\nas.raster.numeric <- as.raster.logical\n\nas.raster.character <- as.raster.logical\n\n\nas.raster.matrix <- function(x, max = 1, ...)\n{\n    if (is.character(x)) {\n        ## Assume to be color names\n        r <- t(x)\n    } else if (is.numeric(x) || is.logical(x)) {\n        ## Assume greyscale or b&w values\n        ## We have to use rgb() indirectly as it\n        ## doesn't hande NAs correctly\n        tx <- t(x)\n        tx.na <- which(is.na(tx))\n        if (length(tx.na)) tx[tx.na] <- 0\n        r <- rgb(tx, tx, tx, maxColorValue = max)\n        if (length(tx.na)) r[tx.na] <- NA\n    } else if (is.raw(x)) { ## non NA's here\n\tstorage.mode(x) <- \"integer\"\n\ttx <- t(x)\n\tr <- rgb(tx, tx, tx, maxColorValue = 255L)\n    } else\n        stop(\"a raster matrix must be character, or numeric, or logical\")\n    ## Transposed, but retain original dimensions\n    dim(r) <- dim(x)\n    class(r) <- \"raster\"\n    r\n}\n\nas.raster.array <- function(x, max = 1, ...)\n{\n    if (!is.numeric(x)) {\n\tif (is.raw(x)) {\n\t    storage.mode(x) <- \"integer\" # memory x 4 (!)\n\t    max <- 255L\n\t} else\n\t    stop(\"a raster array must be numeric\")\n    }\n    if (length(d <- dim(x)) != 3L)\n        stop(\"a raster array must have exactly 3 dimensions\")\n    r <- array(if (d[3L] == 3L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), maxColorValue = max)\n    else if (d[3L] == 4L)\n        rgb(t(x[,,1L]), t(x[,,2L]), t(x[,,3L]), t(x[,,4L]), maxColorValue = max)\n    else\n        stop(\"a raster array must have exactly 3 or 4 planes\"),\n    dim = d[1:2])\n    class(r) <- \"raster\"\n    r\n}\n\n# Conversion to (character) matrix\nas.matrix.raster <- function(x, ...)\n{\n    dim <- dim(x)\n    matrix(x, nrow = dim[1L], ncol = dim[2L], byrow = TRUE)\n}\n\nis.na.raster <- function(x) is.na(as.matrix(x))\nanyNA.raster <- function(x, recursive = FALSE) anyNA(as.matrix(x))\n\n# FIXME:\n# It would be useful to have conversion to array (rgb[a])\n# so that people could play with numeric machinations\n# with raster images\n\nprint.raster <- function(x, ...) print(as.matrix(x), ...)\n\n\n# Subsetting methods\n# Non-standard because raster is ROW-wise\n# Try to piggy-back on existing methods as much as possible\n# IGNORE 'drop' -- i.e. use \"drop = FALSE\" -- in all cases, but  m[i]\n`[.raster` <- function(x, i, j, drop, ...)\n{\n    mdrop <- missing(drop)\n    nA <- nargs() - (!mdrop)\n    if(!mdrop && !identical(drop,FALSE))\n        warning(\"'drop' is always implicitly FALSE in '[.raster'\")\n    m <- as.matrix(x)\n    m <-\n\tif (missing(i)) {\n\t    if(missing(j)) m[ , drop = FALSE] else m[, j, drop = FALSE]\n\t} else if (missing(j)) {\n\t    if (nA == 2) ## is.matrix(i) || is.logical(i))\n\t\treturn(m[i]) # behave as a matrix and directly return character vector\n\t    else if(nA == 3) m[i, , drop = FALSE]\n\t    else stop(\"invalid raster subsetting\")\n\t} else m[i, j, drop = FALSE]\n    as.raster(m)\n}\n\n`[<-.raster` <- function(x, i, j, value)\n{\n    nA <- nargs()\n    m <- as.matrix(x)\n    if (missing(i)) {\n\tif(missing(j)) m[] <- value else m[, j] <- value\n    } else if (missing(j)) {\n\tif (nA == 3) ## typically is.matrix(i) || is.logical(i))\n\t    m[i] <- value\n\telse if(nA == 4) m[i, ] <- value\n\telse stop(\"invalid raster subassignment\")\n    } else m[i, j] <- value\n    as.raster(m)\n}\n\nOps.raster <- function(e1, e2)\n{\n    if (.Generic %in% c(\"==\", \"!=\")) {\n        ## Allows comparison of rasters with each other or with colour names\n        if (is.raster(e1)) e1 <- as.matrix(e1)\n        if (is.raster(e2)) e2 <- as.matrix(e2)\n        ## The result is a logical matrix\n        get(.Generic)(e1, e2)\n    } else {\n        stop(\"operator not meaningful for raster objects\")\n    }\n}\n\n" }
{ "repo_name": "distributions-io/gumbel-cdf", "ref": "refs/heads/master", "path": "test/fixtures/test.partial.R", "content": "options( digits = 16 )\nlibrary( jsonlite )\nlibrary( FAdist )\n\nmu = 0\nbeta = 1\nx = c( -5, -2.5, 0, 2.5, 5 )\ny = pgumbel( x, beta, mu )\n\ncat( y, sep = \",\\n\" )\n\ndata = list(\n\tmu = mu,\n\tbeta = beta,\n\tdata = x,\n\texpected = y\n)\n\nwrite( toJSON( data, digits = 16, auto_unbox = TRUE ), \"./test/fixtures/partial.json\" )\n" }
{ "repo_name": "rlopc/datcom-labs", "ref": "refs/heads/master", "path": "ugr-datcom-md_ans_da-labs/ugr-datcom-md_ans_da-labs_01-clustering/iris-fuzzykmeans-normalizado.R", "content": "iris2=iris\r\niris2$Species=NULL\r\n#normalizo iris2, Notese el uso y sintaxis del bucle\r\nfor (j in 1:4) {x=iris2[,j] ; v=(x-mean(x))/sqrt(var(x)); iris2[,j]=v}\r\n#Trabajo con clustering difuso\r\nfuzzy.result=fanny(iris2,3)\r\nstr(fuzzy.result)\r\nfuzzy.result$membership\r\ntable(fuzzy.result$clustering,iris$Species)\r\nplot(fuzzy.result)\r\n#(Ver descripcion de la funcion)\r\ncluster.stats(dist(iris2),fuzzy.result$clustering,alt.clustering=as.integer(iris$Species))\r\n" }
{ "repo_name": "arnijohnsen/arjtools", "ref": "refs/heads/master", "path": "data-raw/hg18_385k_pos.R", "content": "library(data.table)\n\nhg18_385k_pos <- fread(\n  system.file(\"extdata\", \"hg18_385k_pos.txt\", package = \"arjtools\"),\n  select = c(\"CHROMOSOME\", \"POSITION\")\n)\nsetnames(hg18_385k_pos, c(\"chrom\", \"pos\"))\nsetkey(hg18_385k_pos, chrom, pos)\ndevtools::use_data(hg18_385k_pos)\n\nhg17_385k_pos <- fread(\n  system.file(\"extdata\", \"hg17_385k_pos.txt\", package = \"arjtools\"),\n  select = c(\"CHROMOSOME\", \"POSITION\")\n)\nsetnames(hg17_385k_pos, c(\"chrom\", \"pos\"))\nsetkey(hg17_385k_pos, chrom, pos)\ndevtools::use_data(hg17_385k_pos)\n" }
{ "repo_name": "wbuchanan/concerto", "ref": "refs/heads/master", "path": "R/concerto.updateLoaderEffectShow.R", "content": "#' @name concerto.updateLoaderEffectShow\n#' @aliases concerto.updateLoaderEffectShow\n#' @title concerto.updateLoaderEffectShow\n#' @description Not clearly documented\n#' @param effectShow Not clearly documented\n#' @author Przemyslaw Lis\n#' @import RMySQL\n#' @export\n#' \n\nconcerto.updateLoaderEffectShow <- function(effectShow) {\n    dbName <- RMySQL::dbEscapeStrings(concerto$db$connection, concerto$db$name)\n    sessionID <- RMySQL::dbEscapeStrings(concerto$db$connection, toString(concerto$sessionID))\n    effectShow <- RMySQL::dbEscapeStrings(concerto$db$connection, toString(effectShow))\n    RMySQL::dbSendQuery(concerto$db$connection, statement = \n\tsprintf(\"UPDATE `%s`.`TestSession` SET `loader_effect_show` = '%s' WHERE `id`=%s\", \n        dbName, effectShow, sessionID))\n} \n" }
{ "repo_name": "minkooseo/qviz", "ref": "refs/heads/master", "path": "R/qviz.R", "content": "compute_ellipse_points <- function(x, y, data) {\n  if (NROW(data) == 1) {\n    # In this case, ellipse will return error. Instead, have it render a single point.\n    return(data[, c(x, y)])\n  } else if (NROW(data) == 0) {\n    return(data.frame())\n  }\n  return(ellipse(var(data[, c(x, y)]),\n                 centre=colMeans(data[, c(x, y)]),\n                 level=0.95))\n}\n\nGetPanelMinMax <- function(x, y, data, want_ellipse) {\n  if (NROW(data) == 0) {\n    return(list(xlim=c(NA, NA), ylim=c(NA, NA)))\n  }\n  MinMaxWithMargin <- function(points) {\n    min_value <- min(points)\n    max_value <- max(points)\n    range <- max_value - min_value\n    return (c(min_value - range * 0.05, max_value + range * 0.05))\n  }\n  if (want_ellipse) {\n    ellipse_points <- compute_ellipse_points(x, y, data)\n    return(list(xlim=MinMaxWithMargin(ellipse_points[, 1]),\n                ylim=MinMaxWithMargin(ellipse_points[, 2])))\n  }\n  return(list(xlim=MinMaxWithMargin(data[, x]),\n              ylim=MinMaxWithMargin(data[, y])))\n}\n\n# TODO: Instead of relying on list, create a list of classes and have each class\n# render corresponding chart.\ndraw <- function(x, y, data, want_regression=FALSE, want_pca=FALSE, want_ellipse=FALSE, col=1, pch=1, ...) {\n  drawing <- list()\n  drawing$xlab <- x\n  drawing$ylab <- y\n  panel_min_max <- GetPanelMinMax(x, y, data, want_ellipse)\n  drawing$xlim <- panel_min_max$xlim\n  drawing$ylim <- panel_min_max$ylim\n  drawing$col <- col\n  drawing$pch <- pch\n  drawing$points <- data.frame(x=data[, x], y=data[, y])\n  if (want_regression) {\n    drawing$lm <- lm(data[, y] ~ data[, x])\n  }\n  if (want_ellipse) {\n    drawing$ellipse <- compute_ellipse_points(x, y, data)\n  }\n  return(drawing)\n}\n\n\ndraw_scatter_plot_with_class_ellipse <- function(x, y, lv, data) {\n  i <- 1\n  all_drawings <- list()\n  for (lvl in levels(data[, lv])) {\n    all_drawings[[i]] <- draw(x, y, data[data[, lv] == lvl, ], \n                              want_ellipse=TRUE, col=(i + 1), pch=(i + 1))\n    i <- i + 1\n  }\n  return(all_drawings)\n}\n\n\nshow <- function(drawing, as_new_plot=TRUE, ...) {\n  if (as_new_plot) {\n    plot(drawing$points, type=\"n\", xlim=drawing$xlim, ylim=drawing$ylim,\n         xlab=drawing$xlab, ylab=drawing$ylab, col=drawing$col, pch=drawing$pch, ...)\n  }\n  if (NROW(drawing$points) > 0) {\n    points(drawing$points, col=drawing$col, pch=drawing$pch, ...)\n  }\n  if (!is.null(drawing$lm)) {\n    abline(drawing$lm, col=drawing$col, ...)\n  }\n  if (!is.null(drawing$ellipse) && NROW(drawing$ellipse) > 0) {\n    lines(drawing$ellipse, col=drawing$col, ...)\n  }\n}\n\nget_label <- function(all_drawings) {\n  return(list(\n    xlab=paste(unique(sapply(all_drawings, function(d) { d$xlab })), collapse=\", \"),\n    ylab=paste(unique(sapply(all_drawings, function(d) { d$ylab })), collapse=\", \")))\n}\n\nshow_multiple_drawings <- function(all_drawings, legend=NULL, ...) {\n  if (NROW(all_drawings) > 0) {\n    new_plot_drawn = FALSE\n    for (i in seq_along(all_drawings)) {\n      if (new_plot_drawn == FALSE && NROW(all_drawings[[i]]$points) != 0) {\n        all_xlim <- sapply(all_drawings, function(elem) { elem$xlim })\n        all_ylim <- sapply(all_drawings, function(elem) { elem$ylim })\n        labels <- get_label(all_drawings)\n        plot(all_drawings[[1]]$points, type=\"n\", \n             xlim=c(min(all_xlim, na.rm=TRUE), max(all_xlim, na.rm=TRUE)),\n             ylim=c(min(all_ylim, na.rm=TRUE), max(all_ylim, na.rm=TRUE)),\n             xlab=labels$xlab,\n             ylab=labels$ylab,\n             ...)\n        new_plot_drawn = TRUE        \n      }\n      show(all_drawings[[i]], as_new_plot=FALSE, ...)\n    }\n    legend(\"topleft\", \n           legend=if(is.null(legend)) sapply(all_drawings, function(d) { d$xlab }) else legend,\n           col=seq_along(all_drawings) + 1, pch=seq_along(all_drawings) + 1) \n  }\n}\n\nget_factor_vars <- function(vars, data) {\n  return(vars[sapply(vars, function(v) { is.factor(data[, v]) })])\n}\n\nget_numeric_vars <- function(vars, data) {\n  return(vars[sapply(vars, function(v) { is.numeric(data[, v]) })])\n}\n\nqviz_numeric_vars <- function(vars, data, ...) {\n  numeric_vars <- get_numeric_vars(vars, data)\n  if (NROW(numeric_vars) == 1) {\n    boxplot(data[, numeric_vars], horizontal=TRUE, ylab=numeric_vars[1], ...)\n  } else if (NROW(numeric_vars) > 0) {\n    boxplot(data[, numeric_vars], horizontal=TRUE, ...)\n  }\n}\n\n# Generate title string to use for plot.\n# \"y ~ x (condition)\" if condition is not empty string.\n# \"y ~ x\", otherwise.\nmain_title <- function(y, x, condition) {\n  return(paste0(y, \" ~ \", x, ifelse(condition == \"\", \"\", paste0(\"\\n(\", condition, \")\"))))\n}\n\nqviz_single_rhs <- function(lv, rvars, data, condition=\"\", ...) {\n  i = 1\n  all_drawings <- list()\n  ### For single rvar.\n  for (rv in rvars) {\n    if (!is.factor(data[, lv]) && !is.factor(data[, rv])) {\n      # Draw scatter plot of lv ~ rv and regression.\n      drawing <- draw(rv, lv, data, want_regression=!is.factor(data[, lv]), col=(i + 1), pch=(i + 1))\n      show(drawing, main=main_title(lv, rv, condition), ...)\n      all_drawings[[i]] <- drawing\n      i <- i + 1\n    } else if (is.factor(data[, lv]) && !is.factor(data[, rv])) {\n      # Draw boxplot, and data points over it.\n      # TODO: Change axis as it looks confusing.\n      boxplot(as.formula(paste(rv, \"~\", lv)), data=data, horizontal=TRUE, xlab=rv, ylab=lv, \n              main=main_title(lv, rv, condition), ...)\n      for (lvl_idx in seq(nlevels(data[, lv]))) {\n        sub_data <- subset(data, data[, lv] == levels(data[, lv])[lvl_idx])\n        points(jitter(sub_data[, rv]), jitter(as.numeric(sub_data[, lv])),\n               col=(lvl_idx + 1), pch=(lvl_idx + 1), cex=0.7)\n      }\n      \n      # TODO: Improve the plot for factor ~ factor\n    } else {\n      # Rely on R's plot().\n      plot(as.formula(paste(lv, \"~\", rv)), data=data, main=main_title(lv, rv, condition), ...)\n    }\n  }\n  if (NROW(all_drawings) > 1) {\n    show_multiple_drawings(all_drawings, main=main_title(lv, rv, condition), ...)\n  }\n}\n\nqviz_classification <- function(lv, numeric_rvars, data, condition=\"\") {\n  for (rvar_i in 1:(NROW(numeric_rvars) - 1)) {\n    for (rvar_j in (rvar_i + 1):NROW(numeric_rvars)) {\n      rv_x = numeric_rvars[rvar_j] # Keep y axis the same while changing x\n      rv_y = numeric_rvars[rvar_i]\n      show_multiple_drawings(draw_scatter_plot_with_class_ellipse(rv_x, rv_y, lv, data), \n                             legend=paste(levels(data[, lv])),\n                             main=main_title(lv, paste0(rv_x, \"+\", rv_y), condition))\n    }\n  }\n}\n\nqviz_pca_classification <- function(lv, numeric_rvars, data) {\n  pc <- princomp(data[, numeric_rvars])\n  pca_data <- cbind(as.data.frame(predict(pc, newdata=data)[, 1:2]), \n                    data[, lv])\n  names(pca_data) <- c(\"PC1\", \"PC2\", lv)\n  prop_of_variance <- (summary(pc)$sdev ^ 2) / sum(summary(pc)$sdev ^ 2)\n  subtitle <-sprintf(paste0(\"PC1(Proportion of Variance: %.2f) and \",\n                            \"PC2(Proportion of Variance: %.2f)\"), prop_of_variance[1], prop_of_variance[2])\n  show_multiple_drawings(draw_scatter_plot_with_class_ellipse(\"PC1\", \"PC2\", lv, pca_data),\n                         legend=paste(levels(data[, lv])),\n                         main=paste(lv, \"~ PC1 + PC2\\n\",\n                                    \"PCA of\", paste(numeric_rvars, collapse=\", \")),\n                         sub=subtitle)  \n}\n\nqviz_multiple_rhs <- function(lv, rvars, data) {\n  numeric_rvars <- get_numeric_vars(rvars, data)\n  # If this is classification problem.\n  if (is.factor(data[, lv]) && NROW(numeric_rvars) >= 2) {\n    qviz_classification(lv, numeric_rvars, data)\n    if (NROW(numeric_rvars) >= 3) {\n      qviz_pca_classification(lv, numeric_rvars, data)\n    }\n  }\n  factor_rvars <- get_factor_vars(rvars, data)\n  # For each level of factor, plot single variable and classification ellipse.\n  if (NROW(numeric_rvars) > 0 && NROW(factor_rvars) > 0) {\n    if (NROW(factor_rvars) > 0) {\n      for (frv in factor_rvars) {\n        for (lvl in levels(data[, frv])) {\n          data_for_lvl <- data[data[, frv] == lvl, ]\n          condition <- paste(frv, \"==\", lvl)\n          qviz_single_rhs(lv, numeric_rvars, data_for_lvl, condition)\n          if (is.factor(data[, lv]) && NROW(numeric_rvars) > 1) {\n            qviz_classification(lv, numeric_rvars, data_for_lvl, condition)\n          }\n        }\n      }\n    }\n  }\n}\n\nclean_data_types <- function(data, vars) {\n  for (v in vars) {\n    if (is.ordered(data[, v])) {\n      data[, v] <- as.numeric(data[, v])\n    } else if (is.logical(data[, v]) || is.character(data[, v])) {\n      data[, v] <- as.factor(data[, v])\n    }\n  }\n  return(data)\n}\n\nqviz <- function(formula, data, ...) {\n  lvars <- lhs.vars(formula)\n  rvars <- rhs.vars(formula)\n  data <- clean_data_types(data, c(lvars, rvars))\n  qviz_numeric_vars(lvars, data, main=\"Numeric Ys\")\n  qviz_numeric_vars(rvars, data, main=\"Numeric Xs\")\n  for (lv in lvars) {\n    qviz_single_rhs(lv, rvars, data)\n    qviz_multiple_rhs(lv, rvars, data)\n  }\n}\n" }
{ "repo_name": "marburg-open-courseware/msc-phygeo-ei", "ref": "refs/heads/master", "path": "src/functions/set_environment.R", "content": "# Set path ---------------------------------------------------------------------\nif(Sys.info()[\"sysname\"] == \"Windows\"){\n  filepath_base <- \"D:/active/moc/msc-ui/\"\n} else {\n  filepath_base <- \"/media/permanent/active/moc/msc-ui/\"\n}\n\npath_data <- paste0(filepath_base, \"data/\")\n\npath_aerial <- paste0(path_data, \"aerial/\")\npath_aerial_croped <- paste0(path_aerial, \"croped/\")\npath_aerial_geomoc <- paste0(path_aerial, \"geomoc/\")\npath_aerial_merged <- paste0(path_aerial, \"merged/\")\npath_aerial_org <- paste0(path_aerial, \"org/\")\npath_aerial_preprocessed <- paste0(path_aerial, \"preprocessed/\")\n\npath_hessenforst <- paste0(path_data, \"hessenforst/\")\n\npath_hydrology <- paste0(path_data, \"hydrology/\")\n\npath_lidar <- paste0(path_data, \"lidar/\")\npath_lidar_rasters <- paste0(path_lidar, \"rasters/\")\n\npath_rdata <-paste0(path_data, \"rdata/\")\n\npath_muf_set1m <- paste0(path_data, \"muf_set_1m/\")\npath_muf_set1m_lcc <- paste0(path_muf_set1m, \"lcc/\")\npath_muf_set1m_lcc_ta <- paste0(path_muf_set1m_lcc, \"training_areas/\")\npath_muf_set1m_lcc_ta_rsws091 <- paste0(path_muf_set1m_lcc_ta, \"sample_rsws091/\")\npath_muf_set1m_lidar <- paste0(path_muf_set1m, \"lidar/\")\npath_muf_set1m_sample_non_segm <- paste0(path_muf_set1m, \"sample_non_segm/\")\npath_muf_set1m_sample_segm <- paste0(path_muf_set1m, \"sample_segm/\")\npath_muf_set1m_sample_segm_rgb_idx <- paste0(path_muf_set1m, \"sample_segm_rgb_idx/\")\npath_muf_set1m_sample_rses071 <- paste0(path_muf_set1m, \"sample_rses071/\")\npath_muf_set1m_sample_rsws061 <- paste0(path_muf_set1m, \"sample_rsws061/\")\npath_muf_set1m_sample_rsws093 <- paste0(path_muf_set1m, \"sample_rsws093/\")\npath_muf_set1m_sample_test_01 <- paste0(path_muf_set1m, \"sample_test_01/\")\n\npath_temp <- paste0(path_data, \"temp/\")\n\npath_vectors <- paste0(path_data, \"vectors/\")\n\n\n# Libraries --------------------------------------------------------------------\nlibrary(doParallel)\nlibrary(glcm)\nlibrary(gpm)\nlibrary(mapview)\nlibrary(raster)\nlibrary(rgdal)\nlibrary(rgeos)\nlibrary(satelliteTools)\nlibrary(metTools)\nlibrary(sp)\n\n\n# Functions --------------------------------------------------------------------\npath_functions <- paste0(filepath_base, \"scripts/msc-phygeo-ei/src/functions/\")\nfcts <- list.files(path_functions, pattern = glob2rx(\"*.R\"), full.names = TRUE)\nsapply(fcts[-which(basename(fcts) == \"set_environment.R\")], source)\n\n\n# Other settings ---------------------------------------------------------------\nrasterOptions(tmpdir = path_temp)\n\nsaga_cmd <- \"C:/OSGeo4W64/apps/saga/saga_cmd.exe \"\n# initOTB(\"C:/OSGeo4W64/bin/\")\ninitOTB(\"C:/OSGeo4W64/OTB-5.8.0-win64/OTB-5.8.0-win64/bin/\")\n" }
{ "repo_name": "madmax983/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_algos/glrm/runit_INTERNAL_glrm_cat_1MRows_100Cols_50Levels_xlarge.R", "content": "\n\n\nrtest <- function() {\n\nhdfs_name_node = hadoop.namenode()\n#----------------------------------------------------------------------\n# Parameters for the test.\n#----------------------------------------------------------------------\n\n# Data frame size\nrows <- 1e6\ncols <- 100\nlevels <- 50\nk_dim <- 15\nprint(paste(\"Matrix decomposition rank k =\", k_dim))\n\nprint(paste(\"Creating categorical data frame with rows =\", rows, \"and cols =\", cols, \"with\", levels, \"unique levels each\"))\nsst <- system.time(myframe <- h2o.createFrame(rows = rows, cols = cols, \n                                              randomize = TRUE, categorical_fraction = 1.0, factors = levels,\n                                              integer_fraction = 0.0, binary_fraction = 0.0, \n                                              missing_fraction = 0, has_response = FALSE))\n\nprint(paste(\"Time it took to create frame:\", as.numeric(sst[3])))\n\nprint(\"Running GLRM on frame with one-versus-all multivariate loss and no regularization\")\naat <- system.time(myframe.glrm <- h2o.glrm(training_frame=myframe, k=k_dim, init=\"PlusPlus\", multi_loss=\"Categorical\", regularization_x=\"None\", regularization_y=\"None\", max_iterations=100))\nprint(myframe.glrm)\nprint(paste(\"Time it took to build model:\", aat[3]))\n\nmyframe <- NULL\ngc()\n\n}\n\ndoTest(\"Test\",rtest)\n\n" }
{ "repo_name": "LAJordanger/localgaussSpec", "ref": "refs/heads/master", "path": "inst/scripts/M/P2_fig_02/3_Shiny.R", "content": "##  The \"bivariate cosine\" example from P2_fig_02.\n\nlibrary(localgaussSpec)\n\nmain_dir <- c(\"~\", \"LG_DATA_scripts\", \"P2_fig_02.03\")\ndata_dir <-\n    c(ts.dir = \"dmt_bivariate_e65da95d163c4f4df65813aee851ec99\",\n      approx.dir = \"Approx__1\")\n\nLG_shiny(\n    main_dir = main_dir,\n    data_dir = data_dir)\n" }
{ "repo_name": "Russel88/DAtest", "ref": "refs/heads/master", "path": "R/DA.llm2.R", "content": "#' Log linear regression\n#'\n#' Apply linear regression on multiple features with one \\code{predictor}, with log transformation of relative abundances.\n#' Mixed-effect model is used when a \\code{paired} argument is included, with the \\code{paired} variable as a random intercept.\n#' @param data Either a matrix with counts/abundances, OR a \\code{phyloseq} object. If a matrix/data.frame is provided rows should be taxa/genes/proteins and columns samples\n#' @param predictor The predictor of interest. Either a Factor or Numeric, OR if \\code{data} is a \\code{phyloseq} object the name of the variable in \\code{sample_data(data)} in quotation\n#' @param paired For paired/blocked experimental designs. Either a Factor with Subject/Block ID for running paired/blocked analysis, OR if \\code{data} is a \\code{phyloseq} object the name of the variable in \\code{sample_data(data)} in quotation\n#' @param covars Either a named list with covariables, OR if \\code{data} is a \\code{phyloseq} object a character vector with names of the variables in sample_data(data)\n#' @param out.all If TRUE will output results and p-values from \\code{anova}. If FALSE will output results for 2. level of the \\code{predictor}. If NULL (default) set as TRUE for multi-class \\code{predictor} and FALSE otherwise\n#' @param p.adj Character. P-value adjustment. Default \"fdr\". See \\code{p.adjust} for details\n#' @param delta Numeric. Pseudocount for the log transformation. Default 0.001\n#' @param coeff Integer. The p-value and log2FoldChange will be associated with this coefficient. Default 2, i.e. the 2. level of the \\code{predictor}.\n#' @param allResults If TRUE will return raw results from the \\code{lm}/\\code{lme} function\n#' @param ... Additional arguments for the \\code{lm}/\\code{lme} functions\n#' @return A data.frame with with results.\n#' @examples \n#' # Creating random count_table and predictor\n#' set.seed(4)\n#' mat <- matrix(rnbinom(1500, size = 0.1, mu = 500), nrow = 100, ncol = 15)\n#' rownames(mat) <- 1:100\n#' pred <- c(rep(\"A\", 5), rep(\"B\", 5), rep(\"C\", 5))\n#' \n#' # Running linear model on each feature\n#' res <- DA.llm2(data = mat, predictor = pred)\n#' @import nlme\n#' @export\n\nDA.llm2 <- function(data, predictor, paired = NULL, covars = NULL, out.all = NULL, p.adj = \"fdr\", delta = 0.001, coeff = 2, allResults = FALSE, ...){\n \n  # Extract from phyloseq\n  if(is(data, \"phyloseq\")){\n    DAdata <- DA.phyloseq(data, predictor, paired, covars)\n    count_table <- DAdata$count_table\n    predictor <- DAdata$predictor\n    paired <- DAdata$paired\n    covars <- DAdata$covars\n  } else {\n    count_table <- data\n  }\n  if(!is.null(covars)){\n    for(i in seq_along(covars)){\n      assign(names(covars)[i], covars[[i]])\n    }\n  }\n  \n  # out.all\n  if(is.null(out.all)){\n    if(length(unique(predictor)) == 2) out.all <- FALSE\n    if(length(unique(predictor)) > 2) out.all <- TRUE\n    if(is.numeric(predictor)) out.all <- FALSE\n  }\n  \n  # Relative abundance and log\n  count_table <- as.data.frame.matrix(count_table)\n  count.rel <- apply(count_table,2,function(x) x/sum(x))\n  count.rel <- log(count.rel + delta)\n  \n  # Define design\n  if(is.null(covars)){\n    form <- paste(\"x ~ predictor\")\n  } else {\n    form <- paste(\"x ~ predictor+\",paste(names(covars), collapse=\"+\"),sep = \"\")\n  }\n  \n  # Define function\n  if(is.null(paired)){\n    lmr <- function(x){\n      fit <- NULL\n      tryCatch(\n        fit <- lm(as.formula(form), ...), \n        error = function(e) fit <- NULL)\n      if(!is.null(fit)) {\n        if(nrow(coef(summary(fit))) > 1) {\n          pval <- coef(summary(fit))[coeff,4]\n          ests <- coef(summary(fit))[,1]\n          c(ests,pval)\n        } else NA\n      } else NA \n    }\n  } else {\n    lmr <- function(x){\n      fit <- NULL\n      tryCatch(\n        fit <- lme(as.formula(form), random = ~1|paired, ...), \n        error = function(e) fit <- NULL)\n      if(!is.null(fit)) {\n        if(nrow(coef(summary(fit))) > 1) {\n          pval <- coef(summary(fit))[coeff,5]\n          ests <- coef(summary(fit))[,1]\n          c(ests,pval)\n        } else NA\n      } else NA \n    }\n  }\n  \n  ## for out.all TRUE\n  if(out.all){\n    if(is.null(paired)){\n      lmr <- function(x){\n        fit <- NULL\n        tryCatch(\n          fit <- lm(as.formula(form),...), \n          error = function(e) fit <- NULL)\n        if(!is.null(fit)){\n          ests <- coef(summary(fit))[,1]\n          ano <- anova(fit)[1,]\n          c(ano,ests)\n        }\n      }\n    } else {\n      lmr <- function(x){\n        fit <- NULL\n        tryCatch(\n          fit <- lme(as.formula(form), random = ~1|paired, ...), \n          error = function(e) fit <- NULL)\n        if(!is.null(fit)){\n          ests <- coef(summary(fit))[,1]\n          ano <- anova(fit)[2,]\n          c(ano,ests)\n        }\n      }\n    }\n  }\n  \n  # Run tests\n  if(allResults){\n    if(is.null(paired)){\n      lmr <- function(x){\n        fit <- NULL\n        tryCatch(fit <- lm(as.formula(form), ...), error = function(e) fit <- NULL)  \n      }\n    } else {\n      lmr <- function(x){\n        fit <- NULL\n        tryCatch(\n          fit <- lme(as.formula(form), random = ~1|paired, ...), error = function(e) fit <- NULL)\n      }\n    }\n    return(apply(count.rel,1,lmr))\n  } else {\n    if(out.all){\n      if(is.null(paired)){\n        res <- as.data.frame(do.call(rbind,apply(count.rel,1,lmr)))\n        colnames(res)[1:5] <- c(\"Df\",\"Sum Sq\",\"Mean Sq\",\"F value\",\"pval\")\n      } else {\n        res <- as.data.frame(do.call(rbind,apply(count.rel,1,lmr)))\n        colnames(res)[1:4] <- c(\"numDF\",\"denDF\",\"F-value\",\"pval\")\n      }\n      res <- as.data.frame(lapply(res, unlist))\n    } else {\n      res <- as.data.frame(t(as.data.frame(apply(count.rel,1,lmr))))\n      colnames(res)[ncol(res)] <- \"pval\"\n    }\n    \n    res$pval.adj <- p.adjust(res$pval, method = p.adj)\n    res$Feature <- rownames(res)\n    res$Method <- \"Log Linear reg. 2 (llm2)\"\n    if(is(data, \"phyloseq\")) res <- addTax(data, res)\n    return(res)\n  }  \n}\n" }
{ "repo_name": "dvera/converge", "ref": "refs/heads/master", "path": "R/gffToBed.R", "content": "gffToBed <-\nfunction( gff, extendbp=60, strand=TRUE ){\n\tlibrary(gtools)\n\tcat(\"converting\",gff,\"\\n\")\n\tgffname<-basename(removeext(gff))\n\toutname<-paste(gffname,\".bed\",sep=\"\")\n\tsystem(paste0(\"grep -v '#' \",gff,\" | awk 'BEGIN{i=0};{i+=1;print $1,$4,$5,\\\"\",gffname,\"_\\\"i,1,$7}' OFS='\\t' > \",outname))\n\treturn(outname)\n}\n" }
{ "repo_name": "pkiraly/metadata-qa-marc", "ref": "refs/heads/main", "path": "scripts/tt-histogram.R", "content": "# library(tidyverse)\nlibrary(readr)\n#library(magrittr)\nlibrary(dplyr)\n#' Reads tt-completeness.csv and creates histogram files for all\n#' column. The files are saved under the BASE_OUTPUT_DIR directory specified\n#' in setdir.sh in tt-completeness-histogram-[column name].csv name form where\n#' [column name] is a lower case, hypen separated form of the column name stored\n#' in the input file. Each file has a 'count' and a 'frequency' column.\n#' \n#' In RStudio you can run this script in the console:\n#' system(\"Rscript scripts/tt-histogram.R szte\")\n\nargs = commandArgs(trailingOnly=TRUE)\nif (length(args) == 0) {\n  stop(\"At least one argument must be supplied (input file).n\", call.=FALSE)\n} else if (length(args) == 1) {\n  # default output file\n  output_dir <- args[1]\n}\n\nprefix <- 'tt-completeness'\ncsv <- sprintf(\"%s/%s.csv\", output_dir, prefix)\nif (!file.exists(csv)) {\n  stop(paste(\"input file\", csv, \"does not exist!\"))\n}\ndf <- read_csv(csv, col_types = cols(.default = col_integer(), id = col_character()), progress = TRUE, trim_ws = FALSE)\ndf <- df %>% \n  select(-id)\nnames <- names(df)\n\ntransformed_names <- vector(\"character\", length(names))\nfor (i in seq_along(names)) {\n  name <- names[[i]]\n  col <- rlang::sym(name)\n  histogram <- df %>% \n    select(!!col) %>% \n    group_by(!!col) %>% \n    count() %>%\n    rename(count = name, frequency = n)\n\n  histogram_file <- sprintf(\"%s/%s-histogram-%s.csv\",\n                            output_dir, prefix, name)\n  write_csv(histogram, histogram_file)\n  print(sprintf(\"saving %s into %s\", name, histogram_file))\n}\nprint(\"DONE with tt-histogram.R\")\n" }
{ "repo_name": "hussaibi/libclustER", "ref": "refs/heads/master", "path": "trunk/src/test.graph.scale.R", "content": "source(\"libcluster.R\", echo=FALSE);\ndata.sources.run.graph.scale();\n" }
{ "repo_name": "IALSA/ialsa-bivariate-growth", "ref": "refs/heads/master", "path": "scripts/mplus/group-variables.R", "content": "# load the objects that will subset columns from the results tables\n\n\nadmin <- c(\n  \"software\"\n  ,\"version\"\n  ,\"date\"\n  ,\"time\"\n  ,\"output_file\"\n  ,\"data_file\"\n  ,\"file_path\"\n)\n\nmodel_id <- c(\n   \"study_name\"\n  ,\"model_number\"\n  ,\"subgroup\"\n  ,\"model_type\"\n  ,\"process_a\"\n  ,\"process_b\"\n)\n\nmodel_info <- c(\n  \"subject_count\"\n  ,\"wave_count\"\n  ,\"datapoint_count\"\n  ,\"parameter_count\"\n  ,\"LL\"\n  ,\"aic\"\n  ,\"bic\"\n  ,\"adj_bic\"\n  ,\"aaic\"\n  )\n\nerrors <- c(\n   \"has_converged\"\n  ,\"trust_all\"\n  ,\"mistrust\"\n  ,\"covar_covered\"\n)\n\n# covariance btw intercept of process (A) and intercept of process (B)\nab_TAU_00 <- c(\"ab_TAU_00_est\", \"ab_TAU_00_se\", \"ab_TAU_00_wald\",\"ab_TAU_00_pval\")\n# covariance btw slope of process (A) and slope of process (B)\nab_TAU_11 <- c(\"ab_TAU_11_est\", \"ab_TAU_11_se\", \"ab_TAU_11_wald\", \"ab_TAU_11_pval\")\n# covariance btw intercept of process (A) and slope of process (B)\nab_TAU_01 <- c(\"ab_TAU_01_est\", \"ab_TAU_01_se\", \"ab_TAU_01_wald\",\"ab_TAU_01_pval\")\n# covariance btw slope of process (A) and intercept of process (B)\nab_TAU_10 <- c(\"ab_TAU_10_est\", \"ab_TAU_10_se\", \"ab_TAU_10_wald\", \"ab_TAU_10_pval\")\n# covariance btw residual of process (A) and residual of process (B)\nab_SIGMA  <- c(\"ab_SIGMA_est\", \"ab_SIGMA_se\", \"ab_SIGMA_wald\", \"ab_SIGMA_pval\")\n\n\n# variance of the intercept of process (A)\naa_TAU_00 <- c(\"aa_TAU_00_est\", \"aa_TAU_00_se\",\"aa_TAU_00_wald\", \"aa_TAU_00_pval\")\n# variance of the slope of process (A)\naa_TAU_11 <- c(\"aa_TAU_11_est\", \"aa_TAU_11_se\",\"aa_TAU_11_wald\", \"aa_TAU_11_pval\")\n# covariance of intercept of process (A) and slope of process (A)\naa_TAU_01 <- c(\"aa_TAU_01_est\", \"aa_TAU_01_se\",\"aa_TAU_01_wald\", \"aa_TAU_01_pval\")\n# variance of residual of process (A)\na_SIGMA   <- c(\"a_SIGMA_est\", \"a_SIGMA_se\",\"a_SIGMA_wald\",  \"a_SIGMA_pval\")\n\n\n# variance of the intercept of process (B)\nbb_TAU_00 <- c(\"bb_TAU_00_est\", \"bb_TAU_00_se\", \"bb_TAU_00_wald\", \"bb_TAU_00_pval\")\n# variance of the slope of process (B)\nbb_TAU_11 <- c(\"bb_TAU_11_est\", \"bb_TAU_11_se\", \"bb_TAU_11_wald\",  \"bb_TAU_11_pval\")\n#  covariance btw slope of process (B) and intercept of process (B)\nbb_TAU_10 <- c(\"bb_TAU_10_est\", \"bb_TAU_10_se\", \"bb_TAU_10_wald\", \"bb_TAU_10_pval\")\n# variance of residual of process (B)\nb_SIGMA   <- c(\"b_SIGMA_est\", \"b_SIGMA_se\", \"b_SIGMA_wald\",  \"b_SIGMA_pval\")\n\n\n# intercept of process (A) / average initial status of process (A)\na_GAMMA_00 <- c(\"a_GAMMA_00_est\", \"a_GAMMA_00_se\", \"a_GAMMA_00_wald\",\"a_GAMMA_00_pval\")\n# slope of process (A) / average rate of change of process (A)\na_GAMMA_10 <- c(\"a_GAMMA_10_est\", \"a_GAMMA_10_se\", \"a_GAMMA_10_wald\",\"a_GAMMA_10_pval\")\n# slope of process (B) / average rate of change of process (B)\nb_GAMMA_10 <- c(\"b_GAMMA_10_est\", \"b_GAMMA_10_se\",\"b_GAMMA_10_wald\", \"b_GAMMA_10_pval\")\n# intercept of process (B) /  average initial status of process (B)\nb_GAMMA_00 <- c(\"b_GAMMA_00_est\", \"b_GAMMA_00_se\", \"b_GAMMA_00_wald\", \"b_GAMMA_00_pval\")\n\n\n# intercept of process (A) regressed on Age at baseline\na_GAMMA_01 <-  c(\"a_GAMMA_01_est\", \"a_GAMMA_01_se\", \"a_GAMMA_01_wald\",\"a_GAMMA_01_pval\")\n# slope of process 1 (A) regressed on Age at baseline\na_GAMMA_11 <-  c(\"a_GAMMA_11_est\", \"a_GAMMA_11_se\", \"a_GAMMA_11_wald\",\"a_GAMMA_11_pval\")\n# slope of process 2 (B) regressed on Age at baseline\nb_GAMMA_11 <-  c(\"b_GAMMA_11_est\", \"b_GAMMA_11_se\", \"b_GAMMA_11_wald\",\"b_GAMMA_11_pval\")\n# intercept of process (B) regressed on Age at baseline\nb_GAMMA_01 <-  c(\"b_GAMMA_01_est\", \"b_GAMMA_01_se\", \"b_GAMMA_01_wald\",\"b_GAMMA_01_pval\")\n\n\n# intercept of process (A) regressed on EDUCATION at baseline\na_GAMMA_02 <- c(\"a_GAMMA_02_est\", \"a_GAMMA_02_se\", \"a_GAMMA_02_wald\",\"a_GAMMA_02_pval\")\n# slope of process 1 (A) regressed on EDUCATION at baseline\na_GAMMA_12 <-  c(\"a_GAMMA_12_est\", \"a_GAMMA_12_se\", \"a_GAMMA_12_wald\",\"a_GAMMA_12_pval\")\n# slope of process 2 (B) regressed on EDUCATION at baseline\nb_GAMMA_12 <- c(\"b_GAMMA_12_est\", \"b_GAMMA_12_se\", \"b_GAMMA_12_wald\",\"b_GAMMA_12_pval\")\n# intercept of process (B) regressed on EDUCATION at baseline\nb_GAMMA_02 <- c(\"b_GAMMA_02_est\", \"b_GAMMA_02_se\", \"b_GAMMA_02_wald\",\"b_GAMMA_02_pval\")\n\n\n# intercept of process (A) regressed on HEIGHT at baseline\na_GAMMA_03 <-  c(\"a_GAMMA_03_est\", \"a_GAMMA_03_se\", \"a_GAMMA_03_wald\",\"a_GAMMA_03_pval\")\n# slope of process 1 (A) regressed on HEIGHT at baseline\na_GAMMA_13 <-  c(\"a_GAMMA_13_est\", \"a_GAMMA_13_se\", \"a_GAMMA_13_wald\",\"a_GAMMA_13_pval\")\n# slope of process 2 (B) regressed on HEIGHT at baseline\nb_GAMMA_13 <-  c(\"b_GAMMA_13_est\", \"b_GAMMA_13_se\", \"b_GAMMA_13_wald\",\"b_GAMMA_13_pval\")\n# intercept of process (B) regressed on HEIGHT at baseline\nb_GAMMA_03 <-  c(\"b_GAMMA_03_est\", \"b_GAMMA_03_se\", \"b_GAMMA_03_wald\",\"b_GAMMA_03_pval\")\n\n\n# intercept of process (A) regressed on SMOKING at baseline\na_GAMMA_04 <-  c(\"a_GAMMA_04_est\", \"a_GAMMA_04_se\", \"a_GAMMA_04_wald\",\"a_GAMMA_04_pval\")\n# slope of process 1 (B) regressed on SMOKING at baseline\na_GAMMA_14 <-  c(\"a_GAMMA_14_est\", \"a_GAMMA_14_se\", \"a_GAMMA_14_wald\",\"a_GAMMA_14_pval\")\n# slope of process 2 (A) regressed on SMOKING at baseline\nb_GAMMA_14 <-  c(\"b_GAMMA_14_est\", \"b_GAMMA_14_se\", \"b_GAMMA_14_wald\",\"b_GAMMA_14_pval\")\n# intercept of process (B) regressed on SMOKING at baseline\nb_GAMMA_04 <-  c(\"b_GAMMA_04_est\", \"b_GAMMA_04_se\", \"b_GAMMA_04_wald\",\"b_GAMMA_04_pval\")\n\n\n# intercept of process (A) regressed on CARDIO at baseline\na_GAMMA_05 <-  c(\"a_GAMMA_05_est\", \"a_GAMMA_05_se\", \"a_GAMMA_05_wald\",\"a_GAMMA_05_pval\")\n# slope of process 1 (A) regressed on CARDIO at baseline\na_GAMMA_15 <-  c(\"a_GAMMA_15_est\", \"a_GAMMA_15_se\", \"a_GAMMA_15_wald\",\"a_GAMMA_15_pval\")\n# slope of process 2 (B) regressed on CARDIO at baseline\nb_GAMMA_15 <-  c(\"b_GAMMA_15_est\", \"b_GAMMA_15_se\", \"b_GAMMA_15_wald\",\"b_GAMMA_15_pval\")\n# intercept of process (B) regressed on CARDIO at baseline\nb_GAMMA_05 <-  c(\"b_GAMMA_05_est\", \"b_GAMMA_05_se\", \"b_GAMMA_05_wald\",\"b_GAMMA_05_pval\")\n\n\n# intercept of process (A) regressed on DIABETES at baseline\na_GAMMA_06 <-  c(\"a_GAMMA_06_est\", \"a_GAMMA_06_se\", \"a_GAMMA_06_wald\",\"a_GAMMA_06_pval\")\n# slope of process 1 (A) regressed on DIABETES at baseline\na_GAMMA_16 <-  c(\"a_GAMMA_16_est\", \"a_GAMMA_16_se\", \"a_GAMMA_16_wald\",\"a_GAMMA_16_pval\")\n# slope of process 2 (B) regressed on DIABETES at baseline\nb_GAMMA_16 <-  c(\"b_GAMMA_16_est\", \"b_GAMMA_16_se\", \"b_GAMMA_16_wald\",\"b_GAMMA_16_pval\")\n# intercept of process (B) regressed on DIABETES at baseline\nb_GAMMA_06 <-  c(\"b_GAMMA_06_est\", \"b_GAMMA_06_se\", \"b_GAMMA_06_wald\",\"b_GAMMA_06_pval\")\n\n# estimated\n# correlation b/w intercept of process (A)  and intercept of process (B)\nR_IAIB     <- c(\"R_IAIB_est\", \"R_IAIB_se\",\"R_IAIB_wald\", \"R_IAIB_pval\")\n# correlation b/w slope of process (A)  and slope of process (B)\nR_SASB     <- c(\"R_SASB_est\", \"R_SASB_se\", \"R_SASB_wald\", \"R_SASB_pval\")\n# correlation b/w RESIDUAL of process (A) and RESIDUAL of process (B)\nR_RES_AB   <- c(\"R_RES_AB_est\", \"R_RES_AB_se\",\"R_RES_AB_wald\",  \"R_RES_AB_pval\")\n\n\n# computed\nab_CORR_00 <- c(\"ab_CORR_00\",\"ab_CI95_00_high\", \"ab_CI95_00_low\", \"test_00\" )\nab_CORR_11 <- c(\"ab_CORR_11\",\"ab_CI95_11_high\", \"ab_CI95_11_low\", \"test_11\" )\nab_CORR_residual <- c(\"ab_CORR_residual\",\"ab_CI95_residual_high\", \"ab_CI95_residual_low\", \"test_Res\" )\n\nselected_results <- c( admin, model_id, model_info, errors,\n   ab_TAU_00, ab_TAU_11, ab_TAU_01, ab_TAU_10, ab_SIGMA,\n   aa_TAU_00, aa_TAU_11, aa_TAU_01, a_SIGMA,\n   bb_TAU_00, bb_TAU_11, bb_TAU_10, b_SIGMA,\n   a_GAMMA_00 ,a_GAMMA_10, b_GAMMA_00, b_GAMMA_10,\n   R_IAIB, R_SASB  ,R_RES_AB,\n   a_GAMMA_01, a_GAMMA_11, b_GAMMA_01, b_GAMMA_11,\n   a_GAMMA_02, a_GAMMA_12, b_GAMMA_02, b_GAMMA_12,\n   a_GAMMA_03, a_GAMMA_13, b_GAMMA_03, b_GAMMA_13,\n   a_GAMMA_04, a_GAMMA_14, b_GAMMA_04, b_GAMMA_14,\n   a_GAMMA_05, a_GAMMA_15, b_GAMMA_05, b_GAMMA_15,\n   a_GAMMA_06, a_GAMMA_16, b_GAMMA_06, b_GAMMA_16\n   )\n\n" }
{ "repo_name": "klr324/Ruckertetal_SLR2016", "ref": "refs/heads/master", "path": "RFILES/Scripts/sealevel_VR_2009.R", "content": "###################################################################################\n#\n#  -file = \"sealevel_VR_2009.R\"   Code written March 2014\n#  - Author: Kelsey Ruckert (klr324@psu.edu)\n#\n#  -This function sources the Vermeer and Rahmstorf (2009) model to be sourced into the uncertainty methods as described\n#       in Ruckert et al. (2016). For further\n#       description and references, please read the paper.\n#\n# THIS CODE IS PROVIDED AS-IS WITH NO WARRANTY (NEITHER EXPLICIT\n# NOT IMPLICIT).  I SHARE THIS CODE IN HOPES THAT IT IS USEFUL,\n# BUT I AM NOT LIABLE FOR THE BEHAVIOR OF THIS CODE IN YOUR OWN\n# APPLICATION.  YOU ARE FREE TO SHARE THIS CODE SO LONG AS THE\n# AUTHOR(S) AND VERSION HISTORY REMAIN INTACT.\n#\n# To use this function, simply source this file:\n#   source(\"sealevel_VR_2009.R\")\n#\n###################################################################################\n\nVR_SL_model = function(parameters, Temp){ #inputs are parameters and temperature\n  model.p = length(parameters) #number of parameters in the model\n  a = parameters[1] # sensitivity of sea-level to temperature changes\n  Ti = parameters[2] # temperature when sea-level is zero\n  initialvalue = parameters[3] # initial value of sea-level in 1880\n  b = parameters[4] # rapid response of sea-level term\n  \n  # Estimate the rate of temperature change with smoothing\n  temp.dif = diff(Temp)\n  temprate = rep(NA,to)\n  \n  for(i in 2:(length(Temp)-1)){\n    temprate[i] = 0.5*(temp.dif[i-1] + temp.dif[i])\n  }\n  temprate[1] = temp.dif[1] - (0.5*(temp.dif[2] - temp.dif[1]))\n  temprate[length(Temp)] = temp.dif[length(Temp)-1] + (0.5*(temp.dif[length(Temp)-1] - temp.dif[length(Temp)-2]))\n  \n  # Estimate the rate of sea-level change each year\n  rate = a*(Temp - Ti) + b*temprate \n  values = rep(NA,to)\n  values[1] = initialvalue\n  \n  #Run a forward euler to estimate sea-level over time\n  for(i in from:to){\n    values[i] = values[i-1]+rate[i-1]*timestep\n  }\n  #return sea-level, sea-level rates, and number of parameters\n  return(list(sle = values, slrate = rate, trate = temprate, model.p = model.p))\n}\n\n################################### END ##########################################\n" }
{ "repo_name": "mjfii/The-Fountainhead", "ref": "refs/heads/master", "path": "raw/working_scripts.R", "content": "# determine where we are...\nsetwd(dirname(rstudioapi::getActiveDocumentContext()$path))\n\n# build parse function\nparse.book <- function(x) {\n  # load the text\n  doc <- suppressWarnings(readLines(x))\n  # remove page numbers\n  doc <- doc[!doc %in% c(1:2000)] \n  \n  z<-paste(doc, collapse = ' ')\n  \n  # identify chapter starts\n  chapter_start <- doc %in% paste(c(1:20),'.', sep = '' )\n  \n  chapters <- character(0)\n  ix <- 2\n  \n  for (i in 2:length(chapter_start)) {\n    \n    if (chapter_start[i] == TRUE) {\n      \n      chapters <- c(chapters, paste(doc[ix+1:i-1], collapse = ' ' ))\n      ix <- i+1\n    }\n    \n  }; \n  \n  chapters <- c(chapters, paste(doc[ix:length(chapter_start)], collapse = ' ' ))\n    \n}\n\n# save off files\npeter_keating <- parse.book('fountainhead-part1.txt')\ndevtools::use_data(peter_keating, overwrite = TRUE)\n\nellsworth_m_toohey <- parse.book('fountainhead-part2.txt')\ndevtools::use_data(ellsworth_m_toohey, overwrite = TRUE)\n\ngail_wynand <- parse.book('fountainhead-part3.txt')\ndevtools::use_data(gail_wynand, overwrite = TRUE)\n\nhoward_roark <- parse.book('fountainhead-part4.txt')\ndevtools::use_data(howard_roark, overwrite = TRUE)" }
{ "repo_name": "PoonLab/Kaphi", "ref": "refs/heads/master", "path": "pkg/R/smcABC.R", "content": "# This file is part of Kaphi.\n\n# Kaphi is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n\n# Kaphi is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n\n# You should have received a copy of the GNU General Public License\n# along with Kaphi.  If not, see <http://www.gnu.org/licenses/>.\n\n\n# Based on adaptive SMC algorithm proposed by Del Moral, Pierre, Arnaud\n# Doucet, and Ajay Jasra. \"An adaptive sequential Monte Carlo method for\n# approximate Bayesian computation.\" Statistics and Computing 22.5 (2012):\n# 1009-1020.\n\n\n# global parameters\nresize.amount <- 100\nbisection.max.iter <- 10000\n\n\nsimulate.trees <- function(workspace, theta, model, seed=NA, nthreads=1, ...) {\n\t# @param workspace: smc.workspace object\n\t# @param theta: parameter vector\n\t# @param seed: argument to set.seed()\n  config <- workspace$config\n  if (is.null(body(config$model))) {\n    stop('Simulation method has not been set for configuration.')\n  }\n\tif (!is.na(seed)) {\n\t\tset.seed(seed)\n\t}\n  result <- config$model(theta=theta, nsim=config$nsample, tips=workspace$n.tips,\n                         model=model, seed=seed, labels=workspace$tip.labels, ...)\n  \n  # annotate each tree with its self-kernel score (parallelized)\n  processed <- mclapply(1:config$nsample,\n                        function(i) {\n                        result[[i]] <<- .preprocess.tree(result[[i]], config)\n                        },\n                        mc.cores = nthreads)\n  \n  return(processed)\n}\n\n\n# formally 'distance'\nkernel.dist <- function(t1, t2, decay.factor, rbf.variance, sst.control, norm.mode) {\n  if (is.null(t1$kernel)) {\n    stop(\"t1 missing self kernel in distance()\")\n  }\n  if (is.null(t2$kernel)) {\n    stop(\"t2 missing self kernel in distance()\")\n  }\n  \n  k <- tree.kernel(\n    t1,\n    t2,\n    lambda=decay.factor,\n    sigma=rbf.variance,\n    rho=sst.control\n  )\n  \n  result <- 1. - k / sqrt(t1$kernel * t2$kernel)\n  if (result < 0 || result > 1) {\n    stop(\n      cat(\"ERROR: kernel.dist() value outside range [0,1].\\n\",\n          \"k: \", k, \"\\n\",\n          \"t1$kernel: \", t1$kernel, \"\\n\",\n          \"t2$kernel: \", t2$kernel, \"\\n\"\n      )\n    )\n  }\n  if (is.nan(result)) {\n    cat(\"t1$kernel:\", t1$kernel, \"\\n\")\n    cat(\"t2$kernel:\", t2$kernel, \"\\n\")\n  }\n  return (result)\n}\n\n\n# Applies config$dist expression to trees x and y\ndistance <- function(x, y, config) {\n  \n  result <- eval(parse(text=config$dist)) \n  \n  # One instance of NA or NAN throws error\n  if (is.na(result)==TRUE) {\n    stop(\"One or more distance metrics returns NA\")\n  } else if (is.nan(result)==TRUE) {\n    stop(\"One or more distance metrics returns NAN\")\n  } else {\n    return(result)\n  }\n}\n\n\ninitialize.smc <- function(ws, model, seed=NA, nthreads=1, ...) {\n  config <- ws$config\n  nparams <- length(config$params)\n  colnames(ws$particles) <- config$params\n  for (i in 1:config$nparticle) {\n    # sample particle from prior distribution\n    ws$particles[i,] <- sample.priors(config)\n    \n    # assign uniform weights\n    ws$weights[i] <- 1./config$nparticle\n    \n    # simulate trees from particle\n    ws$sim.trees[[i]] <- simulate.trees(ws, ws$particles[i,], model=model, seed=seed, nthreads=nthreads, ...)\n    \n\t\t# calculate kernel distances for trees (parallelized)\n\t\tws$dists[,i] <- unlist(mclapply(ws$sim.trees[[i]], \n\t\t                                function(sim.tree) {\n\t\t                                distance(ws$obs.tree, sim.tree, config)\n\t\t                                },\n\t\t                                mc.cores=nthreads))\n\t}\n  cat('Initialized SMC workspace.\\n')\n  return(ws)\n}\n\n\n.ess <- function(w) {\n  # effective sample size\n  return(1/sum(w^2))\n}\n\n\n.epsilon.obj.func <- function(ws, epsilon) {\n  # unpack some things\n  config <- ws$config\n  prev.epsilon <- ws$epsilon\n\n  # check that all required objects are present\n  if (is.null(config$quality)) {\n    stop(\"epsilon.obj.func: Config missing setting `quality`, exiting\")\n  }\n\n  # calculate new weights\n  for (i in 1:config$nparticle) {\n    num <- sum(ws$dists[,i] < epsilon)\n    denom <- sum(ws$dists[,i] < prev.epsilon)\n    \n    if (num == denom) {\n      # handle case where numerator and denominator are both zero\n      ws$new.weights[i] <- ws$weights[i]\n    } else {\n      ws$new.weights[i] <- ws$weights[i] * num / denom\n    }\n  }\n\n  # normalize new weights to sum to 1.\n  wsum <- sum(ws$new.weights)\n  ws$new.weights <- ws$new.weights / wsum\n  if (epsilon==0 || wsum==0) {\n    return (-1)  # undefined -- range limited to (0, prev.epsilon)\n  }\n  return (.ess(ws$new.weights) - config$quality * .ess(ws$weights))\n}\n\n\n.next.epsilon <- function(ws, niter) {\n  # Let W_n^i be the weight of the i-th particle at n-th iteration\n  #\n  # The effective sample size is\n  #   ESS({W_n^i}) = 1 / \\sum_{i=1}^{N} (W_n^i)^2\n\t# Use bisection method to solve for epsilon such that:\n  #   ESS(W*, eps) - alpha * ESS(W, eps) = 0, where alpha is quality parameter\n\n  # check that dists have been set\n  if (any(is.na(ws$dists))) {\n    stop(\"NA values in ws$dists; did you forget to run initialize.smc()?\")\n  }\n  config <- ws$config\n  # solve for new epsilon\n  res <- uniroot(function(x) .epsilon.obj.func(ws, x), lower=0,\n                 upper=ws$epsilon, tol=config$step.tolerance, maxiter=1E6)\n  root <- res$root\n  if (root < config$final.epsilon) {\n    cat(\"adjusted root from \", root, \" to \", config$final.epsilon, \"\\n\")\n    root = config$final.epsilon\n  }\n\n  # recalculate new weights at new epsilon\n  #   this is annoying but it's difficult in R to pass `ws` by reference\n  #   to epsilon.obj.func where this has been done already...\n  for (i in 1:config$nparticle) {\n    num <- sum(ws$dists[,i] < root)\n    denom <- sum(ws$dists[,i] < ws$epsilon)\n    ws$weights[i] <- ws$weights[i] * ifelse(num==denom, 1., num/denom)\n    \n  }\n  ws$weights <- ws$weights / sum(ws$weights)  # renormalize weights\n  ws$epsilon <- root\n  return (ws)\n}\n\n\n.resample.particles <- function(ws) {\n  nparticle <- ws$config$nparticle\n  # sample from current population of particles with replacement\n  indices <- sample(1:nparticle, nparticle, replace=TRUE, prob=ws$weights)\n\n  # if there's only one parameter, this returns a vector unless we recast\n  ws$particles <- as.matrix(ws$particles[indices,])\n  colnames(ws$particles) <- ws$config$params\n\n  ws$dists <- ws$dists[,indices]  # transfer columns of kernel distances\n\n  # reset all weights\n  ws$weights <- rep(1./nparticle, times=nparticle)\n  return(ws)\n}\n\n\n\n.perturb.particles <- function(ws, model, nthreads=1, seed=NA, ...) {\n  ##  This implements the Metropolis-Hastings acceptance/rejection step\n  ##  @param ws: workspace\n  ##  @param model: reference to an R function that will simulate trees\n  ##  @param nthreads:  number of threads to run in parallel, defaults to 1 (serial)\n  if (nthreads < 1) {\n    stop(\"User requested less than 1 thread in call to .perturb.particles\")\n  }\n\n  config <- ws$config\n  nparticle <- config$nparticle\n\n  require(parallel, quietly=TRUE)  # load parallel library if not already present\n\n  # iterate over live particles\n  alive <- which(ws$weights > 0)  # indices of live particles\n  ws$alive <- length(alive)\n  \n  res <- mclapply(alive, function (i) {\n    old.particle <- ws$particles[i,]\n    new.particle <- propose(config, old.particle)\n    \n    # calculate prior ratio\n    mh.ratio <- prior.density(config, new.particle) / prior.density(config, old.particle)\n    if (mh.ratio == 0) {\n      return(NULL)    # reject new particle, violates prior assumptions\n    }\n    \n    # calculate proposal ratio\n    mh.ratio <- mh.ratio * proposal.density(config, new.particle, old.particle) / proposal.density(config, old.particle, new.particle)\n    if (mh.ratio == 0) {\n      return(NULL)    # reject new particle, not possible under proposal distribution\n    }\n    \n    # simulate new trees (parallelized)\n    # retain sim.trees in case we revert to previous particle\n    new.trees <- simulate.trees(ws, new.particle, model=model, seed=seed, nthreads=nthreads, ...)\n    new.dists <- unlist(mclapply(new.trees, \n                                    function(sim.tree) {\n                                      distance(ws$obs.tree, sim.tree, config)\n                                    },\n                                    mc.cores=nthreads))\n    \n    # SMC approximation to likelihood ratio\n    old.nbhd <- sum(ws$dists[,i] < ws$epsilon)    # how many samples are in the neighbourhood of data?\n    new.nbhd <- sum(new.dists < ws$epsilon)\n    mh.ratio <- mh.ratio * new.nbhd / old.nbhd\n    \n    output <- list(i, mh.ratio, new.particle, new.trees, new.dists)\n    output\n    \n  }, mc.cores=nthreads)  \n\n  # accept or reject the proposal\n  for (row in res) {\n    if (length(row) == 0) {            # checking for any items that are a returned NULL\n      next\n    }\n    else {\n      i <- row[[1]]\n      if (runif(1) < row[[2]]) {     # always accept if mh.ratio > 1 \n        \n        ws$accept[i] <- TRUE\n        ws$particles[i,] <- row[[3]]    # new.particle\n        ws$dists[,i] <- row[[5]]        # new.dists             \n        ws$sim.trees[[i]] <- row[[4]]   # new.trees\n      }\n      else {\n        ws$accept[i] <- FALSE\n      }\n    }\n  }\n  # creating new attribute ws$accepted in workspace; didn't want dual vector and int behaviour of ws$accept from parallelization\n  ws$accepted <- length(which(ws$accept == TRUE))  # FIXME: <- sum(ws$accept)  # should work..\n  # TODO: use return values to update ws\n  return (ws)\n}\n\n\n\nrun.smc <- function(ws, trace.file='', regex=NA, seed=NA, nthreads=1, verbose=FALSE, model='', maxReject=10, ...) {\n  # @param ws: workspace\n\t# @param obs.tree: object of class 'phylo'\n\t# @param trace.file: (optional) path to a file to write outputs\n\t# @param seed: (optional) integer to set random seed\n\t# @param nthreads: (optional) for running on multiple cores\n\t# @param ...: additional arguments to pass to config@generator via\n  #   simulate.trees()\n  \n  config <- ws$config\n  \n  # clear file and write header row\n  write.table(t(c(\n    'n', 'part.num', 'weight', config$params, paste0('dist.', 1:config$nsample)\n    )), file=trace.file, sep='\\t', quote=FALSE, row.names=FALSE, col.names=FALSE)\n\n  # space for returned values\n  result <- list(niter=0, theta=list(), weights=list(), accept.rate={}, epsilons={})\n\n  # draw particles from prior distribution, assign weights and simulate data\n  ptm <- proc.time()  # start timer\n  cat (\"Initializing SMC-ABC run with\", config$nparticle, \"particles\\n\")\n  ws <- initialize.smc(ws, model, seed=seed, nthreads=nthreads, ...)\n\n  niter <- 0\n  ws$epsilon <- .Machine$double.xmax\n\n  # report stopping conditions\n  if (verbose) {\n    cat (\"ws$epsilon: \", ws$epsilon, \"\\n\");\n    cat (\"config$final.epsilon: \", config$final.epsilon, \"\\n\");\n  }\n\n  while (ws$epsilon != config$final.epsilon) {\n    niter <- niter + 1\n\n    # update epsilon\n    ws <- .next.epsilon(ws, niter)\n\n    # provide some feedback\n    lap <- proc.time() - ptm\n    cat (\"Step \", niter, \" epsilon:\", ws$epsilon, \" ESS:\", .ess(ws$weights),\n             \"accept:\", result$accept.rate[length(result$accept.rate)],\n             \"elapsed:\", round(lap[['elapsed']],1), \"s\\n\")\n\n    # resample particles according to their weights\n    if (.ess(ws$weights) < config$ess.tolerance) {\n      ws <- .resample.particles(ws)\n    }\n\n    # perturb particles\n    ws$accept <- vector()    # vector to keep track of which particles were accepted through parallelization in .perturb.particles\n    ws$alive <- 0\n    ws <- .perturb.particles(ws, model, nthreads=nthreads)  # Metropolis-Hastings sampling\n    \n    # record everything\n    result$theta[[niter]] <- ws$particles\n    result$weights[[niter]] <- ws$weights\n    result$epsilons <- c(result$epsilons, ws$epsilon)\n    result$accept.rate <- c(result$accept.rate, ws$accepted / ws$alive)      # changed ws$accept to ws$accepted; didn't want dual behaviour of ws$accept switching back and forth between vector and int\n\n    # write output to file if specified\n    for (i in 1:config$nparticle) {\n      write.table(\n                x=t(c(niter, i, round(ws$weights[i],10), round(ws$particles[i,],5), round(ws$dists[,i], 5))),\n                file=trace.file,\n                append=TRUE,\n                sep=\"\\t\",\n                row.names=FALSE,\n                col.names=FALSE\n            )\n    }\n\n    # report stopping conditions\n    if (verbose) {\n      cat(\"run.smc niter: \", niter, \"\\n\")\n      cat (\"ws$epsilon: \", ws$epsilon, \"\\n\");\n      cat (\"config$final.epsilon: \", config$final.epsilon, \"\\n\");\n      cat (\"result$accept.rate: \", result$accept.rate, \"\\n\");\n      cat (\"config$final.accept.rate: \", config$final.accept.rate, \"\\n\");\n    }\n\n    # if acceptance rate is low enough, we're done\n    if (result$accept.rate[niter] <= config$final.accept.rate) {\n      ws$epsilon <- config$final.epsilon\n      break  # FIXME: this should be redundant given loop condition above\n    }\n    \n    ## check if the last 10 accept.rates OR last 10 epislon values are the same: if frozen, break\n    if (niter > maxReject &&\n        (length(unique(result$accept.rate[niter:(niter-maxReject-1)])) == 1 \n         || length(unique(result$epsilons[niter:(niter-maxReject-1)])) == 1)) {\n        cat (\"SMC-ABC run has frozen in given parameter space. Please check the ranges in your prior settings.\\n\")\n        break\n    }\n  }\n\n  # finally sample from the estimated posterior distribution\n  ws <- .resample.particles(ws)\n  result$theta[[niter]] <- ws$particles\n  result$weights[[niter]] <- ws$weights\n  result$niter <- niter\n  \n  for (i in 1:config$nparticle) {\n    write.table(\n      x=t(c((niter + 1), i, round(ws$weights[i],10), round(ws$particles[i,],5), round(ws$dists[,i], 5))),\n      file=trace.file,\n      append=TRUE,\n      sep=\"\\t\",\n      row.names=FALSE,\n      col.names=FALSE\n    )\n  }  \n  # pack ws and result into one list to be returned\n  ret <- list(workspace=ws, result=result)\n    \n  return (ret)\n}\n\n\n\n" }
{ "repo_name": "sgibb/MALDIquantTools", "ref": "refs/heads/master", "path": "R/binningCosts-functions.R", "content": "### Copyright 2012 Sebastian Gibb\n## <mail@sebastiangibb.de>\n##\n## This is free software: you can redistribute it and/or modify\n## it under the terms of the GNU General Public License as published by\n## the Free Software Foundation, either version 3 of the License, or\n## (at your option) any later version.\n##\n## It is distributed in the hope that it will be useful,\n## but WITHOUT ANY WARRANTY; without even the implied warranty of\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n## GNU General Public License for more details.\n##\n## See <http://www.gnu.org/licenses/>\n\n#' This function calculates binning costs (differences between masses pre and\n#' post binning).\n#'\n#' @param pre list of \\code{\\linkS4class{MassPeaks}} (pre binning)\n#' @param post list of \\code{\\linkS4class{MassPeaks}} (post binning)\n#' @param relative logical, should costs represent by a costs vs mass position\n#'  ratio?\n#' @return a list of binning costs (percental differences)\n#' @seealso \\code{\\link[MALDIquant]{binPeaks}}\n#' @examples\n#' pre <- list(createMassPeaks(mass=1:5, intensity=1:5),\n#'             createMassPeaks(mass=1:5+0.1, intensity=1:5),\n#'             createMassPeaks(mass=1:5+0.2, intensity=1:5))\n#' post <- binPeaks(pre, tolerance=0.2)\n#'\n#' str(binningCosts(pre, post))\n#' # List of 3\n#' # $ : num [1:5] 0.0909 0.0476 0.0323 0.0244 0.0196\n#' # $ : num [1:5] 0 0 0 0 0\n#' # $ : num [1:5] 0.0909 0.0476 0.0323 0.0244 0.0196\n#' @export\nbinningCosts <- function(pre, post, relative=TRUE) {\n  MALDIquant:::.stopIfNotIsMassPeaksList(pre)\n  MALDIquant:::.stopIfNotIsMassPeaksList(post)\n  stopifnot(length(pre) == length(post))\n\n  costsList <- mapply(FUN=function(pre, post, relative) {\n    costs <- abs(pre@mass-post@mass)\n    \n    if (relative) {\n      costs <- costs/post@mass\n    }\n\n    return(costs)\n  }, pre=pre, post=post, relative=relative, SIMPLIFY=FALSE)\n\n  names(costsList) <- names(post)\n\n  return(costsList)\n}\n" }
{ "repo_name": "johngarvin/R-2.1.1rcc", "ref": "refs/heads/master", "path": "src/library/graphics/R/stem.R", "content": "stem <- function(x, scale = 1, width = 80, atom = 0.00000001) {\n    if (!is.numeric(x) )\n\tstop(\"'x' must be numeric\")\n    x <- x[!is.na(x)]\n    if (length(x)==0) stop(\"no non-missing values\")\n    if (scale <= 0) stop(\"'scale' must be positive\")# unlike S\n    .C(\"stemleaf\", as.double(x), length(x),\n       as.double(scale), as.integer(width), as.double(atom), PACKAGE=\"base\")\n    invisible(NULL)\n}\n" }
{ "repo_name": "XiaosuTong/drsstl", "ref": "refs/heads/master", "path": "R/predNew_mr.R", "content": "#' Prediction at new locations based on the fitting results on HDFS.\n#'\n#' The prediction at new locations are calculated based on the fitting results\n#' saved on HDFS based on the original dataset.\n#'\n#' @param newdata\n#'     A data.frame includes all locations' longitude, latitude, and elevation,\n#'     where the prediction is to be calculated.\n#' @param input\n#'     The path of input file on HDFS. It should be by-month division with all fitting results\n#'     of original dataset\n#' @param output\n#'     The path of output on HDFS where all the intermediate outputs will be saved.\n#' @param info\n#'     The RData path on HDFS which contains all station metadata of original dataset\n#' @param clcontrol\n#'     Should be a list object generated from \\code{mapreduce.control} function.\n#'     The list including all necessary Rhipe parameters and also user tunable\n#'     MapReduce parameters.\n#' @param mlcontrol\n#'     Should be a list object generated from \\code{spacetime.control} function.\n#'     The list including all necessary smoothing parameters of nonparametric fitting.\n#' @author\n#'     Xiaosu Tong\n#' @export\n#' @seealso\n#'     \\code{\\link{spacetime.control}}, \\code{\\link{mapreduce.control}}\n#'\n#' @examples\n#' \\dontrun{\n#'     clcontrol <- mapreduce.control(libLoc=NULL, reduceTask=95, io_sort=100, slow_starts = 0.5)\n#'     mlcontrol <- spacetime.control(\n#'       vari=\"resp\", time=\"date\", n=576, n.p=12, stat_n=7738,\n#'       s.window=\"periodic\", t.window = 241, degree=2, span=0.015, Edeg=2\n#'     )\n#'\n#'     new.grid <- expand.grid(\n#'       lon = seq(-126, -67, by = 0.1),\n#'       lat = seq(25, 49, by = 0.1)\n#'     )\n#'     instate <- !is.na(map.where(\"state\", new.grid$lon, new.grid$lat))\n#'     new.grid <- new.grid[instate, ]\n#'\n#'     elev.fit <- spaloess( elev ~ lon + lat,\n#'       data = station_info,\n#'       degree = 2,\n#'       span = 0.015,\n#'       distance = \"Latlong\",\n#'       normalize = FALSE,\n#'       napred = FALSE,\n#'       alltree = FALSE,\n#'       family=\"symmetric\",\n#'       control=loess.control(surface = \"direct\")\n#'     )\n#'     grid.fit <- predloess(\n#'       object = elev.fit,\n#'       newdata = data.frame(\n#'         lon = new.grid$lon,\n#'         lat = new.grid$lat\n#'       )\n#'     )\n#'     new.grid$elev2 <- log2(grid.fit + 128)\n#'\n#'     predNew_mr(\n#'       newdata=new.grid, input=\"/tmp/output_bymth\", output = \"/tmp\",\n#'       info=\"/tmp/station_info.RData\", mlcontrol=mlcontrol, clcontrol=clcontrol\n#'     )\n#' }\n\npredNew_mr <- function(newdata, input, output, info, mlcontrol=spacetime.control(), clcontrol=mapreduce.control()) {\n\n  if(!is.data.frame(newdata)) {\n    stop(\"new locations must be a data.frame\")\n  }\n\n  FileInput <- input\n  FileOutput <- file.path(output, \"newpred/bymth\")\n\n  D <- ncol(newdata)\n  NM <- names(newdata)\n  N <- nrow(newdata)\n\n  if (D == 2 & mlcontrol$Edeg != 0) {\n    stop(\"elevation is not in the spatial attributes\")\n  }\n  if (D > 3) {\n    stop(\"spatial dimension cannot over 3\")\n  }\n  if (D == 2) {\n    newdata$elev <- 1\n  }\n  if (!all(NM %in% c(\"lon\",\"lat\",\"elev\"))) {\n    stop(\"new locations have different spatial attributes than the historical data on HDFS\")\n  }\n\n  job1 <- list()\n  job1$map <- expression({\n    lapply(seq_along(map.values), function(r) {\n      if(Mlcontrol$Edeg == 2) {\n        newdata$elev2 <- log2(newdata$elev + 128)\n        fml <- as.formula(paste(Mlcontrol$vari, \"~ lon + lat + elev2\"))\n        dropSq <- FALSE\n        condParam <- \"elev2\"\n      } else if(Mlcontrol$Edeg == 1) {\n        newdata$elev2 <- log2(newdata$elev + 128)\n        fml <- as.formula(paste(Mlcontrol$vari, \"~ lon + lat + elev2\"))\n        dropSq <- \"elev2\"\n        condParam <- \"elev2\"\n      } else if (Mlcontrol$Edeg == 0) {\n        fml <- as.formula(paste(Mlcontrol$vari, \"~ lon + lat\"))\n        dropSq <- FALSE\n        condParam <- FALSE\n      }\n\n      if (length(map.keys[[r]]) == 2) {\n        if(nchar(map.keys[[r]][2]) >= 3) {\n          date <- (as.numeric(map.keys[[r]][1]) - 1) * 12 + match(map.keys[[r]][2], month.abb)\n        } else {\n          date <- (as.numeric(map.keys[[r]][1]) - 1) * 12 + as.numeric(map.keys[[r]][2])\n        }\n      } else if (length(map.keys[[r]]) == 1) {\n        date <- map.keys[[r]]\n      }\n\n      value <- arrange(as.data.frame(map.values[[r]]), station.id)\n      value <- cbind(value, station_info[, c(\"lon\",\"lat\",\"elev\")])\n      value <- subset(value, select = -c(station.id))\n      if (Mlcontrol$Edeg != 0) {\n        value$elev2 <- log2(value$elev + 128)\n      }\n      NApred <- any(is.na(value[, Mlcontrol$vari]))\n\n      lo.fit <- spaloess( fml,\n        data        = value,\n        degree      = Mlcontrol$degree,\n        span        = Mlcontrol$span,\n        parametric  = condParam,\n        drop_square = dropSq,\n        family      = Mlcontrol$family,\n        normalize   = FALSE,\n        distance    = \"Latlong\",\n        control     = loess.control(surface = Mlcontrol$surf, iterations = Mlcontrol$siter, cell = Mlcontrol$cell),\n        napred      = NApred,\n        alltree     = match.arg(Mlcontrol$surf, c(\"interpolate\", \"direct\")) == \"interpolate\"\n      )\n      if (Mlcontrol$Edeg != 0) {\n        newPred <- unname(predloess(\n          object = lo.fit,\n          newdata = data.frame(\n            lon = newdata$lon,\n            lat = newdata$lat,\n            elev2 = newdata$elev2\n          )\n        ))\n      } else {\n        newPred <- unname(predloess(\n          object = lo.fit,\n          newdata = data.frame(\n            lon = newdata$lon,\n            lat = newdata$lat\n          )\n        ))\n      }\n\n      for (i in 1:N) {\n        rhcollect(i, c(date, newPred[i]))\n      }\n    })\n  })\n  job1$reduce <- expression(\n    pre = {\n      combine <- numeric()\n    },\n    reduce = {\n      combine <- c(combine, do.call(\"c\", reduce.values))\n    },\n    post = {\n      rhcollect(reduce.key, combine)\n    }\n  )\n  job1$parameters <- list(\n    newdata = newdata,\n    info = info,\n    N = N,\n    Clcontrol = clcontrol,\n    Mlcontrol = mlcontrol\n  )\n  job1$shared <- c(info)\n  job1$setup <- expression(\n    map = {\n      load(strsplit(info, \"/\")[[1]][length(strsplit(info, \"/\")[[1]])])\n      suppressMessages(library(plyr, lib.loc=Clcontrol$libLoc))\n      suppressMessages(library(Spaloess, lib.loc=Clcontrol$libLoc))\n    }\n  )\n  job1$mapred <- list(\n    mapreduce.task.timeout = 0,\n    mapreduce.job.reduces = clcontrol$reduceTask,  #cdh5\n    mapreduce.map.java.opts = clcontrol$map_jvm,\n    mapreduce.map.memory.mb = clcontrol$map_memory,\n    dfs.blocksize = clcontrol$BLK,\n    rhipe_map_bytes_read = clcontrol$map_buffer_read,\n    rhipe_map_buffer_size = clcontrol$map_buffer_size,\n    mapreduce.map.output.compress = TRUE,\n    mapreduce.output.fileoutputformat.compress.type = \"BLOCK\"\n  )\n  job1$input <- rhfmt(FileInput, type=\"sequence\")\n  job1$output <- rhfmt(FileOutput, type=\"sequence\")\n  job1$mon.sec <- 10\n  job1$jobname <- FileOutput\n  job1$readback <- FALSE\n  do.call(\"rhwatch\", job1)\n\n  FileInput <- FileOutput\n  FileOutput <- file.path(output, \"newpred/stlfit\")\n\n  job2 <- list()\n  job2$map <- expression({\n    lapply(seq_along(map.keys), function(r) {\n      value <- arrange(data.frame(matrix(map.values[[r]], ncol=2, byrow=TRUE)), X1)\n\n      fit <- stlplus::stlplus(\n        x=value$X2, t=value$X1, n.p=Mlcontrol$n.p,\n        s.window=Mlcontrol$s.window, s.degree=Mlcontrol$s.degree,\n        t.window=Mlcontrol$t.window, t.degree=Mlcontrol$t.degree,\n        inner=Mlcontrol$inner, outer=Mlcontrol$outer\n      )$data\n      # value originally is a data.frame with 3 columns, vectorize it\n      names(value) <- c(Mlcontrol$time, Mlcontrol$vari)\n      value <- unname(unlist(cbind(subset(value, select = -c(date)), subset(fit, select = c(seasonal, trend)))))\n\n      lapply( (1:Mlcontrol$n), function(i) {\n        rhcollect(i, c(value[c(i, i + Mlcontrol$n, i + Mlcontrol$n * 2)], i, map.keys[[r]]))\n      })\n\n    })\n  })\n  job2$reduce <- expression(\n    pre = {\n      combine <- numeric()\n    },\n    reduce = {\n      combine <- c(combine, do.call(\"c\", reduce.values))\n    },\n    post = {\n      rhcollect(reduce.key, combine)\n    }\n  )\n  job2$parameters <- list(\n    Mlcontrol = mlcontrol,\n    Clcontrol = clcontrol\n  )\n  job2$setup <- expression(\n    map = {\n      suppressMessages(library(stlplus, lib.loc=Clcontrol$libLoc))\n      suppressMessages(library(plyr, lib.loc=Clcontrol$libLoc))\n    }\n  )\n  job2$input <- rhfmt(FileInput, type = \"sequence\")\n  job2$output <- rhfmt(FileOutput, type = \"sequence\")\n  job2$mapred <- list(\n    mapreduce.task.timeout = 0,\n    mapreduce.job.reduces = clcontrol$reduceTask,  #cdh5\n    mapreduce.map.java.opts = clcontrol$map_jvm,\n    mapreduce.map.memory.mb = clcontrol$map_memory,\n    dfs.blocksize = clcontrol$BLK,\n    rhipe_reduce_buff_size = clcontrol$reduce_buffer_size,\n    rhipe_reduce_bytes_read = clcontrol$reduce_buffer_read,\n    rhipe_map_buff_size = clcontrol$map_buffer_size,\n    rhipe_map_bytes_read = clcontrol$map_buffer_read,\n    mapreduce.map.output.compress = TRUE,\n    mapreduce.output.fileoutputformat.compress.type = \"BLOCK\"\n  )\n  job2$mon.sec <- 10\n  job2$readback <- FALSE\n  job2$jobname <- FileOutput\n  do.call(\"rhwatch\", job2)\n\n\n  prefix <- strsplit(input, \"/\")[[1]][1:(length(strsplit(input, \"/\")[[1]]) - 1)]\n  FileInput <- c(file.path(do.call(\"file.path\", as.list(prefix)), \"bymthse\"), FileOutput)\n  FileOutput <- file.path(output, \"newpred/merge\")\n\n\n  job3 <- list()\n  job3$map <- expression({\n    lapply(seq_along(map.keys), function(r) {\n      file <- Sys.getenv(\"mapred.input.file\")\n      value <- arrange(data.frame(matrix(map.values[[r]], ncol=5, byrow=TRUE)), X4, X5)\n      names(value) <- c(\"smoothed\",\"seasonal\",\"trend\", \"date\", \"station.id\")\n      value$remainder <- with(value, smoothed - trend - seasonal)\n      if (grepl(input, file)) {\n        value$new <- 1\n      } else {\n        value$new <- 0\n      }\n      rhcollect(as.numeric(map.keys[[r]]), value)\n    })\n  })\n  job3$reduce <- expression(\n    pre = {\n      combine <- data.frame()\n    },\n    reduce = {\n      combine <- rbind(combine, do.call(\"rbind\", reduce.values))\n    },\n    post = {\n      rownames(combine) <- NULL\n      rhcollect(reduce.key, combine)\n    }\n  )\n  job3$parameters <- list(\n    Mlcontrol = mlcontrol,\n    Clcontrol = clcontrol,\n    info = info,\n    input = FileInput[2]\n  )\n  job3$shared <- c(info)\n  job3$setup <- expression(\n    map = {\n      load(strsplit(info, \"/\")[[1]][length(strsplit(info, \"/\")[[1]])])\n      suppressMessages(library(plyr, lib.loc=Clcontrol$libLoc))\n      suppressMessages(library(Spaloess, lib.loc=Clcontrol$libLoc))\n    }\n  )\n  job3$mapred <- list(\n    mapreduce.task.timeout = 0,\n    mapreduce.job.reduces = clcontrol$reduceTask,  #cdh5\n    mapreduce.map.java.opts = clcontrol$map_jvm,\n    mapreduce.map.memory.mb = clcontrol$map_memory,\n    dfs.blocksize = clcontrol$BLK,\n    rhipe_map_bytes_read = clcontrol$map_buffer_read,\n    rhipe_map_buffer_size = clcontrol$map_buffer_size,\n    mapreduce.map.output.compress = TRUE,\n    mapreduce.output.fileoutputformat.compress.type = \"BLOCK\"\n  )\n  job3$input <- rhfmt(FileInput, type=\"sequence\")\n  job3$output <- rhfmt(FileOutput, type=\"sequence\")\n  job3$mon.sec <- 10\n  job3$jobname <- FileOutput\n  job3$readback <- FALSE\n  do.call(\"rhwatch\", job3)\n\n\n  FileInput <- FileOutput\n  FileOutput <- file.path(output, \"newpred/result_bymth\")\n\n\n  job4 <- list()\n  job4$map <- expression({\n    lapply(seq_along(map.values), function(r) {\n      if(Mlcontrol$Edeg == 2) {\n        newdata$elev2 <- log2(newdata$elev + 128)\n        station_info$elev2 <- log2(station_info$elev + 128)\n        fml <- as.formula(\"remainder ~ lon + lat + elev2\")\n        dropSq <- FALSE\n        condParam <- \"elev2\"\n      } else if(Mlcontrol$Edeg == 1) {\n        newdata$elev2 <- log2(newdata$elev + 128)\n        station_info$elev2 <- log2(station_info$elev + 128)\n        fml <- as.formula(\"remainder ~ lon + lat + elev2\")\n        dropSq <- \"elev2\"\n        condParam <- \"elev2\"\n      } else if (Mlcontrol$Edeg == 0) {\n        fml <- as.formula(\"remainder ~ lon + lat\")\n        dropSq <- FALSE\n        condParam <- FALSE\n      }\n\n      value <- arrange(map.values[[r]], new, station.id)\n      if (Mlcontrol$Edeg != 0) {\n        value <- cbind(value, rbind(station_info[, c(\"lon\",\"lat\",\"elev2\")], newdata[, c(\"lon\",\"lat\",\"elev2\")]))\n      } else {\n        value <- cbind(value, rbind(station_info[, c(\"lon\",\"lat\")], newdata[, c(\"lon\",\"lat\")]))\n      }\n      lo.fit <- spaloess( fml,\n        data        = value,\n        degree      = Mlcontrol$degree,\n        span        = Mlcontrol$span,\n        parametric  = condParam,\n        drop_square = dropSq,\n        family      = Mlcontrol$family,\n        normalize   = FALSE,\n        distance    = \"Latlong\",\n        control     = loess.control(surface = Mlcontrol$surf, iterations = Mlcontrol$siter, cell = Mlcontrol$cell),\n        napred      = FALSE,\n        alltree     = match.arg(Mlcontrol$surf, c(\"interpolate\", \"direct\")) == \"interpolate\"\n      )\n      value$Rspa <- lo.fit$fitted\n      value <- subset(value, new == 1)\n      if (Mlcontrol$Edeg != 0) {\n        value <- subset(value, select = -c(remainder, lon, lat, elev2, date, new))[,c(4,1,2,3,5)]\n      } else {\n        value <- subset(value, select = -c(remainder, lon, lat, date, new))[,c(4,1,2,3,5)]\n      }\n      rownames(value) <- NULL\n      rhcollect(map.keys[[r]], value)\n\n    })\n  })\n  job4$parameters <- list(\n    Mlcontrol = mlcontrol,\n    Clcontrol = clcontrol,\n    info = info,\n    newdata = newdata\n  )\n  job4$shared <- c(info)\n  job4$setup <- expression(\n    map = {\n      load(strsplit(info, \"/\")[[1]][length(strsplit(info, \"/\")[[1]])])\n      suppressMessages(library(plyr, lib.loc=Clcontrol$libLoc))\n      suppressMessages(library(Spaloess, lib.loc=Clcontrol$libLoc))\n    }\n  )\n  job4$mapred <- list(\n    mapreduce.task.timeout = 0,\n    mapreduce.job.reduces = 0,  #cdh5\n    mapreduce.map.java.opts = clcontrol$map_jvm,\n    mapreduce.map.memory.mb = clcontrol$map_memory,\n    dfs.blocksize = clcontrol$BLK,\n    rhipe_map_bytes_read = clcontrol$map_buffer_read,\n    rhipe_map_buffer_size = clcontrol$map_buffer_size,\n    mapreduce.map.output.compress = TRUE,\n    mapreduce.output.fileoutputformat.compress.type = \"BLOCK\"\n  )\n  job4$input <- rhfmt(FileInput, type=\"sequence\")\n  job4$output <- rhfmt(FileOutput, type=\"sequence\")\n  job4$mon.sec <- 10\n  job4$jobname <- FileOutput\n  job4$readback <- FALSE\n  do.call(\"rhwatch\", job4)\n\n\n  FileInput <- FileOutput\n  FileOutput <- file.path(output, \"newpred/result_bystat\")\n\n\n  job5 <- list()\n  job5$map <- expression({\n    lapply(seq_along(map.keys), function(r) {\n      map.values[[r]]$date <- map.keys[[r]]\n      lapply(1:nrow(map.values[[r]]), function(i) {\n        rhcollect(map.values[[r]][i, 1], map.values[[r]][i, -1])\n      })\n    })\n  })\n  job5$reduce <- expression(\n    pre = {\n      combine <- data.frame()\n    },\n    reduce = {\n      combine <- rbind(combine, do.call(\"rbind\", reduce.values))\n    },\n    post = {\n      combine <- arrange(combine, date)\n      rownames(combine) <- NULL\n      rhcollect(reduce.key, combine)\n    }\n  )\n  job5$setup <- expression(\n    reduce = {\n      suppressMessages(library(plyr, lib.loc=Clcontrol$libLoc))\n    }\n  )\n  job5$parameters <- list(\n    Clcontrol = clcontrol\n  )\n  job5$mapred <- list(\n    mapreduce.map.java.opts = clcontrol$map_jvm,\n    mapreduce.map.memory.mb = clcontrol$map_memory,\n    mapreduce.reduce.java.opts = clcontrol$reduce_jvm,\n    mapreduce.reduce.memory.mb = clcontrol$reduce_memory,\n    mapreduce.job.reduces = clcontrol$reduceTask,  #cdh5\n    dfs.blocksize = clcontrol$BLK,\n    mapreduce.task.io.sort.mb = clcontrol$io_sort,\n    mapreduce.map.sort.spill.percent = clcontrol$spill_percent,\n    mapreduce.reduce.shuffle.parallelcopies = clcontrol$reduce_parallelcopies,\n    mapreduce.task.io.sort.factor = clcontrol$task_io_sort_factor,\n    mapreduce.reduce.shuffle.merge.percent = clcontrol$reduce_shuffle_merge_percent,\n    mapreduce.reduce.merge.inmem.threshold = clcontrol$reduce_merge_inmem,\n    mapreduce.reduce.input.buffer.percent = clcontrol$reduce_input_buffer_percent,\n    mapreduce.reduce.shuffle.input.buffer.percent = clcontrol$reduce_shuffle_input_buffer_percent,\n    mapreduce.output.fileoutputformat.compress.type = \"BLOCK\",\n    mapreduce.task.timeout  = 0,\n    mapreduce.job.reduce.slowstart.completedmaps = clcontrol$slow_starts,\n    rhipe_reduce_buff_size = clcontrol$reduce_buffer_size,\n    rhipe_reduce_bytes_read = clcontrol$reduce_buffer_read,\n    rhipe_map_buff_size = clcontrol$map_buffer_size,\n    rhipe_map_bytes_read = clcontrol$map_buffer_read\n  )\n  job5$combiner <- TRUE\n  job5$input <- rhfmt(FileInput, type=\"sequence\")\n  job5$output <- rhfmt(FileOutput, type=\"sequence\")\n  job5$mon.sec <- 10\n  job5$jobname <- FileOutput\n  job5$readback <- FALSE\n  do.call(\"rhwatch\", job5)\n\n  return(NULL)\n\n}\n" }
{ "repo_name": "mervynakash/Titanic.Survivors", "ref": "refs/heads/master", "path": "titanicRandomForest.R", "content": "library(randomForest)\r\nlibrary(caret)\r\n\r\n\r\nrmarkdown::render(\"titanicClean.R\")\r\n\r\n\r\n#################################\r\n######### Dividing Data #########\r\n#################################\r\n\r\ndata$Sex <- factor(data$Sex)\r\ndata$Cabin <- factor(data$Cabin)\r\ndata$Embarked <- factor(data$Embarked)\r\ndata$Title <- factor(data$Title)\r\ndata$newTicket <- factor(data$newTicket)\r\ndata$Name <- factor(data$Name)\r\ndata$Ticket <- factor(data$Ticket)\r\n\r\ndata$Name <- NULL\r\ndata$Ticket <- NULL\r\ndata$Cabin <- NULL\r\n\r\ntrain <- data %>% filter(is.na(Survived) == FALSE)\r\ntrain$Survived <- as.factor(train$Survived)\r\n\r\ntest <- data %>% filter(is.na(Survived) == TRUE)\r\ntest$Survived <- NULL\r\n\r\n\r\n# Splitting the train data to predict whether the model created is \r\n# effective in getting the survivors.\r\nsplit <- sample(seq_len(nrow(train)), size = floor(0.75 * nrow(train)))\r\nnew.train <- train[split,]\r\nnew.test <- train[-split,]\r\n\r\n\r\n\r\n#=================== Random Forest =====================#\r\n\r\nmtry1 <- floor(sqrt(ncol(new.train)-1))\r\n\r\nmtry_new <- c(mtry1-1,mtry1,mtry1+1,mtry1+2,mtry1+3)\r\nacc <- c()\r\nsens <- c()\r\n\r\nfor(i in mtry_new){\r\n  model_rf <- randomForest(Survived~., data = new.train, mtry = i, ntree = 100)\r\n  pred_rf <- predict(model_rf, new.test)\r\n  cm <- confusionMatrix(pred_rf, new.test$Survived, positive = \"1\")\r\n  acc <- c(acc, cm$overall['Accuracy'])\r\n  sens <- c(sens,cm$byClass['Sensitivity'])\r\n}\r\n\r\npos = which.max(acc)\r\n\r\ncontrol = trainControl(method = \"repeatedcv\", repeats = 3)\r\n\r\nmodel_rf_final <- train(Survived~., data = train, method = \"rf\", tuneLength = 10, trControl = control)\r\npred_rf_final2 <- predict(model_rf_final, test)\r\n\r\n\r\nmtry_final <- mtry_new[pos]\r\n\r\nmodel_rf_final <- randomForest(Survived~., data=train, mtry = mtry_final-1, ntree = 500)\r\npred_rf_final <- predict(model_rf_final, test)\r\n\r\nsolution_df <- data.frame(PassengerID = test$PassengerId, Survived = pred_rf_final2)\r\nwrite.csv(solution_df, file = \"solution_rf.csv\", row.names = F)\r\n" }
{ "repo_name": "OuhscBbmc/StatisticalComputing", "ref": "refs/heads/master", "path": "2015_Presentations/11_November/r-generation.R", "content": "library(lattice) # for splom\nlibrary(car)     # for vif\n\n# number of observations to simulate\nnobs = 100\n\n# Using a correlation matrix (let' assume that all variables\n# have unit variance\nM = matrix(c(1, 0.7, 0.7, 0.5,\n             0.7, 1, 0.95, 0.3,\n             0.7, 0.95, 1, 0.3,\n             0.5, 0.3, 0.3, 1), nrow=4, ncol=4)\n\n# Cholesky decomposition\nL = chol(M)\nnvars = dim(L)[1]\n\n# R chol function produces an upper triangular version of L\n# so we have to transpose it.\n# Just to be sure we can have a look at t(L) and the\n# product of the Cholesky decomposition by itself\n\nt(L)\n\n\nt(L) %*% L\n\n\n# Random variables that follow an M correlation matrix\nr = t(L) %*% matrix(rnorm(nvars*nobs), nrow=nvars, ncol=nobs)\nr = t(r)\n\nrdata = as.data.frame(r)\nnames(rdata) = c('resp', 'pred1', 'pred2', 'pred3')\n\n# Plotting and basic stats\nsplom(rdata)\ncor(rdata)\n\n# Model 1: predictors 1 and 3 (correlation is 0.28)\nm1 = lm(resp ~ pred1 + pred3, rdata)\nsummary(m1)\n\n# Model 2: predictors 2 and 3 (correlation is 0.29)\nm2 = lm(resp ~ pred2 + pred3, rdata)\nsummary(m2)\n\n\n# Model 3: correlation between predictors 1 and 2 is 0.96\nm3 = lm(resp ~ pred1 + pred2 + pred3, rdata)\nsummary(m3)\n\n# Variance inflation\nvif(m3)\n\n" }
{ "repo_name": "sachsmc/gridSVG", "ref": "refs/heads/master", "path": "tests/testgrid.R", "content": "# Try to reproduce testsvg.R from grid\nlibrary(grid)\nlibrary(gridSVG)\n\ndev.new(width=6, height=6)\n# Force white background and black foreground\npushViewport(viewport(gp=gpar(col=\"black\", fill=\"white\")))\ngrid.rect()\n# NOTE: that svg.R has transforms that assume that (0, 0)\n# is at bottom-left so y-locations and heights in \"native\"\n# coordinates on an X11 device are not\n# handled properly (by svg.R;  grid draws them fine).\n# So for the outer viewport, we fudge an approximation to the\n# \"pixel\" locations in testsvg.R\n# Ultimate fix requires something in svg.R transforms (ty and th)\n# Also, the viewport pushed above means that the \"native\"\n# device coordinates are no longer available here\npushViewport(viewport(unit(1, \"mm\"), unit(1, \"mm\"),\n                       unit(0.5, \"npc\"), unit(0.6, \"npc\"),\n                       just=c(\"left\", \"bottom\"),\n                       xscale=c(0, 11), yscale=c(0, 11)))\ngrid.rect(gp=gpar(col=\"green\"))\ngrid.lines(1:10, 10:1,\n           default.units=\"native\",\n           gp=gpar(col=\"green\"))\ngrid.polygon(c(1, 3, 4, 1), c(1, 1, 5, 4),\n           default.units=\"native\",\n           gp=gpar(fill=\"grey\", col=NA))\ngrid.rect(rep(6, 2), c(3, 7), 2, 1,\n          just=c(\"left\", \"bottom\"),\n          default.units=\"native\",\n          gp=gpar(fill=\"cyan\"))\ngrid.text(c(\"some text\", \"some more text!\"), 2, 8:7,\n          just=\"left\",\n          default.units=\"native\")\ngrid.circle(rep(8, 2), 3, c(.1, 2),\n            default.units=\"native\",\n            gp=gpar(col=\"blue\", fill=NA))\ngrid.text(\"centred text\", 4, 5,\n          default.units=\"native\", rot=20)\n  pushViewport(viewport(x=6, y=5, w=3, h=1,\n                         default.units=\"native\",\n                         just=c(\"left\", \"bottom\"),\n                         xscale=c(0, 1), yscale=c(0, 1)))\n  grid.rect(0, 0, 1, 1,\n            just=c(\"left\", \"bottom\"),\n            default.units=\"native\",\n            gp=gpar(fill=NA, col=\"black\"))\n  grid.text(\"text in a box\", 0.1, 0.5,\n            just=c(\"left\", \"bottom\"),\n            default.units=\"native\")\n  popViewport()\ngrid.rect(5, 2, 2, 7,\n          default.units=\"native\",\n          just=c(\"left\", \"bottom\"),\n          gp=gpar(fill=\"green\", alpha=.5))\npopViewport()\n\npopViewport()\n\ngrid.export(\"grid.svg\")\ndev.off()\n" }
{ "repo_name": "SchlossLab/Sze_FollowUps_Microbiome_2017", "ref": "refs/heads/master", "path": "code/srn/srn_run_96_RF.R", "content": "### Build the best lesion model possible\r\n### Try XG-Boost, RF, Logit (GLM), C5.0, SVM\r\n### Find the best based on Jenna Wiens suggestions on test and training\r\n## Marc Sze\r\n\r\n#Load needed libraries\r\nsource('code/functions.R')\r\n\r\nloadLibs(c(\"dplyr\", \"caret\",\"scales\", \"doMC\"))\r\n\r\nload(\"exploratory/srn_RF_model_setup.RData\")\r\n\r\n# Set i variable\r\n\r\ni = 96\r\n\r\n#################################################################################\r\n#                                                                               #\r\n#                                                                               #\r\n#               Model Training and Parameter Tuning                             #\r\n#                                                                               #\r\n#################################################################################\r\n\r\n# Call number of processors to use\r\nregisterDoMC(cores = 4)\r\n\r\n#Set up lists to store the data\r\ntest_tune_list <- list()\r\ntest_predictions <- list()\r\n\r\n#Get test data\r\ntrain_test_data <- test_data[eighty_twenty_splits[, i], ]\r\n  \r\n#Train the model\r\ntrain_name <- paste(\"data_split\", i, sep = \"\")\r\n\r\nset.seed(3457)\r\ntest_tune_list[[paste(\"data_split\", i, sep = \"\")]] <- assign(train_name, \r\n  train(lesion ~ ., data = train_test_data, \r\n        method = \"rf\", \r\n        ntree = 2000, \r\n        trControl = fitControl, \r\n        metric = \"ROC\", \r\n        verbose = FALSE))\r\n   \r\ntest_test_data <- test_data[-eighty_twenty_splits[, i], ]\r\n  \r\ntest_predictions[[paste(\"data_split\", i, sep = \"\")]] <- \r\npredict(test_tune_list[[paste(\"data_split\", i, sep = \"\")]], \r\n  test_test_data)\r\n\r\n\r\n# Save image with data and relevant parameters\r\nsave.image(paste(\"exploratory/srn_RF_model_\", i, \".RData\", sep=\"\"))\r\n" }
{ "repo_name": "zachcp/dada2", "ref": "refs/heads/master", "path": "inst/doc/dada-installation.R", "content": "## ----bioc-install, eval=FALSE--------------------------------------------\n#  source(\"http://bioconductor.org/biocLite.R\")\n#  biocLite(suppressUpdates = FALSE)\n#  biocLite(\"ShortRead\", suppressUpdates = FALSE)\n\n## ----install-packages, eval=FALSE----------------------------------------\n#  install.packages(\"path/to/dada2\",\n#                   repos = NULL,\n#                   type = \"source\",\n#                   dependencies = c(\"Depends\", \"Suggests\",\"Imports\"))\n\n## ----install-github-example, eval=FALSE----------------------------------\n#  install.packages(\"~/github/dada2\",\n#                   repos = NULL,\n#                   type = \"source\",\n#                   dependencies = c(\"Depends\", \"Suggests\",\"Imports\"))\n\n## ----packageVersion------------------------------------------------------\npackageVersion(\"dada2\")\n\n## ----bioc-install-missing, eval=FALSE------------------------------------\n#  source(\"http://bioconductor.org/biocLite.R\")\n#  biocLite(\"missing_package_1\")\n#  biocLite(\"missing_package_2\")\n#  # ... and so on\n\n## ----install-packages-rev2, eval=FALSE-----------------------------------\n#  install.packages(\"path/to/dada2\",\n#                   repos = NULL,\n#                   type = \"source\",\n#                   dependencies = c(\"Depends\", \"Suggests\",\"Imports\"))\n\n## ----load-dada2, message=FALSE-------------------------------------------\nlibrary(\"dada2\")\n\n## ----documentation-example, eval=FALSE, message=FALSE--------------------\n#  help(package=\"dada2\")\n#  ?derepFastq\n#  ?dada\n\n" }
{ "repo_name": "robbyjo/assoctool", "ref": "refs/heads/master", "path": "resources/data/assoctool/mlogit.R", "content": "# Association analysis tool\n# Version: 0.1\n# By: Roby Joehanes\n#\n# Copyright 2016-2017 Roby Joehanes\n# This file is distributed under the GNU General Public License version 3.0.\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, version 3 of the License.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nsuppressMessages(library(mlogit));\neval(parse(text=param_cmd));\ndoOne <- function(i, mdata) {\n\tparam_list$data[, opt$omics_var_name] <- get(mdata, i);\n\tresult <- do.call(mlogit, param_list);\n\ttbl <- summary(result)$CoefTable;\n\t# Returns P-value, Effect size, Standard Error, T-statistics\n\tresult <- tbl[, c(4,1,2,3)];\n\tif (rownames(result)[1] == \"(Intercept)\") result <- result[-1,];\n\tif (!is.null(opt$result_var_name)) {\n\t\tif (is.null(..patterns)) ..patterns <<- rownames(result)[grep(opt$result_var_pattern, rownames(result))];\n\t\tresult <- result[..patterns,];\n\t}\n\tif (NROW(result) > 1) result <- as.vector(t(result));\n\tnames(result) <- paste(rep(c(\"P\", \"Fx\", \"SE\", \"T\"), length(..patterns)), rep(..patterns, each=4), sep=\"_\");\n\t\n\treturn (result);\n}\n" }
{ "repo_name": "cassiopagnoncelli/clarity", "ref": "refs/heads/master", "path": "include/helpers/remove-outliers.R", "content": "# Remove outliers of a dataset.\nremoveOutliers <- function(x, na.rm = TRUE, ...) {\n  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm, ...)\n  H <- 1.5 * IQR(x, na.rm = na.rm)\n  y <- x\n  y[x < (qnt[1] - H)] <- NA\n  y[x > (qnt[2] + H)] <- NA\n  y\n}" }
{ "repo_name": "erzk/zooplaR", "ref": "refs/heads/master", "path": "tests/testthat/test-average_area_sold_price.R", "content": "context(\"average_area_sold_price\")\n\ntest_that(\"average_area_sold_price works as expected\", {\n  # Don't run these tests on the CRAN build servers\n  skip_on_cran()\n  \n  expect_error(average_area_sold_price())\n  expect_error(average_area_sold_price(1))\n  \n})" }
{ "repo_name": "cran/remote", "ref": "refs/heads/master", "path": "R/deg2rad.R", "content": "#' Convert degrees to radians\n#' \n#' @export deg2rad\n#' \n#' @param deg vector of degrees to be converted to radians\n#' \n#' @examples\n#' data(vdendool)\n#' \n#' ## latitude in degrees\n#' degrees <- coordinates(vdendool)[, 2]\n#' head(degrees)\n#' \n#' ## latitude in radians\n#' radians <- deg2rad(coordinates(vdendool)[, 2])\n#' head(radians)\n#' \ndeg2rad <- function(deg) {\n  \n  radians <- deg * pi / 180\n  return(radians)\n  \n}" }
{ "repo_name": "environmentalinformatics-marburg/remote", "ref": "refs/heads/master", "path": "R/deg2rad.R", "content": "#' Convert degrees to radians\n#' \n#' @export deg2rad\n#' \n#' @param deg vector of degrees to be converted to radians\n#' \n#' @examples\n#' data(vdendool)\n#' \n#' ## latitude in degrees\n#' degrees <- coordinates(vdendool)[, 2]\n#' head(degrees)\n#' \n#' ## latitude in radians\n#' radians <- deg2rad(coordinates(vdendool)[, 2])\n#' head(radians)\n#' \ndeg2rad <- function(deg) {\n  \n  radians <- deg * pi / 180\n  return(radians)\n  \n}" }
{ "repo_name": "jpritikin/OpenMx", "ref": "refs/heads/master", "path": "inst/models/nightly/PPML_100days_cov_baseline.R", "content": "#\n#   Copyright 2007-2018 by the individuals mentioned in the source code history\n#\n#   Licensed under the Apache License, Version 2.0 (the \"License\");\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n# \n#        http://www.apache.org/licenses/LICENSE-2.0\n# \n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\n\n\n# One hundred days model\n# Each manifest represents a single variable as measured on a different day, days 1-100\n# Three predicting latents. Over the trial, one represents a constant part,\n# one represents a linear part, and one an exponential part\n\nrequire(OpenMx)\n\n\n# Random covariance matrix\n\nmanifests <- unlist(lapply(1:100, function(n) { paste(\"Day\",n, sep=\"\") } ))\nlatents <- c('Const', 'Lin', 'Exp')\n\namtT <- 100\namtStep <- amtT / 99\nConstLoadings <- rep(1,100)\nLinLoadings <- unlist(lapply(1:100, function (n) { n*amtStep/(amtT-1) } ))\nEXPFACTOR <- 0.22\nExpLoadings <- unlist(lapply(1:100, function (n) { -exp(-EXPFACTOR*n*amtStep) } ))\n\nlambda <- cbind(ConstLoadings, LinLoadings, ExpLoadings)\ndataLatents <- matrix(rnorm(3*3), 3, 3)\ndataTest <- lambda %*% dataLatents %*% t(dataLatents) %*% t(lambda) + diag(rep(1,100))\ndataTest <- (dataTest + t(dataTest)) / 2\n\ndataRaw <- mvtnorm::rmvnorm(n=5000, rep(0,100), dataTest)\ncolnames(dataRaw) <- manifests\n\ncolnames(dataTest) <- manifests\nrownames(dataTest) <- manifests\n\nfactorModel <- mxModel(\"One Hundred Days Model\",\n      type=\"RAM\",\n\t  # Vars\n      manifestVars = manifests,\n      latentVars = latents,\n      # A\n\t  mxPath(from='Const',  to=manifests,value=ConstLoadings,\tfree=FALSE),\n\t  mxPath(from='Lin', \tto=manifests,value=LinLoadings,\t\tfree=FALSE),\n\t  mxPath(from='Exp',\tto=manifests,value=ExpLoadings,\t\tfree=FALSE),\n      # S\n\t  mxPath(from=manifests, arrows=2,value=rep(1.0, 100), labels=rep(\"Res\", 100)),\n\t  #mxPath(from=latents, arrows=2,values=1.0),\n\t  mxPath(from=\"Const\", to=latents, arrows=2,values=1.0),\n\t  mxPath(from=\"Lin\", to=latents, arrows=2,values=1.0),\n\t  mxPath(from=\"Exp\", to=latents, arrows=2,values=1.0),\n\t  \n\t  # Means\n#\t  mxPath(from=\"one\", to=latents, values=0, free=TRUE),\n\t  \n\t  # Data\n      mxData(dataTest, type=\"cov\", numObs=100)\n\t)\n\nfactorModelOut <- mxRun(imxPPML(factorModel, FALSE))\n" }
{ "repo_name": "florianhauer/ompl", "ref": "refs/heads/master", "path": "scripts/plannerarena/global.R", "content": "######################################################################\n# Software License Agreement (BSD License)\n#\n#  Copyright (c) 2016, Rice University\n#  All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification, are permitted provided that the following conditions\n#  are met:\n#\n#   * Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   * Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   * Neither the name of the Rice University nor the names of its\n#     contributors may be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n#  \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS\n#  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE\n#  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\n#  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n#  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n#  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN\n#  ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n#  POSSIBILITY OF SUCH DAMAGE.\n######################################################################\n\n# Author: Mark Moll\n\nlibrary(shiny)\nlibrary(shinyjs, warn.conflicts=FALSE)\nlibrary(dplyr, warn.conflicts=FALSE)\nlibrary(tidyr)\nlibrary(ggplot2)\n\n# set default max upload size to 50MB\noptions(shiny.maxRequestSize = getOption(\"shiny.maxRequestSize\", 50000000))\noptions(shiny.reactlog = TRUE)\n\n# see https://groups.google.com/d/msg/shiny-discuss/uSetp4TtW-s/Jktu3fS60RAJ\ndisable <- function(x) {\n  if (inherits(x, 'shiny.tag')) {\n    if (x$name %in% c('input', 'select', 'label'))\n      x$attribs$disabled <- 'disabled'\n    x$children <- disable(x$children)\n  }\n  else if (is.list(x) && length(x) > 0) {\n    for (i in 1:length(x))\n      x[[i]] <- disable(x[[i]])\n  }\n  x\n}\nconditionalDisable <- function(widget, condition) {\n    if (condition)\n        disable(widget)\n    else\n        widget\n}\n\n# see http://stackoverflow.com/questions/29948876/adding-prefix-or-suffix-to-most-data-frame-variable-names-in-piped-r-workflow\ntbl.renamer <- function(tbl, prefix=\"x\", suffix=NULL, index=seq_along(tbl_vars(tbl))) {\n  newnames <- tbl_vars(tbl) # Get old variable names\n  names(newnames) <- newnames\n  names(newnames)[index] <- paste0(prefix,\".\",newnames,suffix)[index] # create a named vector for .dots\n  rename_(tbl,.dots=newnames) # rename the variables\n}\n" }
{ "repo_name": "jvgomez/ompl", "ref": "refs/heads/master", "path": "scripts/plannerarena/global.R", "content": "######################################################################\n# Software License Agreement (BSD License)\n#\n#  Copyright (c) 2016, Rice University\n#  All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification, are permitted provided that the following conditions\n#  are met:\n#\n#   * Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   * Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   * Neither the name of the Rice University nor the names of its\n#     contributors may be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n#  \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS\n#  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE\n#  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\n#  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n#  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n#  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN\n#  ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n#  POSSIBILITY OF SUCH DAMAGE.\n######################################################################\n\n# Author: Mark Moll\n\nlibrary(shiny)\nlibrary(shinyjs, warn.conflicts=FALSE)\nlibrary(dplyr, warn.conflicts=FALSE)\nlibrary(tidyr)\nlibrary(ggplot2)\n\n# set default max upload size to 50MB\noptions(shiny.maxRequestSize = getOption(\"shiny.maxRequestSize\", 50000000))\noptions(shiny.reactlog = TRUE)\n\n# see https://groups.google.com/d/msg/shiny-discuss/uSetp4TtW-s/Jktu3fS60RAJ\ndisable <- function(x) {\n  if (inherits(x, 'shiny.tag')) {\n    if (x$name %in% c('input', 'select', 'label'))\n      x$attribs$disabled <- 'disabled'\n    x$children <- disable(x$children)\n  }\n  else if (is.list(x) && length(x) > 0) {\n    for (i in 1:length(x))\n      x[[i]] <- disable(x[[i]])\n  }\n  x\n}\nconditionalDisable <- function(widget, condition) {\n    if (condition)\n        disable(widget)\n    else\n        widget\n}\n\n# see http://stackoverflow.com/questions/29948876/adding-prefix-or-suffix-to-most-data-frame-variable-names-in-piped-r-workflow\ntbl.renamer <- function(tbl, prefix=\"x\", suffix=NULL, index=seq_along(tbl_vars(tbl))) {\n  newnames <- tbl_vars(tbl) # Get old variable names\n  names(newnames) <- newnames\n  names(newnames)[index] <- paste0(prefix,\".\",newnames,suffix)[index] # create a named vector for .dots\n  rename_(tbl,.dots=newnames) # rename the variables\n}\n" }
{ "repo_name": "davetcoleman/ompl", "ref": "refs/heads/master", "path": "scripts/plannerarena/global.R", "content": "######################################################################\n# Software License Agreement (BSD License)\n#\n#  Copyright (c) 2016, Rice University\n#  All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification, are permitted provided that the following conditions\n#  are met:\n#\n#   * Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   * Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   * Neither the name of the Rice University nor the names of its\n#     contributors may be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n#  \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS\n#  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE\n#  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\n#  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n#  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n#  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN\n#  ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n#  POSSIBILITY OF SUCH DAMAGE.\n######################################################################\n\n# Author: Mark Moll\n\nlibrary(shiny)\nlibrary(shinyjs, warn.conflicts=FALSE)\nlibrary(dplyr, warn.conflicts=FALSE)\nlibrary(tidyr)\nlibrary(ggplot2)\n\n# set default max upload size to 50MB\noptions(shiny.maxRequestSize = getOption(\"shiny.maxRequestSize\", 50000000))\noptions(shiny.reactlog = TRUE)\n\n# see https://groups.google.com/d/msg/shiny-discuss/uSetp4TtW-s/Jktu3fS60RAJ\ndisable <- function(x) {\n  if (inherits(x, 'shiny.tag')) {\n    if (x$name %in% c('input', 'select', 'label'))\n      x$attribs$disabled <- 'disabled'\n    x$children <- disable(x$children)\n  }\n  else if (is.list(x) && length(x) > 0) {\n    for (i in 1:length(x))\n      x[[i]] <- disable(x[[i]])\n  }\n  x\n}\nconditionalDisable <- function(widget, condition) {\n    if (condition)\n        disable(widget)\n    else\n        widget\n}\n\n# see http://stackoverflow.com/questions/29948876/adding-prefix-or-suffix-to-most-data-frame-variable-names-in-piped-r-workflow\ntbl.renamer <- function(tbl, prefix=\"x\", suffix=NULL, index=seq_along(tbl_vars(tbl))) {\n  newnames <- tbl_vars(tbl) # Get old variable names\n  names(newnames) <- newnames\n  names(newnames)[index] <- paste0(prefix,\".\",newnames,suffix)[index] # create a named vector for .dots\n  rename_(tbl,.dots=newnames) # rename the variables\n}\n" }
{ "repo_name": "cxxr-devel/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/sep/tc_sep_6.R", "content": "expected <- eval(parse(text=\"structure(c(1.93536640873922, 0.986800182066523), .Dim = 2L, .Dimnames = list(c(\\\"1\\\", \\\"2\\\")))\"));               \ntest(id=0, code={               \nargv <- eval(parse(text=\"list(structure(c(25.1597633136098, 12.8284023668648), .Dim = 2L, .Dimnames = list(c(\\\"1\\\", \\\"2\\\"))), c(13L, 13L))\"));               \ndo.call(`/`, argv);               \n}, o=expected);               \n\n" }
{ "repo_name": "krlmlr/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/sep/tc_sep_6.R", "content": "expected <- eval(parse(text=\"structure(c(1.93536640873922, 0.986800182066523), .Dim = 2L, .Dimnames = list(c(\\\"1\\\", \\\"2\\\")))\"));               \ntest(id=0, code={               \nargv <- eval(parse(text=\"list(structure(c(25.1597633136098, 12.8284023668648), .Dim = 2L, .Dimnames = list(c(\\\"1\\\", \\\"2\\\"))), c(13L, 13L))\"));               \ndo.call(`/`, argv);               \n}, o=expected);               \n\n" }
{ "repo_name": "kmillar/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/sep/tc_sep_6.R", "content": "expected <- eval(parse(text=\"structure(c(1.93536640873922, 0.986800182066523), .Dim = 2L, .Dimnames = list(c(\\\"1\\\", \\\"2\\\")))\"));               \ntest(id=0, code={               \nargv <- eval(parse(text=\"list(structure(c(25.1597633136098, 12.8284023668648), .Dim = 2L, .Dimnames = list(c(\\\"1\\\", \\\"2\\\"))), c(13L, 13L))\"));               \ndo.call(`/`, argv);               \n}, o=expected);               \n\n" }
{ "repo_name": "kmillar/rho", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/sep/tc_sep_6.R", "content": "expected <- eval(parse(text=\"structure(c(1.93536640873922, 0.986800182066523), .Dim = 2L, .Dimnames = list(c(\\\"1\\\", \\\"2\\\")))\"));               \ntest(id=0, code={               \nargv <- eval(parse(text=\"list(structure(c(25.1597633136098, 12.8284023668648), .Dim = 2L, .Dimnames = list(c(\\\"1\\\", \\\"2\\\"))), c(13L, 13L))\"));               \ndo.call(`/`, argv);               \n}, o=expected);               \n\n" }
{ "repo_name": "ArunChauhan/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/sep/tc_sep_6.R", "content": "expected <- eval(parse(text=\"structure(c(1.93536640873922, 0.986800182066523), .Dim = 2L, .Dimnames = list(c(\\\"1\\\", \\\"2\\\")))\"));               \ntest(id=0, code={               \nargv <- eval(parse(text=\"list(structure(c(25.1597633136098, 12.8284023668648), .Dim = 2L, .Dimnames = list(c(\\\"1\\\", \\\"2\\\"))), c(13L, 13L))\"));               \ndo.call(`/`, argv);               \n}, o=expected);               \n\n" }
{ "repo_name": "rho-devel/rho", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/sep/tc_sep_6.R", "content": "expected <- eval(parse(text=\"structure(c(1.93536640873922, 0.986800182066523), .Dim = 2L, .Dimnames = list(c(\\\"1\\\", \\\"2\\\")))\"));               \ntest(id=0, code={               \nargv <- eval(parse(text=\"list(structure(c(25.1597633136098, 12.8284023668648), .Dim = 2L, .Dimnames = list(c(\\\"1\\\", \\\"2\\\"))), c(13L, 13L))\"));               \ndo.call(`/`, argv);               \n}, o=expected);               \n\n" }
{ "repo_name": "cxxr-devel/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/cummax/tc_cummax_3.R", "content": "expected <- eval(parse(text=\"numeric(0)\"));  \ntest(id=0, code={  \nargv <- eval(parse(text=\"list(list())\"));  \ndo.call(`cummax`, argv);  \n}, o=expected);  \n\n" }
{ "repo_name": "krlmlr/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/cummax/tc_cummax_3.R", "content": "expected <- eval(parse(text=\"numeric(0)\"));  \ntest(id=0, code={  \nargv <- eval(parse(text=\"list(list())\"));  \ndo.call(`cummax`, argv);  \n}, o=expected);  \n\n" }
{ "repo_name": "kmillar/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/cummax/tc_cummax_3.R", "content": "expected <- eval(parse(text=\"numeric(0)\"));  \ntest(id=0, code={  \nargv <- eval(parse(text=\"list(list())\"));  \ndo.call(`cummax`, argv);  \n}, o=expected);  \n\n" }
{ "repo_name": "kmillar/rho", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/cummax/tc_cummax_3.R", "content": "expected <- eval(parse(text=\"numeric(0)\"));  \ntest(id=0, code={  \nargv <- eval(parse(text=\"list(list())\"));  \ndo.call(`cummax`, argv);  \n}, o=expected);  \n\n" }
{ "repo_name": "ArunChauhan/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/cummax/tc_cummax_3.R", "content": "expected <- eval(parse(text=\"numeric(0)\"));  \ntest(id=0, code={  \nargv <- eval(parse(text=\"list(list())\"));  \ndo.call(`cummax`, argv);  \n}, o=expected);  \n\n" }
{ "repo_name": "rho-devel/rho", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/cummax/tc_cummax_3.R", "content": "expected <- eval(parse(text=\"numeric(0)\"));  \ntest(id=0, code={  \nargv <- eval(parse(text=\"list(list())\"));  \ndo.call(`cummax`, argv);  \n}, o=expected);  \n\n" }
{ "repo_name": "SensePlatform/R", "ref": "refs/heads/R-3.5.1", "path": "src/library/graphics/R/legend.R", "content": "#  File src/library/graphics/R/legend.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2016 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\nlegend <-\nfunction(x, y = NULL, legend, fill = NULL, col = par(\"col\"), border=\"black\",\n         lty, lwd, pch, angle = 45, density = NULL, bty = \"o\", bg = par(\"bg\"),\n         box.lwd = par(\"lwd\"), box.lty = par(\"lty\"), box.col = par(\"fg\"),\n\t pt.bg = NA, cex = 1, pt.cex = cex, pt.lwd = lwd,\n\t xjust = 0, yjust = 1, x.intersp = 1, y.intersp = 1, adj = c(0, 0.5),\n\t text.width = NULL, text.col = par(\"col\"), text.font = NULL,\n\t merge = do.lines && has.pch, trace = FALSE,\n\t plot = TRUE, ncol = 1, horiz = FALSE, title = NULL,\n\t inset = 0, xpd, title.col = text.col, title.adj = 0.5,\n         seg.len = 2)\n{\n    ## the 2nd arg may really be `legend'\n    if(missing(legend) && !missing(y) &&\n       (is.character(y) || is.expression(y))) {\n\tlegend <- y\n\ty <- NULL\n    }\n    mfill <- !missing(fill) || !missing(density)\n\n    if(!missing(xpd)) {\n        op <- par(\"xpd\")\n        on.exit(par(xpd=op))\n        par(xpd=xpd)\n    }\n    title <- as.graphicsAnnot(title)\n    if(length(title) > 1) stop(\"invalid 'title'\")\n    legend <- as.graphicsAnnot(legend)\n    n.leg <- if(is.call(legend)) 1 else length(legend)\n    if(n.leg == 0) stop(\"'legend' is of length 0\")\n    auto <-\n\tif (is.character(x))\n\t    match.arg(x, c(\"bottomright\", \"bottom\", \"bottomleft\", \"left\",\n\t\t\t   \"topleft\", \"top\", \"topright\", \"right\", \"center\"))\n\telse NA\n\n    if (is.na(auto)) {\n\txy <- xy.coords(x, y, setLab = FALSE); x <- xy$x; y <- xy$y\n\tnx <- length(x)\n\tif (nx < 1 || nx > 2) stop(\"invalid coordinate lengths\")\n    } else nx <- 0\n\n    xlog <- par(\"xlog\")\n    ylog <- par(\"ylog\")\n\n    rect2 <- function(left, top, dx, dy, density = NULL, angle, ...) {\n\tr <- left + dx; if(xlog) { left <- 10^left; r <- 10^r }\n\tb <- top  - dy; if(ylog) {  top <- 10^top;  b <- 10^b }\n\trect(left, top, r, b, angle = angle, density = density, ...)\n    }\n    segments2 <- function(x1, y1, dx, dy, ...) {\n\tx2 <- x1 + dx; if(xlog) { x1 <- 10^x1; x2 <- 10^x2 }\n\ty2 <- y1 + dy; if(ylog) { y1 <- 10^y1; y2 <- 10^y2 }\n\tsegments(x1, y1, x2, y2, ...)\n    }\n    points2 <- function(x, y, ...) {\n\tif(xlog) x <- 10^x\n\tif(ylog) y <- 10^y\n\tpoints(x, y, ...)\n    }\n    text2 <- function(x, y, ...) {\n\t##--- need to adjust  adj == c(xadj, yadj) ?? --\n\tif(xlog) x <- 10^x\n\tif(ylog) y <- 10^y\n\ttext(x, y, ...)\n    }\n    if(trace)\n\tcatn <- function(...)\n\t    do.call(\"cat\", c(lapply(list(...),formatC), list(\"\\n\")))\n\n    cin <- par(\"cin\")\n    Cex <- cex * par(\"cex\")\t\t# = the `effective' cex for text\n\n    ## at this point we want positive width even for reversed x axis.\n    if(is.null(text.width))\n\ttext.width <- max(abs(strwidth(legend, units=\"user\",\n\t\t\t\t       cex=cex, font = text.font)))\n    else if(!is.numeric(text.width) || text.width < 0)\n\tstop(\"'text.width' must be numeric, >= 0\")\n\n    xc <- Cex * xinch(cin[1L], warn.log=FALSE) # [uses par(\"usr\") and \"pin\"]\n    yc <- Cex * yinch(cin[2L], warn.log=FALSE)\n    if(xc < 0) text.width <- -text.width\n\n    xchar  <- xc\n    xextra <- 0\n    yextra <- yc * (y.intersp - 1)\n    ## watch out for reversed axis here: heights can be negative\n    ymax   <- yc * max(1, strheight(legend, units=\"user\", cex=cex)/yc)\n    ychar <- yextra + ymax\n    if(trace) catn(\"  xchar=\", xchar, \"; (yextra,ychar)=\", c(yextra,ychar))\n\n    if(mfill) {\n\t##= sizes of filled boxes.\n\txbox <- xc * 0.8\n\tybox <- yc * 0.5\n\tdx.fill <- xbox ## + x.intersp*xchar\n    }\n    do.lines <- (!missing(lty) && (is.character(lty) || any(lty > 0))\n\t\t ) || !missing(lwd)\n\n    ## legends per column:\n    n.legpercol <-\n\tif(horiz) {\n\t    if(ncol != 1)\n                warning(gettextf(\"horizontal specification overrides: Number of columns := %d\",\n                                 n.leg), domain = NA)\n\t    ncol <- n.leg\n\t    1\n\t} else ceiling(n.leg / ncol)\n\n    has.pch <- !missing(pch) && length(pch) > 0 # -> default 'merge' is available\n    if(do.lines) {\n\tx.off <- if(merge) -0.7 else 0\n    } else if(merge)\n\twarning(\"'merge = TRUE' has no effect when no line segments are drawn\")\n\n    if(has.pch) {\n\tif(is.character(pch) && !is.na(pch[1L]) &&\n           nchar(pch[1L], type = \"c\") > 1) {\n\t    if(length(pch) > 1)\n\t\twarning(\"not using pch[2..] since pch[1L] has multiple chars\")\n\t    np <- nchar(pch[1L], type = \"c\")\n\t    pch <- substr(rep.int(pch[1L], np), 1L:np, 1L:np)\n\t}\n        ## this coercion was documented but not done in R < 3.0.0\n        if(!is.character(pch)) pch <- as.integer(pch)\n    }\n\n    if (is.na(auto)) {\n\t##- Adjust (x,y) :\n\tif (xlog) x <- log10(x)\n\tif (ylog) y <- log10(y)\n    }\n    if(nx == 2) {\n\t## (x,y) are specifiying OPPOSITE corners of the box\n\tx <- sort(x)\n\ty <- sort(y)\n\tleft <- x[1L]\n\ttop  <- y[2L]\n\tw <- diff(x)# width\n\th <- diff(y)# height\n\tw0 <- w/ncol # column width\n\n\tx <- mean(x)\n\ty <- mean(y)\n\tif(missing(xjust)) xjust <- 0.5\n\tif(missing(yjust)) yjust <- 0.5\n\n    }\n    else {## nx == 1  or  auto\n\t## -- (w,h) := (width,height) of the box to draw -- computed in steps\n\th <- (n.legpercol + !is.null(title)) * ychar + yc\n\tw0 <- text.width + (x.intersp + 1) * xchar\n\tif(mfill)\tw0 <- w0 + dx.fill\n\tif(do.lines)\tw0 <- w0 + (seg.len + x.off)*xchar\n\tw <- ncol*w0 + .5* xchar\n\tif (!is.null(title)\n\t    && (abs(tw <- strwidth(title, units=\"user\", cex=cex) + 0.5*xchar)) > abs(w)) {\n\t    xextra <- (tw - w)/2\n\t    w <- tw\n\t}\n\n\t##-- (w,h) are now the final box width/height.\n\n\tif (is.na(auto)) {\n\t    left <- x - xjust * w\n\t    top\t <- y + (1 - yjust) * h\n\t} else {\n\t    usr <- par(\"usr\")\n\t    inset <- rep_len(inset, 2)\n\t    insetx <- inset[1L]*(usr[2L] - usr[1L])\n\t    left <- switch(auto, \"bottomright\" =,\n\t\t\t   \"topright\" =, \"right\" = usr[2L] - w - insetx,\n\t\t\t   \"bottomleft\" =, \"left\" =, \"topleft\" = usr[1L] + insetx,\n\t\t\t   \"bottom\" =, \"top\" =, \"center\" = (usr[1L] + usr[2L] - w)/2)\n\t    insety <- inset[2L]*(usr[4L] - usr[3L])\n\t    top <- switch(auto, \"bottomright\" =,\n\t\t\t  \"bottom\" =, \"bottomleft\" = usr[3L] + h + insety,\n\t\t\t  \"topleft\" =, \"top\" =, \"topright\" = usr[4L] - insety,\n\t\t\t  \"left\" =, \"right\" =, \"center\" = (usr[3L] + usr[4L] + h)/2)\n\t}\n    }\n\n    if (plot && bty != \"n\") { ## The legend box :\n\tif(trace)\n\t    catn(\"  rect2(\", left, \",\", top,\", w=\", w, \", h=\", h, \", ...)\",\n                 sep = \"\")\n\trect2(left, top, dx = w, dy = h, col = bg, density = NULL,\n              lwd = box.lwd, lty = box.lty, border = box.col)\n    }\n\n    ## (xt[],yt[]) := `current' vectors of (x/y) legend text\n    xt <- left + xchar + xextra +\n\t(w0 * rep.int(0:(ncol-1), rep.int(n.legpercol,ncol)))[1L:n.leg]\n    yt <- top -\t0.5 * yextra - ymax -\n\t(rep.int(1L:n.legpercol,ncol)[1L:n.leg] - 1 + !is.null(title)) * ychar\n\n    if (mfill) {\t\t#- draw filled boxes -------------\n\tif(plot) {\n\t    if(!is.null(fill)) fill <- rep_len(fill, n.leg)\n\t    rect2(left = xt, top=yt+ybox/2, dx = xbox, dy = ybox,\n\t\t  col = fill,\n\t\t  density = density, angle = angle, border = border)\n\t}\n\txt <- xt + dx.fill\n    }\n    if(plot && (has.pch || do.lines))\n\tcol <- rep_len(col, n.leg)\n\n    ## NULL is not documented but people use it.\n    if(missing(lwd) || is.null(lwd))\n\tlwd <- par(\"lwd\") # = default for pt.lwd\n    if (do.lines) {\t\t\t#- draw lines ---------------------\n        ## NULL is not documented\n\tif(missing(lty) || is.null(lty)) lty <- 1\n\tlty <- rep_len(lty, n.leg)\n\tlwd <- rep_len(lwd, n.leg)\n\tok.l <- !is.na(lty) & (is.character(lty) | lty > 0) & !is.na(lwd)\n\tif(trace)\n\t    catn(\"  segments2(\",xt[ok.l] + x.off*xchar, \",\", yt[ok.l],\n\t\t \", dx=\", seg.len*xchar, \", dy=0, ...)\")\n\tif(plot)\n\t    segments2(xt[ok.l] + x.off*xchar, yt[ok.l],\n                      dx = seg.len*xchar, dy = 0,\n\t\t      lty = lty[ok.l], lwd = lwd[ok.l], col = col[ok.l])\n\t# if (!merge)\n\txt <- xt + (seg.len+x.off) * xchar\n    }\n    if (has.pch) {\t\t\t#- draw points -------------------\n\tpch <- rep_len(pch, n.leg)\n\tpt.bg <- rep_len(pt.bg, n.leg)\n\tpt.cex <- rep_len(pt.cex, n.leg)\n\tpt.lwd <- rep_len(pt.lwd, n.leg)\n        ok <- !is.na(pch)\n        if (!is.character(pch)) {\n            ## R 2.x.y omitted pch < 0\n            ok <- ok & (pch >= 0 | pch <= -32)\n        } else {\n            ## like points\n            ok <- ok & nzchar(pch)\n        }\n\tx1 <- (if(merge && do.lines) xt-(seg.len/2)*xchar else xt)[ok]\n\ty1 <- yt[ok]\n\tif(trace)\n\t    catn(\"  points2(\", x1,\",\", y1,\", pch=\", pch[ok],\", ...)\")\n\tif(plot)\n\t    points2(x1, y1, pch = pch[ok], col = col[ok],\n\t\t    cex = pt.cex[ok], bg = pt.bg[ok], lwd = pt.lwd[ok])\n##D\tif (!merge) xt <- xt + dx.pch\n    }\n\n    xt <- xt + x.intersp * xchar\n    if(plot) {\n\tif (!is.null(title))\n            text2(left + w*title.adj, top - ymax, labels = title,\n                  adj = c(title.adj, 0), cex = cex, col = title.col)\n\n\ttext2(xt, yt, labels = legend, adj = adj, cex = cex,\n\t      col = text.col, font = text.font)\n    }\n    invisible(list(rect = list(w = w, h = h, left = left, top = top),\n\t\t   text = list(x = xt, y = yt)))\n}\n" }
{ "repo_name": "aviralg/R-dyntrace", "ref": "refs/heads/master", "path": "src/library/graphics/R/legend.R", "content": "#  File src/library/graphics/R/legend.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2016 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\nlegend <-\nfunction(x, y = NULL, legend, fill = NULL, col = par(\"col\"), border=\"black\",\n         lty, lwd, pch, angle = 45, density = NULL, bty = \"o\", bg = par(\"bg\"),\n         box.lwd = par(\"lwd\"), box.lty = par(\"lty\"), box.col = par(\"fg\"),\n\t pt.bg = NA, cex = 1, pt.cex = cex, pt.lwd = lwd,\n\t xjust = 0, yjust = 1, x.intersp = 1, y.intersp = 1, adj = c(0, 0.5),\n\t text.width = NULL, text.col = par(\"col\"), text.font = NULL,\n\t merge = do.lines && has.pch, trace = FALSE,\n\t plot = TRUE, ncol = 1, horiz = FALSE, title = NULL,\n\t inset = 0, xpd, title.col = text.col, title.adj = 0.5,\n         seg.len = 2)\n{\n    ## the 2nd arg may really be `legend'\n    if(missing(legend) && !missing(y) &&\n       (is.character(y) || is.expression(y))) {\n\tlegend <- y\n\ty <- NULL\n    }\n    mfill <- !missing(fill) || !missing(density)\n\n    if(!missing(xpd)) {\n        op <- par(\"xpd\")\n        on.exit(par(xpd=op))\n        par(xpd=xpd)\n    }\n    title <- as.graphicsAnnot(title)\n    if(length(title) > 1) stop(\"invalid 'title'\")\n    legend <- as.graphicsAnnot(legend)\n    n.leg <- if(is.call(legend)) 1 else length(legend)\n    if(n.leg == 0) stop(\"'legend' is of length 0\")\n    auto <-\n\tif (is.character(x))\n\t    match.arg(x, c(\"bottomright\", \"bottom\", \"bottomleft\", \"left\",\n\t\t\t   \"topleft\", \"top\", \"topright\", \"right\", \"center\"))\n\telse NA\n\n    if (is.na(auto)) {\n\txy <- xy.coords(x, y, setLab = FALSE); x <- xy$x; y <- xy$y\n\tnx <- length(x)\n\tif (nx < 1 || nx > 2) stop(\"invalid coordinate lengths\")\n    } else nx <- 0\n\n    xlog <- par(\"xlog\")\n    ylog <- par(\"ylog\")\n\n    rect2 <- function(left, top, dx, dy, density = NULL, angle, ...) {\n\tr <- left + dx; if(xlog) { left <- 10^left; r <- 10^r }\n\tb <- top  - dy; if(ylog) {  top <- 10^top;  b <- 10^b }\n\trect(left, top, r, b, angle = angle, density = density, ...)\n    }\n    segments2 <- function(x1, y1, dx, dy, ...) {\n\tx2 <- x1 + dx; if(xlog) { x1 <- 10^x1; x2 <- 10^x2 }\n\ty2 <- y1 + dy; if(ylog) { y1 <- 10^y1; y2 <- 10^y2 }\n\tsegments(x1, y1, x2, y2, ...)\n    }\n    points2 <- function(x, y, ...) {\n\tif(xlog) x <- 10^x\n\tif(ylog) y <- 10^y\n\tpoints(x, y, ...)\n    }\n    text2 <- function(x, y, ...) {\n\t##--- need to adjust  adj == c(xadj, yadj) ?? --\n\tif(xlog) x <- 10^x\n\tif(ylog) y <- 10^y\n\ttext(x, y, ...)\n    }\n    if(trace)\n\tcatn <- function(...)\n\t    do.call(\"cat\", c(lapply(list(...),formatC), list(\"\\n\")))\n\n    cin <- par(\"cin\")\n    Cex <- cex * par(\"cex\")\t\t# = the `effective' cex for text\n\n    ## at this point we want positive width even for reversed x axis.\n    if(is.null(text.width))\n\ttext.width <- max(abs(strwidth(legend, units=\"user\",\n\t\t\t\t       cex=cex, font = text.font)))\n    else if(!is.numeric(text.width) || text.width < 0)\n\tstop(\"'text.width' must be numeric, >= 0\")\n\n    xc <- Cex * xinch(cin[1L], warn.log=FALSE) # [uses par(\"usr\") and \"pin\"]\n    yc <- Cex * yinch(cin[2L], warn.log=FALSE)\n    if(xc < 0) text.width <- -text.width\n\n    xchar  <- xc\n    xextra <- 0\n    yextra <- yc * (y.intersp - 1)\n    ## watch out for reversed axis here: heights can be negative\n    ymax   <- yc * max(1, strheight(legend, units=\"user\", cex=cex)/yc)\n    ychar <- yextra + ymax\n    if(trace) catn(\"  xchar=\", xchar, \"; (yextra,ychar)=\", c(yextra,ychar))\n\n    if(mfill) {\n\t##= sizes of filled boxes.\n\txbox <- xc * 0.8\n\tybox <- yc * 0.5\n\tdx.fill <- xbox ## + x.intersp*xchar\n    }\n    do.lines <- (!missing(lty) && (is.character(lty) || any(lty > 0))\n\t\t ) || !missing(lwd)\n\n    ## legends per column:\n    n.legpercol <-\n\tif(horiz) {\n\t    if(ncol != 1)\n                warning(gettextf(\"horizontal specification overrides: Number of columns := %d\",\n                                 n.leg), domain = NA)\n\t    ncol <- n.leg\n\t    1\n\t} else ceiling(n.leg / ncol)\n\n    has.pch <- !missing(pch) && length(pch) > 0 # -> default 'merge' is available\n    if(do.lines) {\n\tx.off <- if(merge) -0.7 else 0\n    } else if(merge)\n\twarning(\"'merge = TRUE' has no effect when no line segments are drawn\")\n\n    if(has.pch) {\n\tif(is.character(pch) && !is.na(pch[1L]) &&\n           nchar(pch[1L], type = \"c\") > 1) {\n\t    if(length(pch) > 1)\n\t\twarning(\"not using pch[2..] since pch[1L] has multiple chars\")\n\t    np <- nchar(pch[1L], type = \"c\")\n\t    pch <- substr(rep.int(pch[1L], np), 1L:np, 1L:np)\n\t}\n        ## this coercion was documented but not done in R < 3.0.0\n        if(!is.character(pch)) pch <- as.integer(pch)\n    }\n\n    if (is.na(auto)) {\n\t##- Adjust (x,y) :\n\tif (xlog) x <- log10(x)\n\tif (ylog) y <- log10(y)\n    }\n    if(nx == 2) {\n\t## (x,y) are specifiying OPPOSITE corners of the box\n\tx <- sort(x)\n\ty <- sort(y)\n\tleft <- x[1L]\n\ttop  <- y[2L]\n\tw <- diff(x)# width\n\th <- diff(y)# height\n\tw0 <- w/ncol # column width\n\n\tx <- mean(x)\n\ty <- mean(y)\n\tif(missing(xjust)) xjust <- 0.5\n\tif(missing(yjust)) yjust <- 0.5\n\n    }\n    else {## nx == 1  or  auto\n\t## -- (w,h) := (width,height) of the box to draw -- computed in steps\n\th <- (n.legpercol + !is.null(title)) * ychar + yc\n\tw0 <- text.width + (x.intersp + 1) * xchar\n\tif(mfill)\tw0 <- w0 + dx.fill\n\tif(do.lines)\tw0 <- w0 + (seg.len + x.off)*xchar\n\tw <- ncol*w0 + .5* xchar\n\tif (!is.null(title)\n\t    && (abs(tw <- strwidth(title, units=\"user\", cex=cex) + 0.5*xchar)) > abs(w)) {\n\t    xextra <- (tw - w)/2\n\t    w <- tw\n\t}\n\n\t##-- (w,h) are now the final box width/height.\n\n\tif (is.na(auto)) {\n\t    left <- x - xjust * w\n\t    top\t <- y + (1 - yjust) * h\n\t} else {\n\t    usr <- par(\"usr\")\n\t    inset <- rep_len(inset, 2)\n\t    insetx <- inset[1L]*(usr[2L] - usr[1L])\n\t    left <- switch(auto, \"bottomright\" =,\n\t\t\t   \"topright\" =, \"right\" = usr[2L] - w - insetx,\n\t\t\t   \"bottomleft\" =, \"left\" =, \"topleft\" = usr[1L] + insetx,\n\t\t\t   \"bottom\" =, \"top\" =, \"center\" = (usr[1L] + usr[2L] - w)/2)\n\t    insety <- inset[2L]*(usr[4L] - usr[3L])\n\t    top <- switch(auto, \"bottomright\" =,\n\t\t\t  \"bottom\" =, \"bottomleft\" = usr[3L] + h + insety,\n\t\t\t  \"topleft\" =, \"top\" =, \"topright\" = usr[4L] - insety,\n\t\t\t  \"left\" =, \"right\" =, \"center\" = (usr[3L] + usr[4L] + h)/2)\n\t}\n    }\n\n    if (plot && bty != \"n\") { ## The legend box :\n\tif(trace)\n\t    catn(\"  rect2(\", left, \",\", top,\", w=\", w, \", h=\", h, \", ...)\",\n                 sep = \"\")\n\trect2(left, top, dx = w, dy = h, col = bg, density = NULL,\n              lwd = box.lwd, lty = box.lty, border = box.col)\n    }\n\n    ## (xt[],yt[]) := `current' vectors of (x/y) legend text\n    xt <- left + xchar + xextra +\n\t(w0 * rep.int(0:(ncol-1), rep.int(n.legpercol,ncol)))[1L:n.leg]\n    yt <- top -\t0.5 * yextra - ymax -\n\t(rep.int(1L:n.legpercol,ncol)[1L:n.leg] - 1 + !is.null(title)) * ychar\n\n    if (mfill) {\t\t#- draw filled boxes -------------\n\tif(plot) {\n\t    if(!is.null(fill)) fill <- rep_len(fill, n.leg)\n\t    rect2(left = xt, top=yt+ybox/2, dx = xbox, dy = ybox,\n\t\t  col = fill,\n\t\t  density = density, angle = angle, border = border)\n\t}\n\txt <- xt + dx.fill\n    }\n    if(plot && (has.pch || do.lines))\n\tcol <- rep_len(col, n.leg)\n\n    ## NULL is not documented but people use it.\n    if(missing(lwd) || is.null(lwd))\n\tlwd <- par(\"lwd\") # = default for pt.lwd\n    if (do.lines) {\t\t\t#- draw lines ---------------------\n        ## NULL is not documented\n\tif(missing(lty) || is.null(lty)) lty <- 1\n\tlty <- rep_len(lty, n.leg)\n\tlwd <- rep_len(lwd, n.leg)\n\tok.l <- !is.na(lty) & (is.character(lty) | lty > 0) & !is.na(lwd)\n\tif(trace)\n\t    catn(\"  segments2(\",xt[ok.l] + x.off*xchar, \",\", yt[ok.l],\n\t\t \", dx=\", seg.len*xchar, \", dy=0, ...)\")\n\tif(plot)\n\t    segments2(xt[ok.l] + x.off*xchar, yt[ok.l],\n                      dx = seg.len*xchar, dy = 0,\n\t\t      lty = lty[ok.l], lwd = lwd[ok.l], col = col[ok.l])\n\t# if (!merge)\n\txt <- xt + (seg.len+x.off) * xchar\n    }\n    if (has.pch) {\t\t\t#- draw points -------------------\n\tpch <- rep_len(pch, n.leg)\n\tpt.bg <- rep_len(pt.bg, n.leg)\n\tpt.cex <- rep_len(pt.cex, n.leg)\n\tpt.lwd <- rep_len(pt.lwd, n.leg)\n        ok <- !is.na(pch)\n        if (!is.character(pch)) {\n            ## R 2.x.y omitted pch < 0\n            ok <- ok & (pch >= 0 | pch <= -32)\n        } else {\n            ## like points\n            ok <- ok & nzchar(pch)\n        }\n\tx1 <- (if(merge && do.lines) xt-(seg.len/2)*xchar else xt)[ok]\n\ty1 <- yt[ok]\n\tif(trace)\n\t    catn(\"  points2(\", x1,\",\", y1,\", pch=\", pch[ok],\", ...)\")\n\tif(plot)\n\t    points2(x1, y1, pch = pch[ok], col = col[ok],\n\t\t    cex = pt.cex[ok], bg = pt.bg[ok], lwd = pt.lwd[ok])\n##D\tif (!merge) xt <- xt + dx.pch\n    }\n\n    xt <- xt + x.intersp * xchar\n    if(plot) {\n\tif (!is.null(title))\n            text2(left + w*title.adj, top - ymax, labels = title,\n                  adj = c(title.adj, 0), cex = cex, col = title.col)\n\n\ttext2(xt, yt, labels = legend, adj = adj, cex = cex,\n\t      col = text.col, font = text.font)\n    }\n    invisible(list(rect = list(w = w, h = h, left = left, top = top),\n\t\t   text = list(x = xt, y = yt)))\n}\n" }
{ "repo_name": "minux/R", "ref": "refs/heads/trunk", "path": "src/library/graphics/R/legend.R", "content": "#  File src/library/graphics/R/legend.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2016 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\nlegend <-\nfunction(x, y = NULL, legend, fill = NULL, col = par(\"col\"), border=\"black\",\n         lty, lwd, pch, angle = 45, density = NULL, bty = \"o\", bg = par(\"bg\"),\n         box.lwd = par(\"lwd\"), box.lty = par(\"lty\"), box.col = par(\"fg\"),\n\t pt.bg = NA, cex = 1, pt.cex = cex, pt.lwd = lwd,\n\t xjust = 0, yjust = 1, x.intersp = 1, y.intersp = 1, adj = c(0, 0.5),\n\t text.width = NULL, text.col = par(\"col\"), text.font = NULL,\n\t merge = do.lines && has.pch, trace = FALSE,\n\t plot = TRUE, ncol = 1, horiz = FALSE, title = NULL,\n\t inset = 0, xpd, title.col = text.col, title.adj = 0.5,\n         seg.len = 2)\n{\n    ## the 2nd arg may really be `legend'\n    if(missing(legend) && !missing(y) &&\n       (is.character(y) || is.expression(y))) {\n\tlegend <- y\n\ty <- NULL\n    }\n    mfill <- !missing(fill) || !missing(density)\n\n    if(!missing(xpd)) {\n        op <- par(\"xpd\")\n        on.exit(par(xpd=op))\n        par(xpd=xpd)\n    }\n    title <- as.graphicsAnnot(title)\n    if(length(title) > 1) stop(\"invalid 'title'\")\n    legend <- as.graphicsAnnot(legend)\n    n.leg <- if(is.call(legend)) 1 else length(legend)\n    if(n.leg == 0) stop(\"'legend' is of length 0\")\n    auto <-\n\tif (is.character(x))\n\t    match.arg(x, c(\"bottomright\", \"bottom\", \"bottomleft\", \"left\",\n\t\t\t   \"topleft\", \"top\", \"topright\", \"right\", \"center\"))\n\telse NA\n\n    if (is.na(auto)) {\n\txy <- xy.coords(x, y, setLab = FALSE); x <- xy$x; y <- xy$y\n\tnx <- length(x)\n\tif (nx < 1 || nx > 2) stop(\"invalid coordinate lengths\")\n    } else nx <- 0\n\n    xlog <- par(\"xlog\")\n    ylog <- par(\"ylog\")\n\n    rect2 <- function(left, top, dx, dy, density = NULL, angle, ...) {\n\tr <- left + dx; if(xlog) { left <- 10^left; r <- 10^r }\n\tb <- top  - dy; if(ylog) {  top <- 10^top;  b <- 10^b }\n\trect(left, top, r, b, angle = angle, density = density, ...)\n    }\n    segments2 <- function(x1, y1, dx, dy, ...) {\n\tx2 <- x1 + dx; if(xlog) { x1 <- 10^x1; x2 <- 10^x2 }\n\ty2 <- y1 + dy; if(ylog) { y1 <- 10^y1; y2 <- 10^y2 }\n\tsegments(x1, y1, x2, y2, ...)\n    }\n    points2 <- function(x, y, ...) {\n\tif(xlog) x <- 10^x\n\tif(ylog) y <- 10^y\n\tpoints(x, y, ...)\n    }\n    text2 <- function(x, y, ...) {\n\t##--- need to adjust  adj == c(xadj, yadj) ?? --\n\tif(xlog) x <- 10^x\n\tif(ylog) y <- 10^y\n\ttext(x, y, ...)\n    }\n    if(trace)\n\tcatn <- function(...)\n\t    do.call(\"cat\", c(lapply(list(...),formatC), list(\"\\n\")))\n\n    cin <- par(\"cin\")\n    Cex <- cex * par(\"cex\")\t\t# = the `effective' cex for text\n\n    ## at this point we want positive width even for reversed x axis.\n    if(is.null(text.width))\n\ttext.width <- max(abs(strwidth(legend, units=\"user\",\n\t\t\t\t       cex=cex, font = text.font)))\n    else if(!is.numeric(text.width) || text.width < 0)\n\tstop(\"'text.width' must be numeric, >= 0\")\n\n    xc <- Cex * xinch(cin[1L], warn.log=FALSE) # [uses par(\"usr\") and \"pin\"]\n    yc <- Cex * yinch(cin[2L], warn.log=FALSE)\n    if(xc < 0) text.width <- -text.width\n\n    xchar  <- xc\n    xextra <- 0\n    yextra <- yc * (y.intersp - 1)\n    ## watch out for reversed axis here: heights can be negative\n    ymax   <- yc * max(1, strheight(legend, units=\"user\", cex=cex)/yc)\n    ychar <- yextra + ymax\n    if(trace) catn(\"  xchar=\", xchar, \"; (yextra,ychar)=\", c(yextra,ychar))\n\n    if(mfill) {\n\t##= sizes of filled boxes.\n\txbox <- xc * 0.8\n\tybox <- yc * 0.5\n\tdx.fill <- xbox ## + x.intersp*xchar\n    }\n    do.lines <- (!missing(lty) && (is.character(lty) || any(lty > 0))\n\t\t ) || !missing(lwd)\n\n    ## legends per column:\n    n.legpercol <-\n\tif(horiz) {\n\t    if(ncol != 1)\n                warning(gettextf(\"horizontal specification overrides: Number of columns := %d\",\n                                 n.leg), domain = NA)\n\t    ncol <- n.leg\n\t    1\n\t} else ceiling(n.leg / ncol)\n\n    has.pch <- !missing(pch) && length(pch) > 0 # -> default 'merge' is available\n    if(do.lines) {\n\tx.off <- if(merge) -0.7 else 0\n    } else if(merge)\n\twarning(\"'merge = TRUE' has no effect when no line segments are drawn\")\n\n    if(has.pch) {\n\tif(is.character(pch) && !is.na(pch[1L]) &&\n           nchar(pch[1L], type = \"c\") > 1) {\n\t    if(length(pch) > 1)\n\t\twarning(\"not using pch[2..] since pch[1L] has multiple chars\")\n\t    np <- nchar(pch[1L], type = \"c\")\n\t    pch <- substr(rep.int(pch[1L], np), 1L:np, 1L:np)\n\t}\n        ## this coercion was documented but not done in R < 3.0.0\n        if(!is.character(pch)) pch <- as.integer(pch)\n    }\n\n    if (is.na(auto)) {\n\t##- Adjust (x,y) :\n\tif (xlog) x <- log10(x)\n\tif (ylog) y <- log10(y)\n    }\n    if(nx == 2) {\n\t## (x,y) are specifiying OPPOSITE corners of the box\n\tx <- sort(x)\n\ty <- sort(y)\n\tleft <- x[1L]\n\ttop  <- y[2L]\n\tw <- diff(x)# width\n\th <- diff(y)# height\n\tw0 <- w/ncol # column width\n\n\tx <- mean(x)\n\ty <- mean(y)\n\tif(missing(xjust)) xjust <- 0.5\n\tif(missing(yjust)) yjust <- 0.5\n\n    }\n    else {## nx == 1  or  auto\n\t## -- (w,h) := (width,height) of the box to draw -- computed in steps\n\th <- (n.legpercol + !is.null(title)) * ychar + yc\n\tw0 <- text.width + (x.intersp + 1) * xchar\n\tif(mfill)\tw0 <- w0 + dx.fill\n\tif(do.lines)\tw0 <- w0 + (seg.len + x.off)*xchar\n\tw <- ncol*w0 + .5* xchar\n\tif (!is.null(title)\n\t    && (abs(tw <- strwidth(title, units=\"user\", cex=cex) + 0.5*xchar)) > abs(w)) {\n\t    xextra <- (tw - w)/2\n\t    w <- tw\n\t}\n\n\t##-- (w,h) are now the final box width/height.\n\n\tif (is.na(auto)) {\n\t    left <- x - xjust * w\n\t    top\t <- y + (1 - yjust) * h\n\t} else {\n\t    usr <- par(\"usr\")\n\t    inset <- rep_len(inset, 2)\n\t    insetx <- inset[1L]*(usr[2L] - usr[1L])\n\t    left <- switch(auto, \"bottomright\" =,\n\t\t\t   \"topright\" =, \"right\" = usr[2L] - w - insetx,\n\t\t\t   \"bottomleft\" =, \"left\" =, \"topleft\" = usr[1L] + insetx,\n\t\t\t   \"bottom\" =, \"top\" =, \"center\" = (usr[1L] + usr[2L] - w)/2)\n\t    insety <- inset[2L]*(usr[4L] - usr[3L])\n\t    top <- switch(auto, \"bottomright\" =,\n\t\t\t  \"bottom\" =, \"bottomleft\" = usr[3L] + h + insety,\n\t\t\t  \"topleft\" =, \"top\" =, \"topright\" = usr[4L] - insety,\n\t\t\t  \"left\" =, \"right\" =, \"center\" = (usr[3L] + usr[4L] + h)/2)\n\t}\n    }\n\n    if (plot && bty != \"n\") { ## The legend box :\n\tif(trace)\n\t    catn(\"  rect2(\", left, \",\", top,\", w=\", w, \", h=\", h, \", ...)\",\n                 sep = \"\")\n\trect2(left, top, dx = w, dy = h, col = bg, density = NULL,\n              lwd = box.lwd, lty = box.lty, border = box.col)\n    }\n\n    ## (xt[],yt[]) := `current' vectors of (x/y) legend text\n    xt <- left + xchar + xextra +\n\t(w0 * rep.int(0:(ncol-1), rep.int(n.legpercol,ncol)))[1L:n.leg]\n    yt <- top -\t0.5 * yextra - ymax -\n\t(rep.int(1L:n.legpercol,ncol)[1L:n.leg] - 1 + !is.null(title)) * ychar\n\n    if (mfill) {\t\t#- draw filled boxes -------------\n\tif(plot) {\n\t    if(!is.null(fill)) fill <- rep_len(fill, n.leg)\n\t    rect2(left = xt, top=yt+ybox/2, dx = xbox, dy = ybox,\n\t\t  col = fill,\n\t\t  density = density, angle = angle, border = border)\n\t}\n\txt <- xt + dx.fill\n    }\n    if(plot && (has.pch || do.lines))\n\tcol <- rep_len(col, n.leg)\n\n    ## NULL is not documented but people use it.\n    if(missing(lwd) || is.null(lwd))\n\tlwd <- par(\"lwd\") # = default for pt.lwd\n    if (do.lines) {\t\t\t#- draw lines ---------------------\n        ## NULL is not documented\n\tif(missing(lty) || is.null(lty)) lty <- 1\n\tlty <- rep_len(lty, n.leg)\n\tlwd <- rep_len(lwd, n.leg)\n\tok.l <- !is.na(lty) & (is.character(lty) | lty > 0) & !is.na(lwd)\n\tif(trace)\n\t    catn(\"  segments2(\",xt[ok.l] + x.off*xchar, \",\", yt[ok.l],\n\t\t \", dx=\", seg.len*xchar, \", dy=0, ...)\")\n\tif(plot)\n\t    segments2(xt[ok.l] + x.off*xchar, yt[ok.l],\n                      dx = seg.len*xchar, dy = 0,\n\t\t      lty = lty[ok.l], lwd = lwd[ok.l], col = col[ok.l])\n\t# if (!merge)\n\txt <- xt + (seg.len+x.off) * xchar\n    }\n    if (has.pch) {\t\t\t#- draw points -------------------\n\tpch <- rep_len(pch, n.leg)\n\tpt.bg <- rep_len(pt.bg, n.leg)\n\tpt.cex <- rep_len(pt.cex, n.leg)\n\tpt.lwd <- rep_len(pt.lwd, n.leg)\n        ok <- !is.na(pch)\n        if (!is.character(pch)) {\n            ## R 2.x.y omitted pch < 0\n            ok <- ok & (pch >= 0 | pch <= -32)\n        } else {\n            ## like points\n            ok <- ok & nzchar(pch)\n        }\n\tx1 <- (if(merge && do.lines) xt-(seg.len/2)*xchar else xt)[ok]\n\ty1 <- yt[ok]\n\tif(trace)\n\t    catn(\"  points2(\", x1,\",\", y1,\", pch=\", pch[ok],\", ...)\")\n\tif(plot)\n\t    points2(x1, y1, pch = pch[ok], col = col[ok],\n\t\t    cex = pt.cex[ok], bg = pt.bg[ok], lwd = pt.lwd[ok])\n##D\tif (!merge) xt <- xt + dx.pch\n    }\n\n    xt <- xt + x.intersp * xchar\n    if(plot) {\n\tif (!is.null(title))\n            text2(left + w*title.adj, top - ymax, labels = title,\n                  adj = c(title.adj, 0), cex = cex, col = title.col)\n\n\ttext2(xt, yt, labels = legend, adj = adj, cex = cex,\n\t      col = text.col, font = text.font)\n    }\n    invisible(list(rect = list(w = w, h = h, left = left, top = top),\n\t\t   text = list(x = xt, y = yt)))\n}\n" }
{ "repo_name": "allr/timeR", "ref": "refs/heads/master", "path": "src/library/graphics/R/legend.R", "content": "#  File src/library/graphics/R/legend.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2016 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\nlegend <-\nfunction(x, y = NULL, legend, fill = NULL, col = par(\"col\"), border=\"black\",\n         lty, lwd, pch, angle = 45, density = NULL, bty = \"o\", bg = par(\"bg\"),\n         box.lwd = par(\"lwd\"), box.lty = par(\"lty\"), box.col = par(\"fg\"),\n\t pt.bg = NA, cex = 1, pt.cex = cex, pt.lwd = lwd,\n\t xjust = 0, yjust = 1, x.intersp = 1, y.intersp = 1, adj = c(0, 0.5),\n\t text.width = NULL, text.col = par(\"col\"), text.font = NULL,\n\t merge = do.lines && has.pch, trace = FALSE,\n\t plot = TRUE, ncol = 1, horiz = FALSE, title = NULL,\n\t inset = 0, xpd, title.col = text.col, title.adj = 0.5,\n         seg.len = 2)\n{\n    ## the 2nd arg may really be `legend'\n    if(missing(legend) && !missing(y) &&\n       (is.character(y) || is.expression(y))) {\n\tlegend <- y\n\ty <- NULL\n    }\n    mfill <- !missing(fill) || !missing(density)\n\n    if(!missing(xpd)) {\n        op <- par(\"xpd\")\n        on.exit(par(xpd=op))\n        par(xpd=xpd)\n    }\n    title <- as.graphicsAnnot(title)\n    if(length(title) > 1) stop(\"invalid 'title'\")\n    legend <- as.graphicsAnnot(legend)\n    n.leg <- if(is.call(legend)) 1 else length(legend)\n    if(n.leg == 0) stop(\"'legend' is of length 0\")\n    auto <-\n\tif (is.character(x))\n\t    match.arg(x, c(\"bottomright\", \"bottom\", \"bottomleft\", \"left\",\n\t\t\t   \"topleft\", \"top\", \"topright\", \"right\", \"center\"))\n\telse NA\n\n    if (is.na(auto)) {\n\txy <- xy.coords(x, y, setLab = FALSE); x <- xy$x; y <- xy$y\n\tnx <- length(x)\n\tif (nx < 1 || nx > 2) stop(\"invalid coordinate lengths\")\n    } else nx <- 0\n\n    xlog <- par(\"xlog\")\n    ylog <- par(\"ylog\")\n\n    rect2 <- function(left, top, dx, dy, density = NULL, angle, ...) {\n\tr <- left + dx; if(xlog) { left <- 10^left; r <- 10^r }\n\tb <- top  - dy; if(ylog) {  top <- 10^top;  b <- 10^b }\n\trect(left, top, r, b, angle = angle, density = density, ...)\n    }\n    segments2 <- function(x1, y1, dx, dy, ...) {\n\tx2 <- x1 + dx; if(xlog) { x1 <- 10^x1; x2 <- 10^x2 }\n\ty2 <- y1 + dy; if(ylog) { y1 <- 10^y1; y2 <- 10^y2 }\n\tsegments(x1, y1, x2, y2, ...)\n    }\n    points2 <- function(x, y, ...) {\n\tif(xlog) x <- 10^x\n\tif(ylog) y <- 10^y\n\tpoints(x, y, ...)\n    }\n    text2 <- function(x, y, ...) {\n\t##--- need to adjust  adj == c(xadj, yadj) ?? --\n\tif(xlog) x <- 10^x\n\tif(ylog) y <- 10^y\n\ttext(x, y, ...)\n    }\n    if(trace)\n\tcatn <- function(...)\n\t    do.call(\"cat\", c(lapply(list(...),formatC), list(\"\\n\")))\n\n    cin <- par(\"cin\")\n    Cex <- cex * par(\"cex\")\t\t# = the `effective' cex for text\n\n    ## at this point we want positive width even for reversed x axis.\n    if(is.null(text.width))\n\ttext.width <- max(abs(strwidth(legend, units=\"user\",\n\t\t\t\t       cex=cex, font = text.font)))\n    else if(!is.numeric(text.width) || text.width < 0)\n\tstop(\"'text.width' must be numeric, >= 0\")\n\n    xc <- Cex * xinch(cin[1L], warn.log=FALSE) # [uses par(\"usr\") and \"pin\"]\n    yc <- Cex * yinch(cin[2L], warn.log=FALSE)\n    if(xc < 0) text.width <- -text.width\n\n    xchar  <- xc\n    xextra <- 0\n    yextra <- yc * (y.intersp - 1)\n    ## watch out for reversed axis here: heights can be negative\n    ymax   <- yc * max(1, strheight(legend, units=\"user\", cex=cex)/yc)\n    ychar <- yextra + ymax\n    if(trace) catn(\"  xchar=\", xchar, \"; (yextra,ychar)=\", c(yextra,ychar))\n\n    if(mfill) {\n\t##= sizes of filled boxes.\n\txbox <- xc * 0.8\n\tybox <- yc * 0.5\n\tdx.fill <- xbox ## + x.intersp*xchar\n    }\n    do.lines <- (!missing(lty) && (is.character(lty) || any(lty > 0))\n\t\t ) || !missing(lwd)\n\n    ## legends per column:\n    n.legpercol <-\n\tif(horiz) {\n\t    if(ncol != 1)\n                warning(gettextf(\"horizontal specification overrides: Number of columns := %d\",\n                                 n.leg), domain = NA)\n\t    ncol <- n.leg\n\t    1\n\t} else ceiling(n.leg / ncol)\n\n    has.pch <- !missing(pch) && length(pch) > 0 # -> default 'merge' is available\n    if(do.lines) {\n\tx.off <- if(merge) -0.7 else 0\n    } else if(merge)\n\twarning(\"'merge = TRUE' has no effect when no line segments are drawn\")\n\n    if(has.pch) {\n\tif(is.character(pch) && !is.na(pch[1L]) &&\n           nchar(pch[1L], type = \"c\") > 1) {\n\t    if(length(pch) > 1)\n\t\twarning(\"not using pch[2..] since pch[1L] has multiple chars\")\n\t    np <- nchar(pch[1L], type = \"c\")\n\t    pch <- substr(rep.int(pch[1L], np), 1L:np, 1L:np)\n\t}\n        ## this coercion was documented but not done in R < 3.0.0\n        if(!is.character(pch)) pch <- as.integer(pch)\n    }\n\n    if (is.na(auto)) {\n\t##- Adjust (x,y) :\n\tif (xlog) x <- log10(x)\n\tif (ylog) y <- log10(y)\n    }\n    if(nx == 2) {\n\t## (x,y) are specifiying OPPOSITE corners of the box\n\tx <- sort(x)\n\ty <- sort(y)\n\tleft <- x[1L]\n\ttop  <- y[2L]\n\tw <- diff(x)# width\n\th <- diff(y)# height\n\tw0 <- w/ncol # column width\n\n\tx <- mean(x)\n\ty <- mean(y)\n\tif(missing(xjust)) xjust <- 0.5\n\tif(missing(yjust)) yjust <- 0.5\n\n    }\n    else {## nx == 1  or  auto\n\t## -- (w,h) := (width,height) of the box to draw -- computed in steps\n\th <- (n.legpercol + !is.null(title)) * ychar + yc\n\tw0 <- text.width + (x.intersp + 1) * xchar\n\tif(mfill)\tw0 <- w0 + dx.fill\n\tif(do.lines)\tw0 <- w0 + (seg.len + x.off)*xchar\n\tw <- ncol*w0 + .5* xchar\n\tif (!is.null(title)\n\t    && (abs(tw <- strwidth(title, units=\"user\", cex=cex) + 0.5*xchar)) > abs(w)) {\n\t    xextra <- (tw - w)/2\n\t    w <- tw\n\t}\n\n\t##-- (w,h) are now the final box width/height.\n\n\tif (is.na(auto)) {\n\t    left <- x - xjust * w\n\t    top\t <- y + (1 - yjust) * h\n\t} else {\n\t    usr <- par(\"usr\")\n\t    inset <- rep_len(inset, 2)\n\t    insetx <- inset[1L]*(usr[2L] - usr[1L])\n\t    left <- switch(auto, \"bottomright\" =,\n\t\t\t   \"topright\" =, \"right\" = usr[2L] - w - insetx,\n\t\t\t   \"bottomleft\" =, \"left\" =, \"topleft\" = usr[1L] + insetx,\n\t\t\t   \"bottom\" =, \"top\" =, \"center\" = (usr[1L] + usr[2L] - w)/2)\n\t    insety <- inset[2L]*(usr[4L] - usr[3L])\n\t    top <- switch(auto, \"bottomright\" =,\n\t\t\t  \"bottom\" =, \"bottomleft\" = usr[3L] + h + insety,\n\t\t\t  \"topleft\" =, \"top\" =, \"topright\" = usr[4L] - insety,\n\t\t\t  \"left\" =, \"right\" =, \"center\" = (usr[3L] + usr[4L] + h)/2)\n\t}\n    }\n\n    if (plot && bty != \"n\") { ## The legend box :\n\tif(trace)\n\t    catn(\"  rect2(\", left, \",\", top,\", w=\", w, \", h=\", h, \", ...)\",\n                 sep = \"\")\n\trect2(left, top, dx = w, dy = h, col = bg, density = NULL,\n              lwd = box.lwd, lty = box.lty, border = box.col)\n    }\n\n    ## (xt[],yt[]) := `current' vectors of (x/y) legend text\n    xt <- left + xchar + xextra +\n\t(w0 * rep.int(0:(ncol-1), rep.int(n.legpercol,ncol)))[1L:n.leg]\n    yt <- top -\t0.5 * yextra - ymax -\n\t(rep.int(1L:n.legpercol,ncol)[1L:n.leg] - 1 + !is.null(title)) * ychar\n\n    if (mfill) {\t\t#- draw filled boxes -------------\n\tif(plot) {\n\t    if(!is.null(fill)) fill <- rep_len(fill, n.leg)\n\t    rect2(left = xt, top=yt+ybox/2, dx = xbox, dy = ybox,\n\t\t  col = fill,\n\t\t  density = density, angle = angle, border = border)\n\t}\n\txt <- xt + dx.fill\n    }\n    if(plot && (has.pch || do.lines))\n\tcol <- rep_len(col, n.leg)\n\n    ## NULL is not documented but people use it.\n    if(missing(lwd) || is.null(lwd))\n\tlwd <- par(\"lwd\") # = default for pt.lwd\n    if (do.lines) {\t\t\t#- draw lines ---------------------\n        ## NULL is not documented\n\tif(missing(lty) || is.null(lty)) lty <- 1\n\tlty <- rep_len(lty, n.leg)\n\tlwd <- rep_len(lwd, n.leg)\n\tok.l <- !is.na(lty) & (is.character(lty) | lty > 0) & !is.na(lwd)\n\tif(trace)\n\t    catn(\"  segments2(\",xt[ok.l] + x.off*xchar, \",\", yt[ok.l],\n\t\t \", dx=\", seg.len*xchar, \", dy=0, ...)\")\n\tif(plot)\n\t    segments2(xt[ok.l] + x.off*xchar, yt[ok.l],\n                      dx = seg.len*xchar, dy = 0,\n\t\t      lty = lty[ok.l], lwd = lwd[ok.l], col = col[ok.l])\n\t# if (!merge)\n\txt <- xt + (seg.len+x.off) * xchar\n    }\n    if (has.pch) {\t\t\t#- draw points -------------------\n\tpch <- rep_len(pch, n.leg)\n\tpt.bg <- rep_len(pt.bg, n.leg)\n\tpt.cex <- rep_len(pt.cex, n.leg)\n\tpt.lwd <- rep_len(pt.lwd, n.leg)\n        ok <- !is.na(pch)\n        if (!is.character(pch)) {\n            ## R 2.x.y omitted pch < 0\n            ok <- ok & (pch >= 0 | pch <= -32)\n        } else {\n            ## like points\n            ok <- ok & nzchar(pch)\n        }\n\tx1 <- (if(merge && do.lines) xt-(seg.len/2)*xchar else xt)[ok]\n\ty1 <- yt[ok]\n\tif(trace)\n\t    catn(\"  points2(\", x1,\",\", y1,\", pch=\", pch[ok],\", ...)\")\n\tif(plot)\n\t    points2(x1, y1, pch = pch[ok], col = col[ok],\n\t\t    cex = pt.cex[ok], bg = pt.bg[ok], lwd = pt.lwd[ok])\n##D\tif (!merge) xt <- xt + dx.pch\n    }\n\n    xt <- xt + x.intersp * xchar\n    if(plot) {\n\tif (!is.null(title))\n            text2(left + w*title.adj, top - ymax, labels = title,\n                  adj = c(title.adj, 0), cex = cex, col = title.col)\n\n\ttext2(xt, yt, labels = legend, adj = adj, cex = cex,\n\t      col = text.col, font = text.font)\n    }\n    invisible(list(rect = list(w = w, h = h, left = left, top = top),\n\t\t   text = list(x = xt, y = yt)))\n}\n" }
{ "repo_name": "krlmlr/r-source", "ref": "refs/heads/master", "path": "src/library/graphics/R/legend.R", "content": "#  File src/library/graphics/R/legend.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2016 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\nlegend <-\nfunction(x, y = NULL, legend, fill = NULL, col = par(\"col\"), border=\"black\",\n         lty, lwd, pch, angle = 45, density = NULL, bty = \"o\", bg = par(\"bg\"),\n         box.lwd = par(\"lwd\"), box.lty = par(\"lty\"), box.col = par(\"fg\"),\n\t pt.bg = NA, cex = 1, pt.cex = cex, pt.lwd = lwd,\n\t xjust = 0, yjust = 1, x.intersp = 1, y.intersp = 1, adj = c(0, 0.5),\n\t text.width = NULL, text.col = par(\"col\"), text.font = NULL,\n\t merge = do.lines && has.pch, trace = FALSE,\n\t plot = TRUE, ncol = 1, horiz = FALSE, title = NULL,\n\t inset = 0, xpd, title.col = text.col, title.adj = 0.5,\n         seg.len = 2)\n{\n    ## the 2nd arg may really be `legend'\n    if(missing(legend) && !missing(y) &&\n       (is.character(y) || is.expression(y))) {\n\tlegend <- y\n\ty <- NULL\n    }\n    mfill <- !missing(fill) || !missing(density)\n\n    if(!missing(xpd)) {\n        op <- par(\"xpd\")\n        on.exit(par(xpd=op))\n        par(xpd=xpd)\n    }\n    title <- as.graphicsAnnot(title)\n    if(length(title) > 1) stop(\"invalid 'title'\")\n    legend <- as.graphicsAnnot(legend)\n    n.leg <- if(is.call(legend)) 1 else length(legend)\n    if(n.leg == 0) stop(\"'legend' is of length 0\")\n    auto <-\n\tif (is.character(x))\n\t    match.arg(x, c(\"bottomright\", \"bottom\", \"bottomleft\", \"left\",\n\t\t\t   \"topleft\", \"top\", \"topright\", \"right\", \"center\"))\n\telse NA\n\n    if (is.na(auto)) {\n\txy <- xy.coords(x, y, setLab = FALSE); x <- xy$x; y <- xy$y\n\tnx <- length(x)\n\tif (nx < 1 || nx > 2) stop(\"invalid coordinate lengths\")\n    } else nx <- 0\n\n    xlog <- par(\"xlog\")\n    ylog <- par(\"ylog\")\n\n    rect2 <- function(left, top, dx, dy, density = NULL, angle, ...) {\n\tr <- left + dx; if(xlog) { left <- 10^left; r <- 10^r }\n\tb <- top  - dy; if(ylog) {  top <- 10^top;  b <- 10^b }\n\trect(left, top, r, b, angle = angle, density = density, ...)\n    }\n    segments2 <- function(x1, y1, dx, dy, ...) {\n\tx2 <- x1 + dx; if(xlog) { x1 <- 10^x1; x2 <- 10^x2 }\n\ty2 <- y1 + dy; if(ylog) { y1 <- 10^y1; y2 <- 10^y2 }\n\tsegments(x1, y1, x2, y2, ...)\n    }\n    points2 <- function(x, y, ...) {\n\tif(xlog) x <- 10^x\n\tif(ylog) y <- 10^y\n\tpoints(x, y, ...)\n    }\n    text2 <- function(x, y, ...) {\n\t##--- need to adjust  adj == c(xadj, yadj) ?? --\n\tif(xlog) x <- 10^x\n\tif(ylog) y <- 10^y\n\ttext(x, y, ...)\n    }\n    if(trace)\n\tcatn <- function(...)\n\t    do.call(\"cat\", c(lapply(list(...),formatC), list(\"\\n\")))\n\n    cin <- par(\"cin\")\n    Cex <- cex * par(\"cex\")\t\t# = the `effective' cex for text\n\n    ## at this point we want positive width even for reversed x axis.\n    if(is.null(text.width))\n\ttext.width <- max(abs(strwidth(legend, units=\"user\",\n\t\t\t\t       cex=cex, font = text.font)))\n    else if(!is.numeric(text.width) || text.width < 0)\n\tstop(\"'text.width' must be numeric, >= 0\")\n\n    xc <- Cex * xinch(cin[1L], warn.log=FALSE) # [uses par(\"usr\") and \"pin\"]\n    yc <- Cex * yinch(cin[2L], warn.log=FALSE)\n    if(xc < 0) text.width <- -text.width\n\n    xchar  <- xc\n    xextra <- 0\n    yextra <- yc * (y.intersp - 1)\n    ## watch out for reversed axis here: heights can be negative\n    ymax   <- yc * max(1, strheight(legend, units=\"user\", cex=cex)/yc)\n    ychar <- yextra + ymax\n    if(trace) catn(\"  xchar=\", xchar, \"; (yextra,ychar)=\", c(yextra,ychar))\n\n    if(mfill) {\n\t##= sizes of filled boxes.\n\txbox <- xc * 0.8\n\tybox <- yc * 0.5\n\tdx.fill <- xbox ## + x.intersp*xchar\n    }\n    do.lines <- (!missing(lty) && (is.character(lty) || any(lty > 0))\n\t\t ) || !missing(lwd)\n\n    ## legends per column:\n    n.legpercol <-\n\tif(horiz) {\n\t    if(ncol != 1)\n                warning(gettextf(\"horizontal specification overrides: Number of columns := %d\",\n                                 n.leg), domain = NA)\n\t    ncol <- n.leg\n\t    1\n\t} else ceiling(n.leg / ncol)\n\n    has.pch <- !missing(pch) && length(pch) > 0 # -> default 'merge' is available\n    if(do.lines) {\n\tx.off <- if(merge) -0.7 else 0\n    } else if(merge)\n\twarning(\"'merge = TRUE' has no effect when no line segments are drawn\")\n\n    if(has.pch) {\n\tif(is.character(pch) && !is.na(pch[1L]) &&\n           nchar(pch[1L], type = \"c\") > 1) {\n\t    if(length(pch) > 1)\n\t\twarning(\"not using pch[2..] since pch[1L] has multiple chars\")\n\t    np <- nchar(pch[1L], type = \"c\")\n\t    pch <- substr(rep.int(pch[1L], np), 1L:np, 1L:np)\n\t}\n        ## this coercion was documented but not done in R < 3.0.0\n        if(!is.character(pch)) pch <- as.integer(pch)\n    }\n\n    if (is.na(auto)) {\n\t##- Adjust (x,y) :\n\tif (xlog) x <- log10(x)\n\tif (ylog) y <- log10(y)\n    }\n    if(nx == 2) {\n\t## (x,y) are specifiying OPPOSITE corners of the box\n\tx <- sort(x)\n\ty <- sort(y)\n\tleft <- x[1L]\n\ttop  <- y[2L]\n\tw <- diff(x)# width\n\th <- diff(y)# height\n\tw0 <- w/ncol # column width\n\n\tx <- mean(x)\n\ty <- mean(y)\n\tif(missing(xjust)) xjust <- 0.5\n\tif(missing(yjust)) yjust <- 0.5\n\n    }\n    else {## nx == 1  or  auto\n\t## -- (w,h) := (width,height) of the box to draw -- computed in steps\n\th <- (n.legpercol + !is.null(title)) * ychar + yc\n\tw0 <- text.width + (x.intersp + 1) * xchar\n\tif(mfill)\tw0 <- w0 + dx.fill\n\tif(do.lines)\tw0 <- w0 + (seg.len + x.off)*xchar\n\tw <- ncol*w0 + .5* xchar\n\tif (!is.null(title)\n\t    && (abs(tw <- strwidth(title, units=\"user\", cex=cex) + 0.5*xchar)) > abs(w)) {\n\t    xextra <- (tw - w)/2\n\t    w <- tw\n\t}\n\n\t##-- (w,h) are now the final box width/height.\n\n\tif (is.na(auto)) {\n\t    left <- x - xjust * w\n\t    top\t <- y + (1 - yjust) * h\n\t} else {\n\t    usr <- par(\"usr\")\n\t    inset <- rep_len(inset, 2)\n\t    insetx <- inset[1L]*(usr[2L] - usr[1L])\n\t    left <- switch(auto, \"bottomright\" =,\n\t\t\t   \"topright\" =, \"right\" = usr[2L] - w - insetx,\n\t\t\t   \"bottomleft\" =, \"left\" =, \"topleft\" = usr[1L] + insetx,\n\t\t\t   \"bottom\" =, \"top\" =, \"center\" = (usr[1L] + usr[2L] - w)/2)\n\t    insety <- inset[2L]*(usr[4L] - usr[3L])\n\t    top <- switch(auto, \"bottomright\" =,\n\t\t\t  \"bottom\" =, \"bottomleft\" = usr[3L] + h + insety,\n\t\t\t  \"topleft\" =, \"top\" =, \"topright\" = usr[4L] - insety,\n\t\t\t  \"left\" =, \"right\" =, \"center\" = (usr[3L] + usr[4L] + h)/2)\n\t}\n    }\n\n    if (plot && bty != \"n\") { ## The legend box :\n\tif(trace)\n\t    catn(\"  rect2(\", left, \",\", top,\", w=\", w, \", h=\", h, \", ...)\",\n                 sep = \"\")\n\trect2(left, top, dx = w, dy = h, col = bg, density = NULL,\n              lwd = box.lwd, lty = box.lty, border = box.col)\n    }\n\n    ## (xt[],yt[]) := `current' vectors of (x/y) legend text\n    xt <- left + xchar + xextra +\n\t(w0 * rep.int(0:(ncol-1), rep.int(n.legpercol,ncol)))[1L:n.leg]\n    yt <- top -\t0.5 * yextra - ymax -\n\t(rep.int(1L:n.legpercol,ncol)[1L:n.leg] - 1 + !is.null(title)) * ychar\n\n    if (mfill) {\t\t#- draw filled boxes -------------\n\tif(plot) {\n\t    if(!is.null(fill)) fill <- rep_len(fill, n.leg)\n\t    rect2(left = xt, top=yt+ybox/2, dx = xbox, dy = ybox,\n\t\t  col = fill,\n\t\t  density = density, angle = angle, border = border)\n\t}\n\txt <- xt + dx.fill\n    }\n    if(plot && (has.pch || do.lines))\n\tcol <- rep_len(col, n.leg)\n\n    ## NULL is not documented but people use it.\n    if(missing(lwd) || is.null(lwd))\n\tlwd <- par(\"lwd\") # = default for pt.lwd\n    if (do.lines) {\t\t\t#- draw lines ---------------------\n        ## NULL is not documented\n\tif(missing(lty) || is.null(lty)) lty <- 1\n\tlty <- rep_len(lty, n.leg)\n\tlwd <- rep_len(lwd, n.leg)\n\tok.l <- !is.na(lty) & (is.character(lty) | lty > 0) & !is.na(lwd)\n\tif(trace)\n\t    catn(\"  segments2(\",xt[ok.l] + x.off*xchar, \",\", yt[ok.l],\n\t\t \", dx=\", seg.len*xchar, \", dy=0, ...)\")\n\tif(plot)\n\t    segments2(xt[ok.l] + x.off*xchar, yt[ok.l],\n                      dx = seg.len*xchar, dy = 0,\n\t\t      lty = lty[ok.l], lwd = lwd[ok.l], col = col[ok.l])\n\t# if (!merge)\n\txt <- xt + (seg.len+x.off) * xchar\n    }\n    if (has.pch) {\t\t\t#- draw points -------------------\n\tpch <- rep_len(pch, n.leg)\n\tpt.bg <- rep_len(pt.bg, n.leg)\n\tpt.cex <- rep_len(pt.cex, n.leg)\n\tpt.lwd <- rep_len(pt.lwd, n.leg)\n        ok <- !is.na(pch)\n        if (!is.character(pch)) {\n            ## R 2.x.y omitted pch < 0\n            ok <- ok & (pch >= 0 | pch <= -32)\n        } else {\n            ## like points\n            ok <- ok & nzchar(pch)\n        }\n\tx1 <- (if(merge && do.lines) xt-(seg.len/2)*xchar else xt)[ok]\n\ty1 <- yt[ok]\n\tif(trace)\n\t    catn(\"  points2(\", x1,\",\", y1,\", pch=\", pch[ok],\", ...)\")\n\tif(plot)\n\t    points2(x1, y1, pch = pch[ok], col = col[ok],\n\t\t    cex = pt.cex[ok], bg = pt.bg[ok], lwd = pt.lwd[ok])\n##D\tif (!merge) xt <- xt + dx.pch\n    }\n\n    xt <- xt + x.intersp * xchar\n    if(plot) {\n\tif (!is.null(title))\n            text2(left + w*title.adj, top - ymax, labels = title,\n                  adj = c(title.adj, 0), cex = cex, col = title.col)\n\n\ttext2(xt, yt, labels = legend, adj = adj, cex = cex,\n\t      col = text.col, font = text.font)\n    }\n    invisible(list(rect = list(w = w, h = h, left = left, top = top),\n\t\t   text = list(x = xt, y = yt)))\n}\n" }
{ "repo_name": "allr/r-instrumented", "ref": "refs/heads/master", "path": "src/library/graphics/R/legend.R", "content": "#  File src/library/graphics/R/legend.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2016 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\nlegend <-\nfunction(x, y = NULL, legend, fill = NULL, col = par(\"col\"), border=\"black\",\n         lty, lwd, pch, angle = 45, density = NULL, bty = \"o\", bg = par(\"bg\"),\n         box.lwd = par(\"lwd\"), box.lty = par(\"lty\"), box.col = par(\"fg\"),\n\t pt.bg = NA, cex = 1, pt.cex = cex, pt.lwd = lwd,\n\t xjust = 0, yjust = 1, x.intersp = 1, y.intersp = 1, adj = c(0, 0.5),\n\t text.width = NULL, text.col = par(\"col\"), text.font = NULL,\n\t merge = do.lines && has.pch, trace = FALSE,\n\t plot = TRUE, ncol = 1, horiz = FALSE, title = NULL,\n\t inset = 0, xpd, title.col = text.col, title.adj = 0.5,\n         seg.len = 2)\n{\n    ## the 2nd arg may really be `legend'\n    if(missing(legend) && !missing(y) &&\n       (is.character(y) || is.expression(y))) {\n\tlegend <- y\n\ty <- NULL\n    }\n    mfill <- !missing(fill) || !missing(density)\n\n    if(!missing(xpd)) {\n        op <- par(\"xpd\")\n        on.exit(par(xpd=op))\n        par(xpd=xpd)\n    }\n    title <- as.graphicsAnnot(title)\n    if(length(title) > 1) stop(\"invalid 'title'\")\n    legend <- as.graphicsAnnot(legend)\n    n.leg <- if(is.call(legend)) 1 else length(legend)\n    if(n.leg == 0) stop(\"'legend' is of length 0\")\n    auto <-\n\tif (is.character(x))\n\t    match.arg(x, c(\"bottomright\", \"bottom\", \"bottomleft\", \"left\",\n\t\t\t   \"topleft\", \"top\", \"topright\", \"right\", \"center\"))\n\telse NA\n\n    if (is.na(auto)) {\n\txy <- xy.coords(x, y, setLab = FALSE); x <- xy$x; y <- xy$y\n\tnx <- length(x)\n\tif (nx < 1 || nx > 2) stop(\"invalid coordinate lengths\")\n    } else nx <- 0\n\n    xlog <- par(\"xlog\")\n    ylog <- par(\"ylog\")\n\n    rect2 <- function(left, top, dx, dy, density = NULL, angle, ...) {\n\tr <- left + dx; if(xlog) { left <- 10^left; r <- 10^r }\n\tb <- top  - dy; if(ylog) {  top <- 10^top;  b <- 10^b }\n\trect(left, top, r, b, angle = angle, density = density, ...)\n    }\n    segments2 <- function(x1, y1, dx, dy, ...) {\n\tx2 <- x1 + dx; if(xlog) { x1 <- 10^x1; x2 <- 10^x2 }\n\ty2 <- y1 + dy; if(ylog) { y1 <- 10^y1; y2 <- 10^y2 }\n\tsegments(x1, y1, x2, y2, ...)\n    }\n    points2 <- function(x, y, ...) {\n\tif(xlog) x <- 10^x\n\tif(ylog) y <- 10^y\n\tpoints(x, y, ...)\n    }\n    text2 <- function(x, y, ...) {\n\t##--- need to adjust  adj == c(xadj, yadj) ?? --\n\tif(xlog) x <- 10^x\n\tif(ylog) y <- 10^y\n\ttext(x, y, ...)\n    }\n    if(trace)\n\tcatn <- function(...)\n\t    do.call(\"cat\", c(lapply(list(...),formatC), list(\"\\n\")))\n\n    cin <- par(\"cin\")\n    Cex <- cex * par(\"cex\")\t\t# = the `effective' cex for text\n\n    ## at this point we want positive width even for reversed x axis.\n    if(is.null(text.width))\n\ttext.width <- max(abs(strwidth(legend, units=\"user\",\n\t\t\t\t       cex=cex, font = text.font)))\n    else if(!is.numeric(text.width) || text.width < 0)\n\tstop(\"'text.width' must be numeric, >= 0\")\n\n    xc <- Cex * xinch(cin[1L], warn.log=FALSE) # [uses par(\"usr\") and \"pin\"]\n    yc <- Cex * yinch(cin[2L], warn.log=FALSE)\n    if(xc < 0) text.width <- -text.width\n\n    xchar  <- xc\n    xextra <- 0\n    yextra <- yc * (y.intersp - 1)\n    ## watch out for reversed axis here: heights can be negative\n    ymax   <- yc * max(1, strheight(legend, units=\"user\", cex=cex)/yc)\n    ychar <- yextra + ymax\n    if(trace) catn(\"  xchar=\", xchar, \"; (yextra,ychar)=\", c(yextra,ychar))\n\n    if(mfill) {\n\t##= sizes of filled boxes.\n\txbox <- xc * 0.8\n\tybox <- yc * 0.5\n\tdx.fill <- xbox ## + x.intersp*xchar\n    }\n    do.lines <- (!missing(lty) && (is.character(lty) || any(lty > 0))\n\t\t ) || !missing(lwd)\n\n    ## legends per column:\n    n.legpercol <-\n\tif(horiz) {\n\t    if(ncol != 1)\n                warning(gettextf(\"horizontal specification overrides: Number of columns := %d\",\n                                 n.leg), domain = NA)\n\t    ncol <- n.leg\n\t    1\n\t} else ceiling(n.leg / ncol)\n\n    has.pch <- !missing(pch) && length(pch) > 0 # -> default 'merge' is available\n    if(do.lines) {\n\tx.off <- if(merge) -0.7 else 0\n    } else if(merge)\n\twarning(\"'merge = TRUE' has no effect when no line segments are drawn\")\n\n    if(has.pch) {\n\tif(is.character(pch) && !is.na(pch[1L]) &&\n           nchar(pch[1L], type = \"c\") > 1) {\n\t    if(length(pch) > 1)\n\t\twarning(\"not using pch[2..] since pch[1L] has multiple chars\")\n\t    np <- nchar(pch[1L], type = \"c\")\n\t    pch <- substr(rep.int(pch[1L], np), 1L:np, 1L:np)\n\t}\n        ## this coercion was documented but not done in R < 3.0.0\n        if(!is.character(pch)) pch <- as.integer(pch)\n    }\n\n    if (is.na(auto)) {\n\t##- Adjust (x,y) :\n\tif (xlog) x <- log10(x)\n\tif (ylog) y <- log10(y)\n    }\n    if(nx == 2) {\n\t## (x,y) are specifiying OPPOSITE corners of the box\n\tx <- sort(x)\n\ty <- sort(y)\n\tleft <- x[1L]\n\ttop  <- y[2L]\n\tw <- diff(x)# width\n\th <- diff(y)# height\n\tw0 <- w/ncol # column width\n\n\tx <- mean(x)\n\ty <- mean(y)\n\tif(missing(xjust)) xjust <- 0.5\n\tif(missing(yjust)) yjust <- 0.5\n\n    }\n    else {## nx == 1  or  auto\n\t## -- (w,h) := (width,height) of the box to draw -- computed in steps\n\th <- (n.legpercol + !is.null(title)) * ychar + yc\n\tw0 <- text.width + (x.intersp + 1) * xchar\n\tif(mfill)\tw0 <- w0 + dx.fill\n\tif(do.lines)\tw0 <- w0 + (seg.len + x.off)*xchar\n\tw <- ncol*w0 + .5* xchar\n\tif (!is.null(title)\n\t    && (abs(tw <- strwidth(title, units=\"user\", cex=cex) + 0.5*xchar)) > abs(w)) {\n\t    xextra <- (tw - w)/2\n\t    w <- tw\n\t}\n\n\t##-- (w,h) are now the final box width/height.\n\n\tif (is.na(auto)) {\n\t    left <- x - xjust * w\n\t    top\t <- y + (1 - yjust) * h\n\t} else {\n\t    usr <- par(\"usr\")\n\t    inset <- rep_len(inset, 2)\n\t    insetx <- inset[1L]*(usr[2L] - usr[1L])\n\t    left <- switch(auto, \"bottomright\" =,\n\t\t\t   \"topright\" =, \"right\" = usr[2L] - w - insetx,\n\t\t\t   \"bottomleft\" =, \"left\" =, \"topleft\" = usr[1L] + insetx,\n\t\t\t   \"bottom\" =, \"top\" =, \"center\" = (usr[1L] + usr[2L] - w)/2)\n\t    insety <- inset[2L]*(usr[4L] - usr[3L])\n\t    top <- switch(auto, \"bottomright\" =,\n\t\t\t  \"bottom\" =, \"bottomleft\" = usr[3L] + h + insety,\n\t\t\t  \"topleft\" =, \"top\" =, \"topright\" = usr[4L] - insety,\n\t\t\t  \"left\" =, \"right\" =, \"center\" = (usr[3L] + usr[4L] + h)/2)\n\t}\n    }\n\n    if (plot && bty != \"n\") { ## The legend box :\n\tif(trace)\n\t    catn(\"  rect2(\", left, \",\", top,\", w=\", w, \", h=\", h, \", ...)\",\n                 sep = \"\")\n\trect2(left, top, dx = w, dy = h, col = bg, density = NULL,\n              lwd = box.lwd, lty = box.lty, border = box.col)\n    }\n\n    ## (xt[],yt[]) := `current' vectors of (x/y) legend text\n    xt <- left + xchar + xextra +\n\t(w0 * rep.int(0:(ncol-1), rep.int(n.legpercol,ncol)))[1L:n.leg]\n    yt <- top -\t0.5 * yextra - ymax -\n\t(rep.int(1L:n.legpercol,ncol)[1L:n.leg] - 1 + !is.null(title)) * ychar\n\n    if (mfill) {\t\t#- draw filled boxes -------------\n\tif(plot) {\n\t    if(!is.null(fill)) fill <- rep_len(fill, n.leg)\n\t    rect2(left = xt, top=yt+ybox/2, dx = xbox, dy = ybox,\n\t\t  col = fill,\n\t\t  density = density, angle = angle, border = border)\n\t}\n\txt <- xt + dx.fill\n    }\n    if(plot && (has.pch || do.lines))\n\tcol <- rep_len(col, n.leg)\n\n    ## NULL is not documented but people use it.\n    if(missing(lwd) || is.null(lwd))\n\tlwd <- par(\"lwd\") # = default for pt.lwd\n    if (do.lines) {\t\t\t#- draw lines ---------------------\n        ## NULL is not documented\n\tif(missing(lty) || is.null(lty)) lty <- 1\n\tlty <- rep_len(lty, n.leg)\n\tlwd <- rep_len(lwd, n.leg)\n\tok.l <- !is.na(lty) & (is.character(lty) | lty > 0) & !is.na(lwd)\n\tif(trace)\n\t    catn(\"  segments2(\",xt[ok.l] + x.off*xchar, \",\", yt[ok.l],\n\t\t \", dx=\", seg.len*xchar, \", dy=0, ...)\")\n\tif(plot)\n\t    segments2(xt[ok.l] + x.off*xchar, yt[ok.l],\n                      dx = seg.len*xchar, dy = 0,\n\t\t      lty = lty[ok.l], lwd = lwd[ok.l], col = col[ok.l])\n\t# if (!merge)\n\txt <- xt + (seg.len+x.off) * xchar\n    }\n    if (has.pch) {\t\t\t#- draw points -------------------\n\tpch <- rep_len(pch, n.leg)\n\tpt.bg <- rep_len(pt.bg, n.leg)\n\tpt.cex <- rep_len(pt.cex, n.leg)\n\tpt.lwd <- rep_len(pt.lwd, n.leg)\n        ok <- !is.na(pch)\n        if (!is.character(pch)) {\n            ## R 2.x.y omitted pch < 0\n            ok <- ok & (pch >= 0 | pch <= -32)\n        } else {\n            ## like points\n            ok <- ok & nzchar(pch)\n        }\n\tx1 <- (if(merge && do.lines) xt-(seg.len/2)*xchar else xt)[ok]\n\ty1 <- yt[ok]\n\tif(trace)\n\t    catn(\"  points2(\", x1,\",\", y1,\", pch=\", pch[ok],\", ...)\")\n\tif(plot)\n\t    points2(x1, y1, pch = pch[ok], col = col[ok],\n\t\t    cex = pt.cex[ok], bg = pt.bg[ok], lwd = pt.lwd[ok])\n##D\tif (!merge) xt <- xt + dx.pch\n    }\n\n    xt <- xt + x.intersp * xchar\n    if(plot) {\n\tif (!is.null(title))\n            text2(left + w*title.adj, top - ymax, labels = title,\n                  adj = c(title.adj, 0), cex = cex, col = title.col)\n\n\ttext2(xt, yt, labels = legend, adj = adj, cex = cex,\n\t      col = text.col, font = text.font)\n    }\n    invisible(list(rect = list(w = w, h = h, left = left, top = top),\n\t\t   text = list(x = xt, y = yt)))\n}\n" }
{ "repo_name": "bedatadriven/renjin", "ref": "refs/heads/master", "path": "packages/graphics/R/legend.R", "content": "#  File src/library/graphics/R/legend.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2016 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\nlegend <-\nfunction(x, y = NULL, legend, fill = NULL, col = par(\"col\"), border=\"black\",\n         lty, lwd, pch, angle = 45, density = NULL, bty = \"o\", bg = par(\"bg\"),\n         box.lwd = par(\"lwd\"), box.lty = par(\"lty\"), box.col = par(\"fg\"),\n\t pt.bg = NA, cex = 1, pt.cex = cex, pt.lwd = lwd,\n\t xjust = 0, yjust = 1, x.intersp = 1, y.intersp = 1, adj = c(0, 0.5),\n\t text.width = NULL, text.col = par(\"col\"), text.font = NULL,\n\t merge = do.lines && has.pch, trace = FALSE,\n\t plot = TRUE, ncol = 1, horiz = FALSE, title = NULL,\n\t inset = 0, xpd, title.col = text.col, title.adj = 0.5,\n         seg.len = 2)\n{\n    ## the 2nd arg may really be `legend'\n    if(missing(legend) && !missing(y) &&\n       (is.character(y) || is.expression(y))) {\n\tlegend <- y\n\ty <- NULL\n    }\n    mfill <- !missing(fill) || !missing(density)\n\n    if(!missing(xpd)) {\n        op <- par(\"xpd\")\n        on.exit(par(xpd=op))\n        par(xpd=xpd)\n    }\n    title <- as.graphicsAnnot(title)\n    if(length(title) > 1) stop(\"invalid 'title'\")\n    legend <- as.graphicsAnnot(legend)\n    n.leg <- if(is.call(legend)) 1 else length(legend)\n    if(n.leg == 0) stop(\"'legend' is of length 0\")\n    auto <-\n\tif (is.character(x))\n\t    match.arg(x, c(\"bottomright\", \"bottom\", \"bottomleft\", \"left\",\n\t\t\t   \"topleft\", \"top\", \"topright\", \"right\", \"center\"))\n\telse NA\n\n    if (is.na(auto)) {\n\txy <- xy.coords(x, y, setLab = FALSE); x <- xy$x; y <- xy$y\n\tnx <- length(x)\n\tif (nx < 1 || nx > 2) stop(\"invalid coordinate lengths\")\n    } else nx <- 0\n\n    xlog <- par(\"xlog\")\n    ylog <- par(\"ylog\")\n\n    rect2 <- function(left, top, dx, dy, density = NULL, angle, ...) {\n\tr <- left + dx; if(xlog) { left <- 10^left; r <- 10^r }\n\tb <- top  - dy; if(ylog) {  top <- 10^top;  b <- 10^b }\n\trect(left, top, r, b, angle = angle, density = density, ...)\n    }\n    segments2 <- function(x1, y1, dx, dy, ...) {\n\tx2 <- x1 + dx; if(xlog) { x1 <- 10^x1; x2 <- 10^x2 }\n\ty2 <- y1 + dy; if(ylog) { y1 <- 10^y1; y2 <- 10^y2 }\n\tsegments(x1, y1, x2, y2, ...)\n    }\n    points2 <- function(x, y, ...) {\n\tif(xlog) x <- 10^x\n\tif(ylog) y <- 10^y\n\tpoints(x, y, ...)\n    }\n    text2 <- function(x, y, ...) {\n\t##--- need to adjust  adj == c(xadj, yadj) ?? --\n\tif(xlog) x <- 10^x\n\tif(ylog) y <- 10^y\n\ttext(x, y, ...)\n    }\n    if(trace)\n\tcatn <- function(...)\n\t    do.call(\"cat\", c(lapply(list(...),formatC), list(\"\\n\")))\n\n    cin <- par(\"cin\")\n    Cex <- cex * par(\"cex\")\t\t# = the `effective' cex for text\n\n    ## at this point we want positive width even for reversed x axis.\n    if(is.null(text.width))\n\ttext.width <- max(abs(strwidth(legend, units=\"user\",\n\t\t\t\t       cex=cex, font = text.font)))\n    else if(!is.numeric(text.width) || text.width < 0)\n\tstop(\"'text.width' must be numeric, >= 0\")\n\n    xc <- Cex * xinch(cin[1L], warn.log=FALSE) # [uses par(\"usr\") and \"pin\"]\n    yc <- Cex * yinch(cin[2L], warn.log=FALSE)\n    if(xc < 0) text.width <- -text.width\n\n    xchar  <- xc\n    xextra <- 0\n    yextra <- yc * (y.intersp - 1)\n    ## watch out for reversed axis here: heights can be negative\n    ymax   <- yc * max(1, strheight(legend, units=\"user\", cex=cex)/yc)\n    ychar <- yextra + ymax\n    if(trace) catn(\"  xchar=\", xchar, \"; (yextra,ychar)=\", c(yextra,ychar))\n\n    if(mfill) {\n\t##= sizes of filled boxes.\n\txbox <- xc * 0.8\n\tybox <- yc * 0.5\n\tdx.fill <- xbox ## + x.intersp*xchar\n    }\n    do.lines <- (!missing(lty) && (is.character(lty) || any(lty > 0))\n\t\t ) || !missing(lwd)\n\n    ## legends per column:\n    n.legpercol <-\n\tif(horiz) {\n\t    if(ncol != 1)\n                warning(gettextf(\"horizontal specification overrides: Number of columns := %d\",\n                                 n.leg), domain = NA)\n\t    ncol <- n.leg\n\t    1\n\t} else ceiling(n.leg / ncol)\n\n    has.pch <- !missing(pch) && length(pch) > 0 # -> default 'merge' is available\n    if(do.lines) {\n\tx.off <- if(merge) -0.7 else 0\n    } else if(merge)\n\twarning(\"'merge = TRUE' has no effect when no line segments are drawn\")\n\n    if(has.pch) {\n\tif(is.character(pch) && !is.na(pch[1L]) &&\n           nchar(pch[1L], type = \"c\") > 1) {\n\t    if(length(pch) > 1)\n\t\twarning(\"not using pch[2..] since pch[1L] has multiple chars\")\n\t    np <- nchar(pch[1L], type = \"c\")\n\t    pch <- substr(rep.int(pch[1L], np), 1L:np, 1L:np)\n\t}\n        ## this coercion was documented but not done in R < 3.0.0\n        if(!is.character(pch)) pch <- as.integer(pch)\n    }\n\n    if (is.na(auto)) {\n\t##- Adjust (x,y) :\n\tif (xlog) x <- log10(x)\n\tif (ylog) y <- log10(y)\n    }\n    if(nx == 2) {\n\t## (x,y) are specifiying OPPOSITE corners of the box\n\tx <- sort(x)\n\ty <- sort(y)\n\tleft <- x[1L]\n\ttop  <- y[2L]\n\tw <- diff(x)# width\n\th <- diff(y)# height\n\tw0 <- w/ncol # column width\n\n\tx <- mean(x)\n\ty <- mean(y)\n\tif(missing(xjust)) xjust <- 0.5\n\tif(missing(yjust)) yjust <- 0.5\n\n    }\n    else {## nx == 1  or  auto\n\t## -- (w,h) := (width,height) of the box to draw -- computed in steps\n\th <- (n.legpercol + !is.null(title)) * ychar + yc\n\tw0 <- text.width + (x.intersp + 1) * xchar\n\tif(mfill)\tw0 <- w0 + dx.fill\n\tif(do.lines)\tw0 <- w0 + (seg.len + x.off)*xchar\n\tw <- ncol*w0 + .5* xchar\n\tif (!is.null(title)\n\t    && (abs(tw <- strwidth(title, units=\"user\", cex=cex) + 0.5*xchar)) > abs(w)) {\n\t    xextra <- (tw - w)/2\n\t    w <- tw\n\t}\n\n\t##-- (w,h) are now the final box width/height.\n\n\tif (is.na(auto)) {\n\t    left <- x - xjust * w\n\t    top\t <- y + (1 - yjust) * h\n\t} else {\n\t    usr <- par(\"usr\")\n\t    inset <- rep_len(inset, 2)\n\t    insetx <- inset[1L]*(usr[2L] - usr[1L])\n\t    left <- switch(auto, \"bottomright\" =,\n\t\t\t   \"topright\" =, \"right\" = usr[2L] - w - insetx,\n\t\t\t   \"bottomleft\" =, \"left\" =, \"topleft\" = usr[1L] + insetx,\n\t\t\t   \"bottom\" =, \"top\" =, \"center\" = (usr[1L] + usr[2L] - w)/2)\n\t    insety <- inset[2L]*(usr[4L] - usr[3L])\n\t    top <- switch(auto, \"bottomright\" =,\n\t\t\t  \"bottom\" =, \"bottomleft\" = usr[3L] + h + insety,\n\t\t\t  \"topleft\" =, \"top\" =, \"topright\" = usr[4L] - insety,\n\t\t\t  \"left\" =, \"right\" =, \"center\" = (usr[3L] + usr[4L] + h)/2)\n\t}\n    }\n\n    if (plot && bty != \"n\") { ## The legend box :\n\tif(trace)\n\t    catn(\"  rect2(\", left, \",\", top,\", w=\", w, \", h=\", h, \", ...)\",\n                 sep = \"\")\n\trect2(left, top, dx = w, dy = h, col = bg, density = NULL,\n              lwd = box.lwd, lty = box.lty, border = box.col)\n    }\n\n    ## (xt[],yt[]) := `current' vectors of (x/y) legend text\n    xt <- left + xchar + xextra +\n\t(w0 * rep.int(0:(ncol-1), rep.int(n.legpercol,ncol)))[1L:n.leg]\n    yt <- top -\t0.5 * yextra - ymax -\n\t(rep.int(1L:n.legpercol,ncol)[1L:n.leg] - 1 + !is.null(title)) * ychar\n\n    if (mfill) {\t\t#- draw filled boxes -------------\n\tif(plot) {\n\t    if(!is.null(fill)) fill <- rep_len(fill, n.leg)\n\t    rect2(left = xt, top=yt+ybox/2, dx = xbox, dy = ybox,\n\t\t  col = fill,\n\t\t  density = density, angle = angle, border = border)\n\t}\n\txt <- xt + dx.fill\n    }\n    if(plot && (has.pch || do.lines))\n\tcol <- rep_len(col, n.leg)\n\n    ## NULL is not documented but people use it.\n    if(missing(lwd) || is.null(lwd))\n\tlwd <- par(\"lwd\") # = default for pt.lwd\n    if (do.lines) {\t\t\t#- draw lines ---------------------\n        ## NULL is not documented\n\tif(missing(lty) || is.null(lty)) lty <- 1\n\tlty <- rep_len(lty, n.leg)\n\tlwd <- rep_len(lwd, n.leg)\n\tok.l <- !is.na(lty) & (is.character(lty) | lty > 0) & !is.na(lwd)\n\tif(trace)\n\t    catn(\"  segments2(\",xt[ok.l] + x.off*xchar, \",\", yt[ok.l],\n\t\t \", dx=\", seg.len*xchar, \", dy=0, ...)\")\n\tif(plot)\n\t    segments2(xt[ok.l] + x.off*xchar, yt[ok.l],\n                      dx = seg.len*xchar, dy = 0,\n\t\t      lty = lty[ok.l], lwd = lwd[ok.l], col = col[ok.l])\n\t# if (!merge)\n\txt <- xt + (seg.len+x.off) * xchar\n    }\n    if (has.pch) {\t\t\t#- draw points -------------------\n\tpch <- rep_len(pch, n.leg)\n\tpt.bg <- rep_len(pt.bg, n.leg)\n\tpt.cex <- rep_len(pt.cex, n.leg)\n\tpt.lwd <- rep_len(pt.lwd, n.leg)\n        ok <- !is.na(pch)\n        if (!is.character(pch)) {\n            ## R 2.x.y omitted pch < 0\n            ok <- ok & (pch >= 0 | pch <= -32)\n        } else {\n            ## like points\n            ok <- ok & nzchar(pch)\n        }\n\tx1 <- (if(merge && do.lines) xt-(seg.len/2)*xchar else xt)[ok]\n\ty1 <- yt[ok]\n\tif(trace)\n\t    catn(\"  points2(\", x1,\",\", y1,\", pch=\", pch[ok],\", ...)\")\n\tif(plot)\n\t    points2(x1, y1, pch = pch[ok], col = col[ok],\n\t\t    cex = pt.cex[ok], bg = pt.bg[ok], lwd = pt.lwd[ok])\n##D\tif (!merge) xt <- xt + dx.pch\n    }\n\n    xt <- xt + x.intersp * xchar\n    if(plot) {\n\tif (!is.null(title))\n            text2(left + w*title.adj, top - ymax, labels = title,\n                  adj = c(title.adj, 0), cex = cex, col = title.col)\n\n\ttext2(xt, yt, labels = legend, adj = adj, cex = cex,\n\t      col = text.col, font = text.font)\n    }\n    invisible(list(rect = list(w = w, h = h, left = left, top = top),\n\t\t   text = list(x = xt, y = yt)))\n}\n" }
{ "repo_name": "h2oai/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_munging/exec/runit_pub-657.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../../scripts/h2o-r-test-setup.R\")\n# library(h2o)\n# library(testthat)\n# conn = h2o.init()\n\n\n\n\ntest.pub.657 <- function() {\n\n    a_initial <- data.frame(v1=c(0,0,0,0), v2=c(1,1,1,1))\n    a <- a_initial\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 70\n    a[,1] <- 70\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 1\n    a[,1] <- 1\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 0\n    a[,1] <- 0\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 1\n    a[,1] <- 1\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    \n}\n\ndoTest(\"Test for pub-657.\", test.pub.657)\n\n" }
{ "repo_name": "michalkurka/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_munging/exec/runit_pub-657.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../../scripts/h2o-r-test-setup.R\")\n# library(h2o)\n# library(testthat)\n# conn = h2o.init()\n\n\n\n\ntest.pub.657 <- function() {\n\n    a_initial <- data.frame(v1=c(0,0,0,0), v2=c(1,1,1,1))\n    a <- a_initial\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 70\n    a[,1] <- 70\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 1\n    a[,1] <- 1\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 0\n    a[,1] <- 0\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 1\n    a[,1] <- 1\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    \n}\n\ndoTest(\"Test for pub-657.\", test.pub.657)\n\n" }
{ "repo_name": "mathemage/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_munging/exec/runit_pub-657.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../../scripts/h2o-r-test-setup.R\")\n# library(h2o)\n# library(testthat)\n# conn = h2o.init()\n\n\n\n\ntest.pub.657 <- function() {\n\n    a_initial <- data.frame(v1=c(0,0,0,0), v2=c(1,1,1,1))\n    a <- a_initial\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 70\n    a[,1] <- 70\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 1\n    a[,1] <- 1\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 0\n    a[,1] <- 0\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 1\n    a[,1] <- 1\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    \n}\n\ndoTest(\"Test for pub-657.\", test.pub.657)\n\n" }
{ "repo_name": "spennihana/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_munging/exec/runit_pub-657.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../../scripts/h2o-r-test-setup.R\")\n# library(h2o)\n# library(testthat)\n# conn = h2o.init()\n\n\n\n\ntest.pub.657 <- function() {\n\n    a_initial <- data.frame(v1=c(0,0,0,0), v2=c(1,1,1,1))\n    a <- a_initial\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 70\n    a[,1] <- 70\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 1\n    a[,1] <- 1\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 0\n    a[,1] <- 0\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 1\n    a[,1] <- 1\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    \n}\n\ndoTest(\"Test for pub-657.\", test.pub.657)\n\n" }
{ "repo_name": "jangorecki/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_munging/exec/runit_pub-657.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../../scripts/h2o-r-test-setup.R\")\n# library(h2o)\n# library(testthat)\n# conn = h2o.init()\n\n\n\n\ntest.pub.657 <- function() {\n\n    a_initial <- data.frame(v1=c(0,0,0,0), v2=c(1,1,1,1))\n    a <- a_initial\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 70\n    a[,1] <- 70\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 1\n    a[,1] <- 1\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 0\n    a[,1] <- 0\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 1\n    a[,1] <- 1\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    \n}\n\ndoTest(\"Test for pub-657.\", test.pub.657)\n\n" }
{ "repo_name": "h2oai/h2o-dev", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_munging/exec/runit_pub-657.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../../scripts/h2o-r-test-setup.R\")\n# library(h2o)\n# library(testthat)\n# conn = h2o.init()\n\n\n\n\ntest.pub.657 <- function() {\n\n    a_initial <- data.frame(v1=c(0,0,0,0), v2=c(1,1,1,1))\n    a <- a_initial\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 70\n    a[,1] <- 70\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 1\n    a[,1] <- 1\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 0\n    a[,1] <- 0\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 1\n    a[,1] <- 1\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    \n}\n\ndoTest(\"Test for pub-657.\", test.pub.657)\n\n" }
{ "repo_name": "nilbody/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_munging/exec/runit_pub-657.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../../scripts/h2o-r-test-setup.R\")\n# library(h2o)\n# library(testthat)\n# conn = h2o.init()\n\n\n\n\ntest.pub.657 <- function() {\n\n    a_initial <- data.frame(v1=c(0,0,0,0), v2=c(1,1,1,1))\n    a <- a_initial\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 70\n    a[,1] <- 70\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 1\n    a[,1] <- 1\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 0\n    a[,1] <- 0\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 1\n    a[,1] <- 1\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    \n}\n\ndoTest(\"Test for pub-657.\", test.pub.657)\n\n" }
{ "repo_name": "YzPaul3/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_munging/exec/runit_pub-657.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../../scripts/h2o-r-test-setup.R\")\n# library(h2o)\n# library(testthat)\n# conn = h2o.init()\n\n\n\n\ntest.pub.657 <- function() {\n\n    a_initial <- data.frame(v1=c(0,0,0,0), v2=c(1,1,1,1))\n    a <- a_initial\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 70\n    a[,1] <- 70\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 1\n    a[,1] <- 1\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 0\n    a[,1] <- 0\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    a.h2o <- as.h2o(a_initial, destination_frame=\"A.hex\")\n    a.h2o[,1] <- 1\n    a[,1] <- 1\n    a.h2o.R <- as.data.frame(a.h2o)\n    expect_that(all(a == a.h2o.R), equals(T))\n\n    \n}\n\ndoTest(\"Test for pub-657.\", test.pub.657)\n\n" }
{ "repo_name": "ChiWang/r-source", "ref": "refs/heads/master", "path": "src/library/utils/R/head.R", "content": "#  File src/library/utils/R/head.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2012 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n### placed in the public domain 2002\n### Patrick Burns patrick@burns-stat.com\n###\n### Adapted for negative arguments by Vincent Goulet\n### <vincent.goulet@act.ulaval.ca>, 2006\n\nhead <- function(x, ...) UseMethod(\"head\")\n\nhead.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(length(x) + n, 0L) else min(n, length(x))\n    x[seq_len(n)]\n}\n\n## head.matrix and tail.matrix are now exported (to be used for other classes)\nhead.data.frame <- head.matrix <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(nrow(x) + n, 0L) else min(n, nrow(x))\n    x[seq_len(n), , drop=FALSE]\n}\nhead.table  <- function(x, n = 6L, ...) {\n    (if(length(dim(x)) == 2L) head.matrix else head.default)(x, n=n)\n}\n\nhead.ftable <- function(x, n = 6L, ...) {\n    r <- format(x)\n    dimnames(r) <- list(rep.int(\"\", nrow(r)), rep.int(\"\", ncol(r)))\n    noquote(head.matrix(r, n = n + nrow(r) - nrow(x), ...))\n}\n\nhead.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(head(lines, n=n))\n}\n\ntail <- function(x, ...) UseMethod(\"tail\")\n\ntail.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    xlen <- length(x)\n    n <- if (n < 0L) max(xlen + n, 0L) else min(n, xlen)\n    x[seq.int(to = xlen, length.out = n)]\n}\n\ntail.data.frame <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    x[seq.int(to = nrx, length.out = n), , drop = FALSE]\n}\n\ntail.matrix <- function(x, n = 6L, addrownums = TRUE, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    sel <- seq.int(to = nrx, length.out = n)\n    ans <- x[sel, , drop = FALSE]\n    if (addrownums && is.null(rownames(x)))\n    \trownames(ans) <- paste0(\"[\", sel, \",]\")\n    ans\n}\ntail.table  <- function(x, n = 6L, addrownums = TRUE, ...) {\n    (if(length(dim(x)) == 2L) tail.matrix else tail.default)(x, n=n,\n\t      addrownums = addrownums, ...)\n}\n\ntail.ftable <- function(x, n = 6L, addrownums = FALSE, ...) {\n    r <- format(x)\n    dimnames(r) <- list(if(!addrownums) rep.int(\"\", nrow(r)),\n\t\t\trep.int(\"\", ncol(r)))\n    noquote(tail.matrix(r, n = n, addrownums = addrownums, ...))\n}\n\ntail.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(tail(lines, n=n))\n}\n" }
{ "repo_name": "o-/Rexperiments", "ref": "refs/heads/master", "path": "src/library/utils/R/head.R", "content": "#  File src/library/utils/R/head.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2012 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n### placed in the public domain 2002\n### Patrick Burns patrick@burns-stat.com\n###\n### Adapted for negative arguments by Vincent Goulet\n### <vincent.goulet@act.ulaval.ca>, 2006\n\nhead <- function(x, ...) UseMethod(\"head\")\n\nhead.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(length(x) + n, 0L) else min(n, length(x))\n    x[seq_len(n)]\n}\n\n## head.matrix and tail.matrix are now exported (to be used for other classes)\nhead.data.frame <- head.matrix <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(nrow(x) + n, 0L) else min(n, nrow(x))\n    x[seq_len(n), , drop=FALSE]\n}\nhead.table  <- function(x, n = 6L, ...) {\n    (if(length(dim(x)) == 2L) head.matrix else head.default)(x, n=n)\n}\n\nhead.ftable <- function(x, n = 6L, ...) {\n    r <- format(x)\n    dimnames(r) <- list(rep.int(\"\", nrow(r)), rep.int(\"\", ncol(r)))\n    noquote(head.matrix(r, n = n + nrow(r) - nrow(x), ...))\n}\n\nhead.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(head(lines, n=n))\n}\n\ntail <- function(x, ...) UseMethod(\"tail\")\n\ntail.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    xlen <- length(x)\n    n <- if (n < 0L) max(xlen + n, 0L) else min(n, xlen)\n    x[seq.int(to = xlen, length.out = n)]\n}\n\ntail.data.frame <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    x[seq.int(to = nrx, length.out = n), , drop = FALSE]\n}\n\ntail.matrix <- function(x, n = 6L, addrownums = TRUE, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    sel <- seq.int(to = nrx, length.out = n)\n    ans <- x[sel, , drop = FALSE]\n    if (addrownums && is.null(rownames(x)))\n    \trownames(ans) <- paste0(\"[\", sel, \",]\")\n    ans\n}\ntail.table  <- function(x, n = 6L, addrownums = TRUE, ...) {\n    (if(length(dim(x)) == 2L) tail.matrix else tail.default)(x, n=n,\n\t      addrownums = addrownums, ...)\n}\n\ntail.ftable <- function(x, n = 6L, addrownums = FALSE, ...) {\n    r <- format(x)\n    dimnames(r) <- list(if(!addrownums) rep.int(\"\", nrow(r)),\n\t\t\trep.int(\"\", ncol(r)))\n    noquote(tail.matrix(r, n = n, addrownums = addrownums, ...))\n}\n\ntail.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(tail(lines, n=n))\n}\n" }
{ "repo_name": "jukiewiczm/renjin", "ref": "refs/heads/master", "path": "packages/utils/src/main/R/head.R", "content": "#  File src/library/utils/R/head.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2012 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n### placed in the public domain 2002\n### Patrick Burns patrick@burns-stat.com\n###\n### Adapted for negative arguments by Vincent Goulet\n### <vincent.goulet@act.ulaval.ca>, 2006\n\nhead <- function(x, ...) UseMethod(\"head\")\n\nhead.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(length(x) + n, 0L) else min(n, length(x))\n    x[seq_len(n)]\n}\n\n## head.matrix and tail.matrix are now exported (to be used for other classes)\nhead.data.frame <- head.matrix <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(nrow(x) + n, 0L) else min(n, nrow(x))\n    x[seq_len(n), , drop=FALSE]\n}\nhead.table  <- function(x, n = 6L, ...) {\n    (if(length(dim(x)) == 2L) head.matrix else head.default)(x, n=n)\n}\n\nhead.ftable <- function(x, n = 6L, ...) {\n    r <- format(x)\n    dimnames(r) <- list(rep.int(\"\", nrow(r)), rep.int(\"\", ncol(r)))\n    noquote(head.matrix(r, n = n + nrow(r) - nrow(x), ...))\n}\n\nhead.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(head(lines, n=n))\n}\n\ntail <- function(x, ...) UseMethod(\"tail\")\n\ntail.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    xlen <- length(x)\n    n <- if (n < 0L) max(xlen + n, 0L) else min(n, xlen)\n    x[seq.int(to = xlen, length.out = n)]\n}\n\ntail.data.frame <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    x[seq.int(to = nrx, length.out = n), , drop = FALSE]\n}\n\ntail.matrix <- function(x, n = 6L, addrownums = TRUE, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    sel <- seq.int(to = nrx, length.out = n)\n    ans <- x[sel, , drop = FALSE]\n    if (addrownums && is.null(rownames(x)))\n    \trownames(ans) <- paste0(\"[\", sel, \",]\")\n    ans\n}\ntail.table  <- function(x, n = 6L, addrownums = TRUE, ...) {\n    (if(length(dim(x)) == 2L) tail.matrix else tail.default)(x, n=n,\n\t      addrownums = addrownums, ...)\n}\n\ntail.ftable <- function(x, n = 6L, addrownums = FALSE, ...) {\n    r <- format(x)\n    dimnames(r) <- list(if(!addrownums) rep.int(\"\", nrow(r)),\n\t\t\trep.int(\"\", ncol(r)))\n    noquote(tail.matrix(r, n = n, addrownums = addrownums, ...))\n}\n\ntail.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(tail(lines, n=n))\n}\n" }
{ "repo_name": "glycerine/bigbird", "ref": "refs/heads/master", "path": "r-3.0.2/src/library/utils/R/head.R", "content": "#  File src/library/utils/R/head.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2012 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n### placed in the public domain 2002\n### Patrick Burns patrick@burns-stat.com\n###\n### Adapted for negative arguments by Vincent Goulet\n### <vincent.goulet@act.ulaval.ca>, 2006\n\nhead <- function(x, ...) UseMethod(\"head\")\n\nhead.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(length(x) + n, 0L) else min(n, length(x))\n    x[seq_len(n)]\n}\n\n## head.matrix and tail.matrix are now exported (to be used for other classes)\nhead.data.frame <- head.matrix <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(nrow(x) + n, 0L) else min(n, nrow(x))\n    x[seq_len(n), , drop=FALSE]\n}\nhead.table  <- function(x, n = 6L, ...) {\n    (if(length(dim(x)) == 2L) head.matrix else head.default)(x, n=n)\n}\n\nhead.ftable <- function(x, n = 6L, ...) {\n    r <- format(x)\n    dimnames(r) <- list(rep.int(\"\", nrow(r)), rep.int(\"\", ncol(r)))\n    noquote(head.matrix(r, n = n + nrow(r) - nrow(x), ...))\n}\n\nhead.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(head(lines, n=n))\n}\n\ntail <- function(x, ...) UseMethod(\"tail\")\n\ntail.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    xlen <- length(x)\n    n <- if (n < 0L) max(xlen + n, 0L) else min(n, xlen)\n    x[seq.int(to = xlen, length.out = n)]\n}\n\ntail.data.frame <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    x[seq.int(to = nrx, length.out = n), , drop = FALSE]\n}\n\ntail.matrix <- function(x, n = 6L, addrownums = TRUE, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    sel <- seq.int(to = nrx, length.out = n)\n    ans <- x[sel, , drop = FALSE]\n    if (addrownums && is.null(rownames(x)))\n    \trownames(ans) <- paste0(\"[\", sel, \",]\")\n    ans\n}\ntail.table  <- function(x, n = 6L, addrownums = TRUE, ...) {\n    (if(length(dim(x)) == 2L) tail.matrix else tail.default)(x, n=n,\n\t      addrownums = addrownums, ...)\n}\n\ntail.ftable <- function(x, n = 6L, addrownums = FALSE, ...) {\n    r <- format(x)\n    dimnames(r) <- list(if(!addrownums) rep.int(\"\", nrow(r)),\n\t\t\trep.int(\"\", ncol(r)))\n    noquote(tail.matrix(r, n = n, addrownums = addrownums, ...))\n}\n\ntail.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(tail(lines, n=n))\n}\n" }
{ "repo_name": "jeffreyhorner/R-Array-Hash", "ref": "refs/heads/master", "path": "src/library/utils/R/head.R", "content": "#  File src/library/utils/R/head.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2012 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n### placed in the public domain 2002\n### Patrick Burns patrick@burns-stat.com\n###\n### Adapted for negative arguments by Vincent Goulet\n### <vincent.goulet@act.ulaval.ca>, 2006\n\nhead <- function(x, ...) UseMethod(\"head\")\n\nhead.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(length(x) + n, 0L) else min(n, length(x))\n    x[seq_len(n)]\n}\n\n## head.matrix and tail.matrix are now exported (to be used for other classes)\nhead.data.frame <- head.matrix <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(nrow(x) + n, 0L) else min(n, nrow(x))\n    x[seq_len(n), , drop=FALSE]\n}\nhead.table  <- function(x, n = 6L, ...) {\n    (if(length(dim(x)) == 2L) head.matrix else head.default)(x, n=n)\n}\n\nhead.ftable <- function(x, n = 6L, ...) {\n    r <- format(x)\n    dimnames(r) <- list(rep.int(\"\", nrow(r)), rep.int(\"\", ncol(r)))\n    noquote(head.matrix(r, n = n + nrow(r) - nrow(x), ...))\n}\n\nhead.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(head(lines, n=n))\n}\n\ntail <- function(x, ...) UseMethod(\"tail\")\n\ntail.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    xlen <- length(x)\n    n <- if (n < 0L) max(xlen + n, 0L) else min(n, xlen)\n    x[seq.int(to = xlen, length.out = n)]\n}\n\ntail.data.frame <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    x[seq.int(to = nrx, length.out = n), , drop = FALSE]\n}\n\ntail.matrix <- function(x, n = 6L, addrownums = TRUE, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    sel <- seq.int(to = nrx, length.out = n)\n    ans <- x[sel, , drop = FALSE]\n    if (addrownums && is.null(rownames(x)))\n    \trownames(ans) <- paste0(\"[\", sel, \",]\")\n    ans\n}\ntail.table  <- function(x, n = 6L, addrownums = TRUE, ...) {\n    (if(length(dim(x)) == 2L) tail.matrix else tail.default)(x, n=n,\n\t      addrownums = addrownums, ...)\n}\n\ntail.ftable <- function(x, n = 6L, addrownums = FALSE, ...) {\n    r <- format(x)\n    dimnames(r) <- list(if(!addrownums) rep.int(\"\", nrow(r)),\n\t\t\trep.int(\"\", ncol(r)))\n    noquote(tail.matrix(r, n = n, addrownums = addrownums, ...))\n}\n\ntail.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(tail(lines, n=n))\n}\n" }
{ "repo_name": "limeng12/r-source", "ref": "refs/heads/trunk", "path": "src/library/utils/R/head.R", "content": "#  File src/library/utils/R/head.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2012 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n### placed in the public domain 2002\n### Patrick Burns patrick@burns-stat.com\n###\n### Adapted for negative arguments by Vincent Goulet\n### <vincent.goulet@act.ulaval.ca>, 2006\n\nhead <- function(x, ...) UseMethod(\"head\")\n\nhead.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(length(x) + n, 0L) else min(n, length(x))\n    x[seq_len(n)]\n}\n\n## head.matrix and tail.matrix are now exported (to be used for other classes)\nhead.data.frame <- head.matrix <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(nrow(x) + n, 0L) else min(n, nrow(x))\n    x[seq_len(n), , drop=FALSE]\n}\nhead.table  <- function(x, n = 6L, ...) {\n    (if(length(dim(x)) == 2L) head.matrix else head.default)(x, n=n)\n}\n\nhead.ftable <- function(x, n = 6L, ...) {\n    r <- format(x)\n    dimnames(r) <- list(rep.int(\"\", nrow(r)), rep.int(\"\", ncol(r)))\n    noquote(head.matrix(r, n = n + nrow(r) - nrow(x), ...))\n}\n\nhead.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(head(lines, n=n))\n}\n\ntail <- function(x, ...) UseMethod(\"tail\")\n\ntail.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    xlen <- length(x)\n    n <- if (n < 0L) max(xlen + n, 0L) else min(n, xlen)\n    x[seq.int(to = xlen, length.out = n)]\n}\n\ntail.data.frame <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    x[seq.int(to = nrx, length.out = n), , drop = FALSE]\n}\n\ntail.matrix <- function(x, n = 6L, addrownums = TRUE, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    sel <- seq.int(to = nrx, length.out = n)\n    ans <- x[sel, , drop = FALSE]\n    if (addrownums && is.null(rownames(x)))\n    \trownames(ans) <- paste0(\"[\", sel, \",]\")\n    ans\n}\ntail.table  <- function(x, n = 6L, addrownums = TRUE, ...) {\n    (if(length(dim(x)) == 2L) tail.matrix else tail.default)(x, n=n,\n\t      addrownums = addrownums, ...)\n}\n\ntail.ftable <- function(x, n = 6L, addrownums = FALSE, ...) {\n    r <- format(x)\n    dimnames(r) <- list(if(!addrownums) rep.int(\"\", nrow(r)),\n\t\t\trep.int(\"\", ncol(r)))\n    noquote(tail.matrix(r, n = n, addrownums = addrownums, ...))\n}\n\ntail.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(tail(lines, n=n))\n}\n" }
{ "repo_name": "lajus/customr", "ref": "refs/heads/master", "path": "src/library/utils/R/head.R", "content": "#  File src/library/utils/R/head.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2012 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n### placed in the public domain 2002\n### Patrick Burns patrick@burns-stat.com\n###\n### Adapted for negative arguments by Vincent Goulet\n### <vincent.goulet@act.ulaval.ca>, 2006\n\nhead <- function(x, ...) UseMethod(\"head\")\n\nhead.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(length(x) + n, 0L) else min(n, length(x))\n    x[seq_len(n)]\n}\n\n## head.matrix and tail.matrix are now exported (to be used for other classes)\nhead.data.frame <- head.matrix <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(nrow(x) + n, 0L) else min(n, nrow(x))\n    x[seq_len(n), , drop=FALSE]\n}\nhead.table  <- function(x, n = 6L, ...) {\n    (if(length(dim(x)) == 2L) head.matrix else head.default)(x, n=n)\n}\n\nhead.ftable <- function(x, n = 6L, ...) {\n    r <- format(x)\n    dimnames(r) <- list(rep.int(\"\", nrow(r)), rep.int(\"\", ncol(r)))\n    noquote(head.matrix(r, n = n + nrow(r) - nrow(x), ...))\n}\n\nhead.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(head(lines, n=n))\n}\n\ntail <- function(x, ...) UseMethod(\"tail\")\n\ntail.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    xlen <- length(x)\n    n <- if (n < 0L) max(xlen + n, 0L) else min(n, xlen)\n    x[seq.int(to = xlen, length.out = n)]\n}\n\ntail.data.frame <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    x[seq.int(to = nrx, length.out = n), , drop = FALSE]\n}\n\ntail.matrix <- function(x, n = 6L, addrownums = TRUE, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    sel <- seq.int(to = nrx, length.out = n)\n    ans <- x[sel, , drop = FALSE]\n    if (addrownums && is.null(rownames(x)))\n    \trownames(ans) <- paste0(\"[\", sel, \",]\")\n    ans\n}\ntail.table  <- function(x, n = 6L, addrownums = TRUE, ...) {\n    (if(length(dim(x)) == 2L) tail.matrix else tail.default)(x, n=n,\n\t      addrownums = addrownums, ...)\n}\n\ntail.ftable <- function(x, n = 6L, addrownums = FALSE, ...) {\n    r <- format(x)\n    dimnames(r) <- list(if(!addrownums) rep.int(\"\", nrow(r)),\n\t\t\trep.int(\"\", ncol(r)))\n    noquote(tail.matrix(r, n = n, addrownums = addrownums, ...))\n}\n\ntail.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(tail(lines, n=n))\n}\n" }
{ "repo_name": "jeffreyhorner/R-Judy-Arrays", "ref": "refs/heads/master", "path": "src/library/utils/R/head.R", "content": "#  File src/library/utils/R/head.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2012 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n### placed in the public domain 2002\n### Patrick Burns patrick@burns-stat.com\n###\n### Adapted for negative arguments by Vincent Goulet\n### <vincent.goulet@act.ulaval.ca>, 2006\n\nhead <- function(x, ...) UseMethod(\"head\")\n\nhead.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(length(x) + n, 0L) else min(n, length(x))\n    x[seq_len(n)]\n}\n\n## head.matrix and tail.matrix are now exported (to be used for other classes)\nhead.data.frame <- head.matrix <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(nrow(x) + n, 0L) else min(n, nrow(x))\n    x[seq_len(n), , drop=FALSE]\n}\nhead.table  <- function(x, n = 6L, ...) {\n    (if(length(dim(x)) == 2L) head.matrix else head.default)(x, n=n)\n}\n\nhead.ftable <- function(x, n = 6L, ...) {\n    r <- format(x)\n    dimnames(r) <- list(rep.int(\"\", nrow(r)), rep.int(\"\", ncol(r)))\n    noquote(head.matrix(r, n = n + nrow(r) - nrow(x), ...))\n}\n\nhead.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(head(lines, n=n))\n}\n\ntail <- function(x, ...) UseMethod(\"tail\")\n\ntail.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    xlen <- length(x)\n    n <- if (n < 0L) max(xlen + n, 0L) else min(n, xlen)\n    x[seq.int(to = xlen, length.out = n)]\n}\n\ntail.data.frame <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    x[seq.int(to = nrx, length.out = n), , drop = FALSE]\n}\n\ntail.matrix <- function(x, n = 6L, addrownums = TRUE, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    sel <- seq.int(to = nrx, length.out = n)\n    ans <- x[sel, , drop = FALSE]\n    if (addrownums && is.null(rownames(x)))\n    \trownames(ans) <- paste0(\"[\", sel, \",]\")\n    ans\n}\ntail.table  <- function(x, n = 6L, addrownums = TRUE, ...) {\n    (if(length(dim(x)) == 2L) tail.matrix else tail.default)(x, n=n,\n\t      addrownums = addrownums, ...)\n}\n\ntail.ftable <- function(x, n = 6L, addrownums = FALSE, ...) {\n    r <- format(x)\n    dimnames(r) <- list(if(!addrownums) rep.int(\"\", nrow(r)),\n\t\t\trep.int(\"\", ncol(r)))\n    noquote(tail.matrix(r, n = n, addrownums = addrownums, ...))\n}\n\ntail.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(tail(lines, n=n))\n}\n" }
{ "repo_name": "patperry/r-source", "ref": "refs/heads/trunk", "path": "src/library/utils/R/head.R", "content": "#  File src/library/utils/R/head.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2012 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n### placed in the public domain 2002\n### Patrick Burns patrick@burns-stat.com\n###\n### Adapted for negative arguments by Vincent Goulet\n### <vincent.goulet@act.ulaval.ca>, 2006\n\nhead <- function(x, ...) UseMethod(\"head\")\n\nhead.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(length(x) + n, 0L) else min(n, length(x))\n    x[seq_len(n)]\n}\n\n## head.matrix and tail.matrix are now exported (to be used for other classes)\nhead.data.frame <- head.matrix <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(nrow(x) + n, 0L) else min(n, nrow(x))\n    x[seq_len(n), , drop=FALSE]\n}\nhead.table  <- function(x, n = 6L, ...) {\n    (if(length(dim(x)) == 2L) head.matrix else head.default)(x, n=n)\n}\n\nhead.ftable <- function(x, n = 6L, ...) {\n    r <- format(x)\n    dimnames(r) <- list(rep.int(\"\", nrow(r)), rep.int(\"\", ncol(r)))\n    noquote(head.matrix(r, n = n + nrow(r) - nrow(x), ...))\n}\n\nhead.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(head(lines, n=n))\n}\n\ntail <- function(x, ...) UseMethod(\"tail\")\n\ntail.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    xlen <- length(x)\n    n <- if (n < 0L) max(xlen + n, 0L) else min(n, xlen)\n    x[seq.int(to = xlen, length.out = n)]\n}\n\ntail.data.frame <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    x[seq.int(to = nrx, length.out = n), , drop = FALSE]\n}\n\ntail.matrix <- function(x, n = 6L, addrownums = TRUE, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    sel <- seq.int(to = nrx, length.out = n)\n    ans <- x[sel, , drop = FALSE]\n    if (addrownums && is.null(rownames(x)))\n    \trownames(ans) <- paste0(\"[\", sel, \",]\")\n    ans\n}\ntail.table  <- function(x, n = 6L, addrownums = TRUE, ...) {\n    (if(length(dim(x)) == 2L) tail.matrix else tail.default)(x, n=n,\n\t      addrownums = addrownums, ...)\n}\n\ntail.ftable <- function(x, n = 6L, addrownums = FALSE, ...) {\n    r <- format(x)\n    dimnames(r) <- list(if(!addrownums) rep.int(\"\", nrow(r)),\n\t\t\trep.int(\"\", ncol(r)))\n    noquote(tail.matrix(r, n = n, addrownums = addrownums, ...))\n}\n\ntail.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(tail(lines, n=n))\n}\n" }
{ "repo_name": "hadley/r-source", "ref": "refs/heads/trunk", "path": "src/library/utils/R/head.R", "content": "#  File src/library/utils/R/head.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2012 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n### placed in the public domain 2002\n### Patrick Burns patrick@burns-stat.com\n###\n### Adapted for negative arguments by Vincent Goulet\n### <vincent.goulet@act.ulaval.ca>, 2006\n\nhead <- function(x, ...) UseMethod(\"head\")\n\nhead.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(length(x) + n, 0L) else min(n, length(x))\n    x[seq_len(n)]\n}\n\n## head.matrix and tail.matrix are now exported (to be used for other classes)\nhead.data.frame <- head.matrix <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(nrow(x) + n, 0L) else min(n, nrow(x))\n    x[seq_len(n), , drop=FALSE]\n}\nhead.table  <- function(x, n = 6L, ...) {\n    (if(length(dim(x)) == 2L) head.matrix else head.default)(x, n=n)\n}\n\nhead.ftable <- function(x, n = 6L, ...) {\n    r <- format(x)\n    dimnames(r) <- list(rep.int(\"\", nrow(r)), rep.int(\"\", ncol(r)))\n    noquote(head.matrix(r, n = n + nrow(r) - nrow(x), ...))\n}\n\nhead.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(head(lines, n=n))\n}\n\ntail <- function(x, ...) UseMethod(\"tail\")\n\ntail.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    xlen <- length(x)\n    n <- if (n < 0L) max(xlen + n, 0L) else min(n, xlen)\n    x[seq.int(to = xlen, length.out = n)]\n}\n\ntail.data.frame <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    x[seq.int(to = nrx, length.out = n), , drop = FALSE]\n}\n\ntail.matrix <- function(x, n = 6L, addrownums = TRUE, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    sel <- seq.int(to = nrx, length.out = n)\n    ans <- x[sel, , drop = FALSE]\n    if (addrownums && is.null(rownames(x)))\n    \trownames(ans) <- paste0(\"[\", sel, \",]\")\n    ans\n}\ntail.table  <- function(x, n = 6L, addrownums = TRUE, ...) {\n    (if(length(dim(x)) == 2L) tail.matrix else tail.default)(x, n=n,\n\t      addrownums = addrownums, ...)\n}\n\ntail.ftable <- function(x, n = 6L, addrownums = FALSE, ...) {\n    r <- format(x)\n    dimnames(r) <- list(if(!addrownums) rep.int(\"\", nrow(r)),\n\t\t\trep.int(\"\", ncol(r)))\n    noquote(tail.matrix(r, n = n, addrownums = addrownums, ...))\n}\n\ntail.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(tail(lines, n=n))\n}\n" }
{ "repo_name": "mirror/r", "ref": "refs/heads/master", "path": "src/library/utils/R/head.R", "content": "#  File src/library/utils/R/head.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2012 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n### placed in the public domain 2002\n### Patrick Burns patrick@burns-stat.com\n###\n### Adapted for negative arguments by Vincent Goulet\n### <vincent.goulet@act.ulaval.ca>, 2006\n\nhead <- function(x, ...) UseMethod(\"head\")\n\nhead.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(length(x) + n, 0L) else min(n, length(x))\n    x[seq_len(n)]\n}\n\n## head.matrix and tail.matrix are now exported (to be used for other classes)\nhead.data.frame <- head.matrix <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(nrow(x) + n, 0L) else min(n, nrow(x))\n    x[seq_len(n), , drop=FALSE]\n}\nhead.table  <- function(x, n = 6L, ...) {\n    (if(length(dim(x)) == 2L) head.matrix else head.default)(x, n=n)\n}\n\nhead.ftable <- function(x, n = 6L, ...) {\n    r <- format(x)\n    dimnames(r) <- list(rep.int(\"\", nrow(r)), rep.int(\"\", ncol(r)))\n    noquote(head.matrix(r, n = n + nrow(r) - nrow(x), ...))\n}\n\nhead.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(head(lines, n=n))\n}\n\ntail <- function(x, ...) UseMethod(\"tail\")\n\ntail.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    xlen <- length(x)\n    n <- if (n < 0L) max(xlen + n, 0L) else min(n, xlen)\n    x[seq.int(to = xlen, length.out = n)]\n}\n\ntail.data.frame <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    x[seq.int(to = nrx, length.out = n), , drop = FALSE]\n}\n\ntail.matrix <- function(x, n = 6L, addrownums = TRUE, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    sel <- seq.int(to = nrx, length.out = n)\n    ans <- x[sel, , drop = FALSE]\n    if (addrownums && is.null(rownames(x)))\n    \trownames(ans) <- paste0(\"[\", sel, \",]\")\n    ans\n}\ntail.table  <- function(x, n = 6L, addrownums = TRUE, ...) {\n    (if(length(dim(x)) == 2L) tail.matrix else tail.default)(x, n=n,\n\t      addrownums = addrownums, ...)\n}\n\ntail.ftable <- function(x, n = 6L, addrownums = FALSE, ...) {\n    r <- format(x)\n    dimnames(r) <- list(if(!addrownums) rep.int(\"\", nrow(r)),\n\t\t\trep.int(\"\", ncol(r)))\n    noquote(tail.matrix(r, n = n, addrownums = addrownums, ...))\n}\n\ntail.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(tail(lines, n=n))\n}\n" }
{ "repo_name": "cxxr-devel/cxxr-svn-mirror", "ref": "refs/heads/master", "path": "src/library/utils/R/head.R", "content": "#  File src/library/utils/R/head.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2012 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n### placed in the public domain 2002\n### Patrick Burns patrick@burns-stat.com\n###\n### Adapted for negative arguments by Vincent Goulet\n### <vincent.goulet@act.ulaval.ca>, 2006\n\nhead <- function(x, ...) UseMethod(\"head\")\n\nhead.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(length(x) + n, 0L) else min(n, length(x))\n    x[seq_len(n)]\n}\n\n## head.matrix and tail.matrix are now exported (to be used for other classes)\nhead.data.frame <- head.matrix <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(nrow(x) + n, 0L) else min(n, nrow(x))\n    x[seq_len(n), , drop=FALSE]\n}\nhead.table  <- function(x, n = 6L, ...) {\n    (if(length(dim(x)) == 2L) head.matrix else head.default)(x, n=n)\n}\n\nhead.ftable <- function(x, n = 6L, ...) {\n    r <- format(x)\n    dimnames(r) <- list(rep.int(\"\", nrow(r)), rep.int(\"\", ncol(r)))\n    noquote(head.matrix(r, n = n + nrow(r) - nrow(x), ...))\n}\n\nhead.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(head(lines, n=n))\n}\n\ntail <- function(x, ...) UseMethod(\"tail\")\n\ntail.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    xlen <- length(x)\n    n <- if (n < 0L) max(xlen + n, 0L) else min(n, xlen)\n    x[seq.int(to = xlen, length.out = n)]\n}\n\ntail.data.frame <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    x[seq.int(to = nrx, length.out = n), , drop = FALSE]\n}\n\ntail.matrix <- function(x, n = 6L, addrownums = TRUE, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    sel <- seq.int(to = nrx, length.out = n)\n    ans <- x[sel, , drop = FALSE]\n    if (addrownums && is.null(rownames(x)))\n    \trownames(ans) <- paste0(\"[\", sel, \",]\")\n    ans\n}\ntail.table  <- function(x, n = 6L, addrownums = TRUE, ...) {\n    (if(length(dim(x)) == 2L) tail.matrix else tail.default)(x, n=n,\n\t      addrownums = addrownums, ...)\n}\n\ntail.ftable <- function(x, n = 6L, addrownums = FALSE, ...) {\n    r <- format(x)\n    dimnames(r) <- list(if(!addrownums) rep.int(\"\", nrow(r)),\n\t\t\trep.int(\"\", ncol(r)))\n    noquote(tail.matrix(r, n = n, addrownums = addrownums, ...))\n}\n\ntail.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(tail(lines, n=n))\n}\n" }
{ "repo_name": "cmosetick/RRO", "ref": "refs/heads/master", "path": "R-src/src/library/utils/R/head.R", "content": "#  File src/library/utils/R/head.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2012 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n### placed in the public domain 2002\n### Patrick Burns patrick@burns-stat.com\n###\n### Adapted for negative arguments by Vincent Goulet\n### <vincent.goulet@act.ulaval.ca>, 2006\n\nhead <- function(x, ...) UseMethod(\"head\")\n\nhead.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(length(x) + n, 0L) else min(n, length(x))\n    x[seq_len(n)]\n}\n\n## head.matrix and tail.matrix are now exported (to be used for other classes)\nhead.data.frame <- head.matrix <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(nrow(x) + n, 0L) else min(n, nrow(x))\n    x[seq_len(n), , drop=FALSE]\n}\nhead.table  <- function(x, n = 6L, ...) {\n    (if(length(dim(x)) == 2L) head.matrix else head.default)(x, n=n)\n}\n\nhead.ftable <- function(x, n = 6L, ...) {\n    r <- format(x)\n    dimnames(r) <- list(rep.int(\"\", nrow(r)), rep.int(\"\", ncol(r)))\n    noquote(head.matrix(r, n = n + nrow(r) - nrow(x), ...))\n}\n\nhead.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(head(lines, n=n))\n}\n\ntail <- function(x, ...) UseMethod(\"tail\")\n\ntail.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    xlen <- length(x)\n    n <- if (n < 0L) max(xlen + n, 0L) else min(n, xlen)\n    x[seq.int(to = xlen, length.out = n)]\n}\n\ntail.data.frame <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    x[seq.int(to = nrx, length.out = n), , drop = FALSE]\n}\n\ntail.matrix <- function(x, n = 6L, addrownums = TRUE, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    sel <- seq.int(to = nrx, length.out = n)\n    ans <- x[sel, , drop = FALSE]\n    if (addrownums && is.null(rownames(x)))\n    \trownames(ans) <- paste0(\"[\", sel, \",]\")\n    ans\n}\ntail.table  <- function(x, n = 6L, addrownums = TRUE, ...) {\n    (if(length(dim(x)) == 2L) tail.matrix else tail.default)(x, n=n,\n\t      addrownums = addrownums, ...)\n}\n\ntail.ftable <- function(x, n = 6L, addrownums = FALSE, ...) {\n    r <- format(x)\n    dimnames(r) <- list(if(!addrownums) rep.int(\"\", nrow(r)),\n\t\t\trep.int(\"\", ncol(r)))\n    noquote(tail.matrix(r, n = n, addrownums = addrownums, ...))\n}\n\ntail.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(tail(lines, n=n))\n}\n" }
{ "repo_name": "skyguy94/R", "ref": "refs/heads/master", "path": "src/library/utils/R/head.R", "content": "#  File src/library/utils/R/head.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2012 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n### placed in the public domain 2002\n### Patrick Burns patrick@burns-stat.com\n###\n### Adapted for negative arguments by Vincent Goulet\n### <vincent.goulet@act.ulaval.ca>, 2006\n\nhead <- function(x, ...) UseMethod(\"head\")\n\nhead.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(length(x) + n, 0L) else min(n, length(x))\n    x[seq_len(n)]\n}\n\n## head.matrix and tail.matrix are now exported (to be used for other classes)\nhead.data.frame <- head.matrix <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(nrow(x) + n, 0L) else min(n, nrow(x))\n    x[seq_len(n), , drop=FALSE]\n}\nhead.table  <- function(x, n = 6L, ...) {\n    (if(length(dim(x)) == 2L) head.matrix else head.default)(x, n=n)\n}\n\nhead.ftable <- function(x, n = 6L, ...) {\n    r <- format(x)\n    dimnames(r) <- list(rep.int(\"\", nrow(r)), rep.int(\"\", ncol(r)))\n    noquote(head.matrix(r, n = n + nrow(r) - nrow(x), ...))\n}\n\nhead.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(head(lines, n=n))\n}\n\ntail <- function(x, ...) UseMethod(\"tail\")\n\ntail.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    xlen <- length(x)\n    n <- if (n < 0L) max(xlen + n, 0L) else min(n, xlen)\n    x[seq.int(to = xlen, length.out = n)]\n}\n\ntail.data.frame <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    x[seq.int(to = nrx, length.out = n), , drop = FALSE]\n}\n\ntail.matrix <- function(x, n = 6L, addrownums = TRUE, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    sel <- seq.int(to = nrx, length.out = n)\n    ans <- x[sel, , drop = FALSE]\n    if (addrownums && is.null(rownames(x)))\n    \trownames(ans) <- paste0(\"[\", sel, \",]\")\n    ans\n}\ntail.table  <- function(x, n = 6L, addrownums = TRUE, ...) {\n    (if(length(dim(x)) == 2L) tail.matrix else tail.default)(x, n=n,\n\t      addrownums = addrownums, ...)\n}\n\ntail.ftable <- function(x, n = 6L, addrownums = FALSE, ...) {\n    r <- format(x)\n    dimnames(r) <- list(if(!addrownums) rep.int(\"\", nrow(r)),\n\t\t\trep.int(\"\", ncol(r)))\n    noquote(tail.matrix(r, n = n, addrownums = addrownums, ...))\n}\n\ntail.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(tail(lines, n=n))\n}\n" }
{ "repo_name": "jagdeesh109/RRO", "ref": "refs/heads/master", "path": "R-src/src/library/utils/R/head.R", "content": "#  File src/library/utils/R/head.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2012 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n### placed in the public domain 2002\n### Patrick Burns patrick@burns-stat.com\n###\n### Adapted for negative arguments by Vincent Goulet\n### <vincent.goulet@act.ulaval.ca>, 2006\n\nhead <- function(x, ...) UseMethod(\"head\")\n\nhead.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(length(x) + n, 0L) else min(n, length(x))\n    x[seq_len(n)]\n}\n\n## head.matrix and tail.matrix are now exported (to be used for other classes)\nhead.data.frame <- head.matrix <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(nrow(x) + n, 0L) else min(n, nrow(x))\n    x[seq_len(n), , drop=FALSE]\n}\nhead.table  <- function(x, n = 6L, ...) {\n    (if(length(dim(x)) == 2L) head.matrix else head.default)(x, n=n)\n}\n\nhead.ftable <- function(x, n = 6L, ...) {\n    r <- format(x)\n    dimnames(r) <- list(rep.int(\"\", nrow(r)), rep.int(\"\", ncol(r)))\n    noquote(head.matrix(r, n = n + nrow(r) - nrow(x), ...))\n}\n\nhead.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(head(lines, n=n))\n}\n\ntail <- function(x, ...) UseMethod(\"tail\")\n\ntail.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    xlen <- length(x)\n    n <- if (n < 0L) max(xlen + n, 0L) else min(n, xlen)\n    x[seq.int(to = xlen, length.out = n)]\n}\n\ntail.data.frame <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    x[seq.int(to = nrx, length.out = n), , drop = FALSE]\n}\n\ntail.matrix <- function(x, n = 6L, addrownums = TRUE, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    sel <- seq.int(to = nrx, length.out = n)\n    ans <- x[sel, , drop = FALSE]\n    if (addrownums && is.null(rownames(x)))\n    \trownames(ans) <- paste0(\"[\", sel, \",]\")\n    ans\n}\ntail.table  <- function(x, n = 6L, addrownums = TRUE, ...) {\n    (if(length(dim(x)) == 2L) tail.matrix else tail.default)(x, n=n,\n\t      addrownums = addrownums, ...)\n}\n\ntail.ftable <- function(x, n = 6L, addrownums = FALSE, ...) {\n    r <- format(x)\n    dimnames(r) <- list(if(!addrownums) rep.int(\"\", nrow(r)),\n\t\t\trep.int(\"\", ncol(r)))\n    noquote(tail.matrix(r, n = n, addrownums = addrownums, ...))\n}\n\ntail.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(tail(lines, n=n))\n}\n" }
{ "repo_name": "hxfeng/R-3.1.2", "ref": "refs/heads/master", "path": "src/library/utils/R/head.R", "content": "#  File src/library/utils/R/head.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2012 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n### placed in the public domain 2002\n### Patrick Burns patrick@burns-stat.com\n###\n### Adapted for negative arguments by Vincent Goulet\n### <vincent.goulet@act.ulaval.ca>, 2006\n\nhead <- function(x, ...) UseMethod(\"head\")\n\nhead.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(length(x) + n, 0L) else min(n, length(x))\n    x[seq_len(n)]\n}\n\n## head.matrix and tail.matrix are now exported (to be used for other classes)\nhead.data.frame <- head.matrix <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(nrow(x) + n, 0L) else min(n, nrow(x))\n    x[seq_len(n), , drop=FALSE]\n}\nhead.table  <- function(x, n = 6L, ...) {\n    (if(length(dim(x)) == 2L) head.matrix else head.default)(x, n=n)\n}\n\nhead.ftable <- function(x, n = 6L, ...) {\n    r <- format(x)\n    dimnames(r) <- list(rep.int(\"\", nrow(r)), rep.int(\"\", ncol(r)))\n    noquote(head.matrix(r, n = n + nrow(r) - nrow(x), ...))\n}\n\nhead.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(head(lines, n=n))\n}\n\ntail <- function(x, ...) UseMethod(\"tail\")\n\ntail.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    xlen <- length(x)\n    n <- if (n < 0L) max(xlen + n, 0L) else min(n, xlen)\n    x[seq.int(to = xlen, length.out = n)]\n}\n\ntail.data.frame <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    x[seq.int(to = nrx, length.out = n), , drop = FALSE]\n}\n\ntail.matrix <- function(x, n = 6L, addrownums = TRUE, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    sel <- seq.int(to = nrx, length.out = n)\n    ans <- x[sel, , drop = FALSE]\n    if (addrownums && is.null(rownames(x)))\n    \trownames(ans) <- paste0(\"[\", sel, \",]\")\n    ans\n}\ntail.table  <- function(x, n = 6L, addrownums = TRUE, ...) {\n    (if(length(dim(x)) == 2L) tail.matrix else tail.default)(x, n=n,\n\t      addrownums = addrownums, ...)\n}\n\ntail.ftable <- function(x, n = 6L, addrownums = FALSE, ...) {\n    r <- format(x)\n    dimnames(r) <- list(if(!addrownums) rep.int(\"\", nrow(r)),\n\t\t\trep.int(\"\", ncol(r)))\n    noquote(tail.matrix(r, n = n, addrownums = addrownums, ...))\n}\n\ntail.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(tail(lines, n=n))\n}\n" }
{ "repo_name": "kalibera/rexp", "ref": "refs/heads/master", "path": "src/library/utils/R/head.R", "content": "#  File src/library/utils/R/head.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2012 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n### placed in the public domain 2002\n### Patrick Burns patrick@burns-stat.com\n###\n### Adapted for negative arguments by Vincent Goulet\n### <vincent.goulet@act.ulaval.ca>, 2006\n\nhead <- function(x, ...) UseMethod(\"head\")\n\nhead.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(length(x) + n, 0L) else min(n, length(x))\n    x[seq_len(n)]\n}\n\n## head.matrix and tail.matrix are now exported (to be used for other classes)\nhead.data.frame <- head.matrix <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    n <- if (n < 0L) max(nrow(x) + n, 0L) else min(n, nrow(x))\n    x[seq_len(n), , drop=FALSE]\n}\nhead.table  <- function(x, n = 6L, ...) {\n    (if(length(dim(x)) == 2L) head.matrix else head.default)(x, n=n)\n}\n\nhead.ftable <- function(x, n = 6L, ...) {\n    r <- format(x)\n    dimnames(r) <- list(rep.int(\"\", nrow(r)), rep.int(\"\", ncol(r)))\n    noquote(head.matrix(r, n = n + nrow(r) - nrow(x), ...))\n}\n\nhead.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(head(lines, n=n))\n}\n\ntail <- function(x, ...) UseMethod(\"tail\")\n\ntail.default <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    xlen <- length(x)\n    n <- if (n < 0L) max(xlen + n, 0L) else min(n, xlen)\n    x[seq.int(to = xlen, length.out = n)]\n}\n\ntail.data.frame <- function(x, n = 6L, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    x[seq.int(to = nrx, length.out = n), , drop = FALSE]\n}\n\ntail.matrix <- function(x, n = 6L, addrownums = TRUE, ...)\n{\n    stopifnot(length(n) == 1L)\n    nrx <- nrow(x)\n    n <- if (n < 0L) max(nrx + n, 0L) else min(n, nrx)\n    sel <- seq.int(to = nrx, length.out = n)\n    ans <- x[sel, , drop = FALSE]\n    if (addrownums && is.null(rownames(x)))\n    \trownames(ans) <- paste0(\"[\", sel, \",]\")\n    ans\n}\ntail.table  <- function(x, n = 6L, addrownums = TRUE, ...) {\n    (if(length(dim(x)) == 2L) tail.matrix else tail.default)(x, n=n,\n\t      addrownums = addrownums, ...)\n}\n\ntail.ftable <- function(x, n = 6L, addrownums = FALSE, ...) {\n    r <- format(x)\n    dimnames(r) <- list(if(!addrownums) rep.int(\"\", nrow(r)),\n\t\t\trep.int(\"\", ncol(r)))\n    noquote(tail.matrix(r, n = n, addrownums = addrownums, ...))\n}\n\ntail.function <- function(x, n = 6L, ...)\n{\n    lines <- as.matrix(deparse(x))\n    dimnames(lines) <- list(seq_along(lines),\"\")\n    noquote(tail(lines, n=n))\n}\n" }
{ "repo_name": "Bioinformatics-Support-Unit/shiny", "ref": "refs/heads/master", "path": "profile_plot/Version 2 /server.R", "content": "library(shiny)\nlibrary(shinyIncubator)\nsource(\"profile_plot.R\")\nload(\"test.Rdata\")\n# shinyServer(function(input, output, session) {\n#   \n# #   source(\"profile_plot.R\")\n# #   load(\"test.Rdata\")\n# \n#   output$distPlot <- renderPlot({\n#     if(input$selection == \"raw\") {\n#       plot_data <- as.matrix(obatch[,1:36])\n#     } else if(input$selection == \"normalised\") {\n#       plot_data <- as.matrix(eset.spike[,1:36])\n#     } \n#     \n#     if(input$sep == \"Yes\"){\n#       enable <- TRUE\n#     } else if(input$sep == \"No\"){\n#       enable <- FALSE\n#     }\n#     \n#     if(input$raw_probes | input$normal_probes){}\n#     \n#     if(input$selection == \"raw\") {\n#       isolate({\n#         withProgress(session, {\n#           setProgress(message = \"Processing Profile...\")\n#           load(\"test.Rdata\")\n#           plot_profile(plot_data[1:input$raw_probes,], treatments = treatment, sep=enable)\n#         })\n#       })\n#     } else {\n#       isolate({\n#         withProgress(session, {\n#           setProgress(message = \"Processing Profile...\")\n#           load(\"test.Rdata\")\n#           plot_profile(plot_data[1:input$normal_probes,], treatments = treatment, sep=enable)\n#         })\n#       })\n#     }    \n#        \n#   })\n# })\n\n\nshinyServer(function(input, output) {\n  \n#   source(\"profile_plot.R\")\n#   load(\"test.Rdata\")\n  values <- reactiveValues()\n  \n  datasetInput <- reactive({\n    switch(input$dataset,\n           \"Raw Data\" = obatch,\n           \"Normalised Data - Pre QC\" = eset.spike)\n  })\n  \n  sepInput <- reactive({\n    switch(input$sep,\n           \"Yes\" = TRUE,\n           \"No\" = FALSE)\n  })\n  \n#   unit <- reactive({\n#     input$unit\n#     rangeInput()\n#   })\n#   \n#   probes <- reactive({\n#     input$probes\n#     rangeInput()\n#   })\n  \n  rangeInput <- reactive({\n    df <- datasetInput()\n    values$range  <- length(df[,1])\n    if(input$unit == \"Percentile\") {\n      values$first  <- ceiling((values$range/100) * input$percentile[1])\n      values$last   <- ceiling((values$range/100) * input$percentile[2])\n    } \n    else if(input$unit == \"Absolute\"){\n        if(input$dataset == \"Raw Data\")\n        {\n          values$first  <- 1\n          values$last   <- input$probes_raw\n        } else {\n          values$first  <- 1\n          values$last   <- input$probes_normalisedI\n        }   \n    }\n  })\n  \n  plotInput <- reactive({\n    df     <- datasetInput()\n    enable <- sepInput()\n    rangeInput()\n#     unit()\n#     probes()\n    p      <- plot_profile(df[values$first:values$last,],\n                           treatments=treatment, \n                           sep=enable)\n  })\n\n  output$plot <- renderPlot({\n    print(plotInput())\n  })\n  \n  output$downloadData <- downloadHandler(\n    filename = function() { paste(input$dataset, '_Data.csv', sep='') },\n    content = function(file) {\n      write.csv(datasetInput(), file)\n    }\n  )\n\n  output$downloadRangeData <- downloadHandler(\n    filename = function() { paste(input$dataset, '_', values$first, '_', values$last, '_Range.csv', sep='') },\n    content = function(file) {\n      write.csv(datasetInput()[values$first:values$last,], file)\n    }\n  )\n  \n  output$downloadPlot <- downloadHandler(\n    filename = function() { paste(input$dataset, '_ProfilePlot.png', sep='') },\n    content = function(file) {\n      png(file)\n      print(plotInput())\n      dev.off()\n    }\n  )\n  \n})\n\n" }
{ "repo_name": "PROBIC/diffsplicing", "ref": "refs/heads/master", "path": "codes/R/negloglik.R", "content": "# Copyright (c) 2015, Hande TOPA\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#     Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n# \n#     Redistributions in binary form must reproduce the above copyright\n#     notice, this list of conditions and the following disclaimer in\n#     the documentation and/or other materials provided with the\n#     distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nnegloglik <-\nfunction(theta,y,mu) {\n\n\t# theta = [alpha; beta]\n\t# y = the data vector, M*R matrix for M transcripts R replicates\n \n\tM=dim(y)[1]\n\tR=dim(y)[2]\n\talpha=theta[1]\n\tbeta=theta[2]\n\n\tif (alpha<0 | beta<0) {\n   \t\tnegloglik=-log(0)\n\t} else {\n  \t\ty_2=y^2\n  \t\tA=as.matrix(rowSums(y_2))\n  \t\tB=2*mu*as.matrix(rowSums(y))-R*(mu^2)\n  \t\tC=A-B\n  \t\tnegloglik=-(M*alpha*log(beta)-M*lgamma(alpha)+M*lgamma(alpha+R/2)-(alpha+R/2)*sum(log(beta+0.5*C)))\n\t}\n\t\n\treturn(negloglik)\n\t\n}\n" }
{ "repo_name": "stephenslab/EbayesThresh", "ref": "refs/heads/master", "path": "R/wfromt.R", "content": "wfromt <- function(tt, s = 1, prior = \"laplace\", a = 0.5) {\n#\n#  Find the weight that has posterior median threshold tt, \n#   given s (sd) and a.\n#\n\tpr <- substring(prior, 1, 1)\n\tif(pr == \"l\"){\n\t  tma <- tt/s - s*a\n\t  wi <- 1/abs(tma)\n\t  wi[tma > -35] <- pnorm(tma[tma > -35])/dnorm(tma[tma > -35])\n          wi <- a * s * wi - beta.laplace(tt, s, a)\n\t}\n\tif(pr == \"c\") {\n\t\tdnz <- dnorm(tt)\n\t\twi <- 1 + (pnorm(tt) - tt * dnz - 1/2)/\n                          (sqrt(pi/2) * dnz * tt^2)\n\t\twi[!is.finite(wi)] <- 1\n\t}\n\treturn(1/wi)\n}\n" }
{ "repo_name": "beansrowning/Modelling", "ref": "refs/heads/master", "path": "Current/Cluster/maltapt1.R", "content": "# Approach 2 gridsearch on the Latvia model\n# 3 Runs, looking at how age of inserted case affects outcome\nrequire(doMPI)\n# Make Cluster\ncl <- startMPIcluster()\nregisterDoMPI(cl)\ntryCatch(require(Rcpp),\n         error = function(e) {\n           Sys.sleep(2)\n           require(Rcpp)\n         })\ntryCatch(require(data.table),\n         error = function(e) {\n          Sys.sleep(2)\n          require(data.table)\n})\ntryCatch(require(adaptivetau),\n         error = function(e) {\n          Sys.sleep(2)\n          require(adaptivetau)\n})\ntryCatch(require(parallel),\n         error = function(e) {\n          Sys.sleep(2)\n          require(parallel)\n})\ntryCatch(require(testpkg),\n         error = function(e) {\n          Sys.sleep(2)\n          require(testpkg)\n})\nsource(\"../../Data/model_global.R\")\nsource(\"gridsearch2_mpi.R\")\nopts <- list(chunkSize = ceiling(10000 / getDoParWorkers()))\n\nsolutions <- new.env()\n# Run 1\n# Insertion rates :   0.01-0.1\n# vaccinations rates: 0.9-1\n# Total grid size : 10 * 11 = 110\n# Search depth : 10,000 runs\n# All other population values fixed\n# Case introduction over the course of 1 year\n# Offest by 2000 days beyond\nprint(paste0(\"Begining Run 1 - \", date()))\nsolutions$t1 <- system.time(solutions$run_1 <- solutionSpace(malta,\n                                    insbound = c(0.01, 0.02, 0.03, 0.04, 0.05,\n                                                0.06, 0.07, 0.08, 0.09, 0.1),\n                                    vaccbound = c(0.9, 0.91, 0.92, 0.93, 0.94,\n                                                  0.95, 0.96, 0.97, 0.98, 0.99, 1),\n                                    len = 365,\n                                    offset = 2000))\nprint(paste0(\"Run 1 done - \", solutions$t1[3]))\nsave(solutions, file = \"../../Data/malta_1.dat\")\n# # Run 2\n# # Insertion rates :   0.01-0.1\n# # vaccinations rates: 0.9-1\n# # Total grid size : 10 * 11 = 110\n# # Search depth : 10,000 runs\n# # All other population values fixed\n# # Old persons twice as likely to be introduced\n# # Case introduction over the course of 1 year\n# # Offest by 2000 days beyond\n# print(paste0(\"Begining Run 2 - \", date()))\n# solutions$t2 <- system.time(solutions$run_2 <- solutionSpace(malta,\n#                                     insbound = c(0.01, 0.02, 0.03, 0.04, 0.05,\n#                                                  0.06, 0.07, 0.08, 0.09, 0.1),\n#                                     vaccbound = c(0.9, 0.91, 0.92, 0.93, 0.94,\n#                                                   0.95, 0.96, 0.97, 0.98, 0.99, 1),\n#                                     len = 365,\n#                                     grp = c(0.5, 1),\n#                                     offset = 2000))\n# print(paste0(\"Run 2 done - \", solutions$t2[3]))\n# save(solutions, file = \"../../Data/malta_1.dat\")\n#\n# # Run 3\n# # Insertion rates :   0.01-0.1\n# # vaccinations rates: 0.9-1\n# # Total grid size : 10 * 11 = 110\n# # Search depth : 10,000 runs\n# # All other population values fixed\n# # young persons twice as likely to be introduced\n# # Case introduction over the course of 1 year\n# # Offest by 2000 days beyond\n# print(paste0(\"Begining Run 3 - \", date()))\n# solutions$t3 <- system.time(solutions$run_3 <- solutionSpace(malta,\n#                                     insbound = c(0.01, 0.02, 0.03, 0.04, 0.05,\n#                                                 0.06, 0.07, 0.08, 0.09, 0.1),\n#                                     vaccbound = c(0.9, 0.91, 0.92, 0.93, 0.94,\n#                                                   0.95, 0.96, 0.97, 0.98, 0.99, 1),\n#                                     len = 365,\n#                                     grp = c(1, 0.5),\n#                                     offset = 2000))\n# print(paste0(\"Run 2 done - \", solutions$t2[3]))\n# save(solutions, file = \"../../Data/malta_1.dat\")\nprint(paste0(\"All done - \", date()))\n\ncloseCluster(cl)\nmpi.quit()\n" }
{ "repo_name": "meisamhe/GPLshared", "ref": "refs/heads/master", "path": "ChoiceModels/R/momMix.R", "content": "momMix=\nfunction(probdraw,compdraw) \n{\n#\n# Revision History:\n#   R. McCulloch 11/04\n#   P. Rossi 3/05  put in backsolve fixed documentation\n#   P. Rossi 9/05 fixed error in mom -- return var not sigma\n#\n# purpose: compute moments of normal mixture averaged over MCMC draws\n#\n# arguments:\n#    probdraw -- ith row is ith draw of probabilities of mixture comp\n#    compdraw -- list of lists of draws of mixture comp moments (each sublist is from mixgibbs)\n#\n# output:\n#   a list with the mean vector, covar matrix, vector of std deve, and corr matrix\n#\n# ----------------------------------------------------------------------------------\n# define function needed\nmom=function(prob,comps){\n# purpose: obtain mu and cov from list of normal components\n#\n# arguments:\n#     prob: vector of mixture probs\n#     comps: list, each member is a list comp with ith normal component ~N(comp[[1]],Sigma), \n#            Sigma = t(R)%*%R, R^{-1} = comp[[2]]\n# returns:\n#  a list with [[1]]=$mu a vector\n#  [[2]]=$sigma a matrix \n#\nnc = length(comps)\ndim = length(comps[[1]][[1]])\nmu = double(dim)\nsigma = matrix(0.0,dim,dim)\nfor(i in 1:nc) {\n   mu = mu+ prob[i]*comps[[i]][[1]]\n}\nvar=matrix(double(dim*dim),ncol=dim)\nfor(i in 1:nc) {\n   mui=comps[[i]][[1]]\n#   root = solve(comps[[i]][[2]])\n   root=backsolve(comps[[i]][[2]],diag(rep(1,dim)))\n   sigma=t(root)%*%root\n   var=var+prob[i]*sigma+prob[i]*(mui-mu)%o%(mui-mu)\n}\nlist(mu=mu,sigma=var)\n}\n#---------------------------------------------------------------------------------------\ndim=length(compdraw[[1]][[1]][[1]])\nnc=length(compdraw[[1]])\ndim(probdraw)=c(length(compdraw),nc)\nmu=double(dim)\nsigma=matrix(double(dim*dim),ncol=dim)\nsd=double(dim)\ncorr=matrix(double(dim*dim),ncol=dim)\nfor(i in 1:length(compdraw)) \n{\n   out=mom(probdraw[i,],compdraw[[i]])\n   sd=sd+sqrt(diag(out$sigma))\n   corr=corr+matrix(nmat(out$sigma),ncol=dim)\n   mu=mu+out$mu\n   sigma=sigma+out$sigma\n}\nmu=mu/length(compdraw)\nsigma=sigma/length(compdraw)\nsd=sd/length(compdraw)\ncorr=corr/length(compdraw)\nreturn(list(mu=mu,sigma=sigma,sd=sd,corr=corr))\n}\n" }
{ "repo_name": "adamryczkowski/clustertools", "ref": "refs/heads/master", "path": "possible_bug_in_parallel.R", "content": "spawn_fn<-function(nr) {\n  env<-new.env()\n  env$a<-nr\n  j<-eval(quote(parallel::mcparallel(a)), envir = env)\n  return(list(job=j, value=nr))\n}\n\ntest_fn<-function(item) {\n  ans<-parallel::mccollect(jobs = item$job)[[1]]\n  if(length(ans)==0) {\n    browser()\n  }\n  if(item$value!=ans) {\n    stop(\"Error\")\n  }\n}\na<-spawn_fn(1)\ntest_fn(a)\n\njobs<-rep(x = list(list()),200)\n\nfor(i in seq(10000)) {\n  job_nr<-sample.int(length(jobs), 1)\n  if(length(jobs[[job_nr]])!=0){\n    test_fn(jobs[[job_nr]])\n  }\n  jobs[[job_nr]]<-spawn_fn(runif(1))\n}\n" }
{ "repo_name": "armgong/DistributedR", "ref": "refs/heads/master", "path": "algorithms/HPdata/R/ddc.R", "content": "#' Load a CSV file into a distributed data frame.\n#'\n#' @section Partitioning between executors:\n#'\n#' We generate as many partitions as files. If there are less files than executors we split each file further.\n#'\n#' E.g. let’s say we have 3 executors and we try to load /tmp/*.csv which expands to [/tmp/file1.csv (500MB) and /tmp/file2.csv (1MB)]. Initially we create 2 partitions (the number of files). As we have more executors (3) than partitions (2) we further divide the biggest file into 2. In the end we have 3 partitions (/tmp/file1.csv from 0 to 250MB, /tmp/file1.csv from 250MB to 500MB and /tmp/file2.csv.\n#'\n#' If globbing is not used we only load one file which will be divided in as many chunks as executors.\n#'\n#' @section Details:\n#'\n#' There is a limitation in the case where the number of lines is less than the number executors.\n#' In this case the load will fail. R's function \\code{read.csv()} can be used instead.\n#'\n#' @param url File URL. Examples: '/tmp/file.csv', 'hdfs:///file.csv'.\n#'\n#'                      We also support globbing. Examples: '/tmp/*.csv', 'hdfs:///tmp/*.csv'.\n#'\n#'                      When globbing all CSV files need to have the same schema and delimiter.\n#' @param schema  Specifies the column names and types.\n#'\n#'                Syntax is: \\code{<col0-name>:<col0_type>,<col1-name>:<col1_type>,...<colN-name>:<colN_type>}.\n#'\n#'                Supported types are: \\code{logical}, \\code{integer}, \\code{int64}, \\code{numeric} and \\code{character}. \n#'\n#'                Example: schema='age:int64,name:character'.\n#'\n#'                Note that due to R not having a proper int64 type we convert it to an R numeric. Type conversion work as follows:\n#'\n#'                \\tabular{ll}{\n#'                    CSV type  |\\tab R type    \\cr\n#'                    -         |\\tab -         \\cr\n#'                    integer   |\\tab integer   \\cr\n#'                    numeric   |\\tab numeric   \\cr\n#'                    logical   |\\tab logical   \\cr\n#'                    int64     |\\tab numeric   \\cr\n#'                    character |\\tab character\n#'                }\n#' @param delimiter Column separator. Example: delimiter='|'. By default delimiter is ','.\n#' @param commentCharacter Discard lines starting with this character. Leading spaces are ignored.\n#' @param hdfsConfigurationFile By default: \\code{paste(system.file(package='hdfsconnector'),'/conf/hdfs.json',sep='')}.\n#'\n#'                              Options are:\n#'                              \\itemize{\n#'                                  \\item webhdfsPort: webhdfs port, integer\n#'                                  \\item hdfsPort: hdfs namenode port, integer\n#'                                  \\item hdfsHost: hdfs namenode host, string\n#'                                  \\item hdfsUser: hdfs username, string\n#'                              }\n#'\n#'                              An example file is:\n#'\n#'                                  \\{ \\cr\n#'                                  \"webhdfsPort\": 50070, \\cr\n#'                                  \"hdfsPort\": 9000, \\cr\n#'                                  \"hdfsHost\": \"172.17.0.3\", \\cr\n#'                                  \"hdfsUser\": \"jorgem\" \\cr\n#'                                  \\}\n#'\n#' @param skipHeader Treat first line as the CSV header and discard it.\n#' @return A distributed data frame representing the CSV file.\n#' @examples\n#' df <- csv2dframe(url=paste(system.file(package='HPdata'),'/tests/data/ex001.csv',sep=''), schema='a:int64,b:character')\n\ncsv2dframe <- function(url, schema, delimiter=',', commentCharacter='#', \n                       hdfsConfigurationFile=paste(system.file(package='hdfsconnector'),'/conf/hdfs.json',sep=''),\n                       skipHeader=FALSE) {\n    options = list()\n    options['schema'] = schema\n    options['delimiter'] = delimiter\n    options['commentCharacter'] = commentCharacter\n    options['hdfsConfigurationFile'] = hdfsConfigurationFile\n    options['skipHeader'] = skipHeader\n\n    options['fileType'] = 'csv'\n    .ddc_read(url, options)\n#    tryCatch({\n#        .ddc_read(url, options)\n#    }, error = function(e){\n#        if (grepl('attempt to set partition',paste(e)) == TRUE) {\n#            # retry with read.csv\n#            warning('CSV file has less lines than executors. Trying with read.csv ...')\n#            if (grepl('hdfs://',url) == TRUE) {\n#                stop('Unable to read hdfs files with read.csv. Try starting Distributed R with only one executor (inst=1).')\n#            }\n#            d <- dframe(npartitions=c(1,1))\n#            foreach(i,\n#                1:npartitions(d),\n#                func <- function(dhs = splits(d,i),\n#                                 url = url) {\n#                    dhs <- read.csv(url)\n#                    update(dhs)\n#            })\n#        }\n#        else {\n#            stop(e)\n#        }\n#    })\n}\n\n#' Load an ORC file into a distributed data frame\n#'\n#' @section Partitioning between executors:\n#'\n#' We generate as many partitions as the total number of ORC stripes.\n#'\n#' E.g. let’s say the customer tries to load /tmp/*.orc which expands to /tmp/file1.orc with 2 stripes and /tmp/file2.orc with 3 stripes. We’ll end up with 2 + 3 = 5 partitions.\n#'\n#' If globbing is not used we only load one file and the number of partitions will be equal to the number of stripes.\n#'\n#' @section Details:\n#'\n#' Type conversions work as follows:\n#'                \\tabular{ll}{\n#'                    ORC type          |\\tab R type        \\cr\n#'                    -                 |\\tab -             \\cr\n#'                    byte/short/int    |\\tab integer       \\cr\n#'                    float/double      |\\tab numeric       \\cr\n#'                    long              |\\tab numeric       \\cr\n#'                    bool              |\\tab logical       \\cr\n#'                    string/binary     |\\tab character     \\cr\n#'                    char/varchar      |\\tab character     \\cr\n#'                    decimal           |\\tab character     \\cr\n#'                    timestamp/date    |\\tab character     \\cr\n#'                    union             |\\tab not supported \\cr\n#'                    struct            |\\tab dataframe     \\cr\n#'                    map               |\\tab dataframe     \\cr\n#'                    list              |\\tab list\n#'                }\n#'\n#' @param url File URL. Examples: '/tmp/file.orc', 'hdfs:///file.orc'.\n#' @param selectedStripes ORC stripes to include. Stripes need to be consecutive. If not specified defaults to all the stripes in the ORC file.\n#' @param hdfsConfigurationFile By default: \\code{paste(system.file(package='hdfsconnector'),'/conf/hdfs.json',sep='')}.\n#'\n#'                              Options are:\n#'                              \\itemize{\n#'                                  \\item webhdfsPort: webhdfs port, integer\n#'                                  \\item hdfsPort: hdfs namenode port, integer\n#'                                  \\item hdfsHost: hdfs namenode host, string\n#'                                  \\item hdfsUser: hdfs username, string\n#'                              }\n#'\n#'                              An example file is:\n#'\n#'                                  \\{ \\cr\n#'                                  \"webhdfsPort\": 50070, \\cr\n#'                                  \"hdfsPort\": 9000, \\cr\n#'                                  \"hdfsHost\": \"172.17.0.3\", \\cr\n#'                                  \"hdfsUser\": \"jorgem\" \\cr\n#'                                  \\}\n#' @return A distributed data frame representing the ORC file.\n#' @examples\n#' df <- orc2dframe(url=paste(system.file(package='HPdata'),'/tests/data/TestOrcFile.test1.orc',sep=''))\n\n\norc2dframe <- function(url, selectedStripes='', \n                       hdfsConfigurationFile=paste(system.file(package='hdfsconnector'),'/conf/hdfs.json',sep='')) {\n    options = list()\n    options['selectedStripes'] = selectedStripes\n    options['hdfsConfigurationFile'] = hdfsConfigurationFile\n\n    options['fileType'] = 'orc'\n    .ddc_read(url, options)\n}\n\n.ddc_read <- function(url, options) {\n    if(!(\"hdfsConfigurationFile\" %in% names(options))) {\n        # set default hdfsConfigurationFile\n        options[\"hdfsConfigurationFile\"] = paste(system.file(package='hdfsconnector'),'/conf/hdfs.json',sep='')\n    }\n\n\n    pm <- get_pm_object()\n    # 1. Schedule file across workers. Handles globbing also.\n    library(hdfsconnector)\n    plan <- hdfsconnector::create_plan(url, options, pm$worker_map())\n\n    hdfsConfigurationStr <- paste(readLines(as.character(options[\"hdfsConfigurationFile\"])),collapse='\\n')\n    for (i in 1:length(plan$configs)) {\n        plan$configs[[i]][\"hdfsConfigurationStr\"] = hdfsConfigurationStr\n        if(\"skipHeader\" %in% names(options)) {\n            plan$configs[[i]][\"skipHeader\"] = as.logical(options[\"skipHeader\"])\n        }\n    }\n\n    if (Sys.getenv('DEBUG_DDC') != '') {\n        print(plan)  # for debugging\n    }\n\n    # set chunk_worker_map in master so dframe partitions are created on the right workers\n    pm$ddc_set_chunk_worker_map(plan$chunk_worker_map)\n\n    #\n    # plan$num_partitions\n    #\n    # plan$configs\n    #   configs is an array of configurations, one for each worker. Each config has all\n    #   the information the worker needs to featch a split\n\n    # 2. Create darray\n    # When using the \"ddc\" policy each split is assigned to a worker according to the plan\n    # generated in pm$ddc_schedule\n    d <- dframe(npartitions=c(plan$num_partitions,1), distribution_policy='ddc')\n\n    # 3. Load each split on the workers\n    foreach(i, \n            1:npartitions(d), \n            func <- function(dhs = splits(d,i),\n                             url = url,\n                             config = plan$configs[[i]]) {\n                library(hdfsconnector)\n                if (config$file_type == \"csv\") {\n                    dhs <- csv2dataframe(config$url,\n                                         schema=config$schema,\n                                         chunkStart=config$chunk_start,\n                                         chunkEnd=config$chunk_end,\n                                         delimiter=config$delimiter,\n                                         commentCharacter=config$comment_character,\n                                         hdfsConfigurationStr=config$hdfsConfigurationStr,\n                                         skipHeader=config$skipHeader)\n                    update(dhs)\n                }\n                else if (config$file_type == \"orc\") {\n                    dhs <- orc2dataframe(url,\n                                         selectedStripes=config$selected_stripes,\n                                         hdfsConfigurationStr=config$hdfsConfigurationStr)\n                    update(dhs)\n                }\n                else {\n                    stop(\"Unsupported file type\")\n                }\n            }\n    )\n\n    c <- NULL\n    if(\"schema\" %in% names(options)) {\n        # CSV\n        c <- hdfsconnector::schema2colnames(as.character(options['schema']))\n    }\n    else if(as.character(options['fileType']) == 'orc') {\n        # ORC\n        c <- hdfsconnector::orccolnames(url, as.character(options['hdfsConfigurationFile']))\n    }\n    colnames(d) <- c;\n    d # return dframe\n}\n" }
{ "repo_name": "siabard/datasciencecoursera", "ref": "refs/heads/master", "path": "DataCleansing/week1.R", "content": "if(!file.exists(\"./data\")) {\n  dir.create(\"./data\")\n}\n\ndownload.file(url=\"https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD\", destfile = \"./data/rows.csv\")\ndownload.file(url=\"https://data.baltimorecity.gov/api/views/dz54-2aru/rows.xlsx?accessType=DOWNLOAD\", destfile=\"./data/rows.xlsx\")\ncameraData <- read.table(\"./data/rows.csv\", sep=\",\", header = TRUE)\n# cameraData <- read.csv(\"./data/rows.csv)\n\n\nfileUrl <- \"http://espn.go.com/nfl/team/_name/bal/baltimore-ravens\"\ndoc <- htmlTreeParse(fileUrl, useInternal=TRUE)\nscores <- xpathSApply(doc, \"//li[@class='score']\", xmlValue)\nteams <- xpathSApply(doc, \"//li[@class='team-name']\", xmlValue)\n                     \n\nlibrary(jsonlite)\njsonData <- fromJSON(\"https://api.github.com/users/jtleek/repos\")\nnames(jsonData)\n\ndownload.file(url = \"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv\", destfile=\"./data/acs.csv\")\n\ncsvData <- read.csv(\"./data/acs.csv\")\nnrow(csvData[csvData$VAL == 24 & !is.na(csvData$VAL),])\ndownload.file(url = \"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx\", destfile = \"./data/gov_ngap.xlsx\" , mode=\"wb\")\n\ndat <- read.xlsx(\"./data/gov_ngap.xlsx\", rowIndex = 18:23, colIndex=7:15, sheetIndex = 1)\nsum(dat$Zip*dat$Ext,na.rm=T)\n\nfileUrl <- \"http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml\"\nxmlData <- xmlTreeParse(fileUrl, useInternal = TRUE)\nnodes <- xmlRoot(xmlData)\n\nzipcodeNode <- xpathSApply(nodes, \"//zipcode\", xmlValue)\nsum(zipcodeNode == \"21231\")\n\n\nfileUrl <- \"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv\"\ndownload.file(url=\"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv\", destfile = \"./data/pid.csv\")\n\nDT <- fread(\"./data/pid.csv\")\n\nsystem.time({mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)})\n\nsystem.time({mean(DT$pwgtp15,by=DT$SEX)})\n\nsystem.time({tapply(DT$pwgtp15,DT$SEX,mean)})\n\nsystem.time({DT[,mean(pwgtp15),by=SEX]})\n\nsystem.time({rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]})\n\nsystem.time({sapply(split(DT$pwgtp15,DT$SEX),mean)})" }
{ "repo_name": "Chandramani/kaggle-competitions", "ref": "refs/heads/master", "path": "african-soil-property-prediction/svm-from-kaggle3.R", "content": "require(e1071)\n\ntrain <- read.csv(\"./training.csv\",header=TRUE,stringsAsFactors=FALSE)\ntest <- read.csv(\"./sorted_test.csv\",header=TRUE,stringsAsFactors=FALSE)\n\nsubmission <- test[,1]\n\nlabels <- train[,c(\"Ca\",\"P\",\"pH\",\"SOC\",\"Sand\")]\n\n# Exclude CO2\ntrain <- train[,c(2:2655,2671:(ncol(train)-5))]\ntest <- test[,c(2:2655,2671:ncol(test))]\n\nidx_to_use <- read.csv(\"./non_correlated_index.csv\",header=F,stringsAsFactors=FALSE)\nidx_to_use <- t(idx_to_use)\n\n##Handle depth as a 0/1 variable\ntrain$Depth <-  with ( train, ifelse ( ( Depth == 'Subsoil' ), 0 , 1 ) )\ntest$Depth <-  with ( test, ifelse ( ( Depth == 'Subsoil' ), 0 , 1 ) ) \n\n\n\nsvms <- lapply(1:ncol(labels),\n               function(i)\n               {\n                 svm(train,labels[,i],cost=10000,scale=FALSE)\n               })\n\npredictions <- sapply(svms,predict,newdata=test)\n\ncolnames(predictions) <- c(\"Ca\",\"P\",\"pH\",\"SOC\",\"Sand\")\nsubmission <- cbind(PIDN=submission,predictions)\n\nwrite.csv(submission,\"beating_benchmark_extended_2.0.csv\",row.names=FALSE,quote=FALSE)\n\n\nprint(\"So far so good\")\n" }
{ "repo_name": "joftius/fstep-tchi", "ref": "refs/heads/master", "path": "tests/test-all.R", "content": "library(testthat)\ntest_check(\"fstep\")\n" }
{ "repo_name": "jtrecenti/sabesp", "ref": "refs/heads/master", "path": "R/sabesp.R", "content": "#' Baixa dados da sabesp\n#' \n#' Baixa dados da sabesp sobre volume armazenado, pluviometria do dia, \n#' pluviometria acumulada no mês e média histórica do mês.\n#' \n#' @param datas vetor de datas (Date ou character vector)\n#' \n#' @return \\code{data.frame com resultados}\n#' \n#' @examples\n#' \\dontrun{\n#' dados <- baixa_sabesp_dias('2015-05-06')\n#' datas <- lubridate::today() - months(0:10)\n#' dados2 <- baixa_sabesp_dias(datas)\n#' }\n#' @export\nbaixa_sabesp_dias <- function(datas) {\n  baixa_sabesp_dia <- function(data) {\n    link <- 'http://www2.sabesp.com.br/mananciais/DivulgacaoSiteSabesp.aspx'\n    txt <- httr::GET(link)\n    viewstate <- txt %>% \n      httr::content('text') %>% \n      xml2::read_html() %>% \n      rvest::html_node('#__VIEWSTATE') %>% \n      rvest::html_attr('value')\n    eventval <- txt %>% \n      httr::content('text') %>% \n      xml2::read_html() %>% \n      rvest::html_node('#__EVENTVALIDATION') %>% \n      rvest::html_attr('value')\n    data <- as.Date(data)\n    dados <- list(cmbDia = lubridate::day(data), \n                  cmbMes = lubridate::month(data), \n                  cmbAno = lubridate::year(data), \n                  Imagebutton1.x = '0', \n                  Imagebutton1.y = '0', \n                  '__VIEWSTATE' = viewstate, \n                  '__EVENTVALIDATION' = eventval)\n    r <- httr::POST(link, body = dados)\n    \n    nomes <- r %>% \n      httr::content('text') %>% \n      xml2::read_html() %>% \n      rvest::html_nodes('img') %>% \n      rvest::html_attr('src') %>% \n      purrr::keep(~stringr::str_detect(.x, '\\\\.gif$')) %>% \n      purrr::map_chr(~stringr::str_match(.x, '/(.+)\\\\.gif')[, 2])\n    \n    d <- r %>% \n      httr::content('text') %>% \n      xml2::read_html() %>% \n      rvest::html_node('#tabDados') %>% \n      rvest::html_table(fill = TRUE) %>%\n      dplyr::select(titulo = X1, info = X2) %>%\n      dplyr::filter(titulo != '') %>%\n      dplyr::mutate(lugar = rep(nomes, each = 4)) %>% \n      dplyr::mutate(info = stringr::str_extract(info, '[\\\\-0-9, %m]+$'),\n                    info = stringr::str_replace_all(info, '^[^:]+:', ''), \n                    info = stringr::str_replace_all(info, ',', '.'), \n                    info = stringr::str_replace_all(info, '[^0-9.\\\\-]', ''),\n                    info = as.numeric(info)) %>% \n      dplyr::tbl_df()\n    return(d)\n  }\n  f <- dplyr::failwith(dplyr::data_frame, baixa_sabesp_dia)\n  datas %>% \n    as.character() %>% \n    {dplyr::data_frame(data = .)} %>% \n    dplyr::group_by(data) %>% \n    dplyr::do(f(.$data)) %>%\n    dplyr::ungroup() %>% \n    dplyr::mutate(data = as.Date(data))\n}\n\n" }
{ "repo_name": "mtennekes/tmap", "ref": "refs/heads/master", "path": "examples/tm_shape.R", "content": "current.mode <- tmap_mode(\"plot\")\n\ndata(World, metro, rivers)\n\ntm_shape(World, projection=4326) + \n    tm_polygons() + \ntm_layout(\"Long lat coordinates (WGS84)\", inner.margins=c(0,0,.1,0), title.size=.8)\n\nWorld$highlighted <- ifelse(World$iso_a3 %in% c(\"GRL\", \"AUS\"), \"gold\", \"gray75\")\ntm_shape(World, projection=3857, ylim=c(.1, 1), relative = TRUE) + \n    tm_polygons(\"highlighted\") + \ntm_layout(\"Web Mercator projection. Although widely used, it is discouraged for\nstatistical purposes. In reality, Australia is 3 times larger than Greenland!\",\n    inner.margins=c(0,0,.1,0), title.size=.6)\n\ntm_shape(World, projection=\"+proj=robin\") + \n    tm_polygons() +\ntm_layout(\n\"Winkel-Tripel projection, adapted as default by the National Geographic Society for world maps.\",\n    inner.margins=c(0,0,.1,0), title.size=.8)\n\ntm_shape(World) +\n    tm_polygons() + \ntm_layout(\"Eckhart IV projection. Recommended in statistical maps for its equal-area property.\",\n    inner.margins=c(0,0,.1,0), title.size=.8)\n\n\n# different levels of simplification\n\\dontrun{\ntm1 <- tm_shape(World, simplify = 0.05) + tm_polygons() + tm_layout(\"Simplification: 0.05\")\ntm2 <- tm_shape(World, simplify = 0.1) + tm_polygons() + tm_layout(\"Simplification: 0.1\")\ntm3 <- tm_shape(World, simplify = 0.25) + tm_polygons() + tm_layout(\"Simplification: 0.25\")\ntm4 <- tm_shape(World, simplify = 0.5) + tm_polygons() + tm_layout(\"Simplification: 0.5\")\n\nrequire(tmaptools)\ntmap_arrange(tm1, tm2, tm3, tm4)\n}\n\n# three groups of layers, each starting with tm_shape\n\\dontrun{\ntm_shape(World) +\n    tm_fill(\"darkolivegreen3\") +\ntm_shape(metro) +\n    tm_bubbles(\"pop2010\", col = \"grey30\", scale=.5) +\ntm_shape(rivers) +\n    tm_lines(\"lightcyan1\") +\ntm_layout(bg.color=\"lightcyan1\", inner.margins=c(0,0,.02,0), legend.show = FALSE)\n}\n\n# restore current mode\ntmap_mode(current.mode)\n" }
{ "repo_name": "tpoisot/esa2014twitter", "ref": "refs/heads/master", "path": "code/author_network_analysis/author_network_create.R", "content": "# The following code produces a temporal directed edgelist of ESA\n# collaborations from the 2010-2014 abstracts (from=coauthor, to=presenter).\n#\n# It assumes that you've downloaded the ESA program to a\n# subfolder (eco.confex.com) using the following shell commands:\n#\n#```\n# wget -r http://eco.confex.com/eco/2010/webprogram/\n# wget -r http://eco.confex.com/eco/2011/webprogram/\n# wget -r http://eco.confex.com/eco/2012/webprogram/\n# wget -r http://eco.confex.com/eco/2013/webprogram/\n# wget -r http://eco.confex.com/eco/2014/webprogram/\n#```\n\nlibrary(plyr)\nlibrary(XML)\nlibrary(stringi)\nlibrary(rlist)\nlibrary(pipeR)\nlibrary(data.table)\nlibrary(magrittr)\nlibrary(dplyr)\nppaths = paste0(\"eco.confex.com/eco/\", 2010:2014, \"/webprogram\")\npaper_files = list.files(ppaths, recursive=TRUE, pattern=\"Paper\\\\d+\\\\.html\",\n                         full.names=TRUE)\nnames(paper_files) = stri_replace_first_fixed(basename(paper_files), \".html\", \"\")\n\n# Make a list of all abstracts.  Elements of each are\n# -   Title\n# -   Year\n# -   Presenting Author\n# -   All other authors\n\nprocess_abstract = function(paper) {\n paper_xml = htmlTreeParse(paper, useInternalNodes = TRUE, trim=TRUE)\n title = xmlValue(paper_xml[['//div[@class=\"subtitle\"]/div[@class=\"subtext\"]']])\n if(is.na(title)) title = stri_trim_both(xmlValue(paper_xml[['//h2[@class=\"subtitle\"]/text()[last()]']]))\n year = as.integer(stri_match_first_regex(xmlValue(paper_xml[['//div[@class=\"datetime\"]']]), \"\\\\d{4}\"))\n presenter = stri_trim_both(xmlValue(paper_xml[['//div[@class=\"paperauthors\"]/div[@class=\"presenter\"]/span[@class=\"name\"]']]))\n coauthors = xpathSApply(paper_xml, '//div[@class=\"paperauthors\"]/div[@class=\"author\"]/span[@class=\"name\"]', xmlValue)\n if(!is.null(coauthors)) coauthors = stri_trim_both(coauthors)\n return(list(title=title, year=year, presenter=presenter, coauthors=coauthors))\n}\n\nabstracts = alply(paper_files, 1, function(paper) {\n  abstract = try(process_abstract(paper), silent=TRUE)\n  return(abstract)\n}, .progress = ifelse(interactive(), \"time\", \"none\"), .dims=TRUE)\n\n\n#errors = list.filter(abstracts, class(.) == \"try-error\")\n# Remove errors. These are mostly 404 errors due to broken links within the\n# ESA program that were followed by wget\nabstracts = list.filter(abstracts, class(.) != \"try-error\")\n\n\n# Some unused code for name disambiguation\n# all_names = sort(unique(list.mapv(abstracts, stri_trim_both(c(presenter, coauthors)))))\n# split_names = stri_split_regex(all_names, \"\\\\s+\")\n# split_names = list.map(split_names, .[!(. %in% c(\"III\", \"II\", \"Jr\", \"Jr.\"))])\n# last_names = list.mapv(split_names, tail(., 1))\n# first_names = list.mapv(split_names, head(., 1))\n# first_initials = stri_sub(first_names, 1,1)\n# first_last = paste(first_names, last_names)[order(last_names, first_names)]\n# all_names = all_names[order(last_names, first_names)]\n# repeats = all_names[duplicated(first_last) | duplicated(first_last, fromLast = TRUE)]\n\nauthor_temporal_edgelist = abstracts %>>%\n  list.filter(!is.null(coauthors)) %>>%\n  list.update(id     =    rep(.name, length(coauthors)),\n              presenter = rep(presenter, length(coauthors)),\n              year      = rep(year, length(coauthors)),\n              title     = NULL) %>>%\n  list.stack %>>%\n  select(2,3,1,4)\n\nnames(author_temporal_edgelist)[2] = \"coauthor\"\nsave(abstracts, author_temporal_edgelist, file=\"author_network_data.Rdata\")\n\n" }
{ "repo_name": "eph04/datasciencecoursera", "ref": "refs/heads/master", "path": "getting_data1.R", "content": "#############################################################################################\n## csv files\nif (!file.exists(\"data\")) {\n  dir.create(\"data\")\n}\nfileUrl <- \"https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD\"\n#download.file(fileUrl, destfile = \"cameras.csv\", method = \"curl\")\ndownload.file(fileUrl, destfile = \"./data/cameras.csv\", mode='wb')\ndateDownloaded <- date()\ncameraData <- read.table(\"./data/cameras.csv\", sep = \",\", header = TRUE) #cameraData <- read.csv(\"./data/cameras.csv\")\n\n#############################################################################################\n## xlsx files\nif(!file.exists(\"data\")) {\n  dir.create(\"data\")\n}\nfileUrl <- \"https://data.baltimorecity.gov/api/views/dz54-2aru/rows.xlsx?accessType=DOWNLOAD\"\n#download.file(fileUrl,destfile=\"./data/cameras.xlsx\",method=\"curl\")\ndownload.file(fileUrl,destfile=\"./data/cameras.xlsx\", mode='wb')\nlibrary(xlsx)\ncameraData <- read.xlsx(\"./data/cameras.xlsx\",sheetIndex=1,header=TRUE)\n\ncolIndex <- 2:3\nrowIndex <- 1:4\ncameraDataSubset <- read.xlsx(\"./data/cameras.xlsx\",sheetIndex=1,\n                              colIndex=colIndex,rowIndex=rowIndex)\n\n#############################################################################################\n## XML files\n\nlibrary(XML)\nfileUrl <- \"http://www.w3schools.com/xml/simple.xml\"\ndoc <- xmlTreeParse(fileUrl,useInternal=TRUE)\nrootNode <- xmlRoot(doc) # root node = complete xml\nxmlName(rootNode) # name of the root tag\n\nnames(rootNode) # nodes name under the root node\n\nrootNode[[1]] # extract first node under root node\n\nrootNode[[1]][[1]] #extract first sub node of the first sub node\nrootNode[[2]][[1]] #extract first sub node of the second sub node\nrootNode[[1]][[2]] #extract second sub node of the first sub node\n\nxmlSApply(rootNode,xmlValue) #gather all values for each subnode of the root\n\nxpathSApply(rootNode,\"//name\",xmlValue) #extract value for all names tags from root node\nxpathSApply(rootNode,\"//price\",xmlValue) #extract value for all prices tags from root node\n\nfileUrl <- \"http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens\" #no values for score\ndoc <- htmlTreeParse(fileUrl,useInternal=TRUE)\nteams <- xpathSApply(doc, \"//div[@class='game-info']\", xmlValue)\nscores <- xpathSApply(doc, \"//div[@class='score']\", xmlValue)\n\n\nfileUrl <- \"http://espn.go.com/mlb/team/_/name/bal/baltimore-orioles\" \ndoc <- htmlTreeParse(fileUrl,useInternal=TRUE)\nscores <- xpathSApply(doc, \"//div[@class='score']\", xmlValue)\nteams <- xpathSApply(doc, \"//div[@class='game-info']\", xmlValue)\nteams2 <- xpathSApply(doc, \"//div[@class='game-info']\", function(e) {s<-strsplit(xmlValue(e), \" \"); paste(s[[1]][-(1)], collapse=\" \")})\n\n#############################################################################################\n## json files\n\nlibrary(jsonlite)\njsonData <- fromJSON(\"https://api.github.com/users/jtleek/repos\")\nnames(jsonData) #name of the nodes attr\nnames(jsonData$owner) #name of the subnode attr\n\nmyjson <- toJSON(iris, pretty=TRUE) #convert a dataframe to JSON\ncat(myjson)\n\niris2 <- fromJSON(myjson)\n\n#############################################################################################\n## data.table\n\nlibrary(data.table)\nDF = data.frame(x=rnorm(9),y=rep(c(\"a\",\"b\",\"c\"),each=3),z=rnorm(9)) #create data frame\nhead(DF,3)\n\nDT = data.table(x=rnorm(9),y=rep(c(\"a\",\"b\",\"c\"),each=3),z=rnorm(9)) #create data table\nhead(DT,3)\n\ntables() #list all tables and attributes\n\nDT[2,] #get second line\nDT[DT$y==\"a\",] #get lines where y==\"a\"\nDT[c(2,3)] # get 2,3 lines\n\nDT[,.(x,y)] # get columns x&y (returns a data.table instead of a vector if I don't use \".()\")\n\n" }
{ "repo_name": "SCP-028/UGA", "ref": "refs/heads/master", "path": "archive/metastasis/cytoscape/serinesynthesis.R", "content": "library(dplyr)\nsetwd(\"~/data/serinesynthesis\")\nload(\"../Cancers_Express.RData\")\nannot <- data.frame(\n  `shared name`=c(\"3-phospho-D-glycerate (PHGDH) 3-phosphonooxypyruvate\",\n                  \"3-phosphonooxypyruvate (PSAT1) phosphoserine\",\n                  \"phosphoserine (PSPH) serine\"),\n  source=c(\"3-phospho-D-glycerate\", \"3-phosphonooxypyruvate\", \"phosphoserine\"),\n  interaction=c(\"PHGDH\", \"PSAT1\", \"PSPH\"),\n  target=c(\"3-phosphonooxypyruvate\", \"phosphoserine\", \"serine\"),\n  check.names=F  # prevent changing `shared name` to `shared.name`\n)\nresult <- vector(\"list\", length(All.DE))\nfor(i in seq_along(All.DE)) {\n  df <- as.data.frame(All.DE[[i]])\n  df <- df[grep(\"(PHGDH)|(PSAT1$)|(PSPH$)\", rownames(df)), ]\n  df$interaction <- sub(\"^.*\\\\|(.*)$\", \"\\\\1\", rownames(df))\n  df$colors <- ifelse(df$p.fdr <= 0.05, ifelse(df$fold.change >= 1, \"red\", \"blue\"), \"black\")\n  result[[i]] <- df\n}\nnames(result) <- names(All.DE)\nfor(i in seq_along(result)) {\n  df <- result[[i]]\n  df <- left_join(df, annot)\n  write.csv(df, file = paste0(names(result)[i], \".csv\"), quote = F, row.names = F)\n}\n" }
{ "repo_name": "scollinspt/teachingPT-DAGs", "ref": "refs/heads/master", "path": "TETrad/AlarmDAG.R", "content": "     Burglar  Earthquack  Alarm  JoeCalls  MaryCalls\n  1        0           0     -1         0          0\n  2        0           0     -1         0          0\n  3        1           1      0        -1         -1\n  4        0           0      1         0          0\n  5        0           0      1         0          0\n\n" }
{ "repo_name": "pdil/usmap", "ref": "refs/heads/master", "path": "tests/testthat/test-transform.R", "content": "context(\"Transforming coordinate data frames\")\n\ntest_that(\"data frame with AK and HI points is transformed\", {\n  data <- data.frame(\n    lon = c(-74.01, -95.36, -118.24, -87.65, -134.42, -157.86),\n    lat = c(40.71, 29.76, 34.05, 41.85, 58.30, 21.31)\n  )\n\n  result <- data.frame(\n    lon = c(-74.01, -95.36, -118.24, -87.65, -134.42, -157.86),\n    lat = c(40.71, 29.76, 34.05, 41.85, 58.30, 21.31),\n    lon.1 = c(2152527.4, 452368.1, -1675573.8, 1021136.9, -772150.3, -460168.7),\n    lat.1 = c(-133036.6, -1674661.0, -1033609.5, -273152.7, -2080259.3, -2054676.9)\n  )\n\n  expect_equal(usmap_transform(data), result, tolerance = 1e-05)\n})\n\ntest_that(\"data frame with AK points is transformed\", {\n  data <- data.frame(\n    lon = c(-74.01, -95.36, -118.24, -87.65, -134.42),\n    lat = c(40.71, 29.76, 34.05, 41.85, 58.30)\n  )\n\n  result <- data.frame(\n    lon = c(-74.01, -95.36, -118.24, -87.65, -134.42),\n    lat = c(40.71, 29.76, 34.05, 41.85, 58.30),\n    lon.1 = c(2152527.4, 452368.1, -1675573.8, 1021136.9, -772150.3),\n    lat.1 = c(-133036.6, -1674661.0, -1033609.5, -273152.7, -2080259.3)\n  )\n\n  expect_equal(usmap_transform(data), result, tolerance = 1e-05)\n})\n\ntest_that(\"data frame with HI points is transformed\", {\n  data <- data.frame(\n    lon = c(-74.01, -95.36, -118.24, -87.65, -157.86),\n    lat = c(40.71, 29.76, 34.05, 41.85, 21.31)\n  )\n\n  result <- data.frame(\n    lon = c(-74.01, -95.36, -118.24, -87.65, -157.86),\n    lat = c(40.71, 29.76, 34.05, 41.85, 21.31),\n    lon.1 = c(2152527.4, 452368.1, -1675573.8, 1021136.9, -460168.7),\n    lat.1 = c(-133036.6, -1674661.0, -1033609.5, -273152.7, -2054676.9)\n  )\n\n  expect_equal(usmap_transform(data), result, tolerance = 1e-05)\n})\n\ntest_that(\"data frame with no AK or HI points is transformed\", {\n  data <- data.frame(\n    lon = c(-74.01, -95.36, -118.24, -87.65),\n    lat = c(40.71, 29.76, 34.05, 41.85)\n  )\n\n  result <- data.frame(\n    lon = c(-74.01, -95.36, -118.24, -87.65),\n    lat = c(40.71, 29.76, 34.05, 41.85),\n    lon.1 = c(2152527.4, 452368.1, -1675573.8, 1021136.9),\n    lat.1 = c(-133036.6, -1674661.0, -1033609.5, -273152.7)\n  )\n\n  expect_equal(usmap_transform(data), result, tolerance = 1e-05)\n})\n\ntest_that(\"error occurs for data with less than 2 columns\", {\n  invalid_data <- data.frame(\n    lon = c(-74.01, -95.36, -118.24, -87.65)\n  )\n\n  expect_error(usmap_transform(invalid_data))\n  expect_error(usmap_transform(data.frame()))\n})\n\ntest_that(\"error occurs for data with non-numeric columns\", {\n  invalid_data1 <- data.frame(\n    lon = c(\"a\", \"b\", \"c\"),\n    lat = c(\"d\", \"e\", \"f\")\n  )\n\n  invalid_data2 <- data.frame(\n    lon = c(\"a\", \"b\", \"c\"),\n    lat = c(1, 2, 3)\n  )\n\n  invalid_data3 <- data.frame(\n    lon = c(1, 2, 3),\n    lat = c(\"d\", \"e\", \"f\")\n  )\n\n  expect_error(usmap_transform(invalid_data1))\n  expect_error(usmap_transform(invalid_data2))\n  expect_error(usmap_transform(invalid_data3))\n})\n" }
{ "repo_name": "Martin-Jung/marfunky", "ref": "refs/heads/master", "path": "R/calenderheat.R", "content": "#' Calendar Heatmap\n#' # an R version of a graphic from:                                            \n#' @source http://stat-computing.org/dataexpo/2009/posters/wicklin-allison.pdf        \n#' @import lattice\n#' @import chron\n#' @import grid\n#' @author Paul Bleicher\n#' calendarHeat: An R function to display time-series data as a calendar heatmap \n#' Copyright 2009 Humedica. All rights reserved.\n#'\n#' This program is free software; you can redistribute it and/or modify\n#' it under the terms of the GNU General Public License as published by\n#' the Free Software Foundation; either version 2 of the License, or\n#' (at your option) any later version.\n#'\n#' This program is distributed in the hope that it will be useful,\n#' but WITHOUT ANY WARRANTY; without even the implied warranty of\n#' MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#' GNU General Public License for more details.\n#'\n#' You can find a copy of the GNU General Public License, Version 2 at:\n#' http://www.gnu.org/licenses/gpl-2.0.html\n\ncalendarHeat <- function(dates, \n                         values, \n                         ncolors=99, \n                         color=\"r2g\", \n                         varname=\"Values\",\n                         date.form = \"%Y-%m-%d\", ...) {\n  require(lattice)\n  require(grid)\n  require(chron)\n  if (class(dates) == \"character\" | class(dates) == \"factor\" ) {\n    dates <- strptime(dates, date.form)\n  }\n  caldat <- data.frame(value = values, dates = dates)\n  min.date <- as.Date(paste(format(min(dates), \"%Y\"),\n                            \"-1-1\",sep = \"\"))\n  max.date <- as.Date(paste(format(max(dates), \"%Y\"),\n                            \"-12-31\", sep = \"\"))\n  dates.f <- data.frame(date.seq = seq(min.date, max.date, by=\"days\"))\n  \n  # Merge moves data by one day, avoid\n  caldat <- data.frame(date.seq = seq(min.date, max.date, by=\"days\"), value = NA)\n  dates <- as.Date(dates) \n  caldat$value[match(dates, caldat$date.seq)] <- values\n  \n  caldat$dotw <- as.numeric(format(caldat$date.seq, \"%w\"))\n  caldat$woty <- as.numeric(format(caldat$date.seq, \"%U\")) + 1\n  caldat$yr <- as.factor(format(caldat$date.seq, \"%Y\"))\n  caldat$month <- as.numeric(format(caldat$date.seq, \"%m\"))\n  yrs <- as.character(unique(caldat$yr))\n  d.loc <- as.numeric()                        \n  for (m in min(yrs):max(yrs)) {\n    d.subset <- which(caldat$yr == m)  \n    sub.seq <- seq(1,length(d.subset))\n    d.loc <- c(d.loc, sub.seq)\n  }  \n  caldat <- cbind(caldat, seq=d.loc)\n  \n  #color styles\n  r2b <- c(\"#0571B0\", \"#92C5DE\", \"#F7F7F7\", \"#F4A582\", \"#CA0020\") #red to blue                                                                               \n  r2g <- c(\"#D61818\", \"#FFAE63\", \"#FFFFBD\", \"#B5E384\")   #red to green\n  w2b <- c(\"#045A8D\", \"#2B8CBE\", \"#74A9CF\", \"#BDC9E1\", \"#F1EEF6\")   #white to blue\n  \n  assign(\"col.sty\", get(color))\n  calendar.pal <- colorRampPalette((col.sty), space = \"Lab\")\n  def.theme <- lattice.getOption(\"default.theme\")\n  cal.theme <-\n    function() {  \n      theme <-\n        list(\n          strip.background = list(col = \"transparent\"),\n          strip.border = list(col = \"transparent\"),\n          axis.line = list(col=\"transparent\"),\n          par.strip.text=list(cex=0.8))\n    }\n  lattice.options(default.theme = cal.theme)\n  yrs <- (unique(caldat$yr))\n  nyr <- length(yrs)\n  print(cal.plot <- levelplot(value~woty*dotw | yr, data=caldat,\n                              as.table=TRUE,\n                              aspect=.12,\n                              layout = c(1, nyr%%7),\n                              between = list(x=0, y=c(1,1)),\n                              strip=TRUE,\n                              main = paste(\"Calendar Heat Map of \", varname, sep = \"\"),\n                              scales = list(\n                                x = list(\n                                  at= c(seq(2.9, 52, by=4.42)),\n                                  labels = month.abb,\n                                  alternating = c(1, rep(0, (nyr-1))),\n                                  tck=0,\n                                  cex = 0.7),\n                                y=list(\n                                  at = c(0, 1, 2, 3, 4, 5, 6),\n                                  labels = c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\",\n                                             \"Friday\", \"Saturday\"),\n                                  alternating = 1,\n                                  cex = 0.6,\n                                  tck=0)),\n                              xlim =c(0.4, 54.6),\n                              ylim=c(6.6,-0.6),\n                              cuts= ncolors - 1,\n                              col.regions = (calendar.pal(ncolors)),\n                              xlab=\"\" ,\n                              ylab=\"\",\n                              colorkey= list(col = calendar.pal(ncolors), width = 0.6, height = 0.5),\n                              subscripts=TRUE\n  ) )\n  panel.locs <- trellis.currentLayout()\n  for (row in 1:nrow(panel.locs)) {\n    for (column in 1:ncol(panel.locs))  {\n      if (panel.locs[row, column] > 0)\n      {\n        trellis.focus(\"panel\", row = row, column = column,\n                      highlight = FALSE)\n        xyetc <- trellis.panelArgs()\n        subs <- caldat[xyetc$subscripts,]\n        dates.fsubs <- caldat[caldat$yr == unique(subs$yr),]\n        y.start <- dates.fsubs$dotw[1]\n        y.end   <- dates.fsubs$dotw[nrow(dates.fsubs)]\n        dates.len <- nrow(dates.fsubs)\n        adj.start <- dates.fsubs$woty[1]\n        \n        for (k in 0:6) {\n          if (k < y.start) {\n            x.start <- adj.start + 0.5\n          } else {\n            x.start <- adj.start - 0.5\n          }\n          if (k > y.end) {\n            x.finis <- dates.fsubs$woty[nrow(dates.fsubs)] - 0.5\n          } else {\n            x.finis <- dates.fsubs$woty[nrow(dates.fsubs)] + 0.5\n          }\n          grid.lines(x = c(x.start, x.finis), y = c(k -0.5, k - 0.5), \n                     default.units = \"native\", gp=gpar(col = \"grey\", lwd = 1))\n        }\n        if (adj.start <  2) {\n          grid.lines(x = c( 0.5,  0.5), y = c(6.5, y.start-0.5), \n                     default.units = \"native\", gp=gpar(col = \"grey\", lwd = 1))\n          grid.lines(x = c(1.5, 1.5), y = c(6.5, -0.5), default.units = \"native\",\n                     gp=gpar(col = \"grey\", lwd = 1))\n          grid.lines(x = c(x.finis, x.finis), \n                     y = c(dates.fsubs$dotw[dates.len] -0.5, -0.5), default.units = \"native\",\n                     gp=gpar(col = \"grey\", lwd = 1))\n          if (dates.fsubs$dotw[dates.len] != 6) {\n            grid.lines(x = c(x.finis + 1, x.finis + 1), \n                       y = c(dates.fsubs$dotw[dates.len] -0.5, -0.5), default.units = \"native\",\n                       gp=gpar(col = \"grey\", lwd = 1))\n          }\n          grid.lines(x = c(x.finis, x.finis), \n                     y = c(dates.fsubs$dotw[dates.len] -0.5, -0.5), default.units = \"native\",\n                     gp=gpar(col = \"grey\", lwd = 1))\n        }\n        for (n in 1:51) {\n          grid.lines(x = c(n + 1.5, n + 1.5), \n                     y = c(-0.5, 6.5), default.units = \"native\", gp=gpar(col = \"grey\", lwd = 1))\n        }\n        x.start <- adj.start - 0.5\n        \n        if (y.start > 0) {\n          grid.lines(x = c(x.start, x.start + 1),\n                     y = c(y.start - 0.5, y.start -  0.5), default.units = \"native\",\n                     gp=gpar(col = \"black\", lwd = 1.75))\n          grid.lines(x = c(x.start + 1, x.start + 1),\n                     y = c(y.start - 0.5 , -0.5), default.units = \"native\",\n                     gp=gpar(col = \"black\", lwd = 1.75))\n          grid.lines(x = c(x.start, x.start),\n                     y = c(y.start - 0.5, 6.5), default.units = \"native\",\n                     gp=gpar(col = \"black\", lwd = 1.75))\n          if (y.end < 6  ) {\n            grid.lines(x = c(x.start + 1, x.finis + 1),\n                       y = c(-0.5, -0.5), default.units = \"native\",\n                       gp=gpar(col = \"black\", lwd = 1.75))\n            grid.lines(x = c(x.start, x.finis),\n                       y = c(6.5, 6.5), default.units = \"native\",\n                       gp=gpar(col = \"black\", lwd = 1.75))\n          } else {\n            grid.lines(x = c(x.start + 1, x.finis),\n                       y = c(-0.5, -0.5), default.units = \"native\",\n                       gp=gpar(col = \"black\", lwd = 1.75))\n            grid.lines(x = c(x.start, x.finis),\n                       y = c(6.5, 6.5), default.units = \"native\",\n                       gp=gpar(col = \"black\", lwd = 1.75))\n          }\n        } else {\n          grid.lines(x = c(x.start, x.start),\n                     y = c( - 0.5, 6.5), default.units = \"native\",\n                     gp=gpar(col = \"black\", lwd = 1.75))\n        }\n        \n        if (y.start == 0 ) {\n          if (y.end < 6  ) {\n            grid.lines(x = c(x.start, x.finis + 1),\n                       y = c(-0.5, -0.5), default.units = \"native\",\n                       gp=gpar(col = \"black\", lwd = 1.75))\n            grid.lines(x = c(x.start, x.finis),\n                       y = c(6.5, 6.5), default.units = \"native\",\n                       gp=gpar(col = \"black\", lwd = 1.75))\n          } else {\n            grid.lines(x = c(x.start + 1, x.finis),\n                       y = c(-0.5, -0.5), default.units = \"native\",\n                       gp=gpar(col = \"black\", lwd = 1.75))\n            grid.lines(x = c(x.start, x.finis),\n                       y = c(6.5, 6.5), default.units = \"native\",\n                       gp=gpar(col = \"black\", lwd = 1.75))\n          }\n        }\n        for (j in 1:12)  {\n          last.month <- max(dates.fsubs$seq[dates.fsubs$month == j])\n          x.last.m <- dates.fsubs$woty[last.month] + 0.5\n          y.last.m <- dates.fsubs$dotw[last.month] + 0.5\n          grid.lines(x = c(x.last.m, x.last.m), y = c(-0.5, y.last.m),\n                     default.units = \"native\", gp=gpar(col = \"black\", lwd = 1.75))\n          if ((y.last.m) < 6) {\n            grid.lines(x = c(x.last.m, x.last.m - 1), y = c(y.last.m, y.last.m),\n                       default.units = \"native\", gp=gpar(col = \"black\", lwd = 1.75))\n            grid.lines(x = c(x.last.m - 1, x.last.m - 1), y = c(y.last.m, 6.5),\n                       default.units = \"native\", gp=gpar(col = \"black\", lwd = 1.75))\n          } else {\n            grid.lines(x = c(x.last.m, x.last.m), y = c(- 0.5, 6.5),\n                       default.units = \"native\", gp=gpar(col = \"black\", lwd = 1.75))\n          }\n        }\n      }\n    }\n    trellis.unfocus()\n  } \n  lattice.options(default.theme = def.theme)\n}\n\n## Example of use: Plot financial data\n## This code is not run.\nif(FALSE) {\n  \n  #create faux data; skip this to use data from a file or stock data\n  #ndays <- 1500   #set number of days\n  #dates <- as.POSIXlt(seq(Sys.Date()- ndays, Sys.Date() - 1, by=\"days\"))\n  #vals <- runif(ndays, -100, 100)\n  \n  #stock data:\n  stock <- \"GOOG\"\n  start.date <- \"2006-01-12\"\n  end.date <- Sys.Date()\n  quote <- paste(\"http://ichart.finance.yahoo.com/table.csv?s=\",\n                 stock,\n                 \"&a=\", substr(start.date,6,7),\n                 \"&b=\", substr(start.date, 9, 10),\n                 \"&c=\", substr(start.date, 1,4), \n                 \"&d=\", substr(end.date,6,7),\n                 \"&e=\", substr(end.date, 9, 10),\n                 \"&f=\", substr(end.date, 1,4),\n                 \"&g=d&ignore=.csv\", sep=\"\")             \n  stock.data <- read.csv(quote, as.is=TRUE)\n  \n  # Plot as calendar heatmap\n  calendarHeat(stock.data$Date, stock.data$Adj.Close, varname=\"MSFT Adjusted Close\")\n}" }
{ "repo_name": "EDiLD/shiny_apps", "ref": "refs/heads/master", "path": "lnorm_vs_gamma_vs_zaga/server.R", "content": "\n\nlibrary(shiny)\nlibrary(gamlss)\n\n# Define server logic required to draw a histogram\nshinyServer(function(input, output) {\n   \n  output$distPlot <- renderPlot({\n    # input to objects\n    mu <- input$mu\n    sigma <- input$sigma\n    nu <- input$nu\n    \n    # transform parameters\n    meanlog    = log(mu^2 / sqrt(sigma^2 + mu^2))\n    sdlog = sqrt(log((sigma^2 / mu^2) + 1))\n    \n    shape = 1 / sigma^2\n    scale = mu / shape\n    \n    # plot density\n    curve(dlnorm(x, meanlog = meanlog, sdlog = sdlog) , 0, 4, ylab = 'density')\n    curve(dgamma(x, scale = scale, shape = shape), 0, 4, \n          col = 'red', add = TRUE)\n    curve(dZAGA(x, mu = mu, sigma = sigma, nu = nu), 0, 4,\n          col = 'blue', add = TRUE)\n    legend('topright', legend = c('lognormal', 'Gamma', 'ZAGA'), lty = 1, col = c('black', 'red', 'blue'))\n    \n  })\n  \n})\n" }
{ "repo_name": "Regional-Fish-Modeling/Sampling-Design", "ref": "refs/heads/master", "path": "yetiSimsV2/trendModelNoCov.R", "content": "model{\r\n  \r\n  # Abundance model  \r\n  \r\n  for(i in 1:nSites){\r\n    for(j in 1:nYears){\r\n      N[i,j] ~ dpois(lambda[i,j])\r\n      log(lambda[i,j]) <- mu + trend*(j-1) +\r\n        site.ran[i] + year.ran[j] + eps[i,j]\r\n    }\r\n  }\r\n  \r\n  ## priors\r\n  mu ~ dnorm(0, 0.01)      # overall intercept\r\n  trend ~ dnorm(0, 0.01)   # linear trend\r\n  \r\n  for(i in 1:nSites){\r\n    site.ran[i] ~ dnorm(0,tau.site)     # random site effects\r\n  }\r\n  tau.site <- pow(sd.site, -2)\r\n  sd.site ~ dunif(0,2)\r\n  sd2.site <- pow(sd.site, 2)\r\n  \r\n  for (j in 1:nYears){\r\n    year.ran[j] ~ dnorm(0, tau.year)  # Random year effect\r\n  }\r\n  tau.year <- pow(sd.year, -2) \r\n  sd.year ~ dunif(0,2)\r\n  sd2.year <- pow(sd.year, 2)\r\n  \r\n  for(i in 1:nSites){\r\n    for(j in 1:nYears){\r\n      eps[i,j] ~ dnorm(0, tau)  # Over-dispersion\r\n    }\r\n  }\r\n  tau <- pow(sigma, -2)\r\n  sigma ~ dunif(0, 2)\r\n  sigma2 <- pow(sigma, 2)\r\n  \r\n  \r\n  # Detection model\r\n  \r\n  for(i in 1:nSites){\r\n    for(j in 1:nYears){\r\n      y[i,j,1] ~ dbin(p[i,j], N[i,j])\r\n      y[i,j,2] ~ dbin(p[i,j], N[i,j]-y[i,j,1])\r\n      y[i,j,3] ~ dbin(p[i,j], N[i,j]-y[i,j,1]-y[i,j,2])\r\n      \r\n      p[i,j] <- 1/(1 + exp(-lp.lim[i,j]))\r\n      lp.lim[i,j] <- min(999, max(-999, lp[i,j]))\r\n      lp[i,j] <- p.mu + p.b*sampday[i,j] + p.site.ran[i]\r\n      # removed \"prcp7day\" for its small effect\r\n    }\r\n  }\r\n  \r\n  ## priors  \r\n  p.mean ~ dunif(0.2,0.8)\r\n  p.mu <- log(p.mean/(1-p.mean))\r\n  p.b ~ dnorm(0, 0.37)\r\n  \r\n  for(i in 1:nSites){\r\n    p.site.ran[i] ~ dnorm(0,tau.p.site) \r\n  }\r\n  tau.p.site <- pow(sd.p.site, -2) \r\n  sd.p.site ~ dunif(0,1)\r\n  sd2.p.site <- pow(sd.p.site, 2)\r\n}" }
{ "repo_name": "ganna10/Meteorology_and_Ozone", "ref": "refs/heads/master", "path": "ERA_Data/TNO_Data/NOX_emissions_2011.R", "content": "setwd(\"~/Documents//Analysis//2015_Meteorology_and_Ozone//ERA_Data//TNO_Data\")\n\nall.data <- read.table(file = \"TNO_MACC_III_emissions_v1_1_2011.txt\", sep = \"\\t\", header = TRUE)\nbenelux <- all.data %>%\n  filter(ISO3 == \"BEL\" | ISO3 == \"NLD\" | ISO3 == \"LUX\")\n\nbenelux.emissions <- benelux %>%\n  select(Lon, Lat, ISO3, NMVOC, NOX)\ntbl_df(benelux.emissions)\n\nbenelux.emissions %>%\n  summarise(Sum.NMVOC.per.day = sum(NMVOC), Sum.NOx.per.day = sum(NOX)) %>%\n  mutate(Sum.NMVOC.per.day = Sum.NMVOC.per.day + 7042 + 2198 + 1462) %>%\n  mutate(NOx.VOC.Ratio = Sum.NOx.per.day / Sum.NMVOC.per.day)" }
{ "repo_name": "aocalderon/PhD", "ref": "refs/heads/master", "path": "Y2Q1/SDB/Presentation2/Figures/test2.R", "content": "size = 1000\r\nj = 0\r\ndim = 20\r\nfor(n in seq(10,100,10)){\r\n  epsilon = n / 10\r\n  r2 = (epsilon / 2)^2\r\n\r\n  x = runif(n, 0, dim)\r\n  y = runif(n, 0, dim)\r\n  \r\n  par(mar=c(0.1,0.1,0.1,0.1))\r\n  pointset=data.frame(x=x,y=y)\r\n  data = sqldf(\"SELECT p1.x AS x1, p1.y AS y1, p2.x AS x2, p2.y AS y2 FROM pointset p1 CROSS JOIN pointset p2 \")\r\n  \r\n  png(paste0(\"n/c-\",j,\".png\"),width=size,height=size)\r\n  j=j+1\r\n  par(mar=c(0.1,0.1,0.1,0.1))\r\n  plot(1, asp=1, axes = F, xlab = \"\", ylab = \"\", type='n',\r\n       xlim = c(0 - epsilon, dim + epsilon), \r\n       ylim = c(0 - epsilon, dim + epsilon))\r\n  pointset=data.frame(x=x,y=y)\r\n  data = sqldf(\"SELECT p1.x AS x1, p1.y AS y1, p2.x AS x2, p2.y AS y2 FROM pointset p1 CROSS JOIN pointset p2 \")\r\n  \r\n  for(i in 1:nrow(data)){\r\n    x1 = data[i,1]\r\n    y1 = data[i,2]\r\n    x2 = data[i,3]\r\n    y2 = data[i,4]\r\n    d = distance(x1,y1,x2,y2)\r\n    if(d <= epsilon && d != 0){\r\n      centers = calculateDisk(x1,y1,x2,y2)\r\n      draw.circle(centers[1], centers[2], epsilon/2, nv = 1000, border = 2, col = NA, lty = 2, lwd = 0.5)\r\n      draw.circle(centers[3], centers[4], epsilon/2, nv = 1000, border = 2, col = NA, lty = 2, lwd = 0.5)    \r\n    }\r\n  }\r\n  points(x, y, pch = 21, cex = 2, col = 1, bg = 1 )\r\n  box()\r\n  obj = list(n=n, e=epsilon)\r\n  text(10, -epsilon, bquote(\"n\" == .(obj$n) ~ epsilon == .(obj$e)), cex=3)\r\n  dev.off()\r\n}" }
{ "repo_name": "kien-kieu/lite", "ref": "refs/heads/master", "path": "wrap/R/RLiTe/R/lite.R", "content": "# Line Tessellation (LiTe) library\n# |||Development version\n# Authors: Katarzyna Adamczyk and Kiên Kiêu.\n# |||Copyright INRA 2006-yyyy.\n# Interdeposit Certification: IDDN.FR.001.030007.000.R.P.2015.000.31235\n# License: GPL v3.\n\nloadModule(\"lite\",TRUE)\n\nsetMethod(\"plot\",signature(x=\"Rcpp_LineTes\",y=\"missing\"),\n          function(x,lty=1,col=1,xlab=\"\",ylab=\"\",axes=F,...) {\n            seg <- x$getSegmentCoords()\n            if(nrow(seg)==0) {\n              stop(\"Cannot plot empty tesselation\")\n            }\n            x <- matrix(seg[,c(1,3)],nrow=2,byrow=TRUE)\n            y <- matrix(seg[,c(2,4)],nrow=2,byrow=TRUE)\n            matplot(x,y,type=\"l\",lty=lty,col=col,xlab=xlab,\n                    ylab=ylab,axes=axes,...)})\n\n### Not possible for Rcpp_TTessel to inherit from Rcpp_LineTes?\nsetMethod(\"plot\",signature(x=\"Rcpp_TTessel\",y=\"missing\"),\n          function(x,lty=1,col=1,xlab=\"\",ylab=\"\",axes=F,...) {\n            seg <- x$getSegmentCoords()\n            if(nrow(seg)==0) {\n              stop(\"Cannot plot empty tesselation\")\n            }\n            x <- matrix(seg[,c(1,3)],nrow=2,byrow=TRUE)\n            y <- matrix(seg[,c(2,4)],nrow=2,byrow=TRUE)\n            matplot(x,y,type=\"l\",lty=lty,col=col,xlab=xlab,\n                    ylab=ylab,axes=axes,...)})\n\ngetSMFSamplingPeriod = function(sim,mod,tes,target.renew){\n    segarray = tes$getSegmentCoords()[-(1:4),]\n    seglist=lapply(1:nrow(segarray),function(i) segarray[i,])\n    segage=rep(1,length(seglist))\n    ok <- FALSE\n    nbIter <- 1\n    while(!ok) {\n        modifs=sim$step(1)\n        nbIter <- nbIter+1\n        if (all(modifs[\"accepted\",]==0)){ # no change\n            segage = segage+1               # everybody get older\n        } else { # there is a change\n            newsegarray=tes$getSegmentCoords()[-(1:4),]\n            newseglist=lapply(1:nrow(newsegarray),function(i) newsegarray[i,])\n            if (modifs[\"accepted\",\"flip\"]==1){ # everybody get older\n                newsegage = segage+1\n            } else {\n                if (modifs[\"accepted\",\"merge\"]==1){\n                    unsupressed=seglist %in% newseglist\n                    newsegage=segage[unsupressed]+1\n                } else {\n                    newsegage=rep(1,length(newseglist))  \n                    newsegage[match(seglist,newseglist)]=\n                        newsegage[match(seglist,newseglist)]+segage\n                }\n            } \n            segarray=newsegarray\n            seglist=newseglist\n            segage=newsegage\n            renew <- sum(segage<nbIter)/length(segage)\n            if(renew>=target.renew)\n                ok <- TRUE\n        }\n    }\n    return(nbIter)\n}\n" }
{ "repo_name": "jread-usgs/mda.streams", "ref": "refs/heads/master", "path": "tests/testthat/test-stage.R", "content": "context(\"bulk staging & posting\")\n\nif(sbtools::is_logged_in()) {\n  set_scheme(\"mda_streams_dev\")\n \n  test_that(\"can stage sitelist and post sites\", {\n    # stage\n    sites <- stage_nwis_sitelist(c('doobs'), 'WI')\n    expect_equal(length(sites), 22)\n    \n    # post\n    post_site(sites, on_exists=\"clear\")\n    expect_true(all.equal(sort(sites), sort(mda.streams:::get_sites())))\n  }) \n  \n  test_that(\"can stage and post nwis data\", {\n    sites <- list_sites()\n\n    # clear any existing doobs data\n    mda.streams:::delete_ts(\"doobs_nwis\", sites)\n    \n    # stage. best to save files in their own subdir of tempdir so they're not overwritten by nwis_files_2\n    folder <- paste0(tempdir(), \"\\\\1\")\n    dir.create(folder, showWarnings=FALSE)\n    nwis_files <- stage_nwis_ts(sites, var=\"doobs\", times=c('2014-01-01', '2014-01-05'), folder=folder)\n    \n    # post\n    post_ts(nwis_files, on_exists=\"skip\")\n    \n    # repairs as needed\n    if(sum(!is.na(locate_ts(\"doobs_nwis\", sites))) != length(nwis_files)) {\n      repair_ts(\"doobs_nwis\", sites) # might need this here. leaving warnings so we know when this runs\n    }\n    \n    # check\n    expect_equal(sum(!is.na(locate_ts(\"doobs_nwis\", sites))), length(nwis_files))\n    sumry <- summarize_ts(\"doobs_nwis\", sites)\n    expect_true(all(is.na(sumry$end_date) | (\n      sumry$end_date < as.POSIXct(\"2014-01-05 00:00:00\", tz=\"UTC\") &\n        sumry$end_date > as.POSIXct(\"2014-01-01 00:00:00\", tz=\"UTC\"))))\n    \n    # stage more\n    folder <- paste0(tempdir(), \"\\\\2\")\n    dir.create(folder, showWarnings=FALSE)\n    nwis_files_2 <- stage_nwis_ts(sites, var=\"doobs\", times=c('2014-01-05', '2014-01-10'), folder=folder)\n    \n    # post with merge\n    post_ts(nwis_files_2, on_exists=\"merge\")\n    if(sum(!is.na(locate_ts(\"doobs_nwis\", sites))) != length(nwis_files)) {\n      repair_ts(\"doobs_nwis\", sites) # more often needed on the merge here. leaving warnings so we know when this runs\n    }\n    \n    # check\n    sumry_2 <- summarize_ts(\"doobs_nwis\", sites)\n    expect_true(all(is.na(sumry$num_rows) | sumry$num_rows < 50 | sumry$num_rows < sumry_2$num_rows))\n    \n  })\n  \n  set_scheme(\"mda_streams\")\n}" }
{ "repo_name": "mnr/five-minutes-of-R", "ref": "refs/heads/master", "path": "01_102_repeatloop.R", "content": "# Copyright Mark Niemann-Ross, 2019\n# Author: Mark Niemann-Ross. mark.niemannross@gmail.com\n# LinkedIn: https://www.linkedin.com/in/markniemannross/\n# Github: https://github.com/mnr\n# More Learning: http://niemannross.com/link/mnratlil\n# Description: repeat loop \n# affiliate: https://linkedin-learning.pxf.io/rweekly_repeatloop\n\n# repeat is one of many flow controls. (i.e. While, for.., if...then)\n\n# unlike other flow controls, repeat DEFAULTS to an endless loop\nbobsYourUncle <- 1 # so create a counter\n\nrepeat {\n  print(bobsYourUncle)\n  bobsYourUncle <- bobsYourUncle + 1\n  if(bobsYourUncle > 10) {break} # do this, or be here forever\n}\n\n# This is considered better programming since intent is obvious\nbobsYourUncle <- 1\nwhile (bobsYourUncle < 11) {\n  print(bobsYourUncle)\n  bobsYourUncle <- bobsYourUncle + 1\n}\n\n# so...when to use repeat\n# perhaps you're testing multiple vectors where and or or will be confusing\n\nrepeat {\n  if (rnorm(1) > 3.8) break # ... if random number greater than 3.8\n  if (as.POSIXlt(Sys.time())$hour > 15) break # ... if later than 3pm\n  # or test for hardware value (useful for IoT)\n}\n" }
{ "repo_name": "allenzhuaz/FixSeqMTP", "ref": "refs/heads/master", "path": "R/FixSeqMTP.R", "content": "#' FixSeqMTP : Tools for Fixed Sequence Multiple Testing Procedures\n#'\n#' The FixSeqMTP package provides three categories of functions for generalized/directional fixed sequence mutliple testing procedures:\n#'@section FWER controlling procedures:\n#'\n#'    \\code{\\link{FSFWER.arbidept.p.adjust}} and  \\code{\\link{FSFWER.arbidept.cv}}\n#'\n#'@section FDR controlling procedures:\n#'\n#'    \\code{\\link{FSFDR.arbidept.p.adjust}} and  \\code{\\link{FSFDR.arbidept.cv}}\n#'\n#'    \\code{\\link{FSFDR.indept.p.adjust}} and  \\code{\\link{FSFDR.indept.cv}}\n#'\n#'@section mdFWER controlling procedures:\n#'\n#'    \\code{\\link{FSmdFWER.arbidept.p.adjust}} and  \\code{\\link{FSmdFWER.arbidept.cv}}\n#'\n#'    \\code{\\link{FSmdFWER.indept.p.adjust}} and  \\code{\\link{FSmdFWER.indept.cv}}\n#'\n#'@author Yalin Zhu, Wenge Guo\n#'@references\n#'   Qiu, Z., Guo, W., & Lynch, G. (2015).\n#'   On generalized fixed sequence procedures for controlling the FWER.\n#'   \\emph{Statistics in medicine}, 34(30), 3968-3983.\n#'\n#'  Lynch, G., Guo, W., Sarkar, S. K., & Finner, H. (2016).\n#'  The Control of the False Discovery Rate in Fixed Sequence Multiple Testing.\n#'  \\emph{arXiv preprint} arXiv:1611.03146.\n#'\n#'  Grandhi, A., Guo, W., & Romano, J. P. (2016).\n#'  Control of Directional Errors in Fixed Sequence Multiple Testing.\n#'  \\emph{arXiv preprint} arXiv:1602.02345.\n#'\n#'@docType package\n#'@name FixSeqMTP\nNULL\n" }
{ "repo_name": "sammorris81/rare-binary", "ref": "refs/heads/master", "path": "markdown/dec2015/knots/dec-sim-4.R", "content": "# load packages and source files\nrm(list=ls())\noptions(warn=2)\nlibrary(fields)\nlibrary(evd)\nlibrary(spBayes)\nlibrary(fields)\nlibrary(SpatialTools)\n# library(microbenchmark)  # comment out for beowulf\nlibrary(mvtnorm)\nlibrary(Rcpp)\nlibrary(numDeriv)\nlibrary(pROC)\nSys.setenv(\"PKG_CXXFLAGS\"=\"-fopenmp\")\nSys.setenv(\"PKG_LIBS\"=\"-fopenmp\")\n\nsource(\"../../code/R/spatial_gev.R\", chdir = TRUE)\nsource(\"../../code/R/spatial_logit.R\", chdir = TRUE)\nsource(\"../../code/R/spatial_probit.R\", chdir = TRUE)\n\n# get the datasets\nload(\"./simdata.RData\")\n\n# data setting and sets to include - written by bash script\nsetMKLthreads(1)\nsetting <- 1\nsets    <- 6:10\nknot.design <- 1\n\n# extract the relevant setting from simdata\ny <- simdata[[setting]]$y\ns <- simdata[[setting]]$s\nx <- simdata[[setting]]$x\ndo.upload <- TRUE\n\n# extract info about simulation settings\nns     <- dim(y)[1]\nnt     <- 1\nnknots <- 441\n\n# testing vs training\nntrain <- 1000\n\nntest   <- ns - ntrain\nntest.1 <- floor(0.05 * ntest)\nntest.0 <- ntest - ntest.1\n\n####################################################################\n#### Start MCMC setup: Most of this is used for the spBayes package\n####################################################################\niters <- 25000; burn <- 15000; update <- 1000; thin <- 1\n# iters <- 100; burn <- 50; update <- 10; thin <- 1\nn.report     <- 10\nbatch.length <- 100\nn.batch      <- floor(iters / batch.length)\nverbose      <- TRUE\ntuning       <- list(\"phi\" = 0.1, \"sigma.sq\" = 0.2, \"beta\" = 1, \"w\" = 5)\nstarting     <- list(\"phi\" = 3/0.5, \"sigma.sq\" = 50, \"beta\" = 0, \"w\" = 0)\npriors       <- list(\"beta.norm\" = list(0, 100),\n                     \"phi.unif\" = c(0.1, 1e4), \"sigma.sq.ig\" = c(1, 1))\ncov.model <- \"exponential\"\ntimings   <- rep(NA, 3)\n# with so many knots, adaptive is time prohibitive\namcmc     <- list(\"n.batch\" = n.batch, \"batch.length\" = batch.length,\n                  \"accept.rate\" = 0.35)\n\nupload.pre <- \"samorris@hpc.stat.ncsu.edu:~/rare-binary/markdown/dec2015-knots/sim-tables\"\n\nfor (i in sets) {\n  # storage for some of the results\n  bs <- matrix(NA, 9, 2)  # place to store brier scores and auc\n  rownames(bs) <- c(\"gev-1\", \"gev-2\", \"gev-3\", \n                    \"probit-1\", \"probit-2\", \"probit-3\",\n                    \"logit-1\", \"logit-2\", \"logit-3\")\n  colnames(bs) <- c(\"bs\", \"auc\")\n  bs.gev  <- bs.pro  <- bs.log  <- rep(NA, 3)\n  auc.gev <- auc.pro <- auc.log <- rep(NA, 3)\n  roc.gev <- roc.pro <- roc.log <- vector(mode = \"list\", length = 3)\n  \n  timings <- matrix(NA, 3, 3)\n  rownames(timings) <- c(\"gev\", \"probit\", \"logit\")\n  colnames(timings) <- c(\"knots 1\", \"knots 2\", \"knots 3\")\n  \n  # start the simulation\n  set.seed(setting * 10 + i)\n  \n  # get the sites where we will be predicting\n  pred.0 <- sample(which(y[, i] == 0), size = ntest.0)\n  pred.1 <- sample(which(y[, i] == 1), size = ntest.1)\n  obs    <- rep(T, ns)\n  obs[c(pred.0, pred.1)] <- F\n  \n  y.i.o  <- matrix(y[obs, i], ntrain, 1)\n  X.o    <- matrix(x[obs], ntrain, 1)\n  s.i.o  <- s[obs, , i]\n  y.i.p  <- matrix(y[!obs, i], ntest, 1)\n  X.p    <- matrix(x[!obs], ntest, 1)\n  s.i.p  <- s[!obs, , i]\n  \n  ntrain.0 <- sum(y.i.o == 0) - ntest.0\n  ntrain.1 <- sum(y.i.o == 1) - ntest.1\n  \n  filename <- paste(\"sim-results/\", setting, \"-\", i, \"-\", knot.design, \".RData\", \n                    sep = \"\")\n  tblname  <- paste(\"sim-tables/\", setting, \"-\", i, \"-\", knot.design, \".txt\", \n                    sep =\"\")\n  \n  #### Knot setup\n  # knot design 1: 21 x 21 grid\n  # knot design 2: SRS of 441 sites\n  # knot design 3: stratified sample of 441 sites where the breakdown matches\n  #                % of sites with 1s and 0s from the training set\n  #\n  # There is a bug in cover.design when you have set nn = FALSE. So, to avoid \n  # the warning, setting number of nearest neighbors to 0 and turning off \n  # nearest neighbors. This will take a bit longer, but ultimately comes back \n  # with a similar design\n  ####\n  knots.1 <- as.matrix(expand.grid(x = seq(0, 1, length = 21), \n                                   y = seq(0, 1, length = 21)))\n  knots.2 <- cover.design(R = s.i.o, nd = nknots, nruns = 1, nn = FALSE, \n                          num.nn = 0)$design\n  \n  phat.o   <- mean(y.i.o)  # what proportion of the sites in training are 1s\n  nknots.1 <- floor(phat.o * nknots)\n  nknots.0 <- floor((1 - phat.o) * nknots)\n  knots.3.0 <- cover.design(R = s.i.o[y.i.o == 0, ], nd = nknots.0, nruns = 1, \n                            nn = FALSE, num.nn = 0)$design\n  knots.3.1 <- cover.design(R = s.i.o[y.i.o == 1, ], nd = nknots.1, nruns = 1, \n                            nn = FALSE, num.nn = 0)$design\n  knots.3 <- rbind(knots.3.0, knots.3.1)\n  \n  if (knot.design == 1) {\n    knots <- knots.1\n  } else if (knot.design == 2) {\n    knots <- knots.2\n  } else {\n    knots <- knots.3\n  }\n  \n  cat(\"Starting: Set\", i, \"\\n\")\n  \n  #### spatial GEV\n  cat(\"  Start gev \\n\")\n  \n  cat(\"    Start mcmc fit - Knots\", knot.design, \" \\n\")\n  mcmc.seed <- i * 10\n  set.seed(mcmc.seed)\n  \n  fit.gev <- spatial_GEV(y = y.i.o, s = s.i.o, x = X.o, knots = knots, \n                         beta.init = log(-log(1 - mean(y.i.o))),\n                         beta.mn = 0, beta.sd = 10,\n                         beta.eps = 0.1, beta.attempts = 50, \n                         xi.init = 0, xi.mn = 0, xi.sd = 0.5, xi.eps = 0.01, \n                         xi.attempts = 50, xi.fix = TRUE, \n                         a.init = 10, a.eps = 0.2, a.attempts = 50, \n                         a.cutoff = 0.1, b.init = 0.5, b.eps = 0.2, \n                         b.attempts = 50, alpha.init = 0.5, alpha.attempts = 50, \n                         a.alpha.joint = TRUE, alpha.eps = 0.0001,\n                         rho.init = 0.1, logrho.mn = -2, logrho.sd = 1, \n                         rho.eps = 0.1, rho.attempts = 50, threads = 1, \n                         iters = iters, burn = burn, \n                         update = update, thin = 1, thresh = 0)\n  \n  cat(\"    Start mcmc predict \\n\")\n  post.prob.gev <- pred.spgev(mcmcoutput = fit.gev, x.pred = X.p,\n                              s.pred = s.i.p, knots = knots,\n                              start = 1, end = iters - burn, update = update)\n  timings[1, knot.design] <- fit.gev$minutes\n  \n  bs.gev[knot.design] <- BrierScore(post.prob.gev, y.i.p)\n  post.prob.gev.med <- apply(post.prob.gev, 2, median)\n  roc.gev[[knot.design]] <- roc(y.i.p ~ post.prob.gev.med)\n  auc.gev[knot.design] <- roc.gev[[knot.design]]$auc\n  \n  print(bs.gev * 100)\n  \n  # copy table to tables folder on beowulf\n  bs.row <- knot.design\n  bs[bs.row, ] <- c(bs.gev[knot.design], auc.gev[knot.design])\n  write.table(bs, file = tblname)\n  if (do.upload) {\n    upload.cmd <- paste(\"scp \", tblname, \" \", upload.pre, sep = \"\")\n    system(upload.cmd)\n  }\n  \n  ###### spatial probit\n  cat(\"  Start probit \\n\")\n  \n  cat(\"    Start mcmc fit - Knots\", knot.design, \" \\n\")\n  mcmc.seed <- mcmc.seed + 1\n  set.seed(mcmc.seed)\n  fit.probit <- probit(Y = y.i.o, X = X.o, s = s.i.o, knots = knots, \n                       iters = iters, burn = burn, update = update)\n  \n  cat(\"    Start mcmc predict \\n\")\n  post.prob.pro <- pred.spprob(mcmcoutput = fit.probit, X.pred = X.p,\n                               s.pred = s.i.p, knots = knots,\n                               start = 1, end = iters - burn, update = update)\n  timings[2, knot.design] <- fit.probit$minutes\n  \n  bs.pro[knot.design] <- BrierScore(post.prob.pro, y.i.p)\n  post.prob.pro.med <- apply(post.prob.pro, 2, median)\n  roc.pro[[knot.design]] <- roc(y.i.p ~ post.prob.pro.med)\n  auc.pro[knot.design] <- roc.pro[[knot.design]]$auc\n  \n  print(bs.pro * 100)\n  \n  # copy table to tables folder on beowulf\n  bs.row <- bs.row + 3\n  bs[bs.row, ] <- c(bs.pro[knot.design], auc.pro[knot.design])\n  write.table(bs, file = tblname)\n  if (do.upload) {\n    upload.cmd <- paste(\"scp \", tblname, \" \", upload.pre, sep = \"\")\n    system(upload.cmd)\n  }\n  \n  ####### spatial logit\n  cat(\"  start logit \\n\")\n  \n  cat(\"    Start mcmc fit - Knots\", knot.design, \" \\n\")\n  mcmc.seed <- mcmc.seed + 1\n  set.seed(mcmc.seed)\n  tic       <- proc.time()[3]\n  fit.logit <- spGLM(formula = y.i.o ~ 1, family = \"binomial\",\n                     coords = s.i.o, knots = knots, starting = starting,\n                     tuning = tuning, priors = priors,\n                     cov.model = cov.model, n.samples = iters,\n                     verbose = verbose, n.report = n.report, amcmc = amcmc)\n  toc        <- proc.time()[3]\n  \n  print(\"    start mcmc predict\")\n  yp.sp.log <- spPredict(sp.obj = fit.logit, pred.coords = s.i.p,\n                         pred.covars = X.p, start = burn + 1,\n                         end = iters, thin = 1, verbose = TRUE,\n                         n.report = 500)\n  \n  post.prob.log <- t(yp.sp.log$p.y.predictive.samples)\n  \n  timings[3, knot.design] <- toc - tic\n  \n  bs.log[knot.design] <- BrierScore(post.prob.log, y.i.p)\n  post.prob.log.med <- apply(post.prob.log, 2, median)\n  roc.log[[knot.design]] <- roc(y.i.p ~ post.prob.log.med)\n  auc.log[knot.design] <- roc.log[[knot.design]]$auc\n  \n  print(bs.log * 100)\n  \n  # copy table to tables folder on beowulf\n  bs.row <- bs.row + 3\n  bs[bs.row, ] <- c(bs.log[knot.design], auc.log[knot.design])\n  write.table(bs, file = tblname)\n  if (do.upload) {\n    upload.cmd <- paste(\"scp \", tblname, \" \", upload.pre, sep = \"\")\n    system(upload.cmd)\n  }\n  \n  cat(\"Finished: Set\", i, \"\\n\")\n  save(fit.gev, bs.gev, roc.gev, auc.gev,\n       fit.probit, bs.pro, roc.pro, auc.pro,\n       fit.logit, bs.log, roc.log, auc.log,\n       y.i.p, y.i.o, knot.design, knots,  \n       s.i.o, s.i.p, timings,\n       file = filename)\n}\n" }
{ "repo_name": "SergeyMirvoda/da2016", "ref": "refs/heads/master", "path": "5.0.R", "content": "library(MASS)\r\ndata(birthwt)\r\nsummary(birthwt)\r\n\r\nhelp(birthwt)\r\n\r\ncolnames(birthwt)\r\n\r\ncolnames(birthwt) <- c(\"birthwt.below.2500\", \"mother.age\", \r\n                       \"mother.weight\", \"race\",\r\n                       \"mother.smokes\", \"previous.prem.labor\", \r\n                       \"hypertension\", \"uterine.irr\",\r\n                       \"doctor.visits\", \"birthwt.grams\")\r\n\r\nbirthwt$race <- factor(c(\"white\", \"black\", \"other\")[birthwt$race])\r\nbirthwt$mother.smokes <- factor(c(\"No\", \"Yes\")[birthwt$mother.smokes + 1])\r\nbirthwt$uterine.irr <- factor(c(\"No\", \"Yes\")[birthwt$uterine.irr + 1])\r\nbirthwt$hypertension <- factor(c(\"No\", \"Yes\")[birthwt$hypertension + 1])\r\n\r\nsummary(birthwt)\r\n\r\nplot (birthwt$race)\r\ntitle (main = \"Расовый состав рожениц в Springfield MA, 1986\")\r\n\r\nplot (birthwt$mother.age)\r\ntitle (main = \"Возраст рожениц в Springfield MA, 1986\")\r\n\r\nplot (sort(birthwt$mother.age))\r\ntitle (main = \"(Отсортировано) Возраст рожениц в Springfield MA, 1986\")\r\n  \r\nplot (birthwt$mother.age, birthwt$birthwt.grams)\r\ntitle (main = \"Вес новорождённого в разрезе возраста роженицы\")\r\n\r\nplot (birthwt$mother.smokes, birthwt$birthwt.grams, \r\n      main=\"Вес новорождённого в разрезе курительной зависимости\", \r\n      ylab = \"Вес (г)\", xlab=\"Зависимость\")\r\n\r\nt.test (birthwt$birthwt.grams[birthwt$mother.smokes == \"Yes\"], \r\n        birthwt$birthwt.grams[birthwt$mother.smokes == \"No\"])\r\n\r\nlinear.model.1 <- lm (birthwt.grams ~ mother.smokes, data=birthwt)\r\nlinear.model.1\r\nsummary(linear.model.1)\r\n\r\nlinear.model.2 <- lm (birthwt.grams ~ mother.age, data=birthwt)\r\nlinear.model.2\r\nsummary(linear.model.1)\r\n\r\nplot(linear.model.2)\r\n\r\nbirthwt.noout <- birthwt[birthwt$mother.age <= 40,]\r\nlinear.model.3 <- lm (birthwt.grams ~ mother.age, data=birthwt.noout)\r\nlinear.model.3\r\nsummary(linear.model.3)\r\n#plot(birthwt$mother.age, birthwt$birthwt.grams)\r\n#abline(linear.model.3)\r\n\r\nlinear.model.3a <- lm (birthwt.grams ~ + mother.smokes + mother.age, data=birthwt.noout)\r\nsummary(linear.model.3a)\r\n\r\nplot(linear.model.3a)\r\n\r\nlinear.model.4 <- lm (birthwt.grams ~ ., data=birthwt.noout)\r\nlinear.model.4\r\n\r\nlinear.model.4a <- lm (birthwt.grams ~ . - birthwt.below.2500, data=birthwt.noout)\r\nsummary(linear.model.4a)\r\nplot(linear.model.4a)\r\n\r\nglm.0 <- glm (birthwt.below.2500 ~ . - birthwt.grams, data=birthwt.noout)\r\nplot(glm.0)\r\n\r\nglm.1 <- glm (birthwt.below.2500 ~ . - birthwt.grams, data=birthwt.noout, family=binomial(link=logit))\r\nsummary(glm.1)\r\nplot(glm.1)\r\n\r\nodds <- seq(1, nrow(birthwt.noout), by=2)\r\nbirthwt.in <- birthwt.noout[odds,]\r\nbirthwt.out <- birthwt.noout[-odds,]\r\n\r\nlinear.model.half <- lm (birthwt.grams ~ . - birthwt.below.2500, data=birthwt.in)\r\nsummary (linear.model.half)\r\n\r\nbirthwt.predict <- predict (linear.model.half)\r\ncor (birthwt.in$birthwt.grams, birthwt.predict)\r\nplot (birthwt.in$birthwt.grams, birthwt.predict)\r\n\r\nbirthwt.predict.out <- predict (linear.model.half, birthwt.out)\r\ncor (birthwt.out$birthwt.grams, birthwt.predict.out)\r\nplot (birthwt.out$birthwt.grams, birthwt.predict.out)\r\n" }
{ "repo_name": "chavli/Coursera-ML-DataScience", "ref": "refs/heads/master", "path": "PracticalMachineLearning/Week4/quiz.R", "content": "library(dplyr)\nlibrary(caret)\nlibrary(lubridate)\n\n# --------------------------------------------------------------------------------------------------\n# Question 1\n# --------------------------------------------------------------------------------------------------\n# Set the variable y to be a factor variable in both the training and test set. Then set the seed to\n# 33833. Fit (1) a random forest predictor relating the factor variable y to the remaining variables\n# and (2) a boosted predictor using the \"gbm\" method. Fit these both with the train() command in the\n# caret package.\nlibrary(ElemStatLearn)\ndata(vowel.train)\ndata(vowel.test)\n\ntrain_data <- vowel.train %>% mutate(y=as.factor(y))\ntest_data <- vowel.test %>% mutate(y=as.factor(y))\nset.seed(33833)\n\nrf_model <- train(y ~ ., data=train_data, method=\"rf\")\ngbm_model <- train(y ~ ., data=train_data, method=\"gbm\")\n\nrf_results <- predict(rf_model, test_data)\ngbm_results <- predict(gbm_model, test_data)\n\nconfusionMatrix(rf_results, test_data$y)\nconfusionMatrix(gbm_results, test_data$y)\n\nagree_idx <- rf_results == gbm_results\nagreed_y <- rf_results[agree_idx]\nagree_acc <- sum(agreed_y == test_data$y[agree_idx]) / length(test_data$y[agree_idx])\n\nagreement <- (sum(rf_results == gbm_results & gbm_results == test_data$y) / length(test_data$y))\n\n# rf train acc: 93.6%, test 60%\n# gbm train acc: 87.9%, test 52%\n# agree idx: 62.2%\n\n# final answer\n# RF Accuracy = 0.6082\n# GBM Accuracy = 0.5152\n# Agreement Accuracy = 0.6361\n\n\n# --------------------------------------------------------------------------------------------------\n# Question 2\n# --------------------------------------------------------------------------------------------------\n# Set the seed to 62433 and predict diagnosis with all the other variables using a random forest (\"rf\")\n# , boosted trees (\"gbm\") and linear discriminant analysis (\"lda\") model. Stack the predictions\n# together using random forests (\"rf\"). What is the resulting accuracy on the test set? Is it better\n# or worse than each of the individual predictions?\nlibrary(caret)\nlibrary(gbm)\nset.seed(3433)\nlibrary(AppliedPredictiveModeling)\ndata(AlzheimerDisease)\nadData = data.frame(diagnosis,predictors)\ninTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]\ntraining = adData[ inTrain,]\ntesting = adData[-inTrain,]\n\nset.seed(62433)\nrf_model <- train(diagnosis ~ ., data = training, method=\"rf\")\ngbm_model <- train(diagnosis ~ ., data = training, method=\"gbm\")\nlda_model <- train(diagnosis ~ ., data = training, method=\"lda\")\n\nrf_train_predict <- predict(rf_model, training)\ngbm_train_predict <- predict(gbm_model, training)\nlda_train_predict <- predict(lda_model, training)\n\n# now create the stacked model\nstacked_data <- data.frame(\n    rf=rf_train_predict\n    , gbm=gbm_train_predict\n    , lda=lda_train_predict\n    , diagnosis=training$diagnosis)\nstacked_model <- train(diagnosis ~ ., data=stacked_data, method=\"rf\")\n\n\nrf_test_predict <- predict(rf_model, testing)\ngbm_test_predict <- predict(gbm_model, testing)\nlda_test_predict <- predict(lda_model, testing)\n\nstacked_test_data <- data.frame(\n    rf=rf_test_predict\n    , gbm=gbm_test_predict\n    , lda=lda_test_predict\n    , diagnosis=testing$diagnosis)\nstacked_predict <- predict(stacked_model, stacked_test_data)\n\nconfusionMatrix(rf_test_predict, testing$diagnosis)\nconfusionMatrix(gbm_test_predict, testing$diagnosis)\nconfusionMatrix(lda_test_predict, testing$diagnosis)\nconfusionMatrix(stacked_predict, testing$diagnosis)\n\n# rf test: 76.8%, gbm: 80.5%, lda: 76.8%, stacked: 79.3%\n# final answer\n# Stacked Accuracy: 0.80 is better than random forests and lda and the same as boosting.\n\n# --------------------------------------------------------------------------------------------------\n# Question 3\n# --------------------------------------------------------------------------------------------------\n# Set the seed to 233 and fit a lasso model to predict Compressive Strength. Which variable is the\n# last coefficient to be set to zero as the penalty increases? (Hint: it may be useful to look up\n# ?plot.enet).\nset.seed(3523)\nlibrary(AppliedPredictiveModeling)\ndata(concrete)\ninTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]\ntraining = concrete[ inTrain,]\ntesting = concrete[-inTrain,]\n\nset.seed(233)\nlasso_model <- train(CompressiveStrength ~. , data=training, method=\"lasso\")\n# plot which variables get 0'd the fastest as a function of lambda\nplot.enet(lasso_model$finalModel, xvar =\"penalty\")\n\n# final answer\n# Cement\n\n# --------------------------------------------------------------------------------------------------\n# Question 4\n# --------------------------------------------------------------------------------------------------\n# Fit a model using the bats() function in the forecast package to the training time series. Then\n# forecast this model for the remaining time points. For how many of the testing points is the true\n# value within the 95% prediction interval bounds?\nlibrary(forecast)\ndat = read.csv(\"data/gaData.csv\")\ntraining = dat[year(dat$date) < 2012,]\ntesting = dat[(year(dat$date)) > 2011,]\ntstrain = ts(training$visitsTumblr)\ntstest = ts(testing$visitsTumblr)\n\nmodel <- bats(y=tstrain)\nprediction <- forecast(model, h = length(tstest),level = 95)\nsum(tstest >= prediction$lower & tstest <= prediction$upper) / length(prediction$upper)\n\n# final answer\n# 96.1% of datapoints fall within the forecast\n\n# --------------------------------------------------------------------------------------------------\n# Question 5\n# --------------------------------------------------------------------------------------------------\n# Set the seed to 325 and fit a support vector machine using the e1071 package to predict Compressive\n# Strength using the default settings. Predict on the testing set. What is the RMSE?\nlibrary(e1071)\nset.seed(3523)\nlibrary(AppliedPredictiveModeling)\ndata(concrete)\ninTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]\ntraining = concrete[ inTrain,]\ntesting = concrete[-inTrain,]\n\nset.seed(325)\nsvm_model <- svm(CompressiveStrength ~ ., data=training)\ntest_predict <-predict(svm_model, testing)\n\nrmse <- sqrt(sum((testing$CompressiveStrength - test_predict)^2) / length(test_predict))\n\n# final answer\n# 6.72" }
{ "repo_name": "bedatadriven/renjin", "ref": "refs/heads/master", "path": "tests/src/test/R/test.solve.R", "content": "library(hamcrest)\n\nmt <- function(s) {\n    rows <- strsplit(s, \"\\n\")[[1]]\n    values <- as.numeric(strsplit(s, \"\\\\s+\")[[1]])\n\n    matrix(values, byrow = TRUE, nrow = length(rows))\n}\n\n\ntest.square <- function() {\n    hilbert <- function(n) { i <- 1:n; 1 / outer(i - 1, i, \"+\") }\n    h8 <- hilbert(8)\n    sh8 <- solve(h8)\n\n\n    expected <- mt(\n             \"64     -2016      20160     -92400      221760     -288288      192192     -51480\n           -2016     84672    -952560    4656960   -11642400    15567552   -10594584    2882880\n           20160   -952560   11430720  -58212000   149688000  -204324119   141261119  -38918880\n          -92400   4656960  -58212000  304919999  -800414996  1109908794  -776936155  216215998\n          221760 -11642400  149688000 -800414996  2134439987 -2996753738  2118916783 -594593995\n         -288288  15567552 -204324119 1109908793 -2996753738  4249941661 -3030050996  856215352\n          192192 -10594584  141261119 -776936154  2118916782 -3030050996  2175421226 -618377753\n          -51480   2882880  -38918880  216215998  -594593995   856215351  -618377753  176679358\")\n\n\n    assertThat(solve(h8), identicalTo(expected, tol = 1))\n}\n\ntest.simple <- function() {\n\n    a <- mt(\"1  1  1\n             0  2  5\n             2  5 -1\")\n\n    b <- c(6, -4, 27)\n\n\n    assertThat(solve(a, b), identicalTo(c(5, 3, -2)))\n}\n\ntest.names <- function() {\n   a <- mt(\"1  1  1\n            0  2  5\n            2  5 -1\")\n\n   dimnames(a) <- list(c(\"x\", \"y\", \"z\"), c(\"a\", \"b\", \"c\"))\n\n   b <- c(6, -4, 27)\n\n   assertThat(solve(a, b), identicalTo(c(a = 5, b = 3, c = -2)))\n\n}\n\ntest.dimnames <- function() {\n   a <- mt(\"1  1  1\n            0  2  5\n            2  5 -1\")\n\n   dimnames(a) <- list(c(\"x\", \"y\", \"z\"), c(\"a\", \"b\", \"c\"))\n\n   b <- matrix(c(6, -4, 27), ncol = 1)\n\n   assertThat(solve(a, b), identicalTo(structure(c(5, 3, -2), .Dim = c(3L, 1L), .Dimnames = list(c(\"a\",  \"b\", \"c\"), NULL))))\n\n}" }
{ "repo_name": "edzer/mss", "ref": "refs/heads/master", "path": "demo/reproduce.R", "content": "# script to reproduce Fig 1 in:\n#\n# Stasch, C., S. Scheider, E. Pebesma, W. Kuhn, 2014.\n# Meaningful Spatial Prediction and Aggregation.\n# Environmental Modelling and Software, 51, 149--165 (open access)\n\npdf(\"intro.pdf\")\noptions(warn=2)            # promote warnings to errors\nlibrary(classInt)          # used for creating class intervals for plotting the maps\nlibrary(sp)\nlibrary(spacetime)\nlibrary(gstat)\nlibrary(cshapes)           # used for loading the country border of Germany\n# install the mss package from github:\n\npar(mfrow=c(2,3), mar=c(0.1,.2, 2,.2))\n\n# CO2 emissions\n\n# retrieving germany for background and aggregation\ncountries.data <- cshp(date=as.Date(\"2008-06-30\")) #retrieve latest borders from 30/6/2008\ngermany <- countries.data[countries.data$CNTRY_NAME==\"Germany\",]\n\n# read the co2 Emissions\nco2all <- read.csv(\"co2_emission_powerplants.csv\", header = TRUE)\n\n# filter power plants without locations (=zero latitudes) and with carbon dioxide emissions in 2008\nco2cleaned <- co2all[co2all$latitude != 0 & co2all$carbon_2007 != 0,\n\t\tc(\"plant_id\", \"name\", \"latitude\", \"longitude\", \"carbon_2007\")]\n\n# convert to spatial points dataframe\ncoords <- cbind(co2cleaned$longitude,co2cleaned$latitude)\nco2sp <- SpatialPointsDataFrame(coords,co2cleaned, proj4string = CRS(proj4string(germany)))\n\n# prepare plot\n#pal <- grey.colors(4, 0.95, 0.55, 2.2)\npal = bpy.colors()\nq5 <- classIntervals(log(co2sp$carbon_2007), n = 5, style = \"quantile\")\nq5Colours <- findColours(q5, pal)\n\n# plot of locations with emissions\nplot(germany)\nplot(co2sp,col = q5Colours,pch=19,add=TRUE)\ntitle(expression(\"CO\"[2]*\" emissions of power plants\"))\nbox(col='grey')\n\n# compute and plot aggregate (sum) of CO2 emissions\nplot(germany)\nco2agg <- aggregate(co2sp[\"carbon_2007\"], germany, sum)\nplot(co2agg,add=TRUE)\ninvisible(text(coordinates(co2agg), labels=as.character(co2agg$carbon_2007), cex=1.3))\ntitle(expression(\"Sum of CO\"[2]*\" emissions\"))\nbox(col='grey')\n\ngrd = spsample(germany, 10000, \"regular\", offset = c(0,0))\ngridded(grd) = TRUE\n\nco2_interpolated <- krige(log(carbon_2007)~1, co2sp, grd)\n#pal_int <- grey.colors(40, 0.95, 0.05, 2.2)\npal_int = bpy.colors()\nimage(co2_interpolated, col = pal_int)\nplot(germany, add = TRUE)\ntitle(expression(\"Interpolated CO\"[2]*\" emissions\"))\nbox(col='grey')\n\n# PM10 \n# read Airbase PM10 data\n# since R 3.1, coordinates will be read as factor, because they otherwise loose precision;\n# hence, we need to specify colClasses\ncc = c(\"factor\", \"Date\", \"Date\", \"factor\", \"factor\", \"numeric\", \"numeric\", \"numeric\")\npm10.tab <- read.table(\"EU_meas_2005_june.dat\", header = TRUE, colClasses = cc)\npm10all = STIDF(SpatialPoints(pm10.tab[c(\"x\", \"y\")]), as.POSIXct(pm10.tab$time), pm10.tab)\npm10all = as(pm10all, \"STSDF\")\npm10sel <- pm10all[,\"2005-06-01\"] #select only one day\nproj4string(pm10sel) <- proj4string(germany) #set crs of pm10 values\npm10germany <- pm10sel[germany, \"PM10\"]\npm10germany <- pm10germany[!is.na(pm10germany$PM10),] #remove NA values \n\n# plot PM10 stations\nq5 <- classIntervals(pm10germany$PM10, n = 5, style = \"quantile\")\nq5Colours <- findColours(q5, pal)\nplot(germany)\nplot(pm10germany,col = q5Colours,pch=19,add=TRUE)\ntitle(expression(\"PM\"[10]*\" measurements\"))\nbox(col='grey')\n\n# compute and plot sum of pm10 emissions\nplot(germany)\nproj4string(pm10germany)<-proj4string(germany)\npm10agg <- aggregate(pm10germany,germany,sum)\nplot(pm10agg,add=TRUE)\ninvisible(text(coordinates(pm10agg), labels=as.character(pm10agg$PM10), cex=1.3))\ntitle(expression(\"Sum of PM\"[10]*\" measurements\"))\nbox(col='grey')\n\nproj4string(pm10germany) <- proj4string(grd)\npm10_interpolated <- krige(PM10~1, pm10germany, grd)\nimage(pm10_interpolated,col=pal_int)\nplot(germany, add=TRUE)\ntitle(expression(\"Interpolated PM\"[10]*\" measurements\"))\nbox(col='grey')\n\noptions(warn=1) # print warnings where they occur\n\n#library(spatstat) # point patterns -- overwrites idw!\n#library(maptools) # convert sp <--> spatstat classes\n\n\n##############################\n### SECTION: demonstration scripts\n##############################\nco2sp = co2sp[\"carbon_2007\"]\n\nlibrary(mss)\nco2 = as(co2sp, \"PointPatternDataFrame\")\nco2_int <- krige(log(carbon_2007)~1, co2, grd)\n\n# Aggregation of a marked point pattern:\nco2.mean = aggregate(co2, germany, mean)\nco2.sum  = aggregate(co2, germany, sum)\n\n# Aggregation of a geostatistical variable:\npm10 = as(pm10germany, \"GeostatisticalDataFrame\")\npm10.mean = aggregate(pm10, germany, mean)\npm10.sum  = aggregate(pm10, germany, sum)\n" }
{ "repo_name": "Lu-He/CodeLand", "ref": "refs/heads/master", "path": "research/ChinaGovernmentSubsidies.R", "content": "###################################\n# Help Function                   #\n###################################\n\ncalcAnnualRet <- function(x){\n  object <- exp(mean(log(1 + x), na.rm = TRUE) * 12) - 1\n  object <- paste0(format(round(object, 3) * 100, nsmall = 1), \"%\")\n}\n\nregHelp <- function(regData, formula) {\n  reg <- summary(lm(eval(formula), data = regData))\n  report <- as.data.table(reg$coefficients, keep.rownames = TRUE)\n  report[, adjRsq := reg$adj.r.squared]\n  return(report)\n}\n\ngetPerfTable <- function(targetUniv, univName){\n  portVW <- CWPortfolio$new(constituentUniverse = wholeUniv[fyear <= endFyear],\n                            targetUniverse = targetUniv,\n                            financials = fin,\n                            name = \"VW Return\",\n                            rebalanceMonth = rebMonth,\n                            allowNegativeWeights = FALSE,\n                            suspendedTrading = FALSE)$calculateWeights()$calculateReturns()\n  \n  portEW <- EWPortfolio$new(constituentUniverse = wholeUniv[fyear <= endFyear],\n                            targetUniverse = targetUniv,\n                            financials = fin,\n                            name = \"EW Return\",\n                            rebalanceMonth = rebMonth,\n                            allowNegativeWeights = FALSE,\n                            suspendedTrading = FALSE)$calculateWeights()$calculateReturns()\n  \n  # monthly returns\n  monthlyRet <- merge(portVW$returns[, .(Date, VW = TR)], portEW$returns[, .(Date, EW = TR)], by = \"Date\", all = TRUE)\n  # return table\n  retTable <- monthlyRet[, lapply(.SD, calcAnnualRet), .SDcols = -\"Date\"]\n  tStat <- monthlyRet[, lapply(.SD, function(x){round(t.test(x)$statistic, 2)}), .SDcols = -\"Date\"]\n  retTable <- as.data.table(rbind(retTable, tStat))\n  retTable[, \":=\" (Name = univName, Statistics = c(\"Ann.Ret\", \"t-Stat\"))]\n  retTable <- select(retTable, Name, Statistics, everything())\n  # regression\n  regTable <- merge(monthlyRet, factorRet, by = \"Date\")\n  regVW <- regHelp(regTable, \"VW ~ MKT + HML + SMB\")\n  regEW <- regHelp(regTable, \"EW ~ MKT + HML + SMB\")\n  \n  regTable <- cbind(regVW[, .(Var = rn, EstimateVW = Estimate, tStatVW = get(\"t value\"))],\n                    regEW[, .(EstimateEW = Estimate, tStatEW = get(\"t value\"))])\n  \n  regTable[, Name := univName]\n  regTable <- select(regTable, Name, everything())\n  regTable[Var == \"(Intercept)\", Var := \"Alpha\"]\n  \n  return(list(retTable, monthlyRet, regTable))\n  \n}\n\n###################################\n# Parameters                      #\n###################################\nregion <- \"China A\"\nexchange <- \"China A\"\ncurrency <- \"USD\"\nstartFyear <- 2007\nendFyear <- 2016\nexcludeFinance <- FALSE\nrebMonth <- 7\n\n###################################\n# Input                           #\n###################################\nmulAssign[wholeUniv, fin, rf, ] <- getInput(inputDir, region = region, exchange = exchange, currency = currency, research = TRUE)\n# modify fyear for future target universe classfication and portfolio creation\nwholeUniv[, fyear := ifelse(month(Date) >= 7, year(Date)-1, year(Date)-2)]\n\nwholeUniv <- wholeUniv[!is.na(lagME) & !is.na(id)]\nfin <- merge(fin, unique(wholeUniv[,.(id,fyear)]), by = c(\"id\",\"fyear\"))\n\nfinCut <- fin[fyear >= startFyear & fyear <= endFyear, .(id, fyear, region, industry, SOE, earnings, govSub)]\nfinCut <- finCut[!is.na(earnings)]\nfinCut[is.na(govSub), govSub := 0] \nfinCut[, earningsExtSub := earnings - govSub]\nsetorder(finCut, id, fyear)\nfinCut[, SOE := na.locf(na.locf(SOE, na.rm = FALSE), na.rm = FALSE, fromLast = TRUE), by = id]\n\nif(excludeFinance == TRUE){\n  finCut <- finCut[industry != \"Finance\"]\n}\n\n\n# csi central SOEcomposite index\ncsiCentralList <- fread(paste0(inputDir, \"CSISoeIndexStkList.csv\"))\ncsiCentralList[, \":=\" (centralSOE = 1, id = sprintf(\"%06d\", id))]\ncsiCentralListCut <- csiCentralList[month == 12][, fyear := year]\n\n\n# csi local SOE composite index\ncsiLocalList <- fread(paste0(inputDir, \"CSILocalStockList.csv\"))\ncsiLocalList[, \":=\" (localSOE = 1, id = sprintf(\"%06d\", id))]\ncsiLocalListCut <- csiLocalList[month == 12][, fyear := year]\n\n\nfinCut <- merge(finCut, csiCentralListCut[, .(fyear, id, centralSOE)], by = c(\"id\", \"fyear\"), all.x = TRUE)\nfinCut <- merge(finCut, csiLocalListCut[, .(fyear, id, localSOE)], by = c(\"id\", \"fyear\"), all.x = TRUE)\nfinCut[, subSOE := ifelse(centralSOE == 1, 1, NA)]\nfinCut[, subSOE := ifelse(localSOE == 1 & is.na(centralSOE), 2, subSOE)]\nfinCut[, subSOE := ifelse(is.na(subSOE), 0, subSOE)]\n\n# factor returns\nfactorRet <- fread(paste0(inputDir, \"FFVW\", region, currency, \".csv\"), drop = 1)[, Date := as.Date(Date)]\n\n###################################\n# Analytics                       #\n###################################\n# ratio of profits\n# ratio by year for all firms \nratioByYear <- finCut[, .(govSubTotal = sum(govSub), earningsTotal = sum(earnings)), by = .(fyear)] \nratioByYear[,  govSubRatio := govSubTotal/earningsTotal]\n\n# ratio by industries\nratioByInd <- finCut[, .(govSubTotal = sum(govSub), earningsTotal = sum(earnings)), by = .(fyear, industry)]\nratioByInd[, govSubRatio := govSubTotal/earningsTotal]\nratioByIndTable <- as.data.table(dcast(ratioByInd, fyear ~ industry, value.var = \"govSubRatio\"))\nnames(ratioByInd) <- c(\"Fyear\", \"Industry\", \"Government Sub Total\", \"Earnings Total\", \"Proportion\")\n\n# plot ratio by industry\npicByFyear <- ggplot(data = ratioByInd, aes(x = Industry, y = Proportion, fill = Industry)) + \n  geom_bar(stat = \"summary\", fun.y = sum) + \n  facet_wrap(~Fyear) + \n  coord_cartesian(ylim = c(-0.5, 0.5)) + \n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())\n\n\npicByInd <- ggplot(data = ratioByInd, aes(x = Fyear, y = Proportion, fill = Fyear)) + \n  geom_bar(stat = \"summary\", fun.y = sum) + \n  facet_wrap(~Industry) + \n  coord_cartesian(ylim = c(-0.5, 0.5)) + \n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())\n\n# ratio by SOE\nratioBySOE <- finCut[, .(govSubTotal = sum(govSub), earningsTotal = sum(earnings)), by = .(fyear, SOE)]\nratioBySOE[, govSubRatio := govSubTotal/earningsTotal]\nratioBySOETable <- as.data.table(dcast(ratioBySOE, fyear ~ SOE, value.var = \"govSubRatio\"))\nnames(ratioBySOETable) <- c(\"Fiscal Year\", \"Non-SOE\", \"SOE\")\n\n# ratio by central SOE and local SOE\nratioBySubSOE <- finCut[fyear >= 2010, .(govSubTotal = sum(govSub), earningsTotal = sum(earnings)), by = .(fyear, subSOE)]\nratioBySubSOE[, govSubRatio := govSubTotal/earningsTotal]\nratioBySubSOETable <- as.data.table(dcast(ratioBySubSOE, fyear ~ subSOE, value.var = \"govSubRatio\")) \nnames(ratioBySubSOETable) <- c(\"Fiscal Year\", \"Non-SOE\", \"Central SOE\", \"Local SOE\")\n\n# profit due to subsidies\n# by year for all firms\nfinCutChange <- finCut[earningsExtSub <= 0 & earnings > 0 ]\nchangeByYear <- merge(finCutChange[, .(changeNum = .N), by = fyear], \n                      finCut[, .(totalNum = .N), by = fyear], by = c(\"fyear\"), all = TRUE)\nchangeByYear[, changeRatio := changeNum/totalNum]\n\n# by industry\nchangeByInd <- merge(finCutChange[, .(changeNum = .N), by = .(fyear, industry)], \n                     finCut[, .(totalNum = .N), by = .(fyear, industry)], by = c(\"fyear\", \"industry\"), all = TRUE)\nchangeByInd[is.na(changeNum), changeNum := 0]\nchangeByInd[, changeRatio := changeNum/totalNum]\nchangeByIndTable <- as.data.table(dcast(changeByInd, fyear ~ industry, value.var = \"changeRatio\"))\nnames(changeByInd) <- c(\"Fyear\", \"Industry\", \"# of Firms Changed\", \"# of Firms Total\", \"Proportion\")\n\n# plot by industry\nchangeByFyear <- ggplot(data = changeByInd, aes(x = Industry, y = Proportion, fill = Industry)) + \n  geom_bar(stat = \"summary\", fun.y = sum) + \n  facet_wrap(~Fyear) + \n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())\n\n\nchangeByInd <- ggplot(data = changeByInd, aes(x = Fyear, y = Proportion, fill = Fyear)) + \n  geom_bar(stat = \"summary\", fun.y = sum) + \n  facet_wrap(~Industry) + \n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())\n\n\n# by SOE\nchangeBySOE <- merge(finCutChange[, .(changeNum = .N), by = .(fyear, SOE)], \n                     finCut[, .(totalNum = .N), by = .(fyear, SOE)], by = c(\"fyear\", \"SOE\"), all = TRUE)\nchangeBySOE[, changeRatio := changeNum/totalNum]\nchangeBySOETable <- as.data.table(dcast(changeBySOE, fyear ~ SOE, value.var = \"changeRatio\"))\nnames(changeBySOETable) <- c(\"Fiscal Year\", \"Non-SOE\", \"SOE\")                                   \n\n# by nonSOE\nchangeBySubSOE <- merge(finCutChange[fyear >= 2010, .(changeNum = .N), by = .(fyear, subSOE)], \n                        finCut[fyear >= 2010, .(totalNum = .N), by = .(fyear, subSOE)], by = c(\"fyear\", \"subSOE\"), all = TRUE)\nchangeBySubSOE[, changeRatio := changeNum/totalNum]\nchangeBySubSOETable <- as.data.table(dcast(changeBySubSOE, fyear ~ subSOE, value.var = \"changeRatio\"))\nnames(changeBySubSOETable) <- c(\"Fiscal Year\", \"Non-SOE\", \"Central SOE\", \"Local SOE\")\n\n\n###################################\n# Create Portfolio                #\n###################################\nwholeUnivCut <- merge(wholeUniv[, .(id, Date, fyear)], finCut, by = c(\"id\", \"fyear\"))\n# profitable due to subsidies\ntargetUnivProSub <- wholeUnivCut[earningsExtSub < 0 & earnings > 0][, .(id, Date)]\n# all the others\ntargetUnivOthers <- wholeUnivCut[!(earningsExtSub < 0 & earnings > 0)][, .(id, Date)]\n# profitable ones in all the others\ntargetUnivOthersPro <- wholeUnivCut[!(earningsExtSub < 0 & earnings > 0) & earnings > 0][, .(id, Date)]\n# not profitable ones in all the others\ntargetUnivOthersNegPro <- wholeUnivCut[!(earningsExtSub < 0 & earnings > 0) & earnings < 0][, .(id, Date)]\n# not profitable even after susidies\ntargetUnivNegProSub <- wholeUnivCut[earningsExtSub < 0 & earnings < 0 & govSub > 0][, .(id, Date)]\n\n\n# calculate performance for each target universe\ntargetUnivList <- list(\"Profitable Due to Subsidies\" = targetUnivProSub, \n                       \"Nonprofitable Even after Subsidies\" = targetUnivNegProSub, \n                       \"The Others\" = targetUnivOthers, \n                       \"Profitable in The Others\" = targetUnivOthersPro, \n                       \"Nonprofitable in The Others\" = targetUnivOthersNegPro)\n\nmonthlyRetList <- retTableList <- regTableList <- list()\nfor(i in 1:length(targetUnivList)){\n  \n  mulAssign[retTable, monthlyRet, regTable] <- getPerfTable(targetUnivList[[i]], names(targetUnivList)[i])\n  \n  monthlyRetList[[i]] <- monthlyRet\n  retTableList[[i]] <- retTable\n  regTableList[[i]] <- regTable\n}\n\nnames(monthlyRetList) <- names(targetUnivList)\nretTableCom <- Reduce(rbind, retTableList)\nregTableCom <- Reduce(rbind, regTableList)\n###################################\n# Output                          #\n###################################\ngenerateQuantActiveExcelReport(paste0(\"Output/ChinaGovSubsidies\", excludeFinance, \".xlsx\"), region = region,\n                                      analytics = list(\"Proportion\" = ratioByYear,\n                                                       \"Proportion by Industry\" = ratioByIndTable,\n                                                       \"ProIndPic\" = picByInd,\n                                                       \"ProYearPic\" = picByFyear,\n                                                       \"Proportion by SOE\" = list(\"SOE\" = ratioBySOETable, \"Central SOE/Local SOE\" = ratioBySubSOETable),\n                                                       \"Status Change\" = changeByYear,\n                                                       \"Status Change by Industry\" = changeByIndTable,\n                                                       \"ChangeIndPic\" = changeByInd,\n                                                       \"ChangeYearPic\" = changeByFyear,\n                                                       \"Status Change by SOE\" = list(\"SOE\" = changeBySOETable, \"Central SOE/Local SOE\" = changeBySubSOETable)))\n\ngenerateQuantActiveExcelReport(paste0(\"Output/ChinaGovSubsidiesPort\", excludeFinance, \".xlsx\"), region = region,\n                                      analytics = list(\"Performance\" = retTableCom,\n                                                       \"Reg Coefs\" = regTableCom,\n                                                       \"Monthly Ret\" = monthlyRetList))\n" }
{ "repo_name": "Jean-Romain/lidR", "ref": "refs/heads/master", "path": "R/algorithm-snag.R", "content": "# ===============================================================================\n#\n# PROGRAMMERS:\n#\n# andrew.sanchezmeador@nau.edu - https://github.com/bi0m3trics\n# jean-romain.roussel.1@ulaval.ca  -  https://github.com/Jean-Romain/lidR\n#\n# COPYRIGHT:\n#\n# Copyright 2017-2018 Jean-Romain Roussel.\n#\n# This file is part of lidR R package.\n#\n# lidR is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>\n#\n# ===============================================================================\n\n#' Snags Segmentation Algorithm\n#'\n#' This function is made to be used in \\link{segment_snags}. It implements an algorithms for snags segmentation\n#' based on Wing et al (2015) (see references). This is an automated filtering algorithm that utilizes\n#' three dimensional neighborhood lidar point-based intensity and density statistics to remove lidar\n#' points associated with live trees and retain lidar points associated with snags.\n#'\n#' Note that this algorithm strictly performs a classification based on user input while\n#' the original publication's methods also included a segmentation step and some pre-\n#' (filtering for first and single returns only) and post-process (filtering for only the\n#' snag classified points prior to segmentation) tasks which are now expected to be performed\n#' by the user. Also, this implementation may have some differences compared with the original\n#' method due to potential mis-interpretation of the Wing et al. manuscript, specifically\n#' Table 2 where they present four groups of conditional assessments with their required\n#' neighborhood point density and average BBPR values (BBPR = branch and bole point ratio;\n#' PDR = point density requirement).\\cr\\cr\n#' This algorithm attributes each point in the point cloud (\\code{snagCls} column) into the\n#' following five snag classes:\n#' \\itemize{\n#' \\item 0: live tree - not a snag\\cr\n#' \\item 1: general snag - the broadest range of snag point situations\\cr\n#' \\item 2: small snag - isolated snags with lower point densities\\cr\n#' \\item 3: live crown edge snag - snags located directly adjacent or intermixing with live trees crowns \\cr\n#' \\item 4: high canopy cover snag - snags protruding above the live canopy in dense conditions (e.g.,\n#' canopy cover >= 55\\%).\n#' }\n#'\n#' @param neigh_radii numeric. A vector of three radii used in quantifying local-area centered\n#' neighborhoods. See Wing et al. (2015) reference page 171 and Figure 4. Defaults are 1.5,\n#' 1, and 2 for the sphere, small cylinder and large cylinder neighborhoods, respectively.\n#'\n#' @param low_int_thrsh numeric. The lower intensity threshold filtering value. See Wing\n#' et al. (2015) page 171. Default is 50.\n#'\n#' @param uppr_int_thrsh numeric. The upper intensity threshold filtering value. See Wing\n#' et al. (2015) page 171. Default is 170.\n#'\n#' @param pt_den_req numeric. Point density requirement based on plot-level point density\n#' defined classes. See Wing et al. (2015) page 172. Default is 3.\n#'\n#' @param BBPRthrsh_mat matrix. A 3x4 matrix providing the four average BBPR (branch and bole\n#' point ratio) values for each of the three neighborhoods (sphere, small cylinder and large\n#' cylinder) to be used for conditional assessments and classification into the following four snag\n#' classes: 1) general snag 2) small snag 3) live crown edge snag 4) high canopy\n#' cover snag. See Wing et al. (2015) page 172 and Table 2. This matrix must be provided by\n#' the user.\n#'\n#' @examples\n#' LASfile <- system.file(\"extdata\", \"MixedConifer.laz\", package=\"lidR\")\n#' # Wing also included -keep_single\n#' poi =\"-keep_first -inside 481260 3812920 481310 3812960\"\n#' las <- readLAS(LASfile, select = \"xyzi\", filter = poi)\n#'\n#' # For the Wing2015 method, supply a matrix of snag BranchBolePtRatio conditional\n#' # assessment thresholds (see Wing et al. 2015, Table 2, pg. 172)\n#' bbpr_thresholds <- matrix(c(0.80, 0.80, 0.70,\n#'                           0.85, 0.85, 0.60,\n#'                           0.80, 0.80, 0.60,\n#'                           0.90, 0.90, 0.55),\n#'                           nrow =3, ncol = 4)\n#'\n#' # Run snag classification and assign classes to each point\n#' las <- segment_snags(las, wing2015(neigh_radii = c(1.5, 1, 2), BBPRthrsh_mat = bbpr_thresholds))\n#'\n#' # Plot it all, tree and snag points...\n#' #plot(las, color=\"snagCls\", colorPalette = rainbow(5))\n#'\n#' # Filter and plot snag points only\n#' snags <- filter_poi(las, snagCls > 0)\n#' #plot(snags, color=\"snagCls\", colorPalette = rainbow(5)[-1])\n#'\n#' # Wing et al's (2015) methods ended with performing tree segmentation on the\n#' # classified and filtered point cloud using the watershed method\n#'\n#' @author\n#' Implementation by Andrew Sánchez Meador & Jean-Romain Roussel\n#'\n#' @references\n#' Wing, Brian M.; Ritchie, Martin W.; Boston, Kevin; Cohen, Warren B.; Olsen, Michael J. 2015.\n#' Individual snag detection using neighborhood attribute filtered airborne lidar data. Remote\n#' Sensing of Environment. 163: 165-179 https://doi.org/10.1016/j.rse.2015.03.013\n#'\n#' @export\n#'\n#' @family snags segmentation algorithms\nwing2015 = function(neigh_radii = c(1.5,1,2), low_int_thrsh = 50, uppr_int_thrsh = 170, pt_den_req = 3, BBPRthrsh_mat = NULL)\n{\n  assert_is_numeric(neigh_radii)\n  assert_all_are_in_closed_range(neigh_radii, 0, 10)\n  assert_is_a_number(low_int_thrsh)\n  assert_all_are_in_closed_range(low_int_thrsh, 0, 255)\n  assert_is_a_number(uppr_int_thrsh)\n  assert_all_are_in_closed_range(uppr_int_thrsh, 0, 255)\n  assert_all_are_true(low_int_thrsh < uppr_int_thrsh)\n  assert_is_a_number(pt_den_req)\n  assert_all_are_in_open_range(pt_den_req, 0, 100)\n\n  if (is.null(BBPRthrsh_mat))\n    stop(\"Branch and bole point ratio thresholds matrix not supplied.\")\n\n  f = function(las)\n  {\n    assert_is_valid_context(LIDRCONTEXTSNG, \"wing2015\")\n    return(C_Wing2015(las, neigh_radii, low_int_thrsh, uppr_int_thrsh, pt_den_req, BBPRthrsh_mat, getThread()))\n  }\n\n  class(f) <- c(LIDRALGORITHMSNG, LIDRALGORITHMOPENMP)\n\n  return(f)\n}\n" }
{ "repo_name": "NovaInstitute/Rpackages", "ref": "refs/heads/master", "path": "novaUtils/R/fixname.R", "content": "#' Make variable names lower case\r\n#' \r\n#' Receives variable names and converts them to lower case, without any spaces \r\n#' or punctuation marks - whether within the name, leading or trailing. \r\n#' The user may specify the replacement character to use as replacement for the\r\n#' punctuation marks and spaces (defaults to underscore ['_']).\r\n#' \r\n#' @param data_names Names to be converted (function converts data into an array).\r\n#' @param replacementChar The character that will be replacing either a \r\n#' punctuation mark, space or underscore. By default this function replaces \r\n#' spaces and punctuation marks with underscores.\r\n#' @return Simplified variable names\r\n#' @export\r\nfixname <- function(data_names, \r\n                    replacementChar = c(\"_\", \".\", \" \")[1]) {\r\n\r\n  if (!(replacementChar %in% c(\"_\", \".\", \" \"))) {\r\n    stop(\"Invalid replacement character specified. Must be either '_', '.' or ' '.\")\r\n  }\r\n  \r\n  data_names <- tolower(data_names)\r\n  \r\n  data_names <- sapply(X = as.array(data_names), FUN = function(nm) {\r\n    nm <- gsub(pattern = \"[[:punct:]]\", replacement = replacementChar, x = nm)\r\n    nm <- gsub(pattern = \"[[:space:]]\", replacement = replacementChar, x = nm)\r\n    nm <- gsub(pattern = \"\\\\.\", replacement = replacementChar, x = nm, fixed = TRUE)\r\n    nm <- gsub(pattern = \"_{2,}\", replacement = replacementChar, x = nm)\r\n    nm <- gsub(pattern = \"^_\", replacement = \"\", x = nm)\r\n    nm <- gsub(pattern = \"_$\", replacement = \"\", x = nm)\r\n    nm <- gsub(pattern = \"\\\\.{2,}\", replacement = replacementChar, x = nm)\r\n    nm <- gsub(pattern = \"^\\\\.\", replacement = \"\", x = nm)\r\n    nm <- gsub(pattern = \"\\\\.$\", replacement = \"\", x = nm)    \r\n    return(nm)\r\n  })\r\n  names(data_names) <- NULL\r\n  \r\n  data_names <- gsub(pattern = \"ï_submission_id\", \r\n                     replacement = \"submission_id\", \r\n                     x = data_names, \r\n                     fixed = TRUE)\r\n  \r\n  return(data_names)\r\n}" }
{ "repo_name": "rlowrance/th", "ref": "refs/heads/master", "path": "Table8Vertical.R", "content": "Table8Vertical <- function(lines) {\n    # return function object $Header()  $Detail() $Get()\n    header.format <- '%15s %8s %8s %8s %8s'\n    data.format   <- '%15s %8.0f %8.0f %8.0f %8.0f'\n\n    Header <- function(col1, col2, col3, col4, col5) {\n        line <- sprintf(header.format, col1, col2, col3, col4, col5)\n        lines$Append(line)\n    }\n\n    Detail <- function(ndays, value1, value2, value3, value4) {\n        line <- sprintf(data.format, ndays, value1, value2, value3, value4)\n        lines$Append(line)\n    }\n\n    Get <- function() {\n        lines$Get()\n    }\n\n    list( Header  = Header\n         ,Detail  = Detail\n         ,Get     = Get\n         )\n}\n" }
{ "repo_name": "LTLA/diffHic", "ref": "refs/heads/master", "path": "inst/tests/test-cluster.R", "content": "####################################################################################################\n# This tests the clusterPairs function.\n\nsuppressWarnings(suppressPackageStartupMessages(require(diffHic)))\n\n####################################################################################################\n\nsimgen <- function(alln, chromos, width, min.space, max.space) {\n\t# Randomly sampling chromosomes to generate both a pair matrix and the chromosome space.\n\toutput <- GRanges()\n\tfor (x in names(chromos)) {\n\t\tn <- chromos[[x]]\n\t\tgaps <- round(runif(n, min.space, max.space))\n\t\tstarts <- cumsum(gaps)\n\t\tends <- starts + width\n\t\tsuppressWarnings(output <- c(output, GRanges(x, IRanges(starts, ends))))\n\t}\n\t\n\t# Randomly sampling pairs.\n   \ttotal <- length(output)\n   \tchosen1 <- round(runif(alln, 1, total))\n   \tchosen2 <- round(runif(alln, 1, total))\n   \tchosen.a <- pmax(chosen1, chosen2)\n   \tchosen.t <- pmin(chosen1, chosen2)\n\n\t# Enforcing uniqueness.\n   \to <- order(chosen.a, chosen.t)\n   \tchosen.a <- chosen.a[o]\n   \tchosen.t <- chosen.t[o]\n   \tis.diff <- c(TRUE, diff(chosen.a)!=0 | diff(chosen.t)!=0)\n\treturn(InteractionSet(list(counts=matrix(0L, nrow=alln, ncol=1)), \n        GInteractions(anchor1=chosen.a, anchor2=chosen.t, region=output, mode=\"reverse\"),\n        colData=DataFrame(totals=100)))\n}\n\ncrisscross <- function(id1, id2) {\n\tout <- split(id1, id2)\n\tif (!all(sapply(out, FUN=function(x) { length(unique(x))==1L }))) {\n\t\treturn(FALSE)\n\t}\n\tout <- split(id2, id1)\n\tif (!all(sapply(out, FUN=function(x) { length(unique(x))==1L }))) {\n\t\treturn(FALSE)\n\t}\n\treturn(TRUE)\n}\n\nclustercomp <- function(data, tol, maxw, split=FALSE, data2=NULL) {\n\tif (!is.null(data2)) { \n\t\toriginal <- data\n        data <- rbind(original, data2)\n\t}\n\n\t# Simulating cluster formation first, by expanding each region and checking for overlaps.\n\t# We use a simple quadratic-time algorithm; slow, but gets the job done.\n    gi <- interactions(data)\n\tregion <- regions(data)\n\tnp <- nrow(data)\n\texpanded <- resize(region, fix=\"center\", width(region)+tol*2)\n\tallap <- findOverlaps(expanded, region)\n#\tnested <- findOverlaps(region, region, type=\"within\")\n\timpossible <- np+1L\n\tmyids <- rep(impossible, np)\n\tlast.id <- 1L\n\n\tfor (x in 1:np) {\n\t\tcura <- gi@anchor1[x]\n\t\tkeep.a <- subjectHits(allap)[queryHits(allap)==cura]\n\t\tcurt <- gi@anchor2[x]\n\t\tkeep.t <- subjectHits(allap)[queryHits(allap)==curt]\n\n#\t\thas.nested.a<- subjectHits(nested)[queryHits(nested)==cura & subjectHits(nested)!=cura] \n#\t\thas.nested.t <- subjectHits(nested)[queryHits(nested)==curt & subjectHits(nested)!=curt] \n#\t\tanchor1.nested <- gi@anchor1 %in% has.nested.a\n#\t\tanchor2.nested <- gi@anchor2 %in% has.nested.t\n#\t\tif (any(anchor1.nested & gi@anchor2 %in% keep.t)) { cat(\"Hooray, nested anchor!\\n\") }\n#\t\tif (any(gi@anchor1 %in% keep.a & anchor2.nested)) { cat(\"Hooray, nested target!\\n\") }\n#\t\tif (any(anchor1.nested & anchor2.nested)) { cat(\"Hooray, nested both!\\n\") }\n\n\t\tpartners <- which(gi@anchor1 %in% keep.a & gi@anchor2 %in% keep.t)\n\t\tpartners <- partners[partners>=x]\n\t\tcurids <- myids[partners]\n\t\tcurids <- curids[curids!=impossible]\n\t\t\n\t\tchosen <- unique(curids)\n\t\tif (length(chosen)>1L) { \n\t\t\tchosen <- min(curids)\n\t\t\tmyids[myids%in%curids]<-chosen\n\t\t} else if (!length(curids)) {\n\t\t\tchosen <- last.id\n\t\t\tlast.id <- last.id + 1L\n\t\t} \n\t\tmyids[partners]\t<- chosen\n\t}\n\n\tif (!is.null(data2)) { \n\t\tcomp <- clusterPairs(original, data2, tol=tol, upper=NULL)$indices\n\t\tcomp <- unlist(comp)\n\t} else {\n\t\tcomp <- clusterPairs(data, tol=tol, upper=NULL)$indices[[1]]\n\t} \n\tif (!crisscross(comp, myids)) { stop(\"mismatches in cluster IDs without bin size restriction\") }\n\n# Some error checking functions; just define 'current' as the pairs of interest.\n# anchor2 <- which(sapply(split(comp, myids), FUN=function(x) { length(unique(x))!=1 }))\n# current <- pairs[myid==anchor2[1],]\n# plot(0,0,xlim=c(1075, 1358),ylim=c(1163,1475))\n# rect(start(region)[current$t], start(region)[current$t], end(region)[current$a], end(region)[current$t], col=rgb(1,0,0,0.5))\n\n\t# Now, splitting each cluster to keep them under maxw.\n\tclusters <- split(1:np, comp)\n\tall.starts <- start(region)\n\tall.ends <- end(region)+1L\n\tlast <- 0\n\tmyid2 <- myids\n\tfor (x in names(clusters)) {\n\t\tactive <- clusters[[x]]\n\t\tactive.a <- gi@anchor1[active]\n\t\tactive.t <- gi@anchor2[active]\n\n\t\tcluster.as <- min(all.starts[active.a])\n\t\tcluster.ae <- max(all.ends[active.a])\n\t\tcluster.ts <- min(all.starts[active.t])\n\t\tcluster.te <- max(all.ends[active.t])\n\n\t\tdiff.a <- cluster.ae - cluster.as \n\t\tmult.a <- max(1, round(diff.a / maxw))\n\t\tjump.a <- diff.a/mult.a\n\n\t\tdiff.t <- cluster.te - cluster.ts \n\t\tmult.t <- max(1, round(diff.t / maxw))\n\t\tjump.t <- diff.t/mult.t\n\n\t\tmid.a <- (all.starts[active.a]+all.ends[active.a]) * 0.5\n\t\tax <- floor((mid.a - cluster.as)/jump.a)\n\t\tmid.t <- (all.starts[active.t]+all.ends[active.t]) * 0.5\n\t\ttx <- floor((mid.t - cluster.ts)/jump.t)\n\n\t\tmyid2[active] <- ax * mult.t + tx + last\n\t\tlast <- last + mult.t*mult.a\n\t}\n\n\t# Comparing it to the actual clustering.\n\tif (!is.null(data2)) { \n\t\tcomp2 <- clusterPairs(original, data2, tol=tol, upper=maxw)\n\t\tid.comp <- unlist(comp2$indices)\n\t} else {\n\t\tcomp2 <- clusterPairs(data, tol=tol, upper=maxw)\n\t\tid.comp <- comp2$indices[[1]]\n\t} \n\tif (!crisscross(id.comp, myid2)) { stop(\"mismatches in cluster IDs when a maximum bin size is applied\") }\n\n    # Checking it's the same with index.only=TRUE.\n\tif (!is.null(data2)) { \n\t\ticomp <- clusterPairs(original, data2, tol=tol, upper=maxw, index.only=TRUE)\n\t} else {\n\t\ticomp <- clusterPairs(data, tol=tol, upper=maxw, index.only=TRUE)\n\t} \n\tif (!identical(icomp, comp2$indices)) { stop(\"different behaviour with index.only=TRUE\") }\n\n\t# Checking the bounding boxes.\n    ix <- as.character(seq_len(max(id.comp)))\n\ta1range <- split(anchors(data, type=\"first\"), id.comp)\n\ta1range <- range(a1range[ix])\n    a2range <- split(anchors(data, type=\"second\"), id.comp)\n\ta2range <- range(a2range[ix])\n\tnames(a1range) <- names(a2range) <- NULL\n\tif (!identical(anchors(comp2$interactions, type=\"first\"), unlist(a1range)) || \n        !identical(anchors(comp2$interactions, type=\"second\"), unlist(a2range))) {\n\t\tstop(\"mismatches in anchor1/anchor2 bounding box coordinates\")\n\t}\n\n\tif (split) { \n\t\treturn(summary(tabulate(id.comp)))\n\t} else {\n\t\treturn(summary(tabulate(comp)))\n\t}\n}\n\n####################################################################################################\n\nset.seed(3413094)\n\nchromos <- c(chrA=10, chrB=20, chrC=40)\ndata <- simgen(100, chromos, 20, 50, 100)\nclustercomp(data, tol=70, maxw=200)\nclustercomp(data, tol=100, maxw=200)\nclustercomp(data, tol=200, maxw=200)\n\ndata <- simgen(100, chromos, 10, 50, 100)\nclustercomp(data, tol=70, maxw=500)\nclustercomp(data, tol=100, maxw=500)\nclustercomp(data, tol=200, maxw=500)\n\ndata <- simgen(500, chromos, 20, 50, 100)\nclustercomp(data, tol=70, maxw=200)\nclustercomp(data, tol=100, maxw=200)\nclustercomp(data, tol=200, maxw=200)\n\ndata <- simgen(500, chromos, 10, 50, 100)\nclustercomp(data, tol=70, maxw=500)\nclustercomp(data, tol=100, maxw=500)\nclustercomp(data, tol=200, maxw=500)\n\ndata <- simgen(1000, chromos, 20, 50, 100)\nclustercomp(data, tol=70, maxw=200)\nclustercomp(data, tol=100, maxw=200)\nclustercomp(data, tol=200, maxw=200)\n\ndata <- simgen(1000, chromos, 10, 50, 100)\nclustercomp(data, tol=70, maxw=500)\nclustercomp(data, tol=100, maxw=500)\nclustercomp(data, tol=200, maxw=500)\n\n# And again, with flipped settings.\t\ndata <- simgen(100, chromos, 50, 10, 20)\nclustercomp(data, tol=0, maxw=200)\nclustercomp(data, tol=20, maxw=200)\nclustercomp(data, tol=50, maxw=200)\n\ndata <- simgen(100, chromos, 100, 10, 20)\nclustercomp(data, tol=0, maxw=500)\nclustercomp(data, tol=20, maxw=500)\nclustercomp(data, tol=50, maxw=500)\n\ndata <- simgen(500, chromos, 50, 10, 20)\nclustercomp(data, tol=0, maxw=100, split=TRUE)\nclustercomp(data, tol=0, maxw=200, split=TRUE)\nclustercomp(data, tol=0, maxw=500, split=TRUE)\n\ndata <- simgen(500, chromos, 100, 10, 20)\nclustercomp(data, tol=0, maxw=100, split=TRUE)\nclustercomp(data, tol=0, maxw=200, split=TRUE)\nclustercomp(data, tol=0, maxw=500, split=TRUE)\n\ndata <- simgen(1000, chromos, 50, 10, 20)\nclustercomp(data, tol=0, maxw=100, split=TRUE)\nclustercomp(data, tol=0, maxw=200, split=TRUE)\nclustercomp(data, tol=0, maxw=500, split=TRUE)\n\ndata <- simgen(1000, chromos, 100, 10, 20)\nclustercomp(data, tol=0, maxw=100, split=TRUE)\nclustercomp(data, tol=0, maxw=200, split=TRUE)\nclustercomp(data, tol=0, maxw=500, split=TRUE)\n\n####################################################################################################\n# Clustering with multiple entries.\n\nset.seed(34133424)\n\ndata <- simgen(100, chromos, 20, 50, 100)\ndata2 <- simgen(100, chromos, 10, 20, 50)\nclustercomp(data, tol=70, maxw=200, data2=data2)\nclustercomp(data, tol=100, maxw=200, data2=data2)\nclustercomp(data, tol=200, maxw=200, data2=data2)\n\ndata <- simgen(100, chromos, 50, 50, 100)\ndata2 <- simgen(100, chromos, 10, 20, 50)\nclustercomp(data, tol=70, maxw=500, data2=data2)\nclustercomp(data, tol=100, maxw=500, data2=data2)\nclustercomp(data, tol=200, maxw=500, data2=data2)\n\ndata <- simgen(500, chromos, 20, 50, 100)\ndata2 <- simgen(500, chromos, 10, 20, 50)\nclustercomp(data, tol=70, maxw=200, data2=data2)\nclustercomp(data, tol=100, maxw=200, data2=data2)\nclustercomp(data, tol=200, maxw=200, data2=data2)\n\ndata <- simgen(500, chromos, 50, 50, 100)\ndata2 <- simgen(500, chromos, 10, 20, 50)\nclustercomp(data, tol=70, maxw=500, data2=data2)\nclustercomp(data, tol=100, maxw=500, data2=data2)\nclustercomp(data, tol=200, maxw=500, data2=data2)\n\ndata <- simgen(1000, chromos, 50, 50, 100)\ndata2 <- simgen(1000, chromos, 10, 20, 50)\nclustercomp(data, tol=70, maxw=200, data2=data2)\nclustercomp(data, tol=100, maxw=200, data2=data2)\nclustercomp(data, tol=200, maxw=200, data2=data2)\n\ndata <- simgen(1000, chromos, 20, 50, 100)\ndata2 <- simgen(1000, chromos, 10, 20, 50)\nclustercomp(data, tol=70, maxw=500, data2=data2)\nclustercomp(data, tol=100, maxw=500, data2=data2)\nclustercomp(data, tol=200, maxw=500, data2=data2)\n\n####################################################################################################\n# Specific clustering tests involving nested things.\n\nrequire(diffHic)\ngetLandscape <- function(astarts, aends, tstarts, tends) { \n\tastarts <- as.integer(astarts)\n\taends <- as.integer(aends)\n\ttstarts <- as.integer(tstarts)\n\ttends <- as.integer(tends)\n\to <- order(astarts, tstarts)\n\t.Call(diffHic:::cxx_cluster_2d, astarts[o], tstarts[o], aends[o], tends[o], tol=1L, TRUE)\n\tinvisible(NULL)\n}\n\nparent.a <- c(10, 20)\nparent.t <- c(10, 20)\n\n# Wholly nested.\n\nnest.a <- c(12, 15)\nnest.t <- c(12, 15)\ngetLandscape(c(parent.a[1], nest.a[1]),\tc(parent.a[2], nest.a[2]), \n\tc(parent.t[1], nest.t[1]), c(parent.t[2], nest.t[2]))\n\n# Nested target, joined anchor\n\nnest.a <- c(18, 21)\nnest.t <- c(12, 15)\ngetLandscape(c(parent.a[1], nest.a[1]),\tc(parent.a[2], nest.a[2]), \n\tc(parent.t[1], nest.t[1]), c(parent.t[2], nest.t[2]))\n\nnest.t <- c(10, 12)\ngetLandscape(c(parent.a[1], nest.a[1]),\tc(parent.a[2], nest.a[2]), \n\tc(parent.t[1], nest.t[1]), c(parent.t[2], nest.t[2]))\n\nnest.t <- c(15, 20)\ngetLandscape(c(parent.a[1], nest.a[1]),\tc(parent.a[2], nest.a[2]), \n\tc(parent.t[1], nest.t[1]), c(parent.t[2], nest.t[2]))\n\n# Nested anchor, joined target.\n\t\nnest.t <- c(18, 21)\nnest.a <- c(12, 15)\ngetLandscape(c(parent.a[1], nest.a[1]),\tc(parent.a[2], nest.a[2]), \n\tc(parent.t[1], nest.t[1]), c(parent.t[2], nest.t[2]))\n\nnest.a <- c(10, 12)\ngetLandscape(c(parent.a[1], nest.a[1]),\tc(parent.a[2], nest.a[2]), \n\tc(parent.t[1], nest.t[1]), c(parent.t[2], nest.t[2]))\n\nnest.a <- c(15, 20)\ngetLandscape(c(parent.a[1], nest.a[1]),\tc(parent.a[2], nest.a[2]), \n\tc(parent.t[1], nest.t[1]), c(parent.t[2], nest.t[2]))\n\n# Nested anchor, extended target.\n\nnest.a <- c(12, 15)\nnest.t <- c(5, 25)\ngetLandscape(c(parent.a[1], nest.a[1]),\tc(parent.a[2], nest.a[2]), \n\tc(parent.t[1], nest.t[1]), c(parent.t[2], nest.t[2]))\n\n# Nested target, extended anchor.\n\nnest.a <- c(5, 25)\nnest.t <- c(12, 15)\ngetLandscape(c(parent.a[1], nest.a[1]),\tc(parent.a[2], nest.a[2]), \n\tc(parent.t[1], nest.t[1]), c(parent.t[2], nest.t[2]))\n\n# Double nested. \n\nparent.a2 <- c(10, 20)\nparent.t2 <- c(30, 40)\n\nnest.a <- c(12, 15)\nnest.t <- c(5, 45)\ngetLandscape(c(parent.a[1], parent.a2[1], nest.a[1]), c(parent.a[2], parent.a2[2], nest.a[2]), \n\tc(parent.t[1], parent.t2[1], nest.t[1]), c(parent.t[2], parent.t2[2], nest.t[2]))\n\nnest.t <- c(15, 35)\ngetLandscape(c(parent.a[1], parent.a2[1], nest.a[1]), c(parent.a[2], parent.a2[2], nest.a[2]), \n\tc(parent.t[1], parent.t2[1], nest.t[1]), c(parent.t[2], parent.t2[2], nest.t[2]))\n\n####################################################################################################\n# End.\n" }
{ "repo_name": "IALSA/ialsa-2015-portland-stencil", "ref": "refs/heads/master", "path": "scripts/common-functions.R", "content": "\n# ---- lookup-names-labels -------------------------------------------------\n# Create function that inspects names and labels\nnames_labels <- function(ds){\n  nl <- data.frame(matrix(NA, nrow=ncol(ds), ncol=2))\n  names(nl) <- c(\"name\",\"label\")\n  for (i in seq_along(names(ds))){\n    # i = 2\n    nl[i,\"name\"] <- attr(ds[i], \"names\")\n    if(is.null(attr(ds[[i]], \"label\")) ){\n      nl[i,\"label\"] <- NA}else{\n        nl[i,\"label\"] <- attr(ds[,i], \"label\")\n      }\n  }\n  return(nl)\n}\n# names_labels(ds=oneFile)\n\n# ---- basic-histogram-discrete -----------------------------------\n# function to create discrete histogram. taken from RAnalysisSkeleton\nhistogram_discrete <- function(\n  d_observed,\n  variable_name,\n  levels_to_exclude   = character(0),\n  main_title          = variable_name,\n  x_title             = NULL,\n  y_title             = \"Number of Included Records\",\n  text_size_percentage= 6,\n  bin_width           = 1L) {\n\n  d_observed <- as.data.frame(d_observed) #Hack so dplyr datasets don't mess up things\n  if( !base::is.factor(d_observed[, variable_name]) )\n    d_observed[, variable_name] <- base::factor(d_observed[, variable_name])\n\n  d_observed$iv <- base::ordered(d_observed[, variable_name], levels=rev(levels(d_observed[, variable_name])))\n\n  ds_count <- plyr::count(d_observed, vars=c(\"iv\"))\n  # if( base::length(levels_to_exclude)>0 ) { }\n  ds_count <- ds_count[!(ds_count$iv %in% levels_to_exclude), ]\n\n  ds_summary <- plyr::ddply(ds_count, .variables=NULL, transform, count=freq, proportion = freq/sum(freq) )\n  ds_summary$percentage <- base::paste0(base::round(ds_summary$proportion*100), \"%\")\n\n  y_title <- base::paste0(y_title, \" (n=\", scales::comma(base::sum(ds_summary$freq)), \")\")\n\n  g <- ggplot(ds_summary, aes_string(x=\"iv\", y=\"count\", fill=\"iv\", label=\"percentage\")) +\n    geom_bar(stat=\"identity\") +\n    geom_text(stat=\"identity\", size=text_size_percentage, hjust=.8) +\n    scale_y_continuous(labels=scales::comma_format()) +\n    labs(title=main_title, x=x_title, y=y_title) +\n    coord_flip()\n\n  theme  <- theme_light(base_size=14) +\n    theme(legend.position = \"none\") +\n    theme(panel.grid.major.y=element_blank(), panel.grid.minor.y=element_blank()) +\n    theme(axis.text.x=element_text(colour=\"gray40\")) +\n    theme(axis.title.x=element_text(colour=\"gray40\")) +\n    theme(axis.text.y=element_text(size=14)) +\n    theme(panel.border = element_rect(colour=\"gray80\")) +\n    theme(axis.ticks.length = grid::unit(0, \"cm\"))\n\n  return( g + theme )\n}\n\n# ---- basic-histogram-continous -----------------------------------\n# function to create continuous histogram. taken from RAnalysisSkeleton\nhistogram_continuous <- function(\n  d_observed,\n  variable_name,\n  bin_width      = NULL,\n  main_title     = variable_name,\n  x_title        = paste0(variable_name, \" (each bin is \", scales::comma(bin_width), \" units wide)\"),\n  y_title        = \"Frequency\",\n  rounded_digits = 0L\n) {\n\n  d_observed <- as.data.frame(d_observed) #Hack so dplyr datasets don't mess up things\n  d_observed <- d_observed[!base::is.na(d_observed[, variable_name]), ]\n\n  ds_mid_points <- base::data.frame(label=c(\"italic(X)[50]\", \"bar(italic(X))\"), stringsAsFactors=FALSE)\n  ds_mid_points$value <- c(stats::median(d_observed[, variable_name]), base::mean(d_observed[, variable_name]))\n  ds_mid_points$value_rounded <- base::round(ds_mid_points$value, rounded_digits)\n\n  g <- ggplot(d_observed, aes_string(x=variable_name)) +\n    geom_histogram(binwidth=bin_width, fill=\"gray70\", color=\"gray90\", position=position_identity()) +\n    geom_vline(xintercept=ds_mid_points$value, color=\"gray30\") +\n    geom_text(data=ds_mid_points, aes_string(x=\"value\", y=0, label=\"value_rounded\"), color=\"tomato\", hjust=c(1, 0), vjust=.5) +\n    scale_x_continuous(labels=scales::comma_format()) +\n    scale_y_continuous(labels=scales::comma_format()) +\n    labs(title=main_title, x=x_title, y=y_title) +\n    theme_light() +\n    theme(axis.ticks.length = grid::unit(0, \"cm\"))\n\n  ds_mid_points$top <- stats::quantile(ggplot2::ggplot_build(g)$panel$ranges[[1]]$y.range, .8)\n  g <- g + ggplot2::geom_text(data=ds_mid_points, ggplot2::aes_string(x=\"value\", y=\"top\", label=\"label\"), color=\"tomato\", hjust=c(1, 0), parse=TRUE)\n  return( g )\n}\n\n# ----- basic-line-graph ------------------------------------------------\nbasic_line <- function(\n  d_observed,\n  variable_name,\n  time_metric,\n  color_name=\"black\",\n  line_alpha=1,\n  line_size =.5,\n  smoothed = FALSE,\n  main_title     = variable_name,\n  x_title        = paste0(\"Time metric: \", time_metric),\n  y_title        = variable_name,\n  rounded_digits = 0L\n) {\n\n  d_observed <- as.data.frame(d_observed) #Hack so dplyr datasets don't mess up things\n  d_observed <- d_observed[!base::is.na(d_observed[, variable_name]), ]\n\n  g <- ggplot(d_observed, aes_string(x=time_metric, y = variable_name))\n  if(!smoothed){\n    g <- g + geom_line(aes_string(group=\"id\"), size=line_size, color=scales::alpha(color_name,line_alpha), na.rm=T)\n  } else{\n    g <- g + geom_smooth(aes_string(group=\"id\"),size=line_size,  method=\"lm\",color=scales::alpha(color_name,line_alpha), na.rm=T, se=F )\n    g <- g + geom_smooth(method=\"loess\", color=\"blue\", size=1, fill=\"gray80\", alpha=.3, na.rm=T)\n\n  }\n\n  g <- g + scale_x_continuous(labels=scales::comma_format()) +\n    scale_y_continuous(labels=scales::comma_format()) +\n    # labs(title=main_title, x=x_title, y=y_title) +\n    theme_light() +\n    theme(axis.ticks.length = grid::unit(0, \"cm\"))\n  return( g )\n}\n\n# g <- basic_line(d, \"cogn_global\", \"fu_year\", \"salmon\", .9, .1, T)\n# g\n" }
{ "repo_name": "XanderHorn/visualizeR", "ref": "refs/heads/master", "path": "R/visualizeR_function.R", "content": "\r\n#' visualizeR - Automated exploratory data analysis for classification problems\r\n#'\r\n#' visualizeR automates exploratory data analysis for classification problems in machine learning. The problem can be two-class or multi-class classification. It is recommended that all ID and Date features be removed before running this algorithm, cleaning the data before running this is also recommended. visualizeR has some data cleaning aspects built into it but cannot account for domain knowledge cleaning.\r\n#' @param df A data.frame object containing plotting features and target/outcome feature. Cannot be left blank.\r\n#' @param Outcome The feature name of the outcome as character format, e.g. 'Target'. Cannot be left blank.\r\n#' @param nrBins The number of bins to use in histogram plots of numerical features should 'stackedHist' be used as the chart type in the parameter 'NumChartType'.\r\n#' @param sample Should a random sample be taken in order to speed the plotting process up.\r\n#' @param clipOutliers Should outliers be fixed in the data using a median approach. Possible values: TRUE,FALSE\r\n#' @param handleMissing Should missing values be corrected with 'Missing' value for categorical variables and median imputation for conitnuous variables. Possible values: TRUE,FALSE. Should this be left as 'N' then missing observations will be removed from the plots.\r\n#' @param CatChartType Indicates the type of chart to use when plotting categorical/factor features. Possible values: 'stackedHist', 'Confusion'\r\n#' @param NumChartType Indicates the type of chart to use when plotting numerical/continuous features. Possible values: 'stackedHist', 'densityLine', 'densityFill', 'boxPlot'\r\n#' @param summaryStats Should summary statistics be printed for predictors in the dataset, summary stats for continuous and frequency tables for categorical variables. Possible values: TRUE,FALSE\r\n#' @param seed Used only for the sampling of the data and to reproduce the plots.\r\n#' @param maxLevels The maximum levels allowed for factor features, if a feature has levels more than the threshold it will not be plotted.\r\n#' @param nrUniques The number of allowed unique values for a feature before it is automatically changed to a categorical feature. If a feature has less than this threshold, the feature will be changed to a categorical feature.\r\n#' @param ouputPath A file path where the plots should be saved in a PDF document. If left blank all plots will be displayed in R.\r\n#' @param outputFileName The name of the file containing all the plots.\r\n#' @keywords visualizeR\r\n#' @export\r\n#' @examples\r\n#' EXAMPLE 1:\r\n#' library(datasets)\r\n#' train <- data.frame(iris)\r\n#'visualizeR(df = train,\r\n#'           Outcome = 'Species',\r\n#'           nrBins = 30,\r\n#'           sample = 1,\r\n#'           clipOutliers = 'Y',\r\n#'           CatChartType = 'stackedHist',\r\n#'           NumChartType = 'boxPlot')\r\n#'           \r\n#'EXAMPLE 2:\r\n#'visualizeR(df = train,\r\n#'Outcome = 'Species',\r\n#'nrBins = 30,\r\n#'sample = 1,\r\n#'clipOutliers = 'Y',\r\n#'CatChartType = 'Confusion',\r\n#'NumChartType = 'stackedHist',\r\n#'summaryStats = 'Y',\r\n#'outputPath = 'C:/Users/User/Documents',\r\n#'outputFileName = 'IrisExploratoryDataAnalysis')\r\n#' @author \r\n#' Xander Horn\r\n\r\n\r\nvisualizeR <- function(df,\r\n                       Outcome,\r\n                       nrBins = 30,\r\n                       sample = 0.3,\r\n                       clipOutliers = TRUE,\r\n                       handleMissing = TRUE,\r\n                       CatChartType = 'stackedHist',\r\n                       NumChartType = 'boxPlot',\r\n                       summaryStats = FALSE,\r\n                       seed = 1234,\r\n                       maxLevels = 25,\r\n                       nrUniques = 20,\r\n                       outputPath = '',\r\n                       outputFileName = 'outputPlots'){\r\n\r\n  if(missing(df)){\r\n    print(\"Please supply a data.frame object to visualizeR\")\r\n  } else if(missing(Outcome)){\r\n    print(\"Please indicate which feature is the Outcome/Target/Label feature as type character in the format 'SomeTargetFeature'.\")\r\n  } else {\r\n  \r\n#**************************************************************************************************\r\n                                    #START FUNCTION\r\n#************************************************************************************************** \r\n  \r\n  library(pacman)\r\n  \r\n  p_load(dplyr,ggplot2,knitr,viridis,sqldf)\r\n  \r\n  options(scipen=999)  \r\n  \r\n\r\n\r\n  print(\"visualizeR | 1. Partial Data Cleaning\")\r\n  print(\"MISSING VALUES ARE ENCODED AS 'Missing' FOR CATEGORICAL AND MEDIAN IMPUTATION IS USED FOR NUMERIC\")\r\n  \r\n  #IF GRAPHS SHOULD BE OUTPUTTED TO A PDF FILE\r\n  if(outputPath != ''){\r\n    PDFPath = paste(outputPath,'/',outputFileName,'.pdf',sep='')\r\n    pdf(file=PDFPath)\r\n  }\r\n  \r\n  #CHANGE OUTCOME FEATURE TO FACTOR FORMAT\r\n  df[,Outcome] <- as.factor(df[,Outcome])\r\n  \r\n  #SAMPLING \r\n  set.seed(seed)\r\n  ind <- sample(nrow(df),sample*nrow(df),replace = F)\r\n  df <- df[ind,]\r\n  \r\n  #REMOVE CATEGORICAL FEATURES WITH MORE THAN x LEVELS\r\n  remove <- length(ncol(df))\r\n  removeInd <- length(ncol(df))\r\n  remove <- NA\r\n  removeInd <- NA\r\n  \r\n  for(i in 1:ncol(df)){\r\n    \r\n    #CONVERT TO FACTOR FEATURES\r\n    if(class(df[,i]) %in% c('factor','character')){\r\n      df[,i] <- as.factor(toupper(as.character(df[,i])))\r\n    } else {\r\n      df[,i] <- as.numeric(df[,i])\r\n    }\r\n    \r\n    if(class(df[,i]) == 'factor'){\r\n      ind <- length(levels(df[,i])) > maxLevels\r\n      removeInd[i] <- ifelse(ind ==T,i,NA)\r\n      remove[i] <- ifelse(ind ==T,names(df)[i],NA)\r\n    } else if(class(df[,i]) == 'numeric' & length(unique(df[,i])) < nrUniques){\r\n      print(paste(\"CHANGED FEATURE: \",names(df)[i],\" TO A FACTOR FEATURE DUE TO LOW UNIQUE VALUES\",sep=''))\r\n      df[,i] <- as.factor(df[,i])\r\n    }\r\n    \r\n  }\r\n  \r\n  if(sum(is.na(removeInd)) != length(removeInd)){\r\n    remove <- remove[!is.na(remove)]\r\n    removeInd <- removeInd[!is.na(removeInd)]\r\n    \r\n    for(i in 1:length(remove)){\r\n      print(paste(\"REMOVED FEATURE: \",remove[i],\" , HAS TOO MANY LEVELS TO PLOT\",sep=''))\r\n      \r\n    }\r\n    \r\n    df <- df[,-removeInd]\r\n  }\r\n    \r\n  print(\"visualizeR | 2. Feature Plotting\")\r\n  #CLIP OUTLIERS\r\n  for(i in 1:ncol(df)){\r\n    \r\n    print(paste(\"Plotting Feature\",names(df)[i],\",\",i,\" Of \",ncol(df),\": Missing Observations = \",sum(is.na(df[,i]))))\r\n    \r\n    if(toupper(clipOutliers) == TRUE & class(df[,i]) == 'numeric'){\r\n      \r\n      feature <- names(df)[i]\r\n      threshold <- quantile(df[,feature],0.99,na.rm = T)\r\n      threshold2 <- quantile(df[,feature],0.01,na.rm = T)\r\n      \r\n      \r\n      outliers <- which(df[,feature] >= threshold)\r\n      outliers2 <- which(df[,feature] <= threshold2)\r\n      \r\n      median <- round(median(df[c(-outliers,-outliers2),feature],na.rm=T))\r\n      df[outliers,feature] <- median\r\n      df[outliers2,feature] <- median\r\n    }\r\n    \r\n    \r\n    #CATEGORICAL FEATURES\r\n    if(class(df[,i]) != 'numeric'){\r\n      \r\n      \r\n      if(handleMissing == TRUE){\r\n      df[,i] <- as.factor(ifelse(is.na(as.character(df[,i])) == T,'Missing',as.character(df[,i])))\r\n      }\r\n      \r\n      #STACKED HISTOGRAM\r\n      if(CatChartType == 'stackedHist'){\r\n        \r\n        if(length(levels(df[,i])) <= 2){\r\n          vizCat <- df %>% \r\n            ggplot(aes(x = df[,Outcome], fill = df[,i])) +\r\n            geom_bar(width = 0.6, position = \"fill\") +\r\n            scale_fill_viridis(name = paste(names(df)[i],\": Levels\",sep=''), discrete = T, begin = 0.5,end = 0.9) +\r\n            ggtitle(paste(\"Outcome By \",names(df)[i],sep='')) + \r\n            labs(x = \"Outcome\", y =\"Percentage Freq\")\r\n          \r\n        } else {\r\n          vizCat <- df %>% \r\n            ggplot(aes(x = df[,Outcome], fill = df[,i])) +\r\n            geom_bar(width = 0.6, position = \"fill\") +\r\n            scale_fill_viridis(name = paste(names(df)[i],\": Levels\",sep=''), discrete = T) +\r\n            ggtitle(paste(\"Outcome By \",names(df)[i],sep='')) + \r\n            labs(x = \"Outcome\", y =\"Percentage Freq\")\r\n        }\r\n        \r\n        #CONFUSION PLOT\r\n      } else if(CatChartType == 'Confusion'){\r\n        vizCat <- ggplot(data=df,aes(df[,i],df[,Outcome]))+ \r\n          geom_bin2d(bins=nrBins)+\r\n          theme(axis.text.x = element_text(angle=90))+\r\n          #scale_fill_viridis(name = \"Outcome Levels\", discrete = F) +\r\n          xlab(names(df)[i])+\r\n          ylab('Outcome')+\r\n          ggtitle(paste(\"Outcome Vs \",names(df)[i],sep=''))\r\n      }\r\n      \r\n      #CONTINEOUS FEATURES  \r\n    } else {\r\n      \r\n      if(handleMissing == TRUE){\r\n      df[,i] <- ifelse(is.na(df[,i]) == T,median(df[,i],na.rm = T),df[,i])\r\n      }\r\n      #HISTOGRAM CHART TYPE\r\n      if(NumChartType == 'stackedHist'){\r\n        \r\n        vizNum <- ggplot(data = df, aes(x = df[,i],fill = df[,Outcome], colour = df[,Outcome])) +\r\n          geom_histogram(bins = nrBins, alpha = 0.8) + \r\n          ggtitle(paste(\"Distribution By Outcome\",sep='')) +\r\n          labs(x = names(df)[i], y =\"Freq\") +\r\n          guides(fill = guide_legend(reverse = TRUE), colour = guide_legend(reverse = TRUE)) \r\n        \r\n        \r\n        #DENSITY LINE CHART TYPE\r\n      } else if(NumChartType == 'densityLine') {\r\n        \r\n        vizNum <-  qplot(df[,i], data=df, geom=\"density\", colour=df[,Outcome],size=I(1),\r\n                         main=\"Density Distribution By Outcome\", xlab=paste(names(df)[i]), \r\n                         ylab=\"Density\") \r\n        \r\n        #DENSITY FILL CHART TYPE  \r\n      } else if(NumChartType == 'densityFill'){\r\n        \r\n        vizNum <- ggplot(df, aes(x = df[,i], fill = df[,Outcome])) +\r\n          geom_density(alpha = 0.5) +\r\n          ggtitle(paste(\"Density Distribution By Outcome\",sep='')) +\r\n          labs(x = names(df)[i], y =\"Density\") +\r\n          guides(fill = guide_legend(reverse = TRUE), colour = guide_legend(reverse = TRUE)) \r\n        \r\n        #BOX PLOTS    \r\n      } else if(NumChartType == 'boxPlot'){\r\n        vizNum <- ggplot(df, aes(x = df[, Outcome], \r\n                    y = df[, i], color = df[, Outcome])) +   \r\n                    geom_boxplot(outlier.colour=\"black\", outlier.shape=16,\r\n                                 outlier.size=2, notch=FALSE) + \r\n                    ggtitle(paste(\"Outcome By \", names(df)[i], \r\n                      sep = \"\")) + labs(x = \"Outcome\", y = names(df)[i])\r\n      }  \r\n    }\r\n    \r\n    viz <- list()\r\n    if(names(df)[i] == Outcome){\r\n      viz <- qplot(df[,Outcome], data = df, fill=df[,Outcome],color = df[,Outcome], main = \"Outcome Distribution\") + \r\n        labs(x = \"Outcome\", y =\"Freq\") \r\n    } else if(class(df[,i]) == 'factor'){\r\n      viz <- vizCat\r\n    } else {\r\n      viz <- vizNum\r\n    } \r\n    print(viz)\r\n  }\r\n  \r\n  if(toupper(summaryStats) == TRUE){\r\n\r\n    print(\"visualizeR | 3. Summary Statistics\")\r\n\r\n    for(i in 1:ncol(df)){\r\n      \r\n      if(class(df[,i]) == 'numeric'){\r\n        print(paste('Feature :',names(df)[i],sep=''))\r\n        print(summary(df[,i]))\r\n      } else {\r\n        print(paste('Feature: ',names(df)[i],sep=''))\r\n        print(prop.table(table(df[,i])))\r\n      }\r\n    }\r\n    \r\n  }\r\n  \r\n  if(outputPath != ''){\r\n    dev.off()\r\n  }\r\n  invisible(gc)\r\n }\r\n}\r\n\r\n\r\n" }
{ "repo_name": "Jean-Romain/lidR", "ref": "refs/heads/master", "path": "tests/testthat/test-delineate_crowns.R", "content": "context(\"delineate_crowns\")\n\nlas = clip_rectangle(mixedconifer, 481270, 3812930, 481310, 3812970)\n\ntest_that(\"delineate_crowns works with convex hulls\", {\n  hulls = delineate_crowns(las)\n\n  expect_is(hulls, \"SpatialPolygonsDataFrame\")\n  expect_equal(dim(hulls), c(length(unique(las$treeID))-1,4))\n})\n\ntest_that(\"delineate_crowns works with bbox hulls\", {\n  hulls = delineate_crowns(las, \"bbox\")\n\n  expect_is(hulls, \"SpatialPolygonsDataFrame\")\n  expect_equal(dim(hulls), c(length(unique(las$treeID))-1,4))\n})\n\ntest_that(\"delineate_crowns works with concave hulls\", {\n\n  # Added if in 3.0.1 because of some flavour misteriously\n  # no longer have the package\n  if (requireNamespace(\"concaveman\", quietly = TRUE))\n  {\n    hulls = delineate_crowns(las, \"concave\")\n\n    expect_is(hulls, \"SpatialPolygonsDataFrame\")\n    expect_equal(dim(hulls), c(length(unique(las$treeID))-1,4))\n  }\n})\n\ntest_that(\"delineate_crowns supports custom metrics\", {\n  hulls = delineate_crowns(las, \"bbox\", func = ~max(Z))\n\n  expect_is(hulls, \"SpatialPolygonsDataFrame\")\n  expect_equal(dim(hulls), c(length(unique(las$treeID))-1,5))\n})\n\nctg = mixedconifer_ctg\nopt_select(ctg) = \"0\"\nopt_chunk_size(ctg) = 100\nopt_chunk_alignment(ctg)  <- c(0,20)\nopt_progress(ctg) <- FALSE\n\ntest_that(\"delineate_crowns works with a custom metrics\", {\n  hulls = delineate_crowns(ctg, func = ~max(Z))\n\n  expect_is(hulls, \"SpatialPolygonsDataFrame\")\n  expect_equal(dim(hulls), c(200,5))\n})\n\n" }
{ "repo_name": "mworkentine/mattsUtils", "ref": "refs/heads/master", "path": "R/microbiome_helpers.R", "content": "\n#' rlog normalize an OTU table\n#'\n#' Use the \\code{\\link{rlog}} function from DESeq2 to normalize an OTU table\n#'\n#' @param physeq valid phyloseq object\n#' @param remove_negs remove negative values from the normalized OTU table (TRUE). Defaults to FALSE\n#' @return The original phyloseq object with the normalized OTU table\n#' @export\n#'\nrlog_norm_otus = function(physeq, remove_negs = FALSE) {\n\n\n  physeq_norm = physeq\n  otu_table(physeq_norm) = otu_table(physeq) + 1\n\n  dds = phyloseq_to_deseq2(physeq_norm, ~1)\n  ddr = rlog(dds, blind = TRUE, fitType = \"local\")\n  counts_norm = as.matrix(assay(ddr))\n\n  if (remove_negs) {\n    counts_norm[counts_norm < 0] = 0\n  }\n\n  otu_table(physeq_norm) = otu_table(counts_norm,taxa_are_rows = TRUE)\n\n  return(physeq_norm)\n\n}\n\n\n#' Log fold-change plot\n#'\n#' Create plot that shows the log-fold change for various taxa\n#'\n#' @param x results from DESeq2 differential analysis, processed with biobroom\n#' @param tax_level the taxonomy level to group the y-axis by\n#' @param colour character, the taxonomy level to colour the points\n#' @param errors logical, should the error bars be included, defaults to FALSE\n#'\n#' @export\n#'\nlfc_plot = function(x, tax_level = \"Genus\", colour = \"Phylum\", errors = FALSE) {\n\n  new_levels = unique(x[[tax_level]][order(x$estimate)])\n  x[[tax_level]] = factor(x[[tax_level]], levels = new_levels)\n  #x %>% mutate(Genus = factor(Genus, levels = unique(Genus[order(estimate)])))  %>%\n   p = x %>%\n     ggplot(aes_string(x = \"estimate\", y = tax_level, colour = colour)) + geom_point(size = 3) +\n      geom_vline(xintercept = 0) +\n      scale_color_brewer(palette = \"Set1\")\n\n   if (errors) {\n     p = p + geom_errorbarh(aes(xmin = estimate - stderror, xmax = estimate + stderror), height = 0.5)\n   }\n\n   return(p)\n}\n\n\n#' Get a list of significant taxa from a DESeq analysis\n#'\n#' @param dds valid DESeq2 object\n#' @param physeq valid phyloseq object\n#' @param cutoff pvalue cutoff\n#' @param ... other parameters passed to \\code{\\link{DESeq2::results}}\n#'\n#' @export\n#' @import biobroom\n#' @import broom\n#'\nget_sig_taxa = function(dds, physeq, cutoff = 0.05, ...) {\n\n  res = results(dds, ...) %>% tidy() %>% filter(p.adjusted < cutoff) %>%\n    dplyr::rename(OTU = gene)\n  tax = tax_table(physeq) %>% data.frame() %>% tibble::rownames_to_column(\"OTU\")\n\n  res = res %>% left_join(tax, by = \"OTU\")\n\n  return(res)\n\n}\n\n\n#' Summarize taxonomy\n#'\n#' Summarize a specified taxonomy within a higher level, usually phylum\n#'\n#' @param physeq a valid phyloseq object\n#' @param grouping_tax the higher level taxonomy to group within\n#' @param summary_tax the lower level taxonomy to summarize\n#' @param filter numeric, filter out members of the summary taxonomy below this fraction\n#'\n#' @return a data frame\n#' @export\n#'\nsummarize_taxonomy = function(physeq, grouping_tax = \"Phylum\", summary_tax = \"Genus\", filter = .01) {\n  physeq %>% psmelt() %>% group_by_(\"OTU\", grouping_tax, summary_tax) %>%\n    summarise(Abundance = median(Abundance)) %>%\n    group_by_(grouping_tax, summary_tax) %>%\n    summarise(tot = sum(Abundance)) %>%\n    mutate(rel = tot / sum(tot)) %>%\n    filter(rel > filter) %>%\n    arrange_(grouping_tax, ~desc(rel))\n}\n\n#' Subset taxa 2\n#'\n#' A programming safe version of \\code{\\link{phyloseq::subset_taxa}}\n#'\n#' @param physeq a valid phyloseq object\n#' @param rank the taxonomic rank of the taxa to subset\n#' @param taxa the taxa name to subset to\n#'\n#' @export\n#'\nsubset_taxa_safe = function(physeq, rank, taxa) {\n\ttx = as.data.frame(tax_table(physeq))\n\totus = rownames(tx[tx[[rank]] == taxa, ])\n\tphyloseq::prune_taxa(otus, physeq)\n}\n\n#' Subset samples 2\n#'\n#' A programming safe version of \\code{\\link{phyloseq::subset_samples}}\n#'\n#' @param physeq a valid phyloseq object\n#' @param var variable (column name) of sample data\n#' @param value filter to keep only this value of the variable\n#'\n#' @export\n#'\nsubset_samples_safe = function(physeq, var, value) {\n  keeps = rownames(phyloseq::sample_data(physeq)[phyloseq::sample_data(physeq)[[var]] == value,])\n  phyloseq::prune_samples(keeps, physeq)\n}\n\n\n#' Replace counts\n#'\n#' Replace OTU counts in a phyloseq object with the normalized counts from a DESeq object containing\n#' the same OTUS\n#'\n#' @export\n#'\nreplace_counts = function(physeq, dds) {\n\n  dds_counts = counts(dds, normalized = TRUE)\n  if (!identical(taxa_names(physeq), rownames(dds_counts))) {\n    stop(\"OTU ids don't match\")\n  }\n  otu_table(physeq) = otu_table(dds_counts, taxa_are_rows = TRUE)\n  return(physeq)\n\n}\n\n\n#' Plot OTUs\n#'\n#' Makes a box plot from a list of OTUs\n#'\n#' @param physeq a valid phyloseq object with raw OTU counts that have not been transformed.\n#' @param otus a character vector of OTU ids, present in physeq.  Can be \"all\" to plot all OTUs\n#' @param xaxis character, the sample column to plot on the x-axis\n#' @param fill character, the sample column to use for colouring the boxes\n#' @param labeller character, taxonomy rank to label the OTUs with.  Can also be \"label\" which will\n#'   label with genus if available and family if genus is unassigned.  In this case a prefix for the\n#'   taxonomic rank is addes as well\n#' @param scales fixed or free scales, passed to facet_wrap\n#' @param palette RColorBrewer palette to use\n#' @param glom character, the taxonomic rank to glom at\n#' @param dds valid DESeq2 object.  If present the normalized count data will be used instead of the\n#'   raw count value.\n#' @param justDf logical, should just the data be returned instead of plotting\n#' @param y_scale character, the abundance scale on the y-axis.  Counts are just the raw OTU counts\n#'   unless a DESeq object is provided with the dds argument, in which case the normalized counts\n#'   will be used.  log_counts will transform the counts by log2 using a pseudcount of +0.5.\n#'   relative will use the relative abundance values instead of counts.\n#'   @param nrow number of facet rows, passed to facet_wrap\n#'   @param ncol same as nrow but for columns\n#'\n#' @importFrom stringr str_c\n#' @export\n#'\nplot_OTUs = function(physeq, otus, xaxis, fill, labeller = \"Genus\", scales = \"free_y\",\n                     palette = \"Set1\", glom = NULL, dds = NULL, justDf = FALSE,\n                     y_scale = c(\"log_counts\", \"counts\", \"relative\"), nrow = NULL, ncol = NULL)\n  {\n\n  y_scale = match.arg(y_scale)\n\n  # replace with normalized counts if dds is present\n  if (!is.null(dds)) {\n    physeq = replace_counts(physeq, dds)\n  }\n\n  # transform to appropriate scale\n  if (y_scale == \"relative\") {\n    physeq = transform_sample_counts(physeq, function(x) x/sum(x))\n    y_axis_title = \"Relative Abundance\"\n  } else if (y_scale == \"log_counts\") {\n    physeq = transform_sample_counts(physeq, function(x) log2(x + 0.5))\n    y_axis_title = \"Log2 Abundance\"\n  } else {\n    y_axis_title = \"Abundance\"\n  }\n\n  if (\"all\" %in% otus) {\n    subset_data = physeq\n  } else {\n    subset_data = prune_taxa(otus, physeq)\n    names(otus) = otus\n  }\n\n\n  if (!is.null(glom)) {\n    subset_data = tax_glom(subset_data, glom)\n  }\n\n  subset_data = subset_data %>% psmelt() %>%\n    mutate(label = case_when(\n      is.na(Phylum) ~ \"Unassigned\",\n      is.na(Class) ~ str_c(\"p:\", Phylum),\n      is.na(Order) ~ str_c(\"c\", Class),\n      is.na(Family) ~ str_c(\"o:\", Order),\n      is.na(Genus) ~ str_c(\"f:\", Family),\n      TRUE ~ str_c(\"g:\", Genus)\n    ))\n\n  if (justDf) return(subset_data)\n\n   p = subset_data %>%\n    ggplot(aes_string(x = xaxis, y = \"Abundance\", fill = fill)) +\n      geom_boxplot() +\n      scale_fill_brewer(palette = palette) +\n      labs(y = y_axis_title)\n\n   facet_names = as.character(subset_data[[labeller]])\n\n  if (length(facet_names) == 0) {\n    warning(\"No taxonomic rank found for the requested labeller\")\n    p = p + facet_wrap(~OTU, scales = \"free_y\")\n  } else {\n    facet_names = stringr::str_replace_na(facet_names, \"Unassigned\")\n    names(facet_names) = subset_data$OTU\n    p = p + facet_wrap(~OTU, scales = scales, labeller = labeller(OTU = facet_names),\n                       nrow = nrow, ncol = ncol)\n  }\n\n  p = p + theme(strip.background = element_blank())\n\n  return(p)\n\n}\n\n\n#' Plot genus by phylum\n#'\n#' Bar plot of all the genera within a specified phylum.\n#'\n#' @param physeq A valid phyloseq object\n#' @param phylum character, the phylum to plot\n#' @param x character, the variable to plot on the x-axis, should be one of\n#'   sample_variables(physeq).  Note that if this is anything other than\n#'   \"Samples\", the default, the mean abundance for each value will be plotted.\n#' @param filter numeric, top fraction of abundance to return\n#' @param  facet_by_fam logical, facet by family, default is FALSE\n#' @param xlabel character, when plotting individual samples the variable to use for labels\n#'\n#' @export\nplot_genus_by_phylum = function(physeq, phylum, x = \"Sample\", filter = NULL,\n                                facet_by_fam = FALSE, xlabel = NULL) {\n\n\ttax = as.data.frame(tax_table(physeq))\n  otus = rownames(tax[tax$Phylum == phylum & !is.na(tax$Phylum), ])\n\tphylum_physeq = prune_taxa(otus, physeq)\n\n\tif (is.numeric(filter)) {\n\t\ttopff = filterfun_sample(topf(filter))\n\t\tphylum_physeq = prune_taxa(genefilter_sample(phylum_physeq, topff), phylum_physeq)\n\t}\n\n\tp_data = phylum_physeq %>% tax_glom(\"Genus\") %>% psmelt()\n\n\tif (x != \"Sample\") {\n\t   p_data = p_data %>%\n\t     group_by_(\"Phylum\", \"Genus\", x) %>%\n\t     summarise(Abundance = mean(Abundance))\n\t}\n\n\tgen = get_taxa_unique(phylum_physeq, \"Genus\")\n\tgen_cols  = colorRampPalette(RColorBrewer::brewer.pal(9, \"Set1\"))(length(gen))\n\tp =  p_data %>%\n\t\tggplot(aes_string(x = x, y = \"Abundance\", fill = \"Genus\")) +\n\t\t\tgeom_bar_interactive(aes(tooltip = Genus, data_id = Genus), stat = \"identity\") +\n\t\t\tscale_fill_manual(values = gen_cols)\n\n\n\tif (facet_by_fam) {\n\t\tp = p + facet_wrap(~Family)\n\t}\n\n\tif (!is.null(xlabel)) {\n\t\txlab = setNames(sample_data(phylum_physeq)[[xlabel]], rownames(sample_data(phylum_physeq)))\n\t\tp = p + scale_x_discrete(label = xlab)\n\t}\n\n\treturn(p)\n}\n\n\n#' Plot number of sequences\n#'\n#' @export\nplot_num_seqs = function(physeq) {\n\n\tnum_seqs_gg = data_frame(num_seqs = sample_sums(physeq), Sample = names(sample_sums(physeq))) %>%\n\t\tmutate(pretty_num = format(num_seqs, big.mark = \",\")) %>%\n\t\tggplot(aes(x = reorder(Sample, num_seqs), y = num_seqs)) +\n\t\tggiraph::geom_point_interactive(aes(tooltip = pretty_num, data_id = pretty_num), size = 4) +\n\t\tlabs(x = \"Sample\", y = \"Number of sequences\") +\n\t\tscale_y_continuous(breaks = seq(0,\n\t\t\t\t\tmax(sample_sums(physeq)) +  0.1*max(sample_sums(physeq)), by = 10000), labels = scales::comma) +\n\t\ttheme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\n\n\treturn(num_seqs_gg)\n\n}\n\n\n#' Make colour palettes for taxa\n#'\n#' @param rank Taxonomic rank to use\n#' @param physeq valid phyloseq object\n#' @param palette Either a valid RColorBrewer palette or \"iwanthue\" to generate\n#'   a random palette\n#' @export\nmake_taxa_colours = function(rank, physeq, palette = \"iwanthue\") {\n  taxa = na.omit(get_taxa_unique(physeq, rank))\n  ncols = length(taxa)\n  if (!palette %in% rownames(RColorBrewer::brewer.pal.info)) {\n    pal = hues::iwanthue(ncols, 0, 360, 40, 70, 50, 95, random = TRUE)\n  } else {\n    palsize = RColorBrewer::brewer.pal.info[palette, \"maxcolors\"]\n    pal = colorRampPalette(RColorBrewer::brewer.pal(palsize, palette))(ncols)\n  }\n  return(setNames(pal, taxa))\n}\n\n#' Plot bar 2\n#'\n#' An alternate version of \\code{\\link{phyloseq::plot_bar}}\n#'\n#' @param physeq valid phyloseq object\n#' @param rank character, the taxa rank to plot, must be one of \\code{rank_names(physeq)}\n#' @param glom logical, should the taxa be aggregated at the level of \\code{rank}\n#' @param x character, the sample variable to plot on the x-axis\n#' @param xlabs character vector, labels to use on the x-axis\n#' @param position character, for geom_bar\n#' @param palette Either a valid RColorBrewer palette or \"iwanthue\" to generate\n#'   a random palette\n#' @param pal_seed Seed for generating palette.  Set to NULL for random\n#'\n#' @return a ggplot object\n#'\n#' @export\n#'\nplot_bar2 = function(physeq, rank = \"Phylum\", glom = TRUE, x = \"Sample\",\n                     xlabs = NULL, position = \"stack\", palette = \"iwanthue\",\n                     pal_seed = 5858) {\n\n  if (!is.null(pal_seed)) set.seed(pal_seed)\n\tcols = make_taxa_colours(rank, physeq, palette)\n\n\tif (glom) {\n\t  plot_data = physeq %>% tax_glom(rank) %>% psmelt()\n\t} else {\n\t  plot_data = physeq %>% psmelt()\n\t}\n\n\tp = plot_data %>%\n\t\tggplot(aes_string(x = x, y = \"Abundance\", fill = rank)) +\n\t\tggiraph::geom_bar_interactive(aes_string(tooltip = rank, data_id = rank),\n\t\t                              stat = \"identity\", position = position) +\n\t\ttheme_bw() +\n\t \tscale_fill_manual(values = cols) +\n\t\ttheme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\n\n\tif (!is.null(xlabs)) {\n\t\txlabs = setNames(plot_data[[xlabs]], plot_data$Sample)\n\t \tp = p + scale_x_discrete(labels = xlabs)\n\t}\n\n\treturn(p)\n}\n\n#' Convert phyloseq otu table to vegan otu table\n#'\n#' @export\nveganotu = function(physeq) {\n\n  OTU = otu_table(physeq)\n  if (taxa_are_rows(OTU)) {\n    OTU = t(OTU)\n  }\n  return(as(OTU, \"matrix\"))\n}\n\n\n#' Make ordination plots\n#'\n#' Function to quickly plot a variety of oridation methods Uses\n#' \\code{\\link{plot_ordination}} from phyloseq\n#'\n#' @param physeq (Required). A phyloseq object\n#' @param ord_methods A character vector containing the names of the ordinations\n#'   to plot. Default is \\code{c(\"NMDS\", \"PCoA\")}.\n#' @param distance A character string. Default is \"bray\".  Name of the distance\n#'   method to use.  Value is passed to \\code{\\link{ordinate}}.\n#' @param colour_var A character string containing the name of the variable to\n#'   use for colouring the plot.  Must be a valid column of\n#'   \\code{sample_data(physeq)}\n#'\n#' @return Returns a \\code{ggplot} object with the plots of interest\n#'\n#' @export\n#'\nmake_ordination_plots = function(physeq, ord_methods = c(\"NMDS\", \"PCoA\"), distance = \"bray\",\n                                 colour_var = NULL){\n\n  ords = lapply(ord_methods, function(x) {\n    ord = ordinate(physeq = physeq, method = x, distance = distance)\n    ord_data = plot_ordination(physeq, ord, justDF = TRUE)\n    ord_data$Method = x\n    colnames(ord_data)[1:2] = c(\"Axis1\", \"Axis2\")\n    return(ord_data)\n  })\n  all_data = do.call(rbind, ords)\n  ggplot(all_data, aes(x = Axis1, y = Axis2)) +\n    geom_point(aes_string(colour = colour_var), size = 3) +\n    facet_wrap(~Method, scales = \"free\") +\n    labs(x = \"Axis 1\", y = \"Axis 2\")\n}\n\n\n\n#' Get a dataframe with richness estimates\n#'\n#' Wrapper around \\code{\\link{estimate_richness}} to get an easier to use dataframe\n#' @param physeq A phyloseq object\n#' @param ... Arguments passed to \\code{estimate_richness}\n#' @return A dataframe with richness estimates and sample data\n#' @export\nget_richness = function(physeq, ...) {\n  rich = estimate_richness(physeq, ...)\n  rich %<>% add_rownames(\"SampleID\") %>%\n    left_join(sample_data(physeq))\n  return(rich)\n}\n\n\n#' A dplyr version of psmelt\n#'\n#' borrowed from http://chuckpr.github.io/blog/melt.html\n#'\n#'\n#' @export\npsmelt_dplyr = function(physeq) {\n    sd = data.frame(sample_data(physeq))\n    tt = data.frame(tax_table(physeq)) %>% add_rownames(\"OTU\")\n    otu_tab = data.frame(otu_table(physeq), check.names = FALSE) %>% add_rownames(\"OTU\")\n    otu_tab %>%\n        left_join(tt) %>%\n        gather_(\"SampleID\", \"Abundance\", setdiff(colnames(otu_tab), \"OTU\")) %>%\n        left_join(sd)\n}\n\n#' VST Tranform a phyloseq object\n#'\n#' Uses the \\code{\\link{getVarianceStabilizedData}} function from DESeq2 to vst\n#' transform an OTU table\n#'\n#' @param physeq a phyloseq object where the OTU table is raw counts\n#' @return a phyloseq object with vst normalized OTU table\n#' @export\nvst_transform = function(physeq, size_factors = NULL) {\n  vst = phyloseq_to_deseq2(physeq, ~1)\n  if (!is.null(size_factors)) {\n    sizeFactors(vst) = size_factors\n  } else {\n    vst = estimateSizeFactors(vst, type = \"poscounts\")\n  }\n\n  vst = estimateDispersions(vst, fitType = \"local\")\n  vst = getVarianceStabilizedData(vst)\n  physeq_vst = physeq\n  otu_table(physeq_vst) = otu_table(vst, taxa_are_rows = TRUE)\n  return(physeq_vst)\n}\n\n\nsplit_species = function(string, n = 2) {\n  splits = str_split(string, \"/\", n + 1)\n  res = map_if(splits, ~length(.x) > 2, ~.x[1:n]) %>%\n    map_chr(str_c, collapse = \"/\")\n  return(res)\n}\n\n\n#' Add taxonomy label\n#'\n#' add a column to the taxonomy table of a phyloseq object that lists the\n#' lowest rank taxonomy assigned to that OTU along with a prefix indicating\n#' the taxonomic rank.\n#'\n#' Example: g:Pseudomonas\n#'\n#' @param physeq a valid phyloseq object that contains a taxonomy table\n#' @param num_species the number of species to retain if more than one are identified\n#' @return a phyloseq object with an additional column on the taxonomy\n#'         table called \"Taxonomy\"\n#' @export\nadd_taxonomy_column = function(physeq, num_species = 2) {\n  tax_df = as.data.frame(tax_table(physeq)@.Data) %>%\n    rownames_to_column(\"OTU\") %>%\n    mutate(Species = split_species(Species, n = num_species)) %>%\n    mutate(Taxonomy =\n      case_when(\n        is.na(Class)  ~ str_c(\"o:\", Phylum),\n        is.na(Order)  ~ str_c(\"o:\", Class),\n        is.na(Family)  ~ str_c(\"o:\", Order),\n        is.na(Genus)   ~ str_c(\"f:\", Family),\n        is.na(Species) ~ str_c(\"g:\", Genus),\n        TRUE ~ str_c(Genus, \" \", Species)\n      )\n    )\n\n  tax = as.matrix(tax_df[, -1])\n  rownames(tax) = tax_df$OTU\n  tax_table(physeq) = tax_table(tax)\n\n  return(physeq)\n}\n\n\n#' Sequence stats\n#' @export\nget_seq_stats = function(physeq) {\n  return(list(\n    ntaxa = ntaxa(physeq),\n    sum = sum(sample_sums(physeq)),\n    nsamples = nsamples(physeq),\n    median = median(sample_sums(physeq)),\n    sd  = sd(sample_sums(physeq)),\n    min = min(sample_sums(physeq)),\n    max = max(sample_sums(physeq))\n    )\n  )\n}\n\n\n" }
{ "repo_name": "hxfeng/R-3.1.2", "ref": "refs/heads/master", "path": "src/library/utils/R/unix/mac.install.R", "content": "#  File src/library/utils/R/unix/mac.install.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2014 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n\nif(substr(R.version$os, 1L, 6L) != \"darwin\") {\n.install.macbinary <-\n    function(pkgs, lib, repos = getOption(\"repos\"),\n             contriburl = contrib.url(repos, type=\"mac.binary\"),\n             method, available = NULL, destdir = NULL,\n             dependencies = FALSE,\n             lock = getOption(\"install.lock\", FALSE), quiet = FALSE,\n             ...)\n    {}\n} else {\n## edited from windows/.install.winbinary\n##\n.install.macbinary <-\n    function(pkgs, lib, repos = getOption(\"repos\"),\n             contriburl = contrib.url(repos, type=\"mac.binary\"),\n             method, available = NULL, destdir = NULL,\n             dependencies = FALSE,\n             lock = getOption(\"install.lock\", FALSE), quiet = FALSE,\n             ...)\n{\n    untar <- function(what, where)\n    {\n        ## FIXME: should this look for Sys.getenv('TAR')?\n        ## Leopard has GNU tar, SL has BSD tar.\n        xcode <- system(paste0(\"tar zxf \\\"\", path.expand(what), \"\\\" -C \\\"\",\n                               path.expand(where), \"\\\"\"), intern=FALSE)\n        if (xcode)\n            warning(gettextf(\"'tar' returned non-zero exit code %d\", xcode),\n                    domain = NA, call. = FALSE)\n    }\n\n    unpackPkg <- function(pkg, pkgname, lib, lock = FALSE)\n    {\n        dir.exists <- function(x) !is.na(isdir <- file.info(x)$isdir) & isdir\n        ## Create a temporary directory and unpack the zip to it\n        ## then get the real package & version name, copying the\n        ## dir over to the appropriate install dir.\n        tmpDir <- tempfile(, lib)\n        if (!dir.create(tmpDir))\n            stop(gettextf(\"unable to create temporary directory %s\",\n                          sQuote(tmpDir)),\n                 domain = NA, call. = FALSE)\n        cDir <- getwd()\n        on.exit(setwd(cDir), add = TRUE)\n        res <- untar(pkg, tmpDir)\n        setwd(tmpDir)\n        ## sanity check: people have tried to install source .tgz files\n        if (!file.exists(file <- file.path(pkgname, \"Meta\", \"package.rds\")))\n            stop(gettextf(\"file %s is not an OS X binary package\", sQuote(pkg)),\n                 domain = NA, call. = FALSE)\n        desc <- readRDS(file)$DESCRIPTION\n        if (length(desc) < 1L)\n            stop(gettextf(\"file %s is not an OS X binary package\", sQuote(pkg)),\n                 domain = NA, call. = FALSE)\n        desc <- as.list(desc)\n        if (is.null(desc$Built))\n            stop(gettextf(\"file %s is not an OS X binary package\", sQuote(pkg)),\n                 domain = NA, call. = FALSE)\n\n        res <- tools::checkMD5sums(pkgname, file.path(tmpDir, pkgname))\n        if(!quiet && !is.na(res) && res) {\n            cat(gettextf(\"package %s successfully unpacked and MD5 sums checked\\n\",\n                         sQuote(pkgname)))\n            flush.console()\n        }\n\n        instPath <- file.path(lib, pkgname)\n        if(identical(lock, \"pkglock\") || isTRUE(lock)) {\n\t    lockdir <- if(identical(lock, \"pkglock\"))\n                file.path(lib, paste(\"00LOCK\", pkgname, sep = \"-\"))\n            else file.path(lib, \"00LOCK\")\n\t    if (file.exists(lockdir)) {\n                stop(gettextf(\"ERROR: failed to lock directory %s for modifying\\nTry removing %s\",\n                              sQuote(lib), sQuote(lockdir)), domain = NA)\n\t    }\n\t    dir.create(lockdir, recursive = TRUE)\n\t    if (!dir.exists(lockdir))\n                stop(gettextf(\"ERROR: failed to create lock directory %s\",\n                              sQuote(lockdir)), domain = NA)\n            ## Back up a previous version\n            if (file.exists(instPath)) {\n                file.copy(instPath, lockdir, recursive = TRUE)\n        \ton.exit({\n         \t    if (restorePrevious) {\n                        try(unlink(instPath, recursive = TRUE))\n        \t    \tsavedcopy <- file.path(lockdir, pkgname)\n        \t    \tfile.copy(savedcopy, lib, recursive = TRUE)\n        \t    \twarning(gettextf(\"restored %s\", sQuote(pkgname)),\n                                domain = NA, call. = FALSE, immediate. = TRUE)\n        \t    }\n        \t}, add=TRUE)\n        \trestorePrevious <- FALSE\n            }\n\t    on.exit(unlink(lockdir, recursive = TRUE), add=TRUE)\n        }\n        ## If the package is already installed, remove it.  If it\n        ## isn't there, the unlink call will still return success.\n        ret <- unlink(instPath, recursive=TRUE)\n        if (ret == 0L) {\n            ## Move the new package to the install lib and\n            ## remove our temp dir\n            ret <- file.rename(file.path(tmpDir, pkgname), instPath)\n            if(!ret) {\n                warning(gettextf(\"unable to move temporary installation %s to %s\",\n                                 sQuote(file.path(tmpDir, pkgname)),\n                                 sQuote(instPath)),\n                        domain = NA, call. = FALSE)\n                restorePrevious <- TRUE # Might not be used\n            }\n        } else\n        stop(gettextf(\"cannot remove prior installation of package %s\",\n                      sQuote(pkgname)), call. = FALSE, domain = NA)\n        setwd(cDir)\n        unlink(tmpDir, recursive=TRUE)\n    }\n\n    if(!length(pkgs)) return(invisible())\n\n    if(is.null(contriburl)) {\n        pkgnames <- basename(pkgs)\n        pkgnames <- sub(\"\\\\.tgz$\", \"\", pkgnames)\n        pkgnames <- sub(\"\\\\.tar\\\\.gz$\", \"\", pkgnames)\n        pkgnames <- sub(\"_.*$\", \"\", pkgnames)\n        ## there is no guarantee we have got the package name right:\n        ## foo.zip might contain package bar or Foo or FOO or ....\n        ## but we can't tell without trying to unpack it.\n        for(i in seq_along(pkgs)) {\n            if(is.na(pkgs[i])) next\n            unpackPkg(pkgs[i], pkgnames[i], lib, lock = lock)\n        }\n        return(invisible())\n    }\n    tmpd <- destdir\n    nonlocalcran <- length(grep(\"^file:\", contriburl)) < length(contriburl)\n    if(is.null(destdir) && nonlocalcran) {\n        tmpd <- file.path(tempdir(), \"downloaded_packages\")\n        if (!file.exists(tmpd) && !dir.create(tmpd))\n            stop(gettextf(\"unable to create temporary directory %s\",\n                          sQuote(tmpd)),\n                 domain = NA)\n    }\n\n    if(is.null(available))\n        available <- available.packages(contriburl = contriburl,\n                                        method = method)\n    pkgs <- getDependencies(pkgs, dependencies, available, lib, binary = TRUE)\n\n    foundpkgs <- download.packages(pkgs, destdir = tmpd, available = available,\n                                   contriburl = contriburl, method = method,\n                                   type = \"mac.binary\", quiet = quiet, ...)\n\n    if(length(foundpkgs)) {\n        update <- unique(cbind(pkgs, lib))\n        colnames(update) <- c(\"Package\", \"LibPath\")\n        for(lib in unique(update[,\"LibPath\"])) {\n            oklib <- lib==update[,\"LibPath\"]\n            for(p in update[oklib, \"Package\"])\n            {\n                okp <- p == foundpkgs[, 1L]\n                if(any(okp))\n                    unpackPkg(foundpkgs[okp, 2L], foundpkgs[okp, 1L], lib,\n                              lock = lock)\n            }\n        }\n        if(!quiet && !is.null(tmpd) && is.null(destdir))\n            cat(\"\\n\", gettextf(\"The downloaded binary packages are in\\n\\t%s\", tmpd),\n                \"\\n\", sep = \"\")\n    } else if(!is.null(tmpd) && is.null(destdir)) unlink(tmpd, recursive = TRUE)\n\n    invisible()\n}\n}\n" }
{ "repo_name": "grishagin/RIGessentials", "ref": "refs/heads/master", "path": "R/add_list2df_as_col.R", "content": "add_list2df_as_col<-\n    function(dframe\n             ,list2add\n             ,newcolname=\"list2add\"){\n\t\t#' @export\n        #' @title\n        #' Add List as Column to Dataframe\n        #' @description \n        #' Takes in a dataframe and a list, such that \\code{length(list2add) == nrow(dframe)}. \n        #' Each dataframe row is replicated as many times as the length of the corresponding list element. \n        #' After that, the list is unlisted and added to the dataframe. \n        #' @param dframe Dataframe to modify.\n        #' @param list2add A list that has the same length as the number of rows in a dataframe.\n        #' @param newcolname A name for the new column.\n        \n        if(nrow(dframe)!=length(list2add)){\n            warning(\"add_list2df_as_col: length(list2add) != nrow(dframe). Returning original dframe.\")\n            return(dframe)\n        }\n        #lengths of list elements\n        lens<-\n            list2add %>% \n            sapply(length)\n        #replicate each row a number of times \n        #equal to the length of the corresponding list element\n        newrows<-\n            rep(1:nrow(dframe)\n                ,times=lens)\n        \n        #cbind the new column (unlisted list) to the modified dframe\n        dframe<-\n            dframe[newrows,] %>% \n            cbind(unlist(list2add))\n        \n        #replace column name\n        colnames(dframe)[ncol(dframe)]<-\n            newcolname\n        \n        return(dframe)\n    }" }
{ "repo_name": "vguillemot/multiblox", "ref": "refs/heads/master", "path": "istacox_method_comparison_MapRedR/scripts/istacox.R", "content": "\n# grad <- function(X, beta, I, R, gamma) {\n#   wij <- mapply( function(i, j) t(exp( X[j, ]%*%beta) / sum( exp(X[R[[sprintf(\"R%i\", i)]], ]%*%beta))), I, R)\n#   names(wij) <- names(R)\n#   xbar <- t(sapply(names(R), function(r) wij[[r]]%*%X[R[[r]], ]) )\n#   grad <- -colSums( X[I, ] - xbar ) + gamma*beta \n# }\n\n\ngrad <- function(X, beta, I, R, alpha, link) {\n#   print(X)\n#   print(beta)\n#   print(X[1,]%*%beta)\n#   print(I)\n  # print(R)\n  # wij <- mapply( function(i, j) t(exp(matrix(X[j, ], nrow=1)%*%matrix(beta, ncol=1)) / sum( exp(X[R[[sprintf(\"R%i\", i)]], ]%*%beta))), I, R, SIMPLIFY = FALSE)\n  # wij <- mapply( function(i, j) exp(beta%*%X[j,] / sum( exp(beta%*%X[R[[sprintf(\"R%i\", i)]], ]))), I, R, SIMPLIFY = FALSE)\n  wij <- mapply( function(i, j) t(exp( X[j, ]%*%beta) / sum( exp(X[R[[sprintf(\"R%i\", i)]], ]%*%beta))), I, R)\n#   print(dim(wij[[1]]))\n#   print(dim(as.matrix(X[R[[1]], ])))\n  names(wij) <- names(R)\n  xbar <- t(sapply(names(R), function(r) as.matrix(wij[[r]])%*%as.matrix(X[R[[r]], ])) )\n  grad <- - colSums( X[I, ] - xbar ) + (1-alpha)*beta - link\n}\n\nneg_step <- function(X, t, beta, alpha, g){\n  return((1/t) * (beta - prox(beta - t * g, t, alpha)))\n}\n\nR_ell <- function(X, beta, I, R, alpha){\n  truc <- mapply( function(i, j) {matrix(X[j, ], nrow=1)%*%matrix(beta, ncol=1) -log(sum( exp(X[R[[sprintf(\"R%i\", i)]], ]%*%beta)))}, I, R, SIMPLIFY = F)\n  - sum(unlist(truc)) + (1-alpha)/2 * norm.l2.2(beta)\n}\n\nprox <- function(beta,t,alpha) ifelse(abs(beta)<t*alpha,0,beta-t*alpha*sign(beta))\n\nistacox_step_line_search <- function(X, beta, I, R, t=10, tau=0.5, alpha, link, kmax=1000){\n  R_l <- R_ell(X, beta, I, R, alpha)\n  g <- grad(X, beta, I, R, alpha, link)\n  Gt <- neg_step(X, t, beta, alpha, g)\n  Rt_l <- R_ell(X, beta - t*Gt, I, R, alpha)\n\n  for (k in 1:kmax) {\n    if (Rt_l <= R_l -t*t(g) %*% Gt + t/2*sum(Gt**2)) break\n    t <- tau*t\n    Gt <- neg_step(X, t, beta, alpha, g)\n    Rt_l <- R_ell(X, beta - t*Gt, I, R, alpha)\n  }\n  return(t)\n}\n\n\nfistacox_step_line_search <- function(X, u, I, R, t=10, tau=0.95, alpha, link, kmax=1000){\n  Ru_l <- R_ell(X, u, I, R, alpha)\n  gradu <- grad(X, u, I, R, alpha, link)\n  x <- prox(u - t*gradu, t, alpha)\n  Rx_l <- R_ell(X, x, I, R, alpha)\n\n  for (k in 1:kmax) {\n    if (Rx_l <= Ru_l +  crossprod(gradu, x-u) + 1/(2*t)*sum((x-u)**2)) break\n    t <- tau*t\n    x <- prox(u - t*gradu,t, alpha)\n    Rx_l <- R_ell(X, x, I, R, alpha)\n  }\n  return(t)\n}\n\nistacox <- function(X, I, R, alpha, kmax=1000, epsilon=1e-4, \n                    fast=FALSE, ada=FALSE, link, beta_init) {\n  ### X is now a list of B matrices (blocks)\n  ### b is the block to be treated by istacox\n  ### D is a B by B matrix indicating which blocks are connected to each other\n  ### I list of non censored individuals, ranked by event time.\n  ### R list of sets of individuals at risk at each non censored ranked time\n  ### alpha L1-norm shrinkage parameter\n    \n  p <- ncol(X)\n  n <- nrow(X)\n  \n  if (!is.null(beta_init)){\n    betaold <- beta_init\n  } else {\n    betaold <- rnorm(p) \n  }\n  \n  t <- 1/max(eigen(t(X)%*%X)$values)\n  # print(\"1er grad\")\n  betanew <- prox(betaold - t*grad(X, betaold, I, R, alpha, link),t,alpha)\n  \n  for (k in 2:kmax) {\n    if (fast) {\n      u <- betanew + (k-1) / (k+2) * (betanew - betaold)\n    } else {\n      u <- betanew\n    }\n    if (ada) {\n      if (fast) {\n        t <- fistacox_step_line_search(X, u, I, R, t, tau=0.95, alpha, link)\n      } else {\n        t <- istacox_step_line_search(X, betaold, I, R, t, tau=0.95, alpha, link)\n      }\n    }\n    betaold <- betanew\n    betanew <- prox(u - t*grad(X, u, I, R, alpha, link), t, alpha)\n#     print(\"arret ?\")\n#     print(k)\n# print(betaold)\n# print(betanew)\n    if (sum((betanew-betaold)**2) < epsilon ) break\n  }\n  return(list(beta=betanew, k=k))\n}\n" }
{ "repo_name": "wuletawu/Rsenal", "ref": "refs/heads/master", "path": "R/varsRfeCV.R", "content": "#' extract cross-validated important variables\n#' \n#' @description \n#' this function approaches the identification of important variables from\n#' \\code{\\link{rfe}} more conservatively than \\code{\\link{caret}}. It uses\n#' the standard deviation (or standard error) of the cross-validated error \n#' metric to identify important variables.\n#' \n#' @param rfe.model a rfe model. See \\code{\\link{rfe}}\n#' @param metric the metric to be used. Note this needs to be the metric used \n#' to calculate the \\code{\\link{rfe}} model\n#' @param maximize logical: Is a higher value of the metric favourable\n#'  (e.g metric = Rsquared) or not (e.g metric = RMSE). maximize=TRUE is \n#'  determined automatically as long as metric is either Rsquared, ROC, Accuracy.\n#'  maximize =FALSE is used for all other metrics. Set this manually if you\n#'  use an other metric where higher values are favourable.\n#' @param sderror If TRUE then standard error is calculated. If FALSE then\n#' standard deviations are used\n#' @return\n#' a character vector of the variable names\n#' \n#' @author\n#' Hanna Meyer, Tim Appelhans\n#' \n#' @export varsRfeCV\n#' @aliases varsRfeCV\n\nvarsRfeCV <- function (rfe.model,\n                       metric = rfe.model$metric,\n                       maximize=FALSE,\n                       sderror=TRUE) {\n  \n  \n  if (metric==\"Rsquared\"||metric==\"ROC\"||metric==\"Accuracy\"){\n    maximize=TRUE\n  } else {\n    maximize =FALSE\n  }\n  \n  data <- as.data.frame(rfe.model$resample)\n  \n  sdv <- c()\n  means <- c()\n  \n  for (i in unique(data$Variables)) {\n    if(!sderror){\n      sdv <- c(sdv,\n               sd(eval(parse(text = paste(\"data$\",\n                                          metric)))[data$Variables == i]))\n    }\n    if(sderror){\n      sdv <- c(sdv,se(eval(parse(text = paste(\"data$\",\n                                              metric)))[data$Variables == i]))\n    }\n    means <- c(means,\n               mean(eval(parse(text=paste(\"data$\",\n                                          metric)))[data$Variables==i]))\n  }\n  if (maximize)  {\n    upr <- means - sdv\n  } else {\n    upr <- means + sdv\n  }\n  #start_var <- min(unique(data$Variables))\n  #start_offset <- abs(start_var - 1)\n  \n  #   print(means)\n  #   print(upr)\n  #   print(upr[rfe.model$bestSubset - start_offset])\n  #   print(means < upr[rfe.model$bestSubset - start_offset])\n  if (maximize){\n    n_vars <- unique(data$Variables)[which(means >\n                                             upr[which(unique(data$Variables)==\n                                                         rfe.model$bestSubset)])[1]]\n  } else{\n    n_vars <- unique(data$Variables)[which(means <\n                                             upr[which(unique(data$Variables)==\n                                                         rfe.model$bestSubset)])[1]]\n  }\n  \n  subset <- rfe.model$variables[rfe.model$variables$Variables==n_vars,]\n  uniqueVars <- unique(subset$var)\n  bestVar<-c()\n  for (i in 1:length(uniqueVars)){\n    bestVar[i]=sum(subset$Overall[subset$var==uniqueVars[i]])/10\n  }\n  names(bestVar) <- uniqueVars\n  bestVar<-sort(bestVar,decreasing=TRUE)[1:n_vars]\n  \n  #bestVar <- rfe.model$control$functions$selectVar(rfe.model$variables, n_vars)\n  #return(rfe.model$optVariables[1:n_vars])\n  return(bestVar)\n  \n}\n" }
{ "repo_name": "StefanPinkernell/mdw", "ref": "refs/heads/master", "path": "R/mdw.R", "content": "#v3\n\n#' Metadata Wrapper\n#'\n#' The metadata wrapper is a tool to handle metadata of distribution models in \n#' a tree like structure.\n#'\n#' Metadata for distribution models can cover information about different kinds of data (e.g.\n#' observation and environmental data), information about the model itself (e.g. the modelling method, \n#' study area, settings/parameters, ...), information about resulting maps (e.g. units, the CRS, extent, ...),\n#' etc. These information are stored as lists of attributes in R objects of the class \\code{\\link{Modul}} \n#' representing these entities. Modul objects can be nested to create a tree like structure to describe the \n#' provenance of all 'ingredients' of the model as well as its output. The object can either directly be stored as R \n#' objects, or exported to a simple XML file.\n#' \n#' @author Stefan Pinkernell, Jan Holstein\n#' @docType package\n#' @name mdw\n#' @seealso \\code{\\link{Modul}}, \\code{\\link{type}}, \\code{\\link{attribute}}, \\code{\\link{children}}, \\code{\\link{summary.Modul}},  \\code{\\link{toXML}}, \\code{\\link{parseXML}}\n#' \n#' @examples\n#' m1<-Modul(type = \"Map\",attributes= c(a=\"Map1\",b=\"Map 2\"))\n#' m2<-Modul(type = \"Map\",attributes = c(a=\"nested Map\"))\n#' m3<-Modul(type = \"Data\",attributes= c(a=\"Data1\",b=\"Data 2\"),\n#'    children = list(child1=m2))\n#' \n#' m5<-Modul(type = \"Map\",attributes= c(a=\"Map3\",b=\"Map4\"))\n#' \n#' m4<-Modul(type = \"Model\",attributes = c(a=\"Attribute1\",b=\"Attribute 2\"),\n#'    children = list(child1=m1,child2=m3,child3=m5))\n#' \n#' summary(m4)\n#' \n#' \\dontrun{\n#' #toXML(m4,file=\"xml/out.xml\") # use variable name for topmost entry (\"m4\" in this case)\n#'\n#' #toXML(m4,file=\"xml/out.xml\",name=\"test\") #set name for topmost entry \n#' \n#' #test<-parseXML(file = \"xml/out.xml\")\n#' #summary(test)\n#' \n#' \n#' ##validate xml\n#' #xsd = xmlTreeParse(\"xml/sdm.xsd\", isSchema =TRUE, useInternal = TRUE)\n#' #doc = xmlInternalTreeParse(\"xml/out.xml\")\n#' #xmlSchemaValidate(xsd, doc)\n#' }\n#' \n#' #Example for a Maxent distribution model:\n#' observationData1<-Modul(type=\"Data\",attributes=c(datatype=\"observation data\",\n#'    datatype=\"presence only data\",source=\"GBIF\",species=\"Eucampia antarctica\"))\n#' predictors1<-Modul(type=\"Data\",attributes=c(datatype=\"environmental data\",\n#'    area=\"global\",resolution=\"1 degree\",depth=\"surface\",\n#'    source=\"World Ocean Atlas 2009\",crs=\"EPSG:4326\"))\n#' projectionDataset_2<-Modul(type=\"Data\",attributes=c(datatype=\"monthly environmental data\",\n#'    month=\"February\",area=\"global\",resolution=\"1 degree\",depth=\"surface\",\n#'    source=\"World Ocean Atlas 2009\",crs=\"EPSG:4326\"))\n#' projectionDataset_8<-Modul(type=\"Data\",attributes=c(datatype=\"monthly environmental data\",\n#'    month=\"August\",area=\"global\",resolution=\"1 degree\",depth=\"surface\",\n#'    source=\"World Ocean Atlas 2009\",crs=\"EPSG:4326\"))\n#' prediction_2<-Modul(type=\"Map\",attributes=c(title=\"Prediction of Eucampia antarctica for \n#'    February\",area=\"global\",crs=\"EPSG:4326\"),children=list(projectionDataset=projectionDataset_8))\n#' prediction_8<-Modul(type=\"Map\",attributes=c(title=\"Prediction of Eucampia antarctica for \n#'    August\",area=\"global\",crs=\"EPSG:4326\"),children=list(projectionDataset=projectionDataset_8))\n#' model1<-Modul(type=\"Model\",attributes=c(method=\"Maxent distribution model\"),children=\n#'    list(observationData=observationData1,predictors=predictors1,pred2=prediction_2,\n#'        pred8=prediction_8))\n#' summary(model1)\n#' \n\nNULL\n\n\n#' Modul\n#' \n#' Objects of this class represent the nodes in the tree of metadata objects. \n#' \n#' @param type Specify the type of object this metadata entry belongs to (e.g. Map, Data, Model)\n#' @param attributes List of attributes\n#' @param children List of child elements \n#' @return model object\n#' @seealso \\code{\\link{mdw}}, \\code{\\link{type}}, \\code{\\link{attribute}}, \\code{\\link{children}}, \\code{\\link{summary.Modul}},  \\code{\\link{toXML}}, \\code{\\link{parseXML}}\n#' @export\n#' @examples\n#' m1<-Modul(type = \"Map\",attributes= c(a=\"Map1\",b=\"Map 2\"))\n#' m2<-Modul(type = \"Map\",attributes = c(a=\"nested Map\"))\n#' m3<-Modul(type = \"Data\",attributes= c(a=\"Data1\",b=\"Data 2\"),children = list(child1=m2))\n#' \n#' m5<-Modul(type = \"Map\",attributes= c(a=\"Map3\",b=\"Map4\"))\n#' \n#' m4<-Modul(type = \"Model\",attributes = c(a=\"Attribute1\",b=\"Attribute 2\"),\n#'    children = list(child1=m1,child2=m3,child3=m5))\n#' \n#' summary(m4)\n#' \n#' \\dontrun{\n#' toXML(m4,file=\"xml/out.xml\") # use variable name for topmost entry (\"m4\" in this case)\n#'\n#' toXML(m4,file=\"xml/out.xml\",name=\"test\") #set name for topmost entry \n#' }\n#' \nModul <- function(type=NULL,attributes=NULL,children=NULL){\n  if(!is.null(type)){\n    me <- list(\n      type=type,\n      attributes=attributes\n    )\n    \n    if(!is.null(children)){\n      me<-c(me,children)\n    }\n    \n    class(me) <- append(\"Modul\",class(me)) # Set the name for the class\n    return(me)  \n  } else {\n    return(NULL)\n  }\n}\n\n\n#' type\n#' \n#' Gets type information of an object of class \\code{\\link{Modul}}.\n#' \n#' @seealso \\code{\\link{Modul}}, \\code{\\link{attribute}}, \\code{\\link{children}}\n#' @export\n#' \ntype <- function(x){\n  UseMethod('type', x)\n}\n\n#' @describeIn type\n#' @param x Object of class \\code{\\link{Modul}}\n#' @return Getter Method: type\n#' @export\n#' \ntype.Modul <- function(x){\n  return(x$type)\n}\n\n#' type<-\n#' \n#' Sets type information of an object of class \\code{\\link{Modul}}.\n#' \n#' @rdname type\n#' @export\n#' \n'type<-' <- function(x,value){\n  UseMethod('type<-', x)\n}\n\n#' @rdname type\n#' @param value New value\n#' @return Setter Method: object of class \\code{\\link{Modul}}\n#' @export\n#' @examples\n#' m1 <- Modul(type=\"Map\")    \n#' type(m1)\n#' type(m1) <- \"Data\"\n#' type(m1)\n#' \n'type<-.Modul' <- function(x, value){\n  x$type <- value\n  return(x)\n}\n\n#' attribute\n#' \n#' Get a nodes attributes\n#' \n#' @seealso \\code{\\link{Modul}}, \\code{\\link{type}}, \\code{\\link{children}}\n#' @export\n#' \nattribute <- function(x){\n  UseMethod('attribute', x)\n}\n\n#' @describeIn attribute\n# @param x Object of class \\code{\\link{Modul}}\n# @return Getter method: attributes\n#' @export\n#' \nattribute.Modul <- function(x){\n  return(x$attributes)\n}\n\n#' attribute<-\n#' \n#' Set a nodes attributes\n#' \n#' @rdname attribute\n#' @export\n#' \n'attribute<-' <- function(x, value, append, index){\n  UseMethod('attribute<-', x)\n}\n\n#' @rdname attribute \n#' @param x Object of class \\code{\\link{Modul}}\n#' @param value New Value\n#' @param append Append attributes (TRUE) or replace existing ones (FALSE)\n#' @param index Index of attribute to replace\n#' @return Setter Method: object of class \\code{\\link{Modul}}\n#' @export\n#' @examples\n#' \n#' m1<-Modul(\"Data\")\n#' \n#' attribute(m1)\n#' \n#' attribute(m1)<-\"test\" #add an attribute without a name\n#' attribute(m1)<-c(a2=\"test2\") #add an attribute with a name\n#' attribute(m1)\n#' \n#' attribute(m1,append=FALSE)<-c(a1=\"attrib1\") #replace the old attributes\n#' attribute(m1)\n#' \n#' attribute(m1)<-c(a2=\"attrib2\",a3=\"attrib3\") #add many attributes \n#' attribute(m1)\n#' \n#' attribute(m1,index=2)<-c(\"replaced attribute\") #replace second attibute\n#' attribute(m1)\n#' \n'attribute<-.Modul' <- function(x, value, append = T, index=NULL){\n  if (is.null(index)){\n    if (!append) {\n      x$attributes <- value\n    } else {\n      x$attributes <- append(x$attributes,value)\n    }\n  }else { #replace at index\n    x$attributes[[index]]<-value\n  }\n  return(x)  \n}\n\n#' children\n#' \n#' Get child elements of a node\n#' \n#' @seealso \\code{\\link{Modul}}, \\code{\\link{type}}, \\code{\\link{attribute}}\n#' @export\n#' \nchildren <- function(x){\n  UseMethod('children', x)\n}\n\n#' @describeIn children\n# @param x Object of class \\code{\\link{Modul}}\n# @return Getter Method: list of children\n#' @export\n#' \nchildren.Modul <- function(x){\n  if (length(x)>2){\n    return (x[3:length(x)])\n  }\n  return(NULL)\n}\n\n#' children<-\n#' \n#' Set child elements of a node\n#' \n#' @rdname children\n#' @export\n#' \n'children<-' <- function(x,value,name){\n  UseMethod('children<-', x)\n}\n\n#' @rdname children\n#' @param x Object of class \\code{\\link{Modul}}\n#' @param value New Value\n#' @param name Name of the child element\n#' @return Setter Method: object of class \\code{\\link{Modul}}\n#' @export\n#' @examples\n#' data1<-Modul(\"Data\")\n#' model1<-Modul(\"Model\")\n#' map1<-Modul(\"Map\")\n#' map2<-Modul(\"Map\")\n#' \n#' children(data1,\"map1\")<-map1\n#' children(data1,\"map2\")<-map2\n#' children(model1,\"data1\")<-data1\n#'\n#' summary(model1)\n#' \n'children<-.Modul' <- function(x, value, name=NULL){\n  oldSize<-length(x)\n  x[[oldSize+1]]<-value\n  names(x)[[oldSize+1]]<-name\n  return(x)\n}\n\n#' summary\n#' \n#' Generic summary method\n#' \n#' @param x Object of class \\code{\\link{Modul}}\n#' \nsummary <- function(x){\n  UseMethod('summary',x)\n}\n\n#' summary.Modul\n#'\n#' Print a summary of the metadata tree.\n#'\n#' @param  x Object of class \\code{\\link{Modul}}\n#' @seealso \\code{\\link{Modul}}\n#' @export\nsummary.Modul <- function(x){\n  cat(info(x,name=deparse(substitute(x))))\n}\n\n\n#generic method. Is called by summary method and calls itself recursively for each child object.\ninfo <- function(x,indentation,name){\n  UseMethod('info', x)\n}\n\n\n#implemenation for class Modul\ninfo.Modul <- function(x,indentation = 0,name=\"unknown Name\"){\n\n  output = NULL\n  indSeq<-\"|\\t\"\n  seperator<-paste(\"\\n\",paste(rep(indSeq,indentation),collapse=\"\"),sep=\"\")\n  \n  output<-paste(output,paste(x$type,name,sep=\": \"),sep=paste(rep(indSeq,indentation),collapse=\"\"))\n  \n  if(!is.null(x$attributes)){ #show attributes\n    output<-paste(output,\"+ Attributes:\",sep=seperator)\n    for(i in 1:length(x$attributes)){\n      newString=NULL\n      if(toString(names(x$attributes)[i])==\"\"){ #not a key value pair, just show the value\n        newString<-x$attributes[i]\n      } else { #it's a key value pair\n        newString<-paste(c(names(x$attributes)[i],x$attributes[i]),collapse=\" = \")\n      }\n      \n      if(nchar(newString)>100){\n        newString<-paste(c(\"|\\t>\",substr(newString,1,100)),collapse = \" \")\n        newString<-paste(newString,\"...\",sep=\",\")\n      } else {\n        newString<-paste(c(\"|\\t>\",newString),collapse = \" \")\n      }\n      \n      output<-paste(output,newString,sep=seperator) #add the entry (key value pair, or just a value)\n    }\n  }\n\n  #process child elements\n  if(length(x)>2){\n    numberOfElements<-length(x)-2\n    output<-paste(output,paste(c(\"+ nested elements:\",toString(numberOfElements)),collapse = \"\"),sep=seperator)\n    for(i in 3:length(x)){\n      childName<-names(x[i])\n      output<-paste(output,info(x[[i]],indentation+1,childName),sep=seperator)\n    }\n  }\n  \n  return(output)\n}\n\n#' toXML\n#' \n#' Export to XML.\n#' \n#' Please note: As the name of an element of class Module is stored in its parent element, the topmost element has no name.\n#' By default, the local variable name is used for export to XML. An alternative name can be set using the name parameter.\n#' \n#' @param x Object of class \\code{\\link{Modul}}\n#' @param name Name of the parent node. As default, the Variablename is used.\n#' @param file Path to the XML file\n#' @seealso \\code{\\link{Modul}}, \\code{\\link{summary.Modul}}, \\code{\\link{parseXML}}, \\code{\\link{getFirstModuleName}}\n#' @export\n#' \ntoXML <- function (x,name,file){\n  UseMethod('toXML',x)\n}  \n\n#' @describeIn toXML\n#' @export\n#' \ntoXML.Modul <- function(x,name=NULL,file=NULL){\n  if(is.null(name)){\n    name<-deparse(substitute(x))\n  }\n  \n  content<-toXMLRec(x,name)\n\n  root<-XML.xmlNode(name = \"sdmMetadata\")#, namespaceDefinitions = c(\"http://www.w3.org/2001/XMLSchema-instance\", \"noNamespaceSchemaLocation\" = \"sdm.xsd\") )\n  XML.xmlAttrs(root,T)<- c( \"xmlns:xsi\"=\"http://www.w3.org/2001/XMLSchema-instance\", \"xsi:noNamespaceSchemaLocation\"=\"sdm.xsd\")\n\n  root<-append.xmlNode(root,content)\n  if(is.null(file)){\n    return(root)\n  } else {\n    saveXML(doc = root, file = file, indent = TRUE)\n  }\n}\n\n#' toXMLRec\n#' \n#' private helper function for toXML\n#' \n#' @param x Node (of type Modul) to be exported in XML\n#' @param name Name of parent node. By default, this is the variable name where the object is stored.\n#' @return xml node\n#' \ntoXMLRec <- function(x,name=NULL){\n  \n  node <- XML::xmlNode(class(x)[[1]])\n  if (is.null(name)) name=\"undefined\"\n  XML::xmlAttrs(node) <- c(Name = name, Type = type(x))\n  \n  #process attributes\n  if(!is.null(x$attributes)){ \n    numberOfAttributes <- length(x$attributes)\n    attrNode<-XML::xmlNode(\"Attributes\")\n    for(i in 1 : numberOfAttributes){\n      key<-NULL \n      value<-NULL\n      if(toString(names(x$attributes)[i])==\"\"){ #not a key value pair, just show the value\n        key<-paste(\"attrib\",toString(i),sep=\"\") #use a fake key in xml (attrib1, attrib2, attrib3, etc)\n        value<-x$attributes[i]\n      } else { #it's a key value pair\n        key<-names(x$attributes[i])\n        value<-x$attributes[i]\n      }\n      tmpAttributeNode<-XML::xmlNode(\"Attribute\")\n      tmpAttributeNode<-XML::append.xmlNode(tmpAttributeNode,xmlNode(\"Key\",value=key))\n      tmpAttributeNode<-XML::append.xmlNode(tmpAttributeNode,xmlNode(\"Value\",value=value))\n      attrNode<-XML::append.xmlNode(attrNode,tmpAttributeNode)  \n    }\n    node<-XML::append.xmlNode(node,attrNode)\n  }\n\n  #process nested children\n  if (length(x)>2){\n    for (i in 3 : length(x)){\n      name=names(x[i])\n      child<-toXMLRec(x[[i]],name=name)\n      node<-XML::append.xmlNode(node,child)\n    }  \n  }\n \n  return(node)\n}\n\n\n# parseXML <- function(file){\n#   UseMethod(\"parseXML\",file)\n# }\n\n#' parseXML\n#' \n#' Parse a tree of Module-object from an XML file.\n#' \n#' Please note: As the name of an element of class Module is stored in its parent element, the topmost element has no name.\n#' By default, the local variable name is used for export to XML. However, the topmost elements name is getting lost during \n#' parsing the XML file. The original name from the XML file can be identified with method \\code{\\link{getFirstModuleName}}.\n#' \n#' @param file Path to the XML file\n#' @return Object of class \\code{\\link{Modul}}\n#' @seealso \\code{\\link{Modul}}, \\code{\\link{summary.Modul}}, \\code{\\link{toXML}}, \\code{\\link{getFirstModuleName}}\n#' @export\n#'\nparseXML <- function(file=NULL){\n  doc<-xmlInternalTreeParse(file)\n  root<-xmlRoot(doc)\n  return(parseXMLrec(root))\n}\n\n\n# parseXMLrec <- function(node){\n#   UseMethod(\"parseXMLrec\",node)  \n# }\n\n\n#' parseXMLrec\n#' \n#' private helper function for parseXML\n#' \n#' @param node Node to inspect recursively\n#' @return Object of class \\code{\\link{Modul}}\n#' \nparseXMLrec <- function(node=NULL){\n  \n  nodeType<-xmlName(node) # should be sdmMetadata or Modul\n    \n  if(nodeType == \"Modul\") {\n    xmlAttr<-xmlAttrs(node)\n    object<-Modul(type = xmlAttr[\"Type\"])\n  } else if (nodeType == \"sdmMetadata\"){ \n    print(paste(\"Original name of first element: \",xmlAttrs(xmlChildren(node)[[1]])[\"Name\"],sep=\"\"))    \n    return(parseXMLrec(xmlChildren(node)[[1]]))\n  } else { #error\n    print(\"unknown type\")\n    return(NULL)\n  }\n  \n  childrenList<-xmlChildren(node)\n  if(length(childrenList)>=1){\n    for (i in 1:length(childrenList)){\n      if (xmlName(childrenList[[i]]) == \"Attributes\") { #Process attributes\n        attrList<-xmlChildren(childrenList[[i]]) #List of Attribute nodes\n        for (j in 1:length(attrList)){\n          attr<-attrList[[j]]\n          tmp<-list() \n          tmp[[ xmlValue(xmlChildren(attr)[[1]]) ]] <- xmlValue(xmlChildren(attr)[[2]]) \n          attribute(object,T) <- tmp\n        }\n      } else if (xmlName(childrenList[[i]]) == \"Modul\"){\n        newChild<-parseXMLrec(childrenList[[i]])\n        name<-xmlAttrs(childrenList[[i]])[\"Name\"]\n        children(object,name)<-newChild\n      } \n    }\n  }\n  \n  return(object)\n  \n}\n\n\n#' getFirstModuleName\n#' \n#' Return the name of the topmost Module node of the XML file. \n#' \n#' @param file Path to the XML file\n#' @return Name of the Module element\n#' @seealso \\code{\\link{Modul}}, \\code{\\link{summary.Modul}}, \\code{\\link{toXML}}, \\code{\\link{parseXML}}\n#' @export\ngetFirstModuleName <- function(file=NULL){\n  doc<-xmlInternalTreeParse(file)\n  node<-xmlRoot(doc)\n  while(xmlName(node)!=\"Modul\"){\n    node<-xmlChildren(node)[[1]]\n  }\n  \n  return(xmlAttrs(node)[\"Name\"])\n}\n" }
{ "repo_name": "adamkucharski/subcritical_chains", "ref": "refs/heads/master", "path": "Supplement_code.R", "content": "# - - - - - - - - - - - - - - -\n# Supplementary code for:\n# Characterizing the transmission potential of zoonotic infections from minor outbreaks\n# Authors: Adam J. Kucharski and W. John Edmunds\n# - - - - - - - - - - - - - - -\n\n# - - - - - - - - - - - - - - -\n# 1 Set up simulation model and inference functions for R0 and susceptibility\n# - - - - - - - - - - - - - - -\n\n#  - - - - - - - - - - \n# 1a Define simulation model\n#  - - - - - - - - - - \n\nsimulate.data <- function (r0,dispk,mm,spillrates,simruns){\n\t# Max number of generations modelled\n\tgenrns=500\n  #Convert mixing vector into matrix for start in group 1\n\tmm01=matrix(mm, nrow = 2, ncol = 2, byrow = TRUE)\n\t# Transpose for start in group 2\n\tmm02=apply(apply(mm01, 1, rev),1,rev)\n  # Store simulation outputs\n\tsimsize=matrix(rep(1, 3*simruns), ncol = 3)\n\t\n# Simulate multiple outbreaks and store distributions\n\t\n\tfor(nn in 1:simruns){\n\t\t\n\t\tif(runif(1)<spillrates[1]){seedgp=1}else{seedgp=2} # randomly choose which group spillover occurs in\n\t\tif(seedgp==1){mm0=mm01}else{mm0=mm02} # Set appropriate matrix\n\t\t\n\t\tr00=r0/max(eigen(mm0)$values) # Normalise matrix so dominant eigenvalue=R0\n\t\tmm=r00*mm0\n\t\t# Define the model\n\t\tcasemodel <- function (genrns, dispk, mm){ \n\t\t\t\n\t\t\tcases1=vector(len=genrns) #number each generation\n\t\t\tcases2=vector(len=genrns) #number each generation\n\t\t\td_ind=1;\n\t\t\tfor (d in 1:genrns){\n\t\t\t\t\n\t\t\t\tif (d==1){\n\t\t\t\t\tcases1[d]=1\n\t\t\t\t\tcases2[d]=0\n\t\t\t\t}else{\n\t\t\t\t\tcases1[d]=sum(rnbinom(cases1[d-1],size=dispk,mu=mm[1,1]))+sum(rnbinom(cases2[d-1],size=dispk,mu=mm[2,1]))\n\t\t\t\t\tcases2[d]=sum(rnbinom(cases1[d-1],size=dispk,mu=mm[1,2]))+sum(rnbinom(cases2[d-1],size=dispk,mu=mm[2,2]))\n\t\t\t\t}# number of new cases in that generation\n\t\t\t}\n\t\t\tcbind(cases1,cases2)\n\t\t}\n    # Simulate model\n\t\tsimcase=casemodel(genrns, dispk, mm)\n\t\t\n\t\tif(seedgp==1){simsize[nn,]=c(seedgp,sum(simcase[,1]),sum(simcase[,2]))}else\n\t\t{simsize[nn,]=c(seedgp,sum(simcase[,2]),sum(simcase[,1]))}\n\t}\n\t\n\tsimsize\n}\n\n#  - - - - - - - - - - \n# 1b Define inference model\n#  - - - - - - - - - - \n\n# START Outbreak size distribution - multi-type model\n\nprobrn1n2<- function(seedgp,n01,n02,rstar,mm){\n\n# Set up matrices for different spillover groups\n\t\nmm01=matrix(mm, nrow = 2, ncol = 2, byrow = TRUE)\nmm02=apply(apply(mm01, 1, rev),1,rev)\n\t\nif(seedgp==1){mm0=mm01}else{mm0=mm02}\nif(seedgp==1){n1=n01;n2=n02}else{n2=n01; n1=n02}\n\t\nr00=rstar/max(eigen(mm0)$values)\nmm=r00*mm0\n\n# Make sure that sum limits are well defined\nif(n2==0|mm[2,1]==0){k1max=1}else{k1max=n1} \n\n# Store contributions to probability\nprob.comb=array(1, dim=c(k1max,n2+1))\n\nfor(kk1 in 1:k1max){\n\tfor(kk2 in 1:(n2+1)){\n\t\t\n\t\ta12=kk2-1 #set cases in group 2 caused by group 1\n\t\tif(mm[2,1]==0){a21=0}else{a21=(kk1-1)} #set cases in group 1 caused by group 2\n\t\tif (a21>n1-1 | a12>n2){stop('Need k<n')}\n\t\tnn=c(n1,n2)\n\t\tgg=matrix(c(n1-a21-1,a12,a21,n2-a12), nrow = 2, ncol = 2, byrow = TRUE)\n\t\t\n# offspring function\n\t\tgen.ij<- function(i,j,c1){ \n\t\t\tt1=nn[i]\n\t\t\t# get product\n\t\t\tprod1=rep(1,c1)\n\t\t\tfor(z in 0:(c1-1)){prod1[z+1]<-(t1+z)/(factorial(c1)^(1/(c1-1)))}\n\t\t\tif(c1==0){prod2=1}else{prod2=prod(prod1)}\n\t\t\t# probability t1 infectives generate c1 cases \n\t\t\tprod2*mm[i,j]^c1*(1+mm[i,j])^(-t1-c1)\n\t\t}\n\t\t\n# offspring probability\n\t\tprob<- function(n2t,a21t){\n\t\t\tif (n2t==0){\n\t\t\t\tgen.ij(1,1,gg[1,1])*gen.ij(1,2,0)/(nn[1])\n\t\t\t}else{\n\t\t\t\tif (n2t>0 & a21t==0){\n\t\t\t\t\ta12*gen.ij(1,1,gg[1,1])*gen.ij(1,2,gg[1,2])*gen.ij(2,1,0)*gen.ij(2,2,gg[2,2])/(nn[1]*nn[2])\n\t\t\t\t}else{\n\t\t\t\t\ta12*gen.ij(1,1,gg[1,1])*gen.ij(1,2,gg[1,2])*gen.ij(2,1,gg[2,1])*gen.ij(2,2,gg[2,2])/(nn[1]*nn[2])\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tprob.comb[kk1,kk2]=prob(n2,a21)\n\t}\n}\nsum(prob.comb) # total probability n1 cases in group 1 & n2 cases in group 2\n}\n\n# END Outbreak size distribution - multi-type model\n\n\n\n# START LIKELIHOOD FUNCTION - multi-type model\n\nlikelihoodM1<- function(simdata,runs,rstar,mm){\n\tliks=rep(1,runs)\n\tfor(z in 1:runs){liks[z]=probrn1n2(simdata[z,1],simdata[z,2],simdata[z,3],rstar,mm)}\n\tsum(log(liks))\n}\n# END LIKELIHOOD FUNCTION - multi-type model\n\n\n# START LIKELIHOOD FUNCTION - single-type model\nhg.rj<- function(j,rstar){ \n\t# get product\n\tprod1=rep(1,j)\n\tfor(z in 0:(j-2)){prod1[z+1]<-j+z}\n\tif(j==0){prod2=1}else{prod2=prod(prod1)}\n\t# probability t1 infectives generate c1 cases\n\tprod2*rstar^(j-1)*(1+rstar)^(-2*j+1)/factorial(j)\n}\n\nlikelihoodHG<- function(simdata,runs,rstar,mm){\n\t\n\tliks=rep(1,runs)\n\tfor(z in 1:runs){liks[z]=hg.rj(simdata[z,2]+simdata[z,3],rstar)}\n\tsum(log(liks))\n}\n# END LIKELIHOOD FUNCTION - single-type model\n\n\n\n# Define functions to find point estimate for R0 and S using multi-type model\n\nR0estimateM1v<- function(simdata,mm,rtest,susc){\n\tsimdata=data1\n\tmm=infmatrix\n  # Define boundaries of likelihood calculation\n\tR0r1=0; R0r2=2; R0range=seq(R0r1,R0r2,by=.001)\n\tkkr1=0; kkr2=1; kkrange=seq(kkr1,kkr2,by=.001)\n\tcollect1 <- data.frame(matrix(NA, nrow=length(R0range),length(kkrange)))\n\tfor(ii in 1:length(R0range)){\n\t\tfor(jj in 1:length(kkrange)){\n\t\t\truns1=length(simdata)/3\n\t\t\trr=R0range[ii]; v1=kkrange[jj]\n\t\t\tmm2=mm; mm2[2]=mm[2]*v1; mm2[4]=mm[4]*v1\n\t\t\tcollect1[ii,jj]=likelihoodM1(simdata,runs1,rr,mm2)\n\t\t}\n\t}\n\tlikm=(collect1==max(collect1))\n\tr0max=max(seq(1,length(R0range))%*%likm)\n\tc(R0range[r0max],kkrange[max(likm[r0max,]*seq(1,length(kkrange)))])\n}\n\n# Find point estimate for R0 using single-type model\n\nR0estimateHG<- function(simdata,mm){\n\tmeansize=mean(simdata[,2]+simdata[,3])\n\t1-1/meansize\n}\n\n\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n# 2 Simulate data, perform inference of R0 & S, and calculate mean squared & absolute error\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\n# Define 4 different scenarios for R0 and relative susceptibility of udner 20 age group:\nsusctab=c(0.25,1,0.25,1)\nrbasictab=c(0.25,0.25,0.75,0.75)\n\n# Number of outbreak clusters to simulate\nireps=50\n\n# Data frame to store outputs\nR0errortable<-data.frame(matrix(NA, nrow=4,ncol=7))\nnames(R0errortable)=c(\"R0\",\"vac\",\"relerrorM1\",\"relerrorHG\",\"abserrorM1\",\"abserrorHG\",\"Reff\")\n\nfor(kk in 1:4){\n\n\tsusc=susctab[kk]\n\trbasic=rbasictab[kk]\n\tR0table=array(NA, dim=c(ireps,4))\n\t\n\t# Define matrix used for inference. In form: from i to i, from i to j, from j to i, from j to j\n\tinfmatrix=c(4.27, 3.04, 1.32, 2.70)\n\t\n\t# Define matrix used for simulation\n\tsimmatrix0=c(4.27, 3.04, 1.32, 2.70)\n\tsimmatrix=c(4.27,3.04*susc,1.32,2.70*susc)\n              \n\t # Define probability of spillover into each group based on UK population distribution\n  seeds=c(0.24/(0.24+0.76*susc), 0.76*susc/(0.24+0.76*susc))\n\t\n  # Scale matrix depending on R0\n\tmm01=matrix(simmatrix0, nrow = 2, ncol = 2, byrow = TRUE)\n\tr00=rbasic/max(eigen(mm01)$values)\n\t\n\tmm02=r00*matrix(simmatrix, nrow = 2, ncol = 2, byrow = TRUE)\n\trtest=max(eigen(mm02)$values)\n  \n  # Simulate data and infer parameters\n\t\n\tfor(ii in 1:ireps){\n\t\tdata1=simulate.data(rtest, 1, simmatrix, seeds, 50)\n\t\trtab1=R0estimateM1v(data1,infmatrix,round(rtest, digits = 2),susc)\n\t\trtab2=R0estimateHG(data1,infmatrix)\n\t\t\n\t\tmm3=infmatrix; mm3[2]=infmatrix[2]*rtab1[2]; mm3[4]=infmatrix[4]*rtab1[2]\n\t\tmm3a=matrix(mm3, nrow = 2, ncol = 2, byrow = TRUE)\n\t\tmm5=matrix(infmatrix, nrow = 2, ncol = 2, byrow = TRUE)\n\t\t\n\t\tR0table[ii,1]=rtab1[1]\n\t\tR0table[ii,2]=rtab2[1]\n\t\tR0table[ii,3]=rtab1[2]\n\t\tR0table[ii,4]=rtab1[1]*max(eigen(mm3a)$values)/max(eigen(mm5)$values)\n\t\t}\n\t\t\n\tR0table1=na.omit(R0table)\n\t\t\t\n\tR0errortable[kk,1]=mean(R0table1[,4])\n\tR0errortable[kk,2]=mean(R0table1[,3])\n\tR0errortable[kk,3]=sqrt(sum(((R0table1[,1]-rtest)/rtest)^2)/ireps) # relative MSE\n\tR0errortable[kk,4]=sqrt(sum(((R0table1[,2]-rtest)/rtest)^2)/ireps) # relative MSE\n\tR0errortable[kk,5]=sum(R0table1[,1]-rtest)/ireps # abs error\n\tR0errortable[kk,6]=sum(R0table1[,2]-rtest)/ireps # abs error\n\tR0errortable[kk,7]=rtest\n\t\t\t\n\twrite.csv(R0table1,paste(\"Vinfer_R0table\",kk,\".csv\",sep=\"\"))\n\twrite.csv(R0errortable,paste(\"Vinfer_.csv\",sep=\"\"))\n\n}\n\n" }
{ "repo_name": "DukeSynthProj/DukeSynthJASA2017", "ref": "refs/heads/master", "path": "SyntheticData/Code/03.02.00.Generator_Race.R", "content": "setwd(\"~/Path/SyntheticData\")\n\nload(\"Synthetic_Files/data_synthetic_race.RData\")\nload(\"Synthetic_Files/bs_synthetic_agency.RData\")\n\nload(\"Aux_Files/data_race.RData\")\nload(\"Aux_Files/bs_agency.RData\")\nload(\"Aux_Files/bs_race.RData\")\nload(\"Aux_Files/bs_eribridge.RData\")\nfor(j in 1:24)\n{\n  bs_race[is.na(bs_race[,j]),j]=\"NA.race\"\n  bs_race[bs_race[,j]==\"NA\",j]=\"NA.race\"\n  bs_eribridge[is.na(bs_eribridge[,j]),j]=\"NA.eribridge\"\n  bs_eribridge[bs_eribridge[,j]==\"NA\",j]=\"NA.eribridge\"\n}\n\n\nlibrary(parallel)\nlibrary(tree)\n\nsource(\"Code/99.02.cartdraw.R\")\nsource(\"Code/99.03.cartdraw2.R\")\n\n\n###### Determining whether Race is unique or not\ndata_race_backup    = data_race\nbs_agency_backup    = bs_agency\nbs_race_backup      = bs_race\nbs_eribridge_backup = bs_eribridge\n\ndata_synthetic_race_backup = data_synthetic_race\nbs_synthetic_agency_backup = bs_synthetic_agency\n\nbs_synthetic = bs_synthetic_agency[,1:18]\nInd_unique_race = simplify2array(mclapply(1:nrow(bs_synthetic),function(j){length(unique(bs_synthetic[j,bs_synthetic[j,]!=\"0\"]))},mc.cores=48))\n\ndata_race = data_race[data_race$unique_race != \"NS\",]\ndata_synthetic_race = data_synthetic_race[Ind_unique_race != 0,]\n\n#### Collapsed variables -- used to split the data\n#Previos race\ncollapse = cbind(\n  as.vector(data_race$init_year), \n  as.vector(data_race$init_agency),\n  as.vector(data_race$gender))\ncollapse = simplify2array(mclapply(1:nrow(data_race),function(j){paste(collapse[j,],collapse=\"-\")},mc.cores=48))\n\nsynthetic_collapse = cbind(\n  as.vector(data_synthetic_race$init_year), \n  as.vector(data_synthetic_race$init_agency),\n  as.vector(data_synthetic_race$gender))\nsynthetic_collapse = simplify2array(mclapply(1:nrow(data_synthetic_race),function(j){paste(synthetic_collapse[j,],collapse=\"-\")},mc.cores=48))\n\nprint(cbind(head(collapse,20),head(synthetic_collapse,20)))\nprint(mean(synthetic_collapse %in% collapse))\n\ndata_race = data.frame(data_race,collapse=collapse)\ndata_synthetic_race = data.frame(data_synthetic_race,collapse=synthetic_collapse)\n\nvariables = c(\"init_year\",\"last_year\",\"total_year\",\"mode_agency\",\"init_agency\",\n              \"number_agency\",\"M\",\"gaps\",\"gender\",\"collapse\")\n\n\nsynthetic_unique_race = matrix(\"0\",nrow=nrow(data_synthetic_race),ncol=1) \n\n\ndata_race_tmp = data.frame(data_race)\ndata_synthetic_race_tmp = data.frame(data_synthetic_race)\n\n\nInd_collapse = data_race_tmp$init_agency != \"0\"\nInd_synthetic_collapse = data_synthetic_race_tmp$init_agency != \"0\"\n\n## Start Algorithm\ndata_general=data_race_tmp[Ind_collapse,,drop=FALSE]\ndata_synthetic_general=data_synthetic_race_tmp[Ind_synthetic_collapse,,drop=FALSE]\nGoal=\"unique_race\"\nvariables=variables\nN.samples=1\n\n\n# Getting collapse for mono_table, mono_tree and multi_tree\ncores.table=30\nsource(\"Code/03.02.01.Init_mono_multi.R\")\n\n\n# Matching with mono_table\npre.cores.table = 20\ncores.table = 30\nsplit.mono.table=15000\ncores.mono.table=40\nsource(\"Code/03.02.03.mono_table.R\")\n\n\n# Fitting mono_tree\nsplit.mono.tree=10000\ncores.mono.tree=30\nN.cartdraw = 10000\ncores.cartdraw = 2\nsource(\"Code/03.02.04.mono_tree.R\")\n\n\n# End\nsynthetic_unique_race = synthetic_Goal\nsave(synthetic_unique_race,file=paste(\"Synthetic_Files/synthetic_unique_race.RData\",sep=\"\"))\n\nprint(\"XXXXXXXXXXXXXXXX\");print(\"XXXXXXXXXXXXXXXX\");print(\"unique_race\")\nprint(\"XXXXXXXXXXXXXXXX\");print(\"XXXXXXXXXXXXXXXX\")\nprint(table(synthetic_unique_race))\nprint(table(data_race$unique_race))\n\n\n#########################################################\n#########################################################\n\n###### Determining Race when race is unique\ndata_race = data_race_backup\ndata_synthetic_race = data_synthetic_race_backup \n\ndata_race = data_race[data_race$unique_race ==\"U\", ]\ndata_synthetic_race = data_synthetic_race[Ind_unique_race != 0, ]\ndata_synthetic_race = data_synthetic_race[synthetic_unique_race ==\"U\", ]\n\n\n#### Collapsed variables -- used to split the data\n#Previos race\ncollapse = cbind(\n  as.vector(data_race$init_year), \n  as.vector(data_race$init_agency))\ncollapse = simplify2array(mclapply(1:nrow(data_race),function(j){paste(collapse[j,],collapse=\"-\")},mc.cores=48))\n\nsynthetic_collapse = cbind(\n  as.vector(data_synthetic_race$init_year), \n  as.vector(data_synthetic_race$init_agency))\nsynthetic_collapse = simplify2array(mclapply(1:nrow(data_synthetic_race),function(j){paste(synthetic_collapse[j,],collapse=\"-\")},mc.cores=48))\n\nprint(cbind(head(collapse,20),head(synthetic_collapse,20)))\nprint(mean(synthetic_collapse %in% collapse))\n\ndata_race = data.frame(data_race,collapse=collapse)\ndata_synthetic_race = data.frame(data_synthetic_race,collapse=synthetic_collapse)\n\nvariables = c(\"init_year\",\"last_year\",\"total_year\",\"mode_agency\",\"init_agency\",\n              \"number_agency\",\"M\",\"gaps\",\"gender\",\"collapse\")\n\n\nsynthetic_init_race = matrix(\"0\",nrow=nrow(data_synthetic_race),ncol=1) \n\n\ndata_race_tmp = data.frame(data_race)\ndata_synthetic_race_tmp = data.frame(data_synthetic_race)\n\n\nInd_collapse = data_race_tmp$init_agency != \"0\"\nInd_synthetic_collapse = data_synthetic_race_tmp$init_agency != \"0\"\n\n## Start Algorithm\ndata_general=data_race_tmp[Ind_collapse,,drop=FALSE]\ndata_synthetic_general=data_synthetic_race_tmp[Ind_synthetic_collapse,,drop=FALSE]\nGoal=\"init_race\"\nvariables=variables\nN.samples=1\n\n\n# Getting collapse for mono_table, mono_tree and multi_tree\ncores.table=30\nsource(\"Code/03.02.01.Init_mono_multi.R\")\n\n\n# Matching with mono_table\npre.cores.table = 20\ncores.table = 30\nsplit.mono.table=15000\ncores.mono.table=40\nsource(\"Code/03.02.03.mono_table.R\")\n\n\n# Fitting mono_tree\nsplit.mono.tree=10000\ncores.mono.tree=30\nN.cartdraw = 10000\ncores.cartdraw = 2\nsource(\"Code/03.02.04.mono_tree.R\")\n\n\n# End\nsynthetic_init_race = synthetic_Goal\nsave(synthetic_init_race,file=paste(\"Synthetic_Files/synthetic_init_race.RData\",sep=\"\"))\n\nprint(\"XXXXXXXXXXXXXXXX\");print(\"XXXXXXXXXXXXXXXX\");print(\"init_race\")\nprint(\"XXXXXXXXXXXXXXXX\");print(\"XXXXXXXXXXXXXXXX\")\nprint(table(synthetic_init_race))\nprint(table(data_race$init_race))\n\n\n#########################################################\n#########################################################\n# YEAR 1\n###### Determining Race when race is not unique\ndata_race = data_race_backup\nbs_agency = bs_agency_backup\nbs_race   = bs_race_backup\nbs_eribridge = bs_eribridge_backup\n\ndata_synthetic_race = data_synthetic_race_backup \nbs_synthetic_agency = bs_synthetic_agency_backup\n\n\nbs_agency = bs_agency[data_race$unique_race ==\"NU\", ]\nbs_race   = bs_race[data_race$unique_race ==\"NU\", ]\nbs_eribridge = bs_eribridge[data_race$unique_race ==\"NU\", ]\nbs_race = cbind(bs_race[,1:18],bs_eribridge[,19:20])\n\ndata_race = data_race[data_race$unique_race ==\"NU\", ]\n\ndata_synthetic_race = data_synthetic_race[Ind_unique_race != 0, ]\ndata_synthetic_race = data_synthetic_race[synthetic_unique_race ==\"NU\", ]\n\nbs_synthetic_agency = bs_synthetic_agency[Ind_unique_race != 0, ]\nbs_synthetic_agency = bs_synthetic_agency[synthetic_unique_race ==\"NU\", ]\n\nbs_synthetic_race = bs_synthetic_agency\n\nJ1 = 1\n\n#### Collapsed variables -- used to split the data\n#Previos race\ncollapse = cbind(\n  as.vector(data_race$init_year), \n  as.vector(bs_agency[,J1]))\ncollapse = simplify2array(mclapply(1:nrow(data_race),function(j){paste(collapse[j,],collapse=\"-\")},mc.cores=48))\n\nsynthetic_collapse = cbind(\n  as.vector(data_synthetic_race$init_year), \n  as.vector(bs_synthetic_agency[,J1]))\nsynthetic_collapse = simplify2array(mclapply(1:nrow(data_synthetic_race),function(j){paste(synthetic_collapse[j,],collapse=\"-\")},mc.cores=48))\n\nprint(cbind(head(collapse,20),head(synthetic_collapse,20)))\nprint(mean(synthetic_collapse %in% collapse))\n\ndata_race = data.frame(data_race,race = bs_race[,J1],agency = bs_agency[,J1],collapse=collapse)\ndata_synthetic_race = data.frame(data_synthetic_race,agency = bs_synthetic_agency[,J1],collapse=synthetic_collapse)\n\nvariables = c(\"init_year\",\"last_year\",\"total_year\",\"mode_agency\",\"init_agency\",\n              \"number_agency\",\"M\",\"gaps\",\"gender\",\"collapse\")\n\n\ndata_race_tmp = data.frame(data_race)\ndata_synthetic_race_tmp = data.frame(data_synthetic_race)\n\n\nInd_collapse = data_race_tmp$agency != \"0\"\nInd_synthetic_collapse = data_synthetic_race_tmp$agency != \"0\"\n\n## Start Algorithm\ndata_general=data_race_tmp[Ind_collapse,,drop=FALSE]\ndata_synthetic_general=data_synthetic_race_tmp[Ind_synthetic_collapse,,drop=FALSE]\nGoal=\"race\"\nvariables=variables\nN.samples=1\n\n\n# Getting collapse for mono_table, mono_tree and multi_tree\ncores.table=30\nsource(\"Code/03.02.01.Init_mono_multi.R\")\n\n\n# Matching with mono_table\npre.cores.table = 20\ncores.table = 30\nsplit.mono.table=15000\ncores.mono.table=40\nsource(\"Code/03.02.03.mono_table.R\")\n\n\n# Fitting mono_tree\nsplit.mono.tree=10000\ncores.mono.tree=30\nN.cartdraw = 10000\ncores.cartdraw = 2\nsource(\"Code/03.02.04.mono_tree.R\")\n\n\n# End\nbs_synthetic_race[Ind_synthetic_collapse,J1] = synthetic_Goal\n\nprint(\"XXXXXXXXXXXXXXXX\");print(\"XXXXXXXXXXXXXXXX\");print(\"race year 1\")\nprint(\"XXXXXXXXXXXXXXXX\");print(\"XXXXXXXXXXXXXXXX\")\nprint(table(bs_synthetic_race[,J1]))\nprint(table(bs_race[,J1]))\n\n\nif(sum((bs_synthetic_race[,J1]==\"0\") - (bs_synthetic_agency[,J1] == \"0\"))>1)\n{\n  print(\"Error: More zeros in synthetic_race\")\n  break()\n}\n\n\n#########################################################\n#########################################################\n# YEAR >= 2\n###### Determining Race when race is not unique\nfor( J1 in 2:18)\n{\n  data_race = data_race_backup\n  bs_agency = bs_agency_backup\n  bs_race   = bs_race_backup\n  bs_eribridge = bs_eribridge_backup\n  \n  data_synthetic_race = data_synthetic_race_backup \n  bs_synthetic_agency = bs_synthetic_agency_backup\n  \n  \n  bs_agency = bs_agency[data_race$unique_race ==\"NU\", ]\n  bs_race   = bs_race[data_race$unique_race ==\"NU\", ]\n  bs_eribridge = bs_eribridge[data_race$unique_race ==\"NU\", ]\n  bs_race = cbind(bs_race[,1:18],bs_eribridge[,19:20])\n  \n  data_race = data_race[data_race$unique_race ==\"NU\", ]\n  \n  data_synthetic_race = data_synthetic_race[Ind_unique_race != 0, ]\n  data_synthetic_race = data_synthetic_race[synthetic_unique_race ==\"NU\", ]\n  \n  bs_synthetic_agency = bs_synthetic_agency[Ind_unique_race != 0, ]\n  bs_synthetic_agency = bs_synthetic_agency[synthetic_unique_race ==\"NU\", ]\n  \n  #bs_synthetic_race = bs_synthetic_agency\n  \n  f<-function(j)\n  {\n    tmp = \"0\"\n    if(sum(bs_race[j,1:(J1-1)]!=\"0\")>0)\n      tmp = tail(bs_race[j,1:(J1-1)][bs_race[j,1:(J1-1)]!=\"0\"],1)\n    tmp\n  }\n  last_race = simplify2array(mclapply(1:nrow(bs_race),f,mc.cores=48))\n  data_race = data.frame(data_race,last_race = last_race)\n  \n  f<-function(j)\n  {\n    tmp = \"0\"\n    if(sum(bs_synthetic_race[j,1:(J1-1)]!=\"0\")>0)\n      tmp = tail(bs_synthetic_race[j,1:(J1-1)][bs_synthetic_race[j,1:(J1-1)]!=\"0\"],1)\n    tmp\n  }\n  synthetic_last_race = simplify2array(mclapply(1:nrow(bs_synthetic_race),f,mc.cores=48))\n  data_synthetic_race = data.frame(data_synthetic_race,last_race = synthetic_last_race)\n  \n  \n  #### Collapsed variables -- used to split the data\n  #Previos race\n  collapse = cbind(\n    as.vector(bs_agency[,J1]),\n    as.vector(data_race$last_race))\n  collapse.1 = simplify2array(mclapply(1:nrow(collapse),function(j){paste(collapse[j,],collapse=\"-\")},mc.cores=48))\n  \n  synthetic_collapse = cbind(\n    as.vector(bs_synthetic_agency[,J1]),\n    as.vector(data_synthetic_race$last_race))\n  synthetic_collapse.1 = simplify2array(mclapply(1:nrow(synthetic_collapse),function(j){paste(synthetic_collapse[j,],collapse=\"-\")},mc.cores=48))\n  \n  \n  collapse = cbind(\n    as.vector(data_race$last_race))\n  collapse.2 = simplify2array(mclapply(1:nrow(data_race),function(j){paste(collapse[j,],collapse=\"-\")},mc.cores=48))\n  \n  synthetic_collapse = cbind(\n    as.vector(data_synthetic_race$last_race))\n  synthetic_collapse.2 = simplify2array(mclapply(1:nrow(data_synthetic_race),function(j){paste(synthetic_collapse[j,],collapse=\"-\")},mc.cores=48))\n  \n  print(cbind(head(collapse.1,20),head(synthetic_collapse.1,20)))\n  print(mean(synthetic_collapse.1 %in% collapse.1))\n  print(cbind(head(collapse.2,20),head(synthetic_collapse.2,20)))\n  print(mean(synthetic_collapse.2 %in% collapse.2))\n  \n  Collapse = cbind(\n    as.vector(collapse.1),\n    as.vector(collapse.2))\n  \n  Synthetic_Collapse = cbind(\n    as.vector(synthetic_collapse.1),\n    as.vector(synthetic_collapse.2))\n  \n  \n  Ind_collapse = matrix(FALSE,nrow = nrow(Collapse),ncol = ncol(Collapse))\n  Ind_synthetic_collapse = matrix(FALSE,nrow = nrow(Synthetic_Collapse),ncol = ncol(Synthetic_Collapse))\n  \n  # -- 1\n  Ind = Collapse[,1] %in% Synthetic_Collapse[,1]\n  unique_collapse = unique(Collapse[Ind])\n  \n  Ind_synthetic_collapse[,1] = Synthetic_Collapse[,1] %in% unique_collapse\n  Ind_collapse[,1] = Collapse[,1] %in% unique_collapse\n  print(apply(Ind_synthetic_collapse,2,sum))\n  \n  \n  for(j in 2:ncol(Synthetic_Collapse))\n  {\n    \n    Ind = Collapse[,j] %in% Synthetic_Collapse[,j] \n    unique_collapse = unique(Collapse[Ind,j])\n    tmp = unique(Synthetic_Collapse[apply(Ind_synthetic_collapse[,1:(j-1),drop=FALSE],1,sum)==0,j])\n    \n    unique_collapse = unique_collapse[unique_collapse %in% tmp]\n    \n    Ind_collapse[,j] = Collapse[,j] %in% unique_collapse\n    \n    Ind_synthetic_collapse[,j] = Synthetic_Collapse[,j] %in% unique_collapse & apply(Ind_synthetic_collapse[,1:(j-1),drop=FALSE],1,sum)==0\n    \n    print(apply(Ind_synthetic_collapse,2,sum)); print(table(apply(Ind_synthetic_collapse,1,sum)))\n  }\n  \n  \n  data_race = data.frame(data_race,race = bs_race[,J1],agency = bs_agency[,J1])\n  data_synthetic_race = data.frame(data_synthetic_race,agency = bs_synthetic_agency[,J1])\n  \n  variables = c(\"init_year\",\"last_year\",\"total_year\",\"mode_agency\",\"init_agency\",\n                \"number_agency\",\"M\",\"gaps\",\"gender\",\"agency\",\"collapse\")\n  \n  \n  for(J in ncol(Synthetic_Collapse):1)\n  {\n    if(sum( Ind_synthetic_collapse[,J]) > 0)\n    {\n      \n      print(\"XXXXXXXXXXXXXXXX\");print(\"XXXXXXXXXXXXXXXX\");print(paste(\"COLLAPSE\",J, \"race\",\"-- Year\",J1))\n      print(\"XXXXXXXXXXXXXXXX\");print(\"XXXXXXXXXXXXXXXX\")\n            \n      data_race_tmp = data.frame(data_race,collapse=Collapse[,J])\n      data_synthetic_race_tmp = data.frame(data_synthetic_race, collapse=Synthetic_Collapse[,J])\n      \n      \n      ## Start Algorithm\n      data_general=data_race_tmp[Ind_collapse[,J] ,,drop=FALSE]\n      data_general=data_general[data_general$race != \"0\" ,,drop=FALSE]\n      data_synthetic_general=data_synthetic_race_tmp[Ind_synthetic_collapse[,J] & bs_synthetic_agency[,J1]!=\"0\",,drop=FALSE]\n      Goal=\"race\"\n      variables=variables\n      N.samples=1\n      \n      \n      # Getting collapse for mono_table, mono_tree and multi_tree\n      cores.table=30\n      source(\"Code/03.02.01.Init_mono_multi.R\")\n      \n      \n      # Matching with mono_table\n      pre.cores.table = 20\n      cores.table = 30\n      split.mono.table=15000\n      cores.mono.table=40\n      source(\"Code/03.02.03.mono_table.R\")\n      \n      \n      # Fitting mono_tree\n      split.mono.tree=10000\n      cores.mono.tree=30\n      N.cartdraw = 10000\n      cores.cartdraw = 2\n      source(\"Code/03.02.04.mono_tree.R\")\n      \n      \n      # End\n      bs_synthetic_race[Ind_synthetic_collapse[,J] & bs_synthetic_agency[,J1]!=\"0\",J1] = synthetic_Goal\n      \n    }\n  }\n  \n  print(\"XXXXXXXXXXXXXXXX\");print(\"XXXXXXXXXXXXXXXX\");print(paste(\"race\",\"-- Year\",J1))\n  print(\"XXXXXXXXXXXXXXXX\");print(\"XXXXXXXXXXXXXXXX\")\n  print(table(bs_synthetic_race[,J1]))\n  print(table(bs_race[,J1]))\n  if(sum((bs_synthetic_race[,J1]==\"0\") - (bs_synthetic_agency[,J1] == \"0\"))>1)\n  {\n    print(\"Error: More zeros in synthetic_race\")\n    print(which(((bs_synthetic_race[,J1]==\"0\") - (bs_synthetic_agency[,J1] == \"0\"))>0))\n    break()\n  }\n}\n\n\nbs_synthetic_race_1.18 = bs_synthetic_agency_backup[,1:18]\nfor(J1 in 1:18)\n{\n  bs_synthetic_race_1.18[Ind_unique_race != 0, J1][synthetic_unique_race==\"U\"] = synthetic_init_race\n  bs_synthetic_race_1.18[bs_synthetic_agency_backup[,J1] == \"0\", J1] = \"0\"\n  bs_synthetic_race_1.18[Ind_unique_race != 0, J1][synthetic_unique_race==\"NU\"] = bs_synthetic_race[,J1]\n}\n\nif(sum((bs_synthetic_race_1.18==\"0\") - (bs_synthetic_agency_backup[,1:18] == \"0\"))>1)\n{\n  print(\"Error: More zeros in synthetic_race\")\n  print(which(((bs_synthetic_race_1.18==\"0\") - (bs_synthetic_agency_backup[,1:18] == \"0\"))>0))\n  break()\n}\n\n\nsave(bs_synthetic_race_1.18,file=paste(\"Synthetic_Files/bs_synthetic_race_1.18.RData\",sep=\"\"))\n\n\n" }
{ "repo_name": "ISRICWorldSoil/SoilGrids250m", "ref": "refs/heads/master", "path": "grids/NEO/NEO_images.R", "content": "## Download NEO images (monthly) from http://neo.sci.gsfc.nasa.gov/\n\nlibrary(XML)\nlibrary(RCurl)\nlibrary(snowfall)\nlibrary(raster)\ngdal.dir <- shortPathName(\"C:/Program files/GDAL\")\ngdal_translate <- paste0(gdal.dir, \"/gdal_translate.exe\")\ngdalwarp <- paste0(gdal.dir, \"/gdalwarp.exe\") \ngdalinfo <- paste0(gdal.dir, \"/gdalinfo.exe\")\n\n\n\n\n\n## via WMS:\nvar.name = c(\"MODAL2_M_AER_OD\")\nwms = \"http://neowms.sci.gsfc.nasa.gov/wms/wms?\"\ni = 1\n\nt1 <- newXMLNode(\"GDAL_WMS\")\nt1.v <- newXMLNode(\"Service\", attrs= c(name=\"WMS\"), parent=t1)\nt1.s <- newXMLNode(\"ServerUrl\", wms, parent=t1.v)\nt1.l <- newXMLNode(\"CoverageName\", var.name[i], parent=t1)\nxml.out <- paste(var.name[i], \".xml\", sep=\"\")\nsaveXML(t1, file=xml.out)\nsystem(paste(gdalinfo, xml.out))\n\n\nfor(i in c(\"MOD05_L2\", \"\")\nsystem(\"wget ftp://neoftp.sci.gsfc.nasa.gov/rgb/\")\nsystem(\"wget http://neo.sci.gsfc.nasa.gov/view.php?datasetId=MYDAL2_E_SKY_WV&date=2015-09-01\")\n\nsystem(paste('wget --user-agent=\\\"Googlebot/2.1 (+http://www.googlebot.com/bot.html)\\\" --accept \\\"*.TIF\\\" -r \\\"http://neo.sci.gsfc.nasa.gov/view.php?datasetId=MYDAL2_M_SKY_WV\\\"'))\n\nsystem(paste('wget --user-agent=\\\"Googlebot/2.1 (+http://www.googlebot.com/bot.html)\\\" -r \\\"http://neo.sci.gsfc.nasa.gov/servlet/RenderData?si=1694760&cs=rgb&format=FLOAT.TIFF&width=3600&height=1800\\\"'))\n\nfor(i in c(2001, 2005, 2010, 2014)){\n  dr.lst <- normalizePath(list.dirs(path=paste0(\"X:\\\\MODIS\\\\6\\\\MOD05_L2\\\\\", i), recursive=FALSE))\n  for(j in 1:length(dr.lst)){\n    setwd(dr.lst[j])\n    x <- strsplit(dr.lst[j], \"\\\\\\\\\")[[1]][6]\n    system(paste0('wget --accept \\\"*.hdf\\\" -nd -N -r http://neo.sci.gsfc.nasa.gov/servlet/RenderData?si=1694760&cs=rgb&format=FLOAT.TIFF&width=3600&height=1800', i ,'/', x)) ## cut-dirs=4\n  }\n}\n" }
{ "repo_name": "Parin/naivebayes", "ref": "refs/heads/master", "path": "naive_bayes.R", "content": "train_naive_bayes <- function(alpha){\r\n  cat(\"training naive bayes with alpha : \", alpha, \" ...\\n\");\r\n  \r\n  p_x_given_y <- matrix(alpha, nrow=k, ncol=n);\r\n  p_y <- matrix(0, nrow=k);\r\n  \r\n  for(i in 1:k){\r\n    \r\n    p_x_given_y[i,] <- p_x_given_y[i,] + col_sums(train_data[which(train_label == i),]);\r\n    p_x_given_y[i,] <- p_x_given_y[i,] / sum(p_x_given_y[i,]);\r\n    p_y[i] <- length(train_label[train_label == i]);\r\n  }\r\n  \r\n  p_y <- p_y/sum(p_y);\r\n  result <- list(p_x_given_y=p_x_given_y, p_y=p_y);\r\n  return(result);\r\n}\r\n\r\npredict_naive_bayes <- function(params, data){\r\n  cat(\"predicting classes ...\\n\");\r\n  \r\n  predict <- log(params$p_x_given_y) %*% t(as.matrix(data)) +\r\n             as.matrix(log(params$p_y))[,rep(1,)];\r\n  \r\n  predicted_label <- apply(predict, 2, which.max);\r\n  return(predicted_label);\r\n}\r\n\r\ncompute_accuracy <- function(predicted_label){\r\n  accuracy <- (sum(predicted_label == test_label)/dim(test_data)[1])*100;  \r\n  cat(\"accuracy : \", accuracy, \"%\\n\");\r\n  return(accuracy);\r\n}\r\n\r\nparams <- train_naive_bayes(alpha);\r\npredicted_label <- predict_naive_bayes(params, test_data);\r\ncompute_accuracy(predicted_label);\r\nconfusion_matrix <- table(predicted_label, test_label);\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n" }
{ "repo_name": "KellyBlack/R-Object-Oriented-Programming", "ref": "refs/heads/master", "path": "chapter7/chapter_7_ex9.R", "content": "positions   <- complex(0) # Initialize the history of positions\r\ncurrentPos  <- 0.0+0.0i   # Start at the origin\r\nNUMBERSTEPS <- 50         # Number of steps to take\r\nangleFacing <- 0.0        # direction it is facing\r\nstdDev      <- 1.0        # std dev. of the change in the angle\r\n\r\nstep <- as.integer(0)\r\nrepeat\r\n    {\r\n        ## Add new people to the line and update the length\r\n        newAngle     <- angle + rnorm(1,0.0,stdDev)\r\n        proposedStep <- currentPos + exp(newAngle*1.0i)\r\n        if(Re(proposedStep) < 0.0)\r\n           next   # Ignore this step. It moves to neg. real parts\r\n\r\n        ## update the position\r\n        angle      <- newAngle\r\n        currentPos <- proposedStep\r\n        positions  <- c(positions,currentPos)\r\n\r\n        ## Update the current time step\r\n        step <- step + as.integer(1)\r\n\r\n        ## Check to see if it is time to stop\r\n        if(step>MAXNUMBER)\r\n            break\r\n    }\r\nplot(Re(positions),Im(positions),type=\"l\")\r\n" }
{ "repo_name": "ousuga/RelDists", "ref": "refs/heads/master", "path": "examples/examples_dFWE.R", "content": "## The probability density function\ncurve(dFWE(x, mu=0.75, sigma=0.5), from=0, to=3, \n      ylim=c(0, 1.7), col=\"red\", las=1, ylab=\"f(x)\")\n\n## The cumulative distribution and the Reliability function\npar(mfrow=c(1, 2))\ncurve(pFWE(x, mu=0.75, sigma=0.5), from=0, to=3, \n      col=\"red\", las=1, ylab=\"F(x)\")\ncurve(pFWE(x, mu=0.75, sigma=0.5, lower.tail=FALSE), \n      from=0, to=3, col=\"red\", las=1, ylab=\"R(x)\")\n\n## The quantile function\np <- seq(from=0, to=0.99999, length.out=100)\nplot(x=qFWE(p, mu=0.75, sigma=0.5), y=p, xlab=\"Quantile\",\n     las=1, ylab=\"Probability\")\ncurve(pFWE(x, mu=0.75, sigma=0.5), from=0, add=TRUE, col=\"red\")\n\n## The random function\nhist(rFWE(n=1000, mu=2, sigma=0.5), freq=FALSE, xlab=\"x\", \n     ylim=c(0, 2), las=1, main=\"\")\ncurve(dFWE(x, mu=2, sigma=0.5), from=0, to=3, add=TRUE, col=\"red\")\n\n## The Hazard function\npar(mfrow=c(1,1))\ncurve(hFWE(x, mu=0.75, sigma=0.5), from=0, to=2, ylim=c(0, 2.5), \n      col=\"red\", ylab=\"Hazard function\", las=1)\n" }
{ "repo_name": "maxwolffe/datasciencecoursera", "ref": "refs/heads/master", "path": "RProgramming/Homework1/pollutantmean.R", "content": "pollutantmean <- function(directory, pollutant, id = 1:332) {\n  ## 'directory' is a character vector of length 1 indicating\n  ## the location of the CSV files\n  \n  ## 'pollutant' is a character vector of length 1 indicating\n  ## the name of the pollutant for which we will calculate the\n  ## mean; either \"sulfate\" or \"nitrate\".\n  \n  ## 'id' is an integer vector indicating the monitor ID numbers\n  ## to be used\n    \n  average <- numeric()\n  count <- 0\n  \n  for (select_id in id){\n    file <- paste(directory, \"/\", sprintf(\"%03d\", select_id), \".csv\", sep = \"\")\n    file_data <- read.csv(file)\n    if (pollutant == \"nitrate\"){\n      average <- c(average, t(file_data[3][t(complete.cases(file_data[3]))]))\n    }\n    else {\n      average <- c(average, t(file_data[2][t(complete.cases(file_data[2]))]))\n    }  \n  }\n  \n  ## Return the mean of the pollutant across all monitors list\n  ## in the 'id' vector (ignoring NA values)\n  ## NOTE: Do not round the result!\n  print(mean(average))\n}\n" }
{ "repo_name": "tudob9/wfgtry", "ref": "refs/heads/master", "path": "R/wfgUtil.R", "content": "# utility functions for the WFG implementation\n\n# wfg.verbose = TRUE # flag: to show debug output\nwfg.verbose = FALSE\n\nto01 = function(x) { # (new) clamp vector x to 0-1-interval\n  x = x-x*(x>1)+(x>1)\n  x = x-x*(x<0)\n  return (x)\n}\n# to01(c(1.1, -0.2))\n\ndominates = function(x, y) {\n  return ( all(x<=y)&&any(x<y) )\n}\n\nshould01 = function(x) {\n  if ( any(x>1.0) | any(x<0.0) ) stop(paste(\"should be between 0 and 1. is: \", x))\n  return (x)\n}\nisNumOrNA = function(x) {\n  if(is.function(x)) return (FALSE) # is.na would show warning\n  return (is.na(x) || is.numeric(x))\n}\n\nshouldError = function(fct, ...) {\n  erred = FALSE\n  tryCatch( do.call(fct, list(...)), error = function(e) { erred <<- TRUE })\n  if(!erred) stop(\"should have thrown an error\")\n}\n\nshouldValues = function(vec) {\n  if (is.null(vec)) stop(\"should not be null\")\n  if (any(is.na(vec))) stop(\"should contain no NA's\")\n  # return (vec)\n  return (invisible(NULL))\n}\n\nrankMatrix = function(mat) {\n  nr = nrow(mat)\n  nc = ncol(mat)\n  return ( matrix(rank(as.vector(mat)), nrow=nr, ncol=nc) )\n}\n\nparseParams = function(strList, i) {\n  # i is the index of the current function. \n  # find the next function (or end):\n  beyond = i+1\n  len = length(strList)\n  while (beyond<=len) {\n    if(is.function(strList[[beyond]])) {\n      break\n    } else {\n      beyond = beyond + 1\n    }\n  }\n  if( i+1 > beyond-1 ) return (list())\n  return (strList[(i+1):(beyond-1)])\n}\n\n# tests:\n\naFunc = function() {}\nequal = function(listA, listB) { # to avoid all.equal's error \"invalid argument type\" when using lists of different length.\n  if( length(listA) != length(listB) ) return (FALSE)\n  namesA = names(listA); namesB = names(listB)\n  tmp2 = all.equal(listA, listB, check.names=TRUE)\n  return (tmp2==TRUE)\n}\n\n" }
{ "repo_name": "seltmann/AreaOfEndemism", "ref": "refs/heads/master", "path": "plant-host-endemism-code/plantHosts/plant-data-idigbio/geoAPI.R", "content": "#grabs all idigbio specimen records based on a list of genus and species names\n#kseltmann (enicospilus@gmail.com) August 2015\n\n#had to download mac binaries and install from desktop to get to work\n#install.packages(\"devtools\")\n#install_github(\"idigbio/ridigbio\")\n#install.packages(\"ridigbio\")\n\nlibrary(devtools)\nlibrary(ridigbio)\n\n\n#get a tab delimited list of names\nsetwd(\"~/Desktop/Dropbox/NEWareaOfEndemism/code/endemismCodeKatja/plantHosts/rawDatasetsV2\")\n\nhostplants <- read.delim(\"../plantHostsMissingData.tsv\", header = FALSE, stringsAsFactors = FALSE, sep = \"\\t\", quote = \"\\\"\")\nhostplants <- unique(hostplants)\nnrow(hostplants)\nhostplants <- cbind(hostplants,1:nrow(hostplants))\ncolnames(hostplants) <- c(\"genus\",\"specificepithet\",\"id\")\nhead(hostplants)\n\n#creates separate spreadsheet for all genus species in tab delimited list\nfor (x in hostplants$id){\nsubsetHosts <- subset(hostplants, id == x)\ng <- subsetHosts$genus\nsE <- subsetHosts$specificepithet\n\nquery <- list(genus=g,specificepithet=sE,geopoint=list(type=\"exists\"))\n  df <- idig_search_records(rq=query,fields=\"all\")\n\n#no georeference remarks? or establishmentMeans?\nallHosts <- df[c(\"uuid\",\"institutioncode\",\"catalognumber\",\"locality\",\"geopoint.lat\",\"geopoint.lon\",\"country\",\"stateprovince\",\"county\",\"municipality\",\"coordinateuncertainty\",\"family\",\"genus\",\"specificepithet\",\"infraspecificepithet\",\"scientificname\")]\n\ncolnames(allHosts) <- c(\"coreid\",\"institutioncode\",\"catalogNumber\",\"locality\", \"decimalLatitude\", \"decimalLongitude\", \"country\", \"stateProvince\", \"county\", \"municipality\",\"coordinateUncertaintyInMeters\",\"family\",\"genus\",\"specificEpithet\",\"infraspecificEpithet\",\"scientificName\")\n\ntitle <- paste(g,\"_\",sE,'.tsv',sep=\"\")\n\n#create a separate file for each genus/species\nwrite.table(allHosts, file=title, sep=\"\\t\", append = FALSE , row.names = FALSE, col.names = FALSE, qmethod = \"double\")\n\n#create one giant file\nwrite.table(allHosts, file=\"all.tsv\", sep=\"\\t\", append = TRUE , row.names = FALSE, col.names = FALSE)\n}\n\n#insert into mysql database using load data all.tsv\n\n#################################\n#################################\n\n\n## information below about the idigbio api and ridigbio\n#https://github.com/cran/ridigbio\n\n#returns all searchable fields\nidig_meta_fields()\n\n#find all functions in package\nlsp <- function(package, all.names = FALSE, pattern) \n{\n  package <- deparse(substitute(package))\n  ls(\n    pos = paste(\"package\", package, sep = \":\"), \n    all.names = all.names, \n    pattern = pattern\n  )\n}\n\nlsp(ridigbio)\n" }
{ "repo_name": "Oshlack/splatter", "ref": "refs/heads/master", "path": "tests/testthat/test-kersplat-simulate.R", "content": "context(\"Kersplat simulations\")\n\ntest_that(\"kersplatSimulate output is valid\", {\n    skip_if_not_installed(\"igraph\")\n    test.params <- newKersplatParams()\n    expect_true(validObject(kersplatSimulate(test.params)))\n})\n" }
{ "repo_name": "cran/NHMM", "ref": "refs/heads/master", "path": "R/OBIC.R", "content": "################################################################\r\n## Copyright 2014 Tracy Holsclaw.\r\n\r\n## This file is part of NHMM.\r\n\r\n## NHMM is free software: you can redistribute it and/or modify it under\r\n## the terms of the GNU General Public License as published by the Free Software\r\n## Foundation, either version 3 of the License, or any later version.\r\n\r\n## NHMM is distributed in the hope that it will be useful, but WITHOUT ANY\r\n## WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR\r\n## A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\r\n\r\n## You should have received a copy of the GNU General Public License along with\r\n## NHMM.  If not, see <http://www.gnu.org/licenses/>.\r\n#############################################################\r\n\r\n\r\n\r\n#' Calculates BIC, AIC, PLS, log-likelihood\r\n#'\r\n#' \\code{OBIC} calculates BIC, AIC, approximate log-likelihood and plots the\r\n#' log-likelihood for all iterations. The log-likelihood plot should \r\n#' be flat to show convergence to a stationary distribution. Minimize \r\n#' the AIC and BIC for the *best* model and maximize PLS.  The log likelihood is approximate in that\r\n#' it is calculated by marginalizing over the current chain of hidden states instead\r\n#' of using a recurrsive algorithm to compute it; every iterations produces an estimation\r\n#' of the log-likelihood.  If yhold is provided the preditive log score (PLS) is also \r\n#' given. \r\n#' \r\n#' Predictive Log Score: mean(log( E(p(yhold|...)))  The expectation is over all of the\r\n#' iterations of the algorithm.  And the mean is over the pT count of yhold. The scale of\r\n#' the PLS is in the unit of t (usually days). \r\n#' \r\n#' @param nhmmobj an object created from the NHMM function\r\n#' @param outfile a directory to put the .png plot\r\n#' @return BIC  \r\n#' @return output: AIC, BIC, PLS [if yhold data was provided], log-likelihood to the GUI and a plot of the log-likelihood\r\n#' @examples #OBIC(my.nhmm) \r\n\r\n\r\n\r\nOBIC=function(nhmmobj, outfile=NULL)\r\n{\r\n  T=nhmmobj$T\r\n  J=nhmmobj$J\r\n  K=nhmmobj$K\r\n  B=nhmmobj$B\r\n  A=nhmmobj$A\r\n  iters=nhmmobj$iters\r\n  burnin=nhmmobj$burnin \r\n  outboo=nhmmobj$outboo\r\n  outdir=nhmmobj$outdir\r\n  loglik=nhmmobj$loglik\r\n  BICp=nhmmobj$BICp\r\n  PLS=nhmmobj$PLS\r\n  \r\n  L=B+K\r\n  \r\n  \r\n  BICf=-2*max(loglik)+BICp*log(T)\r\n  AICf=-2*max(loglik)+2*BICp\r\n  print(paste(\"Parameter count:  \",BICp,sep=\"\"))\r\n  print(paste(\"BIC:  \",round(BICf,2), \" (favored method)\",sep=\"\"))\r\n  print(paste(\"AIC:  \", round(AICf,2),sep=\"\"))\r\n  if(!is.null(PLS)){print(paste(\"PLS:  \",round(PLS,2)))}\r\n  \r\n  if(!is.null(outfile)){ png(paste(outfile,\"BIC.png\",sep=\"\"), width=300, height=300)}\r\n  plot(loglik, xlab=\"Iterations\", ylab=\"Log-likelihood\")\r\n  if(!is.null(outfile)){ dev.off()}\r\n  BICf\r\n}  \r\n  \r\n  " }
{ "repo_name": "YoJimboDurant/latticePresentation", "ref": "refs/heads/master", "path": "xyArrows.R", "content": "require(latticeExtra)\nrequire(openair)\nlibrary(circular)\n\nSys.setenv(TZ='GMT')\n\ntestData <- selectByDate(mydata, start=\"1/7/1999\", end=\"2/7/1999\")\n\nplt <- xyplot(pm10~date, data=testData, type=\"g\", xlab=\"Date\", ylab=quickText(\"pm10 (ug.m-3)\"))\n\ndeltaX <- as.numeric(plt$x.limits[[2]]) - as.numeric(plt$x.limits[[1]])\ndeltaY <- as.numeric(plt$y.limits[[2]]) - as.numeric(plt$y.limits[[1]])\n\npltRatio <- plt$aspect.ratio\nratio <- deltaY/deltaX\nfactor = 3600\nlength = 0.05\n\n#scale wind speed\nscaleWS <- testData$ws/max(testData$ws, na.rm=TRUE)\n\ntestData$u <- sin(rad(testData$wd))  * factor * scaleWS\ntestData$v <- cos(rad(testData$wd)) *  factor * ratio/pltRatio * scaleWS\n\nplt2 <- xyplot(pm10~date, data=testData, x0=testData$date + testData$u ,y0=testData$pm10 + testData$v, x1 = testData$date - testData$u,\n               y1= testData$pm10 - testData$v, length=length, labels=testData$wd,\n               panel=function(...) {\n                 panel.arrows(...)\n                 panel.grid(...)\n                 #panel.text(...)\n                 },\n               xlab=\"Date\", ylab=quickText(\"PM10 (ug.m-3)\"),\n               main=quickText(\"Winds by Date and PM10\"),\n               scales=list(x=list(format=\"%d-%b / %H:%M\"))\n               )\nplt2\n\n\n\n" }
{ "repo_name": "GeoscienceAustralia/ptha", "ref": "refs/heads/master", "path": "misc/probabilistic_inundation_tonga2020/analysis/probabilistic_inundation/plot_depth_raster_at_target_exrates.R", "content": "#\n# Quick raster plots of the inundation depth at a given exceedance-rate\n#\n\nlibrary(rptha)\n# Get a 'zero-contour' [actually this is from the openstreetmap coastline dataset]\nzc = readOGR('../../elevation/Tonga_coast/Tonga_coast_nearlon180.shp', 'Tonga_coast_nearlon180')\n\n# Workhorse plotting function.\n# Pass it a set of tiffs (one per domain) giving the field to plot, as well as arguments that\n# control the filename, the plot region (nukualofa or tongatapu), and any arguments to 'title'\nplot_max_depth<-function(all_depth_tifs, output_file_name_tag, plot_region = 'nukualofa', ...){\n\n    # Depth colours\n    my_col = colorRampPalette(c('purple', 'blue', 'skyblue', 'green', 'yellow', 'orange', 'red', 'black'))(1000)\n    depth_zlim = c(1.0e-02, 10)\n\n    # Choose xlim, ylim, and file controls differently depending on the plot region\n    if(plot_region == 'nukualofa'){\n       \n        XLIM = c(184.75, 184.9)\n        YLIM = c(-21.2, -21.13) \n\n        output_file = paste0(dirname(all_depth_tifs[1]), '/depth_image_nukualofa_', output_file_name_tag, '.png')\n        ASP = 1/cos(mean(YLIM)/180*pi)\n        png(output_file, width=10, height=10*diff(YLIM)/diff(XLIM) * 1/ASP * 1.5, res=300, units='in')\n\n    }else if(plot_region == 'tongatapu'){\n\n        XLIM = c(184.6, 184.96)\n        YLIM = c(-21.28, -21.05) \n\n        output_file = paste0(dirname(all_depth_tifs[1]), '/depth_image_tongatapu_', output_file_name_tag, '.png')\n        ASP = 1/cos(mean(YLIM)/180*pi)\n        png(output_file, width=10, height=10*diff(YLIM)/diff(XLIM) * 1/ASP * 1.23, res=300, units='in')\n\n    }else{\n        stop('unknown plot_region')\n    }\n\n    # Plot the rasters, controlling the plot extent on the first pass\n    plot_ext = extent(c(XLIM, YLIM))\n    for(i in 1:length(all_depth_tifs)){\n        r1 = raster(all_depth_tifs[i])\n        r1[r1 < depth_zlim[1]] = NA\n        r1[r1 > depth_zlim[2]] = depth_zlim[2]\n        if(i == 1){\n            plot(r1, col=my_col, asp=ASP, xlim=XLIM, ylim=YLIM, zlim=depth_zlim, ext=plot_ext, maxpixels=Inf)\n        }else{\n            image(r1, add=TRUE, zlim=depth_zlim, col=my_col, maxpixels=Inf)\n        }\n    }\n    # Add the openstreetmap coast contour\n    plot(zc, add=TRUE, col='slategrey', lwd=2)\n\n    title(...)\n\n    dev.off()\n}\n\n# Loop over the cases of interest, and make the plots\nfor(run_series_name in c(\"ptha18_tonga_MSL0\", \"ptha18_tonga_MSL0_meshrefine2\", \"ptha18_tonga_MSL0.8\")){\n    for(IS_weights in c(\"\", \"alternate_\")){\n        for(plot_region in c('nukualofa', 'tongatapu')){\n\n            sea_level = ifelse(grepl(\"0.8\", run_series_name, fixed=TRUE), 0.8, 0)\n            IS_type = ifelse(IS_weights==\"\", 'selfNormalised', 'regular')\n\n            plot_max_depth(\n                Sys.glob(paste0(IS_weights, run_series_name, '/depth_at_exceedance_rate_1_in_2475_domain_*.tif')),\n                output_file_name_tag='1_in_2475',\n                plot_region = plot_region, \n                main=paste0('Depth with 2% chance of exceedance in 50 yrs \\n Sea-level = ', sea_level, '; ', \n                            IS_type, ' importance sampling weights'), cex.main=1.5)\n\n            plot_max_depth(\n                Sys.glob(paste0(IS_weights, run_series_name, '/depth_at_exceedance_rate_1_in_475_domain_*.tif')),\n                output_file_name_tag='1_in_475',\n                plot_region = plot_region, \n                main=paste0('Depth with 10% chance of exceedance in 50 yrs \\n Sea-level = ', sea_level, '; ', \n                            IS_type, ' importance sampling weights'), cex.main=1.5)\n        }\n    }\n}\n" }
{ "repo_name": "davidmeza1/KA_Interns", "ref": "refs/heads/master", "path": "InvestigationsDashboard/sidebar.R", "content": "## SIDEBAR\n\nsidebar <- dashboardSidebar(\n  sidebarMenu(\n        menuItem(\n            \"Introduction\",\n            tabName = \"start\",\n            icon = icon(\"play\")\n        ),\n        menuItem(\n            \"Tab 1\",\n            tabName = \"tab1\",\n            icon = icon(\"bookmark\")\n        ),\n        menuItem(\n             \"Tab 2\",\n             tabName = \"tab2\",\n             icon = icon(\"bookmark\")\n        ),\n        menuItem(\n             \"Tab 3\",\n             tabName = \"tab3\",\n             icon = icon(\"bookmark\")\n        ),\n        menuItem(\n             \"Tab 4\",\n             tabName = \"tab4\",\n             icon = icon(\"bookmark\")\n        ),\n        menuItem(\n             \"Tab 5\",\n             tabName = \"tab5\",\n             icon = icon(\"bookmark\")\n        ),\n        menuItem(\n             \"Tab 6\",\n             tabName = \"tab6\",\n             icon = icon(\"bookmark\")\n        ),\n        menuItem(\n             \"Tab 7\",\n             tabName = \"tab7\",\n             icon = icon(\"bookmark\")\n        ),\n        menuItem(\n             \"Tab 8\",\n             tabName = \"tab8\",\n             icon = icon(\"bookmark\")\n        ),\n        menuItem(\n             \"Tab 9\",\n             tabName = \"tab9\",\n             icon = icon(\"bookmark\")\n        ),\n        menuItem(\n             \"Tab 10\",\n             tabName = \"tab10\",\n             icon = icon(\"bookmark\")\n        ),\n        menuItem(\n            \"Acknowledgements\",\n            tabName = \"acknowledgements\",\n            icon = icon(\"trophy\")\n        )\n    )\n)\n" }
{ "repo_name": "WilliamCooper/Ranadu", "ref": "refs/heads/master", "path": "R/PressureAltitude.R", "content": "#' @title PressureAltitude\n#' @description Pressure altitude in the ISA standard atmosphere\n#' @details Calculates the altitude in the ISA standard atmosphere corresponding to the specified pressure\n#' @aliases pressureAltitude\n#' @author William Cooper\n#' @export PressureAltitude\n#' @param P A numeric vector representing pressure in hPa \n#' @return A numeric vector giving the pressure altitude in meters\n#' @examples \n#' PALT <- PressureAltitude (500.)\n#' PALT2 <- PressureAltitude (RAFdata$PSXC)\nPressureAltitude <- function (P) {\n# Pressure altitude formula: (P in hPa)\n\tRu <- 8314.32\n\tgzero <- 9.80665\n\tMWD <- 28.9644\n  exp1 <- Ru * 0.0065 / (MWD * gzero)\n  exp2 <- -Ru * 0.001 / (MWD * gzero)\n  exp3 <- -Ru * 0.0028 / (MWD * gzero)\n  p1 <- 1013.25 * (1 - 0.0065 * 11000 / 288.15)^(1/exp1)\n  p2 <- p1 * exp(-gzero * MWD / (216.65 * Ru) * 9000)\n  p3 <- p2 * (1 + 0.001 * 12000 / 216.65)^(1/exp2)\n  p4 <- p3 * (1 + 0.0028 * 15000 / 228.65)^(1/exp3)\n  PALT <- 11000.+(Ru * 216.65) / (gzero * MWD) * log(p1 / P)\n  PALT[P > p1] <- (288.15 / 0.0065) * (1 -(P[P > p1] / 1013.25)^exp1)\n  PALT[P < p2] <- 20000 - (216.65 / 0.001) * (1 - (P[P < p2] / p2)^exp2)\n  PALT[P < p3] <- 32000 - (228.65 / 0.0028) * (1 - (P[P < p3] / p3)^exp3)\n  PALT[P < p4] <- 47000 + (Ru * 270.65) / (gzero * MWD) * log(p4 / P[P < p4])\n#   if (P > 226.3206) {\n#     PALT = (288.15/0.0065)*(1-(P/1013.25)**0.1902632)\n#   } else {\n#     PALT = 11000.+(8314.32*216.65)/(9.80665*28.9644)*log(226.3206/P)\n#   }\n  return (as.vector(PALT))\n}\n\n" }
{ "repo_name": "ranghetti/fidolasen", "ref": "refs/heads/master", "path": "R/s2_dop.R", "content": "#' @title Return the Dates Of Passage over some orbits\n#' @description The function allows to know which Sentinel-2 passages should\n#'  pass over certain orbits during a defined time interval.\n#'  Dates are intended to be in UTC time.\n#'  Notice that this is the expected calendar: some unexpected events\n#'  (e.g. technical problems, or early working phases during first stages of\n#'  acquisition) could cause the data unavailability even if an\n#'  acquisition was expected.\n#'  Notice also that some orbits (030, 073 and 116) acquire across UTC midnight:\n#'  in this cases, the date is assumed to be the one of the acquisition after\n#'  midnight (which corresponds to the date in local time).\n#' @param s2_orbits A vector of Sentinel-2 orbits (as integer numbers\n#'  or 3-length character).\n#'  Default is all the 143 orbits.\n#' @param timewindow Temporal window for querying: Date object\n#'  of length 1 (single day) or 2 (time window).\n#'  Is it possible to pass also integer (or difftime) values, which are\n#'  interpreted as the next n days (if positive) or the past n days\n#'  (if negative).\n#'  Also strings which can be interpreted as time ranges are accepted\n#'  (see examples).\n#'  Default is the next 10 days (one cycle).\n#' @param mission (optional) Vector with the desired Sentinel-2 missions\n#'  (\"2A\", \"2B\" or both). Default is both.\n#' @return A data table with the dates (column \"date\"), the missions\n#' (column \"mission\") and the orbits (column \"orbit\").\n#' An empty data table with the same structure is returned if no passages\n#'  were found with the passed settings.\n#' @author Luigi Ranghetti, phD (2019) \\email{luigi@@ranghetti.info}\n#' @note License: GPL 3.0\n#' @import data.table\n#' @importFrom jsonlite fromJSON\n#' @importFrom methods is\n#' @export\n#'\n#' @examples\n#' # All the passages in a cycle of 10 days over all the orbits\n#' s2_dop()\n#'\n#' # The passages in the current month over two orbits\n#' s2_dop(c(\"022\", \"065\"), \"this month\")\n#'\n#' # The dates in which Sentinel-2A will pass in next six weeks over one orbit\n#' s2_dop(\"022\", \"6 weeks\", mission = \"2A\")$date\n#'\n#' # The date in which Sentinel-2A would be passed in the last 10 days over one orbit\n#' s2_dop(\"022\", \"-10 days\", mission = \"2A\")$date\n#'\n#' # All the orbits covered today\n#' s2_dop(timewindow = Sys.Date(), mission = \"2B\")$orbit\n#'\n#' # The passages in a fixed time window for one orbit\n#' s2_dop(65, as.Date(c(\"2018-08-01\", \"2018-08-31\")))\n#'\n#' # A research with no passages found\n#' s2_dop(22, \"2018-08-16\", mission = \"2A\")\n\n\ns2_dop <- function(s2_orbits = 1:143,\n                   timewindow = \"10 days\",\n                   mission = c(\"2A\", \"2B\")) {\n  \n  # to avoid NOTE on check\n  type <- orbit <- doybase <- orbit <- NULL\n  \n  # generate doybase.json if missing\n  json_path <- create_s2_dop()\n  s2_dop_dt <- data.table(jsonlite::fromJSON(json_path)$dop)\n  \n  ## Check the length of arguments\n  if (length(s2_orbits) == 0 | length(timewindow) == 0 | length(mission) == 0) {\n    return(\n      data.table(\n        \"date\" = as.Date(character(0)),\n        \"mission\" = character(0),\n        \"orbit\" = character(0),\n        stringsAsFactors = FALSE\n      )\n    )\n  }\n  \n  ## Check s2_orbits\n  if (is.numeric(s2_orbits)) {\n    s2_orbits <- str_pad2(as.integer(s2_orbits), 3, \"left\", \"0\")\n  }\n  if (any(\n    as.character(suppressWarnings(as.integer(s2_orbits)), 3, \"left\", \"0\") != gsub(\"^0+\", \"\", s2_orbits),\n    is.na(suppressWarnings(as.integer(s2_orbits))),\n    suppressWarnings(as.integer(s2_orbits))<0,\n    suppressWarnings(as.integer(s2_orbits))>143\n  )) {\n    print_message(\n      type = \"error\",\n      \"Parameter 's2_orbits' contains invalid Sentinel-2 orbits.\"\n    )\n  }\n  \n  \n  ## Check mission\n  if (!all(grepl(\"^2[AB]$\", mission))) {\n    print_message(\n      type = \"error\",\n      \"Parameter 'mission' cannot contain values different from \\\"2A\\\" and/or \\\"2B\\\".\"\n    )\n  }\n  \n  ## Check timewindow\n  # check that it does not contain NAs\n  if (anyNA(timewindow)) {\n    print_message(\n      type = \"error\",\n      \"Parameter 'timewindow' cannot contain NA values.\"\n    )\n  }\n  \n  # Check to be 1 or 2 length\n  if (length(timewindow)>2) {\n    print_message(\n      type = type,\n      \"Parameter 'timewindow' must be of length 1 or 2.\"\n    )\n  }\n  \n  # If difftime, threat as character\n  if (is(timewindow, \"difftime\")) {\n    timewindow <- paste(timewindow, attr(timewindow, \"units\"))\n  }\n  \n  # If 1-length numeric, threat as difftime in days\n  if (length(timewindow)==1 & is(timewindow, \"numeric\")) {\n    timewindow <- as.character(as.integer(timewindow))\n  }\n  \n  # Check character input (to be a date or a time period)\n  if (is(timewindow, \"character\")) {\n    # if it is a date, convert in Date\n    try_date <- tryCatch({timewindow <- as.Date(timewindow)}, error = function(e){\"error\"})\n    # otherwise, check if it is a recognised difftime string\n    if (is.character(try_date)) {\n      timewindow <- strsplit(timewindow, \" \")[[1]]\n      if (timewindow[1]==\"this\") {\n        timewindow[1] <- 1\n      }\n      if (timewindow[1]==\"next\") {\n        timewindow <- timewindow[-1]\n      }\n      if (timewindow[1] %in% c(\"past\",\"last\")) {\n        timewindow <- timewindow[-1]\n        timewindow[1] <- paste0(\"-\",timewindow[1])\n      }\n      if (\n        any(!grepl(\n          \"^((\\\\-?[0-9]+)|(this)|(days?)|(weeks?)|(months?)|(years?))$\",\n          tolower(timewindow)\n        )) |\n        length(timewindow) > 2\n      ) {\n        print_message(\n          type = \"error\",\n          \"Parameter 'timewindow' is not a recognised string.\"\n        )\n      }\n      if (length(timewindow)==1) {\n        if (grepl(\"^((\\\\-?[0-9]+)|(this))$\", tolower(timewindow))) {\n          timewindow <- c(timewindow, \"days\")\n        } else {\n          timewindow <- c(1, timewindow)\n        }\n      }\n      timewindow_start <- switch(\n        tolower(gsub(\"s$\",\"\",timewindow[2])),\n        day = Sys.Date(),\n        week = as.Date(cut(Sys.Date(), \"week\")),\n        month = as.Date(strftime(Sys.Date(),\"%Y-%m-01\")),\n        year = as.Date(strftime(Sys.Date(),\"%Y-01-01\"))\n      )\n      timewindow_all <- if (as.integer(timewindow[1]) > 0) {\n        seq(timewindow_start, length=as.integer(timewindow[1])+1, by=timewindow[2])\n      } else {\n        sort(c(\n          seq(timewindow_start, length=2, by=timewindow[2])[2],\n          seq(timewindow_start, length=-as.integer(timewindow[1]), by=paste(\"-1\",timewindow[2]))\n        ))\n      }\n      timewindow <- timewindow_all[c(1,length(timewindow_all))] + c(0,-1)\n    }\n  }\n  \n  # If it is a POSIXct, convert in Date\n  if (is(timewindow, \"POSIXt\")) {\n    timewindow <- as.Date(strftime(timewindow))\n  }\n  \n  # If it is a 1-length Date, consider as 1-day interval\n  if (length(timewindow)==1 & is(timewindow, \"Date\")) {\n    timewindow <- rep(timewindow, 2)\n  }\n  \n  # If conversions in Date were not sufficient, stop\n  if (!is(timewindow, \"Date\")) {\n    print_message(\n      type = type,\n      \"Parameter 'timewindow' is not in a recognised format.\"\n    )\n  }\n  \n  dates_all <- seq(timewindow[1], timewindow[2], by = \"day\")\n  \n  \n  ## Compute the dates\n  sel_dop_dt <- s2_dop_dt[orbit %in% s2_orbits,]\n  s2a_dates <- if (\"2A\" %in% mission) {\n    dates_all[(as.integer(dates_all)%%10) %in% sel_dop_dt$doybase]\n  } else {\n    as.Date(character(0))\n  }\n  s2b_dates <- if (\"2B\" %in% mission) {\n    dates_all[(as.integer(dates_all)%%10) %in% ((sel_dop_dt$doybase+5)%%10)]\n  } else {\n    as.Date(character(0))\n  }\n  s2a_orbits <- lapply(s2a_dates, function(d) {sel_dop_dt[doybase==as.integer(d)%%10,orbit]})\n  s2b_orbits <- lapply(s2b_dates, function(d) {sel_dop_dt[doybase==(as.integer(d)+5)%%10,orbit]})\n  s2_missions <- c(rep(\"2A\", length(s2a_dates)), rep(\"2B\", length(s2b_dates)))\n  s2_data <- rbindlist(lapply(seq_along(s2_missions), function(i) {\n    expand.grid(\n      \"date\" = c(s2a_dates, s2b_dates)[i],\n      \"mission\" = s2_missions[i],\n      \"orbit\" = c(s2a_orbits, s2b_orbits)[[i]],\n      stringsAsFactors = FALSE\n    )\n  }))\n  \n  if (nrow(s2_data) > 0) {\n    # Order data\n    setorder(s2_data, date, mission, orbit)\n    # Remove unexisting records\n    s2_data <- s2_data[\n      mission == \"2A\" & date >= \"2015-06-27\" |\n        mission == \"2B\" & date >= \"2017-06-29\"\n      ]\n    return(s2_data)\n  } else {\n    return(\n      data.table(\n        \"date\" = as.Date(character(0)),\n        \"mission\" = character(0),\n        \"orbit\" = character(0),\n        stringsAsFactors = FALSE\n      )\n    )\n  }\n  \n}" }
{ "repo_name": "esteful/arch_notebook", "ref": "refs/heads/master", "path": "R_fun/classifica.R", "content": "\"classifica\"<-\n  function(x)\n  {\n    # Aquesta rutina fa una matriu signary per fer balances a partir díun dendrograma\n    # de les variables en un procÈs díuniformitat \n    n<-length(x[[4]])\n    sortida<-matrix(0,n-1,n)\n    signary<-matrix(-2,n-1,n) #-2 per no crear confusions amb valors possibles\n    zeros<-matrix(-2,n-1,n) #cal una matriu a part de signary pels zeros de leafs\n    # ara mirem com sÛn les fusions\n    for (i in 1:n-1) {\n      sortida[i,]<-cutree(x,k=i+1)\n    }\n    # ara resolem lÌnia per lÌnia. La primera i lí˙ltima sÛn especials. \n    # La primera nomÈs tÈ dos grups. Lí˙ltima nomÈs tÈ dos individus\n    for (i in 1:n-1){\n      if (i==1) {\n        posar<-which(sortida[i,]==1)\n        signary[i,posar]<-1\n        # ara cal veure si Ès leaf i posar-hi zeros a sota\n        if (length(posar)==1) {\n          signary[c((i+1):(n-1)),posar]<-0\n          zeros[c((i+1):(n-1)),posar]<-0\n        }\n        posar<-which(sortida[i,]==2)\n        signary[i,posar]<--1\n        if (length(posar)==1) {\n          signary[c((i+1):(n-1)),posar]<-0\n          zeros[c((i+1):(n-1)),posar]<-0\n        }\n      }\n      if (i>1) {\n        if (i<(n-1)) {\n          # prepara un vector f1 on poso els zeros per ser un factor\n          # de la fila i-1 i un vector f2 per ser un factor de la fila i\n          f1<-sortida[(i-1),]\n          f2<-sortida[i,]\n          posar<-which(zeros[(i-1),]==0)\n          if (length(posar)>0) {f1[posar]<-0}\n          posar<-which(zeros[i,]==0)\n          if (length(posar)>0) {f2[posar]<-0}\n          # ara converteixo f1 i f2 en factors, miro i faig signary per levels\n          f1<-as.factor(t(f1))\n          f2<-as.factor(t(f2))\n          # faig una matriu per avaluar si els grups sÛn diferents\n          nf1<-nlevels(f1)\n          nf2<-nlevels(f2)\n          m<-matrix(TRUE,nf1,nf2)\n          if (levels(f2)[1]== \"0\") {des<-2} else {des<-1}\n          for (j in 1:nf1) {\n            for (jj in des:nf2) {\n              posarf1<-which(f1==levels(f1)[j])\n              posarf2<-which(f2==levels(f2)[jj])\n              m[j,jj]<-identical(posarf1,posarf2)\n            }\n          }\n          # ara faig un comptador per buscar dues columnes de F de 1 i -1\n          comptador<-1\n          for (jj in 1:nf2) {\n            if (any(m[,jj])==FALSE) {\n              if (levels(f2)[jj]!= \"0\") {\n                posarf2<-which(f2==levels(f2)[jj])\n                signary[i,posarf2]<-comptador\n                comptador<--1\n              }\n            }\n            if (any(m[,jj])==TRUE) {\n              posarf2<-which(f2==levels(f2)[jj])\n              signary[i,posarf2]<-0\n            }\n          }\n          # ara cal posar 0 a les columnes de leafs\n          posar<-which(signary[i,]==1)\n          if (length(posar)==1) {\n            signary[c((i+1):(n-1)),posar]<-0\n            zeros[c((i+1):(n-1)),posar]<-0\n          }\n          posar<-which(signary[i,]==-1)\n          if (length(posar)==1) {\n            signary[c((i+1):(n-1)),posar]<-0\n            zeros[c((i+1):(n-1)),posar]<-0\n          }\n        }\n        if (i==(n-1)) {\n          f2<-sortida[i,]\n          posar<-which(zeros[i,]==0)\n          if (length(posar)>0) {f2[posar]<-0}\n          comptador=1\n          for (jj in 1:n) {\n            if (f2[jj]!=0) {\n              f2[jj]<-comptador\n              comptador<--1\n            }\n          }\n          signary[i,]<-f2\n        }\n      } \n    }\n    dimnames(sortida)[[2]] <-x[[4]]\n    list(sortida=as.data.frame(sortida), signary=as.data.frame(signary), f1=f1, f2=f2, m=as.data.frame(m), des=des, zeros=as.data.frame(zeros))\n  }\n" }
{ "repo_name": "emeryyi/msda", "ref": "refs/heads/master", "path": "R/predict.msda.R", "content": "predict.msda <- function(object, newx, ...) {\r\n    theta <- object$theta\r\n    mu <- object$mu\r\n    prior <- object$prior\r\n    mubar <- sweep(mu[, -1], 1, mu[, 1], \"+\")/2\r\n    n <- nrow(newx)\r\n    p <- ncol(newx)\r\n    x.train <- object$x\r\n    y.train <- object$y\r\n    nclass <- length(prior)\r\n    nlambda <- length(theta)\r\n    pred <- matrix(0, n, nlambda)\r\n    pred[1] <- which.max(prior)\r\n    for (i in 1:nlambda) {\r\n        nz <- sum(theta[[i]][, 1] != 0)\r\n        if (nz == 0) {\r\n            pred[,i] <- which.max(prior)\r\n        } else {\r\n            xfit <- x.train %*% theta[[i]][, 1:(min(nclass - 1, nz)),drop=FALSE]\n            xfit.sd<-matrix(0,nclass,ncol(xfit))\n            for(j in 1:nclass){\n            xfit.sd[j,]<-apply(xfit[y.train==j,,drop=FALSE],2,sd)}\n            xfit.sd<-apply(xfit.sd,2,min)\n            if(min(xfit.sd)<1e-4){pred[,i]<-which.max(prior)}else{\r\n            l <- lda(xfit, y.train)\r\n            pred[, i] <- predict(l, newx %*% theta[[i]][, 1:(min(nclass - \r\n                1, nz))])$class}\r\n        }\r\n    }\r\n    pred\r\n}\r\n" }
{ "repo_name": "cran/msda", "ref": "refs/heads/master", "path": "R/predict.msda.R", "content": "predict.msda <- function(object, newx, ...) {\r\n    theta <- object$theta\r\n    mu <- object$mu\r\n    prior <- object$prior\r\n    mubar <- sweep(mu[, -1], 1, mu[, 1], \"+\")/2\r\n    n <- nrow(newx)\r\n    p <- ncol(newx)\r\n    x.train <- object$x\r\n    y.train <- object$y\r\n    nclass <- length(prior)\r\n    nlambda <- length(theta)\r\n    pred <- matrix(0, n, nlambda)\r\n    pred[1] <- which.max(prior)\r\n    for (i in 1:nlambda) {\r\n        nz <- sum(theta[[i]][, 1] != 0)\r\n        if (nz == 0) {\r\n            pred[,i] <- which.max(prior)\r\n        } else {\r\n            xfit <- x.train %*% theta[[i]][, 1:(min(nclass - 1, nz)),drop=FALSE]\n            xfit.sd<-matrix(0,nclass,ncol(xfit))\n            for(j in 1:nclass){\n            xfit.sd[j,]<-apply(xfit[y.train==j,,drop=FALSE],2,sd)}\n            xfit.sd<-apply(xfit.sd,2,min)\n            if(min(xfit.sd)<1e-4){pred[,i]<-which.max(prior)}else{\r\n            l <- lda(xfit, y.train)\r\n            pred[, i] <- predict(l, newx %*% theta[[i]][, 1:(min(nclass - \r\n                1, nz))])$class}\r\n        }\r\n    }\r\n    pred\r\n}\r\n" }
{ "repo_name": "iainmstott/popdemo", "ref": "refs/heads/main", "path": "1.0-0/popdemo/R/maxamp.R", "content": "################################################################################\r\n#' Calculate maximal amplification\r\n#'\r\n#' @description\r\n#' Calculate maximal amplification for a population matrix projection model.\r\n#'\r\n#' @param A a square, primitive, non-negative numeric matrix of any dimension\r\n#'\r\n#' @param vector (optional) a numeric vector or one-column matrix describing \r\n#' the age/stage distribution ('demographic structure') used to calculate a \r\n#' 'case-specific' maximal amplification.\r\n#'\r\n#' @param return.N (optional) if \\code{TRUE}, returns population size at the \r\n#' point of maximal amplification (including effects of asymptotic growth and \r\n#' initial population size), alongside standardised maximal amplification.\r\n#'\r\n#' @param return.t (optional) if \\code{TRUE}, returns the time at which maximal \r\n#' amplification occurs in the population projection.\r\n#'\r\n#' @param return.stage (optional) if \\code{TRUE} and \\code{vector=\"n\"}, returns \r\n#' the stage that achieves the bound on maximal amplification.\r\n#'\r\n#' @param conv.iterations the maximum number of iterations allowed when calulating \r\n#' convergence time (see details). Please see \\code{iterations} in \r\n#' \\code{\\link{convt}}.\r\n#'\r\n#' @param conv.accuracy the accuracy of convergence (see details). Please see \r\n#' \\code{accuracy} in \\code{\\link{convt}}.\r\n#'\r\n#' @details \r\n#' \\code{maxamp} returns a standardised measure of maximal amplification, \r\n#' discounting the effects of both initial population size and asymoptotic growth \r\n#' (Stott et al. 2011).\\cr\\cr  \r\n#' If \\code{vector} is not specified then the bound on maximal amplification (the \r\n#' largest maximal amplification that may be achieved) is returned, otherwise a \r\n#' 'case-specific' maximal amplification for the specified matrix and demographic \r\n#' structure is calculated. Note that not all demographic structures will yield a \r\n#' maximal amplification: if the model does not amplify then an error is returned.\\cr\\cr\r\n#' Setting \\code{return.N=T}, \\code{return.t=T} and \\code{return.stage=T} results in \r\n#' the function returning realised population size at maximal amplification \r\n#' (including the effects of asymptotic growth and initial population size), the \r\n#' time at which maximal amplification occurs and (if \\code{vector=\"n\"}), \r\n#' the stage-bias that results in the bound on maximal amplification, respectively.\r\n#' NOTE that \\code{N} is not indicative of maximum possible population size for a \r\n#' non-standardised model: merely the population size at the point of maximal \r\n#' amplification (i.e. largest positive deviation from lambda-max).\\cr\\cr\r\n#' \\code{max.amp} uses a simulation technique, using \\code{\\link{project}} to project \r\n#' the dynamics of the model before evaluating maximum projected density over all t. \r\n#' \\code{conv.accuracy} and \\code{conv.iterations} are passed to \r\n#' \\code{\\link{convt}}, which is used to find the point of model convergence \r\n#' in order to ensure maximal amplification is correctly captured in model projection.\\cr\\cr\r\n#' \\code{maxamp} will not work for imprimitive or reducible matrices.\r\n#'\r\n#' @return \r\n#' If \\code{vector=\"n\"}, the bound on maximal amplification of \\code{A}.\\cr\r\n#' If \\code{vector} is specified, the case-specific maximal amplification of the model.\\cr\r\n#' If \\code{return.N=TRUE}, \\code{return.t=TRUE} and/or \\code{return.stage=TRUE},\r\n#' a list with possible components:\\cr\r\n#' \\describe{\r\n#' \\item{maxamp}{the bound on or case-specific maximal amplification}\r\n#' \\item{N}{the population size at the point of maximal amplification, including the \r\n#' effects of initial population size and asymptotic growth. NOTE that \\code{N} is not \r\n#' indicative of maximum possible population size for a non-standardised model:\r\n#' merely the population size at the point of maximal amplification (i.e. largest \r\n#' positive deviation from lambda-max).}\r\n#' \\item{t}{the projection interval at which maximal amplification is achieved.}\r\n#' \\item{stage}{(only if \\code{vector=\"n\"}), the stage that achieves the bound on \r\n#' maximal amplification.}\r\n#' }\r\n#'\r\n#' @references\r\n#' Neubert & Caswell (1997) Ecology, 78, 653-665.\\cr\r\n#' Stott et al. (2011) Ecol. Lett., 14, 959-970.\\cr\r\n#' Townley & Hodgson (2008) J. Appl. Ecol., 45, 1836-1839.\r\n#'\r\n#' @family TransientIndices\r\n#'\r\n#' @examples\r\n#'   # Create a 3x3 PPM\r\n#'   ( A <- matrix(c(0,1,2,0.5,0.1,0,0,0.6,0.6), byrow=TRUE, ncol=3) )\r\n#'\r\n#'   # Create an initial stage structure\r\n#'   ( initial <- c(1,3,2) )\r\n#'\r\n#'   # Calculate the bound on maximal amplification of A\r\n#'   maxamp(A)\r\n#'\r\n#'   # Calculate the bound on maximal amplification of A and \r\n#'   # return the stage that achieves it\r\n#'   maxamp(A, return.stage=TRUE)\r\n#'\r\n#'   # Calculate case-specific maximal amplification of A\r\n#'   # and initial\r\n#'   maxamp(A, vector=initial)\r\n#'\r\n#'   # Calculate case-specific maximal amplification of A\r\n#'   # and initial and return realised population size and the \r\n#'   # time at which it is achieved\r\n#'   maxamp(A, vector=initial, return.N=TRUE, return.t=TRUE)\r\n#'\r\n#' @concept \r\n#' transient dynamics amplification unstable instability\r\n#'\r\n#' @export\r\n#'\r\nmaxamp <-\r\nfunction(A,vector=\"n\",return.N=FALSE,return.t=FALSE,return.stage=FALSE,conv.iterations=1e+5,conv.accuracy=1e-5){\r\nif(any(length(dim(A))!=2,dim(A)[1]!=dim(A)[2])) stop(\"A must be a square matrix\")\r\norder<-dim(A)[1]\r\nM<-A\r\neigvals<-eigen(M)$values\r\nlmax<-which.max(Re(eigvals))\r\nlambda<-Re(eigvals[lmax])\r\nA<-M/lambda\r\nif(vector[1]==\"n\"){\r\n    maxtime<-max(convt(A,accuracy=conv.accuracy,iterations=conv.iterations))\r\n    projection<-project(A,time=maxtime)\r\n    maxN<-numeric(order)\r\n    times<-numeric(order)\r\n    for(i in 1:order){\r\n        maxN[i]<-max(projection[,i])\r\n        times[i]<-which.max(projection[,i])-1\r\n    }\r\n    rhomax<-max(maxN)\r\n    t<-times[which.max(maxN)]\r\n    stage<-which.max(maxN)\r\n    if(return.N){\r\n        Nt<-rhomax*lambda^t\r\n        if(all(return.t,return.stage)) return(list(maxamp=rhomax,N=Nt,t=t,stage=stage))\r\n        if(all(return.t,!return.stage)) return(list(maxamp=rhomax,N=Nt,t=t))\r\n        if(all(!return.t,return.stage)) return(list(maxamp=rhomax,N=Nt,stage=stage))\r\n        if(!all(return.t,return.stage)) return(list(maxamp=rhomax,N=Nt))\r\n    }\r\n    else{\r\n        if(all(return.t,return.stage)) return(list(maxamp=rhomax,t=t,stage=stage))\r\n        if(all(return.t,!return.stage)) return(list(maxamp=rhomax,t=t))\r\n        if(all(!return.t,return.stage)) return(list(maxamp=rhomax,stage=stage))\r\n        if(!all(return.t,return.stage)) return(rhomax)\r\n    }\r\n}\r\nelse{\r\n    n0<-vector\r\n    vector<-n0/sum(n0)\r\n    maxtime<-convt(A,vector=vector,accuracy=conv.accuracy,iterations=conv.iterations)\r\n    projection<-project(A,vector=vector,time=maxtime)\r\n    rhomax<-max(projection)\r\n    t<-which.max(projection)-1\r\n    if(rhomax>1){\r\n        if(return.N){\r\n            Nt<-rhomax*sum(n0)*lambda^t\r\n            if(return.t) return(list(maxamp=rhomax,N=Nt,t=t)) else(return(list(maxamp=rhomax,N=Nt)))\r\n        }\r\n        else{\r\n            if(return.t) return(list(maxamp=rhomax,t=t)) else(return(rhomax))\r\n        }\r\n    }\r\n    else{\r\n        stop(\"Model does not amplify. Cannot compute maximum amplification\")\r\n    }\r\n}}\r\n" }
{ "repo_name": "iainmstott/popdemo", "ref": "refs/heads/main", "path": "1.1-0/popdemo/R/maxamp.R", "content": "################################################################################\r\n#' Calculate maximal amplification\r\n#'\r\n#' @description\r\n#' Calculate maximal amplification for a population matrix projection model.\r\n#'\r\n#' @param A a square, primitive, non-negative numeric matrix of any dimension\r\n#'\r\n#' @param vector (optional) a numeric vector or one-column matrix describing \r\n#' the age/stage distribution ('demographic structure') used to calculate a \r\n#' 'case-specific' maximal amplification.\r\n#'\r\n#' @param return.N (optional) if \\code{TRUE}, returns population size at the \r\n#' point of maximal amplification (including effects of asymptotic growth and \r\n#' initial population size), alongside standardised maximal amplification.\r\n#'\r\n#' @param return.t (optional) if \\code{TRUE}, returns the time at which maximal \r\n#' amplification occurs in the population projection.\r\n#'\r\n#' @param return.stage (optional) if \\code{TRUE} and \\code{vector=\"n\"}, returns \r\n#' the stage that achieves the bound on maximal amplification.\r\n#'\r\n#' @param conv.iterations the maximum number of iterations allowed when calulating \r\n#' convergence time (see details). Please see \\code{iterations} in \r\n#' \\code{\\link{convt}}.\r\n#'\r\n#' @param conv.accuracy the accuracy of convergence (see details). Please see \r\n#' \\code{accuracy} in \\code{\\link{convt}}.\r\n#'\r\n#' @details \r\n#' \\code{maxamp} returns a standardised measure of maximal amplification, \r\n#' discounting the effects of both initial population size and asymoptotic growth \r\n#' (Stott et al. 2011).\\cr\\cr  \r\n#' If \\code{vector} is not specified then the bound on maximal amplification (the \r\n#' largest maximal amplification that may be achieved) is returned, otherwise a \r\n#' 'case-specific' maximal amplification for the specified matrix and demographic \r\n#' structure is calculated. Note that not all demographic structures will yield a \r\n#' maximal amplification: if the model does not amplify then an error is returned.\\cr\\cr\r\n#' Setting \\code{return.N=T}, \\code{return.t=T} and \\code{return.stage=T} results in \r\n#' the function returning realised population size at maximal amplification \r\n#' (including the effects of asymptotic growth and initial population size), the \r\n#' time at which maximal amplification occurs and (if \\code{vector=\"n\"}), \r\n#' the stage-bias that results in the bound on maximal amplification, respectively.\r\n#' NOTE that \\code{N} is not indicative of maximum possible population size for a \r\n#' non-standardised model: merely the population size at the point of maximal \r\n#' amplification (i.e. largest positive deviation from lambda-max).\\cr\\cr\r\n#' \\code{max.amp} uses a simulation technique, using \\code{\\link{project}} to project \r\n#' the dynamics of the model before evaluating maximum projected density over all t. \r\n#' \\code{conv.accuracy} and \\code{conv.iterations} are passed to \r\n#' \\code{\\link{convt}}, which is used to find the point of model convergence \r\n#' in order to ensure maximal amplification is correctly captured in model projection.\\cr\\cr\r\n#' \\code{maxamp} will not work for imprimitive or reducible matrices.\r\n#'\r\n#' @return \r\n#' If \\code{vector=\"n\"}, the bound on maximal amplification of \\code{A}.\\cr\r\n#' If \\code{vector} is specified, the case-specific maximal amplification of the model.\\cr\r\n#' If \\code{return.N=TRUE}, \\code{return.t=TRUE} and/or \\code{return.stage=TRUE},\r\n#' a list with possible components:\\cr\r\n#' \\describe{\r\n#' \\item{maxamp}{the bound on or case-specific maximal amplification}\r\n#' \\item{N}{the population size at the point of maximal amplification, including the \r\n#' effects of initial population size and asymptotic growth. NOTE that \\code{N} is not \r\n#' indicative of maximum possible population size for a non-standardised model:\r\n#' merely the population size at the point of maximal amplification (i.e. largest \r\n#' positive deviation from lambda-max).}\r\n#' \\item{t}{the projection interval at which maximal amplification is achieved.}\r\n#' \\item{stage}{(only if \\code{vector=\"n\"}), the stage that achieves the bound on \r\n#' maximal amplification.}\r\n#' }\r\n#'\r\n#' @references\r\n#' Neubert & Caswell (1997) Ecology, 78, 653-665.\\cr\r\n#' Stott et al. (2011) Ecol. Lett., 14, 959-970.\\cr\r\n#' Townley & Hodgson (2008) J. Appl. Ecol., 45, 1836-1839.\r\n#'\r\n#' @family TransientIndices\r\n#'\r\n#' @examples\r\n#'   # Create a 3x3 PPM\r\n#'   ( A <- matrix(c(0,1,2,0.5,0.1,0,0,0.6,0.6), byrow=TRUE, ncol=3) )\r\n#'\r\n#'   # Create an initial stage structure\r\n#'   ( initial <- c(1,3,2) )\r\n#'\r\n#'   # Calculate the bound on maximal amplification of A\r\n#'   maxamp(A)\r\n#'\r\n#'   # Calculate the bound on maximal amplification of A and \r\n#'   # return the stage that achieves it\r\n#'   maxamp(A, return.stage=TRUE)\r\n#'\r\n#'   # Calculate case-specific maximal amplification of A\r\n#'   # and initial\r\n#'   maxamp(A, vector=initial)\r\n#'\r\n#'   # Calculate case-specific maximal amplification of A\r\n#'   # and initial and return realised population size and the \r\n#'   # time at which it is achieved\r\n#'   maxamp(A, vector=initial, return.N=TRUE, return.t=TRUE)\r\n#'\r\n#' @concept \r\n#' transient dynamics amplification unstable instability\r\n#'\r\n#' @export\r\n#'\r\nmaxamp <-\r\nfunction(A,vector=\"n\",return.N=FALSE,return.t=FALSE,return.stage=FALSE,conv.iterations=1e+5,conv.accuracy=1e-5){\r\nif(any(length(dim(A))!=2,dim(A)[1]!=dim(A)[2])) stop(\"A must be a square matrix\")\r\norder<-dim(A)[1]\r\nM<-A\r\neigvals<-eigen(M)$values\r\nlmax<-which.max(Re(eigvals))\r\nlambda<-Re(eigvals[lmax])\r\nA<-M/lambda\r\nif(vector[1]==\"n\"){\r\n    maxtime<-max(convt(A,accuracy=conv.accuracy,iterations=conv.iterations))\r\n    projection<-project(A,time=maxtime)\r\n    maxN<-numeric(order)\r\n    times<-numeric(order)\r\n    for(i in 1:order){\r\n        maxN[i]<-max(projection[,i])\r\n        times[i]<-which.max(projection[,i])-1\r\n    }\r\n    rhomax<-max(maxN)\r\n    t<-times[which.max(maxN)]\r\n    stage<-which.max(maxN)\r\n    if(return.N){\r\n        Nt<-rhomax*lambda^t\r\n        if(all(return.t,return.stage)) return(list(maxamp=rhomax,N=Nt,t=t,stage=stage))\r\n        if(all(return.t,!return.stage)) return(list(maxamp=rhomax,N=Nt,t=t))\r\n        if(all(!return.t,return.stage)) return(list(maxamp=rhomax,N=Nt,stage=stage))\r\n        if(!all(return.t,return.stage)) return(list(maxamp=rhomax,N=Nt))\r\n    }\r\n    else{\r\n        if(all(return.t,return.stage)) return(list(maxamp=rhomax,t=t,stage=stage))\r\n        if(all(return.t,!return.stage)) return(list(maxamp=rhomax,t=t))\r\n        if(all(!return.t,return.stage)) return(list(maxamp=rhomax,stage=stage))\r\n        if(!all(return.t,return.stage)) return(rhomax)\r\n    }\r\n}\r\nelse{\r\n    n0<-vector\r\n    vector<-n0/sum(n0)\r\n    maxtime<-convt(A,vector=vector,accuracy=conv.accuracy,iterations=conv.iterations)\r\n    projection<-project(A,vector=vector,time=maxtime)\r\n    rhomax<-max(projection)\r\n    t<-which.max(projection)-1\r\n    if(rhomax>1){\r\n        if(return.N){\r\n            Nt<-rhomax*sum(n0)*lambda^t\r\n            if(return.t) return(list(maxamp=rhomax,N=Nt,t=t)) else(return(list(maxamp=rhomax,N=Nt)))\r\n        }\r\n        else{\r\n            if(return.t) return(list(maxamp=rhomax,t=t)) else(return(rhomax))\r\n        }\r\n    }\r\n    else{\r\n        stop(\"Model does not amplify. Cannot compute maximum amplification\")\r\n    }\r\n}}\r\n" }
{ "repo_name": "mhunter1/OpenMx", "ref": "refs/heads/master", "path": "inst/models/failing/UnivariateTwinAnalysis20090925.R", "content": "# -----------------------------------------------------------------------\n# Program: UnivariateTwinAnalysis20090925.R  \n#  Author: Hermine Maes\n#    Date: Wed Sep 25 11:45:52 EDT 2009\n#\n# Revision History\n#   Hermine Maes -- Wed Sep 25 11:45:52 EDT 2009 UnivariateTwinAnalysis20090925.R\n# TODO include equivalent file for mx 1.x \n# TODO: Add omxCheckCloseEnough() calls\n# -----------------------------------------------------------------------\n\n\n# Simulate Data: two standardized variables t1 & t2 for MZ's & DZ's\n# -----------------------------------------------------------------------\nrequire(OpenMx)\nrequire(MASS)\n\nset.seed(200)\na2<-0.5\t\t#Additive genetic variance component (a squared)\nc2<-0.3\t\t#Common environment variance component (c squared)\ne2<-0.2\t\t#Specific environment variance component (e squared)\nrMZ <- a2+c2\nrDZ <- .5*a2+c2\nMZ  <- mvrnorm (1000, c(0,0), matrix(c(1,rMZ,rMZ,1),2,2))\nDZ  <- mvrnorm (1000, c(0,0), matrix(c(1,rDZ,rDZ,1),2,2))\n\nselVars <- c('t1','t2')\ndimnames(DataMZ) <- list(NULL,selVars)\ndimnames(DataDZ) <- list(NULL,selVars)\nsummary(DataMZ)\nsummary(DataDZ)\ncolMeans(DataMZ,na.rm=TRUE)\ncolMeans(DataDZ,na.rm=TRUE)\ncov(DataMZ,use=\"complete\")\ncov(DataDZ,use=\"complete\")\n\n# Specify and Run Saturated Model with RawData and Matrix-style Input\n# -----------------------------------------------------------------------\ntwinSatModel <- mxModel(\"twinSat\",\n\tmxModel(\"MZ\",\n\t\tmxMatrix(\"Full\", 1, 2, T, c(0,0), dimnames=list(NULL, selVars), name=\"expMeanMZ\"), \n\t\tmxMatrix(\"Lower\", 2, 2, T, .5, dimnames=list(selVars, selVars), name=\"CholMZ\"), \n\t\tmxAlgebra(CholMZ %*% t(CholMZ), name=\"expCovMZ\", dimnames=list(selVars, selVars)), \n\t\tmxData(DataMZ, type=\"raw\"), \n\t\tmxFIMLObjective(\"expCovMZ\", \"expMeanMZ\")\n\t),  \n\tmxModel(\"DZ\",\n\t\tmxMatrix(\"Full\", 1, 2, T, c(0,0), dimnames=list(NULL, selVars), name=\"expMeanDZ\"), \n\t\tmxMatrix(\"Lower\", 2, 2, T, .5, dimnames=list(selVars, selVars), name=\"CholDZ\"), \n\t\tmxAlgebra(CholDZ %*% t(CholDZ), name=\"expCovDZ\", dimnames=list(selVars, selVars)), \n\t\tmxData(DataDZ, type=\"raw\"), \n\t\tmxFIMLObjective(\"expCovDZ\", \"expMeanDZ\")\n\t),\n\tmxAlgebra(MZ.objective + DZ.objective, name=\"twin\"), \n\tmxFitFunctionAlgebra(\"twin\")\n)\ntwinSatFit <- mxRun(twinSatModel)\n\n# Generate Saturated Model Output\n# -----------------------------------------------------------------------\nExpMeanMZ <- mxEval(MZ.expMeanMZ, twinSatFit)\nExpCovMZ  <- mxEval(MZ.expCovMZ,  twinSatFit)\nExpMeanDZ <- mxEval(DZ.expMeanDZ, twinSatFit)\nExpCovDZ  <- mxEval(DZ.expCovDZ,  twinSatFit)\nLL_Sat    <- mxEval(objective,    twinSatFit)\n\n\n# Specify and Run Saturated SubModel 1 equating means across twin order\n# -----------------------------------------------------------------------\ntwinSatModelSub1 <- mxModel(twinSatModel,\n\tmxModel(\"MZ\",\n\t\tmxMatrix(\"Full\", 1, 2, T, 0, \"mMZ\", dimnames=list(NULL, selVars), name=\"expMeanMZ\")\n\t), \n\tmxModel(\"DZ\", \n\t\tmxMatrix(\"Full\", 1, 2, T, 0, \"mDZ\", dimnames=list(NULL, selVars), name=\"expMeanDZ\")\n\t)\n)\ntwinSatFitSub1 <- mxRun(twinSatModelSub1)\n\n# Specify and Run Saturated SubModel 2 equating means across twin order and zygosity\n# -----------------------------------------------------------------------\ntwinSatModelSub2 <- mxModel(twinSatModelSub1,\n\tmxModel(\"MZ\",\n\t\tmxMatrix(\"Full\", 1, 2, T, 0, \"mean\", dimnames=list(NULL, selVars), name=\"expMeanMZ\"), \n\t\tmxMatrix(\"Lower\", 2, 2, T, .5, labels= c(\"var\",\"MZcov\",\"var\"), \n\t\t    dimnames=list(selVars, selVars), name=\"CholMZ\")\n\t), \n\tmxModel(\"DZ\", \n\t\tmxMatrix(\"Full\", 1, 2, T, 0, \"mean\", dimnames=list(NULL, selVars), name=\"expMeanDZ\"), \n\t\tmxMatrix(\"Lower\", 2, 2, T, .5, labels= c(\"var\",\"DZcov\",\"var\"), \n\t\t    dimnames=list(selVars, selVars), name=\"CholDZ\")\n\t)\n)\ntwinSatFitSub2 <- mxRun(twinSatModelSub2)\n\n# Generate Saturated Model Comparison Output\n# -----------------------------------------------------------------------\nLL_Sat  <- mxEval(objective, twinSatFit)\nLL_Sub1 <- mxEval(objective, twinSatFitSub1)\nLRT1    <- LL_Sub1 - LL_Sat\nLL_Sub2 <- mxEval(objective, twinSatFitSub1)\nLRT2    <- LL_Sub2 - LL_Sat\n\n\n# Specify and Run ACE Model with RawData and Matrix-style Input\n# -----------------------------------------------------------------------\ntwinACEModel <- mxModel(\"twinACE\", \n\tmxMatrix(\"Full\", 1, 2, T, 20, \"mean\", dimnames=list(NULL, selVars), name=\"expMean\"), \n\t\t# Matrix expMean for expected mean vector for MZ and DZ twins    \n\tmxMatrix(\"Full\", nrow=1, ncol=1, free=TRUE, values=.6, label=\"a\", name=\"X\"),\n\tmxMatrix(\"Full\", nrow=1, ncol=1, free=TRUE, values=.6, label=\"c\", name=\"Y\"),\n\tmxMatrix(\"Full\", nrow=1, ncol=1, free=TRUE, values=.6, label=\"e\", name=\"Z\"),\n\t\t# Matrices X, Y, and Z to store the a, c, and e path coefficients\n\tmxMatrix(\"Full\", nrow=1, ncol=1, free=FALSE, values=.5, name=\"h\"),\n\tmxAlgebra(X * t(X), name=\"A\"),\n\tmxAlgebra(Y * t(Y), name=\"C\"),\n\tmxAlgebra(Z * t(Z), name=\"E\"),\n\t\t# Matrixes A, C, and E to compute A, C, and E variance components\n\tmxAlgebra(rbind(cbind(A+C+E   , A+C),\n\t\t\t\t\tcbind(A+C     , A+C+E)), dimnames = list(selVars, selVars), name=\"expCovMZ\"),\n\t\t# Matrix expCOVMZ for expected covariance matrix for MZ twins\n\tmxAlgebra(rbind(cbind(A+C+E   , h%x%A+C),\n\t\t\t\t\tcbind(h%x%A+C , A+C+E)), dimnames = list(selVars, selVars), name=\"expCovDZ\"),\n\t\t# Matrix expCOVMZ for expected covariance matrix for DZ twins\n\tmxModel(\"MZ\",\n\t\tmxData(DataMZ, type=\"raw\"), \n\t\tmxFIMLObjective(\"twinACE.expCovMZ\", \"twinACE.expMean\")),\n\tmxModel(\"DZ\", \n\t\tmxData(DataDZ, type=\"raw\"), \n\t\tmxFIMLObjective(\"twinACE.expCovDZ\", \"twinACE.expMean\")),\n\tmxAlgebra(MZ.objective + DZ.objective, name=\"twin\"), \n\tmxFitFunctionAlgebra(\"twin\")\n)\ntwinACEFit <- mxRun(twinACEModel)\n\n# Generate ACE Model Output\n# -----------------------------------------------------------------------\nLL_ACE <- mxEval(objective, twinACEFit)\nLRT_ACE= LL_ACE - LL_Sat\n\n#Retrieve expected mean vector and expected covariance matrices\n\tMZc <- mxEval(expCovMZ, twinACEFit)\n\tDZc <- mxEval(expCovDZ, twinACEFit)\n\tM   <- mxEval(expMean, twinACEFit)\n#Retrieve the A, C, and E variance components\n\tA <- mxEval(A, twinACEFit)\n\tC <- mxEval(C, twinACEFit)\n\tE <- mxEval(E, twinACEFit)\n#Calculate standardized variance components\n\tV  <- (A+C+E)\n\ta2 <- A/V\n\tc2 <- C/V\n\te2 <- E/V\n#Build and print reporting table with row and column names\n\tACEest <- rbind(cbind(A,C,E),cbind(a2,c2,e2)) \n\tACEest <- data.frame(ACEest, row.names=c(\"Variance Components\",\"Standardized VC\"))\n\tnames(ACEest)<-c(\"A\", \"C\", \"E\")\n \tACEest; LL_ACE; LRT_ACE\n\n# Specify and reduced AE Model (drop c $0)\n# -----------------------------------------------------------------------\ntwinAEModel <- mxModel(twinACEModel, name=\"twinAE\",\n    mxMatrix(\"Full\", nrow=1, ncol=1, free=F, values=0, label=\"c\", name=\"Y\")\n)\ntwinAEFit <- mxRun(twinAEModel)\n\n# Generate ACE Model Output\n# -----------------------------------------------------------------------\nLL_AE <- mxEval(objective, twinAEFit)\n#Retrieve expected mean vector and expected covariance matrices\n\tMZc <- mxEval(expCovMZ, twinAEFit)\n\tDZc <- mxEval(expCovDZ, twinAEFit)\n\tM   <- mxEval(expMean, twinAEFit)\n#Retrieve the A, C and E variance components\n\tA <- mxEval(A, twinAEFit)\n\tC <- mxEval(C, twinAEFit)\n\tE <- mxEval(E, twinAEFit)\n#Calculate standardized variance components\n\tV <- (A+C+E)\n\ta2 <- A/V\n\tc2 <- C/V\n\te2 <- E/V\n#Build and print reporting table with row and column names\n\tAEest <- rbind(cbind(A,C,E),cbind(a2,c2,e2)) \n\tAEest <- data.frame(ACEest, row.names=c(\"Variance Components\",\"Standardized VC\"))\n\tnames(ACEest)<-c(\"A\", \"C\", \"E\")\n\tAEest; LL_AE; \n#Calculate and print likelihood ratio test\n\tLRT_ACE_AE <- LL_AE - LL_ACE\n\tLRT_ACE_AE\n" }
{ "repo_name": "JuKa87/OpenMx", "ref": "refs/heads/julian", "path": "inst/models/failing/UnivariateTwinAnalysis20090925.R", "content": "# -----------------------------------------------------------------------\n# Program: UnivariateTwinAnalysis20090925.R  \n#  Author: Hermine Maes\n#    Date: Wed Sep 25 11:45:52 EDT 2009\n#\n# Revision History\n#   Hermine Maes -- Wed Sep 25 11:45:52 EDT 2009 UnivariateTwinAnalysis20090925.R\n# TODO include equivalent file for mx 1.x \n# TODO: Add omxCheckCloseEnough() calls\n# -----------------------------------------------------------------------\n\n\n# Simulate Data: two standardized variables t1 & t2 for MZ's & DZ's\n# -----------------------------------------------------------------------\nrequire(OpenMx)\nrequire(MASS)\n\nset.seed(200)\na2<-0.5\t\t#Additive genetic variance component (a squared)\nc2<-0.3\t\t#Common environment variance component (c squared)\ne2<-0.2\t\t#Specific environment variance component (e squared)\nrMZ <- a2+c2\nrDZ <- .5*a2+c2\nMZ  <- mvrnorm (1000, c(0,0), matrix(c(1,rMZ,rMZ,1),2,2))\nDZ  <- mvrnorm (1000, c(0,0), matrix(c(1,rDZ,rDZ,1),2,2))\n\nselVars <- c('t1','t2')\ndimnames(DataMZ) <- list(NULL,selVars)\ndimnames(DataDZ) <- list(NULL,selVars)\nsummary(DataMZ)\nsummary(DataDZ)\ncolMeans(DataMZ,na.rm=TRUE)\ncolMeans(DataDZ,na.rm=TRUE)\ncov(DataMZ,use=\"complete\")\ncov(DataDZ,use=\"complete\")\n\n# Specify and Run Saturated Model with RawData and Matrix-style Input\n# -----------------------------------------------------------------------\ntwinSatModel <- mxModel(\"twinSat\",\n\tmxModel(\"MZ\",\n\t\tmxMatrix(\"Full\", 1, 2, T, c(0,0), dimnames=list(NULL, selVars), name=\"expMeanMZ\"), \n\t\tmxMatrix(\"Lower\", 2, 2, T, .5, dimnames=list(selVars, selVars), name=\"CholMZ\"), \n\t\tmxAlgebra(CholMZ %*% t(CholMZ), name=\"expCovMZ\", dimnames=list(selVars, selVars)), \n\t\tmxData(DataMZ, type=\"raw\"), \n\t\tmxFIMLObjective(\"expCovMZ\", \"expMeanMZ\")\n\t),  \n\tmxModel(\"DZ\",\n\t\tmxMatrix(\"Full\", 1, 2, T, c(0,0), dimnames=list(NULL, selVars), name=\"expMeanDZ\"), \n\t\tmxMatrix(\"Lower\", 2, 2, T, .5, dimnames=list(selVars, selVars), name=\"CholDZ\"), \n\t\tmxAlgebra(CholDZ %*% t(CholDZ), name=\"expCovDZ\", dimnames=list(selVars, selVars)), \n\t\tmxData(DataDZ, type=\"raw\"), \n\t\tmxFIMLObjective(\"expCovDZ\", \"expMeanDZ\")\n\t),\n\tmxAlgebra(MZ.objective + DZ.objective, name=\"twin\"), \n\tmxFitFunctionAlgebra(\"twin\")\n)\ntwinSatFit <- mxRun(twinSatModel)\n\n# Generate Saturated Model Output\n# -----------------------------------------------------------------------\nExpMeanMZ <- mxEval(MZ.expMeanMZ, twinSatFit)\nExpCovMZ  <- mxEval(MZ.expCovMZ,  twinSatFit)\nExpMeanDZ <- mxEval(DZ.expMeanDZ, twinSatFit)\nExpCovDZ  <- mxEval(DZ.expCovDZ,  twinSatFit)\nLL_Sat    <- mxEval(objective,    twinSatFit)\n\n\n# Specify and Run Saturated SubModel 1 equating means across twin order\n# -----------------------------------------------------------------------\ntwinSatModelSub1 <- mxModel(twinSatModel,\n\tmxModel(\"MZ\",\n\t\tmxMatrix(\"Full\", 1, 2, T, 0, \"mMZ\", dimnames=list(NULL, selVars), name=\"expMeanMZ\")\n\t), \n\tmxModel(\"DZ\", \n\t\tmxMatrix(\"Full\", 1, 2, T, 0, \"mDZ\", dimnames=list(NULL, selVars), name=\"expMeanDZ\")\n\t)\n)\ntwinSatFitSub1 <- mxRun(twinSatModelSub1)\n\n# Specify and Run Saturated SubModel 2 equating means across twin order and zygosity\n# -----------------------------------------------------------------------\ntwinSatModelSub2 <- mxModel(twinSatModelSub1,\n\tmxModel(\"MZ\",\n\t\tmxMatrix(\"Full\", 1, 2, T, 0, \"mean\", dimnames=list(NULL, selVars), name=\"expMeanMZ\"), \n\t\tmxMatrix(\"Lower\", 2, 2, T, .5, labels= c(\"var\",\"MZcov\",\"var\"), \n\t\t    dimnames=list(selVars, selVars), name=\"CholMZ\")\n\t), \n\tmxModel(\"DZ\", \n\t\tmxMatrix(\"Full\", 1, 2, T, 0, \"mean\", dimnames=list(NULL, selVars), name=\"expMeanDZ\"), \n\t\tmxMatrix(\"Lower\", 2, 2, T, .5, labels= c(\"var\",\"DZcov\",\"var\"), \n\t\t    dimnames=list(selVars, selVars), name=\"CholDZ\")\n\t)\n)\ntwinSatFitSub2 <- mxRun(twinSatModelSub2)\n\n# Generate Saturated Model Comparison Output\n# -----------------------------------------------------------------------\nLL_Sat  <- mxEval(objective, twinSatFit)\nLL_Sub1 <- mxEval(objective, twinSatFitSub1)\nLRT1    <- LL_Sub1 - LL_Sat\nLL_Sub2 <- mxEval(objective, twinSatFitSub1)\nLRT2    <- LL_Sub2 - LL_Sat\n\n\n# Specify and Run ACE Model with RawData and Matrix-style Input\n# -----------------------------------------------------------------------\ntwinACEModel <- mxModel(\"twinACE\", \n\tmxMatrix(\"Full\", 1, 2, T, 20, \"mean\", dimnames=list(NULL, selVars), name=\"expMean\"), \n\t\t# Matrix expMean for expected mean vector for MZ and DZ twins    \n\tmxMatrix(\"Full\", nrow=1, ncol=1, free=TRUE, values=.6, label=\"a\", name=\"X\"),\n\tmxMatrix(\"Full\", nrow=1, ncol=1, free=TRUE, values=.6, label=\"c\", name=\"Y\"),\n\tmxMatrix(\"Full\", nrow=1, ncol=1, free=TRUE, values=.6, label=\"e\", name=\"Z\"),\n\t\t# Matrices X, Y, and Z to store the a, c, and e path coefficients\n\tmxMatrix(\"Full\", nrow=1, ncol=1, free=FALSE, values=.5, name=\"h\"),\n\tmxAlgebra(X * t(X), name=\"A\"),\n\tmxAlgebra(Y * t(Y), name=\"C\"),\n\tmxAlgebra(Z * t(Z), name=\"E\"),\n\t\t# Matrixes A, C, and E to compute A, C, and E variance components\n\tmxAlgebra(rbind(cbind(A+C+E   , A+C),\n\t\t\t\t\tcbind(A+C     , A+C+E)), dimnames = list(selVars, selVars), name=\"expCovMZ\"),\n\t\t# Matrix expCOVMZ for expected covariance matrix for MZ twins\n\tmxAlgebra(rbind(cbind(A+C+E   , h%x%A+C),\n\t\t\t\t\tcbind(h%x%A+C , A+C+E)), dimnames = list(selVars, selVars), name=\"expCovDZ\"),\n\t\t# Matrix expCOVMZ for expected covariance matrix for DZ twins\n\tmxModel(\"MZ\",\n\t\tmxData(DataMZ, type=\"raw\"), \n\t\tmxFIMLObjective(\"twinACE.expCovMZ\", \"twinACE.expMean\")),\n\tmxModel(\"DZ\", \n\t\tmxData(DataDZ, type=\"raw\"), \n\t\tmxFIMLObjective(\"twinACE.expCovDZ\", \"twinACE.expMean\")),\n\tmxAlgebra(MZ.objective + DZ.objective, name=\"twin\"), \n\tmxFitFunctionAlgebra(\"twin\")\n)\ntwinACEFit <- mxRun(twinACEModel)\n\n# Generate ACE Model Output\n# -----------------------------------------------------------------------\nLL_ACE <- mxEval(objective, twinACEFit)\nLRT_ACE= LL_ACE - LL_Sat\n\n#Retrieve expected mean vector and expected covariance matrices\n\tMZc <- mxEval(expCovMZ, twinACEFit)\n\tDZc <- mxEval(expCovDZ, twinACEFit)\n\tM   <- mxEval(expMean, twinACEFit)\n#Retrieve the A, C, and E variance components\n\tA <- mxEval(A, twinACEFit)\n\tC <- mxEval(C, twinACEFit)\n\tE <- mxEval(E, twinACEFit)\n#Calculate standardized variance components\n\tV  <- (A+C+E)\n\ta2 <- A/V\n\tc2 <- C/V\n\te2 <- E/V\n#Build and print reporting table with row and column names\n\tACEest <- rbind(cbind(A,C,E),cbind(a2,c2,e2)) \n\tACEest <- data.frame(ACEest, row.names=c(\"Variance Components\",\"Standardized VC\"))\n\tnames(ACEest)<-c(\"A\", \"C\", \"E\")\n \tACEest; LL_ACE; LRT_ACE\n\n# Specify and reduced AE Model (drop c $0)\n# -----------------------------------------------------------------------\ntwinAEModel <- mxModel(twinACEModel, name=\"twinAE\",\n    mxMatrix(\"Full\", nrow=1, ncol=1, free=F, values=0, label=\"c\", name=\"Y\")\n)\ntwinAEFit <- mxRun(twinAEModel)\n\n# Generate ACE Model Output\n# -----------------------------------------------------------------------\nLL_AE <- mxEval(objective, twinAEFit)\n#Retrieve expected mean vector and expected covariance matrices\n\tMZc <- mxEval(expCovMZ, twinAEFit)\n\tDZc <- mxEval(expCovDZ, twinAEFit)\n\tM   <- mxEval(expMean, twinAEFit)\n#Retrieve the A, C and E variance components\n\tA <- mxEval(A, twinAEFit)\n\tC <- mxEval(C, twinAEFit)\n\tE <- mxEval(E, twinAEFit)\n#Calculate standardized variance components\n\tV <- (A+C+E)\n\ta2 <- A/V\n\tc2 <- C/V\n\te2 <- E/V\n#Build and print reporting table with row and column names\n\tAEest <- rbind(cbind(A,C,E),cbind(a2,c2,e2)) \n\tAEest <- data.frame(ACEest, row.names=c(\"Variance Components\",\"Standardized VC\"))\n\tnames(ACEest)<-c(\"A\", \"C\", \"E\")\n\tAEest; LL_AE; \n#Calculate and print likelihood ratio test\n\tLRT_ACE_AE <- LL_AE - LL_ACE\n\tLRT_ACE_AE\n" }
{ "repo_name": "mathusuthan/autodidacti", "ref": "refs/heads/master", "path": "Stanford - ProbStat/01 EDA - Examining Distributions/04 - Quantitative Variable - Measures of Spread - Boxplots.R", "content": "# Load the data\nsetwd(\"./01 EDA - Examining Distributions\")\nload(\"data/actor_2013.RData\")\n\n# View the data\nactor_age\n\n# Five Number Summary\nsummary(actor_age$Age)\n\n# Standard Deviation\nsd(actor_age$Age)\n\n# Variance \nvar(actor_age$Age)\n\n# Inter-quartile Range\nIQR(actor_age$Age)\n\n# Sample Size\nlength(actor_age$Age)\n\n# Quantile\nquantile(actor_age$Age, 0.25)\n" }
{ "repo_name": "dataspecialiste/sagacite", "ref": "refs/heads/master", "path": "Stanford - ProbStat/01 EDA - Examining Distributions/04 - Quantitative Variable - Measures of Spread - Boxplots.R", "content": "# Load the data\nsetwd(\"./01 EDA - Examining Distributions\")\nload(\"data/actor_2013.RData\")\n\n# View the data\nactor_age\n\n# Five Number Summary\nsummary(actor_age$Age)\n\n# Standard Deviation\nsd(actor_age$Age)\n\n# Variance \nvar(actor_age$Age)\n\n# Inter-quartile Range\nIQR(actor_age$Age)\n\n# Sample Size\nlength(actor_age$Age)\n\n# Quantile\nquantile(actor_age$Age, 0.25)\n" }
{ "repo_name": "ChristosChristofidis/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/h2o-package/R/ast.R", "content": "#\n# A collection of methods to parse expressions and produce ASTs.\n#\n# R expressions convolved with H2O objects evaluate lazily.\n\n\n#'\n#' The AST visitor method.\n#'\n#' This method represents a map between an AST S4 object and a regular R list, which is suitable for rjson::toJSON\n#'\n#' Given a node, the `visitor` function recursively Lisp'ifies the node's S4 slots and then returns the list.\n#'\n#' The returned list has two main pieces: the ast to execute and function defintions:\n#'\n#'  { 'ast' : { ... }, 'funs' : {[ ... ]} }\n#'\n#' All ASTNodes have children. All nodes with the @@root slot has a list in the @@children slot that represent operands.\n.visitor<-\nfunction(node) {\n  if (is.list(node))\n    unlist(lapply(node, .visitor), use.names = FALSE)\n  else if (is(node, \"ASTNode\") || is(node, \"ASTSpan\"))\n    paste0(\"(\", node@root@op, \" \", paste0(.visitor(node@children), collapse = \" \"), \")\")\n  else if (is(node, \"ASTSeries\"))\n    paste0(\" \", node@op, paste0(.visitor(node@children), collapse = \";\"), \"}\")\n  else if (is(node, \"ASTEmpty\"))\n    node@key\n  else if (is(node, \"H2OFrame\"))\n    .visitor(.get(node))\n  else\n    node\n}\n\n#'\n#' Get the key or AST\n#'\n#' Key points to a bonified object in the H2O cluster\n.get <- function(H2OFrame) {\n  if(.is.eval(H2OFrame))\n    paste0('%', H2OFrame@frame_id)\n  else\n    H2OFrame@mutable$ast\n}\n\n#'\n#' Check if key points to bonified object in H2O cluster.\n#'\n.is.eval <- function(H2OFrame) {\n  key <- H2OFrame@frame_id\n  res <- .h2o.__remoteSend(H2OFrame@conn, h2oRestApiVersion = 99, paste0(.h2o.__RAPIDS, \"/isEval\"), ast_key=key)\n  res$evaluated\n}\n\n#'\n#' Get the class of the object from the envir.\n#'\n#' The environment is the parent frame\n\n#'\n#' Helper function to recursively unfurl an expression into a list of statements/exprs/calls/names.\n#'\n.as_list<-\nfunction(expr) {\n  if (is.call(expr))\n    lapply(as.list(expr), .as_list)\n  else\n    expr\n}\n\n#'\n#' Convert R expression to an AST.\n#'\n.eval<-\nfunction(x, envir, sub_one = TRUE) {\n  statements <- unlist(lapply(as.list(x), .as_list), recursive = TRUE)\n  anyH2OFrame <- FALSE\n  for (i in statements) {\n    anyH2OFrame <- tryCatch(is(i, \"H2OFrame\") ||\n                            is(get(as.character(i), envir), \"H2OFrame\"),\n                            error = function(e) FALSE)\n    if (anyH2OFrame)\n      break\n  }\n  if (anyH2OFrame)\n    x <- eval(x, envir)\n  .ast.walker(x, envir, FALSE, sub_one)\n}\n\n#'\n#' Walk the R AST directly\n#'\n#' Handles all of the 1 -> 0 indexing issues.\n#' TODO: this method needs to be cleaned up and re-written\n.ast.walker<-\nfunction(expr, envir, neg = FALSE, sub_one = TRUE) {\n  sub <- as.integer(sub_one)\n  if (length(expr) == 1L) {\n    if (is.symbol(expr)) { expr <- get(deparse(expr), envir); return(.ast.walker(expr, envir, neg, sub_one)) }\n    if (is.numeric(expr[[1L]])) {\n      if( expr < 0 ) sub <- 0\n      return(paste0('#', eval(expr[[1L]], envir=envir) - sub))\n    }\n    if (is.character(expr[[1L]])) return(deparse(expr[[1L]]))\n    if (is.character(expr)) return(deparse(expr))\n  }\n  if (isGeneric(deparse(expr[[1L]]))) {\n    # Have a vector => make a list\n    if ((expr[[1L]]) == quote(`c`)) {\n      children <- lapply(expr[-1L], .ast.walker, envir, neg, sub_one)\n      if( length(children)==1 ) {\n        if( is(children[[1]], \"ASTNode\") ) return(children)\n      }\n      op <- new(\"ASTApply\", op=\"llist\")\n      if( is(children[[1]], \"ASTNode\") ) { return(new(\"ASTNode\", root=op, children=children)) }\n      if( !(substr(children[[1]],1,1) == \"#\") ) { op <- new(\"ASTApply\", op=\"slist\") }\n      return(new(\"ASTNode\", root=op, children=children))\n\n    # handle the negative indexing cases\n    } else if (expr[[1L]] == quote(`-`)) {\n      # got some negative indexing!\n\n      # disallow binary ops here\n      if (length(expr) == 3L) {  # have a binary operation, e.g. 50 - 1\n        return(.eval(eval(expr,envir)))\n      }\n\n      new_expr <- as.list(expr[-1L])[[1L]]\n      if (length(new_expr) == 1L) {\n        if (is.symbol(new_expr)) new_expr <- get(deparse(new_expr), envir)\n        if (is.numeric(new_expr[[1L]])) return(paste0('#-', eval(new_expr[[1L]], envir=envir)))  # do not do the +1\n      }\n\n      if (isGeneric(deparse(new_expr[[1L]]))) {\n        if ((new_expr[[1L]]) == quote(`c`)) {\n          if (!identical(new_expr[[2L]][[1L]], quote(`:`))) {\n            children <- lapply(new_expr[-1L], .ast.walker, envir, neg, sub_one)\n            children <- lapply(children, function(x) if (is.character(x)) gsub('#', '', paste0('-', x)) else -x) # scrape off the '#', put in the - and continue...\n            children <- lapply(children, function(x) paste0('#', as.numeric(as.character(x)) - sub))\n            op <- new(\"ASTApply\", op=\"llist\")\n            return(new(\"ASTNode\", root=op, children=children))\n          } else {\n            if (length(as.list(new_expr[-1L])) < 2L) new_expr <- as.list(new_expr[-1L])\n            else return(.ast.walker(substitute(new_expr), envir, neg=TRUE, sub_one))\n          }\n        }\n      }\n\n      # otherwise `:` with negative indexing\n      if (identical(new_expr[[1L]][[1L]], quote(`:`))) {\n        return(new(\"ASTNode\", root = new(\"ASTApply\", op = \":\"),\n               children = list(paste0('#-', eval(new_expr[[1L]][[2L]], envir = envir)),\n                               paste0('#-', eval(new_expr[[1L]][[3L]], envir = envir)))))\n      }\n    } else if (length(expr) == 3L) {  # have a binary operation, e.g. 50 - 1\n      return(.eval(eval(expr,envir),envir))\n    }\n    # end negative expression cases\n  }\n\n  # Create a new ASTSpan\n  if (identical(expr[[1L]], quote(`:`))) {\n    if( eval(expr[[2L]],envir) < 0 ) {\n      neg <- TRUE\n      if( eval(expr[[3L]],envir) >= 0) stop(\"Index range must not include positive and negative values.\")\n    }\n    if (neg)\n      return(new(\"ASTNode\", root = new(\"ASTApply\", op = \":\"),\n                 children = list(paste0('#', eval(expr[[2L]], envir = envir)+1L),\n                                 paste0('#', eval(expr[[3L]], envir = envir)+1L))))\n    else\n      return(new(\"ASTNode\", root = new(\"ASTApply\", op = \":\"),\n                 children = list(paste0('#', eval(expr[[2L]], envir = envir) - 1L),\n                                 paste0('#', eval(expr[[3L]], envir = envir) - 1L))))\n  }\n\n  if (is.vector(expr) && is.numeric(expr)) {\n    neg <- expr[1] < 0\n    sub_one <- !neg\n    if( neg ) { expr <- expr + 1 }\n    children <- lapply(expr, .ast.walker, envir, neg, sub_one)\n    op <- new(\"ASTApply\", op=\"llist\")\n    if( !(substr(children[[1]],1,1) == \"#\") ) { op <- new(\"ASTApply\", op=\"slist\") }\n    return(new(\"ASTNode\", root=op, children=children))\n  }\n  stop(\"No suitable AST could be formed from the expression.\")\n}\n\n#'\n#' Retrieve values from arguments supplied in a function call.\n#'\n#' Developer Note: If a method takes a function as an argument and\n#'                 you wish to pass arguments to that function by the way of `...`\n#'                 then you before passing flowing control to .h2o.nary_op, you MUST\n#'                 label the `...` and list it.\n#'\n#'                   e.g.: Inside of ddply, we have the following \"fun_args\" pattern:\n#'                      .h2o.nary_op(\"ddply\", .data, vars, .fun, fun_args=list(...), .progress)\n.get.value.from.arg<-\nfunction(a, name=NULL) {\n  if (is(a, \"H2OFrame\")) {\n    .get(a)\n  } else if (is(a, \"ASTNode\")) {\n    a\n  } else if (is(a, \"ASTFun\")) {\n    paste0('%', a@name)\n  } else if (is(a, \"ASTEmpty\")) {\n    paste0('%', a@key)\n  } else {\n    res <- eval(a)\n    if (is.null(res)) {\n      \"()\"\n    } else if (is.vector(res)) {\n      if (length(res) > 1L) {\n        if (is.numeric(res)) res <- as.numeric(res)\n        if( is.numeric(res) && all(res%%1==0)) {\n          tt <- paste0('#', res, collapse=\" \")\n          paste0(\"(llist \", tt, \")\")\n        } else if( is.numeric(res) ) {\n          tt <- paste0('#', res, collapse=\" \")\n          paste0(\"(dlist \", tt, \")\")\n        } else {\n          tt <- paste0(unlist(lapply(res, deparse)), collapse= \" \")\n          paste0(\"(slist \", tt, \")\")\n        }\n      } else if (is.numeric(res)) {\n        paste0('#', res)\n      } else if (is.logical(res)) {\n        paste0('%', res)\n      } else {\n        deparse(eval(a))\n      }\n    } else {\n      deparse(eval(a))\n    }\n  }\n}\n\n.args.to.ast<-\nfunction(..., .args = list()) {\n  l <- list(...)\n  if (length(.args) != 0L) l <- .args\n  arg.names <- names(as.list(substitute(l))[-1L])\n  arg_values <- NULL\n  if (\"fun_args\" %in% arg.names) {\n    arg_values <- lapply(seq_along(l), function(i) {\n        if (names(l[i]) == \"fun_args\") {\n          paste(unlist(lapply(unlist(l[i]), function(i) { .get.value.from.arg(i, \"\") })), collapse= ' ')\n        } else .get.value.from.arg(l[[i]], names(l)[i])\n      })\n  } else {\n    arg_values <- lapply(seq_along(l), function(i) { .get.value.from.arg(l[[i]], names(l)[i]) })\n  }\n  arg_values\n}\n" }
{ "repo_name": "weaver-viii/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/h2o-package/R/ast.R", "content": "#\n# A collection of methods to parse expressions and produce ASTs.\n#\n# R expressions convolved with H2O objects evaluate lazily.\n\n\n#'\n#' The AST visitor method.\n#'\n#' This method represents a map between an AST S4 object and a regular R list, which is suitable for rjson::toJSON\n#'\n#' Given a node, the `visitor` function recursively Lisp'ifies the node's S4 slots and then returns the list.\n#'\n#' The returned list has two main pieces: the ast to execute and function defintions:\n#'\n#'  { 'ast' : { ... }, 'funs' : {[ ... ]} }\n#'\n#' All ASTNodes have children. All nodes with the @@root slot has a list in the @@children slot that represent operands.\n.visitor<-\nfunction(node) {\n  if (is.list(node))\n    unlist(lapply(node, .visitor), use.names = FALSE)\n  else if (is(node, \"ASTNode\") || is(node, \"ASTSpan\"))\n    paste0(\"(\", node@root@op, \" \", paste0(.visitor(node@children), collapse = \" \"), \")\")\n  else if (is(node, \"ASTSeries\"))\n    paste0(\" \", node@op, paste0(.visitor(node@children), collapse = \";\"), \"}\")\n  else if (is(node, \"ASTEmpty\"))\n    node@key\n  else if (is(node, \"H2OFrame\"))\n    .visitor(.get(node))\n  else\n    node\n}\n\n#'\n#' Get the key or AST\n#'\n#' Key points to a bonified object in the H2O cluster\n.get <- function(H2OFrame) {\n  if(.is.eval(H2OFrame))\n    paste0('%', H2OFrame@frame_id)\n  else\n    H2OFrame@mutable$ast\n}\n\n#'\n#' Check if key points to bonified object in H2O cluster.\n#'\n.is.eval <- function(H2OFrame) {\n  key <- H2OFrame@frame_id\n  res <- .h2o.__remoteSend(H2OFrame@conn, h2oRestApiVersion = 99, paste0(.h2o.__RAPIDS, \"/isEval\"), ast_key=key)\n  res$evaluated\n}\n\n#'\n#' Get the class of the object from the envir.\n#'\n#' The environment is the parent frame\n\n#'\n#' Helper function to recursively unfurl an expression into a list of statements/exprs/calls/names.\n#'\n.as_list<-\nfunction(expr) {\n  if (is.call(expr))\n    lapply(as.list(expr), .as_list)\n  else\n    expr\n}\n\n#'\n#' Convert R expression to an AST.\n#'\n.eval<-\nfunction(x, envir, sub_one = TRUE) {\n  statements <- unlist(lapply(as.list(x), .as_list), recursive = TRUE)\n  anyH2OFrame <- FALSE\n  for (i in statements) {\n    anyH2OFrame <- tryCatch(is(i, \"H2OFrame\") ||\n                            is(get(as.character(i), envir), \"H2OFrame\"),\n                            error = function(e) FALSE)\n    if (anyH2OFrame)\n      break\n  }\n  if (anyH2OFrame)\n    x <- eval(x, envir)\n  .ast.walker(x, envir, FALSE, sub_one)\n}\n\n#'\n#' Walk the R AST directly\n#'\n#' Handles all of the 1 -> 0 indexing issues.\n#' TODO: this method needs to be cleaned up and re-written\n.ast.walker<-\nfunction(expr, envir, neg = FALSE, sub_one = TRUE) {\n  sub <- as.integer(sub_one)\n  if (length(expr) == 1L) {\n    if (is.symbol(expr)) { expr <- get(deparse(expr), envir); return(.ast.walker(expr, envir, neg, sub_one)) }\n    if (is.numeric(expr[[1L]])) {\n      if( expr < 0 ) sub <- 0\n      return(paste0('#', eval(expr[[1L]], envir=envir) - sub))\n    }\n    if (is.character(expr[[1L]])) return(deparse(expr[[1L]]))\n    if (is.character(expr)) return(deparse(expr))\n  }\n  if (isGeneric(deparse(expr[[1L]]))) {\n    # Have a vector => make a list\n    if ((expr[[1L]]) == quote(`c`)) {\n      children <- lapply(expr[-1L], .ast.walker, envir, neg, sub_one)\n      if( length(children)==1 ) {\n        if( is(children[[1]], \"ASTNode\") ) return(children)\n      }\n      op <- new(\"ASTApply\", op=\"llist\")\n      if( is(children[[1]], \"ASTNode\") ) { return(new(\"ASTNode\", root=op, children=children)) }\n      if( !(substr(children[[1]],1,1) == \"#\") ) { op <- new(\"ASTApply\", op=\"slist\") }\n      return(new(\"ASTNode\", root=op, children=children))\n\n    # handle the negative indexing cases\n    } else if (expr[[1L]] == quote(`-`)) {\n      # got some negative indexing!\n\n      # disallow binary ops here\n      if (length(expr) == 3L) {  # have a binary operation, e.g. 50 - 1\n        return(.eval(eval(expr,envir)))\n      }\n\n      new_expr <- as.list(expr[-1L])[[1L]]\n      if (length(new_expr) == 1L) {\n        if (is.symbol(new_expr)) new_expr <- get(deparse(new_expr), envir)\n        if (is.numeric(new_expr[[1L]])) return(paste0('#-', eval(new_expr[[1L]], envir=envir)))  # do not do the +1\n      }\n\n      if (isGeneric(deparse(new_expr[[1L]]))) {\n        if ((new_expr[[1L]]) == quote(`c`)) {\n          if (!identical(new_expr[[2L]][[1L]], quote(`:`))) {\n            children <- lapply(new_expr[-1L], .ast.walker, envir, neg, sub_one)\n            children <- lapply(children, function(x) if (is.character(x)) gsub('#', '', paste0('-', x)) else -x) # scrape off the '#', put in the - and continue...\n            children <- lapply(children, function(x) paste0('#', as.numeric(as.character(x)) - sub))\n            op <- new(\"ASTApply\", op=\"llist\")\n            return(new(\"ASTNode\", root=op, children=children))\n          } else {\n            if (length(as.list(new_expr[-1L])) < 2L) new_expr <- as.list(new_expr[-1L])\n            else return(.ast.walker(substitute(new_expr), envir, neg=TRUE, sub_one))\n          }\n        }\n      }\n\n      # otherwise `:` with negative indexing\n      if (identical(new_expr[[1L]][[1L]], quote(`:`))) {\n        return(new(\"ASTNode\", root = new(\"ASTApply\", op = \":\"),\n               children = list(paste0('#-', eval(new_expr[[1L]][[2L]], envir = envir)),\n                               paste0('#-', eval(new_expr[[1L]][[3L]], envir = envir)))))\n      }\n    } else if (length(expr) == 3L) {  # have a binary operation, e.g. 50 - 1\n      return(.eval(eval(expr,envir),envir))\n    }\n    # end negative expression cases\n  }\n\n  # Create a new ASTSpan\n  if (identical(expr[[1L]], quote(`:`))) {\n    if( eval(expr[[2L]],envir) < 0 ) {\n      neg <- TRUE\n      if( eval(expr[[3L]],envir) >= 0) stop(\"Index range must not include positive and negative values.\")\n    }\n    if (neg)\n      return(new(\"ASTNode\", root = new(\"ASTApply\", op = \":\"),\n                 children = list(paste0('#', eval(expr[[2L]], envir = envir)+1L),\n                                 paste0('#', eval(expr[[3L]], envir = envir)+1L))))\n    else\n      return(new(\"ASTNode\", root = new(\"ASTApply\", op = \":\"),\n                 children = list(paste0('#', eval(expr[[2L]], envir = envir) - 1L),\n                                 paste0('#', eval(expr[[3L]], envir = envir) - 1L))))\n  }\n\n  if (is.vector(expr) && is.numeric(expr)) {\n    neg <- expr[1] < 0\n    sub_one <- !neg\n    if( neg ) { expr <- expr + 1 }\n    children <- lapply(expr, .ast.walker, envir, neg, sub_one)\n    op <- new(\"ASTApply\", op=\"llist\")\n    if( !(substr(children[[1]],1,1) == \"#\") ) { op <- new(\"ASTApply\", op=\"slist\") }\n    return(new(\"ASTNode\", root=op, children=children))\n  }\n  stop(\"No suitable AST could be formed from the expression.\")\n}\n\n#'\n#' Retrieve values from arguments supplied in a function call.\n#'\n#' Developer Note: If a method takes a function as an argument and\n#'                 you wish to pass arguments to that function by the way of `...`\n#'                 then you before passing flowing control to .h2o.nary_op, you MUST\n#'                 label the `...` and list it.\n#'\n#'                   e.g.: Inside of ddply, we have the following \"fun_args\" pattern:\n#'                      .h2o.nary_op(\"ddply\", .data, vars, .fun, fun_args=list(...), .progress)\n.get.value.from.arg<-\nfunction(a, name=NULL) {\n  if (is(a, \"H2OFrame\")) {\n    .get(a)\n  } else if (is(a, \"ASTNode\")) {\n    a\n  } else if (is(a, \"ASTFun\")) {\n    paste0('%', a@name)\n  } else if (is(a, \"ASTEmpty\")) {\n    paste0('%', a@key)\n  } else {\n    res <- eval(a)\n    if (is.null(res)) {\n      \"()\"\n    } else if (is.vector(res)) {\n      if (length(res) > 1L) {\n        if (is.numeric(res)) res <- as.numeric(res)\n        if( is.numeric(res) && all(res%%1==0)) {\n          tt <- paste0('#', res, collapse=\" \")\n          paste0(\"(llist \", tt, \")\")\n        } else if( is.numeric(res) ) {\n          tt <- paste0('#', res, collapse=\" \")\n          paste0(\"(dlist \", tt, \")\")\n        } else {\n          tt <- paste0(unlist(lapply(res, deparse)), collapse= \" \")\n          paste0(\"(slist \", tt, \")\")\n        }\n      } else if (is.numeric(res)) {\n        paste0('#', res)\n      } else if (is.logical(res)) {\n        paste0('%', res)\n      } else {\n        deparse(eval(a))\n      }\n    } else {\n      deparse(eval(a))\n    }\n  }\n}\n\n.args.to.ast<-\nfunction(..., .args = list()) {\n  l <- list(...)\n  if (length(.args) != 0L) l <- .args\n  arg.names <- names(as.list(substitute(l))[-1L])\n  arg_values <- NULL\n  if (\"fun_args\" %in% arg.names) {\n    arg_values <- lapply(seq_along(l), function(i) {\n        if (names(l[i]) == \"fun_args\") {\n          paste(unlist(lapply(unlist(l[i]), function(i) { .get.value.from.arg(i, \"\") })), collapse= ' ')\n        } else .get.value.from.arg(l[[i]], names(l)[i])\n      })\n  } else {\n    arg_values <- lapply(seq_along(l), function(i) { .get.value.from.arg(l[[i]], names(l)[i]) })\n  }\n  arg_values\n}\n" }
{ "repo_name": "riyuebao/CRI-Workshop-Nov2016-RNAseq", "ref": "refs/heads/master", "path": "notebook_ext/ipynb_data/Rscripts/DESeq2.set_variable.R", "content": "##-- Set up R plot display options in notebook\noptions(jupyter.plot_mimetypes = \"image/svg+xml\") \noptions(repr.plot.width = 6, repr.plot.height = 5)\n\nprint(paste0('Group 1 = ', group1))\nprint(paste0('Group 2 = ', group2))\ncomp <- paste0(group1, 'vs', group2, '.')\nout.prefix <- paste0(out.dir, '/', caller,'/',cancer,'.',\n                    gene.type, '.',comp, caller,'.txt')\n\n" }
{ "repo_name": "cribioinfo/CRI-Workshop-AMIA-2016-RNAseq", "ref": "refs/heads/master", "path": "notebook_ext/ipynb_data/Rscripts/DESeq2.set_variable.R", "content": "##-- Set up R plot display options in notebook\noptions(jupyter.plot_mimetypes = \"image/svg+xml\") \noptions(repr.plot.width = 6, repr.plot.height = 5)\n\nprint(paste0('Group 1 = ', group1))\nprint(paste0('Group 2 = ', group2))\ncomp <- paste0(group1, 'vs', group2, '.')\nout.prefix <- paste0(out.dir, '/', caller,'/',cancer,'.',\n                    gene.type, '.',comp, caller,'.txt')\n\n" }
{ "repo_name": "cloudyr/limer", "ref": "refs/heads/master", "path": "R/base64_to_df.R", "content": "#' Convert base64 encoded data to a data frame\n#'\n#' This function converts raw base64 results into a data frame.\n#' @param x \\dots\n#' @importFrom utils read.csv\n#' @export\n#' @examples \\dontrun{\n#' base64_to_df()\n#' }\n\nbase64_to_df <- function(x) {\n  raw_csv <- rawToChar(base64enc::base64decode(x))\n\n  return(read.csv(textConnection(raw_csv), stringsAsFactors = FALSE, sep = \";\"))\n}\n" }
{ "repo_name": "andrewheiss/limer", "ref": "refs/heads/master", "path": "R/base64_to_df.R", "content": "#' Convert base64 encoded data to a data frame\n#'\n#' This function converts raw base64 results into a data frame.\n#' @param x \\dots\n#' @importFrom utils read.csv\n#' @export\n#' @examples \\dontrun{\n#' base64_to_df()\n#' }\n\nbase64_to_df <- function(x) {\n  raw_csv <- rawToChar(base64enc::base64decode(x))\n\n  return(read.csv(textConnection(raw_csv), stringsAsFactors = FALSE, sep = \";\"))\n}\n" }
{ "repo_name": "codeaudit/Metronome", "ref": "refs/heads/master", "path": "src/test/resources/R/generate_4coef_reg_data.R", "content": "############################################################\n### Parameters\n\n# How many observations\nnumObservations = 100\n\n# Set the maximum number of values we can choose (randomly)\n# for x. Chosen to be 1/4 of the number of observations.\nmaxValueForX = floor(numObservations/10)\n\n# Intercept and coefficent\n\nintercept = 0\n\nx1coef = 1\n\nx2coef = 2\n\nx3coef = 3\n\nx4coef = 4\n\n\n# The variance on the response variable\nvariance = 10\n\n# The filename to use. Will not print if NULL\noutputFilename = \"lrdata.txt\"\n\n# Use command line args if given\nargs = commandArgs(TRUE)\nif (length(args) == 3) {\n  numObservations = as.numeric(args[1])\n#  intercept = as.numeric(args[2])\n#  x1coef = as.numeric(args[3])\n  variance = as.numeric(args[2])\n  outputFilename = args[3]\n}\n\n############################################################\n### Functions\n\n# The linear function that we'll reconstruct\nf <- function(x1, x2, x3, x4) { intercept + x1coef*x1 + x2coef*x2 + x3coef*x3 + x4coef*x4 }\n\n# A function to add the Gaussian error to the output of f\ng <- function(x1, x2, x3, x4) { rnorm(1,f(x1, x2, x3, x4),variance) }\n\n############################################################\n### Create data\n\n\n############################################################\n### Create and output the data\noutput = file(outputFilename,\"w\")\n### blocksize = 1000\n### numblocks = numObservations/blocksize\n\nprint(numObservations)\n\n#for (i in 1: numObservations) {\n\n  # Generate random x values\n  x1vals = floor(runif(numObservations,0,maxValueForX))\n  x2vals = floor(runif(numObservations,0,maxValueForX))\n  x3vals = floor(runif(numObservations,0,maxValueForX))\n  x4vals = floor(runif(numObservations,0,maxValueForX))\n\n  # Given the x values, generate y values according to g\n#  yvals = sapply(x1vals,x2vals,x3vals,x4vals,g)\n\nx1vals\n\nx2vals\n\n  for (j in 1: numObservations) {\n#  \ty = g(x1vals,x2vals,x3vals,x4vals)\n#    cat(yvals[j],\" |f 0:\", x1vals[j],\" 1:\", x2vals[j],\" 2:\", x3vals[j],\" 3:\", x4vals[j],\"\\n\",sep=\"\",file=output)\n    cat(g(x1vals[j],x2vals[j],x3vals[j],x4vals[j]),\" |f 0:\", x1vals[j],\" 1:\", x2vals[j],\" 2:\", x3vals[j],\" 3:\", x4vals[j],\"\\n\",sep=\"\",file=output)\n  }\n  flush(output)\n#}\nclose(output)\n" }
{ "repo_name": "jpatanooga/Metronome", "ref": "refs/heads/master", "path": "src/test/resources/R/generate_4coef_reg_data.R", "content": "############################################################\n### Parameters\n\n# How many observations\nnumObservations = 100\n\n# Set the maximum number of values we can choose (randomly)\n# for x. Chosen to be 1/4 of the number of observations.\nmaxValueForX = floor(numObservations/10)\n\n# Intercept and coefficent\n\nintercept = 0\n\nx1coef = 1\n\nx2coef = 2\n\nx3coef = 3\n\nx4coef = 4\n\n\n# The variance on the response variable\nvariance = 10\n\n# The filename to use. Will not print if NULL\noutputFilename = \"lrdata.txt\"\n\n# Use command line args if given\nargs = commandArgs(TRUE)\nif (length(args) == 3) {\n  numObservations = as.numeric(args[1])\n#  intercept = as.numeric(args[2])\n#  x1coef = as.numeric(args[3])\n  variance = as.numeric(args[2])\n  outputFilename = args[3]\n}\n\n############################################################\n### Functions\n\n# The linear function that we'll reconstruct\nf <- function(x1, x2, x3, x4) { intercept + x1coef*x1 + x2coef*x2 + x3coef*x3 + x4coef*x4 }\n\n# A function to add the Gaussian error to the output of f\ng <- function(x1, x2, x3, x4) { rnorm(1,f(x1, x2, x3, x4),variance) }\n\n############################################################\n### Create data\n\n\n############################################################\n### Create and output the data\noutput = file(outputFilename,\"w\")\n### blocksize = 1000\n### numblocks = numObservations/blocksize\n\nprint(numObservations)\n\n#for (i in 1: numObservations) {\n\n  # Generate random x values\n  x1vals = floor(runif(numObservations,0,maxValueForX))\n  x2vals = floor(runif(numObservations,0,maxValueForX))\n  x3vals = floor(runif(numObservations,0,maxValueForX))\n  x4vals = floor(runif(numObservations,0,maxValueForX))\n\n  # Given the x values, generate y values according to g\n#  yvals = sapply(x1vals,x2vals,x3vals,x4vals,g)\n\nx1vals\n\nx2vals\n\n  for (j in 1: numObservations) {\n#  \ty = g(x1vals,x2vals,x3vals,x4vals)\n#    cat(yvals[j],\" |f 0:\", x1vals[j],\" 1:\", x2vals[j],\" 2:\", x3vals[j],\" 3:\", x4vals[j],\"\\n\",sep=\"\",file=output)\n    cat(g(x1vals[j],x2vals[j],x3vals[j],x4vals[j]),\" |f 0:\", x1vals[j],\" 1:\", x2vals[j],\" 2:\", x3vals[j],\" 3:\", x4vals[j],\"\\n\",sep=\"\",file=output)\n  }\n  flush(output)\n#}\nclose(output)\n" }
{ "repo_name": "wesm/arrow", "ref": "refs/heads/master", "path": "r/R/list.R", "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n#' @include type.R\n\nListType <- R6Class(\"ListType\",\n  inherit = NestedType,\n  active = list(\n    value_field = function() shared_ptr(Field, ListType__value_field(self)),\n    value_type = function() DataType$create(ListType__value_type(self))\n  )\n)\n\n#' @rdname data-type\n#' @export\nlist_of <- function(type) shared_ptr(ListType, list__(type))\n\nLargeListType <- R6Class(\"LargeListType\",\n  inherit = NestedType,\n  active = list(\n    value_field = function() shared_ptr(Field, LargeListType__value_field(self)),\n    value_type = function() DataType$create(LargeListType__value_type(self))\n  )\n)\n\n#' @rdname data-type\n#' @export\nlarge_list_of <- function(type) shared_ptr(LargeListType, large_list__(type))\n\n#' @rdname data-type\n#' @export\nFixedSizeListType <- R6Class(\"FixedSizeListType\",\n  inherit = NestedType,\n  active = list(\n    value_field = function() shared_ptr(Field, FixedSizeListType__value_field(self)),\n    value_type = function() DataType$create(FixedSizeListType__value_type(self)),\n    list_size = function() FixedSizeListType__list_size(self)\n  )\n)\n\n#' @rdname data-type\n#' @export\nfixed_size_list_of <- function(type, list_size) shared_ptr(LargeListType, fixed_size_list__(type, list_size))\n" }
{ "repo_name": "laurentgo/arrow", "ref": "refs/heads/master", "path": "r/R/list.R", "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n#' @include type.R\n\nListType <- R6Class(\"ListType\",\n  inherit = NestedType,\n  active = list(\n    value_field = function() shared_ptr(Field, ListType__value_field(self)),\n    value_type = function() DataType$create(ListType__value_type(self))\n  )\n)\n\n#' @rdname data-type\n#' @export\nlist_of <- function(type) shared_ptr(ListType, list__(type))\n\nLargeListType <- R6Class(\"LargeListType\",\n  inherit = NestedType,\n  active = list(\n    value_field = function() shared_ptr(Field, LargeListType__value_field(self)),\n    value_type = function() DataType$create(LargeListType__value_type(self))\n  )\n)\n\n#' @rdname data-type\n#' @export\nlarge_list_of <- function(type) shared_ptr(LargeListType, large_list__(type))\n\n#' @rdname data-type\n#' @export\nFixedSizeListType <- R6Class(\"FixedSizeListType\",\n  inherit = NestedType,\n  active = list(\n    value_field = function() shared_ptr(Field, FixedSizeListType__value_field(self)),\n    value_type = function() DataType$create(FixedSizeListType__value_type(self)),\n    list_size = function() FixedSizeListType__list_size(self)\n  )\n)\n\n#' @rdname data-type\n#' @export\nfixed_size_list_of <- function(type, list_size) shared_ptr(LargeListType, fixed_size_list__(type, list_size))\n" }
{ "repo_name": "lajus/customr", "ref": "refs/heads/master", "path": "src/library/base/R/interaction.R", "content": "#  File src/library/base/R/interaction.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2012 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n### This is almost like the Primitive \":\" for factors\n### but with drop=TRUE, used in reshape\ninteraction <- function(..., drop = FALSE, sep = \".\", lex.order = FALSE)\n{\n    args <- list(...)\n    narg <- length(args)\n    if (narg == 1L && is.list(args[[1L]])) {\n\targs <- args[[1L]]\n\tnarg <- length(args)\n    }\n    for(i in narg:1L) {\n        f <- as.factor(args[[i]])[, drop = drop]\n        l <- levels(f)\n        if1 <- as.integer(f) - 1L\n        if(i == narg) {\n            ans <- if1\n            lvs <- l\n        } else {\n            if(lex.order) {\n                ll <- length(lvs)\n                ans <- ans + ll * if1\n                lvs <- paste(rep(l, each = ll), rep(lvs, length(l)), sep=sep)\n            } else {\n                ans <- ans * length(l) + if1\n                lvs <- paste(rep(l, length(lvs)),\n                             rep(lvs, each = length(l)), sep=sep)\n            }\n            if(anyDuplicated(lvs)) { ## fix them up\n                ulvs <- unique(lvs)\n                while((i <- anyDuplicated(flv <- match(lvs, ulvs)))) {\n                    lvs <- lvs[-i]\n                    ans[ans+1L == i] <- match(flv[i], flv[1:(i-1)]) - 1L\n                    ans[ans+1L > i] <- ans[ans+1L > i] - 1L\n                }\n                lvs <- ulvs\n            }\n            if(drop) {\n                olvs <- lvs\n                lvs <- lvs[sort(unique(ans+1L))]\n                ans <- match(olvs[ans+1L], lvs) - 1L\n            }\n        }\n    }\n    structure(as.integer(ans+1L), levels=lvs, class = \"factor\")\n}\n" }
{ "repo_name": "cxxr-devel/cxxr-svn-mirror", "ref": "refs/heads/master", "path": "src/library/base/R/interaction.R", "content": "#  File src/library/base/R/interaction.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2012 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n### This is almost like the Primitive \":\" for factors\n### but with drop=TRUE, used in reshape\ninteraction <- function(..., drop = FALSE, sep = \".\", lex.order = FALSE)\n{\n    args <- list(...)\n    narg <- length(args)\n    if (narg == 1L && is.list(args[[1L]])) {\n\targs <- args[[1L]]\n\tnarg <- length(args)\n    }\n    for(i in narg:1L) {\n        f <- as.factor(args[[i]])[, drop = drop]\n        l <- levels(f)\n        if1 <- as.integer(f) - 1L\n        if(i == narg) {\n            ans <- if1\n            lvs <- l\n        } else {\n            if(lex.order) {\n                ll <- length(lvs)\n                ans <- ans + ll * if1\n                lvs <- paste(rep(l, each = ll), rep(lvs, length(l)), sep=sep)\n            } else {\n                ans <- ans * length(l) + if1\n                lvs <- paste(rep(l, length(lvs)),\n                             rep(lvs, each = length(l)), sep=sep)\n            }\n            if(anyDuplicated(lvs)) { ## fix them up\n                ulvs <- unique(lvs)\n                while((i <- anyDuplicated(flv <- match(lvs, ulvs)))) {\n                    lvs <- lvs[-i]\n                    ans[ans+1L == i] <- match(flv[i], flv[1:(i-1)]) - 1L\n                    ans[ans+1L > i] <- ans[ans+1L > i] - 1L\n                }\n                lvs <- ulvs\n            }\n            if(drop) {\n                olvs <- lvs\n                lvs <- lvs[sort(unique(ans+1L))]\n                ans <- match(olvs[ans+1L], lvs) - 1L\n            }\n        }\n    }\n    structure(as.integer(ans+1L), levels=lvs, class = \"factor\")\n}\n" }
{ "repo_name": "tijoseymathew/mlr", "ref": "refs/heads/master", "path": "R/removeConstantFeatures.R", "content": "#' @title Remove constant features from a data set.\n#'\n#' @description\n#' Constant features can lead to errors in some models and obviously provide\n#' no information in the training set that can be learned from.\n#' With the argument \\dQuote{perc}, there is a possibility to also remove\n#' features for which less than \\dQuote{perc} percent of the observations\n#' differ from the mode value.\n#'\n#' @template arg_taskdf\n#' @param perc [\\code{numeric(1)}]\\cr\n#'   The percentage of a feature values in [0, 1) that must differ from the mode value.\n#'   Default is 0, which means only constant features with exactly one observed level are removed.\n#' @param dont.rm [\\code{character}]\\cr\n#'   Names of the columns which must not be deleted.\n#'   Default is no columns.\n#' @param na.ignore [\\code{logical(1)}]\\cr\n#'   Should NAs be ignored in the percentage calculation?\n#'   (Or should they be treated as a single, extra level in the percentage calculation?)\n#'   Note that if the feature has only missing values, it is always removed.\n#'   Default is \\code{FALSE}.\n#' @param tol [\\code{numeric(1)}]\\cr\n#'   Numerical tolerance to treat two numbers as equal.\n#'   Variables stored as \\code{double} will get rounded accordingly before computing the mode.\n#'   Default is \\code{sqrt(.Maschine$double.eps)}.\n#' @template arg_showinfo\n#' @template ret_taskdf\n#' @export\n#' @family eda_and_preprocess\nremoveConstantFeatures = function(obj, perc = 0, dont.rm = character(0L), na.ignore = FALSE, tol = .Machine$double.eps^.5, show.info = getMlrOption(\"show.info\")) {\n  UseMethod(\"removeConstantFeatures\")\n}\n\n#' @export\nremoveConstantFeatures.Task = function(obj, perc = 0, dont.rm = character(0L), na.ignore = FALSE, tol = .Machine$double.eps^.5, show.info = getMlrOption(\"show.info\")) {\n  assertCharacter(dont.rm)\n  dont.rm = union(dont.rm, getTaskTargetNames(obj))\n  data = removeConstantFeatures(getTaskData(obj), perc = perc, dont.rm = dont.rm, na.ignore = na.ignore, tol = tol, show.info = show.info)\n  changeData(task = obj, data = data)\n}\n\n#' @export\nremoveConstantFeatures.data.frame = function(obj, perc = 0, dont.rm = character(0L), na.ignore = FALSE, tol = .Machine$double.eps^.5, show.info = getMlrOption(\"show.info\")) {\n  assertNumber(perc, lower = 0, upper = 1)\n  assertSubset(dont.rm, choices = names(obj))\n  assertFlag(na.ignore)\n  assertNumber(tol, lower = 0)\n  assertFlag(show.info)\n\n  if (any(!dim(obj)))\n    return(obj)\n\n  isEqual = function(x, y) {\n    res = (x == y) | (is.na(x) & is.na(y))\n    replace(res, is.na(res), FALSE)\n  }\n  digits = ceiling(log10(1 / tol))\n  cns = setdiff(colnames(obj), dont.rm)\n  ratio = vnapply(obj[cns], function(x) {\n    if (allMissing(x))\n      return(0)\n    if (is.double(x))\n      x = round(x, digits = digits)\n    m = computeMode(x, na.rm = na.ignore, ties.method = \"first\")\n    if (na.ignore) {\n      mean(m != x, na.rm = TRUE)\n    } else {\n      mean(!isEqual(x, m))\n    }\n  }, use.names = FALSE)\n\n  dropcols = cns[ratio <= perc]\n  if (show.info && length(dropcols))\n    messagef(\"Removing %i columns: %s\", length(dropcols), collapse(dropcols))\n  dropNamed(obj, dropcols)\n}\n\n" }
{ "repo_name": "vinaywv/mlr", "ref": "refs/heads/master", "path": "R/removeConstantFeatures.R", "content": "#' @title Remove constant features from a data set.\n#'\n#' @description\n#' Constant features can lead to errors in some models and obviously provide\n#' no information in the training set that can be learned from.\n#' With the argument \\dQuote{perc}, there is a possibility to also remove\n#' features for which less than \\dQuote{perc} percent of the observations\n#' differ from the mode value.\n#'\n#' @template arg_taskdf\n#' @param perc [\\code{numeric(1)}]\\cr\n#'   The percentage of a feature values in [0, 1) that must differ from the mode value.\n#'   Default is 0, which means only constant features with exactly one observed level are removed.\n#' @param dont.rm [\\code{character}]\\cr\n#'   Names of the columns which must not be deleted.\n#'   Default is no columns.\n#' @param na.ignore [\\code{logical(1)}]\\cr\n#'   Should NAs be ignored in the percentage calculation?\n#'   (Or should they be treated as a single, extra level in the percentage calculation?)\n#'   Note that if the feature has only missing values, it is always removed.\n#'   Default is \\code{FALSE}.\n#' @param tol [\\code{numeric(1)}]\\cr\n#'   Numerical tolerance to treat two numbers as equal.\n#'   Variables stored as \\code{double} will get rounded accordingly before computing the mode.\n#'   Default is \\code{sqrt(.Maschine$double.eps)}.\n#' @template arg_showinfo\n#' @template ret_taskdf\n#' @export\n#' @family eda_and_preprocess\nremoveConstantFeatures = function(obj, perc = 0, dont.rm = character(0L), na.ignore = FALSE, tol = .Machine$double.eps^.5, show.info = getMlrOption(\"show.info\")) {\n  UseMethod(\"removeConstantFeatures\")\n}\n\n#' @export\nremoveConstantFeatures.Task = function(obj, perc = 0, dont.rm = character(0L), na.ignore = FALSE, tol = .Machine$double.eps^.5, show.info = getMlrOption(\"show.info\")) {\n  assertCharacter(dont.rm)\n  dont.rm = union(dont.rm, getTaskTargetNames(obj))\n  data = removeConstantFeatures(getTaskData(obj), perc = perc, dont.rm = dont.rm, na.ignore = na.ignore, tol = tol, show.info = show.info)\n  changeData(task = obj, data = data)\n}\n\n#' @export\nremoveConstantFeatures.data.frame = function(obj, perc = 0, dont.rm = character(0L), na.ignore = FALSE, tol = .Machine$double.eps^.5, show.info = getMlrOption(\"show.info\")) {\n  assertNumber(perc, lower = 0, upper = 1)\n  assertSubset(dont.rm, choices = names(obj))\n  assertFlag(na.ignore)\n  assertNumber(tol, lower = 0)\n  assertFlag(show.info)\n\n  if (any(!dim(obj)))\n    return(obj)\n\n  isEqual = function(x, y) {\n    res = (x == y) | (is.na(x) & is.na(y))\n    replace(res, is.na(res), FALSE)\n  }\n  digits = ceiling(log10(1 / tol))\n  cns = setdiff(colnames(obj), dont.rm)\n  ratio = vnapply(obj[cns], function(x) {\n    if (allMissing(x))\n      return(0)\n    if (is.double(x))\n      x = round(x, digits = digits)\n    m = computeMode(x, na.rm = na.ignore, ties.method = \"first\")\n    if (na.ignore) {\n      mean(m != x, na.rm = TRUE)\n    } else {\n      mean(!isEqual(x, m))\n    }\n  }, use.names = FALSE)\n\n  dropcols = cns[ratio <= perc]\n  if (show.info && length(dropcols))\n    messagef(\"Removing %i columns: %s\", length(dropcols), collapse(dropcols))\n  dropNamed(obj, dropcols)\n}\n\n" }
{ "repo_name": "jpritikin/OpenMx", "ref": "refs/heads/master", "path": "inst/models/failing/CSOLNPmtcars.R", "content": "# ===========\n# = HISTORY =\n# ===========\n# 2017-04-14 04:53PM Check by TBATES and still failing checks lines 48 and 49\n# models/failing/CSOLNPmtcars.R\n# CSOLNP Not taking big enough steps?\n\nlibrary(OpenMx)\n# ===================\n# = test with NPSOL =\n# ===================\n\nexpectedVars = as.numeric(diag(cov(mtcars[,manifests])))\nmlexpected = expectedVars*(31/32) # n-1/n\n\n# ====================================\n# = make a simple independence model =\n# ====================================\nmanifests = c(\"mpg\", \"disp\", \"gear\")\nm1 <- mxModel(\"ind\", type = \"RAM\",\n\tmanifestVars = manifests,\n\tmxPath(from = manifests, arrows = 2),\n\tmxPath(from = \"one\", to = manifests),\n\tmxData(mtcars[,manifests], type=\"raw\")\n)\n\n# ======================================\n# = get parameter estimated parameters =\n# ======================================\nmxOption(NULL, \"Default optimizer\", \"NPSOL\")\nm2 = mxRun(m1)\nobtainedVars = summary(m2)$parameters[1:3,5]\nomxCheckWithinPercentError(obtainedVars[1], mlexpected[1], percent = 0.1)\nomxCheckWithinPercentError(obtainedVars[2], mlexpected[2], percent = 0.1)\nomxCheckWithinPercentError(obtainedVars[3], mlexpected[3], percent = 0.1)\n\n# ====================\n# = switch to CSOLNP =\n# ====================\n\nmxOption(NULL, \"Default optimizer\", \"CSOLNP\")\nm3 = mxRun(m1)\n# In model 'ind' Optimizer returned a non-zero status code 5. The Hessian at the solution does not appear to be convex. See ?mxCheckIdentification for possible diagnosis (Mx status RED).\nomxCheckTrue(mxCheckIdentification(m1)$status)\n\nobtainedVars = summary(m3)$parameters[1:3,5]\n\nomxCheckWithinPercentError(obtainedVars[1], mlexpected[1], percent = 0.01)\nomxCheckWithinPercentError(obtainedVars[2], mlexpected[2], percent = 0.01)\nomxCheckWithinPercentError(obtainedVars[3], mlexpected[3], percent = 0.01)\n# e.g\n# Error: obtainedVars[3] not equal to expectedVars[3]\n# Mean relative difference: 34520.75" }
{ "repo_name": "canaantt/Oncoscape", "ref": "refs/heads/master", "path": "dataPackages/TCGAcoadread/tests/runTests.R", "content": "BiocGenerics:::testPackage(\"TCGAcoadread\")\n" }
{ "repo_name": "klatoza/Oncoscape", "ref": "refs/heads/master", "path": "dataPackages/TCGAcoadread/tests/runTests.R", "content": "BiocGenerics:::testPackage(\"TCGAcoadread\")\n" }
{ "repo_name": "oncoscape/Oncoscape", "ref": "refs/heads/master", "path": "dataPackages/TCGAcoadread/tests/runTests.R", "content": "BiocGenerics:::testPackage(\"TCGAcoadread\")\n" }
{ "repo_name": "pshannon-bioc/Oncoscape", "ref": "refs/heads/master", "path": "dataPackages/TCGAcoadread/tests/runTests.R", "content": "BiocGenerics:::testPackage(\"TCGAcoadread\")\n" }
{ "repo_name": "paultcochrane/rbokeh", "ref": "refs/heads/master", "path": "man-roxygen/par-url.R", "content": "#' @param url a string of URLs or a single string that references a variable name (via @@var_name) that can be found and extracted from the \\code{data} argument\n" }
{ "repo_name": "jrounds/rbokeh", "ref": "refs/heads/master", "path": "man-roxygen/par-url.R", "content": "#' @param url a string of URLs or a single string that references a variable name (via @@var_name) that can be found and extracted from the \\code{data} argument\n" }
{ "repo_name": "timelyportfolio/rBokeh", "ref": "refs/heads/master", "path": "man-roxygen/par-url.R", "content": "#' @param url a string of URLs or a single string that references a variable name (via @@var_name) that can be found and extracted from the \\code{data} argument\n" }
{ "repo_name": "sinhrks/rbokeh", "ref": "refs/heads/master", "path": "man-roxygen/par-url.R", "content": "#' @param url a string of URLs or a single string that references a variable name (via @@var_name) that can be found and extracted from the \\code{data} argument\n" }
{ "repo_name": "mhunter1/OpenMx", "ref": "refs/heads/master", "path": "inst/models/enormous/RAM-3Factor-192Indicators-covdata-a.R", "content": "# ---------------------------------------------------------------------\n# Program: RAM-3Factor-12Indicators.R\n#  Author: Steven M. Boker\n#    Date: Fri Jul 30 13:45:12 EDT 2010\n#\n# This program is a factor model using standard RAM.\n#\n# ---------------------------------------------------------------------\n# Revision History\n#    -- Fri Jul 30 13:45:12 EDT 2010\n#      Created RAM-3Factor-12Indicators.R.\n#\n# ---------------------------------------------------------------------\n\n# ----------------------------------\n# Read libraries and set options.\n\nlibrary(OpenMx)\n\noptions(width=100)\n\n# ---------------------------------------------------------------------\n# Data for factor model.\n\nnumberSubjects <- 1000\nnumberFactors <- 3\nnumberIndPerFactor <- 64\nnumberIndicators <- numberIndPerFactor*numberFactors # must be a multiple of numberFactors\n\nXMatrix <- matrix(rnorm(numberSubjects*numberFactors, mean=0, sd=1), numberSubjects, numberFactors)\n\ntLoadings <- c(1, seq(.5, .9, length.out=(numberIndPerFactor-1)), rep(0, numberIndPerFactor*2),\n  rep(0, numberIndPerFactor*1), 1, seq(.5, .9, length.out=(numberIndPerFactor-1)), rep(0, numberIndPerFactor*1),\n  rep(0, numberIndPerFactor*2), 1, seq(.5, .9, length.out=(numberIndPerFactor-1)))\nBMatrix <- matrix(tLoadings, numberFactors, numberIndicators, byrow=TRUE)\nUMatrix <- matrix(rnorm(numberSubjects*numberIndicators, mean=0, sd=1), numberSubjects, numberIndicators)\nYMatrix <- XMatrix %*% BMatrix + UMatrix\n\ncor(XMatrix)\n\ndimnames(YMatrix) <- list(NULL, paste(\"X\", 1:numberIndicators, sep=\"\"))\n\nYFrame <- data.frame(YMatrix)\n\nround(cor(YFrame), 3)\nround(cov(YFrame), 3)\n\nindicators <- paste(\"X\", 1:numberIndicators, sep=\"\")\ntotalVars <- numberIndicators + numberFactors\n\n# ----------------------------------\n# Build an orthogonal simple structure factor model\n\nlatents <- paste(\"F\", 1:numberFactors, sep=\"\")\n\nuniqueLabels <- paste(\"U_\", indicators, sep=\"\")\nmeanLabels <- paste(\"M_\", latents, sep=\"\")\nfactorVarLabels <- paste(\"Var_\", latents, sep=\"\")\n\nlatents1 <- latents[1]\nindicators1 <- indicators[1:numberIndPerFactor]\nloadingLabels1 <- paste(\"b_F1\", indicators[1:numberIndPerFactor], sep=\"\") \nlatents2 <- latents[2]\nindicators2 <- indicators[numberIndPerFactor+(1:numberIndPerFactor)]\nloadingLabels2 <- paste(\"b_F2\", indicators[numberIndPerFactor+(1:numberIndPerFactor)], sep=\"\") \nlatents3 <- latents[3]\nindicators3 <- indicators[(2*numberIndPerFactor)+(1:numberIndPerFactor)]\nloadingLabels3 <- paste(\"b_F3\", indicators[(2*numberIndPerFactor)+(1:numberIndPerFactor)], sep=\"\") \n\nthreeFactorOrthogonal <- mxModel(\"threeFactorOrthogonal\",\n    type=\"RAM\",\n    manifestVars=c(indicators),\n    latentVars=c(latents,\"dummy1\"),\n    mxPath(from=latents1, to=indicators1, \n           arrows=1, all=TRUE, \n           free=TRUE, values=.2, \n           labels=loadingLabels1),\n    mxPath(from=latents2, to=indicators2, \n           arrows=1, all=TRUE, \n           free=TRUE, values=.2, \n           labels=loadingLabels2),\n    mxPath(from=latents3, to=indicators3, \n           arrows=1, all=TRUE, \n           free=TRUE, values=.2, \n           labels=loadingLabels3),\n    mxPath(from=latents1, to=indicators1[1], \n           arrows=1, \n           free=FALSE, values=1),\n    mxPath(from=latents2, to=indicators2[1], \n           arrows=1, \n           free=FALSE, values=1),\n    mxPath(from=latents3, to=indicators3[1], \n           arrows=1, \n           free=FALSE, values=1),\n    mxPath(from=indicators, \n           arrows=2, \n           free=TRUE, values=.2, \n           labels=uniqueLabels),\n    mxPath(from=latents,\n           arrows=2, \n           free=TRUE, values=.8, \n           labels=factorVarLabels),\n    mxPath(from=\"one\", to=indicators, \n           arrows=1, free=FALSE, values=0),\n    mxPath(from=\"one\", to=c(latents), \n           arrows=1, free=TRUE, values=.1, \n           labels=meanLabels),\n    mxData(observed=cov(YFrame), means=mean(YFrame), \n\tnumObs=nrow(YFrame), type=\"cov\")\n    )\n\nthreeFactorOrthogonalOut <- mxRun(threeFactorOrthogonal)\nsummary(threeFactorOrthogonalOut)\n\n" }
{ "repo_name": "jpritikin/OpenMx", "ref": "refs/heads/master", "path": "inst/models/enormous/RAM-3Factor-192Indicators-covdata-a.R", "content": "# ---------------------------------------------------------------------\n# Program: RAM-3Factor-12Indicators.R\n#  Author: Steven M. Boker\n#    Date: Fri Jul 30 13:45:12 EDT 2010\n#\n# This program is a factor model using standard RAM.\n#\n# ---------------------------------------------------------------------\n# Revision History\n#    -- Fri Jul 30 13:45:12 EDT 2010\n#      Created RAM-3Factor-12Indicators.R.\n#\n# ---------------------------------------------------------------------\n\n# ----------------------------------\n# Read libraries and set options.\n\nlibrary(OpenMx)\n\noptions(width=100)\n\n# ---------------------------------------------------------------------\n# Data for factor model.\n\nnumberSubjects <- 1000\nnumberFactors <- 3\nnumberIndPerFactor <- 64\nnumberIndicators <- numberIndPerFactor*numberFactors # must be a multiple of numberFactors\n\nXMatrix <- matrix(rnorm(numberSubjects*numberFactors, mean=0, sd=1), numberSubjects, numberFactors)\n\ntLoadings <- c(1, seq(.5, .9, length.out=(numberIndPerFactor-1)), rep(0, numberIndPerFactor*2),\n  rep(0, numberIndPerFactor*1), 1, seq(.5, .9, length.out=(numberIndPerFactor-1)), rep(0, numberIndPerFactor*1),\n  rep(0, numberIndPerFactor*2), 1, seq(.5, .9, length.out=(numberIndPerFactor-1)))\nBMatrix <- matrix(tLoadings, numberFactors, numberIndicators, byrow=TRUE)\nUMatrix <- matrix(rnorm(numberSubjects*numberIndicators, mean=0, sd=1), numberSubjects, numberIndicators)\nYMatrix <- XMatrix %*% BMatrix + UMatrix\n\ncor(XMatrix)\n\ndimnames(YMatrix) <- list(NULL, paste(\"X\", 1:numberIndicators, sep=\"\"))\n\nYFrame <- data.frame(YMatrix)\n\nround(cor(YFrame), 3)\nround(cov(YFrame), 3)\n\nindicators <- paste(\"X\", 1:numberIndicators, sep=\"\")\ntotalVars <- numberIndicators + numberFactors\n\n# ----------------------------------\n# Build an orthogonal simple structure factor model\n\nlatents <- paste(\"F\", 1:numberFactors, sep=\"\")\n\nuniqueLabels <- paste(\"U_\", indicators, sep=\"\")\nmeanLabels <- paste(\"M_\", latents, sep=\"\")\nfactorVarLabels <- paste(\"Var_\", latents, sep=\"\")\n\nlatents1 <- latents[1]\nindicators1 <- indicators[1:numberIndPerFactor]\nloadingLabels1 <- paste(\"b_F1\", indicators[1:numberIndPerFactor], sep=\"\") \nlatents2 <- latents[2]\nindicators2 <- indicators[numberIndPerFactor+(1:numberIndPerFactor)]\nloadingLabels2 <- paste(\"b_F2\", indicators[numberIndPerFactor+(1:numberIndPerFactor)], sep=\"\") \nlatents3 <- latents[3]\nindicators3 <- indicators[(2*numberIndPerFactor)+(1:numberIndPerFactor)]\nloadingLabels3 <- paste(\"b_F3\", indicators[(2*numberIndPerFactor)+(1:numberIndPerFactor)], sep=\"\") \n\nthreeFactorOrthogonal <- mxModel(\"threeFactorOrthogonal\",\n    type=\"RAM\",\n    manifestVars=c(indicators),\n    latentVars=c(latents,\"dummy1\"),\n    mxPath(from=latents1, to=indicators1, \n           arrows=1, all=TRUE, \n           free=TRUE, values=.2, \n           labels=loadingLabels1),\n    mxPath(from=latents2, to=indicators2, \n           arrows=1, all=TRUE, \n           free=TRUE, values=.2, \n           labels=loadingLabels2),\n    mxPath(from=latents3, to=indicators3, \n           arrows=1, all=TRUE, \n           free=TRUE, values=.2, \n           labels=loadingLabels3),\n    mxPath(from=latents1, to=indicators1[1], \n           arrows=1, \n           free=FALSE, values=1),\n    mxPath(from=latents2, to=indicators2[1], \n           arrows=1, \n           free=FALSE, values=1),\n    mxPath(from=latents3, to=indicators3[1], \n           arrows=1, \n           free=FALSE, values=1),\n    mxPath(from=indicators, \n           arrows=2, \n           free=TRUE, values=.2, \n           labels=uniqueLabels),\n    mxPath(from=latents,\n           arrows=2, \n           free=TRUE, values=.8, \n           labels=factorVarLabels),\n    mxPath(from=\"one\", to=indicators, \n           arrows=1, free=FALSE, values=0),\n    mxPath(from=\"one\", to=c(latents), \n           arrows=1, free=TRUE, values=.1, \n           labels=meanLabels),\n    mxData(observed=cov(YFrame), means=mean(YFrame), \n\tnumObs=nrow(YFrame), type=\"cov\")\n    )\n\nthreeFactorOrthogonalOut <- mxRun(threeFactorOrthogonal)\nsummary(threeFactorOrthogonalOut)\n\n" }
{ "repo_name": "JuKa87/OpenMx", "ref": "refs/heads/julian", "path": "inst/models/enormous/RAM-3Factor-192Indicators-covdata-a.R", "content": "# ---------------------------------------------------------------------\n# Program: RAM-3Factor-12Indicators.R\n#  Author: Steven M. Boker\n#    Date: Fri Jul 30 13:45:12 EDT 2010\n#\n# This program is a factor model using standard RAM.\n#\n# ---------------------------------------------------------------------\n# Revision History\n#    -- Fri Jul 30 13:45:12 EDT 2010\n#      Created RAM-3Factor-12Indicators.R.\n#\n# ---------------------------------------------------------------------\n\n# ----------------------------------\n# Read libraries and set options.\n\nlibrary(OpenMx)\n\noptions(width=100)\n\n# ---------------------------------------------------------------------\n# Data for factor model.\n\nnumberSubjects <- 1000\nnumberFactors <- 3\nnumberIndPerFactor <- 64\nnumberIndicators <- numberIndPerFactor*numberFactors # must be a multiple of numberFactors\n\nXMatrix <- matrix(rnorm(numberSubjects*numberFactors, mean=0, sd=1), numberSubjects, numberFactors)\n\ntLoadings <- c(1, seq(.5, .9, length.out=(numberIndPerFactor-1)), rep(0, numberIndPerFactor*2),\n  rep(0, numberIndPerFactor*1), 1, seq(.5, .9, length.out=(numberIndPerFactor-1)), rep(0, numberIndPerFactor*1),\n  rep(0, numberIndPerFactor*2), 1, seq(.5, .9, length.out=(numberIndPerFactor-1)))\nBMatrix <- matrix(tLoadings, numberFactors, numberIndicators, byrow=TRUE)\nUMatrix <- matrix(rnorm(numberSubjects*numberIndicators, mean=0, sd=1), numberSubjects, numberIndicators)\nYMatrix <- XMatrix %*% BMatrix + UMatrix\n\ncor(XMatrix)\n\ndimnames(YMatrix) <- list(NULL, paste(\"X\", 1:numberIndicators, sep=\"\"))\n\nYFrame <- data.frame(YMatrix)\n\nround(cor(YFrame), 3)\nround(cov(YFrame), 3)\n\nindicators <- paste(\"X\", 1:numberIndicators, sep=\"\")\ntotalVars <- numberIndicators + numberFactors\n\n# ----------------------------------\n# Build an orthogonal simple structure factor model\n\nlatents <- paste(\"F\", 1:numberFactors, sep=\"\")\n\nuniqueLabels <- paste(\"U_\", indicators, sep=\"\")\nmeanLabels <- paste(\"M_\", latents, sep=\"\")\nfactorVarLabels <- paste(\"Var_\", latents, sep=\"\")\n\nlatents1 <- latents[1]\nindicators1 <- indicators[1:numberIndPerFactor]\nloadingLabels1 <- paste(\"b_F1\", indicators[1:numberIndPerFactor], sep=\"\") \nlatents2 <- latents[2]\nindicators2 <- indicators[numberIndPerFactor+(1:numberIndPerFactor)]\nloadingLabels2 <- paste(\"b_F2\", indicators[numberIndPerFactor+(1:numberIndPerFactor)], sep=\"\") \nlatents3 <- latents[3]\nindicators3 <- indicators[(2*numberIndPerFactor)+(1:numberIndPerFactor)]\nloadingLabels3 <- paste(\"b_F3\", indicators[(2*numberIndPerFactor)+(1:numberIndPerFactor)], sep=\"\") \n\nthreeFactorOrthogonal <- mxModel(\"threeFactorOrthogonal\",\n    type=\"RAM\",\n    manifestVars=c(indicators),\n    latentVars=c(latents,\"dummy1\"),\n    mxPath(from=latents1, to=indicators1, \n           arrows=1, all=TRUE, \n           free=TRUE, values=.2, \n           labels=loadingLabels1),\n    mxPath(from=latents2, to=indicators2, \n           arrows=1, all=TRUE, \n           free=TRUE, values=.2, \n           labels=loadingLabels2),\n    mxPath(from=latents3, to=indicators3, \n           arrows=1, all=TRUE, \n           free=TRUE, values=.2, \n           labels=loadingLabels3),\n    mxPath(from=latents1, to=indicators1[1], \n           arrows=1, \n           free=FALSE, values=1),\n    mxPath(from=latents2, to=indicators2[1], \n           arrows=1, \n           free=FALSE, values=1),\n    mxPath(from=latents3, to=indicators3[1], \n           arrows=1, \n           free=FALSE, values=1),\n    mxPath(from=indicators, \n           arrows=2, \n           free=TRUE, values=.2, \n           labels=uniqueLabels),\n    mxPath(from=latents,\n           arrows=2, \n           free=TRUE, values=.8, \n           labels=factorVarLabels),\n    mxPath(from=\"one\", to=indicators, \n           arrows=1, free=FALSE, values=0),\n    mxPath(from=\"one\", to=c(latents), \n           arrows=1, free=TRUE, values=.1, \n           labels=meanLabels),\n    mxData(observed=cov(YFrame), means=mean(YFrame), \n\tnumObs=nrow(YFrame), type=\"cov\")\n    )\n\nthreeFactorOrthogonalOut <- mxRun(threeFactorOrthogonal)\nsummary(threeFactorOrthogonalOut)\n\n" }
{ "repo_name": "cxxr-devel/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/allequalcharacter/tc_allequalcharacter_1.R", "content": "expected <- TRUE    \ntest(id=1, code={    \nargv <- structure(list(target = structure(c(\"A\", \"E\", \"I\", \"M\", \"Q\",     \n\"U\", \"B\", \"F\", \"J\", \"N\", \"R\", \"V\", \"C\", \"G\", \"K\", \"O\", \"S\", \"W\",     \n\"D\", \"H\", \"L\", \"P\", \"T\", \"X\"), .Dim = c(6L, 4L)), current = structure(c(\"A\",     \n\"E\", \"I\", \"M\", \"Q\", \"U\", \"B\", \"F\", \"J\", \"N\", \"R\", \"V\", \"C\", \"G\",     \n\"K\", \"O\", \"S\", \"W\", \"D\", \"H\", \"L\", \"P\", \"T\", \"X\"), .Dim = c(6L,     \n4L))), .Names = c(\"target\", \"current\"))    \ndo.call('all.equal.character', argv);    \n},  o = expected);    \n    \n" }
{ "repo_name": "krlmlr/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/allequalcharacter/tc_allequalcharacter_1.R", "content": "expected <- TRUE    \ntest(id=1, code={    \nargv <- structure(list(target = structure(c(\"A\", \"E\", \"I\", \"M\", \"Q\",     \n\"U\", \"B\", \"F\", \"J\", \"N\", \"R\", \"V\", \"C\", \"G\", \"K\", \"O\", \"S\", \"W\",     \n\"D\", \"H\", \"L\", \"P\", \"T\", \"X\"), .Dim = c(6L, 4L)), current = structure(c(\"A\",     \n\"E\", \"I\", \"M\", \"Q\", \"U\", \"B\", \"F\", \"J\", \"N\", \"R\", \"V\", \"C\", \"G\",     \n\"K\", \"O\", \"S\", \"W\", \"D\", \"H\", \"L\", \"P\", \"T\", \"X\"), .Dim = c(6L,     \n4L))), .Names = c(\"target\", \"current\"))    \ndo.call('all.equal.character', argv);    \n},  o = expected);    \n    \n" }
{ "repo_name": "kmillar/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/allequalcharacter/tc_allequalcharacter_1.R", "content": "expected <- TRUE    \ntest(id=1, code={    \nargv <- structure(list(target = structure(c(\"A\", \"E\", \"I\", \"M\", \"Q\",     \n\"U\", \"B\", \"F\", \"J\", \"N\", \"R\", \"V\", \"C\", \"G\", \"K\", \"O\", \"S\", \"W\",     \n\"D\", \"H\", \"L\", \"P\", \"T\", \"X\"), .Dim = c(6L, 4L)), current = structure(c(\"A\",     \n\"E\", \"I\", \"M\", \"Q\", \"U\", \"B\", \"F\", \"J\", \"N\", \"R\", \"V\", \"C\", \"G\",     \n\"K\", \"O\", \"S\", \"W\", \"D\", \"H\", \"L\", \"P\", \"T\", \"X\"), .Dim = c(6L,     \n4L))), .Names = c(\"target\", \"current\"))    \ndo.call('all.equal.character', argv);    \n},  o = expected);    \n    \n" }
{ "repo_name": "kmillar/rho", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/allequalcharacter/tc_allequalcharacter_1.R", "content": "expected <- TRUE    \ntest(id=1, code={    \nargv <- structure(list(target = structure(c(\"A\", \"E\", \"I\", \"M\", \"Q\",     \n\"U\", \"B\", \"F\", \"J\", \"N\", \"R\", \"V\", \"C\", \"G\", \"K\", \"O\", \"S\", \"W\",     \n\"D\", \"H\", \"L\", \"P\", \"T\", \"X\"), .Dim = c(6L, 4L)), current = structure(c(\"A\",     \n\"E\", \"I\", \"M\", \"Q\", \"U\", \"B\", \"F\", \"J\", \"N\", \"R\", \"V\", \"C\", \"G\",     \n\"K\", \"O\", \"S\", \"W\", \"D\", \"H\", \"L\", \"P\", \"T\", \"X\"), .Dim = c(6L,     \n4L))), .Names = c(\"target\", \"current\"))    \ndo.call('all.equal.character', argv);    \n},  o = expected);    \n    \n" }
{ "repo_name": "ArunChauhan/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/allequalcharacter/tc_allequalcharacter_1.R", "content": "expected <- TRUE    \ntest(id=1, code={    \nargv <- structure(list(target = structure(c(\"A\", \"E\", \"I\", \"M\", \"Q\",     \n\"U\", \"B\", \"F\", \"J\", \"N\", \"R\", \"V\", \"C\", \"G\", \"K\", \"O\", \"S\", \"W\",     \n\"D\", \"H\", \"L\", \"P\", \"T\", \"X\"), .Dim = c(6L, 4L)), current = structure(c(\"A\",     \n\"E\", \"I\", \"M\", \"Q\", \"U\", \"B\", \"F\", \"J\", \"N\", \"R\", \"V\", \"C\", \"G\",     \n\"K\", \"O\", \"S\", \"W\", \"D\", \"H\", \"L\", \"P\", \"T\", \"X\"), .Dim = c(6L,     \n4L))), .Names = c(\"target\", \"current\"))    \ndo.call('all.equal.character', argv);    \n},  o = expected);    \n    \n" }
{ "repo_name": "rho-devel/rho", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/allequalcharacter/tc_allequalcharacter_1.R", "content": "expected <- TRUE    \ntest(id=1, code={    \nargv <- structure(list(target = structure(c(\"A\", \"E\", \"I\", \"M\", \"Q\",     \n\"U\", \"B\", \"F\", \"J\", \"N\", \"R\", \"V\", \"C\", \"G\", \"K\", \"O\", \"S\", \"W\",     \n\"D\", \"H\", \"L\", \"P\", \"T\", \"X\"), .Dim = c(6L, 4L)), current = structure(c(\"A\",     \n\"E\", \"I\", \"M\", \"Q\", \"U\", \"B\", \"F\", \"J\", \"N\", \"R\", \"V\", \"C\", \"G\",     \n\"K\", \"O\", \"S\", \"W\", \"D\", \"H\", \"L\", \"P\", \"T\", \"X\"), .Dim = c(6L,     \n4L))), .Names = c(\"target\", \"current\"))    \ndo.call('all.equal.character', argv);    \n},  o = expected);    \n    \n" }
{ "repo_name": "ChristosChristofidis/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_jira/runit_pubdev_784_medium.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource('../h2o-runit.R')\n\ntest <- function(conn) {\n  data <- h2o.uploadFile(locate(\"bigdata/laptop/usecases/cup98LRN_z.csv\"))\n  dim(data)\n  split = h2o.splitFrame(data=data,ratios=.8)\n  train = h2o.assign(split[[1]],key=\"train\")\n  test = h2o.assign(split[[2]],key=\"test\")\n  dim(train)\n  dim(test)\n\n  data <- as.h2o(iris)\n  split = h2o.splitFrame(data=data,ratios=.8)\n  train = h2o.assign(split[[1]],key=\"train\")\n  test = h2o.assign(split[[2]],key=\"test\")\n  dim(train)\n  dim(test)\n\n  testEnd()\n}\n\ndoTest(\"PUBDEV-784\", test)\n" }
{ "repo_name": "mrgloom/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_jira/runit_pubdev_784_medium.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource('../h2o-runit.R')\n\ntest <- function(conn) {\n  data <- h2o.uploadFile(locate(\"bigdata/laptop/usecases/cup98LRN_z.csv\"))\n  dim(data)\n  split = h2o.splitFrame(data=data,ratios=.8)\n  train = h2o.assign(split[[1]],key=\"train\")\n  test = h2o.assign(split[[2]],key=\"test\")\n  dim(train)\n  dim(test)\n\n  data <- as.h2o(iris)\n  split = h2o.splitFrame(data=data,ratios=.8)\n  train = h2o.assign(split[[1]],key=\"train\")\n  test = h2o.assign(split[[2]],key=\"test\")\n  dim(train)\n  dim(test)\n\n  testEnd()\n}\n\ndoTest(\"PUBDEV-784\", test)\n" }
{ "repo_name": "tarasane/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_jira/runit_pubdev_784_medium.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource('../h2o-runit.R')\n\ntest <- function(conn) {\n  data <- h2o.uploadFile(locate(\"bigdata/laptop/usecases/cup98LRN_z.csv\"))\n  dim(data)\n  split = h2o.splitFrame(data=data,ratios=.8)\n  train = h2o.assign(split[[1]],key=\"train\")\n  test = h2o.assign(split[[2]],key=\"test\")\n  dim(train)\n  dim(test)\n\n  data <- as.h2o(iris)\n  split = h2o.splitFrame(data=data,ratios=.8)\n  train = h2o.assign(split[[1]],key=\"train\")\n  test = h2o.assign(split[[2]],key=\"test\")\n  dim(train)\n  dim(test)\n\n  testEnd()\n}\n\ndoTest(\"PUBDEV-784\", test)\n" }
{ "repo_name": "bospetersen/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_jira/runit_pubdev_784_medium.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource('../h2o-runit.R')\n\ntest <- function(conn) {\n  data <- h2o.uploadFile(locate(\"bigdata/laptop/usecases/cup98LRN_z.csv\"))\n  dim(data)\n  split = h2o.splitFrame(data=data,ratios=.8)\n  train = h2o.assign(split[[1]],key=\"train\")\n  test = h2o.assign(split[[2]],key=\"test\")\n  dim(train)\n  dim(test)\n\n  data <- as.h2o(iris)\n  split = h2o.splitFrame(data=data,ratios=.8)\n  train = h2o.assign(split[[1]],key=\"train\")\n  test = h2o.assign(split[[2]],key=\"test\")\n  dim(train)\n  dim(test)\n\n  testEnd()\n}\n\ndoTest(\"PUBDEV-784\", test)\n" }
{ "repo_name": "weaver-viii/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_jira/runit_pubdev_784_medium.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource('../h2o-runit.R')\n\ntest <- function(conn) {\n  data <- h2o.uploadFile(locate(\"bigdata/laptop/usecases/cup98LRN_z.csv\"))\n  dim(data)\n  split = h2o.splitFrame(data=data,ratios=.8)\n  train = h2o.assign(split[[1]],key=\"train\")\n  test = h2o.assign(split[[2]],key=\"test\")\n  dim(train)\n  dim(test)\n\n  data <- as.h2o(iris)\n  split = h2o.splitFrame(data=data,ratios=.8)\n  train = h2o.assign(split[[1]],key=\"train\")\n  test = h2o.assign(split[[2]],key=\"test\")\n  dim(train)\n  dim(test)\n\n  testEnd()\n}\n\ndoTest(\"PUBDEV-784\", test)\n" }
{ "repo_name": "PawarPawan/h2o-v3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_jira/runit_pubdev_784_medium.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource('../h2o-runit.R')\n\ntest <- function(conn) {\n  data <- h2o.uploadFile(locate(\"bigdata/laptop/usecases/cup98LRN_z.csv\"))\n  dim(data)\n  split = h2o.splitFrame(data=data,ratios=.8)\n  train = h2o.assign(split[[1]],key=\"train\")\n  test = h2o.assign(split[[2]],key=\"test\")\n  dim(train)\n  dim(test)\n\n  data <- as.h2o(iris)\n  split = h2o.splitFrame(data=data,ratios=.8)\n  train = h2o.assign(split[[1]],key=\"train\")\n  test = h2o.assign(split[[2]],key=\"test\")\n  dim(train)\n  dim(test)\n\n  testEnd()\n}\n\ndoTest(\"PUBDEV-784\", test)\n" }
{ "repo_name": "alexjeen/molgenis", "ref": "refs/heads/master", "path": "molgenis-ontology/src/main/resources/roc-curve.R", "content": "biobank.readExcel <- function(file) {\n  biobanks <- sheetNames(file);\n  aggregateData <- NULL\n  for(biobankName in biobanks){\n    biobankData <- read.xls(file, sheet=which(biobanks == biobankName))\n    group <- rep(which(biobanks==biobankName), nrow(biobankData))\n    biobankData <- cbind(biobankData, group)\n    aggregateData <- rbind(aggregateData, biobankData)\n  }\n  return(aggregateData)\n}\n\nlibrary(gdata)\nlibrary(ggplot2)\n\ninputData <- biobank.readExcel(\"${filePath}\")\n\nlegend <- c(\"1\"=\"blue\", \"2\"=\"red\", \"3\"=\"black\", \"4\"=\"purple1\", \"5\"=\"green4\", \"6\"=\"darkblue\")\nlinestyle <- c(\"1\"=\"solid\",\"2\"=\"longdash\", \"3\"=\"dotted\",\"4\"=\"F1\", \"5\"=\"twodash\", \"6\"=\"1F\")\nlabel <- c(\"roc\")\n\npng(\"${outputFile}\", width = 480, height = 480)\nggplot() + ylab(\"Sensitivity\") + xlab(\"Specificity\") + ggtitle(\"Sensitivity-Specificity\") + \n  theme(plot.title = element_text(lineheight=.8, face=\"bold\")) + xlim(0, 1) + ylim(0, 1) +\n  geom_path(data=inputData, aes(y = TPR, x=FPR, group=group, linetype=as.factor(group), colour=as.factor(group)), size = 0.5) + \n  geom_point(data=inputData, aes(y = TPR, x=FPR, colour=as.factor(group), shape=as.factor(group))) + \n  scale_colour_manual(\"legend\", values=legend, labels=label) + \n  scale_linetype_manual(\"legend\", values=linestyle, labels=label) +\n  scale_shape_manual(\"legend\", values=c(1,1,2,1,1,2), labels=label) + \n  coord_fixed(ratio=1)" }
{ "repo_name": "erwinwinder/molgenis", "ref": "refs/heads/master", "path": "molgenis-ontology/src/main/resources/roc-curve.R", "content": "biobank.readExcel <- function(file) {\n  biobanks <- sheetNames(file);\n  aggregateData <- NULL\n  for(biobankName in biobanks){\n    biobankData <- read.xls(file, sheet=which(biobanks == biobankName))\n    group <- rep(which(biobanks==biobankName), nrow(biobankData))\n    biobankData <- cbind(biobankData, group)\n    aggregateData <- rbind(aggregateData, biobankData)\n  }\n  return(aggregateData)\n}\n\nlibrary(gdata)\nlibrary(ggplot2)\n\ninputData <- biobank.readExcel(\"${filePath}\")\n\nlegend <- c(\"1\"=\"blue\", \"2\"=\"red\", \"3\"=\"black\", \"4\"=\"purple1\", \"5\"=\"green4\", \"6\"=\"darkblue\")\nlinestyle <- c(\"1\"=\"solid\",\"2\"=\"longdash\", \"3\"=\"dotted\",\"4\"=\"F1\", \"5\"=\"twodash\", \"6\"=\"1F\")\nlabel <- c(\"roc\")\n\npng(\"${outputFile}\", width = 480, height = 480)\nggplot() + ylab(\"Sensitivity\") + xlab(\"Specificity\") + ggtitle(\"Sensitivity-Specificity\") + \n  theme(plot.title = element_text(lineheight=.8, face=\"bold\")) + xlim(0, 1) + ylim(0, 1) +\n  geom_path(data=inputData, aes(y = TPR, x=FPR, group=group, linetype=as.factor(group), colour=as.factor(group)), size = 0.5) + \n  geom_point(data=inputData, aes(y = TPR, x=FPR, colour=as.factor(group), shape=as.factor(group))) + \n  scale_colour_manual(\"legend\", values=legend, labels=label) + \n  scale_linetype_manual(\"legend\", values=linestyle, labels=label) +\n  scale_shape_manual(\"legend\", values=c(1,1,2,1,1,2), labels=label) + \n  coord_fixed(ratio=1)" }
{ "repo_name": "adini121/molgenis", "ref": "refs/heads/master", "path": "molgenis-ontology/src/main/resources/roc-curve.R", "content": "biobank.readExcel <- function(file) {\n  biobanks <- sheetNames(file);\n  aggregateData <- NULL\n  for(biobankName in biobanks){\n    biobankData <- read.xls(file, sheet=which(biobanks == biobankName))\n    group <- rep(which(biobanks==biobankName), nrow(biobankData))\n    biobankData <- cbind(biobankData, group)\n    aggregateData <- rbind(aggregateData, biobankData)\n  }\n  return(aggregateData)\n}\n\nlibrary(gdata)\nlibrary(ggplot2)\n\ninputData <- biobank.readExcel(\"${filePath}\")\n\nlegend <- c(\"1\"=\"blue\", \"2\"=\"red\", \"3\"=\"black\", \"4\"=\"purple1\", \"5\"=\"green4\", \"6\"=\"darkblue\")\nlinestyle <- c(\"1\"=\"solid\",\"2\"=\"longdash\", \"3\"=\"dotted\",\"4\"=\"F1\", \"5\"=\"twodash\", \"6\"=\"1F\")\nlabel <- c(\"roc\")\n\npng(\"${outputFile}\", width = 480, height = 480)\nggplot() + ylab(\"Sensitivity\") + xlab(\"Specificity\") + ggtitle(\"Sensitivity-Specificity\") + \n  theme(plot.title = element_text(lineheight=.8, face=\"bold\")) + xlim(0, 1) + ylim(0, 1) +\n  geom_path(data=inputData, aes(y = TPR, x=FPR, group=group, linetype=as.factor(group), colour=as.factor(group)), size = 0.5) + \n  geom_point(data=inputData, aes(y = TPR, x=FPR, colour=as.factor(group), shape=as.factor(group))) + \n  scale_colour_manual(\"legend\", values=legend, labels=label) + \n  scale_linetype_manual(\"legend\", values=linestyle, labels=label) +\n  scale_shape_manual(\"legend\", values=c(1,1,2,1,1,2), labels=label) + \n  coord_fixed(ratio=1)" }
{ "repo_name": "marieke-bijlsma/molgenis", "ref": "refs/heads/master", "path": "molgenis-ontology/src/main/resources/roc-curve.R", "content": "biobank.readExcel <- function(file) {\n  biobanks <- sheetNames(file);\n  aggregateData <- NULL\n  for(biobankName in biobanks){\n    biobankData <- read.xls(file, sheet=which(biobanks == biobankName))\n    group <- rep(which(biobanks==biobankName), nrow(biobankData))\n    biobankData <- cbind(biobankData, group)\n    aggregateData <- rbind(aggregateData, biobankData)\n  }\n  return(aggregateData)\n}\n\nlibrary(gdata)\nlibrary(ggplot2)\n\ninputData <- biobank.readExcel(\"${filePath}\")\n\nlegend <- c(\"1\"=\"blue\", \"2\"=\"red\", \"3\"=\"black\", \"4\"=\"purple1\", \"5\"=\"green4\", \"6\"=\"darkblue\")\nlinestyle <- c(\"1\"=\"solid\",\"2\"=\"longdash\", \"3\"=\"dotted\",\"4\"=\"F1\", \"5\"=\"twodash\", \"6\"=\"1F\")\nlabel <- c(\"roc\")\n\npng(\"${outputFile}\", width = 480, height = 480)\nggplot() + ylab(\"Sensitivity\") + xlab(\"Specificity\") + ggtitle(\"Sensitivity-Specificity\") + \n  theme(plot.title = element_text(lineheight=.8, face=\"bold\")) + xlim(0, 1) + ylim(0, 1) +\n  geom_path(data=inputData, aes(y = TPR, x=FPR, group=group, linetype=as.factor(group), colour=as.factor(group)), size = 0.5) + \n  geom_point(data=inputData, aes(y = TPR, x=FPR, colour=as.factor(group), shape=as.factor(group))) + \n  scale_colour_manual(\"legend\", values=legend, labels=label) + \n  scale_linetype_manual(\"legend\", values=linestyle, labels=label) +\n  scale_shape_manual(\"legend\", values=c(1,1,2,1,1,2), labels=label) + \n  coord_fixed(ratio=1)" }
{ "repo_name": "DionKoolhaas/molgenis", "ref": "refs/heads/master", "path": "molgenis-ontology/src/main/resources/roc-curve.R", "content": "biobank.readExcel <- function(file) {\n  biobanks <- sheetNames(file);\n  aggregateData <- NULL\n  for(biobankName in biobanks){\n    biobankData <- read.xls(file, sheet=which(biobanks == biobankName))\n    group <- rep(which(biobanks==biobankName), nrow(biobankData))\n    biobankData <- cbind(biobankData, group)\n    aggregateData <- rbind(aggregateData, biobankData)\n  }\n  return(aggregateData)\n}\n\nlibrary(gdata)\nlibrary(ggplot2)\n\ninputData <- biobank.readExcel(\"${filePath}\")\n\nlegend <- c(\"1\"=\"blue\", \"2\"=\"red\", \"3\"=\"black\", \"4\"=\"purple1\", \"5\"=\"green4\", \"6\"=\"darkblue\")\nlinestyle <- c(\"1\"=\"solid\",\"2\"=\"longdash\", \"3\"=\"dotted\",\"4\"=\"F1\", \"5\"=\"twodash\", \"6\"=\"1F\")\nlabel <- c(\"roc\")\n\npng(\"${outputFile}\", width = 480, height = 480)\nggplot() + ylab(\"Sensitivity\") + xlab(\"Specificity\") + ggtitle(\"Sensitivity-Specificity\") + \n  theme(plot.title = element_text(lineheight=.8, face=\"bold\")) + xlim(0, 1) + ylim(0, 1) +\n  geom_path(data=inputData, aes(y = TPR, x=FPR, group=group, linetype=as.factor(group), colour=as.factor(group)), size = 0.5) + \n  geom_point(data=inputData, aes(y = TPR, x=FPR, colour=as.factor(group), shape=as.factor(group))) + \n  scale_colour_manual(\"legend\", values=legend, labels=label) + \n  scale_linetype_manual(\"legend\", values=linestyle, labels=label) +\n  scale_shape_manual(\"legend\", values=c(1,1,2,1,1,2), labels=label) + \n  coord_fixed(ratio=1)" }
{ "repo_name": "marijevdgeest/molgenis", "ref": "refs/heads/master", "path": "molgenis-ontology/src/main/resources/roc-curve.R", "content": "biobank.readExcel <- function(file) {\n  biobanks <- sheetNames(file);\n  aggregateData <- NULL\n  for(biobankName in biobanks){\n    biobankData <- read.xls(file, sheet=which(biobanks == biobankName))\n    group <- rep(which(biobanks==biobankName), nrow(biobankData))\n    biobankData <- cbind(biobankData, group)\n    aggregateData <- rbind(aggregateData, biobankData)\n  }\n  return(aggregateData)\n}\n\nlibrary(gdata)\nlibrary(ggplot2)\n\ninputData <- biobank.readExcel(\"${filePath}\")\n\nlegend <- c(\"1\"=\"blue\", \"2\"=\"red\", \"3\"=\"black\", \"4\"=\"purple1\", \"5\"=\"green4\", \"6\"=\"darkblue\")\nlinestyle <- c(\"1\"=\"solid\",\"2\"=\"longdash\", \"3\"=\"dotted\",\"4\"=\"F1\", \"5\"=\"twodash\", \"6\"=\"1F\")\nlabel <- c(\"roc\")\n\npng(\"${outputFile}\", width = 480, height = 480)\nggplot() + ylab(\"Sensitivity\") + xlab(\"Specificity\") + ggtitle(\"Sensitivity-Specificity\") + \n  theme(plot.title = element_text(lineheight=.8, face=\"bold\")) + xlim(0, 1) + ylim(0, 1) +\n  geom_path(data=inputData, aes(y = TPR, x=FPR, group=group, linetype=as.factor(group), colour=as.factor(group)), size = 0.5) + \n  geom_point(data=inputData, aes(y = TPR, x=FPR, colour=as.factor(group), shape=as.factor(group))) + \n  scale_colour_manual(\"legend\", values=legend, labels=label) + \n  scale_linetype_manual(\"legend\", values=linestyle, labels=label) +\n  scale_shape_manual(\"legend\", values=c(1,1,2,1,1,2), labels=label) + \n  coord_fixed(ratio=1)" }
{ "repo_name": "cxxr-devel/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/identical/tc_identical_26.R", "content": "expected <- eval(parse(text=\"TRUE\"));             \ntest(id=0, code={             \nargv <- eval(parse(text=\"list(3.04888344611714e+29, 3.04888344611714e+29, TRUE, TRUE, TRUE, TRUE, FALSE)\"));             \n.Internal(identical(argv[[1]], argv[[2]], argv[[3]], argv[[4]], argv[[5]], argv[[6]], argv[[7]]));             \n}, o=expected);             \n\n" }
{ "repo_name": "krlmlr/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/identical/tc_identical_26.R", "content": "expected <- eval(parse(text=\"TRUE\"));             \ntest(id=0, code={             \nargv <- eval(parse(text=\"list(3.04888344611714e+29, 3.04888344611714e+29, TRUE, TRUE, TRUE, TRUE, FALSE)\"));             \n.Internal(identical(argv[[1]], argv[[2]], argv[[3]], argv[[4]], argv[[5]], argv[[6]], argv[[7]]));             \n}, o=expected);             \n\n" }
{ "repo_name": "kmillar/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/identical/tc_identical_26.R", "content": "expected <- eval(parse(text=\"TRUE\"));             \ntest(id=0, code={             \nargv <- eval(parse(text=\"list(3.04888344611714e+29, 3.04888344611714e+29, TRUE, TRUE, TRUE, TRUE, FALSE)\"));             \n.Internal(identical(argv[[1]], argv[[2]], argv[[3]], argv[[4]], argv[[5]], argv[[6]], argv[[7]]));             \n}, o=expected);             \n\n" }
{ "repo_name": "kmillar/rho", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/identical/tc_identical_26.R", "content": "expected <- eval(parse(text=\"TRUE\"));             \ntest(id=0, code={             \nargv <- eval(parse(text=\"list(3.04888344611714e+29, 3.04888344611714e+29, TRUE, TRUE, TRUE, TRUE, FALSE)\"));             \n.Internal(identical(argv[[1]], argv[[2]], argv[[3]], argv[[4]], argv[[5]], argv[[6]], argv[[7]]));             \n}, o=expected);             \n\n" }
{ "repo_name": "ArunChauhan/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/identical/tc_identical_26.R", "content": "expected <- eval(parse(text=\"TRUE\"));             \ntest(id=0, code={             \nargv <- eval(parse(text=\"list(3.04888344611714e+29, 3.04888344611714e+29, TRUE, TRUE, TRUE, TRUE, FALSE)\"));             \n.Internal(identical(argv[[1]], argv[[2]], argv[[3]], argv[[4]], argv[[5]], argv[[6]], argv[[7]]));             \n}, o=expected);             \n\n" }
{ "repo_name": "rho-devel/rho", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/identical/tc_identical_26.R", "content": "expected <- eval(parse(text=\"TRUE\"));             \ntest(id=0, code={             \nargv <- eval(parse(text=\"list(3.04888344611714e+29, 3.04888344611714e+29, TRUE, TRUE, TRUE, TRUE, FALSE)\"));             \n.Internal(identical(argv[[1]], argv[[2]], argv[[3]], argv[[4]], argv[[5]], argv[[6]], argv[[7]]));             \n}, o=expected);             \n\n" }
{ "repo_name": "h2oai/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_misc/runit_assign.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../scripts/h2o-r-test-setup.R\")\n##\n# Test assigned NAs\n##\n\ntest <- function() {\n\tiris <- as.h2o(iris)\n\tnumNAs <- 40\n  s <- sample(nrow(iris),numNAs)\n  iris[s,5] <- NA\n  print(summary(iris))\n  expect_that(sum(is.na(iris[5])), equals(numNAs))\n  \n}\n\ndoTest(\"Count assigned NAs\", test)\n\n" }
{ "repo_name": "michalkurka/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_misc/runit_assign.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../scripts/h2o-r-test-setup.R\")\n##\n# Test assigned NAs\n##\n\ntest <- function() {\n\tiris <- as.h2o(iris)\n\tnumNAs <- 40\n  s <- sample(nrow(iris),numNAs)\n  iris[s,5] <- NA\n  print(summary(iris))\n  expect_that(sum(is.na(iris[5])), equals(numNAs))\n  \n}\n\ndoTest(\"Count assigned NAs\", test)\n\n" }
{ "repo_name": "mathemage/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_misc/runit_assign.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../scripts/h2o-r-test-setup.R\")\n##\n# Test assigned NAs\n##\n\ntest <- function() {\n\tiris <- as.h2o(iris)\n\tnumNAs <- 40\n  s <- sample(nrow(iris),numNAs)\n  iris[s,5] <- NA\n  print(summary(iris))\n  expect_that(sum(is.na(iris[5])), equals(numNAs))\n  \n}\n\ndoTest(\"Count assigned NAs\", test)\n\n" }
{ "repo_name": "spennihana/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_misc/runit_assign.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../scripts/h2o-r-test-setup.R\")\n##\n# Test assigned NAs\n##\n\ntest <- function() {\n\tiris <- as.h2o(iris)\n\tnumNAs <- 40\n  s <- sample(nrow(iris),numNAs)\n  iris[s,5] <- NA\n  print(summary(iris))\n  expect_that(sum(is.na(iris[5])), equals(numNAs))\n  \n}\n\ndoTest(\"Count assigned NAs\", test)\n\n" }
{ "repo_name": "jangorecki/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_misc/runit_assign.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../scripts/h2o-r-test-setup.R\")\n##\n# Test assigned NAs\n##\n\ntest <- function() {\n\tiris <- as.h2o(iris)\n\tnumNAs <- 40\n  s <- sample(nrow(iris),numNAs)\n  iris[s,5] <- NA\n  print(summary(iris))\n  expect_that(sum(is.na(iris[5])), equals(numNAs))\n  \n}\n\ndoTest(\"Count assigned NAs\", test)\n\n" }
{ "repo_name": "h2oai/h2o-dev", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_misc/runit_assign.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../scripts/h2o-r-test-setup.R\")\n##\n# Test assigned NAs\n##\n\ntest <- function() {\n\tiris <- as.h2o(iris)\n\tnumNAs <- 40\n  s <- sample(nrow(iris),numNAs)\n  iris[s,5] <- NA\n  print(summary(iris))\n  expect_that(sum(is.na(iris[5])), equals(numNAs))\n  \n}\n\ndoTest(\"Count assigned NAs\", test)\n\n" }
{ "repo_name": "nilbody/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_misc/runit_assign.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../scripts/h2o-r-test-setup.R\")\n##\n# Test assigned NAs\n##\n\ntest <- function() {\n\tiris <- as.h2o(iris)\n\tnumNAs <- 40\n  s <- sample(nrow(iris),numNAs)\n  iris[s,5] <- NA\n  print(summary(iris))\n  expect_that(sum(is.na(iris[5])), equals(numNAs))\n  \n}\n\ndoTest(\"Count assigned NAs\", test)\n\n" }
{ "repo_name": "YzPaul3/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_misc/runit_assign.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../scripts/h2o-r-test-setup.R\")\n##\n# Test assigned NAs\n##\n\ntest <- function() {\n\tiris <- as.h2o(iris)\n\tnumNAs <- 40\n  s <- sample(nrow(iris),numNAs)\n  iris[s,5] <- NA\n  print(summary(iris))\n  expect_that(sum(is.na(iris[5])), equals(numNAs))\n  \n}\n\ndoTest(\"Count assigned NAs\", test)\n\n" }
{ "repo_name": "h2oai/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_algos/gbm/runit_GBM_mnist_manyCols_large.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../../scripts/h2o-r-test-setup.R\")\n\n\n\ntest.mnist.manyCols <- function() {\n   fPath = tryCatch({\n      locate(\"bigdata/laptop/mnist/train.csv.gz\")\n    }, warning= function(w) {\n      print(\"File bigdata/laptop/mnist/train.csv.gz could not be found.  Please run ./gradlew syncBigdataLaptop (or gradlew.bat syncBigdataLaptop for Windows) to retrieve the file.\")\n    }, error= function(e) {\n      print(\"File bigdata/laptop/mnist/train.csv.gz could not be found.  Please run ./gradlew syncBigdataLaptop (or gradlew.bat syncBigdataLaptop for Windows) to retrieve the file.\")\n    }, finally = {\n      \n    })\n\n  Log.info(\"Importing mnist train data...\\n\")\n  train.hex <- h2o.uploadFile(fPath, \"train.hex\")\n  train.hex[,785] <- as.factor(train.hex[,785])\n  Log.info(\"Check that tail works...\")\n  tail(train.hex)\n  tail_ <- tail(train.hex)\n  Log.info(\"Doing gbm on mnist training data.... \\n\")\n  gbm.mnist <- h2o.gbm(x= 1:784, y = 785, training_frame = train.hex, ntrees = 1, max_depth = 1, min_rows = 10, learn_rate = 0.01, distribution = \"multinomial\")\n  print(gbm.mnist)\n\n  \n}\n\ndoTest(\"Many Columns Test: MNIST\", test.mnist.manyCols)\n\n" }
{ "repo_name": "michalkurka/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_algos/gbm/runit_GBM_mnist_manyCols_large.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../../scripts/h2o-r-test-setup.R\")\n\n\n\ntest.mnist.manyCols <- function() {\n   fPath = tryCatch({\n      locate(\"bigdata/laptop/mnist/train.csv.gz\")\n    }, warning= function(w) {\n      print(\"File bigdata/laptop/mnist/train.csv.gz could not be found.  Please run ./gradlew syncBigdataLaptop (or gradlew.bat syncBigdataLaptop for Windows) to retrieve the file.\")\n    }, error= function(e) {\n      print(\"File bigdata/laptop/mnist/train.csv.gz could not be found.  Please run ./gradlew syncBigdataLaptop (or gradlew.bat syncBigdataLaptop for Windows) to retrieve the file.\")\n    }, finally = {\n      \n    })\n\n  Log.info(\"Importing mnist train data...\\n\")\n  train.hex <- h2o.uploadFile(fPath, \"train.hex\")\n  train.hex[,785] <- as.factor(train.hex[,785])\n  Log.info(\"Check that tail works...\")\n  tail(train.hex)\n  tail_ <- tail(train.hex)\n  Log.info(\"Doing gbm on mnist training data.... \\n\")\n  gbm.mnist <- h2o.gbm(x= 1:784, y = 785, training_frame = train.hex, ntrees = 1, max_depth = 1, min_rows = 10, learn_rate = 0.01, distribution = \"multinomial\")\n  print(gbm.mnist)\n\n  \n}\n\ndoTest(\"Many Columns Test: MNIST\", test.mnist.manyCols)\n\n" }
{ "repo_name": "mathemage/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_algos/gbm/runit_GBM_mnist_manyCols_large.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../../scripts/h2o-r-test-setup.R\")\n\n\n\ntest.mnist.manyCols <- function() {\n   fPath = tryCatch({\n      locate(\"bigdata/laptop/mnist/train.csv.gz\")\n    }, warning= function(w) {\n      print(\"File bigdata/laptop/mnist/train.csv.gz could not be found.  Please run ./gradlew syncBigdataLaptop (or gradlew.bat syncBigdataLaptop for Windows) to retrieve the file.\")\n    }, error= function(e) {\n      print(\"File bigdata/laptop/mnist/train.csv.gz could not be found.  Please run ./gradlew syncBigdataLaptop (or gradlew.bat syncBigdataLaptop for Windows) to retrieve the file.\")\n    }, finally = {\n      \n    })\n\n  Log.info(\"Importing mnist train data...\\n\")\n  train.hex <- h2o.uploadFile(fPath, \"train.hex\")\n  train.hex[,785] <- as.factor(train.hex[,785])\n  Log.info(\"Check that tail works...\")\n  tail(train.hex)\n  tail_ <- tail(train.hex)\n  Log.info(\"Doing gbm on mnist training data.... \\n\")\n  gbm.mnist <- h2o.gbm(x= 1:784, y = 785, training_frame = train.hex, ntrees = 1, max_depth = 1, min_rows = 10, learn_rate = 0.01, distribution = \"multinomial\")\n  print(gbm.mnist)\n\n  \n}\n\ndoTest(\"Many Columns Test: MNIST\", test.mnist.manyCols)\n\n" }
{ "repo_name": "spennihana/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_algos/gbm/runit_GBM_mnist_manyCols_large.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../../scripts/h2o-r-test-setup.R\")\n\n\n\ntest.mnist.manyCols <- function() {\n   fPath = tryCatch({\n      locate(\"bigdata/laptop/mnist/train.csv.gz\")\n    }, warning= function(w) {\n      print(\"File bigdata/laptop/mnist/train.csv.gz could not be found.  Please run ./gradlew syncBigdataLaptop (or gradlew.bat syncBigdataLaptop for Windows) to retrieve the file.\")\n    }, error= function(e) {\n      print(\"File bigdata/laptop/mnist/train.csv.gz could not be found.  Please run ./gradlew syncBigdataLaptop (or gradlew.bat syncBigdataLaptop for Windows) to retrieve the file.\")\n    }, finally = {\n      \n    })\n\n  Log.info(\"Importing mnist train data...\\n\")\n  train.hex <- h2o.uploadFile(fPath, \"train.hex\")\n  train.hex[,785] <- as.factor(train.hex[,785])\n  Log.info(\"Check that tail works...\")\n  tail(train.hex)\n  tail_ <- tail(train.hex)\n  Log.info(\"Doing gbm on mnist training data.... \\n\")\n  gbm.mnist <- h2o.gbm(x= 1:784, y = 785, training_frame = train.hex, ntrees = 1, max_depth = 1, min_rows = 10, learn_rate = 0.01, distribution = \"multinomial\")\n  print(gbm.mnist)\n\n  \n}\n\ndoTest(\"Many Columns Test: MNIST\", test.mnist.manyCols)\n\n" }
{ "repo_name": "jangorecki/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_algos/gbm/runit_GBM_mnist_manyCols_large.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../../scripts/h2o-r-test-setup.R\")\n\n\n\ntest.mnist.manyCols <- function() {\n   fPath = tryCatch({\n      locate(\"bigdata/laptop/mnist/train.csv.gz\")\n    }, warning= function(w) {\n      print(\"File bigdata/laptop/mnist/train.csv.gz could not be found.  Please run ./gradlew syncBigdataLaptop (or gradlew.bat syncBigdataLaptop for Windows) to retrieve the file.\")\n    }, error= function(e) {\n      print(\"File bigdata/laptop/mnist/train.csv.gz could not be found.  Please run ./gradlew syncBigdataLaptop (or gradlew.bat syncBigdataLaptop for Windows) to retrieve the file.\")\n    }, finally = {\n      \n    })\n\n  Log.info(\"Importing mnist train data...\\n\")\n  train.hex <- h2o.uploadFile(fPath, \"train.hex\")\n  train.hex[,785] <- as.factor(train.hex[,785])\n  Log.info(\"Check that tail works...\")\n  tail(train.hex)\n  tail_ <- tail(train.hex)\n  Log.info(\"Doing gbm on mnist training data.... \\n\")\n  gbm.mnist <- h2o.gbm(x= 1:784, y = 785, training_frame = train.hex, ntrees = 1, max_depth = 1, min_rows = 10, learn_rate = 0.01, distribution = \"multinomial\")\n  print(gbm.mnist)\n\n  \n}\n\ndoTest(\"Many Columns Test: MNIST\", test.mnist.manyCols)\n\n" }
{ "repo_name": "h2oai/h2o-dev", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_algos/gbm/runit_GBM_mnist_manyCols_large.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../../scripts/h2o-r-test-setup.R\")\n\n\n\ntest.mnist.manyCols <- function() {\n   fPath = tryCatch({\n      locate(\"bigdata/laptop/mnist/train.csv.gz\")\n    }, warning= function(w) {\n      print(\"File bigdata/laptop/mnist/train.csv.gz could not be found.  Please run ./gradlew syncBigdataLaptop (or gradlew.bat syncBigdataLaptop for Windows) to retrieve the file.\")\n    }, error= function(e) {\n      print(\"File bigdata/laptop/mnist/train.csv.gz could not be found.  Please run ./gradlew syncBigdataLaptop (or gradlew.bat syncBigdataLaptop for Windows) to retrieve the file.\")\n    }, finally = {\n      \n    })\n\n  Log.info(\"Importing mnist train data...\\n\")\n  train.hex <- h2o.uploadFile(fPath, \"train.hex\")\n  train.hex[,785] <- as.factor(train.hex[,785])\n  Log.info(\"Check that tail works...\")\n  tail(train.hex)\n  tail_ <- tail(train.hex)\n  Log.info(\"Doing gbm on mnist training data.... \\n\")\n  gbm.mnist <- h2o.gbm(x= 1:784, y = 785, training_frame = train.hex, ntrees = 1, max_depth = 1, min_rows = 10, learn_rate = 0.01, distribution = \"multinomial\")\n  print(gbm.mnist)\n\n  \n}\n\ndoTest(\"Many Columns Test: MNIST\", test.mnist.manyCols)\n\n" }
{ "repo_name": "nilbody/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_algos/gbm/runit_GBM_mnist_manyCols_large.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../../scripts/h2o-r-test-setup.R\")\n\n\n\ntest.mnist.manyCols <- function() {\n   fPath = tryCatch({\n      locate(\"bigdata/laptop/mnist/train.csv.gz\")\n    }, warning= function(w) {\n      print(\"File bigdata/laptop/mnist/train.csv.gz could not be found.  Please run ./gradlew syncBigdataLaptop (or gradlew.bat syncBigdataLaptop for Windows) to retrieve the file.\")\n    }, error= function(e) {\n      print(\"File bigdata/laptop/mnist/train.csv.gz could not be found.  Please run ./gradlew syncBigdataLaptop (or gradlew.bat syncBigdataLaptop for Windows) to retrieve the file.\")\n    }, finally = {\n      \n    })\n\n  Log.info(\"Importing mnist train data...\\n\")\n  train.hex <- h2o.uploadFile(fPath, \"train.hex\")\n  train.hex[,785] <- as.factor(train.hex[,785])\n  Log.info(\"Check that tail works...\")\n  tail(train.hex)\n  tail_ <- tail(train.hex)\n  Log.info(\"Doing gbm on mnist training data.... \\n\")\n  gbm.mnist <- h2o.gbm(x= 1:784, y = 785, training_frame = train.hex, ntrees = 1, max_depth = 1, min_rows = 10, learn_rate = 0.01, distribution = \"multinomial\")\n  print(gbm.mnist)\n\n  \n}\n\ndoTest(\"Many Columns Test: MNIST\", test.mnist.manyCols)\n\n" }
{ "repo_name": "YzPaul3/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_algos/gbm/runit_GBM_mnist_manyCols_large.R", "content": "setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource(\"../../../scripts/h2o-r-test-setup.R\")\n\n\n\ntest.mnist.manyCols <- function() {\n   fPath = tryCatch({\n      locate(\"bigdata/laptop/mnist/train.csv.gz\")\n    }, warning= function(w) {\n      print(\"File bigdata/laptop/mnist/train.csv.gz could not be found.  Please run ./gradlew syncBigdataLaptop (or gradlew.bat syncBigdataLaptop for Windows) to retrieve the file.\")\n    }, error= function(e) {\n      print(\"File bigdata/laptop/mnist/train.csv.gz could not be found.  Please run ./gradlew syncBigdataLaptop (or gradlew.bat syncBigdataLaptop for Windows) to retrieve the file.\")\n    }, finally = {\n      \n    })\n\n  Log.info(\"Importing mnist train data...\\n\")\n  train.hex <- h2o.uploadFile(fPath, \"train.hex\")\n  train.hex[,785] <- as.factor(train.hex[,785])\n  Log.info(\"Check that tail works...\")\n  tail(train.hex)\n  tail_ <- tail(train.hex)\n  Log.info(\"Doing gbm on mnist training data.... \\n\")\n  gbm.mnist <- h2o.gbm(x= 1:784, y = 785, training_frame = train.hex, ntrees = 1, max_depth = 1, min_rows = 10, learn_rate = 0.01, distribution = \"multinomial\")\n  print(gbm.mnist)\n\n  \n}\n\ndoTest(\"Many Columns Test: MNIST\", test.mnist.manyCols)\n\n" }
{ "repo_name": "metamx/spark", "ref": "refs/heads/v2.1.0-mmx", "path": "examples/src/main/r/ml/gbt.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# To run this example use\n# ./bin/spark-submit examples/src/main/r/ml/gbt.R\n\n# Load SparkR library into your R session\nlibrary(SparkR)\n\n# Initialize SparkSession\nsparkR.session(appName = \"SparkR-ML-gbt-example\")\n\n# GBT classification model\n\n# $example on:classification$\n# Load training data\ndf <- read.df(\"data/mllib/sample_libsvm_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT classification model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"classification\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:classification$\n\n# GBT regression model\n\n# $example on:regression$\n# Load training data\ndf <- read.df(\"data/mllib/sample_linear_regression_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT regression model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"regression\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:regression$\n" }
{ "repo_name": "javalovelinux/SparkGroovyScript", "ref": "refs/heads/master", "path": "examples/src/main/r/ml/gbt.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# To run this example use\n# ./bin/spark-submit examples/src/main/r/ml/gbt.R\n\n# Load SparkR library into your R session\nlibrary(SparkR)\n\n# Initialize SparkSession\nsparkR.session(appName = \"SparkR-ML-gbt-example\")\n\n# GBT classification model\n\n# $example on:classification$\n# Load training data\ndf <- read.df(\"data/mllib/sample_libsvm_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT classification model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"classification\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:classification$\n\n# GBT regression model\n\n# $example on:regression$\n# Load training data\ndf <- read.df(\"data/mllib/sample_linear_regression_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT regression model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"regression\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:regression$\n" }
{ "repo_name": "javalovelinux/SparkGroovyScript", "ref": "refs/heads/master", "path": "dist/examples/src/main/r/ml/gbt.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# To run this example use\n# ./bin/spark-submit examples/src/main/r/ml/gbt.R\n\n# Load SparkR library into your R session\nlibrary(SparkR)\n\n# Initialize SparkSession\nsparkR.session(appName = \"SparkR-ML-gbt-example\")\n\n# GBT classification model\n\n# $example on:classification$\n# Load training data\ndf <- read.df(\"data/mllib/sample_libsvm_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT classification model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"classification\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:classification$\n\n# GBT regression model\n\n# $example on:regression$\n# Load training data\ndf <- read.df(\"data/mllib/sample_linear_regression_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT regression model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"regression\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:regression$\n" }
{ "repo_name": "u2009cf/spark-radar", "ref": "refs/heads/master", "path": "examples/src/main/r/ml/gbt.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# To run this example use\n# ./bin/spark-submit examples/src/main/r/ml/gbt.R\n\n# Load SparkR library into your R session\nlibrary(SparkR)\n\n# Initialize SparkSession\nsparkR.session(appName = \"SparkR-ML-gbt-example\")\n\n# GBT classification model\n\n# $example on:classification$\n# Load training data\ndf <- read.df(\"data/mllib/sample_libsvm_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT classification model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"classification\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:classification$\n\n# GBT regression model\n\n# $example on:regression$\n# Load training data\ndf <- read.df(\"data/mllib/sample_linear_regression_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT regression model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"regression\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:regression$\n" }
{ "repo_name": "ChaoSBYNN/Tools", "ref": "refs/heads/master", "path": "Data Mining/Spark Examples/src/main/r/ml/gbt.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# To run this example use\n# ./bin/spark-submit examples/src/main/r/ml/gbt.R\n\n# Load SparkR library into your R session\nlibrary(SparkR)\n\n# Initialize SparkSession\nsparkR.session(appName = \"SparkR-ML-gbt-example\")\n\n# GBT classification model\n\n# $example on:classification$\n# Load training data\ndf <- read.df(\"data/mllib/sample_libsvm_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT classification model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"classification\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:classification$\n\n# GBT regression model\n\n# $example on:regression$\n# Load training data\ndf <- read.df(\"data/mllib/sample_linear_regression_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT regression model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"regression\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:regression$\n" }
{ "repo_name": "myfleetingtime/spark2.11_bingo", "ref": "refs/heads/master", "path": "examples/src/main/r/ml/gbt.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# To run this example use\n# ./bin/spark-submit examples/src/main/r/ml/gbt.R\n\n# Load SparkR library into your R session\nlibrary(SparkR)\n\n# Initialize SparkSession\nsparkR.session(appName = \"SparkR-ML-gbt-example\")\n\n# GBT classification model\n\n# $example on:classification$\n# Load training data\ndf <- read.df(\"data/mllib/sample_libsvm_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT classification model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"classification\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:classification$\n\n# GBT regression model\n\n# $example on:regression$\n# Load training data\ndf <- read.df(\"data/mllib/sample_linear_regression_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT regression model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"regression\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:regression$\n" }
{ "repo_name": "ibm-research-ireland/sparkoscope", "ref": "refs/heads/master", "path": "examples/src/main/r/ml/gbt.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# To run this example use\n# ./bin/spark-submit examples/src/main/r/ml/gbt.R\n\n# Load SparkR library into your R session\nlibrary(SparkR)\n\n# Initialize SparkSession\nsparkR.session(appName = \"SparkR-ML-gbt-example\")\n\n# GBT classification model\n\n# $example on:classification$\n# Load training data\ndf <- read.df(\"data/mllib/sample_libsvm_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT classification model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"classification\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:classification$\n\n# GBT regression model\n\n# $example on:regression$\n# Load training data\ndf <- read.df(\"data/mllib/sample_linear_regression_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT regression model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"regression\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:regression$\n" }
{ "repo_name": "fharenheit/template-spark-app", "ref": "refs/heads/master", "path": "src/main/R/ml/gbt.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# To run this example use\n# ./bin/spark-submit examples/src/main/r/ml/gbt.R\n\n# Load SparkR library into your R session\nlibrary(SparkR)\n\n# Initialize SparkSession\nsparkR.session(appName = \"SparkR-ML-gbt-example\")\n\n# GBT classification model\n\n# $example on:classification$\n# Load training data\ndf <- read.df(\"data/mllib/sample_libsvm_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT classification model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"classification\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:classification$\n\n# GBT regression model\n\n# $example on:regression$\n# Load training data\ndf <- read.df(\"data/mllib/sample_linear_regression_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT regression model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"regression\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:regression$\n" }
{ "repo_name": "kimoonkim/spark", "ref": "refs/heads/branch-2.1-kubernetes", "path": "examples/src/main/r/ml/gbt.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# To run this example use\n# ./bin/spark-submit examples/src/main/r/ml/gbt.R\n\n# Load SparkR library into your R session\nlibrary(SparkR)\n\n# Initialize SparkSession\nsparkR.session(appName = \"SparkR-ML-gbt-example\")\n\n# GBT classification model\n\n# $example on:classification$\n# Load training data\ndf <- read.df(\"data/mllib/sample_libsvm_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT classification model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"classification\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:classification$\n\n# GBT regression model\n\n# $example on:regression$\n# Load training data\ndf <- read.df(\"data/mllib/sample_linear_regression_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT regression model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"regression\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:regression$\n" }
{ "repo_name": "ZxlAaron/mypros", "ref": "refs/heads/master", "path": "examples/src/main/r/ml/gbt.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# To run this example use\n# ./bin/spark-submit examples/src/main/r/ml/gbt.R\n\n# Load SparkR library into your R session\nlibrary(SparkR)\n\n# Initialize SparkSession\nsparkR.session(appName = \"SparkR-ML-gbt-example\")\n\n# GBT classification model\n\n# $example on:classification$\n# Load training data\ndf <- read.df(\"data/mllib/sample_libsvm_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT classification model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"classification\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:classification$\n\n# GBT regression model\n\n# $example on:regression$\n# Load training data\ndf <- read.df(\"data/mllib/sample_linear_regression_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT regression model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"regression\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:regression$\n" }
{ "repo_name": "chgm1006/spark-app", "ref": "refs/heads/master", "path": "src/main/R/ml/gbt.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# To run this example use\n# ./bin/spark-submit examples/src/main/r/ml/gbt.R\n\n# Load SparkR library into your R session\nlibrary(SparkR)\n\n# Initialize SparkSession\nsparkR.session(appName = \"SparkR-ML-gbt-example\")\n\n# GBT classification model\n\n# $example on:classification$\n# Load training data\ndf <- read.df(\"data/mllib/sample_libsvm_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT classification model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"classification\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:classification$\n\n# GBT regression model\n\n# $example on:regression$\n# Load training data\ndf <- read.df(\"data/mllib/sample_linear_regression_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT regression model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"regression\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:regression$\n" }
{ "repo_name": "dongjinleekr/spark-doc", "ref": "refs/heads/master", "path": "examples/src/main/r/ml/gbt.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# To run this example use\n# ./bin/spark-submit examples/src/main/r/ml/gbt.R\n\n# Load SparkR library into your R session\nlibrary(SparkR)\n\n# Initialize SparkSession\nsparkR.session(appName = \"SparkR-ML-gbt-example\")\n\n# GBT classification model\n\n# $example on:classification$\n# Load training data\ndf <- read.df(\"data/mllib/sample_libsvm_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT classification model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"classification\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:classification$\n\n# GBT regression model\n\n# $example on:regression$\n# Load training data\ndf <- read.df(\"data/mllib/sample_linear_regression_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT regression model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"regression\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:regression$\n" }
{ "repo_name": "alec-heif/MIT-Thesis", "ref": "refs/heads/master", "path": "spark-bin/examples/src/main/r/ml/gbt.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# To run this example use\n# ./bin/spark-submit examples/src/main/r/ml/gbt.R\n\n# Load SparkR library into your R session\nlibrary(SparkR)\n\n# Initialize SparkSession\nsparkR.session(appName = \"SparkR-ML-gbt-example\")\n\n# GBT classification model\n\n# $example on:classification$\n# Load training data\ndf <- read.df(\"data/mllib/sample_libsvm_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT classification model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"classification\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:classification$\n\n# GBT regression model\n\n# $example on:regression$\n# Load training data\ndf <- read.df(\"data/mllib/sample_linear_regression_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT regression model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"regression\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:regression$\n" }
{ "repo_name": "spark0001/spark2.1.1", "ref": "refs/heads/master", "path": "examples/src/main/r/ml/gbt.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# To run this example use\n# ./bin/spark-submit examples/src/main/r/ml/gbt.R\n\n# Load SparkR library into your R session\nlibrary(SparkR)\n\n# Initialize SparkSession\nsparkR.session(appName = \"SparkR-ML-gbt-example\")\n\n# GBT classification model\n\n# $example on:classification$\n# Load training data\ndf <- read.df(\"data/mllib/sample_libsvm_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT classification model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"classification\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:classification$\n\n# GBT regression model\n\n# $example on:regression$\n# Load training data\ndf <- read.df(\"data/mllib/sample_linear_regression_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT regression model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"regression\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:regression$\n" }
{ "repo_name": "SnappyDataInc/spark", "ref": "refs/heads/snappy/branch-2.1", "path": "examples/src/main/r/ml/gbt.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# To run this example use\n# ./bin/spark-submit examples/src/main/r/ml/gbt.R\n\n# Load SparkR library into your R session\nlibrary(SparkR)\n\n# Initialize SparkSession\nsparkR.session(appName = \"SparkR-ML-gbt-example\")\n\n# GBT classification model\n\n# $example on:classification$\n# Load training data\ndf <- read.df(\"data/mllib/sample_libsvm_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT classification model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"classification\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:classification$\n\n# GBT regression model\n\n# $example on:regression$\n# Load training data\ndf <- read.df(\"data/mllib/sample_linear_regression_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT regression model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"regression\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:regression$\n" }
{ "repo_name": "Panos-Bletsos/spark-cost-model-optimizer", "ref": "refs/heads/master-v2.1.0", "path": "examples/src/main/r/ml/gbt.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# To run this example use\n# ./bin/spark-submit examples/src/main/r/ml/gbt.R\n\n# Load SparkR library into your R session\nlibrary(SparkR)\n\n# Initialize SparkSession\nsparkR.session(appName = \"SparkR-ML-gbt-example\")\n\n# GBT classification model\n\n# $example on:classification$\n# Load training data\ndf <- read.df(\"data/mllib/sample_libsvm_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT classification model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"classification\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:classification$\n\n# GBT regression model\n\n# $example on:regression$\n# Load training data\ndf <- read.df(\"data/mllib/sample_linear_regression_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT regression model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"regression\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:regression$\n" }
{ "repo_name": "big-pegasus/spark", "ref": "refs/heads/snappy/branch-2.1", "path": "examples/src/main/r/ml/gbt.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# To run this example use\n# ./bin/spark-submit examples/src/main/r/ml/gbt.R\n\n# Load SparkR library into your R session\nlibrary(SparkR)\n\n# Initialize SparkSession\nsparkR.session(appName = \"SparkR-ML-gbt-example\")\n\n# GBT classification model\n\n# $example on:classification$\n# Load training data\ndf <- read.df(\"data/mllib/sample_libsvm_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT classification model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"classification\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:classification$\n\n# GBT regression model\n\n# $example on:regression$\n# Load training data\ndf <- read.df(\"data/mllib/sample_linear_regression_data.txt\", source = \"libsvm\")\ntraining <- df\ntest <- df\n\n# Fit a GBT regression model with spark.gbt\nmodel <- spark.gbt(training, label ~ features, \"regression\", maxIter = 10)\n\n# Model summary\nsummary(model)\n\n# Prediction\npredictions <- predict(model, test)\nshowDF(predictions)\n# $example off:regression$\n" }
{ "repo_name": "lorenc5/eiCompare", "ref": "refs/heads/master", "path": "R/mbd_two_minority.R", "content": "mbd_two_minority <- function(md, colnames, two=TRUE) {\n  #takes output from md_bayes_draw\n  # Two candidates, variables labeled the same\n  # V1, V2. When two=F, V3; 3 candidates\n  # VtdAVap_cor, VtdMVap_cor\n  if (two){\n    \n    M.num.v1.est <- md[,\"ccount.VtdMVap_cor.V1\"] \n    M.num.v2.est <- md[,\"ccount.VtdMVap_cor.V2\"]\n    minority <- mean_and_ci(cbind(M.num.v1.est, M.num.v2.est)); minority\n    \n    # White #\n    W.num.v1.est <- md[,\"ccount.VtdAVap_cor.V1\"] \n    W.num.v2.est <- md[,\"ccount.VtdAVap_cor.V2\"]\n    white <- mean_and_ci(cbind(W.num.v1.est, W.num.v2.est)); white\n    \n    # Adjust output to begin to match what we have\n    # Rbind the mean/CI's together\n    res <- rbind(minority, white)\n    # Transpose; ordering needs to be the same\n    minority2 <- t(t(c(res[1,], res[2,])))\n    white2 <- t(t(c(res[3,], res[4,])))\n    \n    rxc <- cbind(minority2, white2) \n    rxc <- round(rxc, 5)*100 # Rounding\n    row.names(rxc)[c(1,4)] <- colnames\n    suppressWarnings( rxc <- data.frame(row.names(rxc), rxc) )\n    colnames(rxc) <- c(\"Candidate\",\"RxC: Minority\", \"RxC: White\")\n    \n    return(list(rowXcolumn = rxc, minority=minority, white=white))\n  } else { # For 3-candidates, spaghetti \n    \n    # Minority Vote #\n    M.num.v1.est <- md[,\"ccount.VtdMVap_cor.V1\"] \n    M.num.v2.est <- md[,\"ccount.VtdMVap_cor.V2\"]\n    M.num.v3.est <- md[,\"ccount.VtdMVap_cor.V3\"]\n    minority <- mean_and_ci(cbind(M.num.v1.est, M.num.v2.est, M.num.v3.est)); minority\n    \n    # White #\n    W.num.v1.est <- md[,\"ccount.VtdAVap_cor.V1\"] \n    W.num.v2.est <- md[,\"ccount.VtdAVap_cor.V2\"]\n    W.num.v3.est <- md[,\"ccount.VtdAVap_cor.V3\"]\n    white <- mean_and_ci(cbind(W.num.v1.est, W.num.v2.est, W.num.v3.est)); white\n    \n    # Adjust output to begin to match what we have\n    # Rbind the mean/CI's together\n    res <- rbind(minority, white)\n    \n    # Transpose; ordering needs to be the same\n    minority2 <- t(t(c(res[1,], res[2,], res[3,])))\n    white2 <- t(t(c(res[4,], res[5,], res[6,])))\n    \n    rxc <- cbind(minority2, white2) \n    rxc <- round(rxc, 5)*100 # Rounding\n    row.names(rxc)[c(1,4,7)] <- colnames\n    suppressWarnings( rxc <- data.frame(row.names(rxc), rxc) )\n    colnames(rxc) <- c(\"Candidate\",\"RxC: Minority\", \"RxC: White\")\n    \n    return(list(rowXcolumn = rxc, minority=minority, white=white))\n  }\n}" }
{ "repo_name": "cran/rv", "ref": "refs/heads/master", "path": "R/sort_rv.R", "content": "#' Distribution of Order Statistics of a Random Vector\n#' \n#' \\code{sort.rv} computes the distribution of the order statistics of a random\n#' vector.\n#' \n#' The result is the \\emph{distribution} of the order statistic of the given\n#' vector \\code{x}: that is, the \\code{sort} function is applied to each\n#' \\emph{row} of the matrix of simulations of \\code{x} (\\code{sims(x)}) and\n#' returned then in random vector form.\n#' \n#' See \\code{\\link{sort}} for further details of the function \\code{sort}.\n#' \n#' @param x a random vector\n#' @param \\dots further arguments passed to \\code{sort.rv}\n#' @return An rv object of the same length as \\code{x}.\n#' @author Jouni Kerman \\email{jouni@@kerman.com}\n#' @seealso \\code{\\link{sort}}\n#' @references Kerman, J. and Gelman, A. (2007). Manipulating and Summarizing\n#' Posterior Simulations Using Random Variable Objects. Statistics and\n#' Computing 17:3, 235-244.\n#' \n#' See also \\code{vignette(\"rv\")}.\n#' @keywords manip\n#' @export\n#' @method sort rv\nsort.rv <- function (x, ...) {\n  simapply(x, base::sort, ...)\n}\n" }
{ "repo_name": "stdlib-js/stdlib", "ref": "refs/heads/develop", "path": "lib/node_modules/@stdlib/math/base/special/besselj0/benchmark/r/benchmark.R", "content": "#!/usr/bin/env Rscript\n#\n# @license Apache-2.0\n#\n# Copyright (c) 2018 The Stdlib Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Set the precision to 16 digits:\noptions( digits = 16 );\n\n#' Run benchmarks.\n#'\n#' @examples\n#' main();\nmain <- function() {\n\t# Define benchmark parameters:\n\tname <- \"besselj0\";\n\titerations <- 1000000L;\n\trepeats <- 3;\n\n\t#' Print the TAP version.\n\t#'\n\t#' @examples\n\t#' print_version();\n\tprint_version <- function() {\n\t\tcat( \"TAP version 13\\n\" );\n\t}\n\n\t#' Print the TAP summary.\n\t#'\n\t#' @param total Total number of tests.\n\t#' @param passing Total number of passing tests.\n\t#'\n\t#' @examples\n\t#' print_summary( 3, 3 );\n\tprint_summary <- function( total, passing ) {\n\t\tcat( \"#\\n\" );\n\t\tcat( paste0( \"1..\", total, \"\\n\" ) ); # TAP plan\n\t\tcat( paste0( \"# total \", total, \"\\n\" ) );\n\t\tcat( paste0( \"# pass  \", passing, \"\\n\" ) );\n\t\tcat( \"#\\n\" );\n\t\tcat( \"# ok\\n\" );\n\t}\n\n\t#' Print benchmark results.\n\t#'\n\t#' @param iterations Number of iterations.\n\t#' @param elapsed Elapsed time in seconds.\n\t#'\n\t#' @examples\n\t#' print_results( 10000L, 0.131009101868 );\n\tprint_results <- function( iterations, elapsed ) {\n\t\trate <- iterations / elapsed;\n\t\tcat( \"  ---\\n\" );\n\t\tcat( paste0( \"  iterations: \", iterations, \"\\n\" ) );\n\t\tcat( paste0( \"  elapsed: \", elapsed, \"\\n\" ) );\n\t\tcat( paste0( \"  rate: \", rate, \"\\n\" ) );\n\t\tcat( \"  ...\\n\" );\n\t}\n\n\t#' Run a benchmark.\n\t#'\n\t#' ## Notes\n\t#'\n\t#' * We compute and return a total \"elapsed\" time, rather than the minimum\n\t#'   evaluation time, to match benchmark results in other languages (e.g.,\n\t#'   Python).\n\t#'\n\t#'\n\t#' @param iterations Number of Iterations.\n\t#' @return Elapsed time in seconds.\n\t#'\n\t#' @examples\n\t#' elapsed <- benchmark( 10000L );\n\tbenchmark <- function( iterations ) {\n\t\t# Run the benchmarks:\n\t\tresults <- microbenchmark::microbenchmark( besselJ( runif(1)*100.0, 0 ), times = iterations );\n\n\t\t# Sum all the raw timing results to get a total \"elapsed\" time:\n\t\telapsed <- sum( results$time );\n\n\t\t# Convert the elapsed time from nanoseconds to seconds:\n\t\telapsed <- elapsed / 1.0e9;\n\n\t\treturn( elapsed );\n\t}\n\n\tprint_version();\n\tfor ( i in 1:repeats ) {\n\t\tcat( paste0( \"# r::\", name, \"\\n\" ) );\n\t\telapsed <- benchmark( iterations );\n\t\tprint_results( iterations, elapsed );\n\t\tcat( paste0( \"ok \", i, \" benchmark finished\", \"\\n\" ) );\n\t}\n\tprint_summary( repeats, repeats );\n}\n\nmain();\n" }
{ "repo_name": "Lijinsr/Convolution-Neural-Network", "ref": "refs/heads/master", "path": "cnn_convolve.R", "content": "cnn_convolve <- function(filter_dim, num_filters, images, w,b) {\n  #options(digits = 2)\n  num_images <- dim(images)[3]\n  image_dim <- dim(images)[1]\n  conv_dim <- image_dim - filter_dim+1\n  col_dim <- dim(images)[2] - filter_dim +1\n  convolved_features <- array(0, c(conv_dim, conv_dim, num_filters,\n                                   num_images))\n  for(image_num in 1:num_images) {\n    for(filter_num in 1:num_filters) {\n      convolved_image <- array(0, c(conv_dim, conv_dim))\n      filters <- w\n      x <- images[,,image_num]\n      y <- filters[,,filter_num]\n      for(i in 1:conv_dim) {\n        for(j in 1: col_dim) {\n          convolved_image[i,j] <- convolve(x[i:(i+filter_dim-1),\n                                             j:(j+filter_dim-1)], y, \n                                           type = \"filter\")\n        }\n      }\n      convolved_image <- convolved_image + b[filter_num]\n      convolved_image <- tanh(convolved_image)\n      \n      convolved_features[,,filter_num, image_num] <- convolved_image\n      #return(convolved_features)\n    }\n  }\n}\n\n" }
{ "repo_name": "okgreece/DescriptiveStats.OBeu", "ref": "refs/heads/master", "path": "R/ds.statistics.R", "content": "#' @title\n#' Calculation of the Statistic Measures\n#'\n#' @description\n#' This function calculates the basic descriptive measures of the input dataset.\n#'\n#' @usage ds.statistics(data, tojson = FALSE)\n#'\n#' @param data A numeric vector, matrix or data frame\n#' @param tojson If TRUE the results are returned in json format, default returns a list\n#'\n#' @details\n#' This function returns the following values of the input data: minimum, maximum, range, mean, median, first and third quantiles, variance,\n#' standart deviation, skewness and kurtosis.\n#'\n#' @return\n#' A list or json file with the following components:\n#' \\itemize{\n#' \\item Min The minimum observed value of the input data\n#' \\item Max The maximum observed value of the input data\n#' \\item Range The range, defined as the difference of the maximum and the minimum value.\n#' \\item Mean The average value of the input data\n#' \\item Median The median value of the input data\n#' \\item Quantiles The 25\\% and 75\\% percentiles\n#' \\item Variance The variance of the input data\n#' \\item Standard Deviation The standard deviation of the input data\n#' \\item Skewness The Skewness of the input data\n#' \\item Kurtosis The Kurtosis of the input data\n#' }\n#'\n#' @author Aikaterini Chatzopoulou, Kleanthis Koupidis, Charalampos Bratsas\n#'\n#' @seealso \\code{\\link{open_spending.ds}}\n#'\n#' @examples\n#' # with matrix as an input and json outpout\n#' Matrix <- cbind(\n#'   Uni05 = (1:200) / 21, Norm = rnorm(200),\n#'   `5T` = rt(200, df = 5), Gam2 = rgamma(200, shape = 2)\n#' )\n#' ds.statistics(Matrix, tojson = TRUE)\n#'\n#' # with vector as an input\n#' vec <- as.vector(iris$Sepal.Width)\n#' ds.statistics(vec, tojson = FALSE)\n#'\n#' # with iris data frame as an input\n#' ds.statistics(iris, tojson = FALSE)\n#'\n#' # OpenBudgets.eu Dataset Example:\n#' ds.statistics(Wuppertal_df$Amount, tojson = TRUE)\n#' @rdname ds.statistics\n#'\n#' @export\n#'\n\nds.statistics <- function(data, tojson = FALSE) {\n\n  # Convert to data frame\n  data <- as.data.frame(data)\n\n  # only numeric data\n  data.num <- nums(data)\n\n  # Calculation of statistics\n  min <- lapply(data.num, min)\n  max <- lapply(data.num, max)\n  range <- as.list(unlist(max) - unlist(min))\n  mean <- lapply(data.num, mean)\n  median <- lapply(data.num, median)\n  var <- lapply(data.num, var)\n  stdev <- lapply(data.num, stats::sd)\n  kurtosis <- ds.kurtosis(data.num)\n  skewness <- ds.skewness(data.num)\n  q <- lapply(data.num, stats::quantile, probs = c(0.25, 0.75))\n\n  # construction of dataframe with all the statistics\n  statistics <- list(\n    Min = min,\n    Max = max,\n    Range = range,\n    Mean = mean,\n    Median = median,\n    Quantiles = q,\n    Variance = var,\n    StandardDeviation = stdev,\n    Kurtosis = kurtosis,\n    Skewness = skewness\n  )\n\n  if (tojson == TRUE) {\n    statistics <- jsonlite::toJSON(statistics)\n  }\n  # Return\n  return(statistics)\n}\n" }
{ "repo_name": "envisionnw/npstoret2R", "ref": "refs/heads/master", "path": "R/R_db_class.R", "content": "############################################################\n# R App Settings\n############################################################\n# Purpose:    Single file access to application settings class\n#\n# Notes:      N/A\n#\n# Sources:  See individual functions\n#              \n# Revisions:  0.1  2014-10-05  B. Campbell  initial version\n#             0.2  2014-11-17  B. Campbell  updated to S4 class\n#             0.3  2014-11-24  B. Campbell  fixed connect slot\n#             0.4  2014-11-29  B. Campbell  updated connect documentation\n# ==========================================================\n\n# ----------------------------\n#  Database S3 Class\n# ----------------------------\n\n# ----------------------------------------------------------------------\n#' @title RODBC\n#' @name RODBC-class\n#' @details Handles ODBC database connections to R.\n#'\n#' @section Requirements:\n#' R Libraries:\n#' \\itemize{\n#'  \\item \\link[RODBC]{RODBC}\n#' }\n#'\n#' @section References:\n#'   \\tabular{lll}{\n#'   \\tab Jan. 30, 2012 \\tab Kyle Brandt \\cr\n#'   \\tab\\tab http://stackoverflow.com/questions/9067492/example-of-using-an-s3-class-in-a-s4-object \\cr\n#'   \\tab Oct. 14, 2011 \\tab Triad Sou. \\cr\n#'   \\tab\\tab http://stackoverflow.com/questions/7758748/documenting-setas-and-setoldclass-with-roxygen \\cr\n#'   }\n#' @section Sources:\n#'   \\tabular{llllllll}{\n#'   \\tab 2014-11-24 \\tab\\tab B. Campbell \\tab\\tab 0.1 \\tab\\tab Initial version \\cr\n#'   }\n#' @section Revisions:\n#'   \\tabular{llllllll}{\n#'   \\tab 0.1   \\tab\\tab 2014-11-24  \\tab\\tab BLC   \\tab\\tab Added to resolve S4 connect slot error \\cr\n#'   }\n#'\n#' @family RODBC\n#' @exportClass RODBC\n# ----------------------------------------------------------------------\nsetOldClass(\"RODBC\")\n\n# ----------------------------------------------------------------------\n#' @title db Class\n#' @details Creates a database object whose connection parameters are contained within the object.\n#'\n#' @slot dbfile         - database name & directory path\n#' @slot user           - database username (for authentication)\n#' @slot pwd            - database password (for authentication)\n#' @slot connect        - database connection\n#' \n#' @examples\n#' ## Create the database object (db) and assign the NPSTORET backend filename.\n#' ## Paths should be written UNIX style with / vs. \\ .\n#' ## So a windows file path like \"C:\\db\\dbname.mdb\" should be written \"C:/db/dbname.mdb\".\n#' \n#' \\dontrun{\n#'  \n#' Connect to your database via a DSN-less connection.\n#' \n#'connect(mynpstoret,dbfile=\"C:/NPSTORET/NPSTORET_BE.MDB\", user=\"myusername\", pwd=\"mypwd\")\n#'}\n#'\n#' @section Requirements:\n#' R Libraries:\n#' \\itemize{\n#'  \\item \\link[RODBC]{RODBC}\n#'  \\item \\link[tools]{tools}\n#' }\n#'\n#' @section References:\n#'   \\tabular{lll}{\n#'   \\tab 2004 \\tab Friedrich Leisch \\cr\n#'   \\tab\\tab http://www.r-project.org/conferences/useR-2004/Keynotes/Leisch.pdf \\cr\n#'   \\tab May 6, 2014 \\tab Rappster \\cr\n#'   \\tab\\tab http://stackoverflow.com/questions/23495627/roxygen2-s4-generic-functions-are-not-exported-unless-a-method-is-also-defined \\cr\n#'   }\n#' @section Sources:\n#'   \\tabular{llllllll}{\n#'   \\tab 2014-10-05 \\tab\\tab B. Campbell \\tab\\tab 0.1 \\tab\\tab Initial version \\cr\n#'   }\n#' @section Revisions:\n#'   \\tabular{llllllll}{\n#'   \\tab 0.1   \\tab\\tab 2014-10-05  \\tab\\tab BLC   \\tab\\tab Initial version \\cr\n#'   \\tab 0.2   \\tab\\tab 2014-11-07  \\tab\\tab BLC   \\tab\\tab Documentation update & removed sqlQUeriesFilepath since SQL queries are integrated \\cr\n#'   \\tab 0.3   \\tab\\tab 2014-11-13  \\tab\\tab BLC   \\tab\\tab Documentation update \\cr\n#'   \\tab 0.4   \\tab\\tab 2014-11-17  \\tab\\tab BLC   \\tab\\tab Converted to S4 class & renamed to db vs. app \\cr\n#'   \\tab 0.5   \\tab\\tab 2014-11-23  \\tab\\tab BLC   \\tab\\tab Changed to Ref class vs S4 to resolve connect slot error \\cr\n#'   \\tab 0.6   \\tab\\tab 2014-12-29  \\tab\\tab BLC   \\tab\\tab Documentation update \\cr\n#'   }\n#'    \n#' @family Application settings\n#' @exportClass db\n# ----------------------------------------------------------------------\n\n# ----------------------------\n# NPSTORET Database Attributes\n# ----------------------------\n#user, pwd, dbpath\nsetClass(\n    # set db file path\n    Class=\"db\",\n        \n    # define the slots\n    representation=representation(\n      dbfile=\"character\",\n      user = \"character\",\n      pwd = \"character\",\n      connect = \"RODBC\"\n      ),\n    \n    # ensure connect is valid\n    prototype = prototype(\n      connect = structure(list(), class=\"RODBC\")\n      )\n    \n    # validate if data is consistent\n    # not called if an initialize function is defined!\n#    validity=function(object){\n#      if(!(file.exists(object@dbfile))){\n#        return(\"Database file doesn't exist.\")\n#        stop (\"Please check your database file location. Also ensure all \\\\ are replaced by //\n#            since R doesn't translate the \\\\ in the same way Windows does.\")\n        \n#      }\n#      return(TRUE)\n#    }\n  )\n\n# ----------------------------\n#  Database Class Methods\n# ----------------------------\n# ----------------------------------------------------------------------\n#' @title setDbFile\n#' @details Set the database file name and path.\n#'\n#' @param dbObject - database object\n#' @param dbfile   - database name & directory path\n#'\n#' @examples\n#' \\dontrun{\n#'   #set database file & path\n#'   setDbFile(db,\"C:/mydatabase/dbfile.accb\")\n#' }\n#'\n#' @section Requirements:\n#' R Libraries:\n#' \\itemize{\n#'  \\item \\link[RODBC]{RODBC}\n#'  \\item \\link[tools]{tools}\n#' }\n#'\n#' @section References:\n#'   \\tabular{lll}{\n#'   \\tab 2004 \\tab Friedrich Leisch \\cr\n#'   \\tab\\tab http://www.r-project.org/conferences/useR-2004/Keynotes/Leisch.pdf \\cr\n#'   \\tab May 6, 2014 \\tab Rappster \\cr\n#'   \\tab\\tab http://stackoverflow.com/questions/23495627/roxygen2-s4-generic-functions-are-not-exported-unless-a-method-is-also-defined \\cr\n#'   \\tab March 11, 2014 \\tab Hadley Wickham \\cr\n#'   \\tab\\tab https://groups.google.com/forum/#!topic/rdevtools/sq3DG0oj058 \\cr\n#'   }\n#' @section Sources:\n#'   \\tabular{llllllll}{\n#'   \\tab 2014-11-18 \\tab\\tab B. Campbell \\tab\\tab 0.1 \\tab\\tab Initial version \\cr\n#'   }\n#' @section Revisions:\n#'   \\tabular{llllllll}{\n#'   \\tab 0.1   \\tab\\tab 2014-11-18  \\tab\\tab BLC   \\tab\\tab Initial version \\cr\n#'   }\n#'    \n#' @family Application settings\n#' @exportMethod setDbFile\n# ----------------------------------------------------------------------\n# setDbFile generic & method\nsetGeneric(name=\"setDbFile\",\n           def=function(dbObject,dbfile){\n             standardGeneric(\"setDbFile\")\n           }\n)\nsetMethod(f=\"setDbFile\",\n          signature=\"db\",\n          definition = function(dbObject, dbfile){\n            dbObject@dbfile <- dbfile            \n            return(dbObject)\n          }\n)\n\n# ----------------------------------------------------------------------\n#' @title setDbUser\n#' @details Sets database username\n#'\n#' @param dbObject - database object\n#' @param user     - database username (for authentication)\n#'\n#' @examples\n#' \\dontrun{\n#'  setDbUser(db,\"myusername\")\n#'}\n#'\n#' @section Requirements:\n#' R Libraries:\n#' \\itemize{\n#'  \\item \\link[RODBC]{RODBC}\n#'  \\item \\link[tools]{tools}\n#' }\n#'\n#' @section References:\n#'   \\tabular{lll}{\n#'   \\tab 2004 \\tab Friedrich Leisch \\cr\n#'   \\tab\\tab http://www.r-project.org/conferences/useR-2004/Keynotes/Leisch.pdf \\cr\n#'   \\tab May 6, 2014 \\tab Rappster \\cr\n#'   \\tab\\tab http://stackoverflow.com/questions/23495627/roxygen2-s4-generic-functions-are-not-exported-unless-a-method-is-also-defined \\cr\n#'   \\tab March 11, 2014 \\tab Hadley Wickham \\cr\n#'   \\tab\\tab https://groups.google.com/forum/#!topic/rdevtools/sq3DG0oj058 \\cr\n#'   }\n#' @section Sources:\n#'   \\tabular{llllllll}{\n#'   \\tab 2014-11-18 \\tab\\tab B. Campbell \\tab\\tab 0.1 \\tab\\tab Initial version \\cr\n#'   }\n#' @section Revisions:\n#'   \\tabular{llllllll}{\n#'   \\tab 0.1   \\tab\\tab 2014-11-18  \\tab\\tab BLC   \\tab\\tab Initial version \\cr\n#'   }\n#'    \n#' @family Application settings\n#' @exportMethod setDbUser\n# ----------------------------------------------------------------------\n# setDbUser generic & method\nsetGeneric(name=\"setDbUser\",\n           def=function(dbObject,user){\n             standardGeneric(\"setDbUser\")\n           }\n)\nsetMethod(f=\"setDbUser\",\n          signature=\"db\",\n          definition = function(dbObject, user){\n            dbObject@user <- user            \n            return(dbObject)\n          }\n)\n\n# ----------------------------------------------------------------------\n#' @title setDbPwd\n#' @details Sets database user password\n#'\n#' @param dbObject - database object\n#' @param pwd      - database password (for authentication)\n#'\n#' @examples\n#' \n#' \\dontrun{\n#'  setDbPwd(db,\"mypassword\")\n#'}\n#'\n#' @section Requirements:\n#' R Libraries:\n#' \\itemize{\n#'  \\item \\link[RODBC]{RODBC}\n#'  \\item \\link[tools]{tools}\n#' }\n#'\n#' @section References:\n#'   \\tabular{lll}{\n#'   \\tab 2004 \\tab Friedrich Leisch \\cr\n#'   \\tab\\tab http://www.r-project.org/conferences/useR-2004/Keynotes/Leisch.pdf \\cr\n#'   \\tab May 6, 2014 \\tab Rappster \\cr\n#'   \\tab\\tab http://stackoverflow.com/questions/23495627/roxygen2-s4-generic-functions-are-not-exported-unless-a-method-is-also-defined \\cr\n#'   \\tab March 11, 2014 \\tab Hadley Wickham \\cr\n#'   \\tab\\tab https://groups.google.com/forum/#!topic/rdevtools/sq3DG0oj058 \\cr\n#'   }\n#' @section Sources:\n#'   \\tabular{llllllll}{\n#'   \\tab 2014-11-18 \\tab\\tab B. Campbell \\tab\\tab 0.1 \\tab\\tab Initial version \\cr\n#'   }\n#' @section Revisions:\n#'   \\tabular{llllllll}{\n#'   \\tab 0.1   \\tab\\tab 2014-11-18  \\tab\\tab BLC   \\tab\\tab Initial version \\cr\n#'   }\n#'    \n#' @family Application settings\n#' @exportMethod setDbPwd\n# ----------------------------------------------------------------------\n# setDbPwd generic & method\nsetGeneric(name=\"setDbPwd\",\n           def=function(dbObject,pwd){\n             standardGeneric(\"setDbPwd\")\n           }\n)\nsetMethod(f=\"setDbPwd\",\n          signature=\"db\",\n          definition = function(dbObject, pwd){\n            dbObject@pwd <- pwd            \n            return(dbObject)\n          }\n)\n# ----------------------------\n# NPSTORET Database Connection\n# ----------------------------\n# ----------------------------------------------------------------------\n#' @title connect\n#' @details Creates DSN-less database RODBC connection\n#'\n#' @param dbObject - database object\n#' @param dbfile   - database name & directory path\n#' @param user     - database username (for authentication)\n#' @param pwd      - database password (for authentication)\n#' \n#' @examples\n#' \n#' \\dontrun{\n#'  connect(mydb, dbfilename=\"C:/database/npstoret.accb\", user=\"myusername\", pwd=\"mypassword\") \n#'}\n#'\n#' @section Requirements:\n#' R Libraries:\n#' \\itemize{\n#'  \\item \\link[RODBC]{RODBC}\n#'  \\item \\link[tools]{tools}\n#' }\n#'\n#' @section References:\n#'   \\tabular{lll}{\n#'   \\tab 2004 \\tab Friedrich Leisch \\cr\n#'   \\tab\\tab http://www.r-project.org/conferences/useR-2004/Keynotes/Leisch.pdf \\cr\n#'   \\tab May 6, 2014 \\tab Rappster \\cr\n#'   \\tab\\tab http://stackoverflow.com/questions/23495627/roxygen2-s4-generic-functions-are-not-exported-unless-a-method-is-also-defined \\cr\n#'   \\tab March 11, 2014 \\tab Hadley Wickham \\cr\n#'   \\tab\\tab https://groups.google.com/forum/#!topic/rdevtools/sq3DG0oj058 \\cr\n#'   \\tab Dec. 12, 2012 \\tab Martin Morgan \\cr\n#'   \\tab\\tab http://stackoverflow.com/questions/13841400/use-s3-virtual-class-as-slot-of-an-s4-class-got-error-got-class-s4-should-b \\cr\n#'   }\n#' @section Sources:\n#'   \\tabular{llllllll}{\n#'   \\tab 2014-11-18 \\tab\\tab B. Campbell \\tab\\tab 0.1 \\tab\\tab Initial version \\cr\n#'   }\n#' @section Revisions:\n#'   \\tabular{llllllll}{\n#'   \\tab 0.1   \\tab\\tab 2014-11-18  \\tab\\tab BLC   \\tab\\tab Initial version \\cr\n#'   \\tab 0.2   \\tab\\tab 2014-12-01  \\tab\\tab BLC   \\tab\\tab Fixed check file bug (use == not =) \\cr\n#'   \\tab 0.3   \\tab\\tab 2014-12-29  \\tab\\tab BLC   \\tab\\tab Updated documentation \\cr\n#'   }\n#'    \n#' @family Application settings\n#' @exportMethod connect\n# ----------------------------------------------------------------------\n# connection generic & method\nsetGeneric(name=\"connect\",\n           def=function(dbObject,dbfile,user,pwd){\n             standardGeneric(\"connect\")\n           }\n)\nsetMethod(f=\"connect\",\n          signature=\"db\",\n          definition = function(dbObject, dbfile, user, pwd){\n            \n            # check file is access app (do outside of connect.app): \n            if((file_ext(dbfile) == 'mapp') | (file_ext(dbfile) == 'accb')){\n\n              # set values\n              setDbFile(dbfile)\n              setUser(user)\n              setPwd(pwd)\n              \n              # check file is readable\n              if(file.access(dbfile,mode=4) == 0){  \n                dbObject@connect <- odbcConnectAccess2007(dbfile)   \n                return(dbObject)\n              }\n            }        \n          }\n)" }
{ "repo_name": "sammorris81/extreme-decomp", "ref": "refs/heads/master", "path": "markdown/fire-analysis/fit-ebf-30-4.R", "content": "rm(list=ls())\n\nsource(file = \"./package_load.R\", chdir = T)\n# Number of bases: 5, 10, 15, 20\nprocess <- \"ebf\" # ebf: empirical basis functions, gsk: gaussian kernels\nmargin  <- \"gsk\" # ebf: empirical basis functions, gsk: gaussian kernels\nL       <- 30    # number of knots to use for the basis functions\ncv      <- 4     # which cross-validation set to use\nresults.file <- paste(\"./cv-results/\", process, \"-\", margin, \"-\", L, \"-\", cv,\n                      \".RData\", sep = \"\")\ntable.file   <- paste(\"./cv-tables/\", process, \"-\", margin, \"-\", L, \"-\", cv,\n                      \".txt\", sep = \"\")\n\n# fit the model and get predictions\nsource(file = \"./fitmodel.R\")\n\nrm(list=ls())\n\nsource(file = \"./package_load.R\", chdir = T)\n# Number of bases: 5, 10, 15, 20\nprocess <- \"ebf\" # ebf: empirical basis functions, gsk: gaussian kernels\nmargin  <- \"gsk\" # ebf: empirical basis functions, gsk: gaussian kernels\nL       <- 30    # number of knots to use for the basis functions\ncv      <- 9     # which cross-validation set to use\nresults.file <- paste(\"./cv-results/\", process, \"-\", margin, \"-\", L, \"-\", cv,\n                      \".RData\", sep = \"\")\ntable.file   <- paste(\"./cv-tables/\", process, \"-\", margin, \"-\", L, \"-\", cv,\n                      \".txt\", sep = \"\")\n\n# fit the model and get predictions\nsource(file = \"./fitmodel.R\")" }
{ "repo_name": "NIASC/VirusMeta", "ref": "refs/heads/master", "path": "blast_module/taxSort.R", "content": "library(Epi)\nlibrary(epicalc)\n\n#############################\n#Read command line arguments#\n#############################\nargs<-commandArgs(TRUE)\nALL_TAXONOMY = sprintf(\"%s\", args[1])\ndivision = sprintf(\"%s\", args[2])\naggregate_nr = sprintf(\"%s\", args[3])\nnr_by_index = sprintf(\"%s\", args[4])\nnr_hg_pre  = sprintf(\"%s\", args[5])\nnr_BAC_pre  = sprintf(\"%s\", args[6])\nnr_PHG_pre  = sprintf(\"%s\", args[7])\nnr_VEC_pre = sprintf(\"%s\", args[8])\nnr_unmapped = sprintf(\"%s\", args[9])\nnr_total = sprintf(\"%s\", args[10])\n\n#######################\n#Read PB final results#\n#######################\nPB<-read.csv(\"final.blast_results\",sep=\"@\")\nPB<-PB[!duplicated(PB$Queryid),]\n\n#######################\n#Read taxonomy results#\n#######################\nALL_tax<-read.table(ALL_TAXONOMY)\ncolnames(ALL_tax)<-c(\"gi\",\"Division\")\n\n###!!!!!!!!!!!!!!!!!!!!!!!\n#Removed\n###!!!!!!!!!!!!!!!!!!!!!!!\n#div<-read.table(division)\n#colnames(div)<-c(\"gi\",\"division\")\n#ALL_tax<-merge(div,ALL_tax,all=T)\n#ALL_tax<-ALL_tax[!duplicated(ALL_tax$gi),]\n\n########################\n#merge with PB\nPB<-merge(ALL_tax,PB,all=T)\n\n#If sequences doesn\"t belong to any pre difenied division, clasify them as others\nPB$Division<-ifelse(is.na(PB$Division),\"Other\",as.character(PB$Division));\nlength(PB$Queryid)\nPB<-PB[,c(\"gi\",\"Division\",\"Queryid\",\"identity\",\"Coverage\",\"Strain\",\"alignment.length\",\"Chimera\",\"Strand\",\"q.start\",\"q.end\",\"s.start\",\"s.end\",\"e.value\",\"bitscore\",\"Length\")]\ncolnames(PB)<-c(\"gi\",\"Division\",\"Queryid\",\"identity\",\"Coverage\",\"Strain\",\"alignment.length\",\"Chimera\",\"Strand\",\"q.start\",\"q.end\",\"s.start\",\"s.end\",\"e.value\",\"bitscore\",\"Length\")\nlength(PB$Queryid)\n######################\n#number of reads     #\n######################\nrun_read_number <- read.table(aggregate_nr)\ncolnames(run_read_number)<-c(\"Queryid\",\"NR\")\nsum(run_read_number$NR)\n\nPB_tmp<-PB\nPB<-merge(PB,run_read_number)\nlength(PB$Queryid)\nsum(PB$NR)\n\nPB_tmp<-PB_tmp[is.na(PB_tmp$Queryid[match(PB_tmp$Queryid,PB$Queryid)]),]\nif (length(PB_tmp$Queryid)>0){\n    PB_tmp$NR<-sapply(1:length(PB_tmp$Queryid), function(x) paste(sample(\"0\", 1, replace=T), collapse=\"\"))\n    PB_tmp$NR<-as.numeric(PB_tmp$NR)\n    PB<-merge(PB,PB_tmp,all=T)\n}\n\n#########\n#VIRUSES#\n#########\n#select only virus related sequences\nPB_VIRAL_TAXA<-PB[PB$Division==\"Viruses\",]\n\n#read virus taxonomy file\nVIRAL_TAXA<-read.table(\"VIRAL_TAXONOMY.txt\",sep=\"@\")\n\ncolnames(VIRAL_TAXA)<-c(\"gi\",\"Division_virus\",\"Kingdom\",\"Family\",\"Genus\",\"Species\")\nVIR<-merge(PB_VIRAL_TAXA,VIRAL_TAXA,all=T)\nVIR<-VIR[!duplicated(VIR$Queryid),]\nVIR<-VIR[!is.na(VIR$Queryid),]\n\nsum(VIR$NR)\n##########################################\n#identify if there is any viruses with Taxa missing values and flag them as corresponding taxa\nVIR$Family<-as.character(VIR$Family)\ntable(VIR$Family)\nVIR$Family<-ifelse(VIR$Family==\"n\", \"unclassified\", VIR$Family)\nVIR$Family<-ifelse(VIR$Division_virus==\"Viruses from environmental samples\",paste(VIR$Family,\"ENV\",sep=\"_\"),as.character(VIR$Family))\ntable(VIR$Family)\n\n######################\n#Prepare NR calculation Humans\nif (file.exists(\"HG\")) {\n   HG<-read.table(\"HG\",header=T)\n   if (file.exists(\"HUMAN_EST\")) {\n       HUMAN_EST<-read.table(\"HUMAN_EST\",header=T)\n       HG<-merge(HG,HUMAN_EST,all=T)\n   }\n   HG<-merge(HG,run_read_number)\n   if (length(HG$Division)>0){\n      HG$Division<-sapply(1:length(HG$Queryid), function(x) paste(sample(\"Human\", 1, replace=T), collapse=\"\"))\n      HG<-HG[!duplicated(HG$Queryid),]\n      HG<-HG[!is.na(HG$NR),]\n      head(HG)\n   }\n}\n\n#Calculate from the whole file\nnr_by_div<-stat.table(index=list(Division),contents=list(sum(NR)),data=PB);\nnr_by_div<-data.frame(nr_by_div[1,1:length(dimnames(nr_by_div)[[2]])]);\nnr_by_div$Division<-row.names(nr_by_div)\ncolnames(nr_by_div)<-c(\"NR\",\"Division\")\nnr_by_div<-nr_by_div[,c(\"Division\",\"NR\")]\nnr_by_div[is.na(nr_by_div)] <- 0;\n\n#add HG pre cleaned\nnr_hg_pre<-read.table(nr_hg_pre)\nnr_by_div$NR[nr_by_div$Division==\"Human\"] <- ((nr_hg_pre$V1*2) + nr_by_div$NR[nr_by_div$Division==\"Human\"])\n\n#add HG blast cleaned\nif (file.exists(\"HG\")) {\n   nr_by_div$NR[nr_by_div$Division==\"Human\"] <- (sum(HG$NR) + nr_by_div$NR[nr_by_div$Division==\"Human\"])\n} else{\n   nr_by_div$NR[nr_by_div$Division==\"Human\"] <- (0 + nr_by_div$NR[nr_by_div$Division==\"Human\"])\n}\n\n#add BACTERIA pre cleaned\nnr_BAC_pre<-read.table(nr_BAC_pre)\nnr_by_div$NR[nr_by_div$Division==\"Bacteria\"] <- ((nr_BAC_pre$V1*2) + nr_by_div$NR[nr_by_div$Division==\"Bacteria\"])\n\n#add PHAGE pre cleaned\nnr_PHG_pre<-read.table(nr_PHG_pre)\nnr_by_div$NR[nr_by_div$Division==\"Phages\"] <- ((nr_PHG_pre$V1*2) + nr_by_div$NR[nr_by_div$Division==\"Phages\"])\n\n\n#add VECTOR pre cleaned and put it in Other\nnr_VEC_pre<-read.table(nr_VEC_pre)\nnr_by_div$NR[nr_by_div$Division==\"Other\"] <- ((nr_VEC_pre$V1*2) + nr_by_div$NR[nr_by_div$Division==\"Other\"])\n\n\n#add unmapped sequences\nnr_unmapped<-read.table(nr_unmapped)\nnr_unmapped$Division<-c(\"Unmapped\")\ncolnames(nr_unmapped)<-c(\"NR\",\"Division\")\nnr_unmapped$NR<-nr_unmapped$NR*2 \nnr_by_div<-merge(nr_by_div,nr_unmapped,all=T)\n\n#Calculate Unknown and Percent Identities\n#TODO: check number of reads from unknown contigs as defined by PB\nnr_total<-read.table(nr_total)\nnr_total$Division<-c(\"Total\")\ncolnames(nr_total)<-c(\"NR\",\"Division\")\nnr_total$NR<-nr_total$NR*2\n\nUnknown<-data.frame(nr_total$NR-sum(nr_by_div$NR))\nUnknown$Division<-c(\"Unknown\")\ncolnames(Unknown)<-c(\"NR\",\"Division\")\nnr_by_div<-merge(nr_by_div,Unknown,all=T)\nnr_by_div<-merge(nr_by_div,nr_total,all=T)\nnr_by_div$Percent<-round((nr_by_div$NR/nr_total$NR)*100,2)\n\n\n#Calculate total number of viral taxonomied\nnr_VIR_by_taxa<-stat.table(index=list(Family),contents=list(sum(NR)),data=VIR);\nnr_VIR_by_taxa<-data.frame(nr_VIR_by_taxa[1,1:length(dimnames(nr_VIR_by_taxa)[[2]])]);\nnr_VIR_by_taxa$Family<-row.names(nr_VIR_by_taxa)\ncolnames(nr_VIR_by_taxa)<-c(\"NR\",\"Family\")\nnr_VIR_by_taxa<-nr_VIR_by_taxa[,c(\"Family\",\"NR\")]\nnr_VIR_by_taxa[is.na(nr_VIR_by_taxa)] <- 0;\n\n####################################################\ntotal_VIR<-data.frame(sum(nr_VIR_by_taxa$NR))\ntotal_VIR$Family<-c(\"Total\")\ncolnames(total_VIR)<-c(\"NR\",\"Family\")\nnr_VIR_by_taxa<-merge(nr_VIR_by_taxa,total_VIR,all=T)\nnr_VIR_by_taxa$Percent<-round((nr_VIR_by_taxa$NR/sum(VIR$NR))*100, 2)\n#TODO: Check this why there dupocation and NAs\nnr_by_div<-nr_by_div[!is.na(nr_by_div$Percent),]\n\n#index for viruses\nNR_index<-read.csv(nr_by_index)\nVIR_index<-merge(VIR,NR_index)\n\n#Now select clean DB for clustering analysis\n#First I will clean Bacteria,Human,Other,Phages seuences if thery have abover 90% identity\nPB$Coverage<-as.numeric(as.character(PB$Coverage))\n#clean_PB<-PB[(PB$identity<70 & PB$Coverage>=70) | PB$Division==\"Viruses\",]\ndirty_PB<-PB[(as.numeric(PB$identity)>=70 & as.numeric(PB$Coverage>=70)) & PB$Division!=\"Viruses\",]\nclean_PB<-PB[is.na(match(PB$Queryid,dirty_PB$Queryid)),]\n#dirty_PB<-PB[is.na(match(PB$Queryid,clean_PB$Queryid)),]\n\n#################\n# write results #\n#################\nwrite.csv(nr_by_div,\"nr_by_div_aggregate.csv\",row.names=F)\nwrite.csv(nr_VIR_by_taxa,\"nr_VIR_by_taxa.csv\",row.names=F)\nwrite.csv(PB,\"nt_final.csv\",row.names=F);\nwrite.csv(VIR,\"viruses.csv\",row.names=F);\nwrite.table(data.frame(clean_PB$Queryid),\"clean_ID.txt\",row.names=F, col.names=F, quote=FALSE, sep=\"\\t\");\nwrite.table(data.frame(dirty_PB$Queryid),\"dirty_ID.txt\",row.names=F, col.names=F, quote=FALSE, sep=\"\\t\");\nwrite.table(data.frame(VIR$Queryid),\"VIR_ID.txt\",row.names=F, col.names=F, quote=FALSE, sep=\"\\t\");\nwrite.csv(VIR_index,\"virus_final_index.csv\",row.names=F)\n\n" }
{ "repo_name": "lawphill/ProjectEuler", "ref": "refs/heads/master", "path": "Problems_26_to_50/Euler032.R", "content": "# A number is pandigital if it is made up of the digits 1:n, where n is the total number\n# of digits in the number. For example, 15234 is a 5-digit pandigital number b/c  it contains\n# 1:5 and is 5 digits long. Each digit can only appear once.\n#\n# 7254 is a pandigital product in that 39 x 186 = 7254, where the number and two of its\n#   factors are together 1:9 pandigital (i.e. contain 1:9 each exactly once)\n#\n# Find the sum of all products (e.g. 7254) which are pandigital products of 1:9\n\n# Only logical possibilities for number of digits in each product and factors would be:\n# x * xxxx = xxxx\n# xx * xxx = xxxx\n\nno_repeats <- function(x,missing_digits){\n  # Returns BOOLEAN\n  # TRUE if x has no repeats and no digit is 0\n  # FALSE if x has repeat digits or any digits is 0\n  r <- rep(TRUE,length(x))\n  for(i in 1:length(x)){\n    dont_match <- missing_digits\n    while(x[i] > 0){\n      mod <- x[i] %% 10\n      if(mod == 0){\n        r[i] <- FALSE\n      }else if(sum(match(dont_match,mod),na.rm=TRUE) > 0){\n        r[i] <- FALSE\n      }\n      dont_match <- c(dont_match,mod)\n      x[i] <- x[i] %/% 10\n    }\n  }\n  return(r)\n}\n\ndigits <- function(x){\n  d <- c()\n  while(x > 0){\n    d <- c(d,x %% 10)\n    x <- x %/% 10\n  }\n  return(rev(d))\n}\n\nposs_products <- 1234:9876\nposs_products <- poss_products[no_repeats(poss_products,c())]\n\nposs_factors1 <- 2:98 # Factor can't be 1\nposs_factors1 <- poss_factors1[no_repeats(poss_factors1,c())]\n\ntotal <- 0\nfor(product in poss_products){\n  is_pandigital <- FALSE\n  used <- digits(product)\n  f1 <- poss_factors1[no_repeats(poss_factors1,used)] # Remove factors w/ duplicates\n  f1 <- f1[product %% f1 == 0] # Remove non-factors\n  if(length(f1) > 0){ # If we have any left\n    f2 <- product / f1\n      \n    for(i in 1:length(f1)){\n      dig_f1 <- digits(f1[i])\n      dig_f2 <- digits(f2[i])\n      # Check that no digits repeat, but that every digit is used\n      if(no_repeats(f2[i],c(used,dig_f1)) & length(c(used,dig_f1,dig_f2))==9){\n        is_pandigital <- TRUE\n      }\n    }\n  }\n  if(is_pandigital){\n    total <- total + product\n  }\n}\nprint(total)\n" }
{ "repo_name": "tidyverse/ggplot2", "ref": "refs/heads/master", "path": "tests/testthat/test-scale-date.R", "content": "context(\"scale_date\")\n\nbase_time <- function(tz = \"\") {\n  as.POSIXct(strptime(\"2015-06-01\", \"%Y-%m-%d\", tz = tz))\n}\n\ndf <- data_frame(\n  time1 = base_time(\"\") + 0:6 * 3600,\n  time2 = base_time(\"UTC\") + 0:6 * 3600,\n  time3 = base_time(\"Australia/Lord_Howe\") + (0:6 + 13) * 3600, # has half hour offset\n  y = seq_along(base_time)\n)\n\ntest_that(\"inherits timezone from data\", {\n  if (!is.null(attr(df$time1, \"tzone\")))\n     skip(\"Local time zone not available\")\n\n  # Local time\n  p <- ggplot(df, aes(y = y)) + geom_point(aes(time1))\n  sc <- layer_scales(p)$x\n\n  expect_true(identical(sc$timezone, NULL))\n  expect_equal(sc$get_labels()[1], \"00:00\")\n\n  # UTC\n  p <- ggplot(df, aes(y = y)) + geom_point(aes(time2))\n  sc <- layer_scales(p)$x\n  expect_equal(sc$timezone, \"UTC\")\n  expect_equal(sc$get_labels()[1], \"00:00\")\n})\n\n\ntest_that(\"first timezone wins\", {\n  p <- ggplot(df, aes(y = y)) +\n    geom_point(aes(time2)) +\n    geom_point(aes(time3), colour = \"red\") +\n    scale_x_datetime(date_breaks = \"hour\", date_labels = \"%H:%M\")\n  sc <- layer_scales(p)$x\n  expect_equal(sc$timezone, \"UTC\")\n})\n\ntest_that(\"not cached across calls\", {\n  scale_x <- scale_x_datetime(date_breaks = \"hour\", date_labels = \"%H:%M\")\n\n  p1 <- ggplot(df, aes(y = y)) + geom_point(aes(time2)) + scale_x\n  p2 <- ggplot(df, aes(y = y)) + geom_point(aes(time3)) + scale_x\n\n  expect_equal(layer_scales(p1)$x$timezone, \"UTC\")\n  expect_equal(layer_scales(p2)$x$timezone, \"Australia/Lord_Howe\")\n})\n\ntest_that(\"datetime size scales work\", {\n  p <- ggplot(df, aes(y = y)) + geom_point(aes(time1, size = time1))\n\n  # Default size range is c(1, 6)\n  expect_equal(range(layer_data(p)$size), c(1, 6))\n})\n\ntest_that(\"datetime alpha scales work\", {\n  p <- ggplot(df, aes(y = y)) + geom_point(aes(time1, alpha = time1))\n\n  # Default alpha range is c(0.1, 1.0)\n  expect_equal(range(layer_data(p)$alpha), c(0.1, 1.0))\n})\n\ntest_that(\"datetime colour scales work\", {\n  p <- ggplot(df, aes(y = y)) +\n    geom_point(aes(time1, colour = time1)) +\n    scale_colour_datetime()\n\n  expect_equal(range(layer_data(p)$colour), c(\"#132B43\", \"#56B1F7\"))\n})\n" }
{ "repo_name": "zross/who-heat", "ref": "refs/heads/master", "path": "WHO_HETK/ext/explore-inequality-panel.R", "content": "tabPanel(tags$h5(\"Explore Inequality\"), value='assess_inequality',\n         tabsetPanel(id=\"assessment_panel\",\n                     tabPanel(h6(\"Disaggregated data - tables\"), value='datatable', \n                              uiOutput('downloadDatatable'),\n                              dataTableOutput(outputId=\"dataTable\")\n                     ), \n                     tabPanel(h6(\"Disaggregated data - graphs\"), value='dataplot',\n                              uiOutput('downloadDataplot'),\n                              plotOutput('theDataPlot_web')), \n                     tabPanel(h6(\"Summary measures - tables\"), value='sumtable', \n                              uiOutput('downloadSummtable'),\n                              dataTableOutput(outputId=\"dataTableInequal\")\n                              ),              \n                     tabPanel(h6(\"Summary measures - graphs\"), value='sumplot',\n                              uiOutput('downloadSummplot'),\n                              plotOutput('theSumPlot_web')\n                              )\n                     )\n)" }
{ "repo_name": "ffivaz/grasp-R", "ref": "refs/heads/master", "path": "R/grasp.step.bic.R", "content": "\"grasp.step.bic\" <-\nfunction (object, scope, scale, direction = c(\"both\", \"backward\", \n    \"forward\"), trace = TRUE, keep = NULL, steps = 1000, ...) \n{\ncat(\"#\", \"\\n\")\ncat(\"# FUNCTION: grasp.step.bic\", \"\\n\")\ncat(\"# (by Splus, adapted by A. Lehmann from step.gam)\", \"\\n\")\ncat(\"# grasp.step.bic is a modified version of step.gam of Splus using BIC instead \",\n\"\\n\")\ncat(\"# of AIC criteria\", \"\\n\")\ncat(\"#\", \"\\n\")\n   \n scope.char <- function(formula) {\n        tt <- terms(formula)\n        tl <- attr(tt, \"term.labels\")\n        if (attr(tt, \"intercept\")) \n            c(\"1\", tl)\n        else tl\n    }\n    re.arrange <- function(keep) {\n        namr <- names(k1 <- keep[[1]])\n        namc <- names(keep)\n        nc <- length(keep)\n        nr <- length(k1)\n        array(unlist(keep, recursive = FALSE), c(nr, nc), list(namr, \n            namc))\n    }\n    untangle.scope <- function(terms, regimens) {\n        a <- attributes(terms)\n        response <- deparse(a$variables[[2]])\n        term.labels <- a$term.labels\n        if (!is.null(a$offset)) {\n            off1 <- deparse(a$variables[[a$offset]])\n        }\n        nt <- length(regimens)\n        select <- integer(nt)\n        for (i in seq(nt)) {\n            j <- match(regimens[[i]], term.labels, 0)\n            if (any(j)) {\n                if (sum(j > 0) > 1) \n                  stop(paste(\"The elements of a regimen\", i, \n                    \"appear more than once in the initial model\", \n                    sep = \" \"))\n                select[i] <- seq(j)[j > 0]\n                term.labels <- term.labels[-sum(j)]\n            }\n            else {\n                if (!(j <- match(\"1\", regimens[[i]], 0))) \n                  stop(paste(\"regimen\", i, \"does not appear in the initial model\", \n                    sep = \" \"))\n                select[i] <- j\n            }\n        }\n        if (length(term.labels)) \n            term.labels <- paste(term.labels, \"+\")\n        if (!is.null(a$offset)) \n            term.labels <- paste(off1, term.labels, sep = \" + \")\n        return(list(response = paste(response, term.labels, sep = \" ~ \"), \n            select = select))\n    }\n    make.step <- function(models, fit, scale, object) {\n        chfrom <- sapply(models, \"[[\", \"from\")\n        chfrom[chfrom == \"1\"] <- \"\"\n        chto <- sapply(models, \"[[\", \"to\")\n        chto[chto == \"1\"] <- \"\"\n        dev <- sapply(models, \"[[\", \"deviance\")\n        df <- sapply(models, \"[[\", \"df.resid\")\n        ddev <- c(NA, diff(dev))\n        ddf <- c(NA, diff(df))\n        BIC <- sapply(models, \"[[\", \"BIC\")\n        heading <- c(\"Stepwise Model Path \\nAnalysis of Deviance Table\", \n            \"\\nInitial Model:\", deparse(as.vector(formula(object))), \n            \"\\nFinal Model:\", deparse(as.vector(formula(fit))), \n            paste(\"\\nScale: \", format(scale), \"\\n\", sep = \"\"))\n        aod <- data.frame(From = chfrom, To = chto, Df = ddf, \n            Deviance = ddev, \"Resid. Df\" = df, \"Resid. Dev\" = dev, \n            BIC = BIC, check.names = FALSE)\n        fit$anova <- as.anova(aod, heading)\n        fit\n    }\n    direction <- match.arg(direction)\n    if (missing(scope)) \n        stop(\"you must supply a scope argument to step.gam(); the gam.scope() function might be useful\")\n    if (!is.character(scope[[1]])) \n        scope <- lapply(scope, scope.char)\n    response <- untangle.scope(object$terms, scope)\n    form.y <- response$response\n    backward <- direction == \"both\" | direction == \"backward\"\n    forward <- direction == \"both\" | direction == \"forward\"\n    items <- response$select\n    family <- family(object)\n    Call <- object$call\n    term.lengths <- sapply(scope, length)\n    n.items <- length(items)\n    visited <- array(FALSE, term.lengths)\n    visited[array(items, c(1, n.items))] <- TRUE\n    if (!is.null(keep)) {\n        keep.list <- vector(\"list\", length(visited))\n        nv <- 1\n    }\n    models <- vector(\"list\", length(visited))\n    nm <- 2\n    form.vector <- character(n.items)\n    for (i in seq(n.items)) form.vector[i] <- scope[[i]][items[i]]\n    form <- deparse(object$formula)\n    if (trace) \n        cat(\"Start: \", form)\n    fit <- object\n    n <- length(fit$fitted)\n    if (missing(scale)) {\n        famname <- family$family[\"name\"]\n        scale <- switch(famname, Poisson = 1, Binomial = 1, deviance.lm(fit)/fit$df.resid)\n    }\n    else if (scale == 0) \n      scale <- deviance.lm(fit)/fit$df.resid\n      ##bBIC <- fit$BIC\nscale <- summary(fit)$dispersion\nbBIC <- deviance(fit) + scale * (n - fit$df.resid) * log(n)\nprint(paste(\"BIC: \", bBIC))\n    if (trace) \n        cat(\"; BIC=\", format(round(bBIC, 4)), \"\\n\")\n    models[[1]] <- list(deviance = deviance(fit), df.resid = fit$df.resid, \n        BIC = bBIC, from = \"\", to = \"\")\n    if (!is.null(keep)) {\n        keep.list[[nv]] <- keep(fit, bBIC)\n        nv <- nv + 1\n    }\n    BIC <- bBIC + 1\n    while (bBIC < BIC & steps > 0) {\n        steps <- steps - 1\n        BIC <- bBIC\n        bitems <- items\n        bfit <- fit\n        for (i in seq(n.items)) {\n            if (backward) {\n                trial <- items\n                trial[i] <- trial[i] - 1\n                if (trial[i] > 0 && !visited[array(trial, c(1, \n                  n.items))]) {\n                  visited[array(trial, c(1, n.items))] <- TRUE\n                  tform.vector <- form.vector\n                  tform.vector[i] <- scope[[i]][trial[i]]\n                  form <- paste(form.y, paste(tform.vector, collapse = \" + \"))\n                  if (trace) \n                    cat(\"Trial: \", form)\n                  tfit <- update(object, eval(parse(text = form)), \n                    trace = FALSE, ...)\n                  # tBIC <- tfit$BIC\ntBIC<-deviance(tfit) + scale * (n - tfit$df.resid) * log(n)\n\n                  if (!is.null(keep)) {\n                    keep.list[[nv]] <- keep(tfit, tBIC)\n                    nv <- nv + 1\n                  }\n                  if (tBIC < bBIC) {\n                    bBIC <- tBIC\n                    bitems <- trial\n                    bfit <- tfit\n                    bform.vector <- tform.vector\n                    bfrom <- form.vector[i]\n                    bto <- tform.vector[i]\n                  }\n                  if (trace) \n                    cat(\"; BIC=\", format(round(tBIC, 4)), \"\\n\")\n                }\n            }\n            if (forward) {\n                trial <- items\n                trial[i] <- trial[i] + 1\n                if (trial[i] <= term.lengths[i] && !visited[array(trial, \n                  c(1, n.items))]) {\n                  visited[array(trial, c(1, n.items))] <- TRUE\n                  tform.vector <- form.vector\n                  tform.vector[i] <- scope[[i]][trial[i]]\n                  form <- paste(form.y, paste(tform.vector, collapse = \" + \"))\n                  if (trace) \n                    cat(\"Trial: \", form)\n                  tfit <- update(object, eval(parse(text = form)), \n                    trace = FALSE, ...)\n                  # tBIC <- tfit$BIC\ntBIC<-deviance(tfit) + scale * (n - tfit$df.resid) * log(n)\n\n                  if (!is.null(keep)) {\n                    keep.list[[nv]] <- keep(tfit, tBIC)\n                    nv <- nv + 1\n                  }\n                  if (tBIC < bBIC) {\n                    bBIC <- tBIC\n                    bitems <- trial\n                    bfit <- tfit\n                    bform.vector <- tform.vector\n                    bfrom <- form.vector[i]\n                    bto <- tform.vector[i]\n                  }\n                  if (trace) \n                    cat(\"; BIC=\", format(round(tBIC, 4)), \"\\n\")\n                }\n            }\n        }\n        if (bBIC >= BIC | steps == 0) {\n            if (!is.null(keep)) \n                fit$keep <- re.arrange(keep.list[seq(nv - 1)])\n            return(make.step(models[seq(nm - 1)], fit, scale, \n                object))\n        }\n        else {\n            if (trace) \n                cat(\"Step : \", deparse(bfit$formula), \"; BIC=\", \n                  format(round(bBIC, 4)), \"\\n\\n\")\n            items <- bitems\n            models[[nm]] <- list(deviance = deviance(bfit), df.resid = bfit$df.resid, \n                BIC = bBIC, from = bfrom, to = bto)\n            nm <- nm + 1\n            fit <- bfit\n            form.vector <- bform.vector\n        }\n    }\n}\n\n" }
{ "repo_name": "jelliot/TradingWithR", "ref": "refs/heads/master", "path": "backtester/profiling.R", "content": "profvis({\nfor (i in 1:length(my.data)) {\n  my.date = as.Date(names(my.data)[i])\n  print(my.date)\n}\n})\n\nprofvis({\n  my.date = as.Date(names(my.data))\n  print(my.date)\n})\n\nprofvis({\n  print(names(my.data))\n})\n\n\nprofvis({\n  for (i in 1:1000) {\n  if (nrow(RUT[my.date]) == 0)\n    stop(paste(my.date, \"not a trading day\"))\n  if (is.null(my.cal) || !exists(\"my.cal\")) # should check dates too\n    stop(\"global calendar (my.cal) does not exist or is NULL\")\n  FloatingProfit(my.df) >= 0.89 * (-1 * InitialCredit(my.df))\n  FloatingProfit(my.df) <= 2 * InitialCredit(my.df) # negative val\n  bizdays(date, as.Date(my.df[1,]$my.exp.date), my.cal) <= 5 \n  as.numeric(Hi(underlying[my.date])) >= ShortCall(my.df)\n  as.numeric(Lo(underlying[my.date])) <= ShortPut(my.df)\n  }\n})\n\nmy.date=\"2010-02-01\"\nmicrobenchmark(FloatingProfit(my.df), \n               InitialCredit(my.df), \n               bizdays(as.Date(my.date), as.Date(my.df[1,24]), my.cal), \n               ShortCall(my.df), \n               ShortPut(my.df), \n               as.numeric(highs[my.date]),\n               as.numeric(lows[my.date]),\n               times=1000)\n\nmicrobenchmark(as.numeric(Hi(underlying[my.date])),\n               as.numeric(Lo(underlying[my.date])),\n               as.numeric(highs[my.date]),\n               as.numeric(lows[my.date]),\n               times=1000)\n\nmicrobenchmark(ShortCall(my.df),\n               ShortPut(my.df),\n               times=1000)\n\n# return the strike price of the short call in a single condor\nNewShortCall = function(trades) {\n  strike = trades[trades$Call.Put == \"C\",][1,6]\n}\n\n# return the strike price of the short put in a single condor\nNewShortPut = function(trades) {\n  strike = trades[trades$Call.Put == \"P\",][2,6]\n}\n\n# return the strike price of the short call in a single condor\nSecNewShortCall = function(trades) {\n  strike = min(trades[trades$Call.Put == \"C\",6])\n}\n\n# return the strike price of the short put in a single condor\nSecNewShortPut = function(trades) {\n  strike = max(trades[trades$Call.Put == \"P\",6])\n}\n\nTriNewShortCall = function(trades) {\n  strike = min(trades[trades[,7] == \"C\",6])\n}\n\n# return the strike price of the short put in a single condor\nTriNewShortPut = function(trades) {\n  strike = max(trades[trades[,7] == \"P\",6])\n}\n\nmicrobenchmark(ShortCall(my.df),\n               ShortPut(my.df),\n               NewShortCall(my.df),\n               NewShortPut(my.df),\n               SecNewShortCall(my.df),\n               SecNewShortPut(my.df),\n               TriNewShortCall(my.df),\n               TriNewShortPut(my.df),\n               times=1000)\n\nmy.truefalse = list()\nfor (i in 89:length(my.data)) {\n  if (!is.null(FindCondor(my.data[[i]])) &&\n      nrow(FindCondor(my.data[[i]])) == 4) {\n    #browser()\n    my.condor = FindCondor(my.data[[i]])\n    my.truefalse[[i]] = SecNewShortCall(my.condor) == ShortCall(my.condor)\n  }\n}\n\nif (all(unlist(my.truefalse)) == TRUE)\n  print(\"PASS for valid iron condors\")\n\n#profile FindCondor\nprofvis({\n  is.list = FALSE\n  my.df = my.data[[1]]\nfor (i in 1:1000) {\n  if (is.list) {\n    my.df = my.df[[1]]\n  }\n  # clean up to only include traditional monthlies\n  # (should be checking this in data export as well)\n  my.df = my.df[!grepl(\"D\\\\d{1,2}$\", my.df$Symbol, perl = TRUE),]\n  # clean up to only include possible candidates\n  my.df = subset(my.df, cal.dte > 49 & cal.dte < 76)\n  # if you have no expirations (a few days like this), return NULL\n  if (nrow(my.df) == 0) {\n  } else {\n    # if you have two expirations, pick the minimum for now\n    my.df = subset(my.df, cal.dte == min(cal.dte))\n    my.df[PickByDelta(my.df$Delta,   8),]$Existing.Posn. =  1\n    my.df[PickByDelta(my.df$Delta,  11),]$Existing.Posn. = -1\n    my.df[PickByDelta(my.df$Delta,  -8),]$Existing.Posn. =  1\n    my.df[PickByDelta(my.df$Delta, -11),]$Existing.Posn. = -1\n    # Existing.Posn is NA by default, so this works but is not intuitive:\n    my.open.trades = my.df[!is.na(my.df$Existing.Posn.),]\n    # add new $orig.price column for later\n    my.open.trades$orig.price = my.open.trades$mid.price\n  }\n}\n})\n\nopen.trades = list(FindCondor(my.data[[100]]))\nmicrobenchmark(FindCondor(my.data[[100]]),\n               ShouldEnter(open.trades, my.data[[120]]),\n               times=1000)\n\n#profile NewFindCondor\nprofvis({\n  is.list = FALSE\n  my.df = my.data[[1]]\n  for (i in 1:1000) {\n    if (is.list) {\n      my.df = my.df[[1]]\n    }\n    # clean up to only include traditional monthlies\n    # (should be checking this in data export as well)\n    my.df = my.df[!grepl(\"D\\\\d{1,2}$\", my.df$Symbol, perl = TRUE),]\n    # clean up to only include possible candidates\n    my.df = subset(my.df, cal.dte > 49 & cal.dte < 76)\n    # if you have no expirations (a few days like this), return NULL\n    if (nrow(my.df) == 0) {\n    } else {\n      # if you have two expirations, pick the minimum for now\n      my.df = subset(my.df, cal.dte == min(cal.dte))\n      my.df[PickByDelta(my.df[,16],   8),1] =  1\n      my.df[PickByDelta(my.df[,16],  11),1] = -1\n      my.df[PickByDelta(my.df[,16],  -8),1] =  1\n      my.df[PickByDelta(my.df[,16], -11),1] = -1\n      # Existing.Posn is NA by default, so this works but is not intuitive:\n      my.open.trades = my.df[!is.na(my.df[,1]),]\n      # add new $orig.price column for later\n      my.open.trades[,28] = my.open.trades[,27]\n    }\n  }\n})\n\nNewFindCondor = function(my.df, is.list = FALSE) {\n  if (is.list) {\n    my.df = my.df[[1]]\n  }\n  # clean up to only include traditional monthlies\n  # (should be checking this in data export as well)\n  my.df = my.df[!grepl(\"D\\\\d{1,2}$\", my.df$Symbol, perl = TRUE),]\n  # clean up to only include possible candidates\n  my.df = subset(my.df, cal.dte > 49 & cal.dte < 76)\n  # if you have no expirations (a few days like this), return NULL\n  if (nrow(my.df) == 0) {\n  } else {\n    # if you have two expirations, pick the minimum for now\n    my.df = subset(my.df, cal.dte == min(cal.dte))\n    my.df[PickByDelta(my.df[,16],   8),1] =  1\n    my.df[PickByDelta(my.df[,16],  11),1] = -1\n    my.df[PickByDelta(my.df[,16],  -8),1] =  1\n    my.df[PickByDelta(my.df[,16], -11),1] = -1\n    # Existing.Posn is NA by default, so this works but is not intuitive:\n    my.open.trades = my.df[!is.na(my.df[,1]),]\n    # add new $orig.price column for later\n    my.open.trades[,28] = my.open.trades[,27]\n    return(my.open.trades)\n  }\n}\n\nFindCondor = function(my.df, is.list = FALSE) {\n  if (is.list) {\n    my.df = my.df[[1]]\n  }\n  # clean up to only include traditional monthlies\n  # (should be checking this in data export as well)\n  my.df = my.df[!grepl(\"D\\\\d{1,2}$\", my.df$Symbol, perl = TRUE),]\n  # clean up to only include possible candidates\n  my.df = subset(my.df, cal.dte > 49 & cal.dte < 76)\n  # if you have no expirations (a few days like this), return NULL\n  if (nrow(my.df) == 0) {\n    return(NULL)\n  } else {\n    # if you have two expirations, pick the minimum for now\n    my.df = subset(my.df, cal.dte == min(cal.dte))\n    my.df[PickByDelta(my.df$Delta,   8),]$Existing.Posn. =  1\n    my.df[PickByDelta(my.df$Delta,  11),]$Existing.Posn. = -1\n    my.df[PickByDelta(my.df$Delta,  -8),]$Existing.Posn. =  1\n    my.df[PickByDelta(my.df$Delta, -11),]$Existing.Posn. = -1\n    # Existing.Posn is NA by default, so this works but is not intuitive:\n    my.open.trades = my.df[!is.na(my.df$Existing.Posn.),]\n    # add new $orig.price column for later\n    my.open.trades$orig.price = my.open.trades$mid.price\n    return(my.open.trades)\n  }\n}\n\nfoo.data = my.data[[1]]\nmicrobenchmark(FindCondor(foo.data),\n               NewFindCondor(foo.data),\n               times=1000)\n\nmicrobenchmark(my.data[[i]][match(open.trades[[j]]$Symbol, my.data[[i]]$Symbol),],\n               my.data[[i]][my.data[[i]]$Symbol %in% open.trades[[j]]$Symbol,],\n               times=1000)\n" }
{ "repo_name": "wilsontom/sRm", "ref": "refs/heads/master", "path": "R/pracma_peak_picking.R", "content": "#' Pracma Peak-Picking\n#'\n#' A wrapper around `pracma::findPeaks` for peak detection and integration\n#'\n#' @param rt a numeric vector of retention time\n#' @param int a numeric vector of intensity\n#'\n#' @return a `tibble` of peak infomation (rt, rtmin, rtmax, int, area, peakId)\n#' @export\n\npracma_peak_picking <- function(rt, int)\n{\n  noise <- estimate_noise(int)\n\n  peak_indicies <- pracma::findpeaks(int, threshold = noise)\n\n  if (is.null(peak_indicies)) {\n    peak_indicies <- pracma::findpeaks(int, threshold = 0)\n  }\n\n\n  peak_indicies <- data.frame(peak_indicies)\n  names(peak_indicies) <- c('height', 'apex', 'left', 'right')\n\n\n  peak_id <- seq(from = 1, to = nrow(peak_indicies))\n\n  rtv <- rt[peak_indicies$apex]\n  rtmin <- rt[peak_indicies$left]\n  rtmax <- rt[peak_indicies$right]\n  intv <- int[peak_indicies$apex]\n\n\n  pkranges <- list()\n  for (i in seq_along(peak_indicies$left)) {\n    pkranges[[i]] <- peak_indicies$left[i]:peak_indicies$right[i]\n  }\n\n  peak_area <- list()\n  for (i in seq_along(pkranges)) {\n    peak_area[[i]] <-\n      pracma::polyarea(int[pkranges[[i]]], rt[pkranges[[i]]])\n  }\n\n\n  pkarea <- unlist(peak_area)\n\n\n  peak_info_tibble <-\n    tibble::tibble(\n      rt = rtv,\n      rtmin = rtmin,\n      rtmax = rtmax,\n      int = intv,\n      area = pkarea,\n      peakId = peak_id\n    )\n\n\n  return(peak_info_tibble)\n\n\n}\n" }
{ "repo_name": "iccat-mse/tuna-mse", "ref": "refs/heads/master", "path": "R/backUp/cf.R", "content": "library(FLBRP)\nlibrary(FLash)\nlibrary(biodyn)\n\ndirDat='~/Desktop/MEGA/papers/submitted/tuna-mse/data'\ndirRes='~/Desktop/MEGA/papers/submitted/tuna-mse/resubmission/data'\n\nload(paste(dirRes,\"/mou.RData\",    sep=\"\"))\nload(paste(dirDat,\"/priors.RData\", sep=\"\"))\nload(paste(dirRes,\"/options.RData\",sep=\"\"))\n\nstart=50; end=100; interval =3\nnits =100 \nseed =7890\n\nset.seed(seed)\n\nplot(mou[[\"21\"]])\nmp =as(mou[[\"21\"]],\"biodyn\")\nmp@control[]\n\n#### MP\n  ## SA Options\n  ctrl=with(priors[iOM,c(\"r\",\"k\",\"p\",\"b0\")],biodyn:::controlFn(r=r,k=k,p=p,b0=b0))\n  if (options$SA[ iSA, \"p\"]==1) ctrl[\"p\",\"val\"]=1\n    \n  val=priors[options$OM[iOM,\"i\"],prr]\n  prArg=list(c(weight=1,a=val,b=val*0.5))\n  names(prArg)=prr\n  prrs=priorFn(prArg)\n  \n  #### MSE\n  res=mseBiodyn(om,br,srDev,\n                control=ctrl,priors=prrs,\n                start=start+rcvPeriod,end=end,interval=3,\n                ftar=ftar,blim=blim,btrig=btrig,\n                uDev   =0.3,trendQ=trendQ,\n                omega =omega,\n                refB  =FLBRP:::refpts(br)[\"msy\",\"biomass\"])\n\n  #### Save results\n  if (is.null(db)) return(res)\n  \n  drv=dbDriver(\"SQLite\")\n  con=dbConnect(drv, dbname=db)\n  \n  mou=cbind(OM=iOM,SA=iSA,HCR=iHCR,tseries(res$mou,br))\n  om =cbind(OM=iOM,SA=iSA,HCR=iHCR,tseries(res$om,br))\n  mp =cbind(OM=iOM,SA=iSA,HCR=iHCR,res$mp)\n  #oem=cbind(OM=iOM,SA=iSA,HCR=iHCR,model.frame(res$oem,drop=T))\n  \n  dbWriteTable(con, \"mou\", mou,append=TRUE)\n  dbWriteTable(con, \"om\",  om, append=TRUE)\n  dbWriteTable(con, \"mp\",  mp, append=TRUE)\n  #dbWriteTable(con, \"oem\", oem,append=TRUE)\n  \n  dbDisconnect(con)}\n\ntrendQ=1+FLQuant(cumsum(rep(0.01,end)),\n                  dimnames=list(year=1:end))\n\nfor (i in 21){\n  res=m_ply(expand.grid(iOM=i,iSA =c(1:2,6:7)[4],\n                              iHCR=c(1,2,7,8)[3]), \n      function(iOM,iSA,iHCR,options,om,br,priors,db) {\n           cat(paste(\"  OM\\t\",iOM,\":\\tSA\\t\",iSA,\":\\tHCR\\t\",iHCR,\n           \"\\n  ======================================\",sep=\"\"))\n        dataPoorMSE(i,iSA,iHCR,options,om,br,priors,\n                    db=NULL,\n                    trendQ=trendQ,\n                    omega=1)},\n            options=options,\n            om     =OMs[[      options$OM[i,\"i\" ]]],\n            br     =BRPs[[     options$OM[i,\"i\" ]]],\n            priors=priors,\n            db=paste(dirDat,\"trendQ\",sep=\"/\"))}\n\ndb=\"/home/laurie/Desktop/MEGA/papers/submitted/tuna-mse/data/breakIt6\"\ndb =paste(dirDat,\"testOem\",sep=\"/\")\ndrv=dbDriver(\"SQLite\")\ncon=dbConnect(drv, dbname=db)\noem =dbGetQuery(con,\"select * from oem\")\n\n\noem2(om,cv,trendQ,omega,refB=FLBRP:::refpts(br)[\"msy\",\"biomass\"],\n         fishDepend=TRUE)\n\n" }
{ "repo_name": "corybrunson/cloud", "ref": "refs/heads/master", "path": "R/huyghen_general_test.R", "content": "#' Test general Huyghen's theorem\n#' \n#' This function calculates both sides of the equation of Huyghen's theorem.\n#' @param cloud An m-by-n matrix of coordinates for m points in n-dimensional\n#'   space.\n#' @param weights An m-element vector of point masses (weights).\n#' @param subspace An k-by-n matrix of coordinates spanning the k-dimensional\n#'   affine subspace.\n#' @param ... Additional arguments (eventually) passed to \\code{\\link{projection}}.\n#' @export\n\nhuyghen_general_test <-\n    function(cloud, weights, subspace, ...) {\n        \n        # If weights is missing...\n        if(missing(weights)) weights <- rep(1, nrow(cloud))\n        # If weights is a scalar...\n        if(length(weights) == 1) weights <- rep(weights, nrow(cloud))\n        \n        # Left side\n        lhs <- residual_square_mean(cloud = cloud, weights = weights,\n                                    subspace = subspace, ...)\n        \n        # Calculate the barycenter of cloud\n        bc <- barycenter(cloud)\n        \n        # Decompose subspace\n        decomp <- affine_decomposition(subspace)\n        \n        # Describe the affine subspace through bc\n        parallel_subspace <- affine_recomposition(bc, decomp$linear_subspace)\n        \n        # Right side\n        rhs <- residual_square_mean(cloud = cloud, weights = weights,\n                                    subspace = parallel_subspace,\n                                    type = \"affine\") +\n            distance(bc, projection(bc, subspace, ...)) ^ 2\n        \n        # Equal?\n        print(all.equal(lhs, rhs))\n        \n        list(lhs = lhs, rhs = rhs)\n    }\n" }
{ "repo_name": "CenterForAssessment/Utah", "ref": "refs/heads/master", "path": "SGP_CONFIG/EOCT/2016/SCIENCE.R", "content": "#############################################################\r\n##  EOCT CONFIGURATION FILE - SECONDARY SCIENCE\r\n#############################################################\r\n\r\nEARTH_SCIENCE_2016.config <- list(\r\n  EARTH_SCIENCE.2016 = list(\r\n    sgp.content.areas=c(rep('SCIENCE', 5), 'EARTH_SCIENCE'),\r\n    sgp.panel.years=as.character(2011:2016),\r\n    sgp.grade.sequences=list(c(4:8, 'EOCT')),\r\n    sgp.projection.sequence = \"EARTH_SCIENCE\", # Not really needed for canonical progressions, but include anyway to be explicit\r\n    sgp.norm.group.preference=1)\r\n)\r\n\r\n###########\r\n# SAGE ONLY\r\n# EARTH_SCIENCE_2016.config <- list(\r\n#   EARTH_SCIENCE.2016 = list(\r\n#     sgp.content.areas=c(rep('SCIENCE', 2), 'EARTH_SCIENCE'),\r\n#     sgp.panel.years=as.character(2014:2016),\r\n#     sgp.grade.sequences=list(c(7:8, 'EOCT')),\r\n#     sgp.projection.grade.sequences=\"NO_PROJECTIONS\",\r\n#     sgp.norm.group.preference=1),\r\n# )\r\n\r\n#############################################################\t\r\nBIOLOGY_2016.config <- list(\r\n  # REPEATER\r\n  BIOLOGY.2016 = list(\r\n    sgp.content.areas=c('BIOLOGY', 'BIOLOGY'), \r\n    sgp.panel.years=as.character(2015:2016),\r\n    sgp.grade.sequences=list(c('EOCT', 'EOCT')),\r\n    sgp.projection.grade.sequences=\"NO_PROJECTIONS\",\r\n    sgp.norm.group.preference=0),  \r\n  \r\n  # VIA EARTH SCIENCE\r\n  BIOLOGY.2016 = list(\r\n    sgp.content.areas=c(rep('SCIENCE', 4), 'EARTH_SCIENCE', 'BIOLOGY'),\r\n    sgp.panel.years=as.character(2011:2016),\r\n    sgp.grade.sequences=list(c(5:8, 'EOCT', 'EOCT')),\r\n    sgp.projection.sequence = \"BIOLOGY\",\r\n    sgp.norm.group.preference=1),  \r\n  \r\n  # VIA GRADE 8\r\n  BIOLOGY.2016 = list(\r\n    sgp.content.areas=c(rep('SCIENCE', 5), 'BIOLOGY'),\r\n    sgp.panel.years=as.character(2011:2016),\r\n    sgp.grade.sequences=list(c(4:8, 'EOCT')),\r\n    sgp.projection.grade.sequences=\"NO_PROJECTIONS\",\r\n    sgp.norm.group.preference=2),\r\n  \r\n  # VIA CHEMISTRY\r\n  BIOLOGY.2016 = list(\r\n    sgp.content.areas=c('CHEMISTRY', 'BIOLOGY'),\r\n    sgp.panel.years=as.character(2015:2016),\r\n    sgp.grade.sequences=list(c('EOCT', 'EOCT')),\r\n      sgp.projection.sequence = \"CHEM_BIO\",\r\n    sgp.norm.group.preference=3),\t\r\n  \r\n  # VIA PHYSICS\r\n  BIOLOGY.2016 = list(\r\n    sgp.content.areas=c('PHYSICS', 'BIOLOGY'),\r\n    sgp.panel.years=as.character(2015:2016),\r\n    sgp.grade.sequences=list(c('EOCT', 'EOCT')),\r\n      sgp.projection.sequence = \"PHYS_BIO\",\r\n    sgp.norm.group.preference=4)\r\n)\r\n\r\n###########\r\n# SAGE ONLY\r\n# BIOLOGY_2016.config <- list(\r\n#   # REPEATER\r\n#   BIOLOGY.2016 = list(\r\n#     sgp.content.areas=c('BIOLOGY', 'BIOLOGY'), \r\n#     sgp.panel.years=as.character(2015:2016),\r\n#     sgp.grade.sequences=list(c('EOCT', 'EOCT')),\r\n#     sgp.projection.grade.sequences=\"NO_PROJECTIONS\",\r\n#     sgp.norm.group.preference=0),  \r\n#   \r\n#   # VIA EARTH SCIENCE\r\n#   BIOLOGY.2016 = list(\r\n#   \tsgp.content.areas=c('SCIENCE', 'EARTH_SCIENCE', 'BIOLOGY'),\r\n#   \tsgp.panel.years=as.character(2014:2016),\r\n#   \tsgp.grade.sequences=list(c(8, 'EOCT', 'EOCT')),\r\n#   \tsgp.projection.grade.sequences=\"NO_PROJECTIONS\",\r\n#   \tsgp.norm.group.preference=1),\r\n# \r\n#   # SKIP EARTH SCIENCE\r\n#   BIOLOGY.2016 = list(\r\n#     sgp.content.areas=c(rep('SCIENCE', 2), 'BIOLOGY'),\r\n#     sgp.panel.years=as.character(2014:2016),\r\n#     sgp.grade.sequences=list(c(7:8, 'EOCT')),\r\n#     sgp.projection.grade.sequences=\"NO_PROJECTIONS\",\r\n#     sgp.norm.group.preference=2),\r\n#   \r\n#   # VIA CHEMISTRY\r\n#   BIOLOGY.2016 = list(\r\n#     sgp.content.areas=c('CHEMISTRY', 'BIOLOGY'),\r\n#     sgp.panel.years=as.character(2015:2016),\r\n#     sgp.grade.sequences=list(c('EOCT', 'EOCT')),\r\n#     sgp.projection.grade.sequences=\"NO_PROJECTIONS\",\r\n#     sgp.norm.group.preference=3),\t\r\n#   \r\n#   # VIA PHYSICS\r\n#   BIOLOGY.2016 = list(\r\n#     sgp.content.areas=c('PHYSICS', 'BIOLOGY'),\r\n#     sgp.panel.years=as.character(2015:2016),\r\n#     sgp.grade.sequences=list(c('EOCT', 'EOCT')),\r\n#     sgp.projection.grade.sequences=\"NO_PROJECTIONS\",\r\n#     sgp.norm.group.preference=4)\r\n# )\r\n\r\n#############################################################\r\n\r\nCHEMISTRY_2016.config <- list(\r\n  # VIA EARTH SCIENCE, BIOLOGY\t\r\n  CHEMISTRY.2016 = list(\r\n    sgp.content.areas=c(rep('SCIENCE', 3), 'EARTH_SCIENCE', 'BIOLOGY', 'CHEMISTRY'),\r\n    sgp.panel.years=as.character(2011:2016),\r\n    sgp.grade.sequences=list(c(6:8, 'EOCT', 'EOCT', 'EOCT')),\r\n    sgp.projection.sequence = \"CHEMISTRY\",\r\n    sgp.norm.group.preference=1),  \r\n  \r\n  # VIA GRADE 8, BIOLOGY\t\r\n  CHEMISTRY.2016 = list(\r\n    sgp.content.areas=c(rep('SCIENCE', 4), 'BIOLOGY', 'CHEMISTRY'),\r\n    sgp.panel.years=as.character(2011:2016),\r\n    sgp.grade.sequences=list(c(5:8, 'EOCT', 'EOCT')),\r\n    sgp.projection.grade.sequences=\"NO_PROJECTIONS\", # No need here since config above contains 'BIOLOGY', 'CHEMISTRY' progression\r\n    sgp.norm.group.preference=2),\r\n\r\n  # VIA PHYSICS\r\n  CHEMISTRY.2016 = list(\r\n    sgp.content.areas=c('PHYSICS', 'CHEMISTRY'),\r\n    sgp.panel.years=as.character(2015:2016),\r\n    sgp.grade.sequences=list(c('EOCT', 'EOCT')),\r\n    sgp.projection.sequence = \"PHYS_CHEM\",\r\n    sgp.norm.group.preference=3)\r\n)\r\n\r\n###########\r\n# SAGE ONLY\r\n# CHEMISTRY_2016.config <- list(\r\n#   # VIA EARTH SCIENCE, BIOLOGY\t\r\n#   CHEMISTRY.2016 = list(\r\n#     sgp.content.areas=c('EARTH_SCIENCE', 'BIOLOGY', 'CHEMISTRY'),\r\n#     sgp.panel.years=as.character(2014:2016),\r\n#     sgp.grade.sequences=list(c('EOCT', 'EOCT', 'EOCT')),\r\n#     sgp.projection.grade.sequences=\"NO_PROJECTIONS\",\r\n#     sgp.norm.group.preference=1),  \r\n#     \r\n#   # VIA PHYSICS\r\n#   CHEMISTRY.2016 = list(\r\n#     sgp.content.areas=c('PHYSICS', 'CHEMISTRY'),\r\n#     sgp.panel.years=as.character(2015:2016),\r\n#     sgp.grade.sequences=list(c('EOCT', 'EOCT')),\r\n#     sgp.projection.grade.sequences=\"NO_PROJECTIONS\",\r\n#     sgp.norm.group.preference=3)\r\n# )\r\n\r\n#############################################################\r\n\r\nPHYSICS_2016.config <- list(\r\n  # VIA CHEMISTRY\r\n  PHYSICS.2016 = list(\r\n    sgp.content.areas=c(rep('SCIENCE', 2), 'EARTH_SCIENCE', 'BIOLOGY', 'CHEMISTRY', 'PHYSICS'),\r\n    sgp.panel.years=as.character(2011:2016),\r\n    sgp.grade.sequences=list(c(7:8, 'EOCT', 'EOCT', 'EOCT', 'EOCT')),\r\n    sgp.projection.sequence = \"PHYSICS\",\r\n    sgp.norm.group.preference=1),  # Made #2 to produce same data originally provided to USOE.  Next Year make CHEMISTRY 1\r\n  \r\n  PHYSICS.2016 = list(\r\n    sgp.content.areas=c(rep('SCIENCE', 3), 'BIOLOGY', 'CHEMISTRY', 'PHYSICS'),\r\n    sgp.panel.years=as.character(2011:2016),\r\n    sgp.grade.sequences=list(c(6:8, 'EOCT', 'EOCT', 'EOCT')),\r\n    sgp.projection.grade.sequences=\"NO_PROJECTIONS\", # No need here since config above contains 'CHEMISTRY', 'PHYSICS'\r\n    sgp.norm.group.preference=1),\r\n  \r\n  # VIA BIOLOGY\r\n  PHYSICS.2016 = list(\r\n    sgp.content.areas=c(rep('SCIENCE',3), 'EARTH_SCIENCE', 'BIOLOGY', 'PHYSICS'),\r\n    sgp.panel.years=as.character(2011:2016),\r\n    sgp.grade.sequences=list(c(6:8,'EOCT','EOCT','EOCT')),\r\n    sgp.projection.sequence = \"BIO_PHYS\",\r\n    sgp.norm.group.preference=3), # Next Year make CHEMISTRY 1 and BIO 2\r\n  \r\n  PHYSICS.2016 = list(\r\n    sgp.content.areas=c(rep('SCIENCE',4), 'BIOLOGY', 'PHYSICS'),\r\n    sgp.panel.years=as.character(2011:2016),\r\n    sgp.grade.sequences=list(c(5:8,'EOCT','EOCT')),\r\n    sgp.projection.grade.sequences=\"NO_PROJECTIONS\",\r\n    sgp.norm.group.preference=3)\r\n)\r\n\r\n###########\r\n# SAGE ONLY\r\n# PHYSICS_2016.config <- list(\r\n#   # VIA CHEMISTRY\r\n#   PHYSICS.2016 = list(\r\n#     sgp.content.areas=c('BIOLOGY', 'CHEMISTRY', 'PHYSICS'),\r\n#     sgp.panel.years=as.character(2014:2016),\r\n#     sgp.grade.sequences=list(c('EOCT', 'EOCT', 'EOCT')),\r\n#     sgp.projection.grade.sequences=\"NO_PROJECTIONS\",\r\n#     sgp.norm.group.preference=1),\r\n\r\n#   \r\n#   # VIA BIOLOGY\r\n#   PHYSICS.2016 = list(\r\n#     sgp.content.areas=c('EARTH_SCIENCE', 'BIOLOGY', 'PHYSICS'),\r\n#     sgp.panel.years=as.character(2014:2016),\r\n#     sgp.grade.sequences=list(c('EOCT','EOCT','EOCT')),\r\n#     sgp.projection.grade.sequences=\"NO_PROJECTIONS\",\r\n#     sgp.norm.group.preference=3)\r\n# )" }
{ "repo_name": "SMRUCC/R-sharp", "ref": "refs/heads/master", "path": "studio/test/test_package/R/constant_symbol_test.R", "content": "const symbol1 as string = [\"aaaaaaa\", \"12345\"];\n\nconst echo = line -> print(line);" }
{ "repo_name": "campsych/concerto-platform", "ref": "refs/heads/master", "path": "src/Concerto/TestBundle/Resources/R/concerto5/R/concerto.template.insertParams.R", "content": "concerto.template.insertParams = function(html,params=list(),removeMissing=T){\n  insertRegex = \"\\\\{\\\\{[^(\\\\}\\\\})|(\\\\{\\\\{)]*\\\\}\\\\}\"\n  matches <- unlist(regmatches(html,gregexpr(insertRegex,html)))\n  offset = 0\n  while(length(matches)>offset){\n    index <- 1\n    while(index<=length(matches)){\n      value <- gsub(\"\\\\{\\\\{\", \"\", matches[index])\n      value <- gsub(\"\\\\}\\\\}\", \"\", value)\n      if(substring(value, 1, 9) == \"template:\") {\n        insert = concerto.directive.template(substring(value,10), params)\n        html <- gsub(matches[index], insert, html, fixed=TRUE)\n      } else if(substring(value, 1, 6) == \"trans:\") {\n        insert = concerto.directive.trans(substring(value,7), params)\n        html <- gsub(matches[index], insert, html, fixed=TRUE)\n      } else if(!is.null(params[[value]]) && !is.na(params[[value]])){\n        insert = as.character(params[[value]])\n        if(Sys.info()['sysname'] == \"Windows\") {\n            if(Encoding(insert) == \"UTF-8\") { insert = enc2native(insert) }\n        }\n        html <- gsub(matches[index], insert, html, fixed=TRUE)\n      }\n      else {\n        if(removeMissing) {\n            html <- gsub(matches[index], \"\", html, fixed=TRUE)\n        } else {\n            offset=offset+1\n        }\n      }\n      index=index+1\n    }\n    matches <- unlist(regmatches(html,gregexpr(insertRegex,html)))\n  }\n  return(html)\n}\n" }
{ "repo_name": "SMRUCC/R-sharp", "ref": "refs/heads/master", "path": "tutorials/strings/loadXML.R", "content": "require(stringr);\n\nstr(fromXML(decode.R_unicode(readText(\"F:\\plot222\\pos.xml\"))));\n\n\npause();" }
{ "repo_name": "michaelgill1969/moodometer", "ref": "refs/heads/master", "path": "moodometer.R", "content": "library(twitteR)\n\nkeyword <- \"boardgame\"\n\nsetup_twitter_oauth(\n    \"qUJs8Ne1koBXqBESCB5QVk0Bi\", \n    \"b5ScMuVT40XAihcJecOFszV63tUPIyvYLM20zEkJaFi4d7l1Tn\",\n    access_token = \"701590844072136704-Z2LuTjZPUH0SSX3EJk35x0tKkYHx1Yh\",\n    access_secret = \"V2aNsga9LbMr1I2OQ5MPBelzcX6w6E3VsyQz35q4gUFze\"\n)\n\nMoodometer <- \n    function(keyword) {\n        tweets <-\n            searchTwitteR(\n                keyword,\n                n = 1000,\n                lang = \"en\"# ,\n                # since = \"2016-01-01\",\n                # geocode = \"39.3600578308,-84.3099365234,1000mi\"\n            )\n        tweets <- strip_retweets(tweets)\n    }\n" }
{ "repo_name": "e3bo/pomp", "ref": "refs/heads/master", "path": "tests/kalman.R", "content": "library(pomp)\nlibrary(reshape2)\nlibrary(plyr)\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(mvtnorm)\nset.seed(1638125322)\n\npng(filename=\"kalman-%02d.png\",res=100)\n\nt <- seq(1,100)\n\nC <- matrix(c(1, 0,  0, 0,\n              0, 1, -1, 0),\n            2,4,byrow=TRUE)\n\ndimX <- ncol(C)\ndimY <- nrow(C)\n\ndimnames(C) <- list(\n    paste0(\"y\",seq_len(dimY)),\n    paste0(\"x\",seq_len(dimX)))\n\nR <- matrix(c(1, 0.3,\n              0.3, 1),\n            nrow=dimY,\n            dimnames=list(rownames(C),rownames(C)))\n\nA <- matrix(c(-0.677822, -0.169411,  0.420662,  0.523571,\n               2.87451,  -0.323604, -0.489533, -0.806087,\n              -1.36617,  -0.592326,  0.567114,  0.345142,\n              -0.807978, -0.163305,  0.668037,  0.468286),\n            nrow=dimX,ncol=dimX,byrow=TRUE,\n            dimnames=list(colnames(C),colnames(C)))\n\nQ <- crossprod(matrix(rnorm(n=dimX*dimX,sd=1/4),4,4))\ndimnames(Q) <- dimnames(A)\n\nX0 <- setNames(rnorm(dimX),colnames(C))\n\nN <- length(t)\nx <- array(dim=c(dimX,N),dimnames=list(variable=colnames(C),time=t))\ny <- array(dim=c(dimY,N),dimnames=list(variable=rownames(C),time=t))\n\nxx <- X0\nsqrtQ <- t(chol(Q))\nsqrtR <- t(chol(R))\nfor (k in seq_along(t)) {\n    x[,k] <- xx <- A %*% xx + sqrtQ %*% rnorm(n=dimX)\n    y[,k] <- C %*% xx + sqrtR %*% rnorm(n=dimY)\n}\n\nkf <- pomp:::kalmanFilter(t,y,X0,A,Q,C,R)\n\ny %>% melt() %>% dcast(time~variable) %>%\n  pomp(times='time',t0=0,\n       rprocess=discrete.time.sim(\n         step.fun=function(x,t,params,...){\n           A%*%x+sqrtQ%*%rnorm(n=ncol(A))\n         },\n         delta.t=1),\n       rmeasure=function(x,t,params,...){\n         C%*%x+sqrtR%*%rnorm(n=nrow(C))\n       },\n       dmeasure=function(y,x,t,params,log,...){\n         dmvnorm(x=t(y-C%*%x),sigma=R,log=log)\n       },\n       initializer=function(params,t0,...){\n         X0\n       },\n       params=c(dummy=3)) %>%\n  pfilter(Np=1000,filter.mean=TRUE) -> pf\n\nenkf <- enkf(pf,h=function(x)C%*%x,R=R,Np=1000)\neakf <- eakf(pf,C=C,R=R,Np=1000)\n\nstopifnot(max(abs(c(kf$loglik,logLik(pf),logLik(enkf),logLik(eakf))-c(-391.0,-391.6,-390.5,-390.8)))<1)\n\nenkf %>% as.data.frame() %>% melt(id.vars=\"time\") %>%\n    ddply(~variable,summarize,n=length(value))\neakf %>% as.data.frame() %>% melt(id.vars=\"time\") %>%\n    ddply(~variable,summarize,n=length(value))\n\nenkf$forecast %>% melt() %>% \n    ggplot(aes(x=time,y=value,group=variable,color=variable))+\n    geom_line()+theme_bw()\n\nidentical(eakf$cond.loglik,cond.logLik(eakf))\nidentical(enkf$pred.mean,pred.mean(enkf))\nidentical(eakf$filter.mean,filter.mean(eakf))\n\ntry({\n    R <- matrix(c(1,0,1,0),2,2)\n    enkf(pf,h=function(x)C%*%x,Np=1000,R=R)\n})\n\ntry({\n    ev <- eigen(R)\n    eakf(pf,C=C,Np=1000,R=R)\n})\n\ndev.off()\n" }
{ "repo_name": "cran/Zelig", "ref": "refs/heads/master", "path": "R/model-binchoice-gee.R", "content": "#' Object for Binary Choice outcomes in Generalized Estimating Equations \n#' for inheritance across models in Zelig\n#'\n#' @import methods\n#' @export Zelig-binchoice-gee\n#' @exportClass Zelig-binchoice-gee\n#'\n#' @include model-zelig.R\n#' @include model-binchoice.R\n#' @include model-gee.R\nzbinchoicegee <- setRefClass(\"Zelig-binchoice-gee\",\n                           contains = c(\"Zelig-gee\",\n                                        \"Zelig-binchoice\"))\n\nzbinchoicegee$methods(\n  initialize = function() {\n    callSuper()\n    .self$family <- \"binomial\"\n    .self$year <- 2011\n    .self$category <- \"continuous\"\n    .self$authors <- \"Patrick Lam\"\n    .self$fn <- quote(geepack::geeglm)\n    # JSON from parent\n  }\n)\n\nzbinchoicegee$methods(\n  param = function(z.out, method=\"mvn\") {\n    simparam.local <- callSuper(z.out, method=method)\n    return(simparam.local$simparam) # no ancillary parameter\n  }\n)\n" }
{ "repo_name": "manuelhorta/stat159-fall2016-proj02", "ref": "refs/heads/master", "path": "code/scripts/pls_regression.R", "content": "#-------------------------------------------\n# -------------------------------------------\n#           PLS Regression\n#  This script computes the pls regression of\n# the credit data.\n# -------------------------------------------\n# -------------------------------------------\n\n\nmessage(\"Begin PLS Script\")\nlibrary(pls)\nset.seed(1)\nsource(\"code/scripts/data-preprocess.R\")\n\n# -------------------------------------------\n# Fit Model\n# -------------------------------------------\npls_fit=plsr(Balance ~ ., \n             data = data.frame(credit_train),\n             scale = FALSE,\n             validation = \"CV\")\nsummary(pls_fit)\n\n# -------------------------------------------\n# Find number of components with best fit\n# -------------------------------------------\nbest_mod_comp_pls <- which.min(pls_fit$validation$PRESS) # 5\n#save(best_mod_comp, file = \"data/pls_best_model_component.RData\")\n\n# -------------------------------------------\n# Plot\n# -------------------------------------------\npng(file = \"images/pls_validation_plot.png\")\nvalidationplot(pls_fit, val.type = \"MSEP\", col='skyblue1')\ndev.off()\nmessage(\"File Save Successful: \", \"pls_validation_plot.png\")\n\n# -------------------------------------------\n# Predict and MSE\n# -------------------------------------------\npls_pred = predict(pls_fit, \n                   credit_test[,-12],\n                   ncomp = best_mod_comp_pls)\n\n\npls_mse <- mean((pls_pred - credit_test[,12])^2)\n#save(pls_mse, file = \"data/pls_mse.RData\")\n\n# -------------------------------------------\n# Refit on full dataset\n# -------------------------------------------\npls_fit_full <- plsr(Balance ~ .,\n                     data = data.frame(scaled_credit),\n                     scale = FALSE, \n                     ncomp = 5)\n\npls_full_coefs <- coefficients(pls_fit_full)\n#save(pls_sum_full, file=\"pls_full_summary.RData\")\n\nsave(pls_fit, best_mod_comp_pls, pls_pred, pls_mse, pls_full_coefs, \n     file ='data/pls-saved-objects.Rdata')\n\n\nsink('data/model-results/pls-results.txt')\n'> best_mod_comp_pls' \nbest_mod_comp_pls\n'> pls test MSE'\npls_mse\n'> pls official coefficients'\npls_full_coefs\nsink(NULL)\n\n\nmessage(\"PLS Script Finished Running\")\n# end\n" }
{ "repo_name": "oncoscape/webapp-R-package", "ref": "refs/heads/master", "path": "hbolouri/oncoDev/Oncoscape/inst/unitTests/websocketTests/websocket_test_MSKDataProviders.R", "content": "library(RUnit)\nlibrary(websockets)\nlibrary(RJSONIO)\nlibrary(Oncoscape)\n#----------------------------------------------------------------------------------------------------\n# before loading and running this script:\n#\n#  start R in another shell\n#    library(Oncoscape); startWebApp(file=NA, port=7781L, openBrowser=FALSE)\n#\n# this provides the web sockets server called by the client created here\n#----------------------------------------------------------------------------------------------------\ncallbackFunction <- function(DATA, WS, ...)\n{\n    unparsed.msg <<- rawToChar(DATA)\n    parsed.msg <- as.list(fromJSON(unparsed.msg))\n    msg.incoming <<- parsed.msg\n\n} # callbackFunction\n#----------------------------------------------------------------------------------------------------\nif(!exists(\"client\")){\n   client <- websocket(\"ws://localhost\", port=7781L)\n   }\nsetCallback(\"receive\", callbackFunction, client);\n#----------------------------------------------------------------------------------------------------\nrunTests = function (levels)\n{\n    test_ping()\n\n    test_get_MSK_GBM_CopyNumber_Data()\n    test_get_MSK_GBM_CopyNumber_Data_bogus_inputs()\n\n    test_get_MSK_GBM_mRNA_Data()    \n    test_get_MSK_GBM_mRNA_Average()\n    test_get_MSK_GBM_mRNA_Average_onSubsets()\n    \n    \n} # runTests\n#----------------------------------------------------------------------------------------------------\ntest_ping <- function()\n{\n   print(\"--- test_ping\")\n   cmd <- \"DataProviderBridge.ping\"\n   status <- \"request\"\n   callback <- \"handleDataProviderPing\"\n   websocket_write(toJSON(list(cmd=cmd, callback=callback, status=status, payload=\"\")), client)\n   \n   system(\"sleep 1\")\n   service(client)\n   checkEquals(names(msg.incoming), c(\"cmd\", \"callback\", \"status\", \"payload\"))\n   checkEquals(msg.incoming$cmd, callback)\n   checkEquals(msg.incoming$status, \"success\")\n   checkEquals(head(msg.incoming$payload), \"ping!\")\n\n} # test_ping\n#----------------------------------------------------------------------------------------------------\ntest_get_MSK_GBM_CopyNumber_Data <- function()\n{\n   print(\"--- test__get_MSK_GBM_CopyNumber_Data\")\n   cmd <- \"get_MSK_GBM_CopyNumber_Data\"\n   status <- \"request\"\n   callback <- \"handle_MSK_GBM_CopyNumber_Data\"\n   entities <-  c(\"1003.2.T.1\", \"1007.T.1\")\n   features <- c(\"CDKN2A\", \"EGFR\")\n   payload <- list(entities=entities, features=features)\n\n   websocket_write(toJSON(list(cmd=cmd, callback=callback, status=status, payload=payload)), client)\n\n   system(\"sleep 1\")\n   service(client)\n   checkEquals(names(msg.incoming), c(\"cmd\", \"callback\", \"status\", \"payload\"))\n   checkEquals(msg.incoming$cmd, \"handle_MSK_GBM_CopyNumber_Data\")\n   checkEquals(msg.incoming$status, \"success\")\n\n   mtx.as.list <- fromJSON(msg.incoming$payload)\n   checkEquals(length(mtx.as.list), 2)\n\n   row1 <- mtx.as.list[[1]]\n   row2 <- mtx.as.list[[2]]\n   checkEquals(row1, list(CDKN2A=-1, EGFR=1, rowname=\"1003.2.T.1\"))\n   checkEquals(row2, list(CDKN2A=-1, EGFR=0, rowname=\"1007.T.1\"))\n\n} # test_get_MSK_GBM_CopyNumber_Data\n#----------------------------------------------------------------------------------------------------\ntest_get_MSK_GBM_CopyNumber_Data_bogus_inputs <- function()\n{\n   print(\"--- test_get_MSK_GBM_CopyNumber_Data_bogus_inputs\")\n   cmd <- \"get_MSK_GBM_CopyNumber_Data\"\n   status <- \"request\"\n   callback <- \"handle_MSK_GBM_CopyNumber_Data\"\n\n   entities <-  c(\"1003.2.T.1\", \"1007.T.1\", \"bogusPatient\")\n   features <- c(\"non-existent gene\", \"CDKN2A\", \"EGFR\")\n\n   payload <- list(entities=entities, features=features)\n\n   websocket_write(toJSON(list(cmd=cmd, callback=callback, status=status, payload=payload)), client)\n\n   system(\"sleep 1\")\n   service(client)\n   checkEquals(names(msg.incoming), c(\"cmd\", \"callback\", \"status\", \"payload\"))\n   checkEquals(msg.incoming$cmd, \"handle_MSK_GBM_CopyNumber_Data\")\n   checkEquals(msg.incoming$status, \"success\")\n\n   mtx.as.list <- fromJSON(msg.incoming$payload)\n   checkEquals(length(mtx.as.list), 2)\n\n   mtx.as.list <- fromJSON(msg.incoming$payload)\n   checkEquals(length(mtx.as.list), 2)\n\n   row1 <- mtx.as.list[[1]]\n   row2 <- mtx.as.list[[2]]\n   checkEquals(row1, list(CDKN2A=-1, EGFR=1, rowname=\"1003.2.T.1\"))\n   checkEquals(row2, list(CDKN2A=-1, EGFR=0, rowname=\"1007.T.1\"))\n   \n      # now send only bogus entities\n\n   entities <-  c(\"MSK.02.0003fubar\", \"MSK.02.0004fubar\", \"bogusPatient\")\n   features <- c(\"AKT1\", \"ATM\", \"non-existent-gene\")\n   payload <- list(entities=entities, features=features)\n\n   websocket_write(toJSON(list(cmd=cmd, callback=callback, status=status, payload=payload)), client)\n\n   system(\"sleep 1\")\n   service(client)\n   checkEquals(names(msg.incoming), c(\"cmd\", \"callback\", \"status\", \"payload\"))\n   checkEquals(msg.incoming$cmd, \"handle_MSK_GBM_CopyNumber_Data\")\n   checkEquals(msg.incoming$status, \"failure\")\n   checkEquals(msg.incoming$payload, \"empty table\")\n\n\n      # now send only bogus features\n\n   entities <-  c(\"MSK.02.0003\", \"MSK.02.0004\", \"bogusPatient\")\n   features <- c(\"AKT1xx\", \"ATMxx\", \"non-existent-gene\")\n   payload <- list(entities=entities, features=features)\n\n   websocket_write(toJSON(list(cmd=cmd, callback=callback, status=status, payload=payload)), client)\n\n   system(\"sleep 1\")\n   service(client)\n   checkEquals(names(msg.incoming), c(\"cmd\", \"callback\", \"status\", \"payload\"))\n   checkEquals(msg.incoming$cmd, \"handle_MSK_GBM_CopyNumber_Data\")\n   checkEquals(msg.incoming$status, \"failure\")\n   checkEquals(msg.incoming$payload, \"empty table\")\n\n} # test_get_MSK_GBM_CopyNumber_Data\n#----------------------------------------------------------------------------------------------------\ntest_get_MSK_GBM_mRNA_Data <- function()\n{\n   print(\"--- test_get_MSK_GBM_mRNA_Data\")\n   cmd <- \"get_MSK_GBM_mRNA_Data\"\n   status <- \"request\"\n   callback <- \"handle_MSK_GBM_mRNA_Data\"\n\n     #               MDM4        REN\n     # 1178.T.1 0.1083581 -0.2805017\n     # 1182.T.1 0.1268618 -0.2463349\n\n   entities <-  c(\"1178.T.1\", \"1182.T.1\")\n   features <- c(\"MDM4\", \"REN\")\n   payload <- list(entities=entities, features=features)\n\n   websocket_write(toJSON(list(cmd=cmd, callback=callback, status=status, payload=payload)), client)\n\n   system(\"sleep 1\")\n   service(client)\n   checkEquals(names(msg.incoming), c(\"cmd\", \"callback\", \"status\", \"payload\"))\n   checkEquals(msg.incoming$cmd, \"handle_MSK_GBM_mRNA_Data\")\n   checkEquals(msg.incoming$status, \"success\")\n\n   mtx.as.list <- fromJSON(msg.incoming$payload)\n   checkEquals(length(mtx.as.list), 2)\n\n   row1 <- mtx.as.list[[1]]\n   row2 <- mtx.as.list[[2]]\n   checkEquals(row1, list(MDM4=0.10836, REN=-0.2805,  rowname=\"1178.T.1\"))\n   checkEquals(row2, list(MDM4=0.12686,  REN=-0.24633, rowname=\"1182.T.1\"))\n\n} # test_get_MSK_GBM_mRNA_Data\n#----------------------------------------------------------------------------------------------------\ntest_get_MSK_GBM_mRNA_Average <- function()\n{\n   print(\"--- test_get_MSK_GBM_mRNA_Average\")\n\n   cmd <- \"get_MSK_GBM_mRNA_Average\"\n   status <- \"request\"\n   callback <- \"handle_MSK_GBM_mRNA_Average\"\n\n     #               MDM4        REN\n     # 1178.T.1 0.1083581 -0.2805017\n     # 1182.T.1 0.1268618 -0.2463349\n\n   entities <-  c(\"1178.T.1\", \"1182.T.1\")\n   payload <- list(entities=entities, features=\"\")\n\n   websocket_write(toJSON(list(cmd=cmd, callback=callback, status=status, payload=payload)), client)\n\n   system(\"sleep 1\")\n   service(client)\n   checkEquals(names(msg.incoming), c(\"cmd\", \"callback\", \"status\", \"payload\"))\n   checkEquals(msg.incoming$cmd, \"handle_MSK_GBM_mRNA_Average\")\n   checkEquals(msg.incoming$status, \"success\")\n   \n   mtx.as.list <- fromJSON(msg.incoming$payload)\n   checkEquals(length(mtx.as.list), 1)\n   mtx.avg <- mtx.as.list[[1]]\n   \n   checkTrue(length(mtx.avg) > 100)   # 150 on (24 jun 2014)\n\n     # last element in this named list should be \"rowname\" and its value should be \"average\"\n   last.item <- length(mtx.avg)\n   checkEquals(names(mtx.avg)[last.item], \"rowname\")\n   checkEquals(mtx.avg[[last.item]], \"average\")\n\n     # check the mean: a rather imprecise test, but one which establishes some sanity in\n     # the data.  mean is, by inspection, 0.784\n   \n   mean <- mean(unlist(mtx.avg[-length(mtx.avg)], use.names=FALSE))\n   checkTrue(mean > 0)\n   checkTrue(mean < 1)\n\n   checkEquals(head(sort(names(mtx.avg))), c(\"AKR1C3\", \"AKT3\", \"ANGPTL4\", \"AQP1\", \"ARC\", \"AVIL\"))\n\n} # test_get_MSK_GBM_mRNA_Average\n#----------------------------------------------------------------------------------------------------\ntest_get_MSK_GBM_mRNA_Average_onSubsets <- function()\n{\n   print(\"--- test_get_MSK_GBM_mRNA_Average_onSubsets\")\n\n   cmd <- \"get_MSK_GBM_mRNA_Average\"\n   status <- \"request\"\n   callback <- \"handle_MSK_GBM_mRNA_Average\"\n\n     #--- first, two genes and two tissues only\n     #               MDM4        REN\n     # 1178.T.1 0.1083581 -0.2805017\n     # 1182.T.1 0.1268618 -0.2463349\n\n   entities <-  c(\"1178.T.1\", \"1182.T.1\")\n   features <- c(\"MDM4\", \"REN\")\n   \n   payload <- list(entities=entities, features=features)\n\n   websocket_write(toJSON(list(cmd=cmd, callback=callback, status=status, payload=payload)), client)\n\n   system(\"sleep 1\")\n   service(client)\n   checkEquals(names(msg.incoming), c(\"cmd\", \"callback\", \"status\", \"payload\"))\n   checkEquals(msg.incoming$cmd, \"handle_MSK_GBM_mRNA_Average\")\n   checkEquals(msg.incoming$status, \"success\")\n   \n   mtx.as.list <- fromJSON(msg.incoming$payload)\n   checkEquals(length(mtx.as.list), 1)\n   mtx.avg <- mtx.as.list[[1]]\n   \n     # last element in this named list should be \"rowname\" and its value should be \"average\"\n\n   checkEquals(mtx.avg, list(MDM4=0.11761, REN=-0.26342, rowname=\"average\"))\n\n     #--- now, two genes and all tissues \n     #               MDM4        REN\n   \n   entities <-  \"\"\n   features <- c(\"MDM4\", \"REN\")\n   \n   payload <- list(entities=entities, features=features)\n   websocket_write(toJSON(list(cmd=cmd, callback=callback, status=status, payload=payload)), client)\n\n   system(\"sleep 1\")\n   service(client)\n   checkEquals(names(msg.incoming), c(\"cmd\", \"callback\", \"status\", \"payload\"))\n   checkEquals(msg.incoming$cmd, \"handle_MSK_GBM_mRNA_Average\")\n   checkEquals(msg.incoming$status, \"success\")\n   \n   mtx.as.list <- fromJSON(msg.incoming$payload)\n   checkEquals(length(mtx.as.list), 1)\n   mtx.avg <- mtx.as.list[[1]]\n   checkEquals(names(mtx.avg), c(\"MDM4\", \"REN\", \"rowname\"))\n\n      # the idiosyncratic way this matrix was calculated ensures that the mean of all values is very nearly zero\n   checkEqualsNumeric(mtx.avg$MDM4, 0, tol=1e10)\n   checkEqualsNumeric(mtx.avg$REN, 0, tol=1e10)\n\n     #--- now, two tissues and all genes\n     #               MDM4        REN\n   \n   entities <-  c(\"1178.T.1\", \"1182.T.1\")\n   features <- \"\"\n   \n   payload <- list(entities=entities, features=features)\n   websocket_write(toJSON(list(cmd=cmd, callback=callback, status=status, payload=payload)), client)\n\n   system(\"sleep 1\")\n   service(client)\n   checkEquals(names(msg.incoming), c(\"cmd\", \"callback\", \"status\", \"payload\"))\n   checkEquals(msg.incoming$cmd, \"handle_MSK_GBM_mRNA_Average\")\n   checkEquals(msg.incoming$status, \"success\")\n   \n   mtx.as.list <- fromJSON(msg.incoming$payload)\n   checkEquals(length(mtx.as.list), 1)\n   mtx.avg <- mtx.as.list[[1]]\n   checkEquals(head(mtx.avg, n=3), list(B2M=5.2763, B4GALT1=1.1114, CLTC=0.15301))\n\n\n} # test_get_MSK_GBM_mRNA_Average_onSubsets\n#----------------------------------------------------------------------------------------------------\n" }
{ "repo_name": "nathanielroth/MWOLeaderBoards", "ref": "refs/heads/master", "path": "Scraps.R", "content": "dfLB <- read.csv(\"LeaderBoards.csv\")\ndfFull <- dfLB\ndfIS <- dfLB[dfLB$ISCL == 'IS',]\ndfClan <- dfLB[dfLB$ISCL == 'Clan',]\ndfLight <- dfLB[dfLB$Class == 'Light',]\ndfMedium <- dfLB[dfLB$Class == 'Medium',]\ndfAssault <- dfLB[dfLB$Class == 'Assault',]\n\n# full DS\n\nlmF_ISCL_Tons  <- lm(Score ~ ISCL + Tons, dfFull)\nsummary(lmF_ISCL_Tons)\n\nlmF_Tons <- lm(Score ~ Tons, dfFull)\nsummary(lmF_Tons)\nplot(dfFull$Tons, dfFull$Score, main=\" Score by Tons\", xlab=\"Tons\", ylab=\"Score\")\nabline(lm(dfFull$Score~dfFull$Tons))\n\nlmf_ISCL <- lm(Score ~ ISCL, dfFull)\nsummary(lmf_ISCL)\n\nfit_ISCL_Tons <- aov(Score ~ ISCL*Tons, data=dfFull)\nsummary(fit_ISCL_Tons)\n\n# IS only\nlmIS <- lm(Score ~ Tons, dfIS)\nsummary(lmIS)\nplot(dfIS$Tons, dfIS$Score, main=\"IS: Score by Tons\", xlab=\"Tons\", ylab=\"Score\")\nabline(lm(dfIS$Score~dfIS$Tons))\n\n\n# Clan only\nlmClan <- lm(Score ~ Tons, dfClan)\nsummary(lmClan)\nplot(dfClan$Tons, dfClan$Score, main=\"Clans: Score by Tons\", xlab=\"Tons\", ylab=\"Score\")\nabline(lm(dfClan$Score~dfClan$Tons))\n\n\n\n# predit Full with IS vs Clan\ndfFull_ISCL_Tons <- dfFull\ndfFull_ISCL_Tons$predAll <- predict(lmF_ISCL_Tons)\ndfFull_ISCL_Tons$residAll <- dfFull_ISCL_Tons$Score - dfFull_ISCL_Tons$predAll\nmech_mean_ISCL_Tons <- aggregate(. ~ Mech, dfFull_ISCL_Tons, function(Score) c(mean = mean(Score)))\nresid_mean_ISCL_Tons <- aggregate(. ~ Mech, dfFull_ISCL_Tons, function(residAll) c(mean = mean(residAll)))\nresid_median_ISCL_Tons <- aggregate(. ~ Mech, dfFull_ISCL_Tons, function(residAll) c(median = median(residAll)))\ndfResults_ISCL_Tons <- mech_mean_ISCL_Tons[,c(\"Mech\",\"Score\")]\ndfResults_ISCL_Tons$pred <- resid_mean_ISCL_Tons$predAll\ndfResults_ISCL_Tons$mean_resid <- resid_mean_ISCL_Tons$residAll\ndfResults_ISCL_Tons$median_resid <- resid_median_ISCL_Tons$residAll\nwrite.csv(dfResults_ISCL_Tons, file=\"results_full_ISCL_Tons.csv\")\n\n\n# predit Full with IS vs Clan\ndfFull_Tons <- dfFull\ndfFull_Tons$predAll <- predict(lmF_Tons)\ndfFull_Tons$residAll <- dfFull_Tons$Score - dfFull_Tons$predAll\nmech_mean_Tons <- aggregate(. ~ Mech, dfFull_Tons, function(Score) c(mean = mean(Score)))\nresid_mean_Tons <- aggregate(. ~ Mech, dfFull_Tons, function(residAll) c(mean = mean(residAll)))\nresid_median_Tons <- aggregate(. ~ Mech, dfFull_Tons, function(residAll) c(median = median(residAll)))\ndfResults_Tons <- mech_mean_Tons[,c(\"Mech\",\"Score\")]\ndfResults_Tons$pred <- resid_mean_Tons$predAll\ndfResults_Tons$mean_resid <- resid_mean_Tons$residAll\ndfResults_Tons$median_resid <- resid_median_Tons$residAll\nwrite.csv(dfResults_Tons, file=\"results_full_tons.csv\")\n\n\n\n\n\n\n\n\n#########################################\n# # normality\n# dfLocust <- dfLB[dfLB$Mech == 'Locust',]\n# hist(dfLocust$Score)\n# shapiro.test(dfLocust$Score)\n\n" }
{ "repo_name": "maxaxam/masters-dataanalytics", "ref": "refs/heads/master", "path": "Year2/STAT40740-MultivariateAnalysis/Assignments/Assignment02-Q2-Exploratory.R", "content": "# Student Number: 14205449\n\n# Question 2\n\n# Clear variables and Set working directory\nrm(list=ls())\nsetwd(\"Assignment02\")\n\nlibrary(class)\nlibrary(klaR)\n\n# (a)\n\n# Load the voting data\n\nload(\"2016_First6Votes_PresentAbsent.Rdata\")\nhead(votes)\n\n# votes.cat <- lapply(votes, factor)\n\ncl <- kmodes(votes, k)\n\n## and visualize with some jitter:\nplot(jitter(votes.cat), col = cl$cluster)\npoints(cl$modes, col = 1:5, pch = 8)\n\nplot(votes, col = cl$cluster)\nload(\"PartyMembership.Rdata\")\n\npar(mfrow=c(1, 1))\nbarplot(votes.tab.ED1, ylim = c(0,130))\nbarplot(votes.tab.ED2, ylim = c(0,130))\nbarplot(votes.tab.Credit, ylim = c(0,130))\nbarplot(votes.tab.Confidence1, ylim = c(0,130))\nbarplot(votes.tab.Confidence2, ylim = c(0,130))\nbarplot(votes.tab.Trade)\n\n# votes.cat <- lapply(votes, factor)\nset.seed(123)\nK <- 10\n\nwithindiffs <- rep(0,K)\n\nfor (k in 1:K) {\n  \n  withindiffs[k] <- sum(kmodes(votes, k)$withindiff)\n}\n\nK <- 10\nsilwidth <- rep(0, K)\nsilwidth[1] <- 0\nfor (k in 2:K) {\n  silwidth[k] <- pam(d,k)$silinfo$avg.width\n}\n\nprint(cbind(1:K, silwidth))\n\n\nset.seed(1)\nx <- rbind(matrix(rbinom(250, 2, 0.25), ncol = 5),\n           matrix(rbinom(250, 2, 0.75), ncol = 5))\ncolnames(x) <- c(\"a\", \"b\", \"c\", \"d\", \"e\")\n\n## run algorithm on x:\n(cl <- kmodes(x, 3))\n\ncl## and visualize with some jitter:\nplot(jitter(x), col = cl$cluster)\npoints(cl$modes, col = 1:5, pch = 8)\n\ntable(votes)\n\nplot(DS0012$BMI.category, DS0012$age.category, xlab = \"BMI\", ylab = \"age\")\n\nplot(votes.cat$ED1, votes.cat$ED2, ylab = \"ED2\", xlab = \"ED1\")\nplot(votes.cat)\n\ncounts <- table(votes)\nplot(counts)\n\nadjRand <- adjustedRandIndex(votes.hclust.average.hcl, votes.lc$predclass)\nadjRand\ntable(votes.hclust.average.hcl, votes.lc$predclass)\n" }
{ "repo_name": "uhkniazi/BRC_Keloid", "ref": "refs/heads/master", "path": "Keloid_main/temp.R", "content": "dfContrast1 = read.csv(file='Results/Control:2vsControl:1.csv')\ndfContrast2 = read.csv(file='Results/Keloid:1vsControl:1.csv')\ndfContrast3 = read.csv(file='Results/Keloid:2vsKeloid:1.csv')\ndfContrast4 = read.csv(file='Results/Keloid:2vsControl:2.csv')\n\ngetFalsePositives = function(x, df){\n   nr = nrow(na.omit(df[df$p.adj <= x & df$Dispersion > 0.4,]))\n   fp = nr*x\n   return(fp)\n}\n\nldf = list(dfContrast1, dfContrast2, dfContrast3, dfContrast4)\ncutoffs = seq(0.01, 1, by = 0.02)\nmCuts = sapply(ldf, function(x){\n  yval = sapply(cutoffs, getFalsePositives, x)\n  return(yval)\n})\n\nrownames(mCuts) = cutoffs\ncolnames(mCuts) = c('C1', 'C2', 'C3', 'C4')\n\nmatplot(mCuts, type='l', lty=1, xlab='FDR Cutoffs', ylab='False Positives', main='No. of false positives and FDR threshold', col=1:4, lwd=2,\n        xaxt='n')\naxis(1, at = 1:length(cutoffs), labels = cutoffs, las=2, cex.axis=0.6)\nlegend('topleft', legend = colnames(mCuts), fill=1:4)\n\n# curve(getFalsePositives, 0.01, 0.2,n = 10, type = 'l', xlab = 'P.adjust cutoff', ylab='False positives', main='Contrast 1')\n# \n# df = dfContrast2\n# curve(getFalsePositives, 0.01, 1,n = 100, type = 'l', xlab = 'P.adjust cutoff', ylab='False positives', main='Contrast 2')\n\nlibrary(TxDb.Hsapiens.UCSC.hg38.knownGene)\noGRLgenes = exonsBy(TxDb.Hsapiens.UCSC.hg38.knownGene, by = 'gene')\ni = i[names(i) %in% names(oGRLgenes)]\noGRLgenes = oGRLgenes[names(i)]\nf = sapply(width(oGRLgenes), sum) > 3000\noGRLgenes = oGRLgenes[f]\nlength(oGRLgenes)\n\n# ### take a sample of exons/transcripts\n# oGRLgenes = exonsBy(TxDb.Hsapiens.UCSC.hg38.knownGene, by = 'tx')\n# ## select transcripts with length close to  mean\noGRLgenes = oGRLgenes[sample(1:length(oGRLgenes), 3000, replace = F)]\nlength(oGRLgenes)\noGRLexons = oGRLgenes\n# temp = unlist(oGRLgenes)\n# temp = temp[width(temp) > 120]\n# temp = temp[sample(1:length(temp), size = 10000, replace = F)]\n# oGRLexons = split(temp, 1:length(temp))\n\ngetTranscriptCoverage = function(bam){\n  ###\n  f_bin_vector = function(start, end, bins){\n    s = floor(seq(start, end, length.out=bins+1))\n    e = s-1\n    e[length(e)] = s[length(s)]\n    length(s) = length(s)-1\n    e = e[2:length(e)]\n    return(data.frame(start=s, end=e))\n  }# f_bin_vector\n  ###\n  which = unlist(range(oGRLgenes))\n  which = resize(which, width = width(which)+100, fix='center')\n  param = ScanBamParam(flag=scanBamFlag(), what = scanBamWhat(), which=which)\n  # read the GAlignments object\n  oGA = readGAlignments(bam, param=param)\n  # get the coverage\n  cov = coverageByTranscript(oGA, oGRLexons, ignore.strand=FALSE)\n  mCoverage = sapply(cov, function(temp) {\n    # create a binned vector to create views on this coverage\n    bins = f_bin_vector(1, sum(width(temp)), bins=2000)\n    # create views on these bins\n    ivCoverage = viewMeans(Views(temp, bins$start, bins$end))\n    ivCoverage = ivCoverage / max(ivCoverage)})\n  mCoverage = t(mCoverage)\n  mCoverage = colMeans(mCoverage, na.rm = T)\n  mCoverage = mCoverage/max(mCoverage)\n  return(mCoverage)\n}\n" }
{ "repo_name": "ugaliguy/Coursera-Data-Science", "ref": "refs/heads/master", "path": "R-Programming/Project-1/pollutantmean.R", "content": "pollutantmean <- function(directory, pollutant, id = 1:332) {\r\n  ## 'directory' is a character vector of length 1 indicating\r\n  ## the location of the CSV files\r\n  \r\n  ## 'pollutant' is a character vector of length 1 indicating\r\n  ## the name of the pollutant for which we will calculate the\r\n  ## mean; either \"sulfate\" or \"nitrate\".\r\n  \r\n  ## 'id' is an integer vector indicating the monitor ID numbers\r\n  ## to be used\r\n  \r\n  ## Return the mean of the pollutant across all monitors list\r\n  ## in the 'id' vector (ignoring NA values)\r\n  ## NOTE: Do not round the result!\r\n  \r\n  \r\n  all_files <- list.files(directory,full.name = TRUE)\r\n  data <- data.frame()\r\n  \r\n  for (i in id){\r\n    data <- rbind(data, read.csv(all_files[i]))\r\n  }\r\n  \r\n  if (pollutant == \"sulfate\"){\r\n    data_mean <- mean(data$sulfate, na.rm = TRUE)\r\n  }\r\n  else if (pollutant == \"nitrate\"){\r\n    data_mean <- mean(data$nitrate, na.rm = TRUE)\r\n  }\r\n  \r\n  data_mean\r\n  \r\n}" }
{ "repo_name": "sestaton/sesbio", "ref": "refs/heads/master", "path": "transposon_annotation/r_scripts/gcov_per_superfam_covar_sum.R", "content": "rm(list=ls())\ngraphics.off()\n\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(plyr)\n\n\n## functions\nco.var <- function(x) ( 100*sd(x)/mean(x) )\n#as_percent <- function(x) { 100 * x}\n\n#1.2m\nhann_1.2m_s11 <- read.table(\"hannuus_1.2m_s11_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_1.2m_s34 <- read.table(\"hannuus_1.2m_s34_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_1.2m_s56 <- read.table(\"hannuus_1.2m_s56_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\n##merge\nhann_1.2m_s11_sfams <- ddply(hann_1.2m_s11, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_1.2m_s34_sfams <- ddply(hann_1.2m_s34, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_1.2m_s56_sfams <- ddply(hann_1.2m_s56, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_1.2m_merged <- Reduce(function(...) merge(..., all=T), list(hann_1.2m_s11_sfams, hann_1.2m_s34_sfams, hann_1.2m_s56_sfams))\nReadNum <- rep(\"1220000\",length(hann_1.2m_merged[,1]))\nhann_1.2m_merged_exp <- cbind(ReadNum, hann_1.2m_merged)\n\n#1.4m\nhann_1.4m_s11 <- read.table(\"hannuus_1.4m_s11_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_1.4m_s34 <- read.table(\"hannuus_1.4m_s34_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_1.4m_s56 <- read.table(\"hannuus_1.4m_s56_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\n##merge\nhann_1.4m_s11_sfams <- ddply(hann_1.4m_s11, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_1.4m_s34_sfams <- ddply(hann_1.4m_s34, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_1.4m_s56_sfams <- ddply(hann_1.4m_s56, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_1.4m_merged <- Reduce(function(...) merge(..., all=T), list(hann_1.4m_s11_sfams, hann_1.4m_s34_sfams, hann_1.4m_s56_sfams))\nReadNum <- rep(\"1420000\",length(hann_1.4m_merged[,1]))\nhann_1.4m_merged_exp <- cbind(ReadNum, hann_1.4m_merged)\n\n#1.6m\nhann_1.6m_s11 <- read.table(\"hannuus_1.6m_s11_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_1.6m_s34 <- read.table(\"hannuus_1.6m_s34_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_1.6m_s56 <- read.table(\"hannuus_1.6m_s56_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\n##merge\nhann_1.6m_s11_sfams <- ddply(hann_1.6m_s11, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_1.6m_s34_sfams <- ddply(hann_1.6m_s34, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_1.6m_s56_sfams <- ddply(hann_1.6m_s56, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_1.6m_merged <- Reduce(function(...) merge(..., all=T), list(hann_1.6m_s11_sfams, hann_1.6m_s34_sfams, hann_1.6m_s56_sfams))\nReadNum <- rep(\"1620000\",length(hann_1.6m_merged[,1]))\nhann_1.6m_merged_exp <- cbind(ReadNum, hann_1.6m_merged)\n\n#1.8m\nhann_1.8m_s11 <- read.table(\"hannuus_1.8m_s11_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_1.8m_s34 <- read.table(\"hannuus_1.8m_s34_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_1.8m_s56 <- read.table(\"hannuus_1.8m_s56_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\n##merge\nhann_1.8m_s11_sfams <- ddply(hann_1.8m_s11, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_1.8m_s34_sfams <- ddply(hann_1.8m_s34, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_1.8m_s56_sfams <- ddply(hann_1.8m_s56, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_1.8m_merged <- Reduce(function(...) merge(..., all=T), list(hann_1.8m_s11_sfams, hann_1.8m_s34_sfams, hann_1.8m_s56_sfams))\nReadNum <- rep(\"1820000\",length(hann_1.8m_merged[,1]))\nhann_1.8m_merged_exp <- cbind(ReadNum, hann_1.8m_merged)\n\n#1m\nhann_1m_s11 <- read.table(\"hannuus_1m_s11_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_1m_s34 <- read.table(\"hannuus_1m_s34_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_1m_s56 <- read.table(\"hannuus_1m_s56_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\n##merge\nhann_1m_s11_sfams <- ddply(hann_1m_s11, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_1m_s34_sfams <- ddply(hann_1m_s34, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_1m_s56_sfams <- ddply(hann_1m_s56, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_1m_merged <- Reduce(function(...) merge(..., all=T), list(hann_1m_s11_sfams, hann_1m_s34_sfams, hann_1m_s56_sfams))\nReadNum <- rep(\"1020000\",length(hann_1m_merged[,1]))\nhann_1m_merged_exp <- cbind(ReadNum, hann_1m_merged)\n\n#20k\nhann_20k_s11 <- read.table(\"hannuus_20k_s11_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_20k_s34 <- read.table(\"hannuus_20k_s34_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_20k_s56 <- read.table(\"hannuus_20k_s56_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\n##merge\nhann_20k_s11_sfams <- ddply(hann_20k_s11, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_20k_s34_sfams <- ddply(hann_20k_s34, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_20k_s56_sfams <- ddply(hann_20k_s56, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_20k_merged <- Reduce(function(...) merge(..., all=T), list(hann_20k_s11_sfams, hann_20k_s34_sfams, hann_20k_s56_sfams))\nReadNum <- rep(\"20000\",length(hann_20k_merged[,1]))\nhann_20k_merged_exp <- cbind(ReadNum, hann_20k_merged)\n\n#220k\nhann_220k_s11 <- read.table(\"hannuus_220k_s11_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_220k_s34 <- read.table(\"hannuus_220k_s34_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_220k_s56 <- read.table(\"hannuus_220k_s56_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\n##merge\nhann_220k_s11_sfams <- ddply(hann_220k_s11, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_220k_s34_sfams <- ddply(hann_220k_s34, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_220k_s56_sfams <- ddply(hann_220k_s56, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_220k_merged <- Reduce(function(...) merge(..., all=T), list(hann_220k_s11_sfams, hann_220k_s34_sfams, hann_220k_s56_sfams))\nReadNum <- rep(\"220000\",length(hann_220k_merged[,1]))\nhann_220k_merged_exp <- cbind(ReadNum, hann_220k_merged)\n\n#420k\nhann_420k_s11 <- read.table(\"hannuus_420k_s11_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_420k_s34 <- read.table(\"hannuus_420k_s34_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_420k_s56 <- read.table(\"hannuus_420k_s56_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\n##merge\nhann_420k_s11_sfams <- ddply(hann_420k_s11, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_420k_s34_sfams <- ddply(hann_420k_s34, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_420k_s56_sfams <- ddply(hann_420k_s56, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_420k_merged <- Reduce(function(...) merge(..., all=T), list(hann_420k_s11_sfams, hann_420k_s34_sfams, hann_420k_s56_sfams))\nReadNum <- rep(\"420000\",length(hann_420k_merged[,1]))\nhann_420k_merged_exp <- cbind(ReadNum, hann_420k_merged)\n\n#620k\nhann_620k_s11 <- read.table(\"hannuus_620k_s11_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_620k_s34 <- read.table(\"hannuus_620k_s34_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_620k_s56 <- read.table(\"hannuus_620k_s56_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\n##merge\nhann_620k_s11_sfams <- ddply(hann_620k_s11, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_620k_s34_sfams <- ddply(hann_620k_s34, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_620k_s56_sfams <- ddply(hann_620k_s56, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_620k_merged <- Reduce(function(...) merge(..., all=T), list(hann_620k_s11_sfams, hann_620k_s34_sfams, hann_620k_s56_sfams))\nReadNum <- rep(\"620000\",length(hann_620k_merged[,1]))\nhann_620k_merged_exp <- cbind(ReadNum, hann_620k_merged)\n\n#820k\nhann_820k_s11 <- read.table(\"hannuus_820k_s11_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_820k_s34 <- read.table(\"hannuus_820k_s34_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\nhann_820k_s56 <-read.table(\"hannuus_820k_s56_latest_rep_annotations_summary.tsv\",sep=\"\\t\",header=T)\n## merge\nhann_820k_s11_sfams <- ddply(hann_820k_s11, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_820k_s34_sfams <- ddply(hann_820k_s34, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_820k_s56_sfams <- ddply(hann_820k_s56, \"Superfamily\", summarize, sd = sd(GPerc), co.var = co.var(GPerc), sum = sum(GPerc))\nhann_820k_merged <- Reduce(function(...) merge(..., all=T), list(hann_820k_s11_sfams, hann_820k_s34_sfams, hann_820k_s56_sfams))\nReadNum <- rep(\"820000\",length(hann_820k_merged[,1]))\nhann_820k_merged_exp <- cbind(ReadNum, hann_820k_merged)\n\nall_merged <- Reduce(function(...) merge(..., all=T), list(hann_20k_merged_exp, hann_220k_merged_exp, hann_420k_merged_exp, hann_620k_merged_exp,\n                                                hann_820k_merged_exp, hann_1m_merged_exp, hann_1.2m_merged_exp, hann_1.4m_merged_exp, hann_1.6m_merged_exp,\n                                                hann_1.8m_merged_exp))\n\n" }
{ "repo_name": "Robinlovelace/bikeR", "ref": "refs/heads/master", "path": "stat19/point-pattern.R", "content": "# point-pattern analysis\n\n## This vignette is about using point-pattern analysis to identify hotspots and compare different sets of points\n\n## Start off getting using to spatstat\nlibrary(spatstat)\nlibrary(rgdal)\nlibrary(maptools)\n# see notes from here also http://www.csiro.au/resources/pf16h\n\nset.seed(120109)\nr <- seq(0, sqrt(2)/6, by = 0.005)\nacB1 <- elide(acB, scale = TRUE)\n# acB1 <- acB1[1:50,] # for tiny subset\nacB1 <- SpatialPoints(acB) # Calculate the G function for the points\nenvacB <- envelope(as(acB1, \"ppp\"), fun = Gest)\n#   , r = r, nrank = 2, nsim = 5) \nplot(envacB, xlab = \"Euclidean distance to nearest neighbour (m)\", main=\"\")\nsummary(envacB)\n\nacBr <- elide(rsample, scale = T)\nenvacBr <- envelope(as(acBr, \"ppp\"), fun = Gest)\nplot(envacBr)\n\n# Now compare with the road network overall:\nroads <- readOGR(\"exclude/\", \"WY-roads\")\nnames(roads)\n# subset roads\n# roadss <- roads[sample(1:nrow(roads), size=10000), ]\nrsample <- spsample(roads, 500000, type=\"random\")\n\n# generate 100 sets of points randomly allocated to the road network\nfor(i in 1:100){\nrsam <- rsample[sample(nrow(coordinates(rsample)), size=nrow(acB)),]\nfname <- paste0(i,\".csv\")\nfname <- paste0(\"exclude/rand-road-points/\",fname)\nwrite.csv(coordinates(rsam), file = fname)\n}\n\n# save bikecrash data\nnrow(acB) / nrow(acWY)\nwrite.csv(coordinates(acB), file = \"exclude/rand-road-points/bike-accident-points.csv\")\n\nsummary(rsample)\nnrow(rsample)\nplot(rsam)\nplot(acB, add=T, col=\"red\")\n\n\n\n\n\n" }
{ "repo_name": "Tenenhaus/RGCCA", "ref": "refs/heads/master", "path": "R/plot_bootstrap_2D.R", "content": "#' Plot a bootstrap in 2D\n#'\n#' Biplot of the top variables from a SGCCA bootstrap with the number of\n#' non-zero occurences in x-axis and the boot-ratio (mean/sd) in y-axis.\n#' Negative weights are colored in red and the positive ones are in green.\n#'\n#' @inheritParams plot2D\n#' @inheritParams get_bootstrap\n#' @param colors reoresenting a vector of colors\n#' @param b A matrix of boostrap\n#' @param x A character for the column to plot in x-axis\n#' @param y A character for the column to plot in y-axis\n#' @param df_b Result of get_bootstrap functions or dataframe #TODO\n#' @examples\n#' library(RGCCA)\n#' data(\"Russett\")\n#' blocks = list(agriculture = Russett[, seq(3)], industry = Russett[, 4:5],\n#'     politic = Russett[, 6:11] )\n#' rgcca_out = rgcca(blocks, sparsity = 0.75, type = \"sgcca\")\n#' boot = bootstrap(rgcca_out, 2, n_cores = 1)\n#' plot_bootstrap_2D(boot, n_cores = 1)\n#' rgcca_out = rgcca(blocks)\n#' boot = bootstrap(rgcca_out, 2, n_cores = 1)\n#' selected.var = get_bootstrap(boot, n_cores = 1)\n#' print(\"i\")\n#'# plot_bootstrap_2D(boot, n_cores = 1)\n#'print(\"j\")\n#' #plot_bootstrap_2D(df_b = selected.var,n_cores=1)\n#' @export\n#' @seealso \\code{\\link[RGCCA]{bootstrap}}, \\code{\\link[RGCCA]{get_bootstrap}}\n\nplot_bootstrap_2D <- function(\n    b = NULL,\n    df_b = NULL,\n    x = \"bootstrap_ratio\",\n    y = \"occurrences\",\n    title = paste(\"Variable selection \\nby\",\n           attributes(b)$n_boot,\n           \"bootstraps\"),\n    colors = NULL,\n    cex = 1,\n    cex_main = 25 * cex,\n    cex_sub = 16 * cex,\n    cex_point = 3 * cex,\n    cex_lab = 19 * cex,\n    comp = 1,\n    i_block = length(b$bootstrap[[1]]),\n    collapse = FALSE,\n    n_cores = parallel::detectCores() - 1) {\n\n    if (missing(b) && missing(df_b))\n        stop(\"Please select a bootstrap object.\")\n    if (!is.null(b)) {\n        df_b <- get_bootstrap(b, comp, i_block, collapse=collapse, n_cores=n_cores)\n    }\n    if (!is.null(df_b))\n        stopifnot(is(df_b, \"df_bootstrap\"))\n\n    title <- paste0(title, collapse = \" \")\n    check_ncol(list(df_b), 1)\n    for (i in c(\"cex\", \"cex_main\", \"cex_sub\", \"cex_point\", \"cex_lab\"))\n        check_integer(i, get(i))\n    check_colors(colors)\n\n    set_occ <- function(x) {\n        match.arg(x, names(attributes(df_b)$indexes))\n        if (x == \"occurrences\" && !x %in% colnames(df_b))\n            return(\"sign\")\n        else\n            return(x)\n    }\n\n    x <- set_occ(x)\n    y <- set_occ(y)\n\n    axis <- function(margin){\n        element_text(\n        face = \"italic\",\n        size = cex_lab * 0.75,\n        margin = margin\n        )\n    }\n\n    transform_x <- function(x){\n        if (\"*\" %in% x) {\n            x[x == \"\"] <- 0\n            x[x == \"*\"] <- 1\n        }\n        return(abs(as.double(x)))\n    }\n\n    p <- ggplot(\n        df_b,\n        aes(\n            x = transform_x(df_b[, x]),\n            y = transform_x(df_b[, y]),\n            label = row.names(df_b),\n            color = as.factor(mean > 0)\n    )) +\n    geom_text(size = cex_point * 0.75) +\n    labs(\n        y =  attributes(df_b)$indexes[[y]],\n        x =  attributes(df_b)$indexes[[x]],\n        title = title\n    ) +\n    theme_classic() +\n    theme_perso(cex, cex_main, cex_sub) +\n    theme(\n        legend.position = \"none\",\n        axis.title.y = axis(margin(0, 20, 0, 0)),\n        axis.title.x = axis(margin(20, 0, 0, 0)),\n        axis.text = element_text(size = 13 * cex)\n    ) +\n    scale_color_manual(values = color_group(seq(2), colors = colors))\n\n    limites <- function(p, x){\n        if (x %in% c(\"sign\", \"occurrences\")) {\n            axis <- deparse(substitute(x))\n            func <- get(paste0(axis, \"lim\"))\n            p <- p + func(0, 1)\n            if (x == \"sign\") {\n                p <- p + \n                    get(paste(\"scale\", axis, \"discrete\", sep = \"_\"))(\n                        labels = c(\"ns\", \"*\"),\n                        limits = c(0, 1)\n                    )\n            }\n        }\n        return(p)\n    }\n\n    p <- suppressMessages(limites(p, x))\n    p <- suppressMessages(limites(p, y))\n\n    return(p)\n}\n" }
{ "repo_name": "cran/rgl", "ref": "refs/heads/master", "path": "R/r3d.rgl.R", "content": "#\n# R3D rendering functions - rgl implementation\n# \n\n# Node Management\n\ngetr3dDefaults <- function(class = NULL, value = NULL) {\n  result <- r3dDefaults\n  if (exists(\"r3dDefaults\", envir = globalenv())) {\n    user <- get(\"r3dDefaults\", envir=.GlobalEnv)\n    for (n in names(user)) {\n      if (is.list(result[[n]]))\n        result[[n]][names(user[[n]])] <- user[[n]]\n      else\n        result[[n]] <- user[[n]]\n    }\n  }\n  if (!is.null(class))\n    result <- result[[class]]\n  if (!is.null(result) && !is.null(value))\n    result <- result[[value]]\n  result\n}\n\nclear3d     <- function(type = c(\"shapes\", \"bboxdeco\", \"material\"), \n                        defaults=getr3dDefaults(),\n                        subscene = 0) {\n    .check3d()\n    rgl.clear( type, subscene = subscene )\n\n    type <- rgl.enum.nodetype(type)\n    if ( 4 %in% type ) { # userviewpoint\n\tdo.call(\"par3d\", defaults[\"FOV\"])\n    }\n    if ( 8 %in% type ) { # modelviewpoint\n        do.call(\"par3d\", defaults[\"userMatrix\"])\n    }\n    if ( 5 %in% type ) { # material\n        if (length(defaults$material))\n    \t    do.call(\"material3d\", defaults$material)\n    }\n    if ( 6 %in% type ) { # background\n    \tdo.call(\"bg3d\", as.list(defaults$bg))\n    }\n}\n\n# Environment\n\n.material3d <- c(\"color\", \"alpha\", \"lit\", \"ambient\", \"specular\",\n    \"emission\", \"shininess\", \"smooth\", \"front\", \"back\", \"size\", \n    \"lwd\", \"fog\", \"point_antialias\", \"line_antialias\",\n    \"texture\", \"textype\", \"texmipmap\",\n    \"texminfilter\", \"texmagfilter\", \"texenvmap\",\n    \"depth_mask\", \"depth_test\", \"isTransparent\",\n    \"polygon_offset\")\n\n.material3d.readOnly <- \"isTransparent\"\n\n# This function expands a list of arguments by putting\n# all entries from Params (i.e. the current settings by default)\n# in place for any entries that are not listed.  \n# Unrecognized args are left in place.\n\n.fixMaterialArgs <- function(..., Params = material3d()) {\n   f <- function(...) list(...)\n   formals(f) <- c(Params, formals(f))\n   names <- as.list(names(Params))\n   names(names) <- names\n   names <- lapply(names, as.name)\n   b <- as.list(body(f))\n   body(f) <- as.call(c(b[1], names, b[-1]))\n   f(...)\n} \n     \n# This one just gets the material args\n# If warn is TRUE, give a warning instead of ignoring extras.\n\n.getMaterialArgs <- function(..., material = list(), warn = FALSE) {\n  fullyNamed <- as.list(match.call(rgl.material, \n                           as.call(c(list(as.name(\"rgl.material\"),\n                                        ...), material))))[-1]\n  good <- names(fullyNamed) %in% .material3d\n  if (warn && !all(good))\n    warning(\"Argument(s) \", paste(names(fullyNamed)[!good], collapse = \", \"), \" not matched.\")\n  fullyNamed[good]\n}\n\nmaterial3d  <- function(...) {\n    args <- list(...)\n    argnames <- setdiff(names(args), .material3d.readOnly)\n    if (!length(args))\n\targnames <- .material3d\n    else {\n\tif (is.null(names(args)) && all(unlist(lapply(args, is.character)))) {\n\t    argnames <- unlist(args)\n\t    args <- NULL\n\t}\n\t\n\tif (length(args) == 1) {\n\t    if (is.list(args[[1]]) | is.null(args[[1]])) {\n\t\targs <- args[[1]]\n\t\targnames <- names(args)\n\t    }\n\t}\n    }\n    value <- rgl.getmaterial()[argnames]\n    if (length(args)) {\n    \targs <- do.call(\".fixMaterialArgs\", args)\n        do.call(\"rgl.material\", args)\n        return(invisible(value))\n    } else if (length(argnames) == 1) return(value[[1]])\n    else return(value)\n}\n\nbg3d        <- function(...) {\n  .check3d(); save <- material3d(); on.exit(material3d(save))\n  bgid <- ids3d(\"background\")$id\n  if (length(bgid) && nrow(flags <- rgl.attrib(bgid[1], \"flags\"))) {\n    sphere <- flags[\"sphere\", 1]\n    fogtype <- if (flags[\"linear_fog\", 1]) \"linear\"\n    else if (flags[\"exp_fog\", 1]) \"exp\"\n    else if (flags[\"exp2_fog\", 1]) \"exp2\"\n    else \"none\"\n  } else {\n    sphere <- FALSE\n    fogtype <- \"none\"\n  }\n  new <- .fixMaterialArgs(sphere = sphere, fogtype = fogtype, \n                          color = c(\"black\", \"white\"), \n  \t\t\t  back = \"lines\", lit = FALSE, Params = save)\n  do.call(\"rgl.bg\", .fixMaterialArgs(..., Params = new))\n}\n\nlight3d     <- function(theta=0,phi=15,x=NULL, ...) {\n  .check3d()\n  if (is.null(x))\n    rgl.light(theta=theta,phi=phi,x=x, ...)\n  else\n    rgl.light(x=x, ...)\n}\n\nview3d      <- function(theta=0,phi=15,...) {\n  .check3d()\n  rgl.viewpoint(theta=theta,phi=phi,...)\n}\n\nbbox3d\t    <- function(xat = NULL, \n                        yat = NULL, \n                        zat = NULL, \n                        xunit = \"pretty\",\n                        yunit = \"pretty\",\n                        zunit = \"pretty\",\n\t\t        expand = 1.03, draw_front = FALSE, ...) {\n  .check3d(); save <- material3d(); on.exit(material3d(save))\n  do.call(\"rgl.bbox\", c(list(xat=xat, yat=yat, zat=zat, \n                             xunit=xunit, yunit=yunit, zunit=zunit, expand=expand,\n                             draw_front=draw_front), \n                        .fixMaterialArgs(..., Params = save)))\n}\n\nobserver3d <- function(x, y=NULL, z=NULL, auto=FALSE) {\n  if (missing(x))\n    location <- c(NA, NA, NA)\n  else {\n    xyz <- xyz.coords(x,y,z)\n    location <- c(xyz$x, xyz$y, xyz$z)\n    if (length(location) != 3) stop(\"A single point must be specified for the observer location\") \n  }    \n  prev <- .C(rgl_getObserver, success=integer(1), ddata=numeric(3), NAOK = TRUE)$ddata\n  .C(rgl_setObserver, success=as.integer(auto), ddata=as.numeric(location), NAOK = TRUE)\n  lowlevel(prev)\n}\n\n# Shapes\n\npoints3d    <- function(x,y=NULL,z=NULL,...) {\n  .check3d(); save <- material3d(); on.exit(material3d(save))\n  do.call(\"rgl.points\", c(list(x=x,y=y,z=z), .fixMaterialArgs(..., Params = save)))\n}\n\nlines3d     <- function(x,y=NULL,z=NULL,...) {\n  .check3d(); save <- material3d(); on.exit(material3d(save))\n  do.call(\"rgl.linestrips\", c(list(x=x,y=y,z=z), .fixMaterialArgs(..., Params = save)))\n}\n\nsegments3d  <- function(x,y=NULL,z=NULL,...) {\n  .check3d(); save <- material3d(); on.exit(material3d(save))\n  do.call(\"rgl.lines\", c(list(x=x,y=y,z=z), .fixMaterialArgs(..., Params = save)))\n}\n\ntriangles3d <- function(x,y=NULL,z=NULL,...) {\n  .check3d(); save <- material3d(); on.exit(material3d(save))\n  do.call(\"rgl.triangles\", c(list(x=x,y=y,z=z), .fixMaterialArgs(..., Params = save)))\n}\n\nquads3d     <- function(x,y=NULL,z=NULL,...) {\n  .check3d(); save <- material3d(); on.exit(material3d(save))\n  do.call(\"rgl.quads\", c(list(x=x,y=y,z=z), .fixMaterialArgs(..., Params = save)))\n}\n\ntext3d      <- function(x, y = NULL, z = NULL,\n\t\t\ttexts, adj = 0.5, pos = NULL, offset = 0.5,\n\t\t\tusePlotmath = is.language(texts), ...) {\n  if (usePlotmath) \n    return(plotmath3d(x = x, y = y, z = z, text = texts, adj = adj, \n                      pos = pos, offset = offset, ...))\n  .check3d(); save <- material3d(); on.exit(material3d(save))\n  new <- .fixMaterialArgs(..., Params = save)\n  do.call(\"rgl.texts\", c(list(x = x, y = y, z = z, text = texts, \n                              adj = adj, pos=pos,\n                              offset = offset), new))\n}\ntexts3d\t    <- text3d\n\nspheres3d   <- function(x, y = NULL, z = NULL, radius = 1, fastTransparency = TRUE, ...) {\n  .check3d(); save <- material3d(); on.exit(material3d(save))\n  do.call(\"rgl.spheres\", c(list(x = x, y = y, z = z, \n  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\tradius = radius, fastTransparency = fastTransparency), .fixMaterialArgs(..., Params = save)))\n}\n\nplanes3d   <- function(a,b=NULL,c=NULL,d=0,...) {\n  .check3d(); save <- material3d(); on.exit(material3d(save))\n  do.call(\"rgl.planes\", c(list(a=a,b=b,c=c,d=d), .fixMaterialArgs(..., Params = save)))\n}\n\nclipplanes3d   <- function(a,b=NULL,c=NULL,d=0) {\n  .check3d()\n  rgl.clipplanes(a=a,b=b,c=c,d=d)\n}\n\nabclines3d   <- function(x,y=NULL,z=NULL,a,b=NULL,c=NULL,...) {\n  .check3d(); save <- material3d(); on.exit(material3d(save))\n  do.call(\"rgl.abclines\", c(list(x=x,y=y,z=z,a=a,b=b,c=c), .fixMaterialArgs(..., Params = save)))\n}\n\nsprites3d   <- function(x, y = NULL, z = NULL, radius = 1, \n\t\t\t\t\t\t\t\t\t\t\t\tshapes = NULL, userMatrix,\n\t\t\t\t\t\t\t\t\t\t\t\tfixedSize = FALSE, ...) {\n  .check3d(); save <- material3d(); on.exit(material3d(save))\n  if (missing(userMatrix)) {\n    userMatrix <- getr3dDefaults()$userMatrix\n    if (is.null(userMatrix)) userMatrix <- diag(4)\n  }\n  savepar <- par3d(skipRedraw=TRUE, ignoreExtent=TRUE)\n  on.exit(par3d(savepar), add=TRUE)\n  force(shapes)\n  par3d(ignoreExtent=savepar$ignoreExtent)\n\n  do.call(\"rgl.sprites\", c(list(x=x,y=y,z=z,radius=radius,shapes=shapes,\n                                userMatrix=userMatrix, fixedSize = fixedSize), \n          .fixMaterialArgs(..., Params = save)))\n}\n\nterrain3d   <- function(x,y=NULL,z=NULL,...,normal_x=NULL,normal_y=NULL,normal_z=NULL) {\n  .check3d(); save <- material3d(); on.exit(material3d(save))\n  do.call(\"rgl.surface\", c(list(x=x,y=z,z=y,coords=c(1,3,2),\n                                normal_x=normal_x,normal_y=normal_z,normal_z=normal_y), \n                           .fixMaterialArgs(..., Params = save)))\n}\nsurface3d   <- terrain3d\n\n# Interaction\n\nselect3d    <- function(...) {\n  .check3d()\n  rgl.select3d(...)\n}\n\n# 3D Generic Object Rendering Attributes\n\ndot3d <- function(x,...) UseMethod(\"dot3d\")\nwire3d  <- function(x,...) UseMethod(\"wire3d\")\nshade3d <- function(x,...) UseMethod(\"shade3d\")\n\n# 3D Generic transformation\n\n\ntranslate3d <- function(obj,x,y,z,...) UseMethod(\"translate3d\")\nscale3d <- function(obj,x,y,z,...) UseMethod(\"scale3d\")\nrotate3d <- function(obj,angle,x,y,z,matrix,...) UseMethod(\"rotate3d\")\ntransform3d <- function(obj,matrix,...) rotate3d(obj, matrix=matrix, ...)\n\nsubdivision3d <- function(x,...) UseMethod(\"subdivision3d\")\n\n# 3D Custom shapes\n\nparticles3d <- function(x,y=NULL,z=NULL,radius=1,...) sprites3d(\n  x=x,y=y,z=z,radius=radius,\n  lit=FALSE,alpha=0.2,\n  textype=\"alpha\",\n  texture=system.file(\"textures/particle.png\",package=\"rgl\"),\n  ...\n)   \n\n# r3d default settings for new windows\n\nr3dDefaults <- list(userMatrix = rotationMatrix(290*pi/180, 1, 0, 0),\n\t\t  mouseMode = c(\"trackball\", \"zoom\", \"fov\", \"pull\"),\n\t\t  FOV = 30,\n\t\t  bg = list(color=\"white\",fogtype = \"none\"),\n\t\t  family = \"sans\",\n\t\t  material = list(color=\"black\", fog = TRUE))\n\nif (.Platform$OS.type == \"windows\")\n\tr3dDefaults$useFreeType <- FALSE\n\nopen3d <- function(..., params = getr3dDefaults(), \n                   useNULL = rgl.useNULL(), silent = FALSE\t) {\n\t\n\t  register_pkgdown_methods()\n\t\n    args <- list(...)\n    if (!is.null(args$antialias) \n        || !is.null(args$antialias <- r3dDefaults$antialias)) {\n    \tsaveopt <- options(rgl.antialias = args$antialias)\n    \ton.exit(options(saveopt))\n    \targs$antialias <- NULL\n    }\n    \n    rgl.open(useNULL)\n    \n    if (!is.null(args$material)) {\n    \tparams$material <- do.call(.fixMaterialArgs, c(args$material, Params=list(params$material)))\n    \targs$material <- NULL\n    }\n    \n    if (length(args) && (is.null(names(args)) \n                      || any(nchar(names(args)) == 0)))\n      stop(\"open3d parameters must be named\")\n    \n    params[names(args)] <- args\n        \n    clear3d(\"material\", defaults = params)\n    params$material <- NULL\n    \n    if (!is.null(params$bg)) {\n      do.call(\"bg3d\", params$bg)\n      params$bg <- NULL\n    }\n \n    do.call(\"par3d\", params)  \n    result <- structure(cur3d(), class = \"rglOpen3d\")\n    if (silent)\n      invisible(result)\n    else\n      result\n}\n\nprint.rglOpen3d <- function(x, ...) {\n  print(unclass(x))\n}\n\nclose3d <- function(dev = cur3d(), silent = TRUE) {\n  for (d in dev[dev != 0]) {\n    set3d(d, silent = silent)\n    rgl.close()\n    if (!silent)\n      message(\"Closed device \", d)\n  }\n  invisible(cur3d())\n}\n\ncur3d <- rgl.cur\n\nset3d <- function(dev, silent = FALSE) {\n  prev <- cur3d()\n  rgl.set(dev, silent = silent)\n  prev\n}\n\n.check3d <- function() {\n    if (result<-cur3d()) return(result)\n    else return(open3d())\n}\n\nrequireWebshot2 <- function() {\n  suppressMessages(res <- requireNamespace(\"webshot2\", quietly = TRUE))\n  res\n}\n\nsnapshot3d <- function(filename = tempfile(fileext = \".png\"), \n                       fmt = \"png\", top = TRUE, ..., scene, width = NULL, height = NULL,\n                       webshot = TRUE) {\n  force(filename)\n  \n  if (webshot && !requireWebshot2()) {\n    warning(\"webshot = TRUE requires the webshot2 package; using rgl.snapshot() instead\")\n    webshot <- FALSE\n  }\n  saveopts <- options(rgl.useNULL = webshot)\n  on.exit(options(saveopts))\n  \n    # The logic here is a little convoluted:\n  # scene  resize  webshot getscene1 plot resize getscene2\n  # no     no      no      \n  # no     no      yes         X\n  # no     yes     no                       X\n  # no     yes     yes         X      X     X      X                 \n  # yes    no      no                 X\n  # yes    no      yes     \n  # yes    yes     no                 X     X\n  # yes    yes     yes                X     X      X\n  \n    \n  resize <- !is.null(width) || !is.null(height)\n  havescene <- !missing(scene)\n  \n  if (havescene) {\n    if (inherits(scene, \"rglWebGL\")) {\n      snapshot <- scene$x$snapshot\n      if (!is.null(snapshot) && is.null(width) && is.null(height))\n        return(saveURI(snapshot, filename))\n      else\n        scene <- attr(scene, \"origScene\")\n    }\n  }\n  if (!havescene && webshot)\n    scene <- scene3d()\n  \n  if ((!havescene && resize && webshot)\n      || (havescene && (resize || !webshot))) {\n    open3d()\n    plot3d(scene)\n    on.exit(close3d(), add = TRUE)\n  }\n  if (resize) {\n    saverect <- rect <- par3d(\"windowRect\")\n    on.exit(par3d(windowRect = saverect), add = TRUE)\n    if (!is.null(width))\n      rect[3] <- rect[1] + width\n    if (!is.null(height))\n      rect[4] <- rect[2] + height\n    par3d(windowRect = rect)\n  }\n  if (webshot) {\n    if (resize)\n      scene <- scene3d()\n    rect <- par3d(\"windowRect\")\n    f1 <- tempfile(fileext = \".html\")\n    on.exit(unlink(f1), add = TRUE)\n    width <- rect[3] - rect[1]\n    height <- rect[4] - rect[2]\n    saveWidget(rglwidget(scene,\n                         elementId = \"webshot\",\n                         width = width,\n                         height = height,\n                         webgl = TRUE), \n               f1)\n    capture.output(webshot2::webshot(f1, file = filename, selector = \"#webshot\",\n                        vwidth = width + 100, vheight = height, ...),\n                   type = \"message\")\n    invisible(filename)\n  } else\n    rgl.snapshot(filename, fmt, top)\n}\n" }
{ "repo_name": "gacarrillor/vec2dtransf", "ref": "refs/heads/master", "path": "R/SimilarityTransformation-methods.R", "content": "### Constructor (Accepts either controlPoints or parameters)\n\"SimilarityTransformation\" <- function(controlPoints=data.frame(), parameters=numeric()) {\n    if (missing(parameters))\n        return(new(\"SimilarityTransformation\", controlPoints=controlPoints))\n    if (missing(controlPoints))\n        return(new(\"SimilarityTransformation\", parameters=parameters))\n    new(\"SimilarityTransformation\", controlPoints=controlPoints, parameters=parameters)\n}\n\n### Calculate parameters from control points\n###     Modifies the original object\n### Arguments:\n### - object is an \"SimilarityTransformation\" object\n###\nif (!isGeneric(\"calculateParameters\"))\n    setGeneric(\"calculateParameters\",function(object){standardGeneric (\"calculateParameters\")})\nsetMethod(f=\"calculateParameters\",signature(object=\"SimilarityTransformation\"),\n    definition=function(object){\n\t\tif (ncol(object@controlPoints) == 0)\n\t\t\tstop(\"Control points were not provided. You could access the parameters directly by calling 'getParameters'.\")\n\n        newObject <- deparse(substitute(object))\n\n        x1=c(object@controlPoints[,1],object@controlPoints[,2])\n        x2=c(object@controlPoints[,2],-object@controlPoints[,1])\n        ones = rep(1,nrow(object@controlPoints))\n        zeros = rep(0,nrow(object@controlPoints))\n        x3=c(ones,zeros)\n        x4=c(zeros,ones)\n        linMod=lm(formula=c(object@controlPoints[,3],object@controlPoints[,4]) ~ x1 + x2 + x3 + x4 - 1)\n\n        object@parameters <- as.vector(coef(linMod))\n        object@residuals <- matrix(linMod$residuals,nrow(object@controlPoints))\n        object@rmse <- sqrt(sum(object@residuals**2)/nrow(object@residuals))\n\n        assign(newObject,object,envir=parent.frame())\n\t\treturn(invisible())\n\t}\n)\n\n" }
{ "repo_name": "andrie/RTVS-docs", "ref": "refs/heads/master", "path": "examples/Introduction_to_R_Open/MRO-MKL-benchmarks.R", "content": "# Microsoft R Open includes the Intel MKL for fast, parallel linear algebra \n# computations. This script runs performance benchmarks using different \n# numbers of threads\n\n# The test uses the package \"version.compare\", available on github\n# Install this package first, if it is not already installed\n\nif (!require(\"version.compare\")){\n  (if (!require(\"devtools\")) install.packages(\"devtools\"))\n  library(\"devtools\") \n  devtools::install_github(\"andrie/version.compare\")\n}\nlibrary(version.compare)\n\n# Determine the local installation path\nr <- findRscript(\n  version = as.character(getRversion())\n)\n\n# Determine how many threads to use, in sequence 1,2,4,8...\n# up to maximum number of physical processors on the machine\n\nthreadsToTest <- if(exists(\"setMKLthreads\")){\n  local({\n    threads <- 2^(0:4)\n    max <- match(RevoUtilsMath:::.Default.Revo.Threads, threads)\n    threads[seq_len(max)]\n  })\n} else {\n  1\n}\n\n# Run the benchmark tests\n# Set scale.factor to 1 for the full tests, lower than 1 for tests on \n# reduces data set sizes\n\nscale.factor <- 0.25\nx <- RevoMultiBenchmark(r, threads = threadsToTest, scale.factor = scale.factor)\n\n# Print a table of results\nprint(x)\n\n# Create a plot\np <- plot(x, theme_size = 12)\nprint(p)\n" }
{ "repo_name": "briandconnelly/numerous", "ref": "refs/heads/master", "path": "R/get_auth_header.R", "content": "#' Build a string for numerous API authentication\n#'\n#' @param api_key The Numerous API key. This can be found in the Numerous mobile apps under Settings > Developer Info.\n#'\n#' @return A HTTP request object that contains the Authorization header\n#' @seealso \\code{\\link{add_headers}}\n#' @importFrom assertthat assert_that is.string\n#' @importFrom base64enc base64encode\n#' @importFrom httr add_headers\n#' @export\n#'\n#' @examples\n#' \\dontrun{\n#' library(numerous)\n#' h <- get_auth_header(api_key = \"nmrs_S7ZEna7Pmjg7\")\n#' }\nget_auth_header <- function(api_key)\n{\n    assert_that(is.string(api_key))\n    \n    auth_str <- paste(\"Basic\", base64encode(charToRaw(paste0(api_key, ':'))))\n    add_headers(Authorization=auth_str)\n}\n" }
{ "repo_name": "andrewdefries/andrewdefries.github.io", "ref": "refs/heads/master", "path": "FDA_Pesticide_Glossary/methfuroxam.R", "content": "library(\"knitr\")\nlibrary(\"rgl\")\n#knit(\"methfuroxam.Rmd\")\n#markdownToHTML('methfuroxam.md', 'methfuroxam.html', options=c(\"use_xhml\"))\n#system(\"pandoc -s methfuroxam.html -o methfuroxam.pdf\")\n\n\nknit2html('methfuroxam.Rmd')\n" }
{ "repo_name": "beechung/Latent-Factor-Models", "ref": "refs/heads/master", "path": "src/RLFM-ars-logistic/R/regression.R", "content": "### Copyright (c) 2011, Yahoo! Inc.  All rights reserved.\n### Copyrights licensed under the New BSD License. See the accompanying LICENSE file for terms.\n###\n### Author: Liang Zhang\n\n# Use bayesglm to fit the coefficients (g0 or d0) for the main effects (alpha or beta)\n#   E.g., output = fit.forMainEffect.bayesglm(alpha, w);\n#   See output$coef (coefficient vector) and output$rss (residual sum of squares)\nfit.forMainEffect.bayesglm <- function(\n    target, feature, lm=F,...\n){\n    if(length(target) != nrow(feature)) stop(\"length(target) != nrow(feature)\");\n        flagSingleCol=0;\n    if(all(feature == 0)){\n        fit = list(coefficients=rep(0,ncol(feature)),fitted.values=0);\n    } else{\n           if( ncol(feature)==1 ) {\n                        flagSingleCol=1;\n                        feature=cbind(feature,matrix(0,nrow=length(feature),ncol=1)); #workaround bayesglm bug\n           }\n        fit = if(!lm)bayesglm(target ~ feature -1, model=FALSE, x=FALSE, y=FALSE, ...) else lm(target ~ feature -1, model=FALSE, x=FALSE, y=FALSE, ...);;\n    }\n\n    if(length(fit$coef) != ncol(feature)) stop(\"length(fit$coef) != ncol(feature)\");\n        output = list();\n        if(flagSingleCol==1){\n                output$coef = fit$coefficients[1];\n        } else{\n        output$coef = fit$coefficients;\n        }\n        output$rss  = sum((fit$fitted.values - target)^2);\n    return(output);\n}\n\n# Use bayesglm to fit the coefficients (G or D) for the factors (u or v)\n#   E.g., output = fit.forFactors.bayesglm(u, w);\n#   See output$coef (coefficient matrix) and output$rss (standard error)\n#   Note: target is a matrix\nfit.forFactors.bayesglm <- function(\n    target, feature, lm=F,...\n){\n    if(nrow(target) != nrow(feature)) stop(\"nrow(target) != nrow(feature)\");\n    nFeatures = ncol(feature);\n    nFactors  = ncol(target);\n        flagSingleCol=0;\n    output = list();\n    output$coef = matrix(NA, nrow=nFeatures, ncol=nFactors);\n    rss = rep(0,nFactors);\n\n        if( ncol(feature)==1 ) {\n                flagSingleCol=1;\n                feature=cbind(feature,matrix(0,nrow=length(feature),ncol=1)); #workaround bayesglm bug\n        }\n\n        for(f in 1:nFactors){\n        if(all(feature == 0)){\n            fit = list(coefficients=rep(0,ncol(feature)),fitted.values=0);\n        }else{\n\n            fit = if(!lm)bayesglm(target[,f] ~ feature -1, model=FALSE, x=FALSE, y=FALSE, ...) else lm(target[,f] ~ feature -1, model=FALSE, x=FALSE, y=FALSE, ...);\n        }\n        if(length(fit$coef) != ncol(feature)) stop(\"length(fit$coef) != ncol(feature)\");\n                if(flagSingleCol==1){\n                  output$coef[,f] = fit$coefficients[1];\n                } else{\n                output$coef[,f] = fit$coefficients;\n                }\n        rss[f] = rss[f] + sum((fit$fitted.values - target[,f])^2);\n    }\n\n    output$rss = rss;\n    return(output);\n}\n\nfit.forMainEffect.glmnet<- function(\n    target, feature, ...\n)\n{\n            cat(\"MainEffect for \",dim(target), \"and feature dim = \", dim(feature) ,\"\\n\");\n            if(length(target) != nrow(feature)) stop(\"length(target) != nrow(feature): \",length(target),\" vs. \",nrow(feature));\n#           if(!is.matrix(feature)) stop(\"feature has to be a matrix\");\n            #cat(\"now run glmnet\\n\");\n            if (sum(abs(feature[,1]-1))==0)\n            {\n                        if (ncol(feature)==1 ) {\n                                output=fit.forMainEffect.bayesglm(target,as.matrix(feature), ...);\n                                return(output);\n                        } else {\n                                x = Matrix(feature[,2:ncol(feature)],sparse=T);\n                        }\n                } else {\n                        stop(\"feature should have a constant term for intercept\");\n            }\n\n            fit0 = cv.glmnet(x,target,nfolds=3,type=\"mse\");\n            #cat(\"lambda=\",fit0$lambda.1se,\"\\n\");\n            #fit = glmnet(x,target,family=\"gaussian\",lambda=fit0$lambda.min);\n            lambdaind = which(fit0$lambda==fit0$lambda.min);\n            a0 = fit0$glmnet.fit$a0[lambdaind];\n            beta = fit0$glmnet.fit$beta[,lambdaind];\n            output = list();\n            output$coef = as.vector(c(a0,beta));\n            fitted.values = feature%*%output$coef;\n            output$rss = sum((fitted.values - target)^2);\n\n            return(output);\n}\nfit.forFactors.glmnet <- function(\n    target, feature, ...\n){\n    cat(\"FactorEffect for \",dim(target), \"and feature dim = \", dim(feature) ,\"\\n\");\n    if(nrow(target) != nrow(feature)) stop(\"nrow(target) != nrow(feature)\");\n#if(!is.matrix(feature)) stop(\"feature has to be a matrix\");\n    if (sum(abs(feature[,1]-1))==0)\n    {\n                if (ncol(feature)==1 ) {\n                        output=fit.forFactors.bayesglm(target=target,feature=as.matrix(feature));\n                        return(output);\n                }else{\n                        x = Matrix(feature[,2:ncol(feature)],sparse=T);\n                }\n        } else {\n                stop(\"feature should has a constant term for intercept\");\n    }\n\n           nFeatures = ncol(feature);\n           nFactors  = ncol(target);\n           output = list();\n           output$coef = matrix(NA, nrow=nFeatures, ncol=nFactors);\n           rss = rep(0,nFactors);\n\n           for(f in 1:nFactors){\n                fit0 = cv.glmnet(x,target[,f],nfolds=3,type=\"mse\");\n           #fit = glmnet(x,target[,f],family=\"gaussian\",lambda=fit0$lambda.min);\n           lambdaind = which(fit0$lambda==fit0$lambda.min);\n               a0 = fit0$glmnet.fit$a0[lambdaind];\n               beta = fit0$glmnet.fit$beta[,lambdaind];\n               output$coef[,f] = as.vector(c(a0,beta));\n           fitted.values = feature%*%output$coef[,f];\n               rss[f] = rss[f] + sum((fitted.values - target[,f])^2);\n           }\n           output$rss = rss;\n           return(output);\n}\n\n\n# Monte-Carlo EM (M-step)\n#   See output$b, output$var_y, ...\nMC_MStep <- function(\n    user, item, y, x, w, z,\n    o, o.sumvar, alpha, alpha.sumvar, beta, beta.sumvar, u, u.sumvar, v, v.sumvar,\n        bfixed=FALSE,\n        b_old=NULL, # not used unless bfixed=T\n    debug=0, lm=F, use.glmnet=F, ...\n){\n    nObs     = length(y);\n    nUsers   = length(alpha);\n    nItems   = length(beta);\n    nFactors = ncol(u);\n\n    output = list();\n\n    # determine b and var_y\n    target = y - o;\n        rss=0;\n        if(bfixed == FALSE){\n                 if(all(x == 0)){\n                     fit = list(fitted.values=0, coefficients=rep(0,ncol(x)));\n                 }else{\n                     fit = lm(target ~ x -1, model=FALSE);\n                 }\n        if(length(fit$coef) != ncol(x)) stop(\"length(fit$coef) != ncol(x)\");\n        rss = sum((fit$fitted.values - target)^2);\n\n        output$b = fit$coefficients;\n        }       else{\n                if(is.null(b_old)) { error(\"No b_old provided in MC_MStep when bfixed=T\\n\");}\n                output$b=b_old;\n        rss = sum(( (x %*% b_old) - target)^2);\n        }\n\n        output$var_y = (rss + o.sumvar) / nObs;\n\n    cat(\"use.glmnet=\",use.glmnet,\"\\n\");\n    # determin g0 and var_alpha\n    if (use.glmnet==F )\n    {\n        fit = fit.forMainEffect.bayesglm(alpha, w, lm=lm, ...);\n    } else\n    {\n        fit = fit.forMainEffect.glmnet(alpha, w, ...);\n    }\n    output$g0 = fit$coef;\n    output$var_alpha = (sum(fit$rss) + alpha.sumvar) / nUsers;\n\n    # determin d0 and var_beta\n    if (use.glmnet==F)\n    {\n        fit = fit.forMainEffect.bayesglm(beta, z, lm=lm,...);\n    } else\n    {\n        fit = fit.forMainEffect.glmnet(beta, z, ...);\n    }\n    output$d0 = fit$coef;\n    output$var_beta = (sum(fit$rss) + beta.sumvar) / nItems;\n\n    # determin G and var_u\n    if (use.glmnet==F)\n    {\n        fit = fit.forFactors.bayesglm(u, w, lm=lm,...);\n    } else\n    {\n        fit = fit.forFactors.glmnet(u, w, ...);\n    }\n    output$G = fit$coef;\n        if(var_u_fixed==FALSE){\n            output$var_u = (sum(fit$rss) + sum(u.sumvar)) / (nUsers * nFactors);\n        } else{\n                if(use_nFactors_based_fixed_u_var==TRUE){\n                        output$var_u = C * 6/sqrt(nFactors);\n                } else{\n            output$var_u = forever_fixed_u_var;\n        }\n            cat(\"var_u is fixed, = \",output$var_u,\"\\n\",sep=\"\");\n        }\n\n    # determin D and var_v\n    if (use.glmnet==F)\n    {\n        fit = fit.forFactors.bayesglm(v, z, lm=lm, ...);\n    } else\n    {\n        fit = fit.forFactors.glmnet(v, z, ...);\n    }\n    output$D = fit$coef;\n\n        if(var_v_fixed==FALSE){\n            output$var_v = (sum(fit$rss) + sum(v.sumvar)) / (nItems * nFactors);\n        } else{\n                if(use_nFactors_based_fixed_v_var==TRUE){\n                  output$var_v = C * 6/sqrt(nFactors);\n                } else{\n                        output$var_v = forever_fixed_v_var;\n                }\n                cat(\"var_v is fixed, = \",output$var_u,\"\\n\",sep=\"\");\n        }\n\n    return(output);\n}\n\nMC_MStep_logistic <- function(\n    user, item, y, x, w, z,\n    o, alpha, alpha.sumvar, beta, beta.sumvar, u, u.sumvar, v, v.sumvar,\n    debug=0, lm=F, use.glmnet=F, ...\n){\n    nObs     = length(y);\n    nUsers   = length(alpha);\n    nItems   = length(beta);\n    nFactors = ncol(u);\n\n    output = list();\n\n    # determine b and var_y\n    if(all(x == 0)){\n        fit = list(fitted.values=0, coefficients=rep(0,ncol(x)));\n    }else{\n        fit = glm(y ~ x -1, family=binomial(link = \"logit\"),offset=o, model=F);\n    }\n    if(length(fit$coef) != ncol(x)) stop(\"length(fit$coef) != ncol(x)\");\n\n    output$b = fit$coefficients;\n\n    cat(\"use.glmnet=\",use.glmnet,\"\\n\");\n    # determin g0 and var_alpha\n    if (use.glmnet==F)\n    {\n        fit = fit.forMainEffect.bayesglm(alpha, w, lm=lm, ...);\n    } else\n    {\n        fit = fit.forMainEffect.glmnet(alpha, w, ...);\n    }\n    output$g0 = fit$coef;\n    output$var_alpha = (sum(fit$rss) + alpha.sumvar) / nUsers;\n\n    # determin d0 and var_beta\n    if (use.glmnet==F)\n    {\n        fit = fit.forMainEffect.bayesglm(beta, z, lm=lm,...);\n    } else\n    {\n        fit = fit.forMainEffect.glmnet(beta, z, ...);\n    }\n    output$d0 = fit$coef;\n    output$var_beta = (sum(fit$rss) + beta.sumvar) / nItems;\n\n    # determin G and var_u\n    if (use.glmnet==F)\n    {\n        fit = fit.forFactors.bayesglm(u, w, lm=lm,...);\n    } else\n    {\n        fit = fit.forFactors.glmnet(u, w, ...);\n    }\n    output$G = fit$coef;\n    output$var_u = (sum(fit$rss) + sum(u.sumvar)) / (nUsers * nFactors);\n\n    # determin D and var_v\n    if (use.glmnet==F)\n    {\n        fit = fit.forFactors.bayesglm(v, z, lm=lm, ...);\n    } else\n    {\n        fit = fit.forFactors.glmnet(v, z, ...);\n    }\n    output$D = fit$coef;\n    output$var_v = (sum(fit$rss) + sum(v.sumvar)) / (nItems * nFactors);\n        cat(\"fit.rss=\",sum(fit$rss),\" v.sumvar=\",sum(v.sumvar),\" var_v=\",output$var_v,\"\\n\");\n    return(output);\n}\n\n\nMC_MStep_factoronly <- function(\n    user, item, y, x, w, z,\n    o, o.sumvar, alpha, alpha.sumvar, beta, beta.sumvar, u, u.sumvar, v, v.sumvar,\n    debug=0, lm=F, use.glmnet=F, ...\n){\n    nObs     = length(y);\n    nUsers   = length(alpha);\n    nItems   = length(beta);\n    nFactors = ncol(u);\n\n    output = list();\n\n    # determine b and var_y\n    target = y - o;\n    if(all(x == 0)){\n        fit = list(fitted.values=0, coefficients=rep(0,ncol(x)));\n    }else{\n        fit = lm(target ~ x -1, model=FALSE);\n    }\n    if(length(fit$coef) != ncol(x)) stop(\"length(fit$coef) != ncol(x)\");\n    rss = sum((fit$fitted.values - target)^2);\n\n    output$b = fit$coefficients;\n    output$var_y = (rss + o.sumvar) / nObs;\n\n    # determin G and var_u\n    if (use.glmnet==F)\n    {\n        fit = fit.forFactors.bayesglm(u, w, lm=lm,...);\n    } else\n    {\n        fit = fit.forFactors.glmnet(u, w, ...);\n    }\n    output$G = fit$coef;\n    output$var_u = (sum(fit$rss) + sum(u.sumvar)) / (nUsers * nFactors);\n\n    # determin D and var_v\n    if (use.glmnet==F)\n    {\n        fit = fit.forFactors.bayesglm(v, z, lm=lm, ...);\n    } else\n    {\n        fit = fit.forFactors.glmnet(v, z, ...);\n    }\n    output$D = fit$coef;\n    output$var_v = (sum(fit$rss) + sum(v.sumvar)) / (nItems * nFactors);\n\n    return(output);\n}\n\n# ICM (regression)\n#   See output$b, output$var_y, ...\nICM_Regression <- function(\n    user, item, y, x, w, z,\n    alpha, beta, u, v,\n    debug=0, lm=F,...\n){\n    nObs     = length(y);\n    nUsers   = length(alpha);\n    nItems   = length(beta);\n    nFactors = ncol(u);\n\n    output = list();\n\n    # determine b and var_y\n    target = y - alpha[user] - beta[item] - apply(u[user,,drop=FALSE] * v[item,,drop=FALSE], 1, sum);\n    if(all(x == 0)){\n        fit = list(fitted.values=0, coefficients=rep(0,ncol(x)));\n    }else{\n        fit = lm(target ~ x -1, model=FALSE);\n    }\n    if(length(fit$coef) != ncol(x)) stop(\"length(fit$coef) != ncol(x)\");\n    rss = sum((fit$fitted.values - target)^2);\n\n    output$b = fit$coefficients;\n    output$var_y = rss / nObs;\n\n    # determin g0 and var_alpha\n    fit = fit.forMainEffect.bayesglm(alpha, w,lm=lm, ...);\n    output$g0 = fit$coef;\n    output$var_alpha = sum(fit$rss) / nUsers;\n\n    # determin d0 and var_beta\n    fit = fit.forMainEffect.bayesglm(beta, z,lm=lm, ...);\n    output$d0 = fit$coef;\n    output$var_beta = sum(fit$rss) / nItems;\n\n    # determin G and var_u\n    fit = fit.forFactors.bayesglm(u, w, lm=lm,...);\n    output$G = fit$coef;\n    output$var_u = sum(fit$rss) / (nUsers * nFactors);\n\n    # determin D and var_v\n    fit = fit.forFactors.bayesglm(v, z, lm=lm,...);\n    output$D = fit$coef;\n    output$var_v = sum(fit$rss) / (nItems * nFactors);\n\n    return(output);\n}\n\nMC_MStep_logistic_arscid <- function(\n    user, item, y, x, b, w, z,\n    o, alpha, alpha.sumvar, beta, beta.sumvar, u, u.sumvar, v, v.sumvar,\n    debug=0, lm=F, use.glmnet=F, fit.ars.alpha=F, fit.regression=T,\n    beta.int=T, main.effects=F, ...\n){\n    nObs     = length(y);\n    nUsers   = length(alpha);\n    nItems   = length(beta);\n    nFactors = ncol(u);\n\n    output = list();\n\n    # find ars alpha ...\n    if (fit.ars.alpha )\n    {\n        ars_alpha = estalpha(y,b,o);\n        output$ars_alpha = ars_alpha;\n    }\n    else\n    {\n        output$ars_alpha = 0.5;\n        ars_alpha = 0.5;\n    }\n\n    # determine b and var_y\n    if(beta.int == F){\n      if(all(x == 0)){\n        fit = list(fitted.values=0, coefficients=rep(0,ncol(x)));\n      }else{\n        if(use.glmnet){\n            if (all(x[,1] == 1))\n            {\n                x.new = Matrix(x[,2:ncol(x)],sparse=TRUE);\n            } else {\n                stop(\"feature should have a constant term for intercept\");\n            }\n            fit0 = cv.glmnet(x=x.new, y=y, offset=o, nfolds=3, family=\"binomial\");\n            lambdaind = which(fit0$lambda==fit0$lambda.min);\n            a0 = fit0$glmnet.fit$a0[lambdaind];\n            coef = fit0$glmnet.fit$beta[,lambdaind];\n            fit = list();\n            fit$coefficients = as.vector(c(a0,coef));\n        } else {\n            # fit as covariate\n            nobs = length(x)\n            x = cbind(matrix(x,length(x),1), rep(0,length(x)))\n\n            fit = bayesglm(y ~ x - 1, family=binomial(link=\"logit\"), offset=o ,model=F, prior.scale = 5);\n\n            fit$coef = fit$coef[1]; fit$coefficients=fit$coefficients[1]; x = matrix(x[,1],nobs,1)\n        }\n      }\n      output$b = fit$coefficients;\n    } else {\n      #fit in random effects heirarchy inside of beta ... but still center for computation ...\n      output$b = b;\n    }\n\n    if (fit.ars.alpha && ncol(as.matrix(x))>1)\n    {\n        stop(\"Currently fit.ars.alpha=T only works for ncol(x)==1\");\n    }\n    cat(\"fit regression=\",fit.regression,\"\\n\")\n    cat(\"intercept in beta prior =\",beta.int,\"\\n\")\n\n    #If fit.regression=T do the normal thing, if not just find vars\n    if(fit.regression){\n      # determin g0 and var_alpha\n      if (use.glmnet==F  ) {\n          fit = fit.forMainEffect.bayesglm(alpha, w, lm=lm,...);\n      } else {\n        fit = fit.forMainEffect.glmnet(alpha, w, ...);\n      }\n      output$g0 = fit$coef;\n      output$var_alpha = (sum(fit$rss) + alpha.sumvar) / nUsers;\n\n      # determin d0 and var_beta ( and b if in heirarchy )\n      if (beta.int) z2 = cbind(1,z) else z2=z\n\n      if (use.glmnet==F) {\n          fit = fit.forMainEffect.bayesglm(beta, z2, lm=lm,...);\n      } else {\n        fit = fit.forMainEffect.glmnet(beta, z2, ...);\n      }\n\n      output$d0 = fit$coef;\n      output$var_beta = (sum(fit$rss) + beta.sumvar) / nItems;\n\n      if(!main.effects){\n                                        # determin G and var_u\n        if (use.glmnet==F)\n          {\n            fit = fit.forFactors.bayesglm(u, w, lm=lm,...);\n          } else\n        {\n          fit = fit.forFactors.glmnet(u, w, ...);\n        }\n        output$G = fit$coef;\n\n\t      # Whether or not identifiable==T, var_u should always be 1\n\t      output$var_u = rep(1,nFactors);\n\n        # determin D and var_v\n        if (use.glmnet==F)\n          {\n            fit = fit.forFactors.bayesglm(v, z, lm=lm, ...);\n          } else\n        {\n          fit = fit.forFactors.glmnet(v, z, ...);\n        }\n        output$D = fit$coef;\n        cat(\"var_v.rss=\",fit$rss,\"\\n\");\n        cat(\"v.sumvar=\",v.sumvar,\"\\n\");\n        output$var_v = (fit$rss + v.sumvar)/nItems;\n      }\n    } else\n    {\n      output$var_alpha = (sum(alpha^2) + alpha.sumvar) / nUsers;\n      output$var_beta = (sum(beta^2) + beta.sumvar) / nItems;\n      output$var_u = rep(1,nFactors);\n      if (!main.effects) output$var_v = (apply(v^2,2,sum)+v.sumvar)/nItems;\n    }\n    if( beta.int == T && fit.regression == F)\n    {\n        output$d0 = rep(0,dim(z)[2]); output$d0[1] = mean(beta);\n        output$var_alpha = (sum(alpha^2) + alpha.sumvar) / nUsers;\n        output$var_beta = (sum((beta - mean(beta))^2) + beta.sumvar) / nItems;\n        output$var_u = rep(1,nFactors);\n        if (!main.effects) output$var_v = (apply(v^2,2,sum)+v.sumvar)/nItems;\n    }\n\n    return(output);\n}\n" }
{ "repo_name": "jcbeer/fsgl", "ref": "refs/heads/master", "path": "FSGLSimulationStudy/scripts/s02_FSGLassoSimulation_TestDataPredictionError.R", "content": "####################################################################\n# Fused Sparse Group Lasso Simulation Study\n# Calculate Test Set Prediction Error\n# and Bias - Variance decomposition\n####################################################################\n# This script is used to analyze the simulation results\n# reported in the manuscript\n# \"Incorporating Prior Information with Fused Sparse Group Lasso:\n# Application to Prediction of Clinical Measures from Neuroimages\"\n### INPUTS: \n# 9 csv files that are simulation study results output from the script\n# s01_FSGLassoSimulation.R, named as follows: \n# compagg.csv, partagg.csv, compdist.csv,\n# spcompagg.csv, sppartagg.csv, spcompdist.csv, \n# exsp.csv, missp.csv, misspsp.csv\n### OUTPUTS:\n## augmented files for plotting and tables, named as follows:\n## original simulation statistics plus test set MSE\n# compaggplot.csv, partaggplot.csv, compdistplot.csv,\n# spcompaggplot.csv, sppartaggplot.csv, spcompdistplot.csv, \n# exspplot.csv, misspplot.csv, misspspplot.csv\n## bias - variance results\n# compaggcv.csv, partaggcv.csv, compdistcv.csv,\n# spcompaggcv.csv, sppartaggcv.csv, spcompdistcv.csv, \n# exspcv.csv, misspcv.csv, misspspcv.csv\n####################################################################\n\n########################################\n# 9 scenarios:\n# 1A. completely aggregated\n# 2B. partially aggregated\n# 3C. completely distributed\n# 4A. sparse group completely aggregated\n# 5B. sparse group parially aggregated\n# 6C. sparse group completely distributed\n# 7B. partially aggregated extra sparse\n# 8B. partially aggregated misspecified\n# 9B. partially aggregated misspecified sparse\n########################################\n\n########################################\n# LOAD DATASETS\n########################################\n# set the working and data directories\n# setwd()\n# datadir <- paste0(getwd(), '/MainSimulationResults/')\ncompagg <- read.csv(paste0(datadir, 'compagg.csv'))\npartagg <- read.csv(paste0(datadir, 'partagg.csv'))\ncompdist <- read.csv(paste0(datadir, 'compdist.csv'))\nspcompagg <- read.csv(paste0(datadir, 'spcompagg.csv'))\nsppartagg <- read.csv(paste0(datadir, 'sppartagg.csv'))\nspcompdist <- read.csv(paste0(datadir, 'spcompdist.csv'))\nexsp <- read.csv(paste0(datadir, 'exsp.csv'))\nmissp <- read.csv(paste0(datadir, 'missp.csv'))\nmisspsp <- read.csv(paste0(datadir, 'misspsp.csv'))\n\n# remove the first column (row names)\ncompagg <- compagg[,2:409]\npartagg <- partagg[,2:409] \ncompdist <- compdist[,2:409]\nspcompagg <- spcompagg[,2:409]\nsppartagg <- sppartagg[,2:409]\nspcompdist <- spcompdist[,2:409]\nexsp <- exsp[,2:409]\nmissp <- missp[,2:409]\nmisspsp <- misspsp[,2:409]\n\n####################################################################\n# CACLULATE ERROR ON TEST DATA\n####################################################################\n########################################\n# SET SOME SIMULATION PARAMETERS\n########################################\n# number of subjects\nn <- 50\n# 20*20 grid = 400 pixels\n# dim 1 is number of rows of image\ndim1 <- 20\n# dim 2 is number of columns of image\ndim2 <- 20\np <- dim1*dim2\n\n######################################################\n# DEFINE GROUP STRUCTURES\n# 16 groups of 25 pixels\n######################################################\n########################################\n### GROUP STRUCTURE A: Completely Aggregated\n# 16 groups of 5*5 blocks\n########################################\nGroupsA <- c(rep(c(rep(1, 5), rep(2, 5), rep(3, 5), rep(4, 5)), 5),\n            rep(c(rep(1, 5), rep(2, 5), rep(3, 5), rep(4, 5)), 5) + 4,\n            rep(c(rep(1, 5), rep(2, 5), rep(3, 5), rep(4, 5)), 5) + 8,\n            rep(c(rep(1, 5), rep(2, 5), rep(3, 5), rep(4, 5)), 5) + 12)\n########################################\n### GROUP STRUCTURE B: Partially Aggregated\n# 16 groups (1) 3*3 + (3) 2*2 + (2) 1*2 blocks\n########################################\nnumbers <- cbind(1:16, c(4:16, 1:3), c(7:16, 1:6), c(10:16, 1:9))\nnumbers.list <- apply(numbers, 1, list)\n# function to create subblock (5*5 square)\nsubblock <- function(x){\n  x <- unlist(x)\n  data <- c(rep(c(rep(x[1], 3), rep(x[2], 2)), 2), rep(x[1], 3), rep(x[4], 2), rep(c(rep(x[2], 2), x[4], rep(x[3], 2)), 2))\n  return(matrix(data, nrow=5, byrow=TRUE))\n}\n# create subblocks\nsubblocks <- lapply(numbers.list, subblock)\n# combine subblocks in rows\nrow1 <- cbind(subblocks[[1]], subblocks[[2]], subblocks[[3]], subblocks[[4]])\nrow2 <- cbind(subblocks[[5]], subblocks[[6]], subblocks[[7]], subblocks[[8]])\nrow3 <- cbind(subblocks[[9]], subblocks[[10]], subblocks[[11]], subblocks[[12]])\nrow4 <- cbind(subblocks[[13]], subblocks[[14]], subblocks[[15]], subblocks[[16]])\nGroupsB <- as.vector(rbind(row1, row2, row3, row4))\n# remove stuff we don't need\nrm('row1', 'row2', 'row3', 'row4', 'numbers', 'numbers.list', 'subblocks')\n########################################\n#### GROUP STRUCTURE C: Completely Distributed\n# 16 groups distributed in 1*1 blocks\n########################################\nGroupsC <- c(rep(1:16, 25))\n########################################\n# OPTIONAL STEP: Visualize the groups\n########################################\n# library(fields)\n# image.plot(matrix(GroupsA, nrow=20, byrow=TRUE), main='Group Structure')\n# image.plot(matrix(GroupsB, nrow=20, byrow=TRUE), main='Group Structure')\n# image.plot(matrix(GroupsC, nrow=20, byrow=TRUE), main='Group Structure')\n\n######################################################\n# DEFINE TRUE COEFFICIENTS\n######################################################\n# set coefficient magnitude\nbeta.value <- 3  \n########################################\n### GROUP STRUCTURE A: Completely Aggregated\n# 16 groups of 5*5 blocks\n########################################\n### TRUE COEFFICIENTS 1A: Complete Group\ntrueBetas1A <- as.numeric(GroupsA == 7)*beta.value\n### TRUE COEFFICIENTS 4A: Sparse Group \ntrueBetas4A <- as.numeric(GroupsA == 7)*beta.value\ntrueBetas4A[c(113, 114, 115, 133, 134, 135, 154, 155, 175, 195)] <- 0\n########################################\n### GROUP STRUCTURE B: Partially Aggregated\n# 16 groups (1) 3*3 + (3) 2*2 + (2) 1*2 blocks\n########################################\n### TRUE COEFFICIENTS 2B: Complete Group\ntrueBetas2B <- as.numeric(GroupsB == 10)*beta.value\n### TRUE COEFFICIENTS 5B: Sparse Group\ntrueBetas5B <- as.numeric(GroupsB == 10)*beta.value\ntrueBetas5B[c(44, 45, 209, 210, 229, 230, 364, 365, 384, 385)] <- 0\n### TRUE COEFFICIENTS 7B: Extra Sparse Group \ntrueBetas7B <- as.numeric(GroupsB == 10)*beta.value\ntrueBetas7B[c(44, 45, 111, 112, 113, 131, 132, 133, 151, 152, 153, 209, 210, 229, 230, 267, 364, 365, 384, 385)] <- 0\n### TRUE COEFFICIENTS 8B: Misspecified Group \ntrueBetas8B <- as.numeric(GroupsB == 10)*beta.value\nrotate <- function(x) t(apply(x, 2, rev))\ntrueBetas8B <- as.vector(rotate(matrix(trueBetas8B, nrow=20)))\n### TRUE COEFFICIENTS 9B: Misspecified Sparse Group \ntrueBetas9B <- as.numeric(GroupsB == 10)*beta.value\ntrueBetas9B[c(44, 45, 209, 210, 229, 230, 364, 365, 384, 385)] <- 0\nrotate <- function(x) t(apply(x, 2, rev))\ntrueBetas9B <- as.vector(rotate(matrix(trueBetas9B, nrow=20)))\n########################################\n#### GROUP STRUCTURE C: Completely Distributed\n# 16 groups distributed in 1*1 blocks\n########################################\n### TRUE COEFFICIENTS 3C: Complete Group\ntrueBetas3C <- as.numeric(GroupsC == 7)*beta.value\n### TRUE COEFFICIENTS 6C: Sparse Group\ntrueBetas6C <- as.numeric(GroupsC == 7)*beta.value\nset.seed(1)\ntrueBetas6C[which(trueBetas6C == 3)[sample(1:25, 10)]] <- 0\n########################################\n# OPTIONAL STEP: Visualize the true coefficients\n########################################\n# image.plot(matrix(trueBetas1A, nrow=20, byrow=TRUE), main='True Coefficients')\n# image.plot(matrix(trueBetas2B, nrow=20, byrow=TRUE), main='True Coefficients')\n# image.plot(matrix(trueBetas3C, nrow=20, byrow=TRUE), main='True Coefficients')\n# image.plot(matrix(trueBetas4A, nrow=20, byrow=TRUE), main='True Coefficients')\n# image.plot(matrix(trueBetas5B, nrow=20, byrow=TRUE), main='True Coefficients')\n# image.plot(matrix(trueBetas6C, nrow=20, byrow=TRUE), main='True Coefficients')\n# image.plot(matrix(trueBetas7B, nrow=20, byrow=TRUE), main='True Coefficients')\n# image.plot(matrix(trueBetas8B, nrow=20, byrow=TRUE), main='True Coefficients')\n# image.plot(matrix(trueBetas9B, nrow=20, byrow=TRUE), main='True Coefficients')\n\n######################################################\n# CACLULATE ERROR ON TEST DATA\n######################################################\n########################################\n# function to calculate prediction error \n# on test set for each seed\n########################################\nprederr <- function(simdata, trueBetas){\n  n <- 50\n  p <- 400\n  simdata$mse.y.test <- rep(NA, 2100)\n  for (seed in 101:200){\n    set.seed(seed)\n    X <- scale(matrix(rnorm(n*p), nrow=n, ncol=p), center=TRUE, scale=FALSE)\n    epsilon <- rnorm(n, mean=0, sd=2)\n    y.test <- X %*% trueBetas + epsilon\n    # get the subset of 21 rows corresponding to a given seed\n    betahat.set <- t(simdata[simdata$seed==(seed - 100), 9:408])\n    yhat.test <- X %*% betahat.set\n    # calculate msey.test\n    simdata$mse.y.test[simdata$seed==(seed - 100)] <- colMeans(sweep(yhat.test, 1, y.test)^2)\n  }\n  return(simdata)\n}\n\n########################################\n# CALCULATE TEST MEAN SQUARED ERROR\n# for each set of true coefficients\n########################################\ncompagg2 <- prederr(compagg, trueBetas1A)\nspcompagg2 <- prederr(spcompagg, trueBetas4A)\npartagg2 <- prederr(partagg, trueBetas2B)\nsppartagg2 <- prederr(sppartagg, trueBetas5B)\ncompdist2 <- prederr(compdist, trueBetas3C)\nspcompdist2 <- prederr(spcompdist, trueBetas6C)\nexsp2 <- prederr(exsp, trueBetas7B)\nmissp2 <- prederr(missp, trueBetas8B)\nmisspsp2 <- prederr(misspsp, trueBetas9B)\n\n########################################\n# save stats output data for making tables and plots\n########################################\nwrite.csv(compagg2[,c(1:8,409)], 'compaggplot.csv', row.names=FALSE)\nwrite.csv(spcompagg2[,c(1:8,409)], 'spcompaggplot.csv', row.names=FALSE)\nwrite.csv(partagg2[,c(1:8,409)], 'partaggplot.csv', row.names=FALSE)\nwrite.csv(sppartagg2[,c(1:8,409)], 'sppartaggplot.csv', row.names=FALSE)\nwrite.csv(compdist2[,c(1:8,409)], 'compdistplot.csv', row.names=FALSE)\nwrite.csv(spcompdist2[,c(1:8,409)], 'spcompdistplot.csv', row.names=FALSE)\nwrite.csv(exsp2[,c(1:8,409)], 'exspplot.csv', row.names=FALSE)\nwrite.csv(missp2[,c(1:8,409)], 'misspplot.csv', row.names=FALSE)\nwrite.csv(misspsp2[,c(1:8,409)], 'misspspplot.csv', row.names=FALSE)\n\n####################################################################\n# ESTIMATE BIAS-VARIANCE DECOMPOSITION OF MSE\n# for 100 new observations\n####################################################################\n########################################\n# function to estimate bias-variance\n# decomposition of mse\n# for 100 new observations\n########################################\nbiasvar <- function(simdata, trueBetas){\n  set.seed(201)\n  n <- 100\n  p <- 400\n  X <- scale(matrix(rnorm(n*p), nrow=n, ncol=p), center=TRUE, scale=FALSE)\n  testdata <- data.frame(id=1:n)\n  testdata$X.beta <- X %*% trueBetas\n  # get alpha / gamma values and estimated betas\n  betahat <- simdata[,c(2:3,9:408)]\n  # sort betahat by the alpha / gamma values\n  betahat.sorted <- betahat[order(betahat$alpha, betahat$gamma),]\n  yhat.test <- X %*% t(betahat.sorted[,3:402])\n  # for each alpha / gamma combination\n  # and each observation\n  # calculate variance of the predicted value\n  # also calculate estimated bias and bias squared\n  alphagamma <- betahat[1:21,1:2]\n  for (i in 1:21){\n    predvals <- yhat.test[,(100*i - 99):(100*i)]\n    testdata[[paste0('yhat.var.alpha', alphagamma[i,1] ,'gamma', alphagamma[i,2])]] <- apply(predvals, 1, var)\n    testdata[[paste0('yhat.bias.alpha', alphagamma[i,1] ,'gamma', alphagamma[i,2])]] <- (rowMeans(predvals) - testdata$X.beta)\n    testdata[[paste0('yhat.bias2.alpha', alphagamma[i,1] ,'gamma', alphagamma[i,2])]] <- (rowMeans(predvals) - testdata$X.beta)^2\n    testdata[[paste0('yhat.mse.alpha', alphagamma[i,1] ,'gamma', alphagamma[i,2])]] <- apply(cbind(testdata$X.beta, predvals), 1, function(x) mean((x[2:101] - (rnorm(100, mean=0, sd=2) + x[1]))^2))\n  }\n  return(testdata)\n}\n\ncompaggbv <- biasvar(compagg, trueBetas1A)\nspcompaggbv <- biasvar(spcompagg, trueBetas4A)\npartaggbv <- biasvar(partagg, trueBetas2B)\nsppartaggbv <- biasvar(sppartagg, trueBetas5B)\ncompdistbv <- biasvar(compdist, trueBetas3C)\nspcompdistbv <- biasvar(spcompdist, trueBetas6C)\nexspbv <- biasvar(exsp, trueBetas7B)\nmisspbv <- biasvar(missp, trueBetas8B)\nmisspspbv <- biasvar(misspsp, trueBetas9B)\n\n########################################\n# save output data for making tables and plots\n########################################\nwrite.csv(compaggbv, 'compaggbv.csv', row.names=FALSE)\nwrite.csv(spcompaggbv, 'spcompaggbv.csv', row.names=FALSE)\nwrite.csv(partaggbv, 'partaggbv.csv', row.names=FALSE)\nwrite.csv(sppartaggbv, 'sppartaggbv.csv', row.names=FALSE)\nwrite.csv(compdistbv, 'compdistbv.csv', row.names=FALSE)\nwrite.csv(spcompdistbv, 'spcompdistbv.csv', row.names=FALSE)\nwrite.csv(exspbv, 'exspbv.csv', row.names=FALSE)\nwrite.csv(misspbv, 'misspbv.csv', row.names=FALSE)\nwrite.csv(misspspbv, 'misspspbv.csv', row.names=FALSE)\n" }
{ "repo_name": "Oshlack/splatter", "ref": "refs/heads/master", "path": "R/lun-simulate.R", "content": "#' Lun simulation\n#'\n#' Simulate single-cell RNA-seq count data using the method described in Lun,\n#' Bach and Marioni \"Pooling across cells to normalize single-cell RNA\n#' sequencing data with many zero counts\".\n#'\n#' @param params LunParams object containing Lun simulation parameters.\n#' @param verbose logical. Whether to print progress messages.\n#' @param sparsify logical. Whether to automatically convert assays to sparse\n#'        matrices if there will be a size reduction.\n#' @param ... any additional parameter settings to override what is provided in\n#'        \\code{params}.\n#'\n#' @details\n#' The Lun simulation generates gene mean expression levels from a gamma\n#' distribution with \\code{shape = mean.shape} and \\code{rate = mean.rate}.\n#' Counts are then simulated from a negative binomial distribution with\n#' \\code{mu = means} and \\code{size = 1 / bcv.common}. In addition each cell is\n#' given a size factor (\\code{2 ^ rnorm(nCells, mean = 0, sd = 0.5)}) and\n#' differential expression can be simulated with fixed fold changes.\n#'\n#' See \\code{\\link{LunParams}} for details of the parameters.\n#'\n#' @return SingleCellExperiment object containing the simulated counts and\n#' intermediate values.\n#'\n#' @references\n#' Lun ATL, Bach K, Marioni JC. Pooling across cells to normalize single-cell\n#' RNA sequencing data with many zero counts. Genome Biology (2016).\n#'\n#' Paper: \\url{dx.doi.org/10.1186/s13059-016-0947-7}\n#'\n#' Code: \\url{https://github.com/MarioniLab/Deconvolution2016}\n#'\n#' @examples\n#' sim <- lunSimulate()\n#'\n#' @importFrom SummarizedExperiment rowData rowData<- colData colData<-\n#' @importFrom SingleCellExperiment SingleCellExperiment\n#' @importFrom stats rnorm rgamma rnbinom\n#' @export\nlunSimulate <- function(params = newLunParams(), sparsify = TRUE,\n                        verbose = TRUE, ...) {\n\n    checkmate::assertClass(params, \"LunParams\")\n\n    if (verbose) {message(\"Getting parameters...\")}\n    params <- setParams(params, ...)\n    params <- expandParams(params)\n\n    # Set random seed\n    seed <- getParam(params, \"seed\")\n    set.seed(seed)\n\n    # Get the parameters we are going to use\n    nGenes <- getParam(params, \"nGenes\")\n    nCells <- getParam(params, \"nCells\")\n    nGroups <- getParam(params, \"nGroups\")\n    groupCells <- getParam(params, \"groupCells\")\n    mean.shape <- getParam(params, \"mean.shape\")\n    mean.rate <- getParam(params, \"mean.rate\")\n    count.disp <- getParam(params, \"count.disp\")\n    de.nGenes <- getParam(params, \"de.nGenes\")\n    de.upProp <- getParam(params, \"de.upProp\")\n    de.upFC <- getParam(params, \"de.upFC\")\n    de.downFC <- getParam(params, \"de.downFC\")\n\n    cell.names <- paste0(\"Cell\", seq_len(nCells))\n    gene.names <- paste0(\"Gene\", seq_len(nGenes))\n\n    if (verbose) {message(\"Simulating means...\")}\n    gene.means <- rgamma(nGenes, shape = mean.shape, rate = mean.rate)\n\n    if (verbose) {message(\"Simulating cell means...\")}\n    if (nGroups == 1) {\n        cell.facs <- 2 ^ rnorm(nCells, sd = 0.5)\n        cell.means <- outer(gene.means, cell.facs, \"*\")\n    } else {\n        groups <- list()\n        cell.facs <- list()\n        de.facs <- list()\n        cell.means <- list()\n        for (idx in seq_len(nGroups)) {\n            groups[[idx]] <- rep(paste0(\"Group\", idx), groupCells[idx])\n\n            cell.facs.group <- 2 ^ rnorm(groupCells[idx], sd = 0.5)\n            cell.facs[[idx]] <- cell.facs.group\n\n            chosen <- de.nGenes[idx] * (idx - 1) + seq_len(de.nGenes[idx])\n            is.up <- seq_len(de.nGenes[idx] * de.upProp[idx])\n            de.up <- chosen[is.up]\n            de.down <- chosen[-is.up]\n\n            de.facs.group <- rep(1, nGenes)\n            de.facs.group[de.up] <- de.upFC[idx]\n            de.facs.group[de.down] <- de.downFC[idx]\n            de.facs[[idx]] <- de.facs.group\n\n            cell.means.group <- outer(gene.means, cell.facs.group)\n            cell.means.group <- cell.means.group * de.facs.group\n            cell.means[[idx]] <- cell.means.group\n        }\n        cell.means <- do.call(cbind, cell.means)\n        cell.facs <- unlist(cell.facs)\n        groups <- unlist(groups)\n    }\n    colnames(cell.means) <- cell.names\n    rownames(cell.means) <- gene.names\n\n    if (verbose) {message(\"Simulating counts...\")}\n    counts <- matrix(rnbinom(\n            as.numeric(nGenes) * as.numeric(nCells),\n            mu = cell.means, size = 1 / count.disp\n        ),\n    nrow = nGenes, ncol = nCells)\n\n    if (verbose) {message(\"Creating final dataset...\")}\n    rownames(counts) <- gene.names\n    colnames(counts) <- cell.names\n\n    cells <- data.frame(Cell = cell.names, CellFac = cell.facs)\n    rownames(cells) <- cell.names\n\n    features <- data.frame(Gene = gene.names, GeneMean = gene.means)\n    rownames(features) <- gene.names\n\n    if (nGroups > 1) {\n        cells$Group <- groups\n        for (idx in seq_along(de.facs)) {\n            features[[paste0(\"DEFacGroup\", idx)]] <- de.facs[[idx]]\n            features[[paste0(\"GeneMeanGroup\", idx)]] <- gene.means *\n                de.facs[[idx]]\n        }\n    }\n\n    sim <- SingleCellExperiment(assays = list(counts = counts,\n                                              CellMeans = cell.means),\n                                rowData = features,\n                                colData = cells,\n                                metadata = list(Params = params))\n\n    if (sparsify) {\n        if (verbose) {message(\"Sparsifying assays...\")}\n        assays(sim) <- sparsifyMatrices(assays(sim), auto = TRUE,\n                                        verbose = verbose)\n    }\n\n    if (verbose) {message(\"Done!\")}\n\n    return(sim)\n}\n" }
{ "repo_name": "mc30/wasp", "ref": "refs/heads/master", "path": "R/utils.R", "content": "###############################################\n# Auxillary functions\n###############################################\n\n\n#' @title Get colors for numeric vector values\n#' @description Function to convert numbers into colors.\n#'\n#' @param x A numeric vector of values.\n#' @param colors A vector of colors for the palette.\n#' @param range A range of values for color picking.\n#' \n#' @return Returns a vector with colors.\n#'\n#' @author Mikhail Churakov (\\email{mikhail.churakov@@pasteur.fr}).\n#' \n#' @export\ngetColorsFromNumbers <- function(x, colors = c(\"blue\", \"white\", \"red\"), range = range(x)) {\n  mColors <- colorRamp(colors)((x - min(range)) / (max(range) - min(range)))\n  return(rgb(mColors[, 1] / 255, mColors[, 2] / 255, mColors[, 3] / 255))\n}" }
{ "repo_name": "swager/grf", "ref": "refs/heads/master", "path": "r-package/grf/R/custom_forest.R", "content": "#' Custom forest\n#' \n#' Trains a custom forest model.\n#'\n#' @param X The covariates used in the regression.\n#' @param Y The outcome.\n#' @param sample.fraction Fraction of the data used to build each tree.\n#'                        Note: If honesty = TRUE, these subsamples will\n#'                        further be cut by a factor of honesty.fraction.\n#' @param mtry Number of variables tried for each split.\n#' @param num.trees Number of trees grown in the forest. Note: Getting accurate\n#'                  confidence intervals generally requires more trees than\n#'                  getting accurate predictions.\n#' @param num.threads Number of threads used in training. If set to NULL, the software\n#'                    automatically selects an appropriate amount.\n#' @param min.node.size A target for the minimum number of observations in each tree leaf. Note that nodes\n#'                      with size smaller than min.node.size can occur, as in the original randomForest package.\n#' @param honesty Whether to use honest splitting (i.e., sub-sample splitting).\n#' @param honesty.fraction The fraction of data that will be used for determining splits if honesty = TRUE. Corresponds \n#'                         to set J1 in the notation of the paper. When using the defaults (honesty = TRUE and \n#'                         honesty.fraction = NULL), half of the data will be used for determining splits\n#' @param alpha A tuning parameter that controls the maximum imbalance of a split.\n#' @param imbalance.penalty A tuning parameter that controls how harshly imbalanced splits are penalized.\n#' @param seed The seed for the C++ random number generator.\n#' @param clusters Vector of integers or factors specifying which cluster each observation corresponds to.\n#' @param samples_per_cluster If sampling by cluster, the number of observations to be sampled from\n#'                            each cluster when training a tree. If NULL, we set samples_per_cluster to the size\n#'                            of the smallest cluster. If some clusters are smaller than samples_per_cluster,\n#'                            the whole cluster is used every time the cluster is drawn. Note that\n#'                            clusters with less than samples_per_cluster observations get relatively\n#'                            smaller weight than others in training the forest, i.e., the contribution\n#'                            of a given cluster to the final forest scales with the minimum of\n#'                            the number of observations in the cluster and samples_per_cluster.\n#'\n#' @return A trained regression forest object.\n#'\n#' @examples \\dontrun{\n#' # Train a custom forest.\n#' n = 50; p = 10\n#' X = matrix(rnorm(n*p), n, p)\n#' Y = X[,1] * rnorm(n)\n#' c.forest = custom_forest(X, Y)\n#'\n#' # Predict using the forest.\n#' X.test = matrix(0, 101, p)\n#' X.test[,1] = seq(-2, 2, length.out = 101)\n#' c.pred = predict(c.forest, X.test)\n#' }\n#'\n#' @export\ncustom_forest <- function(X, Y, sample.fraction = 0.5, mtry = NULL, \n    num.trees = 2000, num.threads = NULL, min.node.size = NULL, honesty = TRUE,\n    honesty.fraction = NULL, alpha = 0.05, imbalance.penalty = 0.0, seed = NULL,\n    clusters = NULL, samples_per_cluster = NULL) {\n\n    validate_X(X)\n    validate_observations(Y, X)\n    \n    mtry <- validate_mtry(mtry, X)\n    num.threads <- validate_num_threads(num.threads)\n    min.node.size <- validate_min_node_size(min.node.size)\n    sample.fraction <- validate_sample_fraction(sample.fraction)\n    seed <- validate_seed(seed)\n    clusters <- validate_clusters(clusters, X)\n    samples_per_cluster <- validate_samples_per_cluster(samples_per_cluster, clusters)\n    honesty.fraction <- validate_honesty_fraction(honesty.fraction, honesty)\n    \n    no.split.variables <- numeric(0)\n    \n    data <- create_data_matrices(X, Y)\n    outcome.index <- ncol(X) + 1\n    ci.group.size <- 1\n\n    forest <- custom_train(data$default, data$sparse, outcome.index, mtry,num.trees, num.threads,\n        min.node.size, sample.fraction, seed, honesty, coerce_honesty_fraction(honesty.fraction),\n        ci.group.size, alpha, imbalance.penalty, clusters, samples_per_cluster)\n    \n    forest[[\"X.orig\"]] <- X\n    forest[[\"Y.orig\"]] <- Y\n\n    class(forest) <- c(\"custom_forest\", \"grf\")\n    forest\n}\n\n#' Predict with a custom forest.\n#'\n#' @param object The trained forest.\n#' @param newdata Points at which predictions should be made. If NULL, makes out-of-bag\n#'                predictions on the training set instead (i.e., provides predictions at\n#'                Xi using only trees that did not use the i-th training example). Note\n#'                that this matrix should have the number of columns as the training\n#'                matrix, and that the columns must appear in the same order.\n#' @param num.threads Number of threads used in training. If set to NULL, the software\n#'                    automatically selects an appropriate amount.\n#' @param ... Additional arguments (currently ignored).\n#'\n#' @return Vector of predictions.\n#'\n#' @examples \\dontrun{\n#' # Train a custom forest.\n#' n = 50; p = 10\n#' X = matrix(rnorm(n*p), n, p)\n#' Y = X[,1] * rnorm(n)\n#' c.forest = custom_forest(X, Y)\n#'\n#' # Predict using the forest.\n#' X.test = matrix(0, 101, p)\n#' X.test[,1] = seq(-2, 2, length.out = 101)\n#' c.pred = predict(c.forest, X.test)\n#' }\n#'\n#' @method predict custom_forest\n#' @export\npredict.custom_forest <- function(object, newdata = NULL, num.threads = NULL, ...) {        \n    forest.short <- object[-which(names(object) == \"X.orig\")]\n\n    X <- object[[\"X.orig\"]]\n    train.data <- create_data_matrices(X, object[[\"Y.orig\"]])\n    outcome.index <- ncol(X) + 1\n\n    num.threads <- validate_num_threads(num.threads)\n\n    if (!is.null(newdata)) {\n        validate_newdata(newdata, X)\n        data <- create_data_matrices(newdata)\n        custom_predict(forest.short, train.data$default, train.data$sparse, outcome.index,\n            data$default, data$sparse, num.threads)\n    } else {\n        custom_predict_oob(forest.short, train.data$default, train.data$sparse, outcome.index, num.threads)\n    }\n}\n" }
{ "repo_name": "michaelquinn32/bpoR", "ref": "refs/heads/master", "path": "Code/final-describe.R", "content": "#       File: final-describe.R\n#       Author: Michael Quinn\n#       Date Created: February 4, 2014\n#       \n#       Summary:\n#                   This is the accompanying code for a paper submitted to the \n#                    Central Asia Business Journal.\n#                   It is a self-contained document and downloads all needed data from the internet\n#                   See abstract for more information\n#\n#                   This program generates descriptive statistics for the stock data\n##########################################\n\n# Descriptive Stats last six months and test portfolio\nend <- 1\nstart <- end + 23\n\nss <- port.hist %>% \n    extract2(\"R\") %>% \n    extract(start:end, ) \n\nfuns <- c(\"Asset\", \"Means\", \"StDev\", \"Total\", \"N\")\nc_summ_hist <- adply(ss, 2, each(mean, sd, sum, length)) %>% setNames(funs)\nattach(c_summ_hist)\nc_summ_new <- port.test$R %>% adply(2, each(mean, sd, sum, length)) %>% setNames(funs)\nCov <- cov(ss)\nCov.New <- cov(port.test$R)\n\n# Plot of stock returns\n##############################\n# Get the market returns\nmarket <- baseline.hist %>% extract2(\"R\") %>% extract(start:end,) %>% c(1,.) %>% cumsum()\n\n# Get the quote dates\nhist.dates <- port.hist %>% extract2(\"R\") %>% rownames() %>% extract((start + 1) : end) %>%\n    as.Date()\n\n# Get the portfolio returs\nreturn.out <- port.hist %>% extract2(\"R\") %>% \n    extract(start:end,) %>% rbind(1, .) %>% apply(2, cumsum)\n\n# Put together for plotting\nhist.plot <- data.frame(SP500 = market, return.out, Date=hist.dates) %>% melt(id = \"Date\")\n\n# Make Plot\np.returns <- ggplot(hist.plot,aes(x=Date,y=value)) +\n    geom_line(aes(group=variable,colour=variable)) + \n    ggtitle(\"Cumulative Returns, Previous 3 Months\") +\n    labs(y=\"Cumulative Return\") + \n    scale_colour_discrete(name=\"Ticker Name\") +\n    theme(legend.position=\"bottom\") + \n    guides(col = guide_legend(nrow = 2)) \n\nprint(p.returns)\n\n# Basic Stats, historical\nstocks.table <- data.frame(\"Annualized Return\" =  52 * c_summ_hist$Means * 100, \n    \"Annualized St. Dev.\" = sqrt(52) * c_summ_hist$StDev * 100, \n    \"Sharpe Ratio\" = 52 * c_summ_hist$Means /(sqrt(52)* c_summ_hist$StDev),\n    check.names = FALSE, row.names = c_summ_hist$Asset) %>% \n    t\n\n# Basic Stats, Test port\n# Looking forward to get the means\ntest.table = data.frame(\"Annualized Return\" =  52 * c_summ_new$Means * 100, \n    \"Annualized St. Dev.\" = sqrt(52) * c_summ_new$StDev * 100,\n    \"Sharpe Ratio\" = 52 * c_summ_new$Means /(sqrt(52)* c_summ_new$StDev),\n    check.names = FALSE, row.names = c_summ_new$Asset) %>% \n    t\n" }
{ "repo_name": "wikimedia-research/Experiportal", "ref": "refs/heads/master", "path": "Analyses/Post-Deployment (2016-03-18)/data.R", "content": "start_date <- as.Date(\"2016-01-17\") # as.Date(\"2016-03-01\")\nend_date <- Sys.Date()-1\nevents <- do.call(rbind, lapply(seq(start_date, end_date, \"day\"), function(date) {\n  cat(\"Fetching Portal EL data from \", as.character(date), \"\\n\")\n  data <- wmf::build_query(\"SELECT LEFT(timestamp, 8) AS date,\n                            event_session_id AS session,\n                            event_destination AS destination,\n                            event_event_type AS type,\n                            event_section_used AS section_used,\n                            timestamp AS ts,\n                            userAgent AS user_agent\",\n                           date = date,\n                           table = \"WikipediaPortal_14377354\",\n                           conditionals = \"((event_cohort IS NULL) OR (event_cohort IN ('null','baseline')))\")\n  return(data)\n}))\nlibrary(magrittr)\nevents$date %<>% lubridate::ymd()\nevents$ts %<>% lubridate::ymd_hms()\nevents <- events[order(events$date, events$session, events$ts), ]\nevents <- events[!duplicated(events[, c(\"session\", \"type\")], fromLast = TRUE), ]\nevents$portal <- ifelse(events$date < \"2016-03-10\", \"old\", \"new\")\nreadr::write_tsv(events, \"portal_events.tsv\", append = file.exists(\"portal_events.tsv\"))\nq(save = \"no\")\n" }
{ "repo_name": "haziqjamil/iprior", "ref": "refs/heads/master", "path": "R/Data.R", "content": "################################################################################\n#\n#   iprior: Linear Regression using I-priors\n#   Copyright (C) 2018  Haziq Jamil\n#\n#   This program is free software: you can redistribute it and/or modify\n#   it under the terms of the GNU General Public License as published by\n#   the Free Software Foundation, either version 3 of the License, or\n#   (at your option) any later version.\n#\n#   This program is distributed in the hope that it will be useful,\n#   but WITHOUT ANY WARRANTY; without even the implied warranty of\n#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#   GNU General Public License for more details.\n#\n#   You should have received a copy of the GNU General Public License\n#   along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n################################################################################\n\n#' Generate simulated data for smoothing models\n#'\n#' @param n Sample size.\n#' @param seed (Optional) Random seed.\n#' @param x.jitter A small amount of jitter is added to the \\code{X} variables\n#'   generated from a normal distribution with mean zero and standard deviation\n#'   equal to \\code{x.jitter}.\n#' @param xlim Limits of the \\code{X} variables to generate from.\n#'\n#' @return A dataframe containing the response variable \\code{y} and\n#'   unidimensional explanatory variable \\code{X}.\n#'\n#' @examples\n#' gen_smooth(10)\n#'\n#' @export\ngen_smooth <- function(n = 150, xlim = c(0.2, 4.6), x.jitter = 0.65,\n                       seed = NULL) {\n  if (!is.null(seed)) set.seed(seed)\n  f <- function(x, truth = FALSE) {\n    35 * dnorm(x, mean = 1, sd = 0.8) +\n      65 * dnorm(x, mean = 4, sd = 1.5) +\n      (x > 4.5) * (exp((1.25 * (x - 4.5))) - 1) +\n      3 * dnorm(x, mean = 2.5, sd = 0.3)\n  }\n  x <- c(seq(xlim[1], 1.9, length = n * 5 / 8),\n         seq(3.7, xlim[2], length = n * 3 / 8))\n  x <- sample(x, size = n)\n  x <- x + rnorm(n, sd = x.jitter)  # adding random fluctuation to the x\n  x <- sort(x)\n  y.err <- rt(n, df = 1)\n  y <- f(x) + sign(y.err) * pmin(abs(y.err), rnorm(n, mean = 4.1))  # adding random\n  data.frame(y = y, X = x)\n}\n\n#' Generate simulated data for multilevel models\n#'\n#' @param n Sample size. Input either a single number for a balanced data set,\n#'   or a vector of length \\code{m} indicating the sample size in each group.\n#' @param seed (Optional) Random seed.\n#' @param m Number of groups/levels.\n#' @param sigma_e The standard deviation of the errors.\n#' @param sigma_u0 The standard deviation of the random intercept.\n#' @param sigma_u1 The standard deviation of the random slopes.\n#' @param sigma_u01 The covariance of between the random intercept and the\n#'   random slope.\n#' @param beta0 The mean of the random intercept.\n#' @param beta1 The mean of the random slope.\n#' @param x.jitter A small amount of jitter is added to the \\code{X} variables\n#'   generated from a normal distribution with mean zero and standard deviation\n#'   equal to \\code{x.jitter}.\n#'\n#' @return A dataframe containing the response variable \\code{y}, the\n#'   unidimensional explanatory variables \\code{X}, and the levels/groups\n#'   (factors).\n#'\n#' @examples\n#' gen_multilevel()\n#'\n#' @export\ngen_multilevel <- function(n = 25, m = 6, sigma_e = 2, sigma_u0 = 2,\n                           sigma_u1 = 2, sigma_u01 = -2, beta0 = 0, beta1 = 2,\n                           x.jitter = 0.5, seed = NULL) {\n  # Generates a data set according to the model \\deqn{y_{ij} = \\beta_{0j} +\n  # \\beta{1j}X_{ij} + \\epsilon_{ij}} \\deqn{\\beta_{0j} \\sim \\text{N}(0,\n  # \\sigma_{u0}^2)} \\deqn{\\beta_{1j} \\sim \\text{N}(0, \\sigma_{u1}^2)}\n  # \\deqn{\\text{Cov}(\\beta_{0j}, \\beta_{1j}) = \\sigma_{u01}} with\n  # \\eqn{i=1,\\dots,n_j}  samples and \\eqn{j=1,\\dots,m} groups.\n  if (!is.null(seed)) set.seed(seed)\n  beta <- mvtnorm::rmvnorm(m, c(beta0, beta1),\n                           sigma = matrix(c(sigma_u0 ^ 2, sigma_u01,\n                                            sigma_u01, sigma_u1 ^ 2), nrow = 2))\n  if (length(n) == 1) {\n    n <- rep(n, m)\n  } else if (length(n) != m) {\n    stop(\"n must have length equal to the number of groups (m).\", call. = FALSE)\n  }\n  dat <- as.data.frame(matrix(NA, nrow = sum(n), ncol = 3))\n  n.cum <- c(0, cumsum(n))\n  for (j in seq_len(m)) {\n    x <- seq(0, 5, length = n[j]) + rnorm(n[j], sd = x.jitter)\n    y <- beta[j, 1] + beta[j, 2] * x + rnorm(n[j], sd = sigma_e)\n    dat[(n.cum[j] + 1):n.cum[j + 1], ] <- cbind(y, x, j)\n  }\n  names(dat) <- c(\"y\", \"X\", \"grp\")\n  dat$grp <- factor(dat$grp)\n  dat\n}\n\n#' High school and beyond dataset\n#'\n#' A national longitudinal survey of of students from public and private high\n#' schools in the United States, with information such as students' cognitive\n#' and non-cognitive skills, high school experiences, work experiences and\n#' future plans collected.\n#'\n#' @format A data frame of 7185 observations on 3 variables. \\describe{\n#'   \\item{\\code{mathach}}{Math achievement.} \\item{\\code{ses}}{Socio-Economic\n#'   status.} \\item{\\code{schoolid}}{Categorical variable indicating the school\n#'   the student went to. Treated as \\code{\\link{factor}}.} }\n#'\n#' @source \\href{http://www.icpsr.umich.edu/icpsrweb/ICPSR/studies/7896}{High\n#'   School and Beyond, 1980: A Longitudinal Survey of Students in the United\n#'   States (ICPSR 7896)}\n#'\n#' @references Rabe-Hesketh, S., & Skrondal, A. (2008). \\emph{Multilevel and\n#'   longitudinal modeling using Stata}. STATA press.\n#' @references Raudenbush, S. W. (2004). \\emph{HLM 6: Hierarchical linear and\n#'   nonlinear modeling}. Scientific Software International.\n#' @references Raudenbush, S. W., & Bryk, A. S. (2002). \\emph{Hierarchical\n#'   linear models: Applications and data analysis methods} (Vol. 1). Sage.\n#'\n#' @examples\n#' data(hsb)\n#' str(hsb)\n\"hsb\"\n\n#' High school and beyond dataset\n#'\n#' Smaller subset of \\code{hsb}.\n#'\n#' A random subset of size 16 out of the original 160 groups.\n#'\n#' @format A data frame of 661 observations on 3 variables. \\describe{\n#'   \\item{\\code{mathach}}{Math achievement.} \\item{\\code{ses}}{Socio-Economic\n#'   status.} \\item{\\code{schoolid}}{Categorical variable indicating the school\n#'   the student went to. Treated as \\code{factor}.} }\n#'\n#' @examples\n#' data(hsbsmall)\n#' str(hsbsmall)\n\"hsbsmall\"\n\n#' Air pollution and mortality\n#'\n#' Data on the relation between weather, socioeconomic, and air pollution\n#' variables and mortality rates in 60 Standard Metropolitan Statistical Areas\n#' (SMSAs) of the USA, for the years 1959-1961.\n#'\n#' @format A data frame of 16 observations on 16 variables.\n#' \\describe{\n#'   \\item{\\code{Mortality}}{Total age-adjusted mortality rate per 100,000.}\n#'   \\item{\\code{Rain}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{Humid}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{JanTemp}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{JulTemp}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{Over65}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{Popn}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{Educ}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{Hous}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{Dens}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{NonW}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{WhiteCol}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{Poor}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{HC}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{NOx}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{SO2}}{Mean annual precipitation in inches.}\n#' }\n#' @references McDonald, G. C. and Schwing, R. C. (1973). Instabilities of\n#'   regression estimates relating air pollution to mortality.\n#'   \\emph{Technometrics}, 15(3):463-481.\n#'\n#' @examples\n#' data(pollution)\n#' str(pollution)\n\"pollution\"\n\n#' Results of I-prior cross-validation experiment on Tecator data set\n#'\n#' Results of I-prior cross-validation experiment on Tecator data set\n#'\n#' For the fBm and SE kernels, it seems numerical issues arise when using a\n#' direct optimisation approach. Terminating the algorithm early (say using a\n#' relaxed stopping criterion) seems to help.\n#'\n#' @format Results from iprior_cv cross validation experiment. This is a list of\n#'   seven, with each component bearing the results for the linear, quadratic,\n#'   cubic, fBm-0.5, fBm-MLE and SE I-prior models. The seventh is a summarised\n#'   table of the results.\n#'\n#' @examples\n#' # Results from the six experiments\n#' print(tecator.cv[[1]], \"RMSE\")\n#' print(tecator.cv[[2]], \"RMSE\")\n#' print(tecator.cv[[3]], \"RMSE\")\n#' print(tecator.cv[[4]], \"RMSE\")\n#' print(tecator.cv[[5]], \"RMSE\")\n#' print(tecator.cv[[6]], \"RMSE\")\n#'\n#' # Summary of results\n#' print(tecator.cv[[7]])\n#'\n#' \\dontrun{\n#'\n#' # Prepare data set\n#' data(tecator, package = \"caret\")\n#' endpoints <- as.data.frame(endpoints)\n#' colnames(endpoints) <- c(\"water\", \"fat\", \"protein\")\n#' absorp <- -t(diff(t(absorp)))  # this takes first differences using diff()\n#' fat <- endpoints$fat\n#'\n#' # Here is the code to replicate the results\n#' mod1.cv <- iprior_cv(fat, absorp, folds = Inf)\n#' mod2.cv <- iprior_cv(fat, absorp, folds = Inf, kernel = \"poly2\",\n#'                      est.offset = TRUE)\n#' mod3.cv <- iprior_cv(fat, absorp, folds = Inf, kernel = \"poly3\",\n#'                      est.offset = TRUE)\n#' mod4.cv <- iprior_cv(fat, absorp, method = \"em\", folds = Inf, kernel = \"fbm\",\n#'                      control = list(stop.crit = 1e-2))\n#' mod5.cv <- iprior_cv(fat, absorp, folds = Inf, kernel = \"fbm\",\n#'                      est.hurst = TRUE, control = list(stop.crit = 1e-2))\n#' mod6.cv <- iprior_cv(fat, absorp, folds = Inf, kernel = \"se\",\n#'                      est.lengthscale = TRUE, control = list(stop.crit = 1e-2))\n#'\n#' tecator_res_cv <- function(mod) {\n#'   res <- as.numeric(apply(mod$res[, -1], 2, mean))  # Calculate RMSE\n#'   c(\"Training RMSE\" = res[1], \"Test RMSE\" = res[2])\n#' }\n#'\n#' tecator_tab_cv <- function() {\n#'   tab <- t(sapply(list(mod1.cv, mod2.cv, mod3.cv, mod4.cv, mod5.cv, mod6.cv),\n#'                   tecator_res_cv))\n#'   rownames(tab) <- c(\"Linear\", \"Quadratic\", \"Cubic\", \"fBm-0.5\", \"fBm-MLE\",\n#'                      \"SE-MLE\")\n#'   tab\n#' }\n#'\n#' tecator.cv <- list(\n#'   \"linear\"   = mod1.cv,\n#'   \"qudratic\" = mod2.cv,\n#'   \"cubic\"    = mod3.cv,\n#'   \"fbm-0.5\"  = mod4.cv,\n#'   \"fbm-MLE\"  = mod5.cv,\n#'   \"SE\"       = mod6.cv,\n#'   \"summary\"  = tecator_tab_cv()\n#' )\n#' }\n#'\n#'\n\"tecator.cv\"\n" }
{ "repo_name": "haziqj/iprior", "ref": "refs/heads/master", "path": "R/Data.R", "content": "################################################################################\n#\n#   iprior: Linear Regression using I-priors\n#   Copyright (C) 2018  Haziq Jamil\n#\n#   This program is free software: you can redistribute it and/or modify\n#   it under the terms of the GNU General Public License as published by\n#   the Free Software Foundation, either version 3 of the License, or\n#   (at your option) any later version.\n#\n#   This program is distributed in the hope that it will be useful,\n#   but WITHOUT ANY WARRANTY; without even the implied warranty of\n#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#   GNU General Public License for more details.\n#\n#   You should have received a copy of the GNU General Public License\n#   along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n################################################################################\n\n#' Generate simulated data for smoothing models\n#'\n#' @param n Sample size.\n#' @param seed (Optional) Random seed.\n#' @param x.jitter A small amount of jitter is added to the \\code{X} variables\n#'   generated from a normal distribution with mean zero and standard deviation\n#'   equal to \\code{x.jitter}.\n#' @param xlim Limits of the \\code{X} variables to generate from.\n#'\n#' @return A dataframe containing the response variable \\code{y} and\n#'   unidimensional explanatory variable \\code{X}.\n#'\n#' @examples\n#' gen_smooth(10)\n#'\n#' @export\ngen_smooth <- function(n = 150, xlim = c(0.2, 4.6), x.jitter = 0.65,\n                       seed = NULL) {\n  if (!is.null(seed)) set.seed(seed)\n  f <- function(x, truth = FALSE) {\n    35 * dnorm(x, mean = 1, sd = 0.8) +\n      65 * dnorm(x, mean = 4, sd = 1.5) +\n      (x > 4.5) * (exp((1.25 * (x - 4.5))) - 1) +\n      3 * dnorm(x, mean = 2.5, sd = 0.3)\n  }\n  x <- c(seq(xlim[1], 1.9, length = n * 5 / 8),\n         seq(3.7, xlim[2], length = n * 3 / 8))\n  x <- sample(x, size = n)\n  x <- x + rnorm(n, sd = x.jitter)  # adding random fluctuation to the x\n  x <- sort(x)\n  y.err <- rt(n, df = 1)\n  y <- f(x) + sign(y.err) * pmin(abs(y.err), rnorm(n, mean = 4.1))  # adding random\n  data.frame(y = y, X = x)\n}\n\n#' Generate simulated data for multilevel models\n#'\n#' @param n Sample size. Input either a single number for a balanced data set,\n#'   or a vector of length \\code{m} indicating the sample size in each group.\n#' @param seed (Optional) Random seed.\n#' @param m Number of groups/levels.\n#' @param sigma_e The standard deviation of the errors.\n#' @param sigma_u0 The standard deviation of the random intercept.\n#' @param sigma_u1 The standard deviation of the random slopes.\n#' @param sigma_u01 The covariance of between the random intercept and the\n#'   random slope.\n#' @param beta0 The mean of the random intercept.\n#' @param beta1 The mean of the random slope.\n#' @param x.jitter A small amount of jitter is added to the \\code{X} variables\n#'   generated from a normal distribution with mean zero and standard deviation\n#'   equal to \\code{x.jitter}.\n#'\n#' @return A dataframe containing the response variable \\code{y}, the\n#'   unidimensional explanatory variables \\code{X}, and the levels/groups\n#'   (factors).\n#'\n#' @examples\n#' gen_multilevel()\n#'\n#' @export\ngen_multilevel <- function(n = 25, m = 6, sigma_e = 2, sigma_u0 = 2,\n                           sigma_u1 = 2, sigma_u01 = -2, beta0 = 0, beta1 = 2,\n                           x.jitter = 0.5, seed = NULL) {\n  # Generates a data set according to the model \\deqn{y_{ij} = \\beta_{0j} +\n  # \\beta{1j}X_{ij} + \\epsilon_{ij}} \\deqn{\\beta_{0j} \\sim \\text{N}(0,\n  # \\sigma_{u0}^2)} \\deqn{\\beta_{1j} \\sim \\text{N}(0, \\sigma_{u1}^2)}\n  # \\deqn{\\text{Cov}(\\beta_{0j}, \\beta_{1j}) = \\sigma_{u01}} with\n  # \\eqn{i=1,\\dots,n_j}  samples and \\eqn{j=1,\\dots,m} groups.\n  if (!is.null(seed)) set.seed(seed)\n  beta <- mvtnorm::rmvnorm(m, c(beta0, beta1),\n                           sigma = matrix(c(sigma_u0 ^ 2, sigma_u01,\n                                            sigma_u01, sigma_u1 ^ 2), nrow = 2))\n  if (length(n) == 1) {\n    n <- rep(n, m)\n  } else if (length(n) != m) {\n    stop(\"n must have length equal to the number of groups (m).\", call. = FALSE)\n  }\n  dat <- as.data.frame(matrix(NA, nrow = sum(n), ncol = 3))\n  n.cum <- c(0, cumsum(n))\n  for (j in seq_len(m)) {\n    x <- seq(0, 5, length = n[j]) + rnorm(n[j], sd = x.jitter)\n    y <- beta[j, 1] + beta[j, 2] * x + rnorm(n[j], sd = sigma_e)\n    dat[(n.cum[j] + 1):n.cum[j + 1], ] <- cbind(y, x, j)\n  }\n  names(dat) <- c(\"y\", \"X\", \"grp\")\n  dat$grp <- factor(dat$grp)\n  dat\n}\n\n#' High school and beyond dataset\n#'\n#' A national longitudinal survey of of students from public and private high\n#' schools in the United States, with information such as students' cognitive\n#' and non-cognitive skills, high school experiences, work experiences and\n#' future plans collected.\n#'\n#' @format A data frame of 7185 observations on 3 variables. \\describe{\n#'   \\item{\\code{mathach}}{Math achievement.} \\item{\\code{ses}}{Socio-Economic\n#'   status.} \\item{\\code{schoolid}}{Categorical variable indicating the school\n#'   the student went to. Treated as \\code{\\link{factor}}.} }\n#'\n#' @source \\href{http://www.icpsr.umich.edu/icpsrweb/ICPSR/studies/7896}{High\n#'   School and Beyond, 1980: A Longitudinal Survey of Students in the United\n#'   States (ICPSR 7896)}\n#'\n#' @references Rabe-Hesketh, S., & Skrondal, A. (2008). \\emph{Multilevel and\n#'   longitudinal modeling using Stata}. STATA press.\n#' @references Raudenbush, S. W. (2004). \\emph{HLM 6: Hierarchical linear and\n#'   nonlinear modeling}. Scientific Software International.\n#' @references Raudenbush, S. W., & Bryk, A. S. (2002). \\emph{Hierarchical\n#'   linear models: Applications and data analysis methods} (Vol. 1). Sage.\n#'\n#' @examples\n#' data(hsb)\n#' str(hsb)\n\"hsb\"\n\n#' High school and beyond dataset\n#'\n#' Smaller subset of \\code{hsb}.\n#'\n#' A random subset of size 16 out of the original 160 groups.\n#'\n#' @format A data frame of 661 observations on 3 variables. \\describe{\n#'   \\item{\\code{mathach}}{Math achievement.} \\item{\\code{ses}}{Socio-Economic\n#'   status.} \\item{\\code{schoolid}}{Categorical variable indicating the school\n#'   the student went to. Treated as \\code{factor}.} }\n#'\n#' @examples\n#' data(hsbsmall)\n#' str(hsbsmall)\n\"hsbsmall\"\n\n#' Air pollution and mortality\n#'\n#' Data on the relation between weather, socioeconomic, and air pollution\n#' variables and mortality rates in 60 Standard Metropolitan Statistical Areas\n#' (SMSAs) of the USA, for the years 1959-1961.\n#'\n#' @format A data frame of 16 observations on 16 variables.\n#' \\describe{\n#'   \\item{\\code{Mortality}}{Total age-adjusted mortality rate per 100,000.}\n#'   \\item{\\code{Rain}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{Humid}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{JanTemp}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{JulTemp}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{Over65}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{Popn}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{Educ}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{Hous}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{Dens}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{NonW}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{WhiteCol}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{Poor}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{HC}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{NOx}}{Mean annual precipitation in inches.}\n#'   \\item{\\code{SO2}}{Mean annual precipitation in inches.}\n#' }\n#' @references McDonald, G. C. and Schwing, R. C. (1973). Instabilities of\n#'   regression estimates relating air pollution to mortality.\n#'   \\emph{Technometrics}, 15(3):463-481.\n#'\n#' @examples\n#' data(pollution)\n#' str(pollution)\n\"pollution\"\n\n#' Results of I-prior cross-validation experiment on Tecator data set\n#'\n#' Results of I-prior cross-validation experiment on Tecator data set\n#'\n#' For the fBm and SE kernels, it seems numerical issues arise when using a\n#' direct optimisation approach. Terminating the algorithm early (say using a\n#' relaxed stopping criterion) seems to help.\n#'\n#' @format Results from iprior_cv cross validation experiment. This is a list of\n#'   seven, with each component bearing the results for the linear, quadratic,\n#'   cubic, fBm-0.5, fBm-MLE and SE I-prior models. The seventh is a summarised\n#'   table of the results.\n#'\n#' @examples\n#' # Results from the six experiments\n#' print(tecator.cv[[1]], \"RMSE\")\n#' print(tecator.cv[[2]], \"RMSE\")\n#' print(tecator.cv[[3]], \"RMSE\")\n#' print(tecator.cv[[4]], \"RMSE\")\n#' print(tecator.cv[[5]], \"RMSE\")\n#' print(tecator.cv[[6]], \"RMSE\")\n#'\n#' # Summary of results\n#' print(tecator.cv[[7]])\n#'\n#' \\dontrun{\n#'\n#' # Prepare data set\n#' data(tecator, package = \"caret\")\n#' endpoints <- as.data.frame(endpoints)\n#' colnames(endpoints) <- c(\"water\", \"fat\", \"protein\")\n#' absorp <- -t(diff(t(absorp)))  # this takes first differences using diff()\n#' fat <- endpoints$fat\n#'\n#' # Here is the code to replicate the results\n#' mod1.cv <- iprior_cv(fat, absorp, folds = Inf)\n#' mod2.cv <- iprior_cv(fat, absorp, folds = Inf, kernel = \"poly2\",\n#'                      est.offset = TRUE)\n#' mod3.cv <- iprior_cv(fat, absorp, folds = Inf, kernel = \"poly3\",\n#'                      est.offset = TRUE)\n#' mod4.cv <- iprior_cv(fat, absorp, method = \"em\", folds = Inf, kernel = \"fbm\",\n#'                      control = list(stop.crit = 1e-2))\n#' mod5.cv <- iprior_cv(fat, absorp, folds = Inf, kernel = \"fbm\",\n#'                      est.hurst = TRUE, control = list(stop.crit = 1e-2))\n#' mod6.cv <- iprior_cv(fat, absorp, folds = Inf, kernel = \"se\",\n#'                      est.lengthscale = TRUE, control = list(stop.crit = 1e-2))\n#'\n#' tecator_res_cv <- function(mod) {\n#'   res <- as.numeric(apply(mod$res[, -1], 2, mean))  # Calculate RMSE\n#'   c(\"Training RMSE\" = res[1], \"Test RMSE\" = res[2])\n#' }\n#'\n#' tecator_tab_cv <- function() {\n#'   tab <- t(sapply(list(mod1.cv, mod2.cv, mod3.cv, mod4.cv, mod5.cv, mod6.cv),\n#'                   tecator_res_cv))\n#'   rownames(tab) <- c(\"Linear\", \"Quadratic\", \"Cubic\", \"fBm-0.5\", \"fBm-MLE\",\n#'                      \"SE-MLE\")\n#'   tab\n#' }\n#'\n#' tecator.cv <- list(\n#'   \"linear\"   = mod1.cv,\n#'   \"qudratic\" = mod2.cv,\n#'   \"cubic\"    = mod3.cv,\n#'   \"fbm-0.5\"  = mod4.cv,\n#'   \"fbm-MLE\"  = mod5.cv,\n#'   \"SE\"       = mod6.cv,\n#'   \"summary\"  = tecator_tab_cv()\n#' )\n#' }\n#'\n#'\n\"tecator.cv\"\n" }
{ "repo_name": "cschroed-usgs/nar_data", "ref": "refs/heads/master", "path": "tests/testthat/test_annual_load.R", "content": "library(testthat)\nlibrary(validate)\ncontext(\"annual load\")\noptions(scipen=999)\ntemp_aloads<-aloads\ntemp_aloads$TONS_N<-as.numeric(temp_aloads$TONS)\ntemp_aloads$TONS_L95_N<-as.numeric(temp_aloads$TONS_L95)\ntemp_aloads$TONS_U95_N<-as.numeric(temp_aloads$TONS_U95)\ntemp_aloads$FWC_N<-as.numeric(temp_aloads$FWC)\ntemp_aloads$YIELD_N<-as.numeric(temp_aloads$YIELD)\ntemp_aloads$mod1<-as.character(temp_aloads$MODTYPE)\ntemp_aloads[temp_aloads$mod1%in%\"REGHIST\",\"mod1\"]<-\"REG\"\ntemp_aloads<-temp_aloads[!is.na(temp_aloads$TONS_L95_N),]\ntemp_aloads<-temp_aloads[!is.na(temp_aloads$TONS_U95_N),]\n\n\ntest_that(\"annual load has the correct columns\", {\n\texpect_has_names(aloads, c(\n\t\t\"CONSTIT\",\n\t\t\"FWC\",\n\t\t\"MODTYPE\",\n\t\t\"SITE_ABB\",\n\t\t\"SITE_FLOW_ID\",\n\t\t\"SITE_QW_ID\",\n\t\t\"TONS\",\n\t\t\"TONS_L95\",\n\t\t\"TONS_U95\",\n\t\t\"YIELD\",\n\t\t\"WY\"\n\t))\n})\n\ntest_that(\"annual load's columns are correctly typed\", {\n\tresult <- validate::check_that(aloads,\n\t\tis.integer(WY),\n\t\tis.character(c(\n\t\t\tSITE_ABB,\n\t\t\tSITE_QW_ID,\n\t\t\tSITE_FLOW_ID,TONS, TONS_L95, TONS_U95, FWC, YIELD\n\t\t)),\n\t\tis.factor(CONSTIT),\n\t\tis.factor(MODTYPE)\n\t)\n\t\n\texpect_no_errors(result)\n})\n\n\ntest_that(\"annual load has a reasonable range of values\", {\n\tresult <- validate::check_that(temp_aloads, \n\t\tTONS_N > 0,\n\t\tTONS_N < 5E8,\n\t\tTONS_L95_N < TONS_U95_N,\n\t\tTONS_L95_N < TONS_N,\n\t\tTONS_N < TONS_U95_N,\n\t\tnchar(SITE_ABB) == 4,\n\t\tWY < 2020,\n\t\tWY > 1950\n\t)\n\texpect_no_errors(result)\n})\n\n\n\ntest_that(\"annual loads for the MISS site are included\", {\n\tmiss_sites <- subset(aloads, SITE_ABB == 'MISS')\n\texpect_gt(nrow(miss_sites), 0)\n\t\n\t})\n\n\ntest_that(\"annual loads for the GULF are included\", {\n  gulf_sites <- subset(aloads, SITE_ABB == 'GULF')\n  expect_gt(nrow(gulf_sites), 0)\n})\n\n\ntest_that(\"the expected modtypes are present\", {\n  expected <- sort(c(\"REG\",\"REG_2\",\"REG_3\",\"REG_4\",\"REGHIST\",\"DAILY\"))\n  actual <- sort(unique(as.character(aloads$MODTYPE)))\n  expect_equal(actual, expected)\n  \n})\n\ntest_that(\"Load data have the correct number of significant digits\", {\n  result <- validate::check_that(temp_aloads, \n                                 \n                                 count_sig_figs(temp_aloads$TONS_N/1E8) <= 3,\n                                 count_sig_figs(temp_aloads$TONS_L95_N/1E8) <= 3,\n                                 count_sig_figs(temp_aloads$TONS_U95_N/1E8) <= 3,\n                                 count_sig_figs(temp_aloads$FWC_N/1E8) <= 3,\n                                 count_sig_figs(temp_aloads$YIELD_N/1E8) <= 3\n                                 \n                                 )\n  expect_no_errors(result) \n})\n\ntest_that(\"There are no duplicate values\", {\n  aloads_without_ignored_modtypes <- subset(aloads, !(MODTYPE %in% c('COMP', 'CONTIN')))\n  unique_columns <- aloads_without_ignored_modtypes[c('SITE_QW_ID', 'CONSTIT', 'WY')]\n  expect_no_duplicates(unique_columns)\n  \n})\n\ntest_that(\"Most recent water year has all of the necessary sites \", {\n  temp_aloads_recent<-temp_aloads[temp_aloads$WY%in%max(temp_aloads$WY),] \n  expected <- sort(c(\"HAZL\",\"PADU\",\"GRAN\",\"CLIN\",\"WAPE\",\"KEOS\",\"VALL\",\"GRAF\",\"SIDN\",\"OMAH\",\"ELKH\",\"LOUI\",\"DESO\",\"HERM\",\"THEB\",\"SEDG\",\"HARR\",\"KERS\",\"MORG\",\"BELL\",\n                     \"STFR\",\"MELV\",\"SUMN\",\"STTH\",\"GULF\",\"NEWH\",\"CANN\",\"MISS\"))\n  actual <- sort(unique(temp_aloads_recent[temp_aloads_recent$SITE_ABB%in%expected,\"SITE_ABB\"]))\n\n  \n  expect_equal(actual, expected)\n})" }
{ "repo_name": "USGS-CIDA/nar_data", "ref": "refs/heads/master", "path": "tests/testthat/test_annual_load.R", "content": "library(testthat)\nlibrary(validate)\ncontext(\"annual load\")\noptions(scipen=999)\ntemp_aloads<-aloads\ntemp_aloads$TONS_N<-as.numeric(temp_aloads$TONS)\ntemp_aloads$TONS_L95_N<-as.numeric(temp_aloads$TONS_L95)\ntemp_aloads$TONS_U95_N<-as.numeric(temp_aloads$TONS_U95)\ntemp_aloads$FWC_N<-as.numeric(temp_aloads$FWC)\ntemp_aloads$YIELD_N<-as.numeric(temp_aloads$YIELD)\ntemp_aloads$mod1<-as.character(temp_aloads$MODTYPE)\ntemp_aloads[temp_aloads$mod1%in%\"REGHIST\",\"mod1\"]<-\"REG\"\ntemp_aloads<-temp_aloads[!is.na(temp_aloads$TONS_L95_N),]\ntemp_aloads<-temp_aloads[!is.na(temp_aloads$TONS_U95_N),]\n\n\ntest_that(\"annual load has the correct columns\", {\n\texpect_has_names(aloads, c(\n\t\t\"CONSTIT\",\n\t\t\"FWC\",\n\t\t\"MODTYPE\",\n\t\t\"SITE_ABB\",\n\t\t\"SITE_FLOW_ID\",\n\t\t\"SITE_QW_ID\",\n\t\t\"TONS\",\n\t\t\"TONS_L95\",\n\t\t\"TONS_U95\",\n\t\t\"YIELD\",\n\t\t\"WY\"\n\t))\n})\n\ntest_that(\"annual load's columns are correctly typed\", {\n\tresult <- validate::check_that(aloads,\n\t\tis.integer(WY),\n\t\tis.character(c(\n\t\t\tSITE_ABB,\n\t\t\tSITE_QW_ID,\n\t\t\tSITE_FLOW_ID,TONS, TONS_L95, TONS_U95, FWC, YIELD\n\t\t)),\n\t\tis.factor(CONSTIT),\n\t\tis.factor(MODTYPE)\n\t)\n\t\n\texpect_no_errors(result)\n})\n\n\ntest_that(\"annual load has a reasonable range of values\", {\n\tresult <- validate::check_that(temp_aloads, \n\t\tTONS_N > 0,\n\t\tTONS_N < 5E8,\n\t\tTONS_L95_N < TONS_U95_N,\n\t\tTONS_L95_N < TONS_N,\n\t\tTONS_N < TONS_U95_N,\n\t\tnchar(SITE_ABB) == 4,\n\t\tWY < 2020,\n\t\tWY > 1950\n\t)\n\texpect_no_errors(result)\n})\n\n\n\ntest_that(\"annual loads for the MISS site are included\", {\n\tmiss_sites <- subset(aloads, SITE_ABB == 'MISS')\n\texpect_gt(nrow(miss_sites), 0)\n\t\n\t})\n\n\ntest_that(\"annual loads for the GULF are included\", {\n  gulf_sites <- subset(aloads, SITE_ABB == 'GULF')\n  expect_gt(nrow(gulf_sites), 0)\n})\n\n\ntest_that(\"the expected modtypes are present\", {\n  expected <- sort(c(\"REG\",\"REG_2\",\"REG_3\",\"REG_4\",\"REGHIST\",\"DAILY\"))\n  actual <- sort(unique(as.character(aloads$MODTYPE)))\n  expect_equal(actual, expected)\n  \n})\n\ntest_that(\"Load data have the correct number of significant digits\", {\n  result <- validate::check_that(temp_aloads, \n                                 \n                                 count_sig_figs(temp_aloads$TONS_N/1E8) <= 3,\n                                 count_sig_figs(temp_aloads$TONS_L95_N/1E8) <= 3,\n                                 count_sig_figs(temp_aloads$TONS_U95_N/1E8) <= 3,\n                                 count_sig_figs(temp_aloads$FWC_N/1E8) <= 3,\n                                 count_sig_figs(temp_aloads$YIELD_N/1E8) <= 3\n                                 \n                                 )\n  expect_no_errors(result) \n})\n\ntest_that(\"There are no duplicate values\", {\n  aloads_without_ignored_modtypes <- subset(aloads, !(MODTYPE %in% c('COMP', 'CONTIN')))\n  unique_columns <- aloads_without_ignored_modtypes[c('SITE_QW_ID', 'CONSTIT', 'WY')]\n  expect_no_duplicates(unique_columns)\n  \n})\n\ntest_that(\"Most recent water year has all of the necessary sites \", {\n  temp_aloads_recent<-temp_aloads[temp_aloads$WY%in%max(temp_aloads$WY),] \n  expected <- sort(c(\"HAZL\",\"PADU\",\"GRAN\",\"CLIN\",\"WAPE\",\"KEOS\",\"VALL\",\"GRAF\",\"SIDN\",\"OMAH\",\"ELKH\",\"LOUI\",\"DESO\",\"HERM\",\"THEB\",\"SEDG\",\"HARR\",\"KERS\",\"MORG\",\"BELL\",\n                     \"STFR\",\"MELV\",\"SUMN\",\"STTH\",\"GULF\",\"NEWH\",\"CANN\",\"MISS\"))\n  actual <- sort(unique(temp_aloads_recent[temp_aloads_recent$SITE_ABB%in%expected,\"SITE_ABB\"]))\n\n  \n  expect_equal(actual, expected)\n})" }
{ "repo_name": "beanumber/imdb", "ref": "refs/heads/master", "path": "tests/testthat.R", "content": "library(testthat)\nlibrary(imdb)\n\ntest_check(\"imdb\")\n" }
{ "repo_name": "ColumbusCollaboratory/electron-quick-start", "ref": "refs/heads/master", "path": "R-Portable-Mac/library/xgboost/demo/cross_validation.R", "content": "require(xgboost)\n# load in the agaricus dataset\ndata(agaricus.train, package='xgboost')\ndata(agaricus.test, package='xgboost')\ndtrain <- xgb.DMatrix(agaricus.train$data, label = agaricus.train$label)\ndtest <- xgb.DMatrix(agaricus.test$data, label = agaricus.test$label)\n\nnround <- 2\nparam <- list(max_depth=2, eta=1, silent=1, nthread=2, objective='binary:logistic')\n\ncat('running cross validation\\n')\n# do cross validation, this will print result out as\n# [iteration]  metric_name:mean_value+std_value\n# std_value is standard deviation of the metric\nxgb.cv(param, dtrain, nround, nfold=5, metrics={'error'})\n\ncat('running cross validation, disable standard deviation display\\n')\n# do cross validation, this will print result out as\n# [iteration]  metric_name:mean_value+std_value\n# std_value is standard deviation of the metric\nxgb.cv(param, dtrain, nround, nfold=5,\n       metrics='error', showsd = FALSE)\n\n###\n# you can also do cross validation with cutomized loss function\n# See custom_objective.R\n##\nprint ('running cross validation, with cutomsized loss function')\n\nlogregobj <- function(preds, dtrain) {\n  labels <- getinfo(dtrain, \"label\")\n  preds <- 1/(1 + exp(-preds))\n  grad <- preds - labels\n  hess <- preds * (1 - preds)\n  return(list(grad = grad, hess = hess))\n}\nevalerror <- function(preds, dtrain) {\n  labels <- getinfo(dtrain, \"label\")\n  err <- as.numeric(sum(labels != (preds > 0)))/length(labels)\n  return(list(metric = \"error\", value = err))\n}\n\nparam <- list(max_depth=2, eta=1, silent=1,\n              objective = logregobj, eval_metric = evalerror)\n# train with customized objective\nxgb.cv(params = param, data = dtrain, nrounds = nround, nfold = 5)\n\n# do cross validation with prediction values for each fold\nres <- xgb.cv(params = param, data = dtrain, nrounds = nround, nfold = 5, prediction = TRUE)\nres$evaluation_log\nlength(res$pred)\n" }
{ "repo_name": "RPGOne/Skynet", "ref": "refs/heads/Miho", "path": "xgboost-master/R-package/demo/cross_validation.R", "content": "require(xgboost)\n# load in the agaricus dataset\ndata(agaricus.train, package='xgboost')\ndata(agaricus.test, package='xgboost')\ndtrain <- xgb.DMatrix(agaricus.train$data, label = agaricus.train$label)\ndtest <- xgb.DMatrix(agaricus.test$data, label = agaricus.test$label)\n\nnround <- 2\nparam <- list(max_depth=2, eta=1, silent=1, nthread=2, objective='binary:logistic')\n\ncat('running cross validation\\n')\n# do cross validation, this will print result out as\n# [iteration]  metric_name:mean_value+std_value\n# std_value is standard deviation of the metric\nxgb.cv(param, dtrain, nround, nfold=5, metrics={'error'})\n\ncat('running cross validation, disable standard deviation display\\n')\n# do cross validation, this will print result out as\n# [iteration]  metric_name:mean_value+std_value\n# std_value is standard deviation of the metric\nxgb.cv(param, dtrain, nround, nfold=5,\n       metrics='error', showsd = FALSE)\n\n###\n# you can also do cross validation with cutomized loss function\n# See custom_objective.R\n##\nprint ('running cross validation, with cutomsized loss function')\n\nlogregobj <- function(preds, dtrain) {\n  labels <- getinfo(dtrain, \"label\")\n  preds <- 1/(1 + exp(-preds))\n  grad <- preds - labels\n  hess <- preds * (1 - preds)\n  return(list(grad = grad, hess = hess))\n}\nevalerror <- function(preds, dtrain) {\n  labels <- getinfo(dtrain, \"label\")\n  err <- as.numeric(sum(labels != (preds > 0)))/length(labels)\n  return(list(metric = \"error\", value = err))\n}\n\nparam <- list(max_depth=2, eta=1, silent=1,\n              objective = logregobj, eval_metric = evalerror)\n# train with customized objective\nxgb.cv(params = param, data = dtrain, nrounds = nround, nfold = 5)\n\n# do cross validation with prediction values for each fold\nres <- xgb.cv(params = param, data = dtrain, nrounds = nround, nfold = 5, prediction = TRUE)\nres$evaluation_log\nlength(res$pred)\n" }
{ "repo_name": "sammorris81/spatial-skew-t", "ref": "refs/heads/master", "path": "code/analysis/ozone/US-east/us-east-14.R", "content": "options(warn=2)\nlibrary(fields)\nlibrary(SpatialTools)\nlibrary(mvtnorm)\n\nrm(list=ls())\nload('us-east-setup.RData')\nsource('../../../R/mcmc.R', chdir=T)\nsource('../../../R/auxfunctions.R')\n\nsetting <- 14\nmethod <- \"t\"\nnknots <- 15\nkeep.knots <- F\nthreshold <- 0\ntau.init <- 0.05\nthresh.quant <- F\nskew <- T\noutputfile <- paste(\"us-east-\", setting, \".RData\", sep=\"\")\n\nstart <- proc.time()\n\nfit <- vector(mode=\"list\", length=2)\n\nfor(val in 1:2){\n\n\tset.seed(setting*100 + val)\n\n\tcat(\"CV\", val, \"started \\n\")\n\tval.idx <- cv.lst[[val]]\n\ty.o <- Y[-val.idx,]\n\tX.o <- X[-val.idx, , ]\n\tS.o <- S[-val.idx,]\n\n\ty.p <- Y[val.idx,]\n\tX.p <- X[val.idx, , ]\n\tS.p <- S[val.idx,]\n\n\ttic.set <- proc.time()\n\tfit[[val]] <- mcmc(y=y.o, s=S.o, x=X.o, x.pred=X.p, s.pred=S.p,\n\t                   method=method, skew=skew, keep.knots=keep.knots,\n\t                   thresh.all=threshold, thresh.quant=thresh.quant, nknots=nknots,\n                       iters=30000, burn=25000, update=500, iterplot=F,\n                       beta.init=beta.init, tau.init=tau.init, gamma.init=0.5,\n                       rho.init=1, rho.upper=5, nu.init=0.5, nu.upper=10)\n\ttoc.set <- proc.time()\n\ttime.set <- (toc.set - tic.set)[3]\n\n\telap.time.val <- (proc.time() - start)[3]\n\tavg.time.val <- elap.time.val / val\n\tcat(\"CV\", val, \"finished\", round(avg.time.val, 2), \"per dataset \\n\")\n\tsave(fit, file=outputfile)\n}\n" }
{ "repo_name": "sammorris81/spatial-skew-t", "ref": "refs/heads/master", "path": "code/analysis/ozone/US-east-nocmaq/us-east-14.R", "content": "options(warn=2)\nlibrary(fields)\nlibrary(SpatialTools)\nlibrary(mvtnorm)\n\nrm(list=ls())\nload('us-east-setup.RData')\nsource('../../../R/mcmc.R', chdir=T)\nsource('../../../R/auxfunctions.R')\n\nsetting <- 14\nmethod <- \"t\"\nnknots <- 15\nkeep.knots <- F\nthreshold <- 0\ntau.init <- 0.05\nthresh.quant <- F\nskew <- T\noutputfile <- paste(\"us-east-\", setting, \".RData\", sep=\"\")\n\nstart <- proc.time()\n\nfit <- vector(mode=\"list\", length=2)\n\nfor(val in 1:2){\n\n\tset.seed(setting*100 + val)\n\n\tcat(\"CV\", val, \"started \\n\")\n\tval.idx <- cv.lst[[val]]\n\ty.o <- Y[-val.idx,]\n\tX.o <- X[-val.idx, , ]\n\tS.o <- S[-val.idx,]\n\n\ty.p <- Y[val.idx,]\n\tX.p <- X[val.idx, , ]\n\tS.p <- S[val.idx,]\n\n\ttic.set <- proc.time()\n\tfit[[val]] <- mcmc(y=y.o, s=S.o, x=X.o, x.pred=X.p, s.pred=S.p,\n\t                   method=method, skew=skew, keep.knots=keep.knots,\n\t                   thresh.all=threshold, thresh.quant=thresh.quant, nknots=nknots,\n                       iters=30000, burn=25000, update=500, iterplot=F,\n                       beta.init=beta.init, tau.init=tau.init, gamma.init=0.5,\n                       rho.init=1, rho.upper=5, nu.init=0.5, nu.upper=10)\n\ttoc.set <- proc.time()\n\ttime.set <- (toc.set - tic.set)[3]\n\n\telap.time.val <- (proc.time() - start)[3]\n\tavg.time.val <- elap.time.val / val\n\tcat(\"CV\", val, \"finished\", round(avg.time.val, 2), \"per dataset \\n\")\n\tsave(fit, file=outputfile)\n}\n" }
{ "repo_name": "BatzoglouLabSU/SIMLR", "ref": "refs/heads/SIMLR", "path": "R/network.diffusion.R", "content": "# perform network diffusion of K steps over the network A\n\"network.diffusion\" <- function( A, K ) {\n    \n    # set the values of the diagonal of A to 0\n    diag(A) = 0\n    \n    # compute the sign matrix of A\n    sign_A = A\n    sign_A[which(A>0,arr.ind=TRUE)] = 1\n    sign_A[which(A<0,arr.ind=TRUE)] = -1\n    \n    # compute the dominate set for A and K\n    P = dominate.set(abs(A),min(K,nrow(A)-1)) * sign_A\n    \n    # sum the absolute value of each row of P\n    DD = apply(abs(P),MARGIN=1,FUN=sum)\n    \n    # set DD+1 to the diagonal of P\n    diag(P) = DD + 1\n    \n    # compute the transition field of P\n    P = transition.fields(P)\n    \n    # compute the eigenvalues and eigenvectors of P\n    eigen_P = eigen(P)\n    U = eigen_P$vectors\n    D = eigen_P$values\n    \n    # set to d the real part of the diagonal of D\n    d = Re(D + .Machine$double.eps)\n    \n    # perform the diffusion\n    alpha = 0.8\n    beta = 2\n    d = ((1-alpha)*d)/(1-alpha*d^beta)\n    \n    # set to D the real part of the diagonal of d\n    D = array(0,c(length(Re(d)),length(Re(d))))\n    diag(D) = Re(d)\n    \n    # finally compute W\n    W = U %*% D %*% t(U)\n    diagonal_matrix = array(0,c(nrow(W),ncol(W)))\n    diag(diagonal_matrix) = 1\n    W = (W * (1-diagonal_matrix)) / apply(array(0,c(nrow(W),ncol(W))),MARGIN=2,FUN=function(x) {x=(1-diag(W))})\n    diag(D) = diag(D)[length(diag(D)):1]\n    W = diag(DD) %*% W\n    W = (W + t(W)) / 2\n    \n    W[which(W<0,arr.ind=TRUE)] = 0\n    \n    return(W)\n    \n}\n\n# compute the dominate set for the matrix aff.matrix and NR.OF.KNN\n\"dominate.set\" <- function( aff.matrix, NR.OF.KNN ) {\n    \n    # create the structure to save the results\n    PNN.matrix = array(0,c(nrow(aff.matrix),ncol(aff.matrix)))\n    \n    # sort each row of aff.matrix in descending order and saves the sorted \n    # array and a collection of vectors with the original indices\n    res.sort = apply(t(aff.matrix),MARGIN=2,FUN=function(x) {return(sort(x, decreasing = TRUE, index.return = TRUE))})\n    sorted.aff.matrix = t(apply(as.matrix(1:length(res.sort)),MARGIN=1,function(x) { return(res.sort[[x]]$x) }))\n    sorted.indices = t(apply(as.matrix(1:length(res.sort)),MARGIN=1,function(x) { return(res.sort[[x]]$ix) }))\n    \n    # get the first NR.OF.KNN columns of the sorted array\n    res = sorted.aff.matrix[,1:NR.OF.KNN]\n    \n    # create a matrix of NR.OF.KNN columns by binding vectors of \n    # integers from 1 to the number of rows/columns of aff.matrix\n    inds = array(0,c(nrow(aff.matrix),NR.OF.KNN))\n    inds = apply(inds,MARGIN=2,FUN=function(x) {x=1:nrow(aff.matrix)})\n    \n    # get the first NR.OF.KNN columns of the indices of aff.matrix\n    loc = sorted.indices[,1:NR.OF.KNN]\n    \n    # assign to PNN.matrix the sorted indices\n    PNN.matrix[(as.vector(loc)-1)*nrow(aff.matrix)+as.vector(inds)] = as.vector(res)\n    \n    # compute the final results and return them\n    PNN.matrix = (PNN.matrix + t(PNN.matrix))/2\n    \n    return(PNN.matrix)\n    \n}\n\n# compute the transition field of the given matrix\n\"transition.fields\" <- function( W ) {\n    \n    # get any index of columns with all 0s\n    zero.index = which(apply(W,MARGIN=1,FUN=sum)==0)\n    \n    # compute the transition fields\n    W = dn(W,'ave')\n    \n    w = sqrt(apply(abs(W),MARGIN=2,FUN=sum)+.Machine$double.eps)\n    W = W / t(apply(array(0,c(nrow(W),ncol(W))),MARGIN=2,FUN=function(x) {x=w}))\n    W = W %*% t(W)\n    \n    # set to 0 the elements of zero.index\n    W[zero.index,] = 0\n    W[,zero.index] = 0\n    \n    return(W)\n    \n}\n\n# normalizes a symmetric kernel\n\"dn\" = function( w, type ) {\n    \n    # compute the sum of any column\n    D = apply(w,MARGIN=2,FUN=sum)\n    \n    # type \"ave\" returns D^-1*W\n    if(type==\"ave\") {\n        D = 1 / D\n        D_temp = matrix(0,nrow=length(D),ncol=length(D))\n        D_temp[cbind(1:length(D),1:length(D))] = D\n        D = D_temp\n        wn = D %*% w\n    }\n    # type \"gph\" returns D^-1/2*W*D^-1/2\n    else if(type==\"gph\") {\n        D = 1 / sqrt(D)\n        D_temp = matrix(0,nrow=length(D),ncol=length(D))\n        D_temp[cbind(1:length(D),1:length(D))] = D\n        D = D_temp\n        wn = D %*% (w %*% D)\n    }\n    else {\n        stop(\"Invalid type!\")\n    }\n    \n    return(wn)\n    \n}\n" }
{ "repo_name": "ChengchenZhao/DrSeq2", "ref": "refs/heads/master", "path": "lib/Rscript/network.diffusion.R", "content": "# perform network diffusion of K steps over the network A\n\"network.diffusion\" <- function( A, K ) {\n    \n    # set the values of the diagonal of A to 0\n    diag(A) = 0\n    \n    # compute the sign matrix of A\n    sign_A = A\n    sign_A[which(A>0,arr.ind=TRUE)] = 1\n    sign_A[which(A<0,arr.ind=TRUE)] = -1\n    \n    # compute the dominate set for A and K\n    P = dominate.set(abs(A),min(K,nrow(A)-1)) * sign_A\n    \n    # sum the absolute value of each row of P\n    DD = apply(abs(P),MARGIN=1,FUN=sum)\n    \n    # set DD+1 to the diagonal of P\n    diag(P) = DD + 1\n    \n    # compute the transition field of P\n    P = transition.fields(P)\n    \n    # compute the eigenvalues and eigenvectors of P\n    eigen_P = eigen(P)\n    U = eigen_P$vectors\n    D = eigen_P$values\n    \n    # set to d the real part of the diagonal of D\n    d = Re(D + .Machine$double.eps)\n    \n    # perform the diffusion\n    alpha = 0.8\n    beta = 2\n    d = ((1-alpha)*d)/(1-alpha*d^beta)\n    \n    # set to D the real part of the diagonal of d\n    D = array(0,c(length(Re(d)),length(Re(d))))\n    diag(D) = Re(d)\n    \n    # finally compute W\n    W = U %*% D %*% t(U)\n    diagonal_matrix = array(0,c(nrow(W),ncol(W)))\n    diag(diagonal_matrix) = 1\n    W = (W * (1-diagonal_matrix)) / apply(array(0,c(nrow(W),ncol(W))),MARGIN=2,FUN=function(x) {x=(1-diag(W))})\n    diag(D) = diag(D)[length(diag(D)):1]\n    W = diag(DD) %*% W\n    W = (W + t(W)) / 2\n    \n    W[which(W<0,arr.ind=TRUE)] = 0\n    \n    return(W)\n    \n}\n\n# compute the dominate set for the matrix aff.matrix and NR.OF.KNN\n\"dominate.set\" <- function( aff.matrix, NR.OF.KNN ) {\n    \n    # create the structure to save the results\n    PNN.matrix = array(0,c(nrow(aff.matrix),ncol(aff.matrix)))\n    \n    # sort each row of aff.matrix in descending order and saves the sorted \n    # array and a collection of vectors with the original indices\n    res.sort = apply(t(aff.matrix),MARGIN=2,FUN=function(x) {return(sort(x, decreasing = TRUE, index.return = TRUE))})\n    sorted.aff.matrix = t(apply(as.matrix(1:length(res.sort)),MARGIN=1,function(x) { return(res.sort[[x]]$x) }))\n    sorted.indices = t(apply(as.matrix(1:length(res.sort)),MARGIN=1,function(x) { return(res.sort[[x]]$ix) }))\n    \n    # get the first NR.OF.KNN columns of the sorted array\n    res = sorted.aff.matrix[,1:NR.OF.KNN]\n    \n    # create a matrix of NR.OF.KNN columns by binding vectors of \n    # integers from 1 to the number of rows/columns of aff.matrix\n    inds = array(0,c(nrow(aff.matrix),NR.OF.KNN))\n    inds = apply(inds,MARGIN=2,FUN=function(x) {x=1:nrow(aff.matrix)})\n    \n    # get the first NR.OF.KNN columns of the indices of aff.matrix\n    loc = sorted.indices[,1:NR.OF.KNN]\n    \n    # assign to PNN.matrix the sorted indices\n    PNN.matrix[(as.vector(loc)-1)*nrow(aff.matrix)+as.vector(inds)] = as.vector(res)\n    \n    # compute the final results and return them\n    PNN.matrix = (PNN.matrix + t(PNN.matrix))/2\n    \n    return(PNN.matrix)\n    \n}\n\n# compute the transition field of the given matrix\n\"transition.fields\" <- function( W ) {\n    \n    # get any index of columns with all 0s\n    zero.index = which(apply(W,MARGIN=1,FUN=sum)==0)\n    \n    # compute the transition fields\n    W = dn(W,'ave')\n    \n    w = sqrt(apply(abs(W),MARGIN=2,FUN=sum)+.Machine$double.eps)\n    W = W / t(apply(array(0,c(nrow(W),ncol(W))),MARGIN=2,FUN=function(x) {x=w}))\n    W = W %*% t(W)\n    \n    # set to 0 the elements of zero.index\n    W[zero.index,] = 0\n    W[,zero.index] = 0\n    \n    return(W)\n    \n}\n\n# normalizes a symmetric kernel\n\"dn\" = function( w, type ) {\n    \n    # compute the sum of any column\n    D = apply(w,MARGIN=2,FUN=sum)\n    \n    # type \"ave\" returns D^-1*W\n    if(type==\"ave\") {\n        D = 1 / D\n        D_temp = matrix(0,nrow=length(D),ncol=length(D))\n        D_temp[cbind(1:length(D),1:length(D))] = D\n        D = D_temp\n        wn = D %*% w\n    }\n    # type \"gph\" returns D^-1/2*W*D^-1/2\n    else if(type==\"gph\") {\n        D = 1 / sqrt(D)\n        D_temp = matrix(0,nrow=length(D),ncol=length(D))\n        D_temp[cbind(1:length(D),1:length(D))] = D\n        D = D_temp\n        wn = D %*% (w %*% D)\n    }\n    else {\n        stop(\"Invalid type!\")\n    }\n    \n    return(wn)\n    \n}\n" }
{ "repo_name": "cran/Boom", "ref": "refs/heads/master", "path": "R/thin.R", "content": "thin <- function (x, thin) {\n  ## Thins the data in x, so that every thin'th observation is\n  ## returned.  This is useful for making plots of large MCMC objects,\n  ## where overplotting or memory constraints make plotting the whole\n  ## object undesirable.\n  ##\n  ## Args:\n  ##   x: A numeric vector, matrix, or array to be thinned.  If a\n  ##     matrix or array then the first dimension corresponds to the\n  ##     observation number, and will be used for thinning.\n  ##   thin: The frequency with which to thin.  E.g. if thin == 10\n  ##     then every 10th observation will be returned.\n  ##\n  ## Returns:\n  ##    The thinned subset of x.\n  stopifnot(is.numeric(thin) && length(thin) == 1)\n  if (thin <= 1)\n    return(x)\n  if (is.array(x)) {\n    nr <- dim(x)[1]\n  } else if (is.numeric(x)) {\n    nr <- length(x)\n  } else stop(\"x must be a numeric type in thin()\")\n\n  top <- floor(nr/thin)\n  indx <- (1:top) * thin\n\n  if (is.matrix(x)) {\n    return(x[indx, ])\n  } else if (is.numeric(x)) {\n    return(x[indx])\n  } else if (is.array(x)) {\n    stop(\"how do you drop the first N sub-components of an array?\")\n  }\n}\n" }
{ "repo_name": "cflagg/NEON-Data-Skills", "ref": "refs/heads/gh-pages", "path": "code/R/knitRMD-2-MD_mac.R", "content": "#!/usr/bin/env Rscript\n\n#input <- commandArgs(trailingOnly = TRUE)\n#KnitPost <- function(input, base.url = \"/\") {\n#  require(knitr)\n#  opts_knit$set(base.url = base.url)\n#  fig.path <- paste0(\"_posts/figs/\", sub(\".Rmd$\", \"\", basename(input)), \"/\")\n#  opts_chunk$set(fig.path = fig.path)\n#  opts_chunk$set(fig.cap = \"center\")\n#  render_jekyll()\n#  print(paste0(\"/_posts/\", sub(\".Rmd$\", \"\", basename(input)), \".md\"))\n#  knit(input, output = paste0(\"/_posts/\", sub(\".Rmd$\", \"\", basename(input)), \".md\"), envir = parent.frame())\n#}\n\n#KnitPost(input)\n\n#http://www.jonzelner.net/jekyll/knitr/r/2014/07/02/autogen-knitr/\n\n# Determine whether i'm on a MAC or PC, then define paths\nif(.Platform$OS.type == \"windows\") {\n  print(\"defining windows paths\")\n  #this is the path to the github repo on my PC\n  gitRepoPath <- \"C:/Users/lwasser/Documents/GitHub/NEON-Data-Skills/\" \n} else {\n    print(\"defining MAC paths\")\n    #this is the MAC path to the github repo\n    #gitRepoPath <- \"~/Documents/GitHub_Lwasser/NEON_DataSkills/\"\n    gitRepoPath <- \"~/Documents/GitHub/NEON-Data-Skills/\"\n    }\n\n#repoCodePath <- \"code/R/rmdFiles\"\nrepoCodePath <- \"code/R/\"\n#get the working dir where the data are stored\nwd <- getwd()\n#specify the file to be knit\n#\n#file <- \"2014-11-05-Intro-HDF5-R.Rmd\"\n#file <- \"2015-05-21-R-Timeseries-HDF5.Rmd\"\n#file <- \"2015-05-27-R-dplyr-GREPL-Summarise-Piping.Rmd\"\n#file  <- \"2015-06-08-Work-With-Hyperspectral-Data-In-R.Rmd\"\n#file <- \"2015-08-08-Plot-Hyperspectral-Spectra.Rmd\"\nfile <- \"2015-06-08-RasterStack-RGB-Images-in-R-Using-HSI.Rmd\"\n#file <- \"2015-06-30-Create-Lidar-CHM-R.Rmd\"\n#file <- \"2014-11-03-Working-With-Rasters-in-R-Python-GIS.Rmd\"\n#file <- \"2015-07-22-Introduction-to-Raster-Data-In-R.Rmd\"\n\n\n#copy .Rmd file to local working directory where the data are located\n#file.copy(from = (paste0(gitRepoPath,repoCodePath,file)), to=wd, overwrite = TRUE)\n\n#specify where should the file go within the GH repo\n#postsDir <- (\"_posts/SPATIALDATA/\")\npostsDir <- (\"_posts/HYPERSPECTRAL/\")\n\n#define the file path\nimagePath <- \"images/rfigs/\"\n# poth to RMD files\n\nrequire(knitr)\n\n#set the base url for images and links in the md file\nbase.url=\"{{ site.baseurl }}/\"\ninput=file\nopts_knit$set(base.url = base.url)\n#setup path to images\nprint(paste0(imagePath, sub(\".Rmd$\", \"\", basename(input)), \"/\"))\n\nfigDir <- print(paste0(\"images/\", sub(\".Rmd$\", \"\", basename(input)), \"/\"))\n\n#make sure image directory exists\n#if it doesn't exist, create it\n#note this will fail if the sub dir doesn't exist\nif (file.exists(paste0(wd,\"/\",\"images\"))){\n    print(\"image dir exists - all good\")\n  } else {\n    #create image directory structure\n    dir.create(file.path(wd, \"images/\"))\n    dir.create(file.path(wd, \"images/rfigs\"))\n    dir.create(file.path(wd, figDir))\n    print(\"image directories created!\")\n  }\n\nfig.path <- paste0(\"images/rfigs/\", sub(\".Rmd$\", \"\", basename(input)), \"/\")\nopts_chunk$set(fig.path = fig.path)\nopts_chunk$set(fig.cap = \" \")\n#render_jekyll()\nrender_markdown(strict = TRUE)\nprint(paste0(gitRepoPath,postsDir, sub(\".Rmd$\", \"\", basename(input)), \".md\"))\n#knit(input, output = paste0(filepath,\"/_posts/HDF5/\", sub(\".Rmd$\", \"\", basename(input)), \".md\"), envir = parent.frame())\n#knit the markdown doc\nknit(input, output = paste0(gitRepoPath,postsDir, sub(\".Rmd$\", \"\", basename(input)), \".md\"), envir = parent.frame())\npaste0(wd,\"/_posts\")\n\n\n#### COPY EVERYTHING OVER to the GIT SITE###\n#copy markdown directory over\n#note: this should all become variables\n\n#the code below isn't necessary if i knit it directly\n#file.copy(list.dirs(\"~/Documents/1_Workshops/R_HDF5Intr_NEON/_posts/HDF5\", full.names = TRUE), \"~/Documents/GitHub_Lwasser/NEON_DataSkills/_posts\", recursive=TRUE)\n#copy image directory over\nfile.copy(paste0(wd,\"/\",imagePath), paste0(gitRepoPath,\"images/\"), recursive=TRUE)\n#copy rmd file to the rmd directory on git\nfile.copy(paste0(wd,\"/\",file), paste0(gitRepoPath,\"code/R\"), recursive=TRUE)\n\n#delete local repo copies of RMD files just so things are cleaned up??\n\n## OUTPUT STUFF TO R ##\n#output code in R format\nrCodeOutput <- paste0(gitRepoPath,\"code/R/\", sub(\".Rmd$\", \"\", basename(input)), \".R\")\nrCodeOutput\n#purl the code to R\npurl(file, output = rCodeOutput)\n\n" }
{ "repo_name": "lwasser/NEON-Data-Skills", "ref": "refs/heads/gh-pages", "path": "code/R/knitRMD-2-MD_mac.R", "content": "#!/usr/bin/env Rscript\n\n#input <- commandArgs(trailingOnly = TRUE)\n#KnitPost <- function(input, base.url = \"/\") {\n#  require(knitr)\n#  opts_knit$set(base.url = base.url)\n#  fig.path <- paste0(\"_posts/figs/\", sub(\".Rmd$\", \"\", basename(input)), \"/\")\n#  opts_chunk$set(fig.path = fig.path)\n#  opts_chunk$set(fig.cap = \"center\")\n#  render_jekyll()\n#  print(paste0(\"/_posts/\", sub(\".Rmd$\", \"\", basename(input)), \".md\"))\n#  knit(input, output = paste0(\"/_posts/\", sub(\".Rmd$\", \"\", basename(input)), \".md\"), envir = parent.frame())\n#}\n\n#KnitPost(input)\n\n#http://www.jonzelner.net/jekyll/knitr/r/2014/07/02/autogen-knitr/\n\n# Determine whether i'm on a MAC or PC, then define paths\nif(.Platform$OS.type == \"windows\") {\n  print(\"defining windows paths\")\n  #this is the path to the github repo on my PC\n  gitRepoPath <- \"C:/Users/lwasser/Documents/GitHub/NEON-Data-Skills/\" \n} else {\n    print(\"defining MAC paths\")\n    #this is the MAC path to the github repo\n    #gitRepoPath <- \"~/Documents/GitHub_Lwasser/NEON_DataSkills/\"\n    gitRepoPath <- \"~/Documents/GitHub/NEON-Data-Skills/\"\n    }\n\n#repoCodePath <- \"code/R/rmdFiles\"\nrepoCodePath <- \"code/R/\"\n#get the working dir where the data are stored\nwd <- getwd()\n#specify the file to be knit\n#\n#file <- \"2014-11-05-Intro-HDF5-R.Rmd\"\n#file <- \"2015-05-21-R-Timeseries-HDF5.Rmd\"\n#file <- \"2015-05-27-R-dplyr-GREPL-Summarise-Piping.Rmd\"\n#file  <- \"2015-06-08-Work-With-Hyperspectral-Data-In-R.Rmd\"\n#file <- \"2015-08-08-Plot-Hyperspectral-Spectra.Rmd\"\nfile <- \"2015-06-08-RasterStack-RGB-Images-in-R-Using-HSI.Rmd\"\n#file <- \"2015-06-30-Create-Lidar-CHM-R.Rmd\"\n#file <- \"2014-11-03-Working-With-Rasters-in-R-Python-GIS.Rmd\"\n#file <- \"2015-07-22-Introduction-to-Raster-Data-In-R.Rmd\"\n\n\n#copy .Rmd file to local working directory where the data are located\n#file.copy(from = (paste0(gitRepoPath,repoCodePath,file)), to=wd, overwrite = TRUE)\n\n#specify where should the file go within the GH repo\n#postsDir <- (\"_posts/SPATIALDATA/\")\npostsDir <- (\"_posts/HYPERSPECTRAL/\")\n\n#define the file path\nimagePath <- \"images/rfigs/\"\n# poth to RMD files\n\nrequire(knitr)\n\n#set the base url for images and links in the md file\nbase.url=\"{{ site.baseurl }}/\"\ninput=file\nopts_knit$set(base.url = base.url)\n#setup path to images\nprint(paste0(imagePath, sub(\".Rmd$\", \"\", basename(input)), \"/\"))\n\nfigDir <- print(paste0(\"images/\", sub(\".Rmd$\", \"\", basename(input)), \"/\"))\n\n#make sure image directory exists\n#if it doesn't exist, create it\n#note this will fail if the sub dir doesn't exist\nif (file.exists(paste0(wd,\"/\",\"images\"))){\n    print(\"image dir exists - all good\")\n  } else {\n    #create image directory structure\n    dir.create(file.path(wd, \"images/\"))\n    dir.create(file.path(wd, \"images/rfigs\"))\n    dir.create(file.path(wd, figDir))\n    print(\"image directories created!\")\n  }\n\nfig.path <- paste0(\"images/rfigs/\", sub(\".Rmd$\", \"\", basename(input)), \"/\")\nopts_chunk$set(fig.path = fig.path)\nopts_chunk$set(fig.cap = \" \")\n#render_jekyll()\nrender_markdown(strict = TRUE)\nprint(paste0(gitRepoPath,postsDir, sub(\".Rmd$\", \"\", basename(input)), \".md\"))\n#knit(input, output = paste0(filepath,\"/_posts/HDF5/\", sub(\".Rmd$\", \"\", basename(input)), \".md\"), envir = parent.frame())\n#knit the markdown doc\nknit(input, output = paste0(gitRepoPath,postsDir, sub(\".Rmd$\", \"\", basename(input)), \".md\"), envir = parent.frame())\npaste0(wd,\"/_posts\")\n\n\n#### COPY EVERYTHING OVER to the GIT SITE###\n#copy markdown directory over\n#note: this should all become variables\n\n#the code below isn't necessary if i knit it directly\n#file.copy(list.dirs(\"~/Documents/1_Workshops/R_HDF5Intr_NEON/_posts/HDF5\", full.names = TRUE), \"~/Documents/GitHub_Lwasser/NEON_DataSkills/_posts\", recursive=TRUE)\n#copy image directory over\nfile.copy(paste0(wd,\"/\",imagePath), paste0(gitRepoPath,\"images/\"), recursive=TRUE)\n#copy rmd file to the rmd directory on git\nfile.copy(paste0(wd,\"/\",file), paste0(gitRepoPath,\"code/R\"), recursive=TRUE)\n\n#delete local repo copies of RMD files just so things are cleaned up??\n\n## OUTPUT STUFF TO R ##\n#output code in R format\nrCodeOutput <- paste0(gitRepoPath,\"code/R/\", sub(\".Rmd$\", \"\", basename(input)), \".R\")\nrCodeOutput\n#purl the code to R\npurl(file, output = rCodeOutput)\n\n" }
{ "repo_name": "terrytangyuan/reticulate", "ref": "refs/heads/master", "path": "tests/testthat/test-python-lists.R", "content": "context(\"lists\")\n\ntest_that(\"R named lists become Python dictionaries\", {\n  skip_if_no_python()\n  l <- list(a = 1, b = 2, c = 3)\n  reflected <- test$reflect(l)\n  expect_equal(l$a, reflected$a)\n  expect_equal(l$b, reflected$b)\n  expect_equal(l$c, reflected$c)\n})\n\ntest_that(\"Python dictionaries become R named lists\", {\n  skip_if_no_python()\n  l <- list(a = 1, b = 2, c = 3)\n  dict <- test$makeDict()\n  expect_equal(length(dict), length(l))\n  expect_equal(dict$a, l$a)\n  expect_equal(dict$b, l$b)\n  expect_equal(dict$c, l$c)\n})\n\ntest_that(\"R unnamed lists become Python lists\", {\n  skip_if_no_python()\n  l <- list(1L, 2L, 3L)\n  expect_equal(test$asString(l), \"[1, 2, 3]\")\n})\n\ntest_that(\"Python unnamed tuples become R unnamed lists\", {\n  skip_if_no_python()\n  l <- list(1, 2, 3)\n  tuple1 <- test$makeTuple()\n  expect_equal(tuple1, l)\n\n  expect_equal(length(tuple(l)), length(l))\n})\n\n\ntest_that(\"length method for Python lists works\", {\n  skip_if_no_python()\n  py <- import_builtins(convert = FALSE)\n  l <- py$list()\n  l$append(1)\n  l$append(2)\n  l$append(3)\n  expect_equal(length(l), 3)\n})\n\ntest_that(\"tuples are converted recursively just like lists\", {\n  skip_if_no_python()\n  t <- test$makeTupleWithOrderedDict()\n  expect_equal(class(t[[2]]), \"list\")\n})\n\n" }
{ "repo_name": "rstudio/reticulate", "ref": "refs/heads/master", "path": "tests/testthat/test-python-lists.R", "content": "context(\"lists\")\n\ntest_that(\"R named lists become Python dictionaries\", {\n  skip_if_no_python()\n  l <- list(a = 1, b = 2, c = 3)\n  reflected <- test$reflect(l)\n  expect_equal(l$a, reflected$a)\n  expect_equal(l$b, reflected$b)\n  expect_equal(l$c, reflected$c)\n})\n\ntest_that(\"Python dictionaries become R named lists\", {\n  skip_if_no_python()\n  l <- list(a = 1, b = 2, c = 3)\n  dict <- test$makeDict()\n  expect_equal(length(dict), length(l))\n  expect_equal(dict$a, l$a)\n  expect_equal(dict$b, l$b)\n  expect_equal(dict$c, l$c)\n})\n\ntest_that(\"R unnamed lists become Python lists\", {\n  skip_if_no_python()\n  l <- list(1L, 2L, 3L)\n  expect_equal(test$asString(l), \"[1, 2, 3]\")\n})\n\ntest_that(\"Python unnamed tuples become R unnamed lists\", {\n  skip_if_no_python()\n  l <- list(1, 2, 3)\n  tuple1 <- test$makeTuple()\n  expect_equal(tuple1, l)\n\n  expect_equal(length(tuple(l)), length(l))\n})\n\n\ntest_that(\"length method for Python lists works\", {\n  skip_if_no_python()\n  py <- import_builtins(convert = FALSE)\n  l <- py$list()\n  l$append(1)\n  l$append(2)\n  l$append(3)\n  expect_equal(length(l), 3)\n})\n\ntest_that(\"tuples are converted recursively just like lists\", {\n  skip_if_no_python()\n  t <- test$makeTupleWithOrderedDict()\n  expect_equal(class(t[[2]]), \"list\")\n})\n\n" }
{ "repo_name": "nimble-dev/nimble", "ref": "refs/heads/devel", "path": "packages/nimble/inst/classic-bugs/vol2/beetles/beetles-inits.R", "content": "\"alpha.star\" <- 0\n\"beta\" <- 0\n" }
{ "repo_name": "nxdao2000/nimble", "ref": "refs/heads/devel", "path": "packages/nimble/inst/classic-bugs/vol2/beetles/beetles-inits.R", "content": "\"alpha.star\" <- 0\n\"beta\" <- 0\n" }
{ "repo_name": "NLMichaud/nimble", "ref": "refs/heads/devel", "path": "packages/nimble/inst/classic-bugs/vol2/beetles/beetles-inits.R", "content": "\"alpha.star\" <- 0\n\"beta\" <- 0\n" }
{ "repo_name": "awslabs/aws-big-data-blog", "ref": "refs/heads/master", "path": "aws-blog-sparkr-geospatial-analysis/SparkRGeoInt.R", "content": "##############################################\n#    GeoSpatial Analysis Using SparkR on EMR\n#         ---By Gopal Wunnava, Senior Consultant, Amazon Web Services\n#This code demonstrates how to implement a geospatial use case using SparkR on EMR\n#We will use the GDELT dataset for this purpose which is available on EMR\n##############################################\n#Prerequisite: run \"sudo yum install libjpeg-turbo-devel\" to load this package which contains file jpeglib.h\n#Required R libraries -please ensure they have been installed\n#If not done so, install the required packages for creating geospatial application using command below\n#install.packages(c(\"plyr\",\"dplyr\",\"mapproj\",\"RgoogleMaps\",\"ggmap\"))\n\n#Save visualization results to pdf below\n#you can use setwd to change the path from the default current working directory\nsetwd(\"/home/hadoop\")\npdf(\"SparkRGEOINTEMR.pdf\")\n#load the required packages\nlibrary(RgoogleMaps)\nlibrary(ggmap)\nlibrary(mapproj)\nlibrary(plyr)\n\n#set up Hive Context\nhiveContext <- sparkRHive.init(sc)\nsql(hiveContext,\n\"\n CREATE EXTERNAL TABLE IF NOT EXISTS gdelt (\n GLOBALEVENTID BIGINT,\n SQLDATE INT,\n MonthYear INT,\n Year INT,\n FractionDate DOUBLE,\n Actor1Code STRING,\n Actor1Name STRING,\n Actor1CountryCode STRING,\n Actor1KnownGroupCode STRING,\n Actor1EthnicCode STRING,\n Actor1Religion1Code STRING,\n Actor1Religion2Code STRING,\n Actor1Type1Code STRING,\n Actor1Type2Code STRING,\n Actor1Type3Code STRING,\n Actor2Code STRING,\n Actor2Name STRING,\n Actor2CountryCode STRING,\n Actor2KnownGroupCode STRING,\n Actor2EthnicCode STRING,\n Actor2Religion1Code STRING,\n Actor2Religion2Code STRING,\n Actor2Type1Code STRING,\n Actor2Type2Code STRING,\n Actor2Type3Code STRING,\n IsRootEvent INT,\n EventCode STRING,\n EventBaseCode STRING,\n EventRootCode STRING,\n QuadClass INT,\n GoldsteinScale DOUBLE,\n NumMentions INT,\n NumSources INT,\n NumArticles INT,\n AvgTone DOUBLE,\n Actor1Geo_Type INT,\n Actor1Geo_FullName STRING,\n Actor1Geo_CountryCode STRING,\n Actor1Geo_ADM1Code STRING,\n Actor1Geo_Lat FLOAT,\n Actor1Geo_Long FLOAT,\n Actor1Geo_FeatureID INT,\n Actor2Geo_Type INT,\n Actor2Geo_FullName STRING,\n Actor2Geo_CountryCode STRING,\n Actor2Geo_ADM1Code STRING,\n Actor2Geo_Lat FLOAT,\n Actor2Geo_Long FLOAT,\n Actor2Geo_FeatureID INT,\n ActionGeo_Type INT,\n ActionGeo_FullName STRING,\n ActionGeo_CountryCode STRING,\n ActionGeo_ADM1Code STRING,\n ActionGeo_Lat FLOAT,\n ActionGeo_Long FLOAT,\n ActionGeo_FeatureID INT,\n DATEADDED INT,\n SOURCEURL STRING )\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE\nLOCATION 's3://support.elasticmapreduce/training/datasets/gdelt'\n\");\n\ngdelt<-sql(hiveContext,\"SELECT * FROM gdelt where ActionGeo_CountryCode IN ('IN','US') AND Year >= 2014\")\n\nregisterTempTable(gdelt, \"gdelt\")\ncacheTable(hiveContext, \"gdelt\")\n\n#====rename cols for readability\nnames(gdelt)[names(gdelt)==\"actiongeo_countrycode\"]=\"cn\"\nnames(gdelt)[names(gdelt)==\"actiongeo_lat\"]=\"lat\"\nnames(gdelt)[names(gdelt)==\"actiongeo_long\"]=\"long\"\nnames(gdelt)\n\n#------------------------USE CASE 1-------------------------\n#use Case 1 identifies where specific events of interest are taking place on a map(based on eventcode)\ngdt=gdelt[,\n\t\t\t c(\"sqldate\",\n\t\t\t \"eventcode\",\n\t\t\t \"globaleventid\", \n\t\t\t \"cn\",\n\t         \"year\",\n\t         \"lat\",\n\t         \"long\")\n\t\t     ]\nregisterTempTable(gdt, \"gdt\")\ncacheTable(hiveContext, \"gdt\")\n\n#Filter on specific events from above to focus on events of interest related to Economy\n#For this puprose, we will select CAMEO codes that pertain to economy\n#Refer to http://data.gdeltproject.org/documentation/CAMEO.Manual.1.1b3.pdf for details on these codes\necocodes <- c(\"0211\",\"0231\",\"0311\",\"0331\",\"061\",\"071\")\ngdeltusinf <- filter(gdt,gdt$eventcode %in% ecocodes)\nregisterTempTable(gdeltusinf, \"gdeltusinf\")\ncacheTable(hiveContext, \"gdeltusinf\")\n\ndflocale1=collect(select(gdeltusinf,\"*\"))\n#save native R dataframe results locally\nsave(dflocale1, file = \"gdeltlocale1.Rdata\")\n\n#Now filter these events by country (US and India) and plot on map\ndflocalus1=subset(dflocale1,dflocale1$cn=='US')\ndflocalin1=subset(dflocale1,dflocale1$cn=='IN')\n\n#get list of unique economy related CAMEO codes in US and India\nunique(dflocalus1$eventcode)\nunique(dflocalin1$eventcode)\n\n#Spatial Analysis of chosen events related to Economy in the US\nplot.new()\ntitle(\"GDELT Analysis for Economy related Events in 2014-2015\")\nmap=qmap('USA',zoom=3)\n\nmap + geom_point(data = dflocalus1, aes(x = dflocalus1$long, y = dflocalus1$lat), color=\"red\", size=0.5, alpha=0.5)\ntitle(\"All GDELT Event Locations in USA related to Economy in 2014-2015\")\n\n#now lets filter based on specific codes, 0211 (Economic Co-op for Appeals) first\ndflocalus0211=subset(dflocalus1,dflocalus1$eventcode=='0211')\nx0211=geom_point(data = dflocalus0211, aes(x = dflocalus0211$long, y = dflocalus0211$lat), color=\"blue\", size=2, alpha=0.5)\n\nmap+x0211\ntitle(\"GDELT Event Locations in USA: Economic Co-op(appeals)-Code 0211\")\n\n#lets filter further based on specific codes, 0231 (Economic Aid for Appeals) next\ndflocalus0231=subset(dflocalus1,dflocalus1$eventcode=='0231')\nx0231=geom_point(data = dflocalus0231, aes(x = dflocalus0231$long, y = dflocalus0231$lat), color=\"yellow\", size=2, alpha=0.5)\n\nmap+x0231\ntitle(\"GDELT Event Locations in USA:Economic Aid(appeals)-Code 0231\")\n\n#To display multiple events on a map, each with a different color\n\ndflocalus0311=subset(dflocalus1,dflocalus1$eventcode=='0311')\ndflocalus0331=subset(dflocalus1,dflocalus1$eventcode=='0331')\ndflocalus061=subset(dflocalus1,dflocalus1$eventcode=='061')\ndflocalus071=subset(dflocalus1,dflocalus1$eventcode=='071')\n\nx0211=geom_point(data = dflocalus0211, aes(x = dflocalus0211$long, y = dflocalus0211$lat), color=\"blue\", size=3, alpha=0.5)\nx0231=geom_point(data = dflocalus0231, aes(x = dflocalus0231$long, y = dflocalus0231$lat), color=\"yellow\", size=1, alpha=0.5)\nx0311=geom_point(data = dflocalus0311, aes(x = dflocalus0311$long, y = dflocalus0311$lat), color=\"red\", size=1, alpha=0.5)\nx0331=geom_point(data = dflocalus0331, aes(x = dflocalus0331$long, y = dflocalus0331$lat), color=\"green\", size=1, alpha=0.5)\nx061=geom_point(data = dflocalus061, aes(x = dflocalus061$long, y = dflocalus061$lat), color=\"orange\", size=1, alpha=0.5)\nx071=geom_point(data = dflocalus071, aes(x = dflocalus071$long, y = dflocalus071$lat), color=\"violet\", size=1, alpha=0.5)\n\n#Now let us display locations of 3 chosen CAMEO events related to economy in the US\nmap+x0211+x0231+x0311\n\nlegend('bottomleft',c(\"0211:Appeal for Economic Co-op\",\"0231:Appeal for Economic Aid\",\"0311:Intent for Economic Co-op\"),col=c(\"blue\",\"yellow\",\"red\"),pch=16)\ntitle(\"GDELT Locations In USA: Economy related Events in 2014-2015\")\n\n#Spatial Analysis of chosen events in India\nplot.new()\ntitle(\"GDELT Analysis in India: Events related to Economy in 2014-2015\")\n#title(\"Following Map will show locations of chosen GDELT Events in India\")\nmap=qmap('India',zoom=4)\nmap + geom_point(data = dflocalin1, aes(x = dflocalin1$long, y = dflocalin1$lat), color=\"red\", size=1, alpha=0.5)\ntitle(\"GDELT Event Locations in India:Events related to Economy\")\n\n#now lets filter based on specific codes, 0211 (Economic Co-op for Appeals) first\ndflocalin0211=subset(dflocalin1,dflocalin1$eventcode=='0211')\nx0211=geom_point(data = dflocalin0211, aes(x = dflocalin0211$long, y = dflocalin0211$lat), color=\"violet\", size=3, alpha=0.5)\nmap+x0211\ntitle(\"GDELT Event Locations in India:Economic Co-op (appeals) - code 0211\")\n\n#lets filter further based on specific codes, 0231 (Economic Aid for Appeals) next\ndflocalin0231=subset(dflocalin1,dflocalin1$eventcode=='0231')\nx0231=geom_point(data = dflocalin0231, aes(x = dflocalin0231$long, y = dflocalin0231$lat), color=\"yellow\", size=3, alpha=0.5)\nmap+x0231\ntitle(\"GDELT Event Locations in India: Economic Aid (appeals)-code 0231\")\n\n#To display multiple events on a map, each with a different color\ndflocalin0311=subset(dflocalin1,dflocalin1$eventcode=='0311')\ndflocalin0331=subset(dflocalin1,dflocalin1$eventcode=='0331')\ndflocalin061=subset(dflocalin1,dflocalin1$eventcode=='061')\ndflocalin071=subset(dflocalin1,dflocalin1$eventcode=='071')\n\n\nx0311=geom_point(data = dflocalin0311, aes(x = dflocalin0311$long, y = dflocalin0311$lat), color=\"orange\", size=1, alpha=0.5)\nx0331=geom_point(data = dflocalin0331, aes(x = dflocalin0331$long, y = dflocalin0331$lat), color=\"blue\", size=3, alpha=0.5)\nx061=geom_point(data = dflocalin061, aes(x = dflocalin061$long, y = dflocalin061$lat), color=\"orange\", size=1, alpha=0.5)\nx071=geom_point(data = dflocalin071, aes(x = dflocalin071$long, y = dflocalin071$lat), color=\"red\", size=1, alpha=0.5)\n\n#Now let us display locations of 3 chosen CAMEO events related to economy in India\nmap+x0231+x0331+x071\nlegend('topright',c(\"0231:Appeal for Economic Aid\",\"0331:Intent for Economic Aid\",\"071:Provide Economic Aid\"),col=c(\"yellow\",\"blue\",\"red\"),pch=16)\ntitle(\"GDELT Event Locations in India:Economic Events in Years 2014-2015\")\n\n#------------------------USE CASE 2-------------------------\n#Use Case 2 identifies Top 25 locations with highest density/frequency of events\ngdy<-sql(hiveContext,\"SELECT cn,\t\t\t\t\t\t\t  \n\t\t\t\t\t\t\t lat,\n\t\t\t\t\t\t\t long,\n\t\t\t\t\t\t\t year,\n\t\t\t\t\t\t\t count(eventcode) as totevents\t\t       \n\t\t\t\t\t\t\t FROM gdt\n\t\t\t\t\t\t\t GROUP BY cn,lat,long,year\n\t\t\t\t\t\t\t \")\n\nregisterTempTable(gdy, \"gdy\")\ncacheTable(hiveContext, \"gdy\")\n\n#save results locally\ndflocal2=collect(select(gdy,\"*\"))\nsave(dflocal2, file = \"gdeltlocal2.Rdata\")\n\n#Determine top 25 locations with highest density of events in each country\n#First let us filter by country\n#We can use a different color for each year (i.e filter by year within each country and apply diff color)\n\n#filter by US and IN\ndflocalus2=subset(dflocal2,dflocal2$cn=='US')\ndflocalin2=subset(dflocal2,dflocal2$cn=='IN')\n#then by count of events\ndfustop=head(arrange(dflocalus2,desc(totevents)),25)\ndfintop=head(arrange(dflocalin2,desc(totevents)),25)\ndfustop\ndfintop\n\n#let's plot on map for US\nplot.new()\ntitle(\"GDELT Event Locations in 2014-2015:Density of Economy based Events\",font=3)\nmap=qmap('USA',zoom=3)\n\nmap + geom_point(data = dfustop, aes(x = dfustop$long, y = dfustop$lat), color=\"red\", size=4, alpha=0.5)\ntitle(\"Top 25 GDELT Economy related Event Locations in USA\",font=2)\n\n#lets plot on map for IN\nmap=qmap('India',zoom=4)\n\nmap + geom_point(data = dfintop, aes(x = dfintop$long, y = dfintop$lat), color=\"red\", size=4, alpha=0.5)\ntitle(\"Top 25 GDELT Economy related Event Locations in India\",font=2)\ndev.off()\n#stop sparkR context\nsparkR.stop()\nls()\n###################Core Code Logic above###########################" }
{ "repo_name": "IanMeyers/aws-big-data-blog", "ref": "refs/heads/master", "path": "aws-blog-sparkr-geospatial-analysis/SparkRGeoInt.R", "content": "##############################################\n#    GeoSpatial Analysis Using SparkR on EMR\n#         ---By Gopal Wunnava, Senior Consultant, Amazon Web Services\n#This code demonstrates how to implement a geospatial use case using SparkR on EMR\n#We will use the GDELT dataset for this purpose which is available on EMR\n##############################################\n#Prerequisite: run \"sudo yum install libjpeg-turbo-devel\" to load this package which contains file jpeglib.h\n#Required R libraries -please ensure they have been installed\n#If not done so, install the required packages for creating geospatial application using command below\n#install.packages(c(\"plyr\",\"dplyr\",\"mapproj\",\"RgoogleMaps\",\"ggmap\"))\n\n#Save visualization results to pdf below\n#you can use setwd to change the path from the default current working directory\nsetwd(\"/home/hadoop\")\npdf(\"SparkRGEOINTEMR.pdf\")\n#load the required packages\nlibrary(RgoogleMaps)\nlibrary(ggmap)\nlibrary(mapproj)\nlibrary(plyr)\n\n#set up Hive Context\nhiveContext <- sparkRHive.init(sc)\nsql(hiveContext,\n\"\n CREATE EXTERNAL TABLE IF NOT EXISTS gdelt (\n GLOBALEVENTID BIGINT,\n SQLDATE INT,\n MonthYear INT,\n Year INT,\n FractionDate DOUBLE,\n Actor1Code STRING,\n Actor1Name STRING,\n Actor1CountryCode STRING,\n Actor1KnownGroupCode STRING,\n Actor1EthnicCode STRING,\n Actor1Religion1Code STRING,\n Actor1Religion2Code STRING,\n Actor1Type1Code STRING,\n Actor1Type2Code STRING,\n Actor1Type3Code STRING,\n Actor2Code STRING,\n Actor2Name STRING,\n Actor2CountryCode STRING,\n Actor2KnownGroupCode STRING,\n Actor2EthnicCode STRING,\n Actor2Religion1Code STRING,\n Actor2Religion2Code STRING,\n Actor2Type1Code STRING,\n Actor2Type2Code STRING,\n Actor2Type3Code STRING,\n IsRootEvent INT,\n EventCode STRING,\n EventBaseCode STRING,\n EventRootCode STRING,\n QuadClass INT,\n GoldsteinScale DOUBLE,\n NumMentions INT,\n NumSources INT,\n NumArticles INT,\n AvgTone DOUBLE,\n Actor1Geo_Type INT,\n Actor1Geo_FullName STRING,\n Actor1Geo_CountryCode STRING,\n Actor1Geo_ADM1Code STRING,\n Actor1Geo_Lat FLOAT,\n Actor1Geo_Long FLOAT,\n Actor1Geo_FeatureID INT,\n Actor2Geo_Type INT,\n Actor2Geo_FullName STRING,\n Actor2Geo_CountryCode STRING,\n Actor2Geo_ADM1Code STRING,\n Actor2Geo_Lat FLOAT,\n Actor2Geo_Long FLOAT,\n Actor2Geo_FeatureID INT,\n ActionGeo_Type INT,\n ActionGeo_FullName STRING,\n ActionGeo_CountryCode STRING,\n ActionGeo_ADM1Code STRING,\n ActionGeo_Lat FLOAT,\n ActionGeo_Long FLOAT,\n ActionGeo_FeatureID INT,\n DATEADDED INT,\n SOURCEURL STRING )\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE\nLOCATION 's3://support.elasticmapreduce/training/datasets/gdelt'\n\");\n\ngdelt<-sql(hiveContext,\"SELECT * FROM gdelt where ActionGeo_CountryCode IN ('IN','US') AND Year >= 2014\")\n\nregisterTempTable(gdelt, \"gdelt\")\ncacheTable(hiveContext, \"gdelt\")\n\n#====rename cols for readability\nnames(gdelt)[names(gdelt)==\"actiongeo_countrycode\"]=\"cn\"\nnames(gdelt)[names(gdelt)==\"actiongeo_lat\"]=\"lat\"\nnames(gdelt)[names(gdelt)==\"actiongeo_long\"]=\"long\"\nnames(gdelt)\n\n#------------------------USE CASE 1-------------------------\n#use Case 1 identifies where specific events of interest are taking place on a map(based on eventcode)\ngdt=gdelt[,\n\t\t\t c(\"sqldate\",\n\t\t\t \"eventcode\",\n\t\t\t \"globaleventid\", \n\t\t\t \"cn\",\n\t         \"year\",\n\t         \"lat\",\n\t         \"long\")\n\t\t     ]\nregisterTempTable(gdt, \"gdt\")\ncacheTable(hiveContext, \"gdt\")\n\n#Filter on specific events from above to focus on events of interest related to Economy\n#For this puprose, we will select CAMEO codes that pertain to economy\n#Refer to http://data.gdeltproject.org/documentation/CAMEO.Manual.1.1b3.pdf for details on these codes\necocodes <- c(\"0211\",\"0231\",\"0311\",\"0331\",\"061\",\"071\")\ngdeltusinf <- filter(gdt,gdt$eventcode %in% ecocodes)\nregisterTempTable(gdeltusinf, \"gdeltusinf\")\ncacheTable(hiveContext, \"gdeltusinf\")\n\ndflocale1=collect(select(gdeltusinf,\"*\"))\n#save native R dataframe results locally\nsave(dflocale1, file = \"gdeltlocale1.Rdata\")\n\n#Now filter these events by country (US and India) and plot on map\ndflocalus1=subset(dflocale1,dflocale1$cn=='US')\ndflocalin1=subset(dflocale1,dflocale1$cn=='IN')\n\n#get list of unique economy related CAMEO codes in US and India\nunique(dflocalus1$eventcode)\nunique(dflocalin1$eventcode)\n\n#Spatial Analysis of chosen events related to Economy in the US\nplot.new()\ntitle(\"GDELT Analysis for Economy related Events in 2014-2015\")\nmap=qmap('USA',zoom=3)\n\nmap + geom_point(data = dflocalus1, aes(x = dflocalus1$long, y = dflocalus1$lat), color=\"red\", size=0.5, alpha=0.5)\ntitle(\"All GDELT Event Locations in USA related to Economy in 2014-2015\")\n\n#now lets filter based on specific codes, 0211 (Economic Co-op for Appeals) first\ndflocalus0211=subset(dflocalus1,dflocalus1$eventcode=='0211')\nx0211=geom_point(data = dflocalus0211, aes(x = dflocalus0211$long, y = dflocalus0211$lat), color=\"blue\", size=2, alpha=0.5)\n\nmap+x0211\ntitle(\"GDELT Event Locations in USA: Economic Co-op(appeals)-Code 0211\")\n\n#lets filter further based on specific codes, 0231 (Economic Aid for Appeals) next\ndflocalus0231=subset(dflocalus1,dflocalus1$eventcode=='0231')\nx0231=geom_point(data = dflocalus0231, aes(x = dflocalus0231$long, y = dflocalus0231$lat), color=\"yellow\", size=2, alpha=0.5)\n\nmap+x0231\ntitle(\"GDELT Event Locations in USA:Economic Aid(appeals)-Code 0231\")\n\n#To display multiple events on a map, each with a different color\n\ndflocalus0311=subset(dflocalus1,dflocalus1$eventcode=='0311')\ndflocalus0331=subset(dflocalus1,dflocalus1$eventcode=='0331')\ndflocalus061=subset(dflocalus1,dflocalus1$eventcode=='061')\ndflocalus071=subset(dflocalus1,dflocalus1$eventcode=='071')\n\nx0211=geom_point(data = dflocalus0211, aes(x = dflocalus0211$long, y = dflocalus0211$lat), color=\"blue\", size=3, alpha=0.5)\nx0231=geom_point(data = dflocalus0231, aes(x = dflocalus0231$long, y = dflocalus0231$lat), color=\"yellow\", size=1, alpha=0.5)\nx0311=geom_point(data = dflocalus0311, aes(x = dflocalus0311$long, y = dflocalus0311$lat), color=\"red\", size=1, alpha=0.5)\nx0331=geom_point(data = dflocalus0331, aes(x = dflocalus0331$long, y = dflocalus0331$lat), color=\"green\", size=1, alpha=0.5)\nx061=geom_point(data = dflocalus061, aes(x = dflocalus061$long, y = dflocalus061$lat), color=\"orange\", size=1, alpha=0.5)\nx071=geom_point(data = dflocalus071, aes(x = dflocalus071$long, y = dflocalus071$lat), color=\"violet\", size=1, alpha=0.5)\n\n#Now let us display locations of 3 chosen CAMEO events related to economy in the US\nmap+x0211+x0231+x0311\n\nlegend('bottomleft',c(\"0211:Appeal for Economic Co-op\",\"0231:Appeal for Economic Aid\",\"0311:Intent for Economic Co-op\"),col=c(\"blue\",\"yellow\",\"red\"),pch=16)\ntitle(\"GDELT Locations In USA: Economy related Events in 2014-2015\")\n\n#Spatial Analysis of chosen events in India\nplot.new()\ntitle(\"GDELT Analysis in India: Events related to Economy in 2014-2015\")\n#title(\"Following Map will show locations of chosen GDELT Events in India\")\nmap=qmap('India',zoom=4)\nmap + geom_point(data = dflocalin1, aes(x = dflocalin1$long, y = dflocalin1$lat), color=\"red\", size=1, alpha=0.5)\ntitle(\"GDELT Event Locations in India:Events related to Economy\")\n\n#now lets filter based on specific codes, 0211 (Economic Co-op for Appeals) first\ndflocalin0211=subset(dflocalin1,dflocalin1$eventcode=='0211')\nx0211=geom_point(data = dflocalin0211, aes(x = dflocalin0211$long, y = dflocalin0211$lat), color=\"violet\", size=3, alpha=0.5)\nmap+x0211\ntitle(\"GDELT Event Locations in India:Economic Co-op (appeals) - code 0211\")\n\n#lets filter further based on specific codes, 0231 (Economic Aid for Appeals) next\ndflocalin0231=subset(dflocalin1,dflocalin1$eventcode=='0231')\nx0231=geom_point(data = dflocalin0231, aes(x = dflocalin0231$long, y = dflocalin0231$lat), color=\"yellow\", size=3, alpha=0.5)\nmap+x0231\ntitle(\"GDELT Event Locations in India: Economic Aid (appeals)-code 0231\")\n\n#To display multiple events on a map, each with a different color\ndflocalin0311=subset(dflocalin1,dflocalin1$eventcode=='0311')\ndflocalin0331=subset(dflocalin1,dflocalin1$eventcode=='0331')\ndflocalin061=subset(dflocalin1,dflocalin1$eventcode=='061')\ndflocalin071=subset(dflocalin1,dflocalin1$eventcode=='071')\n\n\nx0311=geom_point(data = dflocalin0311, aes(x = dflocalin0311$long, y = dflocalin0311$lat), color=\"orange\", size=1, alpha=0.5)\nx0331=geom_point(data = dflocalin0331, aes(x = dflocalin0331$long, y = dflocalin0331$lat), color=\"blue\", size=3, alpha=0.5)\nx061=geom_point(data = dflocalin061, aes(x = dflocalin061$long, y = dflocalin061$lat), color=\"orange\", size=1, alpha=0.5)\nx071=geom_point(data = dflocalin071, aes(x = dflocalin071$long, y = dflocalin071$lat), color=\"red\", size=1, alpha=0.5)\n\n#Now let us display locations of 3 chosen CAMEO events related to economy in India\nmap+x0231+x0331+x071\nlegend('topright',c(\"0231:Appeal for Economic Aid\",\"0331:Intent for Economic Aid\",\"071:Provide Economic Aid\"),col=c(\"yellow\",\"blue\",\"red\"),pch=16)\ntitle(\"GDELT Event Locations in India:Economic Events in Years 2014-2015\")\n\n#------------------------USE CASE 2-------------------------\n#Use Case 2 identifies Top 25 locations with highest density/frequency of events\ngdy<-sql(hiveContext,\"SELECT cn,\t\t\t\t\t\t\t  \n\t\t\t\t\t\t\t lat,\n\t\t\t\t\t\t\t long,\n\t\t\t\t\t\t\t year,\n\t\t\t\t\t\t\t count(eventcode) as totevents\t\t       \n\t\t\t\t\t\t\t FROM gdt\n\t\t\t\t\t\t\t GROUP BY cn,lat,long,year\n\t\t\t\t\t\t\t \")\n\nregisterTempTable(gdy, \"gdy\")\ncacheTable(hiveContext, \"gdy\")\n\n#save results locally\ndflocal2=collect(select(gdy,\"*\"))\nsave(dflocal2, file = \"gdeltlocal2.Rdata\")\n\n#Determine top 25 locations with highest density of events in each country\n#First let us filter by country\n#We can use a different color for each year (i.e filter by year within each country and apply diff color)\n\n#filter by US and IN\ndflocalus2=subset(dflocal2,dflocal2$cn=='US')\ndflocalin2=subset(dflocal2,dflocal2$cn=='IN')\n#then by count of events\ndfustop=head(arrange(dflocalus2,desc(totevents)),25)\ndfintop=head(arrange(dflocalin2,desc(totevents)),25)\ndfustop\ndfintop\n\n#let's plot on map for US\nplot.new()\ntitle(\"GDELT Event Locations in 2014-2015:Density of Economy based Events\",font=3)\nmap=qmap('USA',zoom=3)\n\nmap + geom_point(data = dfustop, aes(x = dfustop$long, y = dfustop$lat), color=\"red\", size=4, alpha=0.5)\ntitle(\"Top 25 GDELT Economy related Event Locations in USA\",font=2)\n\n#lets plot on map for IN\nmap=qmap('India',zoom=4)\n\nmap + geom_point(data = dfintop, aes(x = dfintop$long, y = dfintop$lat), color=\"red\", size=4, alpha=0.5)\ntitle(\"Top 25 GDELT Economy related Event Locations in India\",font=2)\ndev.off()\n#stop sparkR context\nsparkR.stop()\nls()\n###################Core Code Logic above###########################" }
{ "repo_name": "hvivani/aws-big-data-blog", "ref": "refs/heads/master", "path": "aws-blog-sparkr-geospatial-analysis/SparkRGeoInt.R", "content": "##############################################\n#    GeoSpatial Analysis Using SparkR on EMR\n#         ---By Gopal Wunnava, Senior Consultant, Amazon Web Services\n#This code demonstrates how to implement a geospatial use case using SparkR on EMR\n#We will use the GDELT dataset for this purpose which is available on EMR\n##############################################\n#Prerequisite: run \"sudo yum install libjpeg-turbo-devel\" to load this package which contains file jpeglib.h\n#Required R libraries -please ensure they have been installed\n#If not done so, install the required packages for creating geospatial application using command below\n#install.packages(c(\"plyr\",\"dplyr\",\"mapproj\",\"RgoogleMaps\",\"ggmap\"))\n\n#Save visualization results to pdf below\n#you can use setwd to change the path from the default current working directory\nsetwd(\"/home/hadoop\")\npdf(\"SparkRGEOINTEMR.pdf\")\n#load the required packages\nlibrary(RgoogleMaps)\nlibrary(ggmap)\nlibrary(mapproj)\nlibrary(plyr)\n\n#set up Hive Context\nhiveContext <- sparkRHive.init(sc)\nsql(hiveContext,\n\"\n CREATE EXTERNAL TABLE IF NOT EXISTS gdelt (\n GLOBALEVENTID BIGINT,\n SQLDATE INT,\n MonthYear INT,\n Year INT,\n FractionDate DOUBLE,\n Actor1Code STRING,\n Actor1Name STRING,\n Actor1CountryCode STRING,\n Actor1KnownGroupCode STRING,\n Actor1EthnicCode STRING,\n Actor1Religion1Code STRING,\n Actor1Religion2Code STRING,\n Actor1Type1Code STRING,\n Actor1Type2Code STRING,\n Actor1Type3Code STRING,\n Actor2Code STRING,\n Actor2Name STRING,\n Actor2CountryCode STRING,\n Actor2KnownGroupCode STRING,\n Actor2EthnicCode STRING,\n Actor2Religion1Code STRING,\n Actor2Religion2Code STRING,\n Actor2Type1Code STRING,\n Actor2Type2Code STRING,\n Actor2Type3Code STRING,\n IsRootEvent INT,\n EventCode STRING,\n EventBaseCode STRING,\n EventRootCode STRING,\n QuadClass INT,\n GoldsteinScale DOUBLE,\n NumMentions INT,\n NumSources INT,\n NumArticles INT,\n AvgTone DOUBLE,\n Actor1Geo_Type INT,\n Actor1Geo_FullName STRING,\n Actor1Geo_CountryCode STRING,\n Actor1Geo_ADM1Code STRING,\n Actor1Geo_Lat FLOAT,\n Actor1Geo_Long FLOAT,\n Actor1Geo_FeatureID INT,\n Actor2Geo_Type INT,\n Actor2Geo_FullName STRING,\n Actor2Geo_CountryCode STRING,\n Actor2Geo_ADM1Code STRING,\n Actor2Geo_Lat FLOAT,\n Actor2Geo_Long FLOAT,\n Actor2Geo_FeatureID INT,\n ActionGeo_Type INT,\n ActionGeo_FullName STRING,\n ActionGeo_CountryCode STRING,\n ActionGeo_ADM1Code STRING,\n ActionGeo_Lat FLOAT,\n ActionGeo_Long FLOAT,\n ActionGeo_FeatureID INT,\n DATEADDED INT,\n SOURCEURL STRING )\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE\nLOCATION 's3://support.elasticmapreduce/training/datasets/gdelt'\n\");\n\ngdelt<-sql(hiveContext,\"SELECT * FROM gdelt where ActionGeo_CountryCode IN ('IN','US') AND Year >= 2014\")\n\nregisterTempTable(gdelt, \"gdelt\")\ncacheTable(hiveContext, \"gdelt\")\n\n#====rename cols for readability\nnames(gdelt)[names(gdelt)==\"actiongeo_countrycode\"]=\"cn\"\nnames(gdelt)[names(gdelt)==\"actiongeo_lat\"]=\"lat\"\nnames(gdelt)[names(gdelt)==\"actiongeo_long\"]=\"long\"\nnames(gdelt)\n\n#------------------------USE CASE 1-------------------------\n#use Case 1 identifies where specific events of interest are taking place on a map(based on eventcode)\ngdt=gdelt[,\n\t\t\t c(\"sqldate\",\n\t\t\t \"eventcode\",\n\t\t\t \"globaleventid\", \n\t\t\t \"cn\",\n\t         \"year\",\n\t         \"lat\",\n\t         \"long\")\n\t\t     ]\nregisterTempTable(gdt, \"gdt\")\ncacheTable(hiveContext, \"gdt\")\n\n#Filter on specific events from above to focus on events of interest related to Economy\n#For this puprose, we will select CAMEO codes that pertain to economy\n#Refer to http://data.gdeltproject.org/documentation/CAMEO.Manual.1.1b3.pdf for details on these codes\necocodes <- c(\"0211\",\"0231\",\"0311\",\"0331\",\"061\",\"071\")\ngdeltusinf <- filter(gdt,gdt$eventcode %in% ecocodes)\nregisterTempTable(gdeltusinf, \"gdeltusinf\")\ncacheTable(hiveContext, \"gdeltusinf\")\n\ndflocale1=collect(select(gdeltusinf,\"*\"))\n#save native R dataframe results locally\nsave(dflocale1, file = \"gdeltlocale1.Rdata\")\n\n#Now filter these events by country (US and India) and plot on map\ndflocalus1=subset(dflocale1,dflocale1$cn=='US')\ndflocalin1=subset(dflocale1,dflocale1$cn=='IN')\n\n#get list of unique economy related CAMEO codes in US and India\nunique(dflocalus1$eventcode)\nunique(dflocalin1$eventcode)\n\n#Spatial Analysis of chosen events related to Economy in the US\nplot.new()\ntitle(\"GDELT Analysis for Economy related Events in 2014-2015\")\nmap=qmap('USA',zoom=3)\n\nmap + geom_point(data = dflocalus1, aes(x = dflocalus1$long, y = dflocalus1$lat), color=\"red\", size=0.5, alpha=0.5)\ntitle(\"All GDELT Event Locations in USA related to Economy in 2014-2015\")\n\n#now lets filter based on specific codes, 0211 (Economic Co-op for Appeals) first\ndflocalus0211=subset(dflocalus1,dflocalus1$eventcode=='0211')\nx0211=geom_point(data = dflocalus0211, aes(x = dflocalus0211$long, y = dflocalus0211$lat), color=\"blue\", size=2, alpha=0.5)\n\nmap+x0211\ntitle(\"GDELT Event Locations in USA: Economic Co-op(appeals)-Code 0211\")\n\n#lets filter further based on specific codes, 0231 (Economic Aid for Appeals) next\ndflocalus0231=subset(dflocalus1,dflocalus1$eventcode=='0231')\nx0231=geom_point(data = dflocalus0231, aes(x = dflocalus0231$long, y = dflocalus0231$lat), color=\"yellow\", size=2, alpha=0.5)\n\nmap+x0231\ntitle(\"GDELT Event Locations in USA:Economic Aid(appeals)-Code 0231\")\n\n#To display multiple events on a map, each with a different color\n\ndflocalus0311=subset(dflocalus1,dflocalus1$eventcode=='0311')\ndflocalus0331=subset(dflocalus1,dflocalus1$eventcode=='0331')\ndflocalus061=subset(dflocalus1,dflocalus1$eventcode=='061')\ndflocalus071=subset(dflocalus1,dflocalus1$eventcode=='071')\n\nx0211=geom_point(data = dflocalus0211, aes(x = dflocalus0211$long, y = dflocalus0211$lat), color=\"blue\", size=3, alpha=0.5)\nx0231=geom_point(data = dflocalus0231, aes(x = dflocalus0231$long, y = dflocalus0231$lat), color=\"yellow\", size=1, alpha=0.5)\nx0311=geom_point(data = dflocalus0311, aes(x = dflocalus0311$long, y = dflocalus0311$lat), color=\"red\", size=1, alpha=0.5)\nx0331=geom_point(data = dflocalus0331, aes(x = dflocalus0331$long, y = dflocalus0331$lat), color=\"green\", size=1, alpha=0.5)\nx061=geom_point(data = dflocalus061, aes(x = dflocalus061$long, y = dflocalus061$lat), color=\"orange\", size=1, alpha=0.5)\nx071=geom_point(data = dflocalus071, aes(x = dflocalus071$long, y = dflocalus071$lat), color=\"violet\", size=1, alpha=0.5)\n\n#Now let us display locations of 3 chosen CAMEO events related to economy in the US\nmap+x0211+x0231+x0311\n\nlegend('bottomleft',c(\"0211:Appeal for Economic Co-op\",\"0231:Appeal for Economic Aid\",\"0311:Intent for Economic Co-op\"),col=c(\"blue\",\"yellow\",\"red\"),pch=16)\ntitle(\"GDELT Locations In USA: Economy related Events in 2014-2015\")\n\n#Spatial Analysis of chosen events in India\nplot.new()\ntitle(\"GDELT Analysis in India: Events related to Economy in 2014-2015\")\n#title(\"Following Map will show locations of chosen GDELT Events in India\")\nmap=qmap('India',zoom=4)\nmap + geom_point(data = dflocalin1, aes(x = dflocalin1$long, y = dflocalin1$lat), color=\"red\", size=1, alpha=0.5)\ntitle(\"GDELT Event Locations in India:Events related to Economy\")\n\n#now lets filter based on specific codes, 0211 (Economic Co-op for Appeals) first\ndflocalin0211=subset(dflocalin1,dflocalin1$eventcode=='0211')\nx0211=geom_point(data = dflocalin0211, aes(x = dflocalin0211$long, y = dflocalin0211$lat), color=\"violet\", size=3, alpha=0.5)\nmap+x0211\ntitle(\"GDELT Event Locations in India:Economic Co-op (appeals) - code 0211\")\n\n#lets filter further based on specific codes, 0231 (Economic Aid for Appeals) next\ndflocalin0231=subset(dflocalin1,dflocalin1$eventcode=='0231')\nx0231=geom_point(data = dflocalin0231, aes(x = dflocalin0231$long, y = dflocalin0231$lat), color=\"yellow\", size=3, alpha=0.5)\nmap+x0231\ntitle(\"GDELT Event Locations in India: Economic Aid (appeals)-code 0231\")\n\n#To display multiple events on a map, each with a different color\ndflocalin0311=subset(dflocalin1,dflocalin1$eventcode=='0311')\ndflocalin0331=subset(dflocalin1,dflocalin1$eventcode=='0331')\ndflocalin061=subset(dflocalin1,dflocalin1$eventcode=='061')\ndflocalin071=subset(dflocalin1,dflocalin1$eventcode=='071')\n\n\nx0311=geom_point(data = dflocalin0311, aes(x = dflocalin0311$long, y = dflocalin0311$lat), color=\"orange\", size=1, alpha=0.5)\nx0331=geom_point(data = dflocalin0331, aes(x = dflocalin0331$long, y = dflocalin0331$lat), color=\"blue\", size=3, alpha=0.5)\nx061=geom_point(data = dflocalin061, aes(x = dflocalin061$long, y = dflocalin061$lat), color=\"orange\", size=1, alpha=0.5)\nx071=geom_point(data = dflocalin071, aes(x = dflocalin071$long, y = dflocalin071$lat), color=\"red\", size=1, alpha=0.5)\n\n#Now let us display locations of 3 chosen CAMEO events related to economy in India\nmap+x0231+x0331+x071\nlegend('topright',c(\"0231:Appeal for Economic Aid\",\"0331:Intent for Economic Aid\",\"071:Provide Economic Aid\"),col=c(\"yellow\",\"blue\",\"red\"),pch=16)\ntitle(\"GDELT Event Locations in India:Economic Events in Years 2014-2015\")\n\n#------------------------USE CASE 2-------------------------\n#Use Case 2 identifies Top 25 locations with highest density/frequency of events\ngdy<-sql(hiveContext,\"SELECT cn,\t\t\t\t\t\t\t  \n\t\t\t\t\t\t\t lat,\n\t\t\t\t\t\t\t long,\n\t\t\t\t\t\t\t year,\n\t\t\t\t\t\t\t count(eventcode) as totevents\t\t       \n\t\t\t\t\t\t\t FROM gdt\n\t\t\t\t\t\t\t GROUP BY cn,lat,long,year\n\t\t\t\t\t\t\t \")\n\nregisterTempTable(gdy, \"gdy\")\ncacheTable(hiveContext, \"gdy\")\n\n#save results locally\ndflocal2=collect(select(gdy,\"*\"))\nsave(dflocal2, file = \"gdeltlocal2.Rdata\")\n\n#Determine top 25 locations with highest density of events in each country\n#First let us filter by country\n#We can use a different color for each year (i.e filter by year within each country and apply diff color)\n\n#filter by US and IN\ndflocalus2=subset(dflocal2,dflocal2$cn=='US')\ndflocalin2=subset(dflocal2,dflocal2$cn=='IN')\n#then by count of events\ndfustop=head(arrange(dflocalus2,desc(totevents)),25)\ndfintop=head(arrange(dflocalin2,desc(totevents)),25)\ndfustop\ndfintop\n\n#let's plot on map for US\nplot.new()\ntitle(\"GDELT Event Locations in 2014-2015:Density of Economy based Events\",font=3)\nmap=qmap('USA',zoom=3)\n\nmap + geom_point(data = dfustop, aes(x = dfustop$long, y = dfustop$lat), color=\"red\", size=4, alpha=0.5)\ntitle(\"Top 25 GDELT Economy related Event Locations in USA\",font=2)\n\n#lets plot on map for IN\nmap=qmap('India',zoom=4)\n\nmap + geom_point(data = dfintop, aes(x = dfintop$long, y = dfintop$lat), color=\"red\", size=4, alpha=0.5)\ntitle(\"Top 25 GDELT Economy related Event Locations in India\",font=2)\ndev.off()\n#stop sparkR context\nsparkR.stop()\nls()\n###################Core Code Logic above###########################" }
{ "repo_name": "mcwalters/RNeo4j", "ref": "refs/heads/master", "path": "R/getLabeledNodes.R", "content": "getLabeledNodes = function(graph, .label, ...) UseMethod(\"getLabeledNodes\")\n\ngetLabeledNodes.default = function(x, ...) {\n  stop(\"Invalid object. Must supply graph object.\")\n}\n\ngetLabeledNodes.graph = function(graph, .label, ...) {\n  stopifnot(is.character(.label))\n\n  url = paste(attr(graph, \"root\"), \"label\", .label, \"nodes\", sep = \"/\")\n  param = c(...)\n  \n  if(length(param) > 1)\n    stop(\"Can only search by one property.\")\n\n  if(length(param) == 1) {\n    url = paste0(url, \"?\", names(param), \"=\")\n    \n    if(is.character(param[[1]])) {\n      param[[1]] = URLencode(param[[1]], reserved = TRUE)\n      url = paste0(url, \"%22\", param[[1]], \"%22\")\n    } else if(is.numeric(param[[1]])) {\n      url = paste0(url, param[[1]])\n    } else if(is.logical(param[[1]])) {\n      if(param[[1]]) {\n        url = paste0(url, \"true\")\n      } else {\n        url = paste0(url, \"false\")\n      }\n    } else {\n      stop(\"Property value must be character, numeric, or logical.\")\n    }\n  }\n\n  result = http_request(url, \"GET\", graph)\n\n  if(length(result) == 0) {\n    return(invisible())\n  }\n  \n  nodes = lapply(result, function(r) configure_result(r, attr(graph, \"username\"), attr(graph, \"password\"), attr(graph, \"auth_token\")))\n  return(nodes)\n}\n" }
{ "repo_name": "danpaulsmith/RNeo4j", "ref": "refs/heads/master", "path": "R/getLabeledNodes.R", "content": "getLabeledNodes = function(graph, .label, ...) UseMethod(\"getLabeledNodes\")\n\ngetLabeledNodes.default = function(x, ...) {\n  stop(\"Invalid object. Must supply graph object.\")\n}\n\ngetLabeledNodes.graph = function(graph, .label, ...) {\n  stopifnot(is.character(.label))\n\n  url = paste(attr(graph, \"root\"), \"label\", .label, \"nodes\", sep = \"/\")\n  param = c(...)\n  \n  if(length(param) > 1)\n    stop(\"Can only search by one property.\")\n\n  if(length(param) == 1) {\n    url = paste0(url, \"?\", names(param), \"=\")\n    \n    if(is.character(param[[1]])) {\n      param[[1]] = URLencode(param[[1]], reserved = TRUE)\n      url = paste0(url, \"%22\", param[[1]], \"%22\")\n    } else if(is.numeric(param[[1]])) {\n      url = paste0(url, param[[1]])\n    } else if(is.logical(param[[1]])) {\n      if(param[[1]]) {\n        url = paste0(url, \"true\")\n      } else {\n        url = paste0(url, \"false\")\n      }\n    } else {\n      stop(\"Property value must be character, numeric, or logical.\")\n    }\n  }\n\n  result = http_request(url, \"GET\", graph)\n\n  if(length(result) == 0) {\n    return(invisible())\n  }\n  \n  nodes = lapply(result, function(r) configure_result(r, attr(graph, \"username\"), attr(graph, \"password\"), attr(graph, \"auth_token\")))\n  return(nodes)\n}\n" }
{ "repo_name": "esparza83/RNeo4j", "ref": "refs/heads/master", "path": "R/getLabeledNodes.R", "content": "getLabeledNodes = function(graph, .label, ...) UseMethod(\"getLabeledNodes\")\n\ngetLabeledNodes.default = function(x, ...) {\n  stop(\"Invalid object. Must supply graph object.\")\n}\n\ngetLabeledNodes.graph = function(graph, .label, ...) {\n  stopifnot(is.character(.label))\n\n  url = paste(attr(graph, \"root\"), \"label\", .label, \"nodes\", sep = \"/\")\n  param = c(...)\n  \n  if(length(param) > 1)\n    stop(\"Can only search by one property.\")\n\n  if(length(param) == 1) {\n    url = paste0(url, \"?\", names(param), \"=\")\n    \n    if(is.character(param[[1]])) {\n      param[[1]] = URLencode(param[[1]], reserved = TRUE)\n      url = paste0(url, \"%22\", param[[1]], \"%22\")\n    } else if(is.numeric(param[[1]])) {\n      url = paste0(url, param[[1]])\n    } else if(is.logical(param[[1]])) {\n      if(param[[1]]) {\n        url = paste0(url, \"true\")\n      } else {\n        url = paste0(url, \"false\")\n      }\n    } else {\n      stop(\"Property value must be character, numeric, or logical.\")\n    }\n  }\n\n  result = http_request(url, \"GET\", graph)\n\n  if(length(result) == 0) {\n    return(invisible())\n  }\n  \n  nodes = lapply(result, function(r) configure_result(r, attr(graph, \"username\"), attr(graph, \"password\"), attr(graph, \"auth_token\")))\n  return(nodes)\n}\n" }
{ "repo_name": "CancerInSilico/CancerInSilico", "ref": "refs/heads/master", "path": "inst/scripts/profile_standard.R", "content": "library(CancerInSilico)\nx <- inSilicoCellModel(24,6,0.1)\n" }
{ "repo_name": "tsherma4/CellBasedModel", "ref": "refs/heads/master", "path": "inst/scripts/profile_standard.R", "content": "library(CancerInSilico)\nx <- inSilicoCellModel(24,6,0.1)\n" }
{ "repo_name": "tsherma4/CancerInSilico", "ref": "refs/heads/master", "path": "inst/scripts/profile_standard.R", "content": "library(CancerInSilico)\nx <- inSilicoCellModel(24,6,0.1)\n" }
{ "repo_name": "FertigLab/CancerInSilico", "ref": "refs/heads/master", "path": "inst/scripts/profile_standard.R", "content": "library(CancerInSilico)\nx <- inSilicoCellModel(24,6,0.1)\n" }
{ "repo_name": "h2oai/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/ensemble/h2oEnsemble-package/tests/testthat/test-ensemble-default.R", "content": "context(\"ensemble-default\")\n\ntest_that( \"h2o.ensemble run with default args produces valid results (binomial)\", {\n  testthat::skip_on_cran()\n  \n  # Import a sample binary outcome train/test set into H2O\n  h2o.init(nthreads = -1)\n  train_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_train_5k.csv\"\n  test_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_test_5k.csv\"\n  train <- h2o.importFile(train_csv)\n  test <- h2o.importFile(test_csv)\n  y <- \"response\"\n  x <- setdiff(names(train), y)\n  family <- \"binomial\"\n  train[,y] <- as.factor(train[,y])\n  test[,y] <- as.factor(test[,y])\n  \n  # Train an ensemble model with default args\n  fit <- h2o.ensemble(x = x, y = y, training_frame = train)\n  \n  # Check that `fit` output is reasonable\n  expect_equal( length(fit$learner), 4 )\n  expect_equal( length(fit$basefits), 4 )\n  expect_equal( dim(fit$levelone), c(5000, 5) )\n  expect_true( inherits(fit$metafit, \"H2OBinomialModel\") )\n  for (h2ofit in fit$basefits) {\n    expect_true( inherits(h2ofit, \"H2OBinomialModel\") )\n  }\n  \n  # Predict on test set\n  pp <- predict(fit, test)\n  expect_equal( dim(pp$pred), c(5000, 3) )\n  expect_equal( dim(pp$basepred), c(5000, 4) )\n  \n  # Ensemble test AUC (not reproducible with default base learner library)\n  predictions <- as.data.frame(pp$pred)[,3]  #third column, p1 is P(Y==1)\n  labels <- as.data.frame(test[,y])[,1]\n  auc <- cvAUC::AUC(predictions = predictions, labels = labels)\n  expect_less_than(abs(auc - 0.78), 0.01)\n  \n  # Base learner test AUC (for comparison)\n  base_auc <- sapply(seq(4), function(l) cvAUC::AUC(predictions = as.data.frame(pp$basepred)[,l], labels = labels)) \n  expect_less_than( max(base_auc), auc )\n})\n\n\ntest_that( \"h2o.ensemble run with default args produces valid results (gaussian)\", {\n  testthat::skip_on_cran()\n  \n  # Import a sample binary outcome train/test set into H2O\n  h2o.init(nthreads = -1)\n  train_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_train_5k.csv\"\n  test_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_test_5k.csv\"\n  train <- h2o.importFile(train_csv)\n  test <- h2o.importFile(test_csv)\n  y <- \"response\"\n  x <- setdiff(names(train), y)\n  family <- \"gaussian\"\n  \n  # Train an ensemble model with default args\n  fit <- h2o.ensemble(x = x, y = y, training_frame = train)\n  \n  # Check that `fit` output is reasonable\n  expect_equal( length(fit$learner), 4 )\n  expect_equal( length(fit$basefits), 4 )\n  expect_equal( dim(fit$levelone), c(5000, 5) )\n  expect_true( inherits(fit$metafit, \"H2ORegressionModel\") )\n  for (h2ofit in fit$basefits) {\n    expect_true( inherits(h2ofit, \"H2ORegressionModel\") )\n  }\n  \n  # Predict on test set\n  pp <- predict(fit, test)\n  expect_equal( dim(pp$pred), c(5000, 1) )\n  expect_equal( dim(pp$basepred), c(5000, 4) )\n  \n  # Ensemble test AUC (not reproducible with default base learner library)\n  # We still use AUC as a test here even though it's regression (testing only)\n  predictions <- as.data.frame(pp$pred)[,1]  #first column for regression\n  labels <- as.data.frame(test[,y])[,1]\n  auc <- cvAUC::AUC(predictions = predictions, labels = labels)\n  expect_less_than(abs(auc - 0.78), 0.01)\n  \n  # Base learner test AUC (for comparison)\n  base_auc <- sapply(seq(4), function(l) cvAUC::AUC(predictions = as.data.frame(pp$basepred)[,l], labels = labels)) \n  expect_less_than( max(base_auc), auc )\n})\n\n" }
{ "repo_name": "michalkurka/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/ensemble/h2oEnsemble-package/tests/testthat/test-ensemble-default.R", "content": "context(\"ensemble-default\")\n\ntest_that( \"h2o.ensemble run with default args produces valid results (binomial)\", {\n  testthat::skip_on_cran()\n  \n  # Import a sample binary outcome train/test set into H2O\n  h2o.init(nthreads = -1)\n  train_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_train_5k.csv\"\n  test_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_test_5k.csv\"\n  train <- h2o.importFile(train_csv)\n  test <- h2o.importFile(test_csv)\n  y <- \"response\"\n  x <- setdiff(names(train), y)\n  family <- \"binomial\"\n  train[,y] <- as.factor(train[,y])\n  test[,y] <- as.factor(test[,y])\n  \n  # Train an ensemble model with default args\n  fit <- h2o.ensemble(x = x, y = y, training_frame = train)\n  \n  # Check that `fit` output is reasonable\n  expect_equal( length(fit$learner), 4 )\n  expect_equal( length(fit$basefits), 4 )\n  expect_equal( dim(fit$levelone), c(5000, 5) )\n  expect_true( inherits(fit$metafit, \"H2OBinomialModel\") )\n  for (h2ofit in fit$basefits) {\n    expect_true( inherits(h2ofit, \"H2OBinomialModel\") )\n  }\n  \n  # Predict on test set\n  pp <- predict(fit, test)\n  expect_equal( dim(pp$pred), c(5000, 3) )\n  expect_equal( dim(pp$basepred), c(5000, 4) )\n  \n  # Ensemble test AUC (not reproducible with default base learner library)\n  predictions <- as.data.frame(pp$pred)[,3]  #third column, p1 is P(Y==1)\n  labels <- as.data.frame(test[,y])[,1]\n  auc <- cvAUC::AUC(predictions = predictions, labels = labels)\n  expect_less_than(abs(auc - 0.78), 0.01)\n  \n  # Base learner test AUC (for comparison)\n  base_auc <- sapply(seq(4), function(l) cvAUC::AUC(predictions = as.data.frame(pp$basepred)[,l], labels = labels)) \n  expect_less_than( max(base_auc), auc )\n})\n\n\ntest_that( \"h2o.ensemble run with default args produces valid results (gaussian)\", {\n  testthat::skip_on_cran()\n  \n  # Import a sample binary outcome train/test set into H2O\n  h2o.init(nthreads = -1)\n  train_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_train_5k.csv\"\n  test_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_test_5k.csv\"\n  train <- h2o.importFile(train_csv)\n  test <- h2o.importFile(test_csv)\n  y <- \"response\"\n  x <- setdiff(names(train), y)\n  family <- \"gaussian\"\n  \n  # Train an ensemble model with default args\n  fit <- h2o.ensemble(x = x, y = y, training_frame = train)\n  \n  # Check that `fit` output is reasonable\n  expect_equal( length(fit$learner), 4 )\n  expect_equal( length(fit$basefits), 4 )\n  expect_equal( dim(fit$levelone), c(5000, 5) )\n  expect_true( inherits(fit$metafit, \"H2ORegressionModel\") )\n  for (h2ofit in fit$basefits) {\n    expect_true( inherits(h2ofit, \"H2ORegressionModel\") )\n  }\n  \n  # Predict on test set\n  pp <- predict(fit, test)\n  expect_equal( dim(pp$pred), c(5000, 1) )\n  expect_equal( dim(pp$basepred), c(5000, 4) )\n  \n  # Ensemble test AUC (not reproducible with default base learner library)\n  # We still use AUC as a test here even though it's regression (testing only)\n  predictions <- as.data.frame(pp$pred)[,1]  #first column for regression\n  labels <- as.data.frame(test[,y])[,1]\n  auc <- cvAUC::AUC(predictions = predictions, labels = labels)\n  expect_less_than(abs(auc - 0.78), 0.01)\n  \n  # Base learner test AUC (for comparison)\n  base_auc <- sapply(seq(4), function(l) cvAUC::AUC(predictions = as.data.frame(pp$basepred)[,l], labels = labels)) \n  expect_less_than( max(base_auc), auc )\n})\n\n" }
{ "repo_name": "mathemage/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/ensemble/h2oEnsemble-package/tests/testthat/test-ensemble-default.R", "content": "context(\"ensemble-default\")\n\ntest_that( \"h2o.ensemble run with default args produces valid results (binomial)\", {\n  testthat::skip_on_cran()\n  \n  # Import a sample binary outcome train/test set into H2O\n  h2o.init(nthreads = -1)\n  train_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_train_5k.csv\"\n  test_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_test_5k.csv\"\n  train <- h2o.importFile(train_csv)\n  test <- h2o.importFile(test_csv)\n  y <- \"response\"\n  x <- setdiff(names(train), y)\n  family <- \"binomial\"\n  train[,y] <- as.factor(train[,y])\n  test[,y] <- as.factor(test[,y])\n  \n  # Train an ensemble model with default args\n  fit <- h2o.ensemble(x = x, y = y, training_frame = train)\n  \n  # Check that `fit` output is reasonable\n  expect_equal( length(fit$learner), 4 )\n  expect_equal( length(fit$basefits), 4 )\n  expect_equal( dim(fit$levelone), c(5000, 5) )\n  expect_true( inherits(fit$metafit, \"H2OBinomialModel\") )\n  for (h2ofit in fit$basefits) {\n    expect_true( inherits(h2ofit, \"H2OBinomialModel\") )\n  }\n  \n  # Predict on test set\n  pp <- predict(fit, test)\n  expect_equal( dim(pp$pred), c(5000, 3) )\n  expect_equal( dim(pp$basepred), c(5000, 4) )\n  \n  # Ensemble test AUC (not reproducible with default base learner library)\n  predictions <- as.data.frame(pp$pred)[,3]  #third column, p1 is P(Y==1)\n  labels <- as.data.frame(test[,y])[,1]\n  auc <- cvAUC::AUC(predictions = predictions, labels = labels)\n  expect_less_than(abs(auc - 0.78), 0.01)\n  \n  # Base learner test AUC (for comparison)\n  base_auc <- sapply(seq(4), function(l) cvAUC::AUC(predictions = as.data.frame(pp$basepred)[,l], labels = labels)) \n  expect_less_than( max(base_auc), auc )\n})\n\n\ntest_that( \"h2o.ensemble run with default args produces valid results (gaussian)\", {\n  testthat::skip_on_cran()\n  \n  # Import a sample binary outcome train/test set into H2O\n  h2o.init(nthreads = -1)\n  train_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_train_5k.csv\"\n  test_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_test_5k.csv\"\n  train <- h2o.importFile(train_csv)\n  test <- h2o.importFile(test_csv)\n  y <- \"response\"\n  x <- setdiff(names(train), y)\n  family <- \"gaussian\"\n  \n  # Train an ensemble model with default args\n  fit <- h2o.ensemble(x = x, y = y, training_frame = train)\n  \n  # Check that `fit` output is reasonable\n  expect_equal( length(fit$learner), 4 )\n  expect_equal( length(fit$basefits), 4 )\n  expect_equal( dim(fit$levelone), c(5000, 5) )\n  expect_true( inherits(fit$metafit, \"H2ORegressionModel\") )\n  for (h2ofit in fit$basefits) {\n    expect_true( inherits(h2ofit, \"H2ORegressionModel\") )\n  }\n  \n  # Predict on test set\n  pp <- predict(fit, test)\n  expect_equal( dim(pp$pred), c(5000, 1) )\n  expect_equal( dim(pp$basepred), c(5000, 4) )\n  \n  # Ensemble test AUC (not reproducible with default base learner library)\n  # We still use AUC as a test here even though it's regression (testing only)\n  predictions <- as.data.frame(pp$pred)[,1]  #first column for regression\n  labels <- as.data.frame(test[,y])[,1]\n  auc <- cvAUC::AUC(predictions = predictions, labels = labels)\n  expect_less_than(abs(auc - 0.78), 0.01)\n  \n  # Base learner test AUC (for comparison)\n  base_auc <- sapply(seq(4), function(l) cvAUC::AUC(predictions = as.data.frame(pp$basepred)[,l], labels = labels)) \n  expect_less_than( max(base_auc), auc )\n})\n\n" }
{ "repo_name": "spennihana/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/ensemble/h2oEnsemble-package/tests/testthat/test-ensemble-default.R", "content": "context(\"ensemble-default\")\n\ntest_that( \"h2o.ensemble run with default args produces valid results (binomial)\", {\n  testthat::skip_on_cran()\n  \n  # Import a sample binary outcome train/test set into H2O\n  h2o.init(nthreads = -1)\n  train_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_train_5k.csv\"\n  test_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_test_5k.csv\"\n  train <- h2o.importFile(train_csv)\n  test <- h2o.importFile(test_csv)\n  y <- \"response\"\n  x <- setdiff(names(train), y)\n  family <- \"binomial\"\n  train[,y] <- as.factor(train[,y])\n  test[,y] <- as.factor(test[,y])\n  \n  # Train an ensemble model with default args\n  fit <- h2o.ensemble(x = x, y = y, training_frame = train)\n  \n  # Check that `fit` output is reasonable\n  expect_equal( length(fit$learner), 4 )\n  expect_equal( length(fit$basefits), 4 )\n  expect_equal( dim(fit$levelone), c(5000, 5) )\n  expect_true( inherits(fit$metafit, \"H2OBinomialModel\") )\n  for (h2ofit in fit$basefits) {\n    expect_true( inherits(h2ofit, \"H2OBinomialModel\") )\n  }\n  \n  # Predict on test set\n  pp <- predict(fit, test)\n  expect_equal( dim(pp$pred), c(5000, 3) )\n  expect_equal( dim(pp$basepred), c(5000, 4) )\n  \n  # Ensemble test AUC (not reproducible with default base learner library)\n  predictions <- as.data.frame(pp$pred)[,3]  #third column, p1 is P(Y==1)\n  labels <- as.data.frame(test[,y])[,1]\n  auc <- cvAUC::AUC(predictions = predictions, labels = labels)\n  expect_less_than(abs(auc - 0.78), 0.01)\n  \n  # Base learner test AUC (for comparison)\n  base_auc <- sapply(seq(4), function(l) cvAUC::AUC(predictions = as.data.frame(pp$basepred)[,l], labels = labels)) \n  expect_less_than( max(base_auc), auc )\n})\n\n\ntest_that( \"h2o.ensemble run with default args produces valid results (gaussian)\", {\n  testthat::skip_on_cran()\n  \n  # Import a sample binary outcome train/test set into H2O\n  h2o.init(nthreads = -1)\n  train_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_train_5k.csv\"\n  test_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_test_5k.csv\"\n  train <- h2o.importFile(train_csv)\n  test <- h2o.importFile(test_csv)\n  y <- \"response\"\n  x <- setdiff(names(train), y)\n  family <- \"gaussian\"\n  \n  # Train an ensemble model with default args\n  fit <- h2o.ensemble(x = x, y = y, training_frame = train)\n  \n  # Check that `fit` output is reasonable\n  expect_equal( length(fit$learner), 4 )\n  expect_equal( length(fit$basefits), 4 )\n  expect_equal( dim(fit$levelone), c(5000, 5) )\n  expect_true( inherits(fit$metafit, \"H2ORegressionModel\") )\n  for (h2ofit in fit$basefits) {\n    expect_true( inherits(h2ofit, \"H2ORegressionModel\") )\n  }\n  \n  # Predict on test set\n  pp <- predict(fit, test)\n  expect_equal( dim(pp$pred), c(5000, 1) )\n  expect_equal( dim(pp$basepred), c(5000, 4) )\n  \n  # Ensemble test AUC (not reproducible with default base learner library)\n  # We still use AUC as a test here even though it's regression (testing only)\n  predictions <- as.data.frame(pp$pred)[,1]  #first column for regression\n  labels <- as.data.frame(test[,y])[,1]\n  auc <- cvAUC::AUC(predictions = predictions, labels = labels)\n  expect_less_than(abs(auc - 0.78), 0.01)\n  \n  # Base learner test AUC (for comparison)\n  base_auc <- sapply(seq(4), function(l) cvAUC::AUC(predictions = as.data.frame(pp$basepred)[,l], labels = labels)) \n  expect_less_than( max(base_auc), auc )\n})\n\n" }
{ "repo_name": "h2oai/h2o-dev", "ref": "refs/heads/master", "path": "h2o-r/ensemble/h2oEnsemble-package/tests/testthat/test-ensemble-default.R", "content": "context(\"ensemble-default\")\n\ntest_that( \"h2o.ensemble run with default args produces valid results (binomial)\", {\n  testthat::skip_on_cran()\n  \n  # Import a sample binary outcome train/test set into H2O\n  h2o.init(nthreads = -1)\n  train_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_train_5k.csv\"\n  test_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_test_5k.csv\"\n  train <- h2o.importFile(train_csv)\n  test <- h2o.importFile(test_csv)\n  y <- \"response\"\n  x <- setdiff(names(train), y)\n  family <- \"binomial\"\n  train[,y] <- as.factor(train[,y])\n  test[,y] <- as.factor(test[,y])\n  \n  # Train an ensemble model with default args\n  fit <- h2o.ensemble(x = x, y = y, training_frame = train)\n  \n  # Check that `fit` output is reasonable\n  expect_equal( length(fit$learner), 4 )\n  expect_equal( length(fit$basefits), 4 )\n  expect_equal( dim(fit$levelone), c(5000, 5) )\n  expect_true( inherits(fit$metafit, \"H2OBinomialModel\") )\n  for (h2ofit in fit$basefits) {\n    expect_true( inherits(h2ofit, \"H2OBinomialModel\") )\n  }\n  \n  # Predict on test set\n  pp <- predict(fit, test)\n  expect_equal( dim(pp$pred), c(5000, 3) )\n  expect_equal( dim(pp$basepred), c(5000, 4) )\n  \n  # Ensemble test AUC (not reproducible with default base learner library)\n  predictions <- as.data.frame(pp$pred)[,3]  #third column, p1 is P(Y==1)\n  labels <- as.data.frame(test[,y])[,1]\n  auc <- cvAUC::AUC(predictions = predictions, labels = labels)\n  expect_less_than(abs(auc - 0.78), 0.01)\n  \n  # Base learner test AUC (for comparison)\n  base_auc <- sapply(seq(4), function(l) cvAUC::AUC(predictions = as.data.frame(pp$basepred)[,l], labels = labels)) \n  expect_less_than( max(base_auc), auc )\n})\n\n\ntest_that( \"h2o.ensemble run with default args produces valid results (gaussian)\", {\n  testthat::skip_on_cran()\n  \n  # Import a sample binary outcome train/test set into H2O\n  h2o.init(nthreads = -1)\n  train_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_train_5k.csv\"\n  test_csv <- \"https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_test_5k.csv\"\n  train <- h2o.importFile(train_csv)\n  test <- h2o.importFile(test_csv)\n  y <- \"response\"\n  x <- setdiff(names(train), y)\n  family <- \"gaussian\"\n  \n  # Train an ensemble model with default args\n  fit <- h2o.ensemble(x = x, y = y, training_frame = train)\n  \n  # Check that `fit` output is reasonable\n  expect_equal( length(fit$learner), 4 )\n  expect_equal( length(fit$basefits), 4 )\n  expect_equal( dim(fit$levelone), c(5000, 5) )\n  expect_true( inherits(fit$metafit, \"H2ORegressionModel\") )\n  for (h2ofit in fit$basefits) {\n    expect_true( inherits(h2ofit, \"H2ORegressionModel\") )\n  }\n  \n  # Predict on test set\n  pp <- predict(fit, test)\n  expect_equal( dim(pp$pred), c(5000, 1) )\n  expect_equal( dim(pp$basepred), c(5000, 4) )\n  \n  # Ensemble test AUC (not reproducible with default base learner library)\n  # We still use AUC as a test here even though it's regression (testing only)\n  predictions <- as.data.frame(pp$pred)[,1]  #first column for regression\n  labels <- as.data.frame(test[,y])[,1]\n  auc <- cvAUC::AUC(predictions = predictions, labels = labels)\n  expect_less_than(abs(auc - 0.78), 0.01)\n  \n  # Base learner test AUC (for comparison)\n  base_auc <- sapply(seq(4), function(l) cvAUC::AUC(predictions = as.data.frame(pp$basepred)[,l], labels = labels)) \n  expect_less_than( max(base_auc), auc )\n})\n\n" }
{ "repo_name": "cxxr-devel/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/asraw/tc_asraw_2.R", "content": "expected <- eval(parse(text=\"raw(0)\"));   \ntest(id=0, code={   \nargv <- eval(parse(text=\"list(integer(0))\"));   \ndo.call(`as.raw`, argv);   \n}, o=expected);   \n\n" }
{ "repo_name": "krlmlr/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/asraw/tc_asraw_2.R", "content": "expected <- eval(parse(text=\"raw(0)\"));   \ntest(id=0, code={   \nargv <- eval(parse(text=\"list(integer(0))\"));   \ndo.call(`as.raw`, argv);   \n}, o=expected);   \n\n" }
{ "repo_name": "kmillar/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/asraw/tc_asraw_2.R", "content": "expected <- eval(parse(text=\"raw(0)\"));   \ntest(id=0, code={   \nargv <- eval(parse(text=\"list(integer(0))\"));   \ndo.call(`as.raw`, argv);   \n}, o=expected);   \n\n" }
{ "repo_name": "kmillar/rho", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/asraw/tc_asraw_2.R", "content": "expected <- eval(parse(text=\"raw(0)\"));   \ntest(id=0, code={   \nargv <- eval(parse(text=\"list(integer(0))\"));   \ndo.call(`as.raw`, argv);   \n}, o=expected);   \n\n" }
{ "repo_name": "ArunChauhan/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/asraw/tc_asraw_2.R", "content": "expected <- eval(parse(text=\"raw(0)\"));   \ntest(id=0, code={   \nargv <- eval(parse(text=\"list(integer(0))\"));   \ndo.call(`as.raw`, argv);   \n}, o=expected);   \n\n" }
{ "repo_name": "rho-devel/rho", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/asraw/tc_asraw_2.R", "content": "expected <- eval(parse(text=\"raw(0)\"));   \ntest(id=0, code={   \nargv <- eval(parse(text=\"list(integer(0))\"));   \ndo.call(`as.raw`, argv);   \n}, o=expected);   \n\n" }
{ "repo_name": "cxxr-devel/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/colnamesassign_/tc_colnamesassign__1.R", "content": "expected <- structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), .Dim = c(200L, \n5L), .Dimnames = list(NULL, c(\"X1\", \"X2\", \"X3\", \"X4\", \"X5\")))\ntest(id=5, code={\nargv <- structure(list(x = structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0), .Dim = c(200L, 5L)), value = c(\"X1\", \"X2\", \"X3\", \"X4\", \n\"X5\")), .Names = c(\"x\", \"value\"))\ndo.call('colnames<-', argv);\n},  o = expected);\n\n" }
{ "repo_name": "krlmlr/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/colnamesassign_/tc_colnamesassign__1.R", "content": "expected <- structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), .Dim = c(200L, \n5L), .Dimnames = list(NULL, c(\"X1\", \"X2\", \"X3\", \"X4\", \"X5\")))\ntest(id=5, code={\nargv <- structure(list(x = structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0), .Dim = c(200L, 5L)), value = c(\"X1\", \"X2\", \"X3\", \"X4\", \n\"X5\")), .Names = c(\"x\", \"value\"))\ndo.call('colnames<-', argv);\n},  o = expected);\n\n" }
{ "repo_name": "kmillar/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/colnamesassign_/tc_colnamesassign__1.R", "content": "expected <- structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), .Dim = c(200L, \n5L), .Dimnames = list(NULL, c(\"X1\", \"X2\", \"X3\", \"X4\", \"X5\")))\ntest(id=5, code={\nargv <- structure(list(x = structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0), .Dim = c(200L, 5L)), value = c(\"X1\", \"X2\", \"X3\", \"X4\", \n\"X5\")), .Names = c(\"x\", \"value\"))\ndo.call('colnames<-', argv);\n},  o = expected);\n\n" }
{ "repo_name": "kmillar/rho", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/colnamesassign_/tc_colnamesassign__1.R", "content": "expected <- structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), .Dim = c(200L, \n5L), .Dimnames = list(NULL, c(\"X1\", \"X2\", \"X3\", \"X4\", \"X5\")))\ntest(id=5, code={\nargv <- structure(list(x = structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0), .Dim = c(200L, 5L)), value = c(\"X1\", \"X2\", \"X3\", \"X4\", \n\"X5\")), .Names = c(\"x\", \"value\"))\ndo.call('colnames<-', argv);\n},  o = expected);\n\n" }
{ "repo_name": "ArunChauhan/cxxr", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/colnamesassign_/tc_colnamesassign__1.R", "content": "expected <- structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), .Dim = c(200L, \n5L), .Dimnames = list(NULL, c(\"X1\", \"X2\", \"X3\", \"X4\", \"X5\")))\ntest(id=5, code={\nargv <- structure(list(x = structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0), .Dim = c(200L, 5L)), value = c(\"X1\", \"X2\", \"X3\", \"X4\", \n\"X5\")), .Names = c(\"x\", \"value\"))\ndo.call('colnames<-', argv);\n},  o = expected);\n\n" }
{ "repo_name": "rho-devel/rho", "ref": "refs/heads/master", "path": "src/extra/testr/filtered-test-suite/colnamesassign_/tc_colnamesassign__1.R", "content": "expected <- structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), .Dim = c(200L, \n5L), .Dimnames = list(NULL, c(\"X1\", \"X2\", \"X3\", \"X4\", \"X5\")))\ntest(id=5, code={\nargv <- structure(list(x = structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n0, 0, 0), .Dim = c(200L, 5L)), value = c(\"X1\", \"X2\", \"X3\", \"X4\", \n\"X5\")), .Names = c(\"x\", \"value\"))\ndo.call('colnames<-', argv);\n},  o = expected);\n\n" }
{ "repo_name": "ChiWang/r-source", "ref": "refs/heads/master", "path": "src/library/graphics/R/par.R", "content": "#  File src/library/graphics/R/par.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2014 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n##-- These are the ones used in ../../../main/par.c  Query(..) :\n##-- Documentation in\t\t../../../include/Graphics.h\n.Pars <- c(\n           \"xlog\", \"ylog\", ## must be before xaxp, yaxp\n\t   \"adj\", \"ann\", \"ask\", \"bg\", \"bty\",\n\t   \"cex\", \"cex.axis\", \"cex.lab\", \"cex.main\", \"cex.sub\", \"cin\",\n\t   \"col\", \"col.axis\", \"col.lab\", \"col.main\", \"col.sub\",\n           \"cra\", \"crt\", \"csi\",\"cxy\",\t\"din\", \"err\", \"family\",\n           \"fg\", \"fig\", \"fin\",\n\t   \"font\", \"font.axis\", \"font.lab\", \"font.main\", \"font.sub\",\n           \"lab\", \"las\", \"lend\", \"lheight\", \"ljoin\", \"lmitre\", \"lty\", \"lwd\",\n           \"mai\", \"mar\", \"mex\", \"mfcol\", \"mfg\", \"mfrow\", \"mgp\", \"mkh\",\n\t   \"new\", \"oma\", \"omd\", \"omi\", \"page\", \"pch\", \"pin\", \"plt\", \"ps\", \"pty\",\n\t   \"smo\", \"srt\", \"tck\", \"tcl\", \"usr\",\n\t   \"xaxp\", \"xaxs\", \"xaxt\",  \"xpd\",\n\t   \"yaxp\", \"yaxs\", \"yaxt\", \"ylbias\"\n\t   )\n# Replaced with function to evaluate readonly pars because \"gamma\"\n# was at one time readonly on a per-device basis\n# .Pars.readonly <- c(\"cin\",\"cra\",\"csi\",\"cxy\",\"din\")\n\npar <- function (..., no.readonly = FALSE)\n{\n    .Pars.readonly <- c(\"cin\",\"cra\",\"csi\",\"cxy\",\"din\",\"page\")\n    single <- FALSE\n    args <- list(...)\n    if (!length(args))\n\targs <- as.list(if (no.readonly) .Pars[-match(.Pars.readonly, .Pars)]\n                        else .Pars)\n    else {\n\tif (all(unlist(lapply(args, is.character))))\n\t    args <- as.list(unlist(args))\n\tif (length(args) == 1) {\n\t    if (is.list(args[[1L]]) | is.null(args[[1L]]))\n\t\targs <- args[[1L]]\n\t    else\n\t\tif(is.null(names(args)))\n\t\t    single <- TRUE\n\t}\n    }\n    value <- .External2(C_par, args)\n    if(single) value <- value[[1L]]\n    if(!is.null(names(args))) invisible(value) else value\n}\n\nclip <- function(x1, x2, y1, y2)\n    invisible(.External.graphics(C_clip, x1, x2, y1, y2))\n\n" }
{ "repo_name": "o-/Rexperiments", "ref": "refs/heads/master", "path": "src/library/graphics/R/par.R", "content": "#  File src/library/graphics/R/par.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2014 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n##-- These are the ones used in ../../../main/par.c  Query(..) :\n##-- Documentation in\t\t../../../include/Graphics.h\n.Pars <- c(\n           \"xlog\", \"ylog\", ## must be before xaxp, yaxp\n\t   \"adj\", \"ann\", \"ask\", \"bg\", \"bty\",\n\t   \"cex\", \"cex.axis\", \"cex.lab\", \"cex.main\", \"cex.sub\", \"cin\",\n\t   \"col\", \"col.axis\", \"col.lab\", \"col.main\", \"col.sub\",\n           \"cra\", \"crt\", \"csi\",\"cxy\",\t\"din\", \"err\", \"family\",\n           \"fg\", \"fig\", \"fin\",\n\t   \"font\", \"font.axis\", \"font.lab\", \"font.main\", \"font.sub\",\n           \"lab\", \"las\", \"lend\", \"lheight\", \"ljoin\", \"lmitre\", \"lty\", \"lwd\",\n           \"mai\", \"mar\", \"mex\", \"mfcol\", \"mfg\", \"mfrow\", \"mgp\", \"mkh\",\n\t   \"new\", \"oma\", \"omd\", \"omi\", \"page\", \"pch\", \"pin\", \"plt\", \"ps\", \"pty\",\n\t   \"smo\", \"srt\", \"tck\", \"tcl\", \"usr\",\n\t   \"xaxp\", \"xaxs\", \"xaxt\",  \"xpd\",\n\t   \"yaxp\", \"yaxs\", \"yaxt\", \"ylbias\"\n\t   )\n# Replaced with function to evaluate readonly pars because \"gamma\"\n# was at one time readonly on a per-device basis\n# .Pars.readonly <- c(\"cin\",\"cra\",\"csi\",\"cxy\",\"din\")\n\npar <- function (..., no.readonly = FALSE)\n{\n    .Pars.readonly <- c(\"cin\",\"cra\",\"csi\",\"cxy\",\"din\",\"page\")\n    single <- FALSE\n    args <- list(...)\n    if (!length(args))\n\targs <- as.list(if (no.readonly) .Pars[-match(.Pars.readonly, .Pars)]\n                        else .Pars)\n    else {\n\tif (all(unlist(lapply(args, is.character))))\n\t    args <- as.list(unlist(args))\n\tif (length(args) == 1) {\n\t    if (is.list(args[[1L]]) | is.null(args[[1L]]))\n\t\targs <- args[[1L]]\n\t    else\n\t\tif(is.null(names(args)))\n\t\t    single <- TRUE\n\t}\n    }\n    value <- .External2(C_par, args)\n    if(single) value <- value[[1L]]\n    if(!is.null(names(args))) invisible(value) else value\n}\n\nclip <- function(x1, x2, y1, y2)\n    invisible(.External.graphics(C_clip, x1, x2, y1, y2))\n\n" }
{ "repo_name": "jeffreyhorner/R-Array-Hash", "ref": "refs/heads/master", "path": "src/library/graphics/R/par.R", "content": "#  File src/library/graphics/R/par.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2014 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n##-- These are the ones used in ../../../main/par.c  Query(..) :\n##-- Documentation in\t\t../../../include/Graphics.h\n.Pars <- c(\n           \"xlog\", \"ylog\", ## must be before xaxp, yaxp\n\t   \"adj\", \"ann\", \"ask\", \"bg\", \"bty\",\n\t   \"cex\", \"cex.axis\", \"cex.lab\", \"cex.main\", \"cex.sub\", \"cin\",\n\t   \"col\", \"col.axis\", \"col.lab\", \"col.main\", \"col.sub\",\n           \"cra\", \"crt\", \"csi\",\"cxy\",\t\"din\", \"err\", \"family\",\n           \"fg\", \"fig\", \"fin\",\n\t   \"font\", \"font.axis\", \"font.lab\", \"font.main\", \"font.sub\",\n           \"lab\", \"las\", \"lend\", \"lheight\", \"ljoin\", \"lmitre\", \"lty\", \"lwd\",\n           \"mai\", \"mar\", \"mex\", \"mfcol\", \"mfg\", \"mfrow\", \"mgp\", \"mkh\",\n\t   \"new\", \"oma\", \"omd\", \"omi\", \"page\", \"pch\", \"pin\", \"plt\", \"ps\", \"pty\",\n\t   \"smo\", \"srt\", \"tck\", \"tcl\", \"usr\",\n\t   \"xaxp\", \"xaxs\", \"xaxt\",  \"xpd\",\n\t   \"yaxp\", \"yaxs\", \"yaxt\", \"ylbias\"\n\t   )\n# Replaced with function to evaluate readonly pars because \"gamma\"\n# was at one time readonly on a per-device basis\n# .Pars.readonly <- c(\"cin\",\"cra\",\"csi\",\"cxy\",\"din\")\n\npar <- function (..., no.readonly = FALSE)\n{\n    .Pars.readonly <- c(\"cin\",\"cra\",\"csi\",\"cxy\",\"din\",\"page\")\n    single <- FALSE\n    args <- list(...)\n    if (!length(args))\n\targs <- as.list(if (no.readonly) .Pars[-match(.Pars.readonly, .Pars)]\n                        else .Pars)\n    else {\n\tif (all(unlist(lapply(args, is.character))))\n\t    args <- as.list(unlist(args))\n\tif (length(args) == 1) {\n\t    if (is.list(args[[1L]]) | is.null(args[[1L]]))\n\t\targs <- args[[1L]]\n\t    else\n\t\tif(is.null(names(args)))\n\t\t    single <- TRUE\n\t}\n    }\n    value <- .External2(C_par, args)\n    if(single) value <- value[[1L]]\n    if(!is.null(names(args))) invisible(value) else value\n}\n\nclip <- function(x1, x2, y1, y2)\n    invisible(.External.graphics(C_clip, x1, x2, y1, y2))\n\n" }
{ "repo_name": "limeng12/r-source", "ref": "refs/heads/trunk", "path": "src/library/graphics/R/par.R", "content": "#  File src/library/graphics/R/par.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2014 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n##-- These are the ones used in ../../../main/par.c  Query(..) :\n##-- Documentation in\t\t../../../include/Graphics.h\n.Pars <- c(\n           \"xlog\", \"ylog\", ## must be before xaxp, yaxp\n\t   \"adj\", \"ann\", \"ask\", \"bg\", \"bty\",\n\t   \"cex\", \"cex.axis\", \"cex.lab\", \"cex.main\", \"cex.sub\", \"cin\",\n\t   \"col\", \"col.axis\", \"col.lab\", \"col.main\", \"col.sub\",\n           \"cra\", \"crt\", \"csi\",\"cxy\",\t\"din\", \"err\", \"family\",\n           \"fg\", \"fig\", \"fin\",\n\t   \"font\", \"font.axis\", \"font.lab\", \"font.main\", \"font.sub\",\n           \"lab\", \"las\", \"lend\", \"lheight\", \"ljoin\", \"lmitre\", \"lty\", \"lwd\",\n           \"mai\", \"mar\", \"mex\", \"mfcol\", \"mfg\", \"mfrow\", \"mgp\", \"mkh\",\n\t   \"new\", \"oma\", \"omd\", \"omi\", \"page\", \"pch\", \"pin\", \"plt\", \"ps\", \"pty\",\n\t   \"smo\", \"srt\", \"tck\", \"tcl\", \"usr\",\n\t   \"xaxp\", \"xaxs\", \"xaxt\",  \"xpd\",\n\t   \"yaxp\", \"yaxs\", \"yaxt\", \"ylbias\"\n\t   )\n# Replaced with function to evaluate readonly pars because \"gamma\"\n# was at one time readonly on a per-device basis\n# .Pars.readonly <- c(\"cin\",\"cra\",\"csi\",\"cxy\",\"din\")\n\npar <- function (..., no.readonly = FALSE)\n{\n    .Pars.readonly <- c(\"cin\",\"cra\",\"csi\",\"cxy\",\"din\",\"page\")\n    single <- FALSE\n    args <- list(...)\n    if (!length(args))\n\targs <- as.list(if (no.readonly) .Pars[-match(.Pars.readonly, .Pars)]\n                        else .Pars)\n    else {\n\tif (all(unlist(lapply(args, is.character))))\n\t    args <- as.list(unlist(args))\n\tif (length(args) == 1) {\n\t    if (is.list(args[[1L]]) | is.null(args[[1L]]))\n\t\targs <- args[[1L]]\n\t    else\n\t\tif(is.null(names(args)))\n\t\t    single <- TRUE\n\t}\n    }\n    value <- .External2(C_par, args)\n    if(single) value <- value[[1L]]\n    if(!is.null(names(args))) invisible(value) else value\n}\n\nclip <- function(x1, x2, y1, y2)\n    invisible(.External.graphics(C_clip, x1, x2, y1, y2))\n\n" }
{ "repo_name": "jeffreyhorner/R-Judy-Arrays", "ref": "refs/heads/master", "path": "src/library/graphics/R/par.R", "content": "#  File src/library/graphics/R/par.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2014 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n##-- These are the ones used in ../../../main/par.c  Query(..) :\n##-- Documentation in\t\t../../../include/Graphics.h\n.Pars <- c(\n           \"xlog\", \"ylog\", ## must be before xaxp, yaxp\n\t   \"adj\", \"ann\", \"ask\", \"bg\", \"bty\",\n\t   \"cex\", \"cex.axis\", \"cex.lab\", \"cex.main\", \"cex.sub\", \"cin\",\n\t   \"col\", \"col.axis\", \"col.lab\", \"col.main\", \"col.sub\",\n           \"cra\", \"crt\", \"csi\",\"cxy\",\t\"din\", \"err\", \"family\",\n           \"fg\", \"fig\", \"fin\",\n\t   \"font\", \"font.axis\", \"font.lab\", \"font.main\", \"font.sub\",\n           \"lab\", \"las\", \"lend\", \"lheight\", \"ljoin\", \"lmitre\", \"lty\", \"lwd\",\n           \"mai\", \"mar\", \"mex\", \"mfcol\", \"mfg\", \"mfrow\", \"mgp\", \"mkh\",\n\t   \"new\", \"oma\", \"omd\", \"omi\", \"page\", \"pch\", \"pin\", \"plt\", \"ps\", \"pty\",\n\t   \"smo\", \"srt\", \"tck\", \"tcl\", \"usr\",\n\t   \"xaxp\", \"xaxs\", \"xaxt\",  \"xpd\",\n\t   \"yaxp\", \"yaxs\", \"yaxt\", \"ylbias\"\n\t   )\n# Replaced with function to evaluate readonly pars because \"gamma\"\n# was at one time readonly on a per-device basis\n# .Pars.readonly <- c(\"cin\",\"cra\",\"csi\",\"cxy\",\"din\")\n\npar <- function (..., no.readonly = FALSE)\n{\n    .Pars.readonly <- c(\"cin\",\"cra\",\"csi\",\"cxy\",\"din\",\"page\")\n    single <- FALSE\n    args <- list(...)\n    if (!length(args))\n\targs <- as.list(if (no.readonly) .Pars[-match(.Pars.readonly, .Pars)]\n                        else .Pars)\n    else {\n\tif (all(unlist(lapply(args, is.character))))\n\t    args <- as.list(unlist(args))\n\tif (length(args) == 1) {\n\t    if (is.list(args[[1L]]) | is.null(args[[1L]]))\n\t\targs <- args[[1L]]\n\t    else\n\t\tif(is.null(names(args)))\n\t\t    single <- TRUE\n\t}\n    }\n    value <- .External2(C_par, args)\n    if(single) value <- value[[1L]]\n    if(!is.null(names(args))) invisible(value) else value\n}\n\nclip <- function(x1, x2, y1, y2)\n    invisible(.External.graphics(C_clip, x1, x2, y1, y2))\n\n" }
{ "repo_name": "patperry/r-source", "ref": "refs/heads/trunk", "path": "src/library/graphics/R/par.R", "content": "#  File src/library/graphics/R/par.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2014 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n##-- These are the ones used in ../../../main/par.c  Query(..) :\n##-- Documentation in\t\t../../../include/Graphics.h\n.Pars <- c(\n           \"xlog\", \"ylog\", ## must be before xaxp, yaxp\n\t   \"adj\", \"ann\", \"ask\", \"bg\", \"bty\",\n\t   \"cex\", \"cex.axis\", \"cex.lab\", \"cex.main\", \"cex.sub\", \"cin\",\n\t   \"col\", \"col.axis\", \"col.lab\", \"col.main\", \"col.sub\",\n           \"cra\", \"crt\", \"csi\",\"cxy\",\t\"din\", \"err\", \"family\",\n           \"fg\", \"fig\", \"fin\",\n\t   \"font\", \"font.axis\", \"font.lab\", \"font.main\", \"font.sub\",\n           \"lab\", \"las\", \"lend\", \"lheight\", \"ljoin\", \"lmitre\", \"lty\", \"lwd\",\n           \"mai\", \"mar\", \"mex\", \"mfcol\", \"mfg\", \"mfrow\", \"mgp\", \"mkh\",\n\t   \"new\", \"oma\", \"omd\", \"omi\", \"page\", \"pch\", \"pin\", \"plt\", \"ps\", \"pty\",\n\t   \"smo\", \"srt\", \"tck\", \"tcl\", \"usr\",\n\t   \"xaxp\", \"xaxs\", \"xaxt\",  \"xpd\",\n\t   \"yaxp\", \"yaxs\", \"yaxt\", \"ylbias\"\n\t   )\n# Replaced with function to evaluate readonly pars because \"gamma\"\n# was at one time readonly on a per-device basis\n# .Pars.readonly <- c(\"cin\",\"cra\",\"csi\",\"cxy\",\"din\")\n\npar <- function (..., no.readonly = FALSE)\n{\n    .Pars.readonly <- c(\"cin\",\"cra\",\"csi\",\"cxy\",\"din\",\"page\")\n    single <- FALSE\n    args <- list(...)\n    if (!length(args))\n\targs <- as.list(if (no.readonly) .Pars[-match(.Pars.readonly, .Pars)]\n                        else .Pars)\n    else {\n\tif (all(unlist(lapply(args, is.character))))\n\t    args <- as.list(unlist(args))\n\tif (length(args) == 1) {\n\t    if (is.list(args[[1L]]) | is.null(args[[1L]]))\n\t\targs <- args[[1L]]\n\t    else\n\t\tif(is.null(names(args)))\n\t\t    single <- TRUE\n\t}\n    }\n    value <- .External2(C_par, args)\n    if(single) value <- value[[1L]]\n    if(!is.null(names(args))) invisible(value) else value\n}\n\nclip <- function(x1, x2, y1, y2)\n    invisible(.External.graphics(C_clip, x1, x2, y1, y2))\n\n" }
{ "repo_name": "hadley/r-source", "ref": "refs/heads/trunk", "path": "src/library/graphics/R/par.R", "content": "#  File src/library/graphics/R/par.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2014 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n##-- These are the ones used in ../../../main/par.c  Query(..) :\n##-- Documentation in\t\t../../../include/Graphics.h\n.Pars <- c(\n           \"xlog\", \"ylog\", ## must be before xaxp, yaxp\n\t   \"adj\", \"ann\", \"ask\", \"bg\", \"bty\",\n\t   \"cex\", \"cex.axis\", \"cex.lab\", \"cex.main\", \"cex.sub\", \"cin\",\n\t   \"col\", \"col.axis\", \"col.lab\", \"col.main\", \"col.sub\",\n           \"cra\", \"crt\", \"csi\",\"cxy\",\t\"din\", \"err\", \"family\",\n           \"fg\", \"fig\", \"fin\",\n\t   \"font\", \"font.axis\", \"font.lab\", \"font.main\", \"font.sub\",\n           \"lab\", \"las\", \"lend\", \"lheight\", \"ljoin\", \"lmitre\", \"lty\", \"lwd\",\n           \"mai\", \"mar\", \"mex\", \"mfcol\", \"mfg\", \"mfrow\", \"mgp\", \"mkh\",\n\t   \"new\", \"oma\", \"omd\", \"omi\", \"page\", \"pch\", \"pin\", \"plt\", \"ps\", \"pty\",\n\t   \"smo\", \"srt\", \"tck\", \"tcl\", \"usr\",\n\t   \"xaxp\", \"xaxs\", \"xaxt\",  \"xpd\",\n\t   \"yaxp\", \"yaxs\", \"yaxt\", \"ylbias\"\n\t   )\n# Replaced with function to evaluate readonly pars because \"gamma\"\n# was at one time readonly on a per-device basis\n# .Pars.readonly <- c(\"cin\",\"cra\",\"csi\",\"cxy\",\"din\")\n\npar <- function (..., no.readonly = FALSE)\n{\n    .Pars.readonly <- c(\"cin\",\"cra\",\"csi\",\"cxy\",\"din\",\"page\")\n    single <- FALSE\n    args <- list(...)\n    if (!length(args))\n\targs <- as.list(if (no.readonly) .Pars[-match(.Pars.readonly, .Pars)]\n                        else .Pars)\n    else {\n\tif (all(unlist(lapply(args, is.character))))\n\t    args <- as.list(unlist(args))\n\tif (length(args) == 1) {\n\t    if (is.list(args[[1L]]) | is.null(args[[1L]]))\n\t\targs <- args[[1L]]\n\t    else\n\t\tif(is.null(names(args)))\n\t\t    single <- TRUE\n\t}\n    }\n    value <- .External2(C_par, args)\n    if(single) value <- value[[1L]]\n    if(!is.null(names(args))) invisible(value) else value\n}\n\nclip <- function(x1, x2, y1, y2)\n    invisible(.External.graphics(C_clip, x1, x2, y1, y2))\n\n" }
{ "repo_name": "cmosetick/RRO", "ref": "refs/heads/master", "path": "R-src/src/library/graphics/R/par.R", "content": "#  File src/library/graphics/R/par.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2014 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n##-- These are the ones used in ../../../main/par.c  Query(..) :\n##-- Documentation in\t\t../../../include/Graphics.h\n.Pars <- c(\n           \"xlog\", \"ylog\", ## must be before xaxp, yaxp\n\t   \"adj\", \"ann\", \"ask\", \"bg\", \"bty\",\n\t   \"cex\", \"cex.axis\", \"cex.lab\", \"cex.main\", \"cex.sub\", \"cin\",\n\t   \"col\", \"col.axis\", \"col.lab\", \"col.main\", \"col.sub\",\n           \"cra\", \"crt\", \"csi\",\"cxy\",\t\"din\", \"err\", \"family\",\n           \"fg\", \"fig\", \"fin\",\n\t   \"font\", \"font.axis\", \"font.lab\", \"font.main\", \"font.sub\",\n           \"lab\", \"las\", \"lend\", \"lheight\", \"ljoin\", \"lmitre\", \"lty\", \"lwd\",\n           \"mai\", \"mar\", \"mex\", \"mfcol\", \"mfg\", \"mfrow\", \"mgp\", \"mkh\",\n\t   \"new\", \"oma\", \"omd\", \"omi\", \"page\", \"pch\", \"pin\", \"plt\", \"ps\", \"pty\",\n\t   \"smo\", \"srt\", \"tck\", \"tcl\", \"usr\",\n\t   \"xaxp\", \"xaxs\", \"xaxt\",  \"xpd\",\n\t   \"yaxp\", \"yaxs\", \"yaxt\", \"ylbias\"\n\t   )\n# Replaced with function to evaluate readonly pars because \"gamma\"\n# was at one time readonly on a per-device basis\n# .Pars.readonly <- c(\"cin\",\"cra\",\"csi\",\"cxy\",\"din\")\n\npar <- function (..., no.readonly = FALSE)\n{\n    .Pars.readonly <- c(\"cin\",\"cra\",\"csi\",\"cxy\",\"din\",\"page\")\n    single <- FALSE\n    args <- list(...)\n    if (!length(args))\n\targs <- as.list(if (no.readonly) .Pars[-match(.Pars.readonly, .Pars)]\n                        else .Pars)\n    else {\n\tif (all(unlist(lapply(args, is.character))))\n\t    args <- as.list(unlist(args))\n\tif (length(args) == 1) {\n\t    if (is.list(args[[1L]]) | is.null(args[[1L]]))\n\t\targs <- args[[1L]]\n\t    else\n\t\tif(is.null(names(args)))\n\t\t    single <- TRUE\n\t}\n    }\n    value <- .External2(C_par, args)\n    if(single) value <- value[[1L]]\n    if(!is.null(names(args))) invisible(value) else value\n}\n\nclip <- function(x1, x2, y1, y2)\n    invisible(.External.graphics(C_clip, x1, x2, y1, y2))\n\n" }
{ "repo_name": "jagdeesh109/RRO", "ref": "refs/heads/master", "path": "R-src/src/library/graphics/R/par.R", "content": "#  File src/library/graphics/R/par.R\n#  Part of the R package, http://www.R-project.org\n#\n#  Copyright (C) 1995-2014 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  http://www.r-project.org/Licenses/\n\n##-- These are the ones used in ../../../main/par.c  Query(..) :\n##-- Documentation in\t\t../../../include/Graphics.h\n.Pars <- c(\n           \"xlog\", \"ylog\", ## must be before xaxp, yaxp\n\t   \"adj\", \"ann\", \"ask\", \"bg\", \"bty\",\n\t   \"cex\", \"cex.axis\", \"cex.lab\", \"cex.main\", \"cex.sub\", \"cin\",\n\t   \"col\", \"col.axis\", \"col.lab\", \"col.main\", \"col.sub\",\n           \"cra\", \"crt\", \"csi\",\"cxy\",\t\"din\", \"err\", \"family\",\n           \"fg\", \"fig\", \"fin\",\n\t   \"font\", \"font.axis\", \"font.lab\", \"font.main\", \"font.sub\",\n           \"lab\", \"las\", \"lend\", \"lheight\", \"ljoin\", \"lmitre\", \"lty\", \"lwd\",\n           \"mai\", \"mar\", \"mex\", \"mfcol\", \"mfg\", \"mfrow\", \"mgp\", \"mkh\",\n\t   \"new\", \"oma\", \"omd\", \"omi\", \"page\", \"pch\", \"pin\", \"plt\", \"ps\", \"pty\",\n\t   \"smo\", \"srt\", \"tck\", \"tcl\", \"usr\",\n\t   \"xaxp\", \"xaxs\", \"xaxt\",  \"xpd\",\n\t   \"yaxp\", \"yaxs\", \"yaxt\", \"ylbias\"\n\t   )\n# Replaced with function to evaluate readonly pars because \"gamma\"\n# was at one time readonly on a per-device basis\n# .Pars.readonly <- c(\"cin\",\"cra\",\"csi\",\"cxy\",\"din\")\n\npar <- function (..., no.readonly = FALSE)\n{\n    .Pars.readonly <- c(\"cin\",\"cra\",\"csi\",\"cxy\",\"din\",\"page\")\n    single <- FALSE\n    args <- list(...)\n    if (!length(args))\n\targs <- as.list(if (no.readonly) .Pars[-match(.Pars.readonly, .Pars)]\n                        else .Pars)\n    else {\n\tif (all(unlist(lapply(args, is.character))))\n\t    args <- as.list(unlist(args))\n\tif (length(args) == 1) {\n\t    if (is.list(args[[1L]]) | is.null(args[[1L]]))\n\t\targs <- args[[1L]]\n\t    else\n\t\tif(is.null(names(args)))\n\t\t    single <- TRUE\n\t}\n    }\n    value <- .External2(C_par, args)\n    if(single) value <- value[[1L]]\n    if(!is.null(names(args))) invisible(value) else value\n}\n\nclip <- function(x1, x2, y1, y2)\n    invisible(.External.graphics(C_clip, x1, x2, y1, y2))\n\n" }
{ "repo_name": "b-cell-immunology/sciReptor", "ref": "refs/heads/master", "path": "lib/lib_flowcyt_sorting_indexed.R", "content": "# Name:\t\t\tlib_flowcyt_sorting_indexed.R\n# Verson:\t\t0.1.2  (2015-07-21)\n# Author(s):\tChristian Busse\n# Maintainer:\tChristian Busse (christian.busse@dkfz-heidelberg.de)\n# Licence:\t\tAGPL3\n# Provides:\t\tFunctions to read index sort location data along with the flow cytometric parameters from FCS files produced by BD's \n#\t\t\t\tFACSDiVa software (Versions 7 and 8 should work, but only 8 is tested). The evalulated keywords are not standardized\n#\t\t\t\tup to FCS3.1, therefore data from non-BD machines/software will most likely not work.\n#\t\t\t\tPlease note: BD's Sortware software uses a completely different approach to record index sort data and is therefore\n#\t\t\t\tcurrently not supported.\n# Requires:\t\tBioconductor flowCore package\n# \n#\n\nlibrary(flowCore)\n\n# Defines the supported versions of FCS files\n#\nconfig.fcs.versions.valid\t\t\t<- c(\"FCS2.0\",\"FCS3.0\",\"FCS3.1\")\nconfig.fcs.versions.supported\t\t<- c(\"FCS2.0\",\"FCS3.0\",\"FCS3.1\")\nconfig.software.identifier.keywords\t<- c(\"CREATOR\",\"APPLICATION\")\nconfig.software.whitelist <- c(\"BD FACSDiva Software Version 8.0\", \"BD FACSDiva Software Version 8.0.1\")\nconfig.software.blacklist <- c(\"BD FACS<e2><84><a2> Sortware 1.2.0.142\")\n\n# Call:\t\t\tfunc.read.indexed.FCS(filename)\n#\n# Parameters:\t<filename>:\t\t\t\t\t\t\tComplete filename (if necessary including path) of the FCS file\n#\n# Returns:\t\tA list containing the following elements:\n#\t\t\t\t- \"file\"        [string]\tThe filename of the FCS (Note that this is the current filename, not the one stored in the $FIL keyword)\n#\t\t\t\t- \"fcs.version\" [string]\tThe \"FCSx.y\" version string of the file\n#\t\t\t\t- \"dimensions\"  [vector]\tThe dimensions of the target plate as stored in \"INDEX SORTING DEVICE_DIMENSION\". First value is rows, second columns.\n#\t\t\t\t- \"keyword\"\t\t[matrix]\tThe list of keywords.\n#\t\t\t\t- \"data\"\t\t[matrix]\tThe flow cytometric anf location data for all $TOT events in the file ($PAR + 3 columns). Note that \"row\" and \"column\"\n#\t\t\t\t\t\t\t\t\t\t\tare real coordinates NOT offsets to well A01. \"events\" are counted from A01 left-to-right, then top-to-bottom.\n#\n# Description:\tThis function reads an FCS file containing index sort data and returns a list data structure containing flow cytometric and location data.\n#\nfunc.read.indexed.FCS <- function(fcs.file.full, debug.level) {\n\n    if(missing(debug.level)) {\n\t\tdebug.level <- 2\n\t}\n\n\t# Sanity checks 1. There is no flowCore internal function to test for the actual FCS version, therefore this information is retrieved\n\t# via a direct read (which is basically the same thing that isFCSfile() does). As of 06/2015, index sorting data is not specified in the current FCS 3.1 standard,\n\t# which might result in isolated problems in the processing of index information. Therefore there is a two-stage test plus the check of flowCore support for a given\n\t# FCS version. However, it seems like there is hardly any difference in the way how FACSDiva writtes FCS 3.0 and 3.1 files, therefore no compatibility problem were\n\t# found (yet).\n\t#\n\tif( ! file.exists(fcs.file.full)) {\n\t\tinternal.func.logging(paste(\"File \\\"\", fcs.file.full, \"\\\" does not exist.\", sep=\"\"), 0, debug.level)\n\t\treturn(list())\n\t}\n\tfcs.version <- readChar(fcs.file.full, 6)\n\tif( ! fcs.version %in% config.fcs.versions.supported) {\n\t\tif(fcs.version %in% config.fcs.versions.valid) {\n\t\t\tinternal.func.logging(paste(\"File \\\"\", fcs.file.full, \"\\\" uses currently unsupported FCS version \", fcs.version, \".\", sep=\"\"), 1, debug.level)\n\t\t\tinternal.func.logging(paste(\"Current flowCore support for this FCS version is \", as.character(unname(isFCSfile(fcs.file.full))[1]), \".\", sep=\"\"), 3, debug.level)\n\t\t\treturn(list())\n\t\t} else {\n\t\t\tinternal.func.logging(paste(\"File \\\"\", fcs.file.full, \"\\\" is not a valid FCS file.\", sep=\"\"), 1, debug.level)\n\t\t\treturn(list())\n\t\t}\n\t}\n\tfcs.data <- read.FCS(fcs.file.full)\n\n\n\t# Sanity checks 2. The current version of the FCS format does not specify a way to handle index sort data. In addition, BD has two different sorter\n\t# applications (FACSDiva and Sortware), which use completely different ways to store index data. Finally, files might have been recorded during an\n\t# index sort without actually containing index sort location data. Until further information is provided from BD, the testing for the\n\t# \"INDEX SORT DEVICE TYPE\" and \"INDEX SORT SORTED LOCATION COUNT\" seems to be the most reliable option to sort this out.\n\t#\n\tfcs.keyword.software.all<-description(fcs.data)[names(description(fcs.data)) %in% config.software.identifier.keywords]\n\tif (length(fcs.keyword.software.all) > 1) {\n\t\tif (length(fcs.keyword.software.all) == rle(unlist(unname(fcs.keyword.software.all)))$length[1]) {\n\t\t\tfcs.keyword.software.current <- names(fcs.keyword.software.all)[1]\n\t\t} else {\n\t\t\tstop(paste(\n\t\t\t\t\"File \",\n\t\t\t\tfcs.file.full,\n\t\t\t\t\" contains multiple software identfier keywords with non-identical values (keywords: \",\n\t\t\t\tpaste(names(fcs.keyword.software.all),collapse=\", \"),\n\t\t\t\t\") Aborting!\",\n\t\t\t\tsep=\"\"\n\t\t\t))\n\t\t}\n\t} else {\n\t\tif (length(fcs.keyword.software.all) < 1) {\n\t\t\tstop(paste(\"File \",fcs.file.full, \" does not contain a valid software identfier keyword.\", sep=\"\"))\n\t\t} else {\n\t\t\tfcs.keyword.software.current <- names(fcs.keyword.software.all)[1]\n\t\t }\n\t}\n\n\tfcs.value.software.current <- description(fcs.data)[[fcs.keyword.software.current]]\n\tif (fcs.value.software.current %in% config.software.blacklist) {\n\t\tstop(paste(\n\t\t\t\"File \", fcs.file.full, \" was generated by blacklisted \\\"\", fcs.keyword.software.current, \"\\\"=\\\"\", fcs.value.software.current, \"\\\".\",\n\t\t\tsep=\"\"\n\t\t))\n\t}\n\tif (! description(fcs.data)[[fcs.keyword.software.current]] %in% config.software.whitelist) {\n\t\twarning(paste(\n\t\t\t\"File \", fcs.file.full, \" was generated by \\\"\", fcs.keyword.software.current, \"\\\"=\\\"\", fcs.value.software.current,\n\t\t\t\"\\\", which is NOT on the whitelist.\",\n\t\t\tsep=\"\"\n\t\t))\n\t}\n\n\n\tif(is.null(description(fcs.data)$\"INDEX SORTING SORTED LOCATION COUNT\")) {\n\t\tif(is.null(description(fcs.data)$\"INDEX SORTING DEVICE TYPE\")) {\n\t\t\tstop(paste(\"File \",fcs.file.full, \" does not contain index sort data. Aborting!\", sep=\"\"))\n\t\t} else {\n\t\t\tstop(paste(\"File \",fcs.file.full, \" does contain some index sort data but no location information. Aborting!\", sep=\"\"))\n\t\t}\n\t}\n\n\tfcs.index.sorting.sorted.location.count <- as.integer(description(fcs.data)$\"INDEX SORTING SORTED LOCATION COUNT\")\n\n\tif(fcs.index.sorting.sorted.location.count != as.integer(description(fcs.data)$\"$TOT\")) {\n\t\tstop(\n\t\t\tpaste(\n\t\t\t\t\"File \", fcs.file.full, \" has non-identical counts for total (n=\", as.integer(description(fcs.data)$\"$TOT\"),\n\t\t\t\t\") versus sorted events (n=\", fcs.index.sorting.sorted.location.count,\n\t\t\t\t\"). Aborting!\",\n\t\t\t\tsep=\"\"\n\t\t\t)\n\t\t)\n\t}\n\n\tif (nrow(exprs(fcs.data)) != fcs.index.sorting.sorted.location.count) {\n\t\tstop(\n\t\t\tpaste(\n\t\t\t\t\"Number of entries in the raw data table (n=\", nrow(exprs(fcs.data)),\n\t\t\t\t\") in file \", fcs.file.full, \" does not match the number of sorted cells (n=\", fcs.index.sorting.sorted.location.count,\"). Aborting!\",\n\t\t\t\tsep=\"\"\n\t\t\t)\n\t\t)\n\t}\n\n\t# Get metainfo on the type and size of the sorting device.\n\t# ATTENTION: The device definition in BD FACSDiVa software is transposed (rows x columns) in comparision to the \"normal\" plate format (columns x rows).\n\t# However this is only true for the device definition, the index sorting information are in a \"<column_offset>,<row_offset>;\" format.\n\t#\n\tfcs.index.sorting.device.type <- as.integer(description(fcs.data)$\"INDEX SORTING DEVICE TYPE\")\n\tfcs.index.sorting.device.dimension <- as.integer(unlist(strsplit(description(fcs.data)$\"INDEX SORTING DEVICE_DIMENSION\",\":\")))\n\tnames(fcs.index.sorting.device.dimension) <- c(\"rows\",\"columns\")\n\n\n\t# Parse INDEX SORTING LOCATION data into a single table. The information of order in the metainfo has to be stored since it is the only way\n\t# to associate the location data with the cytometry data.\n\t# The transposition of the lapply output and the \"byrow\" of the following matrix command are necessary since the number of locations given\n\t# in one line of the INDEX SORTING LOCATIONS metainfo varies. Further note that the values in the metainfo are offsets to well A01, therefore\n\t# all numbers are incremented by one to obtain the row / column number.\n\t#\n\tfcs.index.sorting.wells <- matrix(\n\t\tunlist(\n\t\t\tlapply(\n\t\t\t\tgrep(\"INDEX SORTING LOCATIONS\", names(description(fcs.data)),value=TRUE), \n\t\t\t\tfunction(fcs.index.sorting.location.current) {\n\t\t\t\t\ttemp.sorting.locations <- matrix(\n\t\t\t\t\t\tas.integer(\n\t\t\t\t\t\t\tunlist(\n\t\t\t\t\t\t\t\tstrsplit(\n\t\t\t\t\t\t\t\t\tunlist(\n\t\t\t\t\t\t\t\t\t\tstrsplit(\n\t\t\t\t\t\t\t\t\t\t\tdescription(fcs.data)[[fcs.index.sorting.location.current]],\n\t\t\t\t\t\t\t\t\t\t\t\";\"\n\t\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\t\t\t\",\"\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t) + 1,\n\t\t\t\t\t\tncol=2,\n\t\t\t\t\t\tbyrow=TRUE,\n\t\t\t\t\t\tdimnames=list(NULL,c(\"row\", \"column\"))\n\t\t\t\t\t)\n\t\t\t\t\ttemp.sorting.locations <- cbind(\n\t\t\t\t\t\ttemp.sorting.locations,\n\t\t\t\t\t\tmatrix(\n\t\t\t\t\t\t\tc(\n\t\t\t\t\t\t\t\t(temp.sorting.locations[,\"row\"]-1) * fcs.index.sorting.device.dimension[\"columns\"] + temp.sorting.locations[,\"column\"],\n\t\t\t\t\t\t\t\trep(as.integer(sub(\"^INDEX\\ SORTING\\ LOCATIONS_\",\"\",fcs.index.sorting.location.current)), dim(temp.sorting.locations)[1]),\n\t\t\t\t\t\t\t\tseq(1,dim(temp.sorting.locations)[1])\n\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\tncol=3,\n\t\t\t\t\t\t\tdimnames=list(NULL,c(\"event\",\"line\",\"element\"))\n\t\t\t\t\t\t)\n\t\t\t\t\t)\n\t\t\t\t\tt(temp.sorting.locations)\n\t\t\t\t}\n\t\t\t)\n\t\t),\n\t\tncol=5,\n\t\tbyrow=TRUE,\n\t\tdimnames=list(NULL,c(\"row\", \"column\",\"event\",\"line\",\"element\"))\n\t)\n\n\t# Sanity checks 3. Test whether:\n\t# - there are any line+element combinations that are non-unique\n\t# - index sorting locations are outside of the dimensions of the device\n\t# - there are any column+row combinations that are non-unique\n\t# - there are any event numbers that are non-unique\n\t# - the number of parsed locations is identical to the number of recorded cells\n\t#\n\tif (any(duplicated(paste(fcs.index.sorting.wells[,\"line\"],fcs.index.sorting.wells[,\"element\"],sep=\"_\")))) {\n\t\tstop(\n\t\t\tpaste(\n\t\t\t\t\"Collision in lines and elements parsed from the INDEX SORTING LOCATIONS of file \", fcs.file.full, \". Aborting!\",\n\t\t\t\tsep=\"\"\n\t\t\t)\n\t\t)\n\t}\n\n\tif ( \n\t\tmax(fcs.index.sorting.wells[,\"row\"]) > fcs.index.sorting.device.dimension[\"rows\"] || min(fcs.index.sorting.wells[,\"row\"]) < 1 ||\n\t\tmax(fcs.index.sorting.wells[,\"column\"]) > fcs.index.sorting.device.dimension[\"columns\"] || min(fcs.index.sorting.wells[,\"column\"]) < 1\n\t) {\n\t\tstop(\n\t\t\tpaste(\n\t\t\t\t\"Index sorting locations (\",\n\t\t\t\tmin(fcs.index.sorting.wells[,\"column\"]), \">\", max(fcs.index.sorting.wells[,\"column\"]),\n\t\t\t\t\":\",\n\t\t\t\tmin(fcs.index.sorting.wells[,\"row\"]), \">\", max(fcs.index.sorting.wells[,\"row\"]),\n\t\t\t\t\") in file \",\n\t\t\t\tfcs.file.full,\n\t\t\t\t\"are outside of device dimensions (\", fcs.index.sorting.device.dimension[\"columns\"], \":\", fcs.index.sorting.device.dimension[\"rows\"],\n\t\t\t\t\"). Aborting!\",\n\t\t\t\tsep=\"\"\n\t\t\t)\n\t\t)\n\t}\n\n\tif (any(duplicated(paste(fcs.index.sorting.wells[,\"row\"],fcs.index.sorting.wells[,\"column\"],sep=\"_\")))) {\n\t\tstop(\n\t\t\tpaste(\n\t\t\t\t\"Collision in columns and rows parsed from the INDEX SORTING LOCATIONS of file \", fcs.file.full, \". Aborting!\",\n\t\t\t\tsep=\"\"\n\t\t\t)\n\t\t)\n\t}\n\n\tif (any(duplicated(fcs.index.sorting.wells[,\"event\"]))) {\n\t\tstop(\n\t\t\tpaste(\n\t\t\t\t\"Collision in event numbers calculated from the INDEX SORTING LOCATIONS of file \", fcs.file.full, \". Aborting!\",\n\t\t\t\tsep=\"\"\n\t\t\t)\n\t\t)\n\t}\n\n\tif (nrow(fcs.index.sorting.wells) != fcs.index.sorting.sorted.location.count) {\n\t\tstop(\n\t\t\tpaste(\n\t\t\t\t\"Number of parsed INDEX SORTING LOCATIONS (n=\", nrow(fcs.index.sorting.wells),\n\t\t\t\t\") in file \", fcs.file.full, \" does not match the number of sorted events (n=\", fcs.index.sorting.sorted.location.count,\"). Aborting!\",\n\t\t\t\tsep=\"\"\n\t\t\t)\n\t\t)\n\t}\n\n\t# Combine cytometry and location data. This requires that the location data is ordered according to the sequence in which the cells were sorted.\n\t#\n\tfcs.index.sorting.wells <- fcs.index.sorting.wells[order(fcs.index.sorting.wells[,\"line\"],fcs.index.sorting.wells[,\"element\"]),]\n\t\n\tif(is.null(description(fcs.data)$SPILL)) {\n\t\twarning(paste(\"File \",fcs.file.full, \" lacks $SPILL compensation matrix. Skipping compensation!\", sep=\"\"))\n\t\tfcs.data.export <- exprs(fcs.data)\n\t} else {\n\t\tfcs.data.export <- exprs(compensate(fcs.data, description(fcs.data)$SPILL))\n\t}\n\tfcs.data.export <- cbind(\n\t\tfcs.data.export,\n\t\tfcs.index.sorting.wells[,c(\"event\",\"row\",\"column\")]\n\t)\n\n\tfcs.data.export <- fcs.data.export[order(fcs.data.export[,\"event\"]),]\n\n\n\treturn(\n\t\tlist(\n\t\t\tfile=fcs.file.full,\n\t\t\tfcs.version=fcs.version,\n\t\t\tdimensions=fcs.index.sorting.device.dimension,\n\t\t\tdescription=description(fcs.data),\n\t\t\tdata=fcs.data.export\n\t\t)\n\t)\n}\n\ninternal.func.logging<-function(log.message, log.severity, log.level){\n\tlut.severity <- c(\"FATAL\",\"ERROR\",\"WARNING\",\"INFO\",\"DEBUG\",\"DEBUG+\")\n\tif(log.severity <= log.level){\n\t\tcat(\n\t\t\tpaste(\n\t\t\t\t\"[lib_flowcyt_sorting_indexed.R][\",\n\t\t\t\tlut.severity[log.severity+1],\n\t\t\t\t\"] \",\n\t\t\t\tlog.message,\n\t\t\t\t\"\\n\",\n\t\t\t\tsep=\"\"\n\t\t\t)\n\t\t)\n\t}\n}\n" }
{ "repo_name": "wch/r-source", "ref": "refs/heads/trunk", "path": "src/library/stats/R/interaction.plot.R", "content": "#  File src/library/stats/R/interaction.plot.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2013 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\ninteraction.plot <-\n    function(x.factor, trace.factor, response, fun=mean,\n\t     type = c(\"l\", \"p\", \"b\", \"o\", \"c\"), legend = TRUE,\n             trace.label=deparse1(substitute(trace.factor)), fixed=FALSE,\n             xlab = deparse1(substitute(x.factor)), ylab = ylabel,\n             ylim = range(cells, na.rm=TRUE),\n             lty = nc:1, col = 1, pch = c(1L:9, 0, letters),\n             xpd = NULL, leg.bg = par(\"bg\"), leg.bty = \"n\",\n             xtick = FALSE, xaxt = par(\"xaxt\"), axes = TRUE, ...)\n{\n    ylabel <- paste(deparse1(substitute(fun)), \"of \",\n                    deparse1(substitute(response)))\n    type <- match.arg(type)\n    cells <- tapply(response, list(x.factor, trace.factor), fun)\n    nr <- nrow(cells); nc <- ncol(cells)\n    xvals <- 1L:nr\n    ## See if the x.factor labels are a sensible scale\n    if(is.ordered(x.factor)) {\n        wn <- getOption(\"warn\")\n        options(warn=-1)\n        xnm <- as.numeric(levels(x.factor))\n        options(warn=wn)\n        if(!anyNA(xnm)) xvals <- xnm\n    }\n    xlabs <- rownames(cells)\n    ylabs <- colnames(cells)\n    nch <- max(sapply(ylabs, nchar, type=\"width\"))\n    if(is.null(xlabs)) xlabs <- as.character(xvals)\n    if(is.null(ylabs)) ylabs <- as.character(1L:nc)\n    xlim <- range(xvals)\n    xleg <- xlim[2L] + 0.05 * diff(xlim)\n    xlim <- xlim + c(-0.2/nr, if(legend) 0.2 + 0.02*nch else 0.2/nr) * diff(xlim)\n    dev.hold(); on.exit(dev.flush())\n    matplot(xvals, cells, ..., type = type, xlim = xlim, ylim = ylim,\n            xlab = xlab, ylab = ylab, axes = axes, xaxt = \"n\",\n            col = col, lty = lty, pch = pch)\n    if(axes && xaxt != \"n\") {\n\t## swallow ... arguments intended for matplot():\n\taxisInt <- function(x, main, sub, lwd, bg, log, asp, ...)\n\t    axis(1, x, ...)\n\tmgp. <- par(\"mgp\") ; if(!xtick) mgp.[2L] <- 0\n\taxisInt(1, at = xvals, labels = xlabs, tick = xtick, mgp = mgp.,\n\t\txaxt = xaxt, ...)\n    }\n    if(legend) {\n        yrng <- diff(ylim)\n        yleg <- ylim[2L] - 0.1 * yrng\n        if(!is.null(xpd) || { xpd. <- par(\"xpd\")\n                              !is.na(xpd.) && !xpd. && (xpd <- TRUE)}) {\n            op <- par(xpd = xpd)\n            on.exit(par(op), add = TRUE)\n        }\n        text(xleg, ylim[2L] - 0.05 * yrng, paste(\"  \", trace.label), adj = 0)\n        if(!fixed) {\n            ## sort them on the value at the last level of x.factor\n            ord <- sort.list(cells[nr,  ], decreasing = TRUE)\n            ylabs <- ylabs[ord]\n            lty <- lty[1 + (ord - 1) %% length(lty)]\n            col <- col[1 + (ord - 1) %% length(col)]\n            pch <- pch[ord]\n        }\n\n        legend(xleg, yleg, legend = ylabs, col = col,\n               pch = if(type %in% c(\"p\",\"b\")) pch,# NULL works\n               lty = if(type %in% c(\"l\",\"b\")) lty,# NULL works\n               bty = leg.bty, bg = leg.bg)\n    }\n    invisible()\n}\n" }
{ "repo_name": "rlowrance/th", "ref": "refs/heads/master", "path": "CIChart.R", "content": "CIChart <- function(axis.names, axis.values, names, values, values.low, values.high\n                     ,show.zero.value) {\n    # Cleveland dot plot showing confidence intervals\n    # ref: G Graphics Cookbook, p 42 and following\n    df <- data.frame( stringsAsFactors = FALSE\n                     ,names = names\n                     ,values = values\n                     ,values.low = values.low\n                     ,values.high = values.high\n                     )\n    gg <- ggplot( df\n                 ,aes( x = values\n                      ,y = reorder(names, length(names):1)\n                      )\n                 )\n    g1 <-\n        gg +\n        xlab(axis.values) +\n        ylab(axis.names) +\n        geom_point(aes(x = values), size = 3) +\n        geom_point(aes(x = values.low), size = 2) +\n        geom_point(aes(x = values.high), size = 2) +\n        theme_bw() +\n        theme( panel.grid.major.x = element_blank()\n              ,panel.grid.minor.x = element_blank()\n              ,panel.grid.major.y = element_line( colour = 'grey60'\n                                                 ,linetype = 'dashed'\n                                                 )\n              )\n    g <- if (show.zero.value) g1 + coord_cartesian(xlim = c(0, 1.1 * max(values.high))) else g1 \n    g\n}\n" }
{ "repo_name": "rfarouni/rfarouni.github.io", "ref": "refs/heads/master", "path": "assets/projects/BayesianIRT/shinyStan_for_shinyapps/server.R", "content": "# shinyStan\n# Copyright (c) 2015 Jonah Gabry & shinyStan team\n# All rights reserved.\n\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nlibrary(\"shiny\")\nlibrary(\"shinyBS\")\n\n\n# options(shiny.trace=TRUE)\n# load the helper functions\nsource(\"helper_functions/shinyStan_helpers.R\", local=TRUE)\n\n# extract the content of the shinystan_object slots\nsource(\"server_files/utilities/extract_shinystan_object.R\", local=TRUE)\n\n# Begin shinyServer -------------------------------------------------------\n# _________________________________________________________________________\nshinyServer(function(input, output, session) {\n\n  # Stop the app when button is clicked\n  observe({\n    if(input$nav == \"quit\") {\n      stopApp()\n    }\n  })\n\n  # load utility functions & all files for dynamic UIs\n  output_files <- list.files(path = \"server_files/outputs\", full.names = TRUE)\n  utility_files <- list.files(path = \"server_files/utilities\", full.names = TRUE)\n  ui_files <- list.files(path = \"server_files/dynamic_ui\", full.names = TRUE)\n  help_files <- list.files(path = \"server_files/help_and_glossary\", full.names = TRUE)\n  \n  for (f in c(ui_files, utility_files, output_files, help_files)) {\n    source(f, local = TRUE)\n  }\n\n\n  #### tooltips & popovers ####  \n  tooltip_ids <- c(\"download_multiview\", \"dynamic_trace_stack\", \"download_all_summary\", \"tex_options\")\n  tooltip_msgs <- c(\"Will be a list object with one element per plot.\", \n                    \"If 'Stacked' is selected, the chains will be stacked on top of one another rather than drawing them independently. The first series specified in the input data will wind up on top of the chart and the last will be on bottom. Note that the y-axis values no longer correspond to the true values when this option is enabled.\",\n                    \"Save as data.frame (.RData)\", \"Print latex table to R console\")\n  for (id in seq_along(tooltip_ids)) {\n    addTooltip(session, id = tooltip_ids[id], trigger = \"hover\", placement = \"right\",\n               title = tooltip_msgs[id])\n  }\n  \n  popover_ids <- c(paste0(\"tex_\", c(\"booktabs\", \"long\")))\n  popover_msgs <- c(\"From print.xtable {xtable}: If TRUE, the toprule, midrule and bottomrule tags from the LaTex 'booktabs' package are used rather than hline for the horizontal line tags.\",\n                     \"For tables longer than a single page. If TRUE, will use LaTeX package 'longtable'.\")\n  for (id in seq_along(popover_ids)) {\n    addPopover(session, id = popover_ids[id], trigger = \"hover\", placement = \"right\",\n               title = popover_msgs[id])\n  }\n\n\n  #### DATATABLE: summary stats (all parameters) ####\n  output$all_summary_out <- renderDataTable({\n    summary_stats()\n  }, options = list(\n    search = list(regex = TRUE), # allow regular expression when searching for parameter names\n    processing = TRUE,\n    pagingType = \"full\", # show only first, previous, next, last buttons (no page numbers)\n    pageLength = 10,\n    lengthMenu = list(c(5, 10, 20, 50, -1), c('5', '10', '20', '50', 'All')),\n    orderClasses = TRUE,\n    scrollY = 400,\n    scrollX = TRUE,\n    scrollCollapse = FALSE,\n    columnDefs = list(list(width=\"100px\", targets=list(0))),\n    initComplete = I( # change background color of table header\n      'function(settings, json) {\n      $(this.api().table().header()).css({\"background-color\": \"#346fa1\", \"color\": \"#fff\"});\n      }'),\n    rowCallback = I(\n      'function(row, data) {\n        // Bold cells in the first column\n          $(\"td:eq(0)\", row).css(\"font-weight\", \"bold\");\n      }')\n  ))\n  # download the table\n  output$download_all_summary <- downloadHandler(\n    filename = paste0('shinystan_summary_stats.RData'),\n    content = function(file) {\n      shinystan_summary_stats <- summary_stats()\n      save(shinystan_summary_stats, file = file)\n    }\n  )\n  # latex the table\n  # Stop the app when button is clicked\n#   observe({\n#     latex <- input$tex_go\n#     if (latex > 0) summary_stats_latex()\n#   })\n  observeEvent(input$tex_go, handlerExpr = {\n    summary_stats_latex()\n  })\n  #### DATATABLE: summary stats (sampler) ####\n  output$sampler_summary <- renderDataTable({\n    summary_stats_sampler()\n  }, options = list(\n    processing = TRUE,\n    scrollX = TRUE,\n    paging = FALSE,\n    searching = FALSE,\n    info = FALSE,\n    orderClasses = TRUE,\n    initComplete = I( # change background color of table header\n      'function(settings, json) {\n      $(this.api().table().header()).css({\"background-color\": \"#346fa1\", \"color\": \"#fff\"});\n      }'),\n    rowCallback = I(\n      'function(row, data) {\n        // Bold cells in the first column\n          $(\"td:eq(0)\", row).css(\"font-weight\", \"bold\");\n      }')\n  ))\n  #### PLOT: multiple parameters ####\n  output$plot_param_vertical_out <- renderPlot({\n    plot_param_vertical()\n  }, height = calc_height_param_plot)\n  # download the plot\n  output$download_multiparam_plot <- downloadHandler(\n    filename = 'shinystan_param_plot.RData',\n    content = function(file) {\n      shinystan_param_plot <- plot_param_vertical()\n      save(shinystan_param_plot, file = file)\n    }\n  )\n  #### PLOT: n_eff / total sample size ####\n  output$n_eff_plot_out <- renderPlot({\n    x <- n_eff_plot()\n    suppressMessages(suppressWarnings(print(x)))\n  })\n  #### PLOT: ratio of mcmc se to posterior sd  ####\n  output$mcse_over_sd_plot_out <- renderPlot({\n    x <- mcse_over_sd_plot()\n    suppressMessages(suppressWarnings(print(x)))\n  })\n  #### PLOT: rhat ####\n  output$rhat_plot_out <- renderPlot({\n    x <- rhat_plot()\n    suppressMessages(suppressWarnings(print(x)))\n  })\n  #### TEXT: n_eff warnings ####\n  output$n_eff_warnings_title <- renderText({\n    paste0(\"The following parameters have an effective sample size less than \", input$n_eff_threshold,\"% of the total number of samples: \")\n  })\n  output$n_eff_warnings <- renderText({\n    n_eff_warnings()\n  })\n  #### TEXT: rhat warnings ####\n  output$rhat_warnings_title <- renderText({\n    paste0(\"The following parameters have an Rhat value above \", input$rhat_threshold,\": \")\n  })\n  output$rhat_warnings <- renderText({\n    rhat_warnings()\n  })\n  #### TEXT: mcmc se to posterior sd warnings ####\n  output$mcse_over_sd_warnings_title <- renderText({\n    paste0(\"The following parameters have a Monte Carlo standard error greater than \", input$mcse_threshold ,\"% of the posterior standard deviation:\")\n  })\n  output$mcse_over_sd_warnings <- renderText({\n    mcse_over_sd_warnings()\n  })\n\n  #### PLOT: autocorrelation ####\n  output$autocorr_plot_out <- renderPlot({\n    autocorr_plot()\n  })\n  # download the plot\n  output$download_autocorr <- downloadHandler(\n    filename = paste0('shinystan_autocorr.RData'),\n    content = function(file) {\n      shinystan_autocorr <- autocorr_plot\n      save(shinystan_autocorr, file = file)\n    }\n  )\n  #### PLOT: trace plots (multiple parameters) ####\n  output$multi_trace_plot_out <- renderPlot({\n    x <- multi_trace_plot()\n    suppressWarnings(print(x)) # this avoids warnings about removing rows when using tracezoom feature\n  }, height = calc_height_trace_plot)\n  # download the plot\n  output$download_multi_trace <- downloadHandler(\n    filename = paste0('shinystan_multi_trace.RData'),\n    content = function(file) {\n      shinystan_multi_trace <- multi_trace_plot()\n      save(shinystan_multi_trace, file = file)\n    }\n  )\n\n  #### TEXT: parameter name ####\n  output$param_name <- renderText({\n    input$param\n  })\n  #### TABLE: summary stats (single parameter) ####\n  output$parameter_summary_out <- renderDataTable({\n    as.data.frame(round(parameter_summary(), 2))\n  }, options = list(\n    paging = FALSE, searching = FALSE, info = FALSE, ordering = FALSE,\n    initComplete = I( # change background color of table header\n      'function(settings, json) {\n      $(this.api().table().header()).css({\"background-color\": \"white\", \"color\": \"black\"});\n      }')\n  ))\n  #### PLOT: Multiview ####\n  output$multiview_param_name <- renderUI(strong(style = \"font-size: 250%; color: #f9dd67;\", input$param))\n  output$multiview_trace <- renderPlot(trace_plot_multiview())\n  output$multiview_density <- renderPlot(density_plot_multiview())\n  output$multiview_autocorr <- renderPlot(autocorr_plot_multiview())\n  # download multiview plot\n  output$download_multiview <- downloadHandler(\n    filename = 'shinystan_multiview.RData',\n    content = function(file) {\n      param_name <- input$param\n      shinystan_multiview <- list()\n      shinystan_multiview[[paste0(\"trace_\", param_name)]] <- trace_plot_multiview()\n      shinystan_multiview[[paste0(\"density\", param_name)]] <- density_plot_multiview()\n      shinystan_multiview[[paste0(\"ac_\", param_name)]] <- autocorr_plot_multiview()\n      save(shinystan_multiview, file = file)\n    }\n  )\n\n  ### PLOT: histogram ####\n  output$hist_plot_out <- renderPlot({\n    x <- hist_plot()\n    suppressMessages(suppressWarnings(print(x)))\n  })\n  # download plot\n  output$download_histogram <- downloadHandler(\n    filename = 'shinystan_histogram.RData',\n    content = function(file) {\n      shinystan_histogram <- hist_plot()\n      save(shinystan_histogram, file = file)\n    }\n  )\n  #### PLOT: dynamic trace plot ####\n  output$dynamic_trace_plot_out <- dygraphs::renderDygraph({\n    dynamic_trace_plot()\n  })\n  ### PLOT: density ####\n  output$density_plot_out <- renderPlot({\n    density_plot()\n  })\n  # download plot\n  output$download_density <- downloadHandler(\n    filename = 'shinystan_density.RData',\n    content = function(file) {\n      shinystan_density <- density_plot()\n      save(shinystan_density, file = file)\n    }\n  )\n#   #### PLOT: trivariate 3D ####\n#   output$trivariate_plot_out <- threejs::renderScatterplotThree({\n#     trivariate_plot()\n#   })\n  #### PLOT: bivariate ####\n  output$bivariate_plot_out <- renderPlot({\n    bivariate_plot()\n  })\n  output$download_bivariate <- downloadHandler(\n    filename = 'shinystan_bivariate.RData',\n    content = function(file) {\n      shinystan_bivariate <- bivariate_plot()\n      save(shinystan_bivariate, file = file)\n    }\n  )\n\n  #### TEXT: User's model notes ####\n  observeEvent(input$save_user_model_info, handlerExpr = {\n    if (input$user_model_info != \"\")\n      shinystan_object@user_model_info <<- input$user_model_info\n  })\n  output$user_text_saved <- renderText({\n    input$save_user_model_info # take dependency on action button\n    if (input$save_user_model_info != 0) {\n      print(paste(\"Saved:  \", format(Sys.time(), \"%a %b %d %Y %X\")))\n    }\n  })\n\n}) # End shinyServer\n" }
{ "repo_name": "essicolo/AgFun", "ref": "refs/heads/master", "path": "ilrNA.R", "content": "ilrNA = function(comp, sbp, bal) {\r\n  comp = unclass(comp)\r\n  bal = unclass(bal)\r\n  for (n in 1:nrow(comp)){\r\n    for (p in 1:ncol(comp)) {\r\n      for (q in 1:ncol(bal)) {\r\n        if (sbp[q,p] != 0 & is.na(comp[n,p])) bal[n,q] <- NA\r\n      }\r\n    }\r\n  }\r\n  return(bal)\r\n}" }
{ "repo_name": "andrewdefries/andrewdefries.github.io", "ref": "refs/heads/master", "path": "FDA_Pesticide_Glossary/2-Methoxy-5-nitrophe.R", "content": "library(\"knitr\")\nlibrary(\"rgl\")\n#knit(\"2-Methoxy-5-nitrophe.Rmd\")\n#markdownToHTML('2-Methoxy-5-nitrophe.md', '2-Methoxy-5-nitrophe.html', options=c(\"use_xhml\"))\n#system(\"pandoc -s 2-Methoxy-5-nitrophe.html -o 2-Methoxy-5-nitrophe.pdf\")\n\n\nknit2html('2-Methoxy-5-nitrophe.Rmd')\n" }
{ "repo_name": "AlteryxLabs/AlteryxRhelper", "ref": "refs/heads/master", "path": "R/io.R", "content": "#' Alteryx Read Function\n#'\n#' This function reads data from an Alteryx input stream. Every time a\n#' macro/workflow runs this function, the input data gets saved as an rds file\n#' with the prefix \".input\", to the macro/workflow directory. This allows the R\n#' code to be run outside of Alteryx.\n#'\n#' @export\n#' @param name name\n#' @param mode mode\n#' @param bIncludeRowNames include row names\n#' @param default default\nread.Alteryx2 = function(name, mode = \"data.frame\",\n    bIncludeRowNames = FALSE, default){\n  inAlteryx = function(){'package:AlteryxRDataX' %in% search()}\n  if (inAlteryx()){\n    wdir = getOption('alteryx.wd', '%Engine.WorkflowDirectory%')\n    requireNamespace('AlteryxRDataX')\n    d <- AlteryxRDataX::read.Alteryx(name = name, mode = mode, bIncludeRowNames = FALSE)\n    # i need to figure out a way to make things work during debug as well\n    # as production. it would be ideal to expose this flag in the macro ui\n    # so that an end user can flip the debug mode to inspect further\n    if (getOption('alteryx.debug', F)){\n      f <- paste0(wdir, '.input', name, '.rds')\n      msg <- paste0('Saving input ', name, ' to ', f)\n      AlteryxRDataX::AlteryxMessage(msg)\n      if (file.exists(f)){file.remove(f)}\n      saveRDS(d, file = f)\n    }\n  } else {\n    if (file.exists(f <- paste0('.input', name, '.rds'))){\n      d <- readRDS(f)\n    } else if (!(missing(default))){\n      d <- default\n    } else {\n      stop(paste(\"Missing Input\", name))\n    }\n  }\n  return(d)\n}\n\n#' Alteryx Write Function\n#'\n#'\n#' @export\n#' @param data data\n#' @param nOutput output connection number\n#' @param bIncludeRowNames include row names\n#' @param source source\nwrite.Alteryx2 = function(data, nOutput = 1, bIncludeRowNames = FALSE, source = \"\"){\n  if (inAlteryx()){\n    requireNamespace('AlteryxRDataX')\n    AlteryxRDataX::write.Alteryx(data = data, nOutput = nOutput,\n      bIncludeRowNames = bIncludeRowNames, source = source\n    )\n  } else {\n    head(data)\n  }\n}\n\n#' Alteryx Graph Function\n#'\n#'\n#' @export\n#' @param expr expression to generate graph\n#' @param nOutput output connection number\n#' @param width width\n#' @param height height\n#' @param ... additional arguments\nAlteryxGraph2 = function(expr, nOutput = 1, width = 576, height = 576, ...){\n  print_ = function(expr){if (inherits(expr, 'ggplot')){print(expr)} else{expr}}\n  if ('package:AlteryxRDataX' %in% search()){\n    requireNamespace('AlteryxRDataX')\n    AlteryxRDataX::AlteryxGraph(nOutput, width = width, height = height, ...)\n    print_(expr)\n    invisible(dev.off())\n  } else {\n    print_(expr)\n  }\n}\n" }
{ "repo_name": "LTLA/diffHic", "ref": "refs/heads/master", "path": "R/normalizeCNV.R", "content": "normalizeCNV <- function(data, margins, prior.count=3, span=0.3, maxk=500, \n                         assay.data=1, assay.marg=1, ...)\n# This performs two-dimensional loess smoothing, using the counts and the \n# marginal counts to compute the abundance and the marginal fold-changes,\n# respectively. Both are used as covariates in the model to smooth out any\n# systematic differences in interaction intensity. The aim is to get rid\n# of any CNV-induced bias, quantified by the differences in the marginals.\n#\n# written by Aaron Lun\n# created 11 September 2014\n# last modified 18 September 2017\n{\n    # Checking for proper type.\n    .check_StrictGI(data)\n    data.binprs <- assay(data, assay.data)\n    data.margin <- assay(margins, assay.marg)\n    if (is.null(data$totals) || is.null(margins$totals)) { \n        stop(\"'totals' should be non-NULL for 'data' and 'margins'\")\n    } else if (!identical(margins$totals, data$totals)) { \n\t\twarning(\"library sizes should be identical for 'margins' and 'data'\")\n\t}\n\n    # Smaller prior for bin pair count to calculate offsets;\n    # larger prior for margin counts to stabilize covariates.\n\tcont.cor <- 0.5\n\tcont.cor.scaled <- cont.cor * data$totals/mean(data$totals)\n\tab <- aveLogCPM(data.binprs, lib.size=data$totals, prior.count=cont.cor)\n\tmave <- aveLogCPM(data.margin, lib.size=margins$totals, prior.count=prior.count)\n\n\t# Generating covariates.\n\tmab <- cpm(data.margin, lib.size=margins$totals, log=TRUE, prior.count=prior.count) - mave\n\tmatched <- matchMargins(data, margins)\t\n\tma.adjc <- mab[matched$anchor1,,drop=FALSE] \n\tmt.adjc <- mab[matched$anchor2,,drop=FALSE]\n\n\toffsets <- matrix(0, nrow=nrow(data), ncol=ncol(data))\n\tfor (lib in seq_len(ncol(data))) {\n\t\tma.fc <- ma.adjc[,lib]\n\t\tmt.fc <- mt.adjc[,lib]\n\n\t\t# Anchor/target distinction is arbitrary, so this coerces otherwise-identical \n\t\t# points into the same part of the covariate space (see comment below).\n\t\tmfc1 <- (ma.fc + mt.fc)/2\n\t\tmfc2 <- abs(ma.fc - mt.fc)\n\t\tall.cov <- list(mfc1, mfc2, ab)\n\t\n\t\t# Fitting a loess surface with the specified covariates.\t\n\t\ti.fc <- log2(data.binprs[,lib] + cont.cor.scaled[lib]) - ab \n\t\tcov.fun <- do.call(lp, c(all.cov, nn=span, deg=1))\n\t\tfit <- locfit(i.fc ~ cov.fun, maxk=maxk, ..., lfproc=locfit.robust) \n\t\toffsets[,lib] <- fitted(fit)\n\t}\n\n\toffsets <- offsets/log2(exp(1))\n\toffsets <- offsets - rowMeans(offsets)\n\treturn(offsets)\n}\n\n##################### COMMENT ##########################\n# You get two copy number changes for each bin pair, one for each region. The\n# simplest coercion involves defining one covariate as the maximum change, and\n# the other covariate as the minimum change. However, this gives a covariate\n# space that is cut off past the diagonal. This won't be happily fitted in\n# locfit, as it uses a rectangular grid (check out Computational Methods in\n# Loader's book). You need all corners of the grid to interpolate, but one of\n# those corners will be useless if it hangs on the wrong side of the diagonal.\n# Instead, we rotate the space by 45 degrees, such that the diagonal is now a\n# vertical line. The boundary of the grid now coincides with the true boundary\n# of the space. All corners will now have sensible evaluations, such that \n# interpolation will be more reliable.\n########################################################\n\nmatchMargins <- function(data, margins) \n# This function just matches the bin pairs in 'data' to the two indices of\n# 'margins' that each bin corresponds to.\n#\n# written by Aaron Lun\n# created 17 September 2014\t\n# last modified 8 December 2015 \n{\n    .check_StrictGI(data)\n    anchor1 <- anchors(data, type=\"first\", id=TRUE)\n    anchor2 <- anchors(data, type=\"second\", id=TRUE)\n\n\t# Checking to ensure that the regions are the same, and matching otherwise.\n\tif (any(regions(data)!=rowRanges(margins))) {\n        m <- match(regions(data), rowRanges(margins))\n        anchor1 <- m[anchor1]\n        anchor2 <- m[anchor2]\n        if (any(is.na(anchor1)) || any(is.na(anchor2))) {\n            stop(\"regions in 'data' missing from 'margins'\")\n        }\n    }\n\treturn(data.frame(anchor1=anchor1, anchor2=anchor2))\n}\t\n" }
{ "repo_name": "jasonyaopku/Data-Processing-in-R", "ref": "refs/heads/master", "path": "classes/class2/Code_class2/code_class2.R", "content": "#notice the directory\nsetwd(\"~/百度云同步盘/Classes/Data-Processing-in-R/Student HW/class 1\")\n#vector calculation\na=c(1,2,3,4,5,6);\nb=2;\na/b;\nc=c(2,4);\na/c\nd=c(1,2,3,4)\na/d\n\n#multi condition\nage=17;\ngender=\"male\"\n(age>17)&(gender==\"male\")\n(age>=17)|(age>10)&(age<9)\n\n#multi loops\nfor(var1 in c(1,2,3))\n{\n  for(var2 in c(2,4,6,8,10))\n  {\n    print(var1*var2);\n  }\n  print(\"one loop end!\");\n}\n\n#dataframe element\nnames = c(\"zhangsan\",\"lisi\");\nages = c(18,19);\ndf.test = data.frame(names,ages);\nnames = c(names,\"shenyufeng\")\nnames=c(1,2);\n\n\n#question 11,12\nnames = c(\"zhangsan\",\"lisi\",\"wangwu\");\nheights = c(165,175,170);\nweights = c(55,65,70);\nuser.data = data.frame(names,heights);\nstudent.num = length(names);\nuser.data$names = as.character(user.data$names);\nfor(i in 1:student.num)\n{\n  print(user.data[i,1]);\n  if(user.data[i,2]>=170)\n  {\n    print(user.data[i,1])\n  }\n}\n\nuser.data = cbind(user.data,weights);\n\n#set wd\n#setwd(\"~/百度云同步盘/Classes/Data-Processing-in-R/Student HW/class 1\");\n\n#read data\nuser.data = read.table(file = \"datatest.txt\");\nuser.data = read.table(file = \"datatest.txt\",sep = ',');\nuser.data = read.table(file = \"datatest.txt\",sep = ',',header = TRUE);\nuser.data = read.table(file = \"datatest.txt\",sep = ' ',header = TRUE);\nuser.data = read.csv(\"datatest.csv\");\n\n#write data\nwrite.table(user.data);\nwrite.table(user.data,\"output.data.txt\");\nwrite.table(user.data,\"output.data.txt\",row.names = FALSE);\nwrite.table(user.data,\"output.data.txt\",row.names = FALSE,col.names = FALSE);\n\nwrite.table(user.data,\"output.data.txt\",row.names = FALSE,sep = \"++++\");\nwrite.csv(user.data,\"output.data.csv\");\n\n#homework analysis \nHW.Filenames = dir();\nhead(HW.Filenames);\nstudent.names = read.table(file=\"names.txt\",header = TRUE)\nanalysis.result = data.frame(student.names);\nanalysis.result$is_submit = rep(FALSE,length(student.names));\nanalysis.result$times = rep(0,length(student.names));\nanalysis.result$is_normal = rep(FALSE,length(student.names));\nstudent.names$name = as.character(student.names$name)\nfor(i in 1:nrow(student.names))\n{\n  \n#   for(j in 1:length(HW.Filenames))\n#   {\n    namei=student.names[i,1];\n    print(namei);\n    indexi = grep(namei,HW.Filenames);\n    analysis.result[i,2] = length(indexi)>0;\n    analysis.result[i,3] = length(indexi);\n    namei.files = HW.Filenames[indexi];\n    is_normali = length(grep(\"DSJJYB\",namei.files))>0;\n    analysis.result[i,4] = is_normali;\n # }\n}\n\n#probability of submitting\nmean(analysis.result$is_submit)\nmean(analysis.result$times[analysis.result$times>0])\nmax(analysis.result$times)\nmean(analysis.result$is_normal)\n\n\n" }
{ "repo_name": "James-Thorson/VAST", "ref": "refs/heads/master", "path": "R/check_fit.R", "content": "#' Check fit for VAST model\n#'\n#' \\code{check_fit} checks bounds and throws an informative message if any look bad\n#'\n#' If `check_fit` identifies an issue in estimated parameters, then the model structure should typically be changed.\n#' Recommended model changes differ somewhat for univariate and multivariate models as explained below.\n#'\n#' For univariate models:\n#' \\itemize{\n#' \\item If `ln_H_input` are approaching extreme values (i.e., > 5 or < -5), then turn consider turning off anisotropy, `make_data(..., Aniso=FALSE)`\n#' \\item If `L_beta1_z` is approaching zero (i.e., +/- 0.001), then turn off random-effects for temporal variation in the first intercept `RhoConfig[\"Beta1\"]=3`\n#' \\item If `L_beta2_z` is approaching zero (i.e., +/- 0.001), then turn off random-effects for temporal variation in the first intercept `RhoConfig[\"Beta2\"]=3`\n#' \\item If `L_omega1_z` is approaching zero (i.e., +/- 0.001), then turn off spatial effects for the 1st linear predictor `FieldConfig[\"Omega1\"]=0`\n#' \\item If `L_omega2_z` is approaching zero (i.e., +/- 0.001), then turn off spatial effects for the 1st second predictor `FieldConfig[\"Omega2\"]=0`\n#' \\item If `L_epsilon1_z` is approaching zero (i.e., +/- 0.001), then turn off spatio-temporal effects for the 1st linear predictor `FieldConfig[\"Epsilon1\"]=0`\n#' \\item If `L_epsilon2_z` is approaching zero (i.e., +/- 0.001), then turn off spatio-temporal effects for the 1st second predictor `FieldConfig[\"Epsilon2\"]=0`\n#' \\item If `Beta_rho1_f` is approaching one (i.e., > 0.999), then turn consider reducing to a random-walk structure for the intercept of the 1st linear predictor `RhoConfig[\"Beta1\"]=2`\n#' \\item If `Beta_rho2_f` is approaching one (i.e., > 0.999), then turn consider reducing to a random-walk structure for the intercept of the 2st linear predictor `RhoConfig[\"Beta2\"]=2`\n#' \\item If `Epsilon_rho1_f` is approaching one (i.e., > 0.999), then turn consider reducing to a random-walk structure for spatio-temporal variation of the 1st linear predictor `RhoConfig[\"Epsilon1\"]=2`\n#' \\item If `Epsilon_rho2_f` is approaching one (i.e., > 0.999), then turn consider reducing to a random-walk structure for spatio-temporal variation of the 2nd linear predictor `RhoConfig[\"Epsilon2\"]=2`\n#' }\n#'\n#' For multivariate models, these same principles apply, but there are more options to simplify model structure.\n#' For example, if any `L_beta1_z` is approaching zero (i.e., +/- 0.001), then consider using `fit_model(...,Map=[custom-map])` to turn off individual parameters;\n#' or if using a factor model then reduce the number of factors by decreasing FieldConfig[\"Beta1\"]`\n#'\n#' @param parameter_estimates output from \\code{\\link[TMBhelper]{fit_tmb}}\n#' @param check_gradients Boolean stating whether to check bounds as well as other issues\n#' @param quiet Boolean stating whether to print warnings to terminal\n#' @return Did an automated check find an obvious problem code (TRUE is bad; FALSE is good)\n#'\n#' @export\n#' @md\n# Using https://cran.r-project.org/web/packages/roxygen2/vignettes/rd-formatting.html for guidance on markdown-enabled documentation\ncheck_fit = function( parameter_estimates, check_gradients=FALSE, quiet=FALSE ){\n\n  # Initialize code for good model\n  problem_found = FALSE\n\n  # Check for informative bounds\n  On_bounds = ifelse( parameter_estimates$par<(parameter_estimates$diagnostics[,'Lower']+0.0001) | parameter_estimates$par>(parameter_estimates$diagnostics[,'Upper']-0.0001), TRUE, FALSE )\n  if( any(On_bounds) ){\n    problem_found = TRUE\n    if(quiet==FALSE){\n      message(\"\\nCheck bounds for the following parameters:\")\n      print( parameter_estimates$diagnostics[which(On_bounds),] )\n    }\n  }\n\n  # Check for stuff at zero\n  At_zero = ifelse( abs(parameter_estimates$par) < 0.0001, TRUE, FALSE )\n  if( any(At_zero) ){\n    problem_found = TRUE\n    if(quiet==FALSE){\n      message(\"\\nThe following parameters appear to be approaching zero:\")\n      print( parameter_estimates$diagnostics[which(At_zero),] )\n      if( length(grep( \"L_\", names(parameter_estimates$par[which(At_zero)]))) ){\n        message(\"Please turn off factor-model variance parameters `L_` that are approaching zero and re-run the model\")\n      }\n    }\n  }\n\n  # Check bad gradients\n  Bad_gradient = ifelse( abs(parameter_estimates$diagnostics[,'final_gradient']) > 0.001, TRUE, FALSE )\n  if( check_gradients==TRUE && any(Bad_gradient) ){\n    problem_found = TRUE\n    if(quiet==FALSE){\n      message(\"\\nThe following parameters has a bad final gradient:\")\n      print( parameter_estimates$diagnostics[which(Bad_gradient),] )\n    }\n  }\n\n  # Return\n  return( invisible(problem_found) )\n}\n\n" }
{ "repo_name": "EFavDB/PubmedCentral_Scraper", "ref": "refs/heads/master", "path": "R/eSearch.R", "content": "## eSearch.R\n## PERFORM SIMPLE SEARCH (ESearch) ON PUBMED CENTRAL AND \n## RETRIEVE UID'S OUTPUT BY QUERY\n\n##==================================================================\n## INPUT ARGUMENTS: \n##  -searchterms (list): a list of length 2, containing sets of strings\n##    to be searched, where there should exist at least one match to each\n##    set of strings.  The order of the sets does not matter.\n##  -nreturns (integer): maximum number of query records to retrieve,\n##    up to a maximum of 10,000\n##  -database (string): abbreviation for ncbi database to search (see ncbi\n##    eutils documentation)\n##  -sortby (string): method used to sort id's in the esearch output (see\n##    ncbi eutils documentation)\n## OUTPUT: a vector of uid's/pmcid's (pubmed central id's)\n##\n## EXAMPLE:\n##  eSearch(list(topic=c(\"trastuzumab\",\"herceptin\"),\n##                plottype=c(\"tumor growth\", \"tumor volume\",\n##                          \"tumor size\", \"tumor inhibition\",\n##                          \"tumor growth inhibition\", \"tgi\",\n##                          \"tumor response\", \"tumor regression\")), \n##          nreturns=10, database=\"pmc\", sortby=\"relevance\")\n\n##==================================================================\nlibrary(\"RCurl\")\nlibrary(\"XML\")\nlibrary(\"httr\")\nlibrary(\"rvest\")\nlibrary(\"stringr\")\n\neSearch = function(searchterms, nreturns=10, database=\"pmc\", sortby=\"relevance\") {\n  \n  ## STRING TOGETHER SEARCH TERMS TO FORM QUERY\n  stringTerms = function(x) {\n    ## x is a vector of strings\n    ## paste quotation marks around each string element\n    x = paste0(\"\\\"\",x,\"\\\"\")\n    longstring = paste(x, collapse=\"+OR+\")\n    ## substitute '+' for spaces\n    longstring = gsub(\"\\\\s+\", \"\\\\+\", longstring)\n    ## wrap entire long string in parentheses\n    longstring = paste0(\"(\",longstring,\")\")\n    return(longstring)\n  }\n  query = paste(sapply(searchterms, stringTerms), collapse=\"+AND+\")\n  query = paste0(\"term=\",query)\n  \n  ##==================================================================\n  \n  ## NCBI DATABASE - BASE URL OF API AND GENERAL API OPTIONS\n  ## base eutils url\n  url.base = \"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n  ## eSearch utility\n  esearch =  \"esearch.fcgi?\"\n  \n  ## database to search (default is pubmed central to use with article scraper)\n  db = paste0(\"db=\",database)\n  \n  ## maximum number of uid's to be retrieved (max=100k)\n  retmax = paste0(\"retmax=\",nreturns)\n  \n  ## method used to sort uid's output by eSearch\n  sortmethod = paste0(\"sort=\",sortby)\n  \n  ## compose url for eSearch\n  url.esearch = paste0(url.base, esearch, db, \"&\", retmax,\n                       \"&\", sortmethod, \"&\", query)\n  \n  ## get and parse xml data returned by eSearch\n  data.esearch = getURL(url.esearch)\n  data.xml = xmlParse(data.esearch)\n  \n  ## get uid's/pmcid's\n  uids = data.xml %>% xml_nodes(\"Id\") %>% xml_text()\n  return(uids)\n}" }
{ "repo_name": "ColumbusCollaboratory/electron-quick-start", "ref": "refs/heads/master", "path": "R-Portable-Win/library/ggplot2/doc/profiling.R", "content": "## ----setup, include = FALSE----------------------------------------------\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\"\n)\n\n## ------------------------------------------------------------------------\nlibrary(ggplot2)\nlibrary(profvis)\n\np <- ggplot(mtcars, aes(x = mpg, y = disp)) + \n  geom_point() + \n  facet_grid(gear ~ cyl)\n\nprofile <- profvis(for (i in seq_len(100)) ggplotGrob(p))\n\nprofile\n\n## ---- eval=FALSE, include=FALSE------------------------------------------\n#  saveRDS(profile, file.path('profilings', paste0(packageVersion('ggplot2'), '.rds')))\n\n" }
{ "repo_name": "tqsclass/coursera", "ref": "refs/heads/master", "path": "GettingCleaningData/run_Analysis.R", "content": "## Coursera Getting & Cleaning Data Peer Assessment\n##\n## Requirements:\n##  The working directory must be set to a directory that contains an\n##  unzipped copy of 'UCI HAR Dataset'.  When set properly, the\n##  directory will have two folders ('test' and 'train') and six *.txt\n##  files with names like 'features.txt'.\n##\n##  This is an example from the author's Apple machine:\n     setwd(\"~/Developer/data_science/R/Getting and Cleaning Data/UCI HAR Dataset\")\n##\n##  Two libraries are required in your R environment:  'reshape2'\n##   and 'knitr'\n\n\n#### Step 0  Initialize R and Set Useful Global Data\n##\n\n## Load Required Libraries\nlibrary(reshape2)\nlibrary(knitr)\n\n## Read column labels (e.g. feature names) for the datasets.\nmean_columns <- read.table(\"meanFeatures.txt\", sep=\"\")\nstd_columns <- read.table(\"stdFeatures.txt\", sep=\"\")\ncolumn_numbers <- c(mean_columns[,1],std_columns[,1])\nactivity_labels <- read.table(\"activity_labels.txt\",sep=\"\")\nnames(activity_labels) <- c(\"Ordinal\",\"Label\")\n\n\n#### Step 1  Process Test Data\n##\n##   read data file and extract columns we are interested in\n\nraw_test <- read.table(\"./test/X_test.txt\",sep=\"\")\ntest <- raw_test[,column_numbers]\n\n## add columns for activity and subject\ntest_subjects <- read.table(\"./test/subject_test.txt\",sep=\"\")\ntest_activities <- read.table(\"./test/y_test.txt\",sep=\"\")\ntest <-cbind(test_subjects,test_activities,test)\nnames(test) <- c(\"Subject\",\"Activity\",as.character(mean_columns[,2]),as.character(std_columns[,2]));\n\n## convert activity ordinals to labels for readability, e.g. 1=\"WALKING\"\ntest[,2] <- as.factor(test[,2])\nlevels(test[,2]) <- activity_labels[,2]\n\n## clean up\nrm(raw_test)\n\n\n#### Step 2  Process Training Data\n##\n##   read data file and extract columns we are interested in\nraw_train <- read.table(\"./train/X_train.txt\",sep=\"\")\ntrain <- raw_train[,column_numbers]\n\n## add columns for activity and subject\ntrain_subjects <- read.table(\"./train/subject_train.txt\",sep=\"\")\ntrain_activities <- read.table(\"./train/y_train.txt\",sep=\"\")\ntrain <-cbind(train_subjects,train_activities,train)\nnames(train) <- c(\"Subject\",\"Activity\",as.character(mean_columns[,2]),as.character(std_columns[,2]));\n\n## convert activity ordinals to labels for readability, e.g. 1=\"WALKING\"\ntrain[,2] <- as.factor(train[,2])\nlevels(train[,2]) <- activity_labels[,2]\n\n## clean up\nrm(raw_train)\n\n\n#### Step 3 Join Files\n##\n##   Join both data sets and write the final working data set to disk\n##   to save our results so far.\n\nfinal <- rbind(train,test)\nwrite.table(final,\"final.csv\",sep=\", \",row.names=FALSE)\n\n\n#### Step 4 Tidy Dataset\n##\n\nfinal_melt <- melt(final,id=c(\"Subject\",\"Activity\"));\ntidy_data <- dcast(final_melt, Activity +Subject ~ variable,mean)\nwrite.table(tidy_data,\"tidy_data.csv\",sep=\"' \",row.names=FALSE)\n\n##  Here is an example of what you can do with this dataset. This line\n##  gives us the quantiles of the \"Mean X Body Acceleration\" values\n##  for all 30 subjects while 'walking':     \nquantile(tidy_data[tidy_data[,1]==\"WALKING\",3])\n##  It can help you determine if there is enough variance in this\n##  measurement to be useful for classification purposes.\n\n#### #### #### The End! #### #### ####\n\n##  Archival Code\n##  everything below here is old code used during initial development\n##  saved simply for archival purposes\n\n## alternate  (same result for std; mean has 13 additional columns)\n# features <- read.table(\"features.txt\",sep=\"\")\n# features <- features[,-1]\n# mean_columns <- grep(\"mean\",features, ignore.case=FALSE)\n# std_columns <- grep(\"std\",features, ignore.case=FALSE)\n\n# convert activity ordinals to labels for readability\n# u1 <- test[,2] == 1\n# u2 <- test[,2] == 2\n# u3 <- test[,2] == 3\n# u4 <- test[,2] == 4\n# u5 <- test[,2] == 5\n# u6 <- test[,2] == 6\n# \n# test[u1,2] <- as.factor(activity_labels[1,\"Label\"])  # coerce col from integer to character\n# test[u2,2] <- as.factor(activity_labels[2,\"Label\"])\n# test[u3,2] <- as.factor(activity_labels[3,\"Label\"])\n# test[u4,2] <- as.factor(activity_labels[4,\"Label\"])\n# test[u5,2] <- as.factor(activity_labels[5,\"Label\"])\n# test[u6,2] <- as.factor(activity_labels[6,\"Label\"])\n\n#sapply(final[,3:length(final)],mean)\n\n" }
{ "repo_name": "ULHPC/tutorials", "ref": "refs/heads/devel", "path": "maths/R/_targets.R", "content": "library(targets)\nlibrary(tarchetypes)\nsource(\"R/functions.R\")\noptions(tidyverse.quiet = TRUE)\n# necessary packages to load in isolated R sessions\ntar_option_set(packages = c(\"tidyverse\", \"fst\"))\n# for debugging, activate the following, a workspace will be created in\n# _targets/workspaces/failed_object. Where `failed_object` is the name of failure\n# Load with tar_workspace(failed_object)\n# when done, remove with tar_destroy(destroy = \"workspace\")\n#tar_option_set(error = \"workspace\")\n\n#library(future)\n#plan(multicore)\n\n# Define the pipeline\ntar_pipeline(\n  tar_url(gp_url, \"https://raw.githubusercontent.com/jennybc/gapminder/master/inst/extdata/gapminder.tsv\"),\n  tar_file(gp_file, download_file(gp_url, \"gapminder.tsv\")),\n  tar_fst_tbl(gp, read_tsv(gp_file, col_types = cols())),\n  tar_fst_tbl(gp_grp,\n              group_by(gp, continent, country) %>%\n                tar_group(),\n              # tell downstream targets about the grouping\n              iteration = \"group\"),\n  tar_target(models,\n             ml_lifeExp(gp_grp),\n             pattern = map(gp_grp),\n             # lm is complex, combine in a list\n             iteration = \"list\"),\n  tar_target(r2,\n             extract_r2(models),\n             pattern = map(models),\n             # now vector is enough\n             iteration = \"vector\"),\n  tar_render(report, \"report.Rmd\")\n)\n" }
{ "repo_name": "gitronald/demotables", "ref": "refs/heads/master", "path": "R/dtable.R", "content": "#' Generate descriptive frequencies and statistics tables\n#'\n#' Simplifies the process and reduces the amount of code involved in generating\n#' descriptive frequencies and statistics tables by taking your entire dataset\n#' as input and generating the tables it predicts you would need given various\n#' variable aspects such as class.\n#'\n#' @param data1 a \\code{data.frame} or \\code{matrix}\n#' @param vars select which columns of \\code{data1} to analyze\n#' @param frequencies select which columns to create frequencies tables for\n#' @param statistics select which columns to create statistics tables for\n#' @param neat logical, if \\code{TRUE} returns rounded and formatted tables\n#' @param as.list logical, if \\code{TRUE} it returns frequencies and statistics\n#'   tables in a list format, split by \\code{variables}\n#' @param sizesort logical, if \\code{TRUE} returns data sorted by frequency and\n#'   mean\n#'\n#' @return Returns descriptive frequencies and statistics tables for all\n#'   variables in \\code{data1} by default. Predicts whether to create a\n#'   frequencies table, statistics table, or both for each varible based on\n#'   information gathered using \\code{dvariable}.\n#'\n#' @export\n#'\n#' @examples\n#' # Analyze all data\n#' dtable(iris2)\n#'\n#' #Analyze two or more variables\n#' dtable(iris2, vars = c(\"Color\", \"Sold\", \"LikelyToBuy\"))\n#'\n#' # Analyze a single variable\n#' dtable(iris2, vars = \"Color\")\n#'\n#' # Return raw output\n#' dtable(iris2, neat = FALSE)\n#'\n#' # Return list output\n#' dtable(iris2, as.list = TRUE)\n#'\n#' # Frequencies sorted by size\n#' dtable(iris2, sizesort = TRUE)\n#'\ndtable <- function (data1,\n                    vars = NULL,\n                    frequencies = NULL,\n                    statistics = NULL,\n                    neat = TRUE, as.list = FALSE, sizesort = FALSE){\n\n  if(is.null(vars)) vars <- names(data1)\n  var.details  <- dvariable(data1, vars)  # Extract variable details\n\n  if(is.null(frequencies)) { # Default to dvariable prediction\n    frequencies <- paste(var.details[var.details[\"frequencies\"] == 1, \"variable\"])\n  }\n  if(is.null(statistics)) { # Default to dvariable prediction\n    statistics  <- paste(var.details[var.details[\"statistics\"] == 1, \"variable\"])\n  }\n\n  dtable <- create_list(c(\"Frequencies\", \"Statistics\"), 1)\n\n  dtable$Frequencies <- lapply(frequencies, dfactor,\n                               data1 = data1, neat = neat,\n                               sizesort = sizesort)\n\n  dtable$Statistics <- lapply(statistics, dnumeric,\n                              data = data1, neat = neat,\n                              sizesort = sizesort)\n\n  if(!as.list){\n    # Column names correction for rbind\n    dtable$Frequencies = lapply(dtable$Frequencies,\n                                function(x) {\n                                  names(x)[3] = 'group'\n                                  return(x)\n                                })\n    dtable$Frequencies <- do.call(rbind, dtable$Frequencies)\n\n    dtable$Statistics <- do.call(rbind, dtable$Statistics)\n  }\n\n  if(!is.null(dtable$Frequencies)) {\n    dtable$Frequencies$dataset <- gsub(\"data1\", deparse(substitute(data1)),\n                                       dtable$Frequencies$dataset)\n  }\n  if(!is.null(dtable$Statistics)) {\n    dtable$Statistics$dataset <- gsub(\"data1\", deparse(substitute(data1)),\n                                      dtable$Statistics$dataset)\n  }\n\n  return(dtable)\n}\n" }
{ "repo_name": "spsanderson/bio-informatics", "ref": "refs/heads/master", "path": "Lace Code/pROC_curves_ER_Score.R", "content": "lace <- read.csv(\"lace for R.csv\")\r\nlibrary(pROC)\r\n\r\nplot.roc(lace$FAILURE, lace$LACE.ER.SCORE, # data\r\n         percent=TRUE, # show all values in percent\r\n         partial.auc=c(100, 90), \r\n         partial.auc.correct=TRUE, # define a partial AUC (pAUC)\r\n         print.auc=TRUE, #display pAUC value on the plot with following options:\r\n         print.auc.pattern=\"Corrected pAUC (100-90%% SP):\\n%.1f%%\",\r\n         print.auc.col=\"#1c61b6\",\r\n         auc.polygon=TRUE, \r\n         auc.polygon.col=\"#1c61b6\", # show pAUC as a polygon\r\n         max.auc.polygon=TRUE, \r\n         max.auc.polygon.col=\"#1c61b622\", # also show the 100% polygon\r\n         main=\"Partial AUC (pAUC)\")\r\n\r\nplot.roc(lace$FAILURE, lace$LACE.ER.SCORE,\r\n         percent=TRUE, add=TRUE, \r\n         type=\"n\", # add to plot, but don't re-add the ROC itself (useless)\r\n         partial.auc=c(100, 90), \r\n         partial.auc.correct=TRUE,\r\n         partial.auc.focus=\"se\", # focus pAUC on the sensitivity\r\n         print.auc=TRUE, \r\n         print.auc.pattern=\"Corrected pAUC (100-90%% SE):\\n%.1f%%\", \r\n         print.auc.col=\"#008600\",\r\n         print.auc.y=40, # do not print auc over the previous one\r\n         auc.polygon=TRUE, \r\n         auc.polygon.col=\"#008600\",\r\n         max.auc.polygon=TRUE, \r\n         max.auc.polygon.col=\"#00860022\")\r\n\r\n##\r\n\r\nrocobj <- plot.roc(lace$FAILURE, lace$LACE.ER.SCORE,\r\n                   main=\"Confidence intervals\", \r\n                   percent=TRUE,\r\n                   ci=TRUE, # compute AUC (of AUC by default)\r\n                   print.auc=TRUE) # print the AUC (will contain the CI)\r\n\r\nciobj <- ci.se(rocobj, # CI of sensitivity\r\n               specificities=seq(0, 100, 5)) # over a select set of specificities\r\n\r\nplot(ciobj, type=\"shape\", col=\"#1c61b6AA\") # plot as a blue shape\r\n\r\nplot(ci(rocobj, of=\"thresholds\", thresholds=\"best\")) # add one threshold\r\n\r\n##\r\n\r\nrocobj <- plot.roc(lace$FAILURE, lace$LACE.ER.SCORE, \r\n                   percent = TRUE, \r\n                   main=\"Smoothing\")\r\n\r\nlines(smooth(rocobj), # smoothing (default: binormal)\r\n      col = \"#1c61b6\")\r\n\r\nlines(smooth(rocobj, method = \"density\"), # density smoothing\r\n      col = \"#008600\")\r\n\r\nlines(smooth(rocobj, method = \"fitdistr\", # fit a distribution\r\n             density = \"logistic\"), # let the distribution be logistic\r\n      col = \"#840000\")\r\n\r\nlegend(\"bottomright\", \r\n       legend = c(\"Empirical\", \"Binormal\", \"Density\", \r\n                  \"Fitdistr\\n(Logistic)\"), \r\n       col = c(\"black\", \"#1c61b6\", \"#008600\", \"#840000\")\r\n       ,lwd = 2)\r\n\r\n##\r\n\r\nrocobj <- plot.roc(lace$FAILURE, lace$LACE.ER.SCORE,\r\n                   main=\"Confidence intervals of specificity/sensitivity\", \r\n                   percent=TRUE,\r\n                   ci=TRUE, of=\"se\", # ci of sensitivity\r\n                   specificities=seq(0, 100, 5), # on a select set of specificities\r\n                   ci.type=\"shape\", ci.col=\"#1c61b6AA\") # plot the CI as a blue shape\r\n\r\nplot(ci.sp(rocobj, sensitivities=seq(0, 100, 5)), # ci of specificity\r\n     type=\"bars\") # print this one as bars\r\n\r\n##\r\n\r\nplot.roc(lace$FAILURE, lace$LACE.ER.SCORE,\r\n         main=\"Confidence interval of a threshold\", percent=TRUE,\r\n         ci=TRUE, of=\"thresholds\", # compute AUC (of threshold)\r\n         thresholds=\"best\", # select the (best) threshold\r\n         print.thres=\"best\") # also highlight this threshold on the plot\r\n\r\n##\r\n\r\nrocobj1 <- plot.roc(lace$FAILURE, lace$TOTAL.LACE,\r\n                    main=\"Statistical comparison\", \r\n                    percent=TRUE, col=\"#1c61b6\")\r\n\r\nrocobj2 <- lines.roc(lace$FAILURE, lace$LACE.ER.SCORE, \r\n                     percent=TRUE, col=\"#008600\")\r\n\r\ntestobj <- roc.test(rocobj1, rocobj2)\r\n\r\ntext(50, 50, labels=paste(\"p-value =\", \r\n                          format.pval(testobj$p.value)), adj=c(0, .5))\r\n\r\nlegend(\"bottomright\", legend=c(\"Total Lace\", \"ER Visits Only\"), \r\n       col=c(\"#1c61b6\", \"#008600\"), lwd=2)" }
{ "repo_name": "ep1804/el", "ref": "refs/heads/master", "path": "R/t2.R", "content": "\n#' T2 model and score\n#'\n#' @param data    vector, matrix, or data.frame.\n#' @param cluster list.  kmeans cluster model\n#' @param plot    logical. plot or not\n#' @param alpha   numeric. critical index\n#'\n#' @return list(fit=list(cluster, icovs, alpha, ucl), score)\n#' @export\n#'\n#' @examples el.t2(iris[,-5], el.kmeans(iris[,-5], 3)$fit)\n#' \nel.t2 <- function(data, cluster = NULL, alpha = 0.05, plot = TRUE) {\n  \n  if (is.vector(data)) {\n    if (!el.isValid(data, 'single')) return()\n    d <- data[!is.na(data)]\n    d <- as.matrix(d)\n  } else{\n    if (!el.isValid(data, 'multiple')) return()\n    d <- data[stats::complete.cases(data), ]\n  }\n  \n  if (nrow(d) < 1) {\n    logger.warn(\"There's no non-NA data.\")\n    return()\n  }\n  \n  if (is.null(cluster)) {\n    cluster <- list(k = 1, center = as.matrix(stats::kmeans(d, 1)$center, nrow = 1))\n    clusterScore <- rep(1, nrow(d))\n  } else{\n    clusterScore <- el.kmeansScore(d, cluster)\n  }\n  \n  icovs <- lapply(1:cluster$k, function(i) {\n    d1 <- d[clusterScore == i, ]\n    if (nrow(d1) <= ncol(d1)) {\n      logger.warn(\"Too small cluster: %d\", i)\n      NA\n    } else{\n      el.inv(stats::cov(d1))\n    }\n  })\n  \n  fit <- list(cluster = cluster, icovs = icovs)\n  score <- el.t2Score(data, fit, FALSE)\n  ucl <- el.limit(score, alpha)\n  fit <- c(fit, list(alpha = alpha, ucl = ucl))\n  \n  if(plot){\n    oldPar <- graphics::par(no.readonly = T)\n    plot(score, ylab='T2 score', type='l')\n    graphics::abline(h = fit$ucl, col='red')\n    graphics::par(oldPar)\n  }\n  \n  list(fit = fit, score=score)\n}\n\n\n#' T2 score given T2 model\n#'\n#' @param data    vector, matrix, or data.frame.\n#' @param fit     list(cluster, icovs, alpha, ucl). T2 model \n#' @param plot    logical. plot or not\n#'\n#' @return vector. T2 scores\n#' @export\n#'\n#' @examples el.t2Score(iris[,-5], el.t2(iris[,-5], el.kmeans(iris[,-5], 3)$fit)$fit)\n#' \nel.t2Score <- function(data, fit, plot = TRUE) {\n  \n  if(is.vector(data)){\n    if(!el.isValid(data, 'single')) return()\n    d <- as.matrix(data)\n  }else{\n    if(!el.isValid(data, 'multiple')) return()\n    d <- as.matrix(data)\n  }\n  \n  clus <- el.kmeansScore(d, fit$cluster)\n  \n  res <- sapply(1:nrow(d), function(i){\n    \n    if(sum(is.na(d[i])) > 0)\n      NA\n    else{\n      cl <- clus[i]\n      \n      if(is.na(cl) | ! is.matrix(fit$icovs[[cl]]))\n        NA\n      else{\n        v <- as.vector(d[i,]) - as.vector(fit$cluster$center[cl,])\n        v %*% fit$icovs[[cl]] %*% v\n      }\n    }\n  })\n  \n  if(plot){\n    oldPar <- graphics::par(no.readonly = T)\n    plot(res, ylab='T2 score', type='l')\n    graphics::abline(h = fit$ucl, col='red')\n    graphics::par(oldPar)\n  }\n  \n  res\n}" }
{ "repo_name": "R-Lum/Luminescence", "ref": "refs/heads/master", "path": "tests/testthat/test_convert_Activity2Concentration.R", "content": "data <- data.frame(\n    NUCLIDES = c(\"U-238\", \"Th-232\", \"K-40\"),\n    VALUE = c(40,80,100),\n    VALUE_ERROR = c(4,8,10),\n    stringsAsFactors = FALSE)\n\ntest_that(\"check class and length of output\", {\n  testthat::skip_on_cran()\n  local_edition(3)\n\n  expect_equal(is(convert_Activity2Concentration(data)), c(\"RLum.Results\", \"RLum\"))\n  expect_equal(is(convert_Activity2Concentration(data, verbose = FALSE)), c(\"RLum.Results\", \"RLum\"))\n  expect_equal(length(convert_Activity2Concentration(data)), 1)\n  expect_error(convert_Activity2Concentration())\n  expect_error(convert_Activity2Concentration(data = data.frame(a = 1, b = 2)))\n\n})\n\ntest_that(\"check values from output example\", {\n  testthat::skip_on_cran()\n  local_edition(3)\n\n  results <- convert_Activity2Concentration(data)\n  expect_equal(round(sum(results$data$`CONC. ERROR (ppm/%)`),5),  2.32815)\n\n\n})\n" }
{ "repo_name": "debarros/CSIAccountabilityWkbk", "ref": "refs/heads/master", "path": "ListExitedStudents.R", "content": "# ListExitedStudents.R\n# This script finds all of the students who have recently exited GTH \n\n# Set a cutoff for what you consider to be a recent exit\nDeadline = schoolYear(x = \"end\", y = schoolYear(\"end\") - 400) - 15               # Defaults to June 15 of prior school year\n\n\n# Narrow the data to the relevant students\nexiters = Workbook[!(VbetterComp(Workbook$Discharge.Reason, \"never attended\")),] # remove the no-show students\nexiters = exiters[exiters$`Still.Enrolled?` %in% c(\"No\", \"NO\", \"no\"),]           # Subset to just existed students\nexiters = exiters[exiters$Date.left.GTH >= Deadline,]                            # Subset to just students who left after the deadline\nexiters = exiters[!(exiters$Discharge.Reason %in% c(\"graduated\",\"Graduated\")),]  # Remove students who graduated\n\n\n# Format and create the output\nexiters = exiters[,c(\"Local.ID.(optional)\", \"Last.Name\", \"First.Name\", \"Cohort.Year.(year.1st.entered.9th)\", \n                     \"Date.First.Enrolled.at.GTH\", \"Date.left.GTH\", \"Discharge.Reason\", \"Subsequent.Placement\")]\nwrite.csv(exiters, \"RecentlyExitedStudents.csv\", row.names = F)\n" }
{ "repo_name": "sjewo/tmap", "ref": "refs/heads/master", "path": "pkg/R/lines_midpoints.R", "content": "lines_midpoints <- function (sldf) \n{\n\t#stopifnot(is.projected(sldf))\n\tLns <- slot(sldf, \"lines\")\n\thash_lns <- sapply(Lns, function(x) length(slot(x, \"Lines\")))\n\tN <- sum(hash_lns)\n\tmidpoints <- matrix(NA, ncol = 2, nrow = N)\n\tInd <- integer(length = N)\n\tii <- 1\n\tfor (i in 1:length(Lns)) {\n\t\tLnsi <- slot(Lns[[i]], \"Lines\")\n\t\tfor (j in 1:hash_lns[i]) {\n\t\t\tInd[ii] <- i\n\t\t\tmidpoints[ii, ] <- get_midpoint(slot(Lnsi[[j]], \"coords\"))\n\t\t\tii <- ii + 1\n\t\t}\n\t}\n\tif (is(sldf, \"SpatialLinesDataFrame\")) {\n\t\tdf0 <- slot(sldf, \"data\")[Ind, ]\n\t\tdf <- as.data.frame(cbind(df0, Ind))\n\t}\n\telse df <- data.frame(Ind = Ind)\n\tspdf <- SpatialPointsDataFrame(midpoints, data = df, proj4string = CRS(proj4string(sldf)))\n\treturn(spdf)\n}\n\nget_midpoint <- function (coords) {\n\tdist <- sqrt((diff(coords[, 1])^2 + (diff(coords[, 2]))^2))\n\tdist_mid <- sum(dist)/2\n\tdist_cum <- c(0, cumsum(dist))\n\tend_index <- which(dist_cum > dist_mid)[1]\n\tstart_index <- end_index - 1\n\tstart <- coords[start_index, ]\n\tend <- coords[end_index, ]\n\tdist_remaining <- dist_mid - dist_cum[start_index]\n\tstart + (end - start) * (dist_remaining/dist[start_index])\n}" }
{ "repo_name": "EccRiley/Riley", "ref": "refs/heads/master", "path": "R/RisBinary.R", "content": "RisBinary <- function(x) {\n    ifelse(base::setequal(unique(na.omit(x)), c(0, 1)),\n           TRUE, FALSE)\n}\n" }
{ "repo_name": "katrinleinweber/datasciencecoursera", "ref": "refs/heads/master", "path": "DDP_Shiny_Joel/ui.R", "content": "library(shiny)\n\nshinyUI(fluidPage(\n  \n  titlePanel(\"Shiny Joel Test\"),\n  \n  verticalLayout(fluid = TRUE,\n    \n    checkboxGroupInput(\n      \"questions\", label = \"Do they/you… (Check all that apply)\", \n      choices = c(\"use source control?\" = \"control\",\n                  \"build in one step?\" = \"build\",\n                  \"build daily?\" = \"daily\",\n                  \"have bug database?\" = \"tracker\",\n                  \"fix bugs before writing new code?\" = \"fix\",\n                  \"have up-to-date schedule?\" = \"schedule\",\n                  \"have spec?\" = \"spec\",\n                  \"provide quiet working conditions?\" = \"quiet\",\n                  \"use best tools money can buy?\" = \"tools\",\n                  \"have testers?\" = \"testers\",\n                  \"write code during candidate interview?\" = \"interview\",\n                  \"use hallway usability testing?\"= \"hallway\")\n    ),\n    \n    h3(textOutput(\"points\", inline = TRUE), \" points out of 12\"),\n    p(\"Source & further reading: \", a(\"JoelOnSoftware.com/2000/08/09\",\n      href = \"https://www.joelonsoftware.com/2000/08/09/the-joel-test-12-steps-to-better-code/\"\n    ))\n    \n  )\n))\n" }
{ "repo_name": "cran/exactmaxsel", "ref": "refs/heads/master", "path": "R/maxsel.R", "content": "###     Computation of the maximally selected statistics from data\r\n###\r\n### Copyright 2006-09 Anne-Laure Boulesteix \r\n###\r\n### \r\n###\r\n###\r\n### This file is part of the `exactmaxstat' library for R and related languages.\r\n### It is made available under the terms of the GNU General Public\r\n### License, version 2, or at your option, any later version,\r\n### incorporated herein by reference.\r\n### \r\n### This program is distributed in the hope that it will be\r\n### useful, but WITHOUT ANY WARRANTY; without even the implied\r\n### warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR\r\n### PURPOSE.  See the GNU General Public License for more\r\n### details.\r\n### \r\n### You should have received a copy of the GNU General Public\r\n### License along with this program; if not, write to the Free\r\n### Software Foundation, Inc., 59 Temple Place - Suite 330, Boston,\r\n### MA 02111-1307, USA\r\n\r\n\r\nmaxsel<-function(x,y=NULL,type,statistic)\r\n{\r\n \r\nstatvector<-c()\r\n\r\n \r\nif (!is.null(y))\r\n {\r\n notNA<-which(!is.na(x)&!is.na(y))\r\n x<-x[notNA]\r\n y<-y[notNA]\r\n levelsx<-sort(union(x,x))\r\n K<-length(levelsx)\r\n n0<-sum(y==0)\r\n n1<-length(y)-n0\r\n\r\n \r\n if (type==\"ord\")\r\n  {\r\n  for (k in 1:(K-1))\r\n   {\r\n   c1<-sum(y==0&(x<=levelsx[k]))\r\n   c2<-sum(y==1&(x<=levelsx[k]))\r\n   c3<-n0-c1\r\n   c4<-n1-c2\r\n   if (statistic==\"chi2\")\r\n    {\r\n    statvector<-c(statvector,chisq.test(matrix(c(c1,c2,c3,c4),2,2),correct=FALSE)$statistic)\r\n    }\r\n   if (statistic==\"gini\")\r\n    {\r\n    statvector<-c(statvector,ginigain(matrix(c(c1,c2,c3,c4),2,2)))\r\n    }   \r\n   }\r\n  }\r\n\r\n if (type==\"ord2\")\r\n  {\r\n  for (k in 1:(K-1))\r\n   {\r\n   for (j in (k+1):K)\r\n    {\r\n    c1<-sum(y==0&(x>levelsx[k])&(x<=levelsx[j]))\r\n    c2<-sum(y==1&(x>levelsx[k])&(x<=levelsx[j]))\r\n    c3<-n0-c1\r\n    c4<-n1-c2\r\n    if (statistic==\"chi2\")\r\n     {\r\n     statvector<-c(statvector,chisq.test(matrix(c(c1,c2,c3,c4),2,2),correct=FALSE)$statistic)\r\n     }\r\n    if (statistic==\"gini\")\r\n     {\r\n     statvector<-c(statvector,ginigain(matrix(c(c1,c2,c3,c4),2,2)))\r\n     }\r\n    }   \r\n   }\r\n  }\r\n\r\n   \r\n\r\n if (type==\"cat\")\r\n  {\r\n  xfac<-factor(x,labels=1:K)\r\n  x<-as.numeric(xfac)\r\n  prop2<-as.numeric(tapply(xfac[y==1],xfac[y==1],length))/as.numeric(tapply(xfac,xfac,length))\r\n \r\n  for (k in 1:(K-1))\r\n   {\r\n   left<-is.element(x,order(-prop2)[1:k])\r\n   c1<-sum(y==0&left)\r\n   c2<-sum(y==1&left)\r\n   c3<-n0-c1\r\n   c4<-n1-c2\r\n   if (statistic==\"chi2\")\r\n    {\r\n    statvector<-c(statvector,chisq.test(matrix(c(c1,c2,c3,c4),2,2),correct=FALSE)$statistic)\r\n    }\r\n   if (statistic==\"gini\")\r\n    {\r\n    statvector<-c(statvector,ginigain(matrix(c(c1,c2,c3,c4),2,2)))\r\n    }   \r\n   }\r\n  } \r\n }\r\n\r\n\r\n\r\nif (is.null(y)&ncol(as.matrix(x))>1)\r\n {\r\n if (!is.numeric(x))\r\n  stop(\"x must be given as a numeric vector\") \r\n if (nrow(x)!=2)\r\n  stop(\"x must have 2 rows and K columns\")\r\n \r\n K<-ncol(x)\r\n zero<-c()\r\n for (k in 1:K)\r\n  {\r\n  if (sum(x[,k])==0)\r\n   {\r\n   zero<-c(zero,k)\r\n   }\r\n  }\r\n if (length(zero)>0) \r\n  {\r\n  x<-x[,-zero]\r\n  }\r\n K<-ncol(x)\r\n n0<-sum(x[1,])\r\n n1<-sum(x[2,])\r\n\r\n if (K<2)\r\n  stop(\"x must have at least 2 columns\")\r\n if (K<3&(type==\"cat\"))\r\n  {\r\n  type<-\"ord\"\r\n  }\r\n \r\n if (type==\"ord\")\r\n  {\r\n  for (k in 1:(K-1))\r\n   {\r\n   c1<-sum(x[1,1:k])\r\n   c2<-sum(x[2,1:k])\r\n   c3<-n0-c1\r\n   c4<-n1-c2\r\n   if (statistic==\"chi2\")\r\n    {\r\n    statvector<-c(statvector,chisq.test(matrix(c(c1,c2,c3,c4),2,2),correct=FALSE)$statistic)\r\n    }\r\n   if (statistic==\"gini\")\r\n    {\r\n    statvector<-c(statvector,ginigain(matrix(c(c1,c2,c3,c4),2,2)))\r\n    }   \r\n   }\r\n  }\r\n  \r\n\r\nif (type==\"ord2\")\r\n  {\r\n  for (k in 1:(K-1))\r\n   {\r\n   for (j in (k+1):K)\r\n    {\r\n    c1<-sum(x[1,(k+1):j])\r\n    c2<-sum(x[2,(k+1):j])\r\n    c3<-n0-c1\r\n    c4<-n1-c2\r\n    if (statistic==\"chi2\")\r\n     {\r\n     statvector<-c(statvector,chisq.test(matrix(c(c1,c2,c3,c4),2,2),correct=FALSE)$statistic)\r\n     }\r\n    if (statistic==\"gini\")\r\n     {\r\n     statvector<-c(statvector,ginigain(matrix(c(c1,c2,c3,c4),2,2)))\r\n     }\r\n    }   \r\n   }\r\n  }\r\n\r\n \r\n  \r\n   \r\n  \r\n if (type==\"cat\")\r\n  {\r\n  prop2<-x[2,]/apply(x,FUN=sum,MARGIN=2)\r\n  x<-x[,order(-prop2)] \r\n  for (k in 1:(K-1))\r\n   {\r\n   c1<-sum(x[1,1:k])\r\n   c2<-sum(x[2,1:k])\r\n   c3<-n0-c1\r\n   c4<-n1-c2\r\n   if (statistic==\"chi2\")\r\n    {\r\n    statvector<-c(statvector,chisq.test(matrix(c(c1,c2,c3,c4),2,2),correct=FALSE)$statistic)\r\n    }\r\n   if (statistic==\"gini\")\r\n    {\r\n    statvector<-c(statvector,ginigain(matrix(c(c1,c2,c3,c4),2,2)))\r\n    }    \r\n   }\r\n  }  \r\n }\r\n \r\n    \r\nmaxsel<-max(as.numeric(statvector))\r\nreturn(maxsel)\r\n\r\n}\r\n\r\n\r\n\r\n\r\n\r\n" }
{ "repo_name": "sf585978/speedR", "ref": "refs/heads/master", "path": "R/getCourseCorrection_nls.R", "content": "#' Get Course Correction Parameter using Nonlinear Regression of Time on Expected Values\n#' \n#' This function estimates and returns the course correction parameter, gamma, for courses that are not the base course.\n#' @param race The numeric ID of the race you want to estimate the correction for\n#' @return gammaFit The best fit estimate for the course correction parameter, gamma.\n#' @keywords speed rating, cross country, handicapping\n#' @export\n#' @examples \n#' getCourseCorrection(race = \"mWilliams15\", results, referenceRunners, guess, baseID = \"mGeneseo15\")\n\ngetCourseCorrection_nls <- function(race,\n                                results,\n                                referenceRunners,\n                                guess,\n                                alpha = 4.4, \n                                beta = 2355, \n                                baseID = \"mGeneseo15\") {\n  courseCorrections <- numeric(length(race))\n  require(readr)\n  require(dplyr)\n  for(i in 1:length(race)) {\n    if (race[i] == baseID) {\n      message(\"Race is same as base race.\")\n      courseCorrections[i] <- 0\n    } else {\n      results2 <- suppressWarnings(results %>%\n                                     filter(raceID == race[i]) %>%\n                                     filter(seconds > quantile(seconds, \n                                                               0.03)) %>%\n                                     filter(seconds < quantile(seconds,\n                                                               0.95)) %>%\n                                     inner_join(referenceRunners,\n                                                by = c(\"name\", \"school\")))\n      if(nrow(results2) == 0) {\n        message(paste(\"No reference runners found for \", race[i], \".\", sep =\"\"))\n        courseCorrections[i] <- NA\n        next()\n      }\n      x <- results2$seconds\n      y <- results2$refSR\n      weights <- 1 - (results2$place/400)\n      tt <- try(\n        nls(y ~ SR_CourseCorrection(x, \n                                    alpha,\n                                    beta,\n                                    gamma),\n            weights = weights,\n            start = list(gamma = guess), \n            control = (maxiter = 500))\n      )\n      if (is(tt, \"try-error\")) {\n        message(paste(\"There was a problem with estimating the course correction for \",\n                      race[i], \".\", sep = \"\"))\n        courseCorrections[i] <- NA\n        next()\n      } else {\n        gammaFit <- nls(y ~ SR_CourseCorrection(x, \n                                                alpha,\n                                                beta,\n                                                gamma), \n                        weights = weights,\n                        start = list(gamma = guess), \n                        control = (maxiter = 500))\n      }\n      plot(x, y, main = race[i])\n      curve(SR_CourseCorrection(x, alpha, beta, gammaFit$m$getPars()), \n            add = TRUE,\n            col = \"red\")\n      courseCorrections[i] <- gammaFit$m$getPars()\n    }\n  }\n  courseCorrections <- data.frame(raceID = race, gamma = courseCorrections)\n  courseCorrections$raceID <- as.character(courseCorrections$raceID)\n  return(courseCorrections)\n}\n" }
{ "repo_name": "InnovationValueInitiative/IVI-RA", "ref": "refs/heads/master", "path": "data-raw/nma.R", "content": "rm(list = ls())\nlibrary(\"iviRA\")\nlibrary(\"data.table\")\nlibrary(\"MCMCpack\")\nsource(\"func.R\")\ntreatments <- fread(\"treatments.csv\")\nntreats <- nrow(treatments)\n\n# LOAD DATA --------------------------------------------------------------------\nnma.acr.naive.coda <- fread(\"nma-acr-naive-re-coda.csv\")\nnma.acr.naive.crosswalk <- fread(\"nma-acr-naive-re-crosswalk.csv\")\nnma.haq.naive.coda <- fread(\"nma-haq-naive-re-coda.csv\")\nnma.haq.naive.crosswalk <- fread(\"nma-haq-naive-re-crosswalk.csv\")\nnma.das28.naive.coda <- fread(\"nma-das28-naive-re-coda.csv\")\nnma.das28.naive.crosswalk <- fread(\"nma-das28-naive-re-crosswalk.csv\")\n\n# ACR RESPONSE PROBABILITIES ---------------------------------------------------\n#### NMA FROM NICE\n### bio naive\n## table 37 Stevenson (also assuming RTX is equal to \n# i.v. ABT per p. 248 in NICE report)\npo <- vector(mode = \"list\", ntreats)\nnames(po) <- treatments$sname \npo[[\"cdmards\"]] <- c(0.298, 0.123, 0.042)\npo[[\"abtivmtx\"]] <- c(0.573, 0.328, 0.156)\npo[[\"adamtx\"]] <- c(0.615, 0.368, 0.183)\npo[[\"ada\"]] <- c(0.499, 0.264, 0.115)\npo[[\"triple\"]] <- c(0.503, 0.266, 0.117) #previously labeled as tt\npo[[\"etnmtx\"]] <- c(0.713, 0.472, 0.263)\npo[[\"etn\"]] <- c(0.645, 0.398, 0.205)\npo[[\"golmtx\"]] <- c(0.642, 0.395, 0.202)\npo[[\"ifxmtx\"]] <- c(0.595, 0.348, 0.169)\npo[[\"placebo\"]] <- c(0.175, 0.059, 0.016)\npo[[\"tczmtx\"]] <- c(0.706, 0.464, 0.256)\npo[[\"tcz\"]] <- c(0.717, 0.477, 0.266)\npo[[\"czpmtx\"]] <- c(0.564, 0.319, 0.150)\npo[[\"abtscmtx\"]] <- c(0.638, 0.391, 0.199)\npo[[\"nbt\"]] <- c(0, 0, 0)\npo[[\"rtxmtx\"]] <- c(0.573, 0.328, 0.156) # assumed to be same as abt iv\npo[[\"tofmtx\"]] <- c(NA, NA, NA)\npo[[\"rtx\"]] <- c(NA, NA, NA)\npo[[\"tof\"]] <- c(NA, NA, NA)\npo[[\"czp\"]] <- c(NA, NA, NA)\npo[[\"gol\"]] <- c(NA, NA, NA)\npo[[\"adabiosbwwdmtx\"]]<- c(NA, NA, NA)\npo[[\"anamtx\"]]<- c(NA, NA, NA)\npo[[\"bct\"]]<- c(NA, NA, NA)\npo[[\"bctmtx\"]]<- c(NA, NA, NA)\npo[[\"etnbiosszzsmtx\"]]<- c(NA, NA, NA)\npo[[\"etnbiosykromtx\"]]<- c(NA, NA, NA)\npo[[\"sar\"]]<- c(NA, NA, NA)\npo[[\"sarmtx\"]]<- c(NA, NA, NA)\npo[[\"upamtx\"]]<- c(NA, NA, NA)\npo[[\"ifxbiosqbtxmtx\"]]<- c(NA, NA, NA)\npo <- do.call(\"rbind\", po)\npo <- cbind(1 - po[, 1], po)\npo <- po[match(treatments$sname, rownames(po)), ]\n\n## probabilities in mutually exclusive categories\np <- matrix(NA, nrow = ntreats, ncol = 4)\nrownames(p) <- rownames(po)\np[, 1] <- 1 - po[, 2] \np[, 2] <- po[, 2] - po[, 3]\np[, 3] <- po[, 3] - po[, 4]\np[, 4] <- po[, 4]\n\n## Guess Parameters of Dirichlet Distribution\n# simulate cDMARDs\nnsims <- 1000\nnobs <- 500\nacr.probs <- MCMCpack::rdirichlet(1000, nobs * p[which(rownames(p) == \"cdmards\"), ])\nacr.probs2 <- matrix(NA, nrow = nsims, ncol = 3)\nacr.probs2[, 1] <- 1 - acr.probs[, 1]\nacr.probs2[, 2] <- acr.probs[, 3] + acr.probs[, 4]\nacr.probs2[, 3] <- acr.probs[, 4]\n\n# check 95% credible interval for cDMARds \n# In Steveson, ACR 20 = .298 (.255-.344), ACR 50 = .123 (.098-.1530),\n# ACR 70 = .042 (.031-.056)\napply(acr.probs2, 2, quantile, c(.025, .975))\n\n## Store Results\nnma.acr.naive.nice <- list(p = p, p.overlap = po, nobs = nobs)\n\n#### IVI NMA\n### bio naive\n## missing trials\nnma.acr.naive.coda <- nma_add_missing(nma.acr.naive.coda, nma.acr.naive.crosswalk)\nnma.acr.naive.crosswalk[, coda_num := paste0(\"d[\", num, \"]\")]\nnma.acr.naive.crosswalk <- nma.acr.naive.crosswalk[sname != \"\"]\n\n# match nma therapies with model therapies\nnma.acr.naive.crosswalk <- dt_reorder(nma.acr.naive.crosswalk, \"sname\",\n                                      treatments$sname)\n\n# reorder coda \ncoda.num <- nma.acr.naive.crosswalk$coda_num\nnma.acr.naive.coda <- nma.acr.naive.coda[, c(\"A\", \"z[1]\", \"z[2]\", \"z[3]\", coda.num), with = FALSE]\nsetnames(nma.acr.naive.coda, colnames(nma.acr.naive.coda),\n         c(\"A\", \"z1\", \"z2\", \"z3\", paste0(\"d_\", nma.acr.naive.crosswalk$sname)))\nnma.acr.naive <- list(mean = apply(nma.acr.naive.coda, 2, mean),\n                      vcov = cov(nma.acr.naive.coda))\n\n# CHANGE IN HAQ ----------------------------------------------------------------\n#### IVI NMA\n### bio naive\n## missing trials\nnma.haq.naive.coda <- nma_add_missing(nma.haq.naive.coda, nma.haq.naive.crosswalk)\nnma.haq.naive.crosswalk[, coda_num := paste0(\"d[\", num, \"]\")]\nnma.haq.naive.crosswalk <- nma.haq.naive.crosswalk[sname != \"\"]\n\n# match nma therapies with model therapies\nnma.haq.naive.crosswalk <- dt_reorder(nma.haq.naive.crosswalk, \"sname\",\n                                      treatments$sname)\n\n# reorder coda\ncoda.num <- nma.haq.naive.crosswalk$coda_num\nnma.haq.naive.coda <- nma.haq.naive.coda[, c(\"A\", coda.num), with = FALSE]\nsetnames(nma.haq.naive.coda, colnames(nma.haq.naive.coda),\n         c(\"A\", paste0(\"d_\", nma.haq.naive.crosswalk$sname)))\nnma.haq.naive <- list(mean = apply(nma.haq.naive.coda, 2, mean),\n                      vcov = cov(nma.haq.naive.coda))\n\n# CHANGE IN DAS28 --------------------------------------------------------------\n### bio naive\n## missing trials\nnma.das28.naive.coda <- nma_add_missing(nma.das28.naive.coda, nma.das28.naive.crosswalk)\nnma.das28.naive.crosswalk[, coda_num := paste0(\"d[\", num, \"]\")]\nnma.das28.naive.crosswalk <- nma.das28.naive.crosswalk[sname != \"\"]\n\n# match nma therapies with model therapies\nnma.das28.naive.crosswalk <- dt_reorder(nma.das28.naive.crosswalk, \"sname\",\n                                      treatments$sname)\n\n# reorder coda\ncoda.num <- nma.das28.naive.crosswalk$coda_num\nnma.das28.naive.coda <- nma.das28.naive.coda[, c(\"A\", coda.num), with = FALSE]\nsetnames(nma.das28.naive.coda, colnames(nma.das28.naive.coda),\n         c(\"A\", paste0(\"d_\", nma.das28.naive.crosswalk$sname)))\nnma.das28.naive <- list(mean = apply(nma.das28.naive.coda, 2, mean),\n                      vcov = cov(nma.das28.naive.coda))\n\n# SAVE PARAMETERS --------------------------------------------------------------\n# save\nsave(nma.acr.naive, file = \"../data/nma-acr-naive.rda\", compress = \"bzip2\")\nsave(nma.acr.naive.nice, file = \"../data/nma-acr-naive-nice.rda\", compress = \"bzip2\")\nsave(nma.das28.naive, file = \"../data/nma-das28-naive.rda\", compress = \"bzip2\")\nsave(nma.haq.naive, file = \"../data/nma-haq-naive.rda\", compress = \"bzip2\")\n" }
{ "repo_name": "timyates/EnsemblRest", "ref": "refs/heads/master", "path": "R/Mapping.R", "content": "# Mapping Calls\n\nmapping = function( asm_one, region, asm_two, species ) {\n  lapply( .load.and.parse( .Ensembl$mapping, c( species=species, asm_one=asm_one, region=region, asm_two=asm_two ) )$mappings, function( x ) {\n    as( as( data.frame( space=c( x$original$seq_region_name, x$mapped$seq_region_name ),\n                        start=c( as.numeric( x$original$start ), as.numeric( x$mapped$start ) ),\n                        end=c( as.numeric( x$original$end ), as.numeric( x$mapped$end ) ),\n                        strand=c( .strandString( x$original$strand ), .strandString( x$mapped$strand ) ),\n                        assembly=c( x$original$assembly, x$mapped$assembly ),\n                        coordinate_system=c( x$original$coordinate_system, x$mapped$coordinate_system ),\n                        type=c( 'original', 'mapped' ),\n                        stringsAsFactors=F ), 'RangedData' ), 'GRanges' )\n  } )\n}\n\nmappingCdna = function( id, region, species=NULL, object=c( NA, 'gene', 'transcript' ) ) {\n  object = match.arg( object )\n  params = c()\n  if( !is.na( object ) ) params = c( params, .make.params( object=object ) )\n  if( !is.null( species ) ) params = c( params, .make.params( species=species ) )\n  as( as( do.call( 'rbind', lapply( .load.and.parse( .Ensembl$mapping.cdna, c( id=id, region=region ), params )$mappings, function( e ) {\n    data.frame( space=e$seq_region_name, start=e$start, end=e$end, strand=.strandString( e$strand ), gap=e$gap, rank=e$rank )\n  } ) ), 'RangedData' ), 'GRanges' )\n}\n\nmappingCds = function( id, region, species=NULL, object=c( NA, 'gene', 'transcript' ) ) {\n  object = match.arg( object )\n  params = c()\n  if( !is.na( object ) ) params = c( params, .make.params( object=object ) )\n  if( !is.null( species ) ) params = c( params, .make.params( species=species ) )\n  as( as( do.call( 'rbind', lapply( .load.and.parse( .Ensembl$mapping.cds, c( id=id, region=region ), params )$mappings, function( e ) {\n    data.frame( space=e$seq_region_name, start=e$start, end=e$end, strand=.strandString( e$strand ), gap=e$gap, rank=e$rank )\n  } ) ), 'RangedData' ), 'GRanges' )\n}\n\nmappingTranslation = function( id, region, species=NULL, object=c( NA, 'gene', 'transcript' ) ) {\n  object = match.arg( object )\n  params = c()\n  if( !is.na( object ) ) params = c( params, .make.params( object=object ) )\n  if( !is.null( species ) ) params = c( params, .make.params( species=species ) )\n  as( as( do.call( 'rbind', lapply( .load.and.parse( .Ensembl$mapping.translation, c( id=id, region=region ), params )$mappings, function( e ) {\n    data.frame( space=e$seq_region_name, start=e$start, end=e$end, strand=.strandString( e$strand ), gap=e$gap, rank=e$rank )\n  } ) ), 'RangedData' ), 'GRanges' )\n}\n" }
{ "repo_name": "maressyl/R.Rgb", "ref": "refs/heads/master", "path": "Rgb/R/draw.hist.R", "content": "# Draws an histogram with a bar for each row of slice\r\n# Author : Sylvain Mareschal <maressyl@gmail.com>\r\n# License : GPL3 http://www.gnu.org/licenses/gpl.html\r\n\r\ndraw.hist = function(\r\n\t\tslice,\r\n\t\tstart,\r\n\t\tend,\r\n\t\tcolumn = \"value\",\r\n\t\tfillColor = \"#666666\",\r\n\t\tborder = \"#666666\",\r\n\t\tcex.lab = 1,\r\n\t\torigin = 0,\r\n\t\tbty = \"o\",\r\n\t\tfg = \"#000000\",\r\n\t\tylim = NA,\r\n\t\t...\r\n\t) {\r\n\t# Coercions\r\n\tif(is.numeric(start)) start <- as.integer(start)\r\n\tif(is.numeric(end))   end <- as.integer(end)\r\n\t\r\n\t# Checks\r\n\tif(!is.integer(start))         stop(\"'start' must be integer or numeric\")\r\n\tif(!is.integer(end))           stop(\"'end' must be integer or numeric\")\r\n\tif(!is.data.frame(slice))      stop(\"'slice' must be a data.frame\")\r\n\tif(!\"start\" %in% names(slice)) stop(\"'slice' needs a 'start' column\")\r\n\tif(!\"end\" %in% names(slice))   stop(\"'slice' needs a 'end' column\")\r\n\tif(!column %in% names(slice))  stop(\"'column' can not be found in 'slice'\")\r\n\t\r\n\t# Automatic ylim\r\n\tif(!is.numeric(ylim) || length(ylim) != 2L || all(is.na(ylim))) {\r\n\t\t# Both boundaries to be guessed\r\n\t\tif(any(!is.na(slice[[column]]))) { ylim <- range(slice[[column]], na.rm=TRUE)\r\n\t\t} else                           { ylim <- c(0L, 1L)\r\n\t\t}\r\n\t} else if(is.na(ylim[1])) {\r\n\t\t# Bottom only\r\n\t\tif(any(!is.na(slice[[column]]))) { ylim[1] <- min(slice[[column]], na.rm=TRUE)\r\n\t\t} else                           { ylim[1] <- 0L\r\n\t\t}\r\n\t} else if(is.na(ylim[2])) {\r\n\t\t# Top only\r\n\t\tif(any(!is.na(slice[[column]]))) { ylim[2] <- max(slice[[column]], na.rm=TRUE)\r\n\t\t} else                           { ylim[2] <- 0L\r\n\t\t}\r\n\t}\r\n\t\r\n\t# Background\r\n\tdraw.bg(\r\n\t\tstart = start,\r\n\t\tend = end,\r\n\t\tcex.lab = cex.lab,\r\n\t\tbty = bty,\r\n\t\tfg = fg,\r\n\t\tylim = ylim,\r\n\t\t...\r\n\t)\r\n\t\r\n\tif(nrow(slice) > 0) {\r\n\t\t# Draw high boxes behind\r\n\t\tslice <- slice[ order(slice[[column]], decreasing=TRUE) ,]\r\n\t\t\r\n\t\t# Box filling\r\n\t\tif(is.function(fillColor)) {\r\n\t\t\tenvironment(fillColor) <- environment()\r\n\t\t\tfillColor <- fillColor()\r\n\t\t}\r\n\t\t\r\n\t\t# Box border\r\n\t\tif(is.function(border)) {\r\n\t\t\tenvironment(border) <- environment()\r\n\t\t\tborder <- border()\r\n\t\t} else if(identical(border, \"fillColor\")) {\r\n\t\t\tborder <- fillColor\r\n\t\t}\r\n\t\t\r\n\t\t# Boxes\r\n\t\tgraphics::rect(\r\n\t\t\txleft = slice$start,\r\n\t\t\txright = slice$end,\r\n\t\t\tytop = slice[[column]],\r\n\t\t\tybottom = if(is.numeric(origin)) { origin } else { slice[[origin]] },\r\n\t\t\tcol = fillColor,\r\n\t\t\tborder = border\r\n\t\t)\r\n\t} else {\r\n\t\t# No box (not enough)\r\n\t\tgraphics::text(\r\n\t\t\tx = mean(graphics::par(\"usr\")[1:2]),\r\n\t\t\ty = mean(graphics::par(\"usr\")[3:4]),\r\n\t\t\tlabel = paste(nrow(slice), \"element(s) in this range\"),\r\n\t\t\tcol = fg,\r\n\t\t\tadj = c(0.5, 0.5),\r\n\t\t\tcex = cex.lab\r\n\t\t)\r\n\t}\r\n\t\r\n\t# Surrounding box\r\n\tgraphics::box(\r\n\t\twhich = \"plot\",\r\n\t\tcol = fg,\r\n\t\tbty = bty\r\n\t)\r\n}\r\n" }
{ "repo_name": "yuxiangtan/QueryFuse", "ref": "refs/heads/master", "path": "QueryFuse_v1/KS_test_for_split_pval.R", "content": "#/share/apps/R/R-2.12.2_gnu412_x86_64_vanilla/bin/Rscript KS_test_for_split_pval.R in_string=\"matrix of barplot table\"\r\n\r\n#   Copyright {2015} Yuxiang Tan\r\n#\r\n#   Licensed under the Apache License, Version 2.0 (the \"License\");\r\n#   you may not use this file except in compliance with the License.\r\n#   You may obtain a copy of the License at\r\n#\r\n#       http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n#   Unless required by applicable law or agreed to in writing, software\r\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\r\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n#   See the License for the specific language governing permissions and\r\n#   limitations under the License.\r\n\r\n\r\n#check arguments\r\nfor (e in commandArgs()) {\r\n        ta = strsplit(e,\"=\",fixed=TRUE)\r\n        if(! is.na(ta[[1]][2])) {\r\n                temp = ta[[1]][2]\r\n                if(substr(ta[[1]][1],nchar(ta[[1]][1]),nchar(ta[[1]][1])) == \"I\") {\r\n                temp = as.integer(temp)\r\n                }\r\n        if(substr(ta[[1]][1],nchar(ta[[1]][1]),nchar(ta[[1]][1])) == \"N\") {\r\n                temp = as.numeric(temp)\r\n                }\r\n        assign(ta[[1]][1],temp)\r\n        } else {\r\n        assign(ta[[1]][1],TRUE)\r\n        }\r\n}\r\n\r\nin_dist=as.numeric(strsplit(in_string,\"_\")[[1]])\r\nlen_dist=length(in_dist)\r\ntemp_dist=rep(mean(in_dist)/len_dist,len_dist)\r\n\r\nout_val=ks.test(in_dist,temp_dist)$p.value\r\n\r\nprint(out_val)" }
{ "repo_name": "nickmckay/LiPD-utilities", "ref": "refs/heads/master", "path": "R/R/treeTs.R", "content": "#' Lump metadata variables with other rows in Ts (treeTS)\n#'\n#' @param Ts \n#' @param sc \n#'\n#' @return A lumped Ts\n#' @export\n#'\nlumpTsMetaVariables <- function(Ts,sc=c(\"eps\",\"rbar\",\"ncores\")){\n\n#what variables will be used for id\ndsn <- sapply(Ts,\"[[\",\"dataSetName\")\ntN <- sapply(Ts,\"[[\",\"tableNumber\")\ntT <- sapply(Ts,\"[[\",\"tableType\")\nmN <- sapply(Ts,\"[[\",\"modelNumber\") #NPM: going to need to be smarter about this for when it's a mix of model and no model\nif(is.list(mN)){#then at least some don't have modelNumbers... ignore for now.\n  hasModel <- FALSE\n}else{\n  hasModel <- TRUE\n}\n#get a paleo/chron number\nif(Ts[[1]]$mode==\"paleo\"){\n  pN <- sapply(Ts,\"[[\",\"paleoNumber\")\n}else{\n  pN <- sapply(Ts,\"[[\",\"chronNumber\")\n}\n\n\nvarNames <- sapply(Ts,\"[[\",\"paleoData_variableName\")\n\nfor(v in 1:length(sc)){\n  ind <- which(varNames == sc[v])\n  for(i in 1:length(ind)){\n    this <- Ts[[ind[i]]]\n    \n    #get identifying information...\n    if(hasModel){\n    wtp <- which(this$dataSetName == dsn & \n                   this$tableNumber == tN &\n                   this$tableType == tT &\n                   this$paleoNumber == pN &\n                   this$modelNumber == mN)\n    }else{\n      wtp <- which(this$dataSetName == dsn & \n                     this$tableNumber == tN &\n                     this$tableType == tT &\n                     this$paleoNumber == pN )\n    }\n    \n    for(w in wtp){#loop through the matching rows\n      Ts[[w]][[sc[v]]] <- this$paleoData_values #assign in the values\n      Ts[[w]][[paste0(sc[v],\"-TSid\")]]<- this$paleoData_TSid #assign in the TSid\n      Ts[[w]][[paste0(sc[v],\"-units\")]]<- this$paleoData_units #assign in the TSid\n    }\n  }\n}\n\n#delete the original rows\nreturn(Ts[-which(varNames %in% sc)])\n}\n\n\n" }
{ "repo_name": "kaliczp/Thorntwaite2", "ref": "refs/heads/master", "path": "testettest.R", "content": "et.test(200, test.meteo.xts$t, test.meteo.xts$P, PET.proj, test.meteo.xts$ET-CREMAP)\n\nSOIL.MAX <- optimize(et.test, interval=c(100,10000), temp = test.meteo.xts$t, prec=test.meteo.xts$P, pet.real=PET.proj, cremap=test.meteo.xts$ET-CREMAP)$minimum\n\n## Visual scanning SOIL.MAX parameter\ntempsoilm <- seq(100,1000,50)\ntempsoilm.df <- data.frame(soilmax=tempsoilm, RSS=numeric(length(tempsoilm)))\nfor(tti in 1:nrow(tempsoilm.df)){\n  tempsoilm.df[tti,\"RSS\"] <- et.test(tempsoilm.df[tti,\"soilmax\"],test.meteo.xts$t, test.meteo.xts$P, PET.proj, test.meteo.xts$ET.CREMAP)\n}\nplot(tempsoilm.df, type=\"p\", pch=\".\", xlab=\"SOIL_MAX\", ylab=\"RSS\")\n\net.calc(SOIL.MAX,Temp = test.meteo.xts$t, Prec=test.meteo.xts$P, PET.real=PET.proj)\n" }
{ "repo_name": "abarbour/stress", "ref": "refs/heads/master", "path": "R/RcppExports.R", "content": "# Generated by using Rcpp::compileAttributes() -> do not edit by hand\n# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393\n\nrcpp_hello <- function() {\n    .Call('_stress_rcpp_hello', PACKAGE = 'stress')\n}\n\n" }
{ "repo_name": "WMBEdmands/CompMS2miner", "ref": "refs/heads/master", "path": "R/dynamicNoiseFilter.R", "content": "#' Dynamic Noise filtration \n#' \n#' @param spectrum.df a dataframe or matrix with two columns:\n#' 1. Mass/ Mass-to-charge ratio\n#' 2. Intensity\n#' @param DNF dynamic noise filter minimum signal to noise threshold \n#' (default = 2), calculated as the ratio between the linear model predicted \n#' intensity value and the actual intensity.\n#' @param minPeaks minimum number of signal peaks following dynamic \n#' noise filtration (default = 5).\n#' @param maxPeaks maximum number of signal peaks the function will continue\n#' until both the minimum DNF signal to noise ratio is exceeding and the number\n#' of peaks is lower than the maximum (default = 5).\n#' \n#' @return a list containing 3 objects:\n#' \\enumerate{\n#' \\item Above.noise The dynamic noise filtered matrix/ dataframe \n#' \\item metaData a dataframe with the following column names:\n#'        1. Noise.level the noise level determined by the dynamic noise filter \n#'           function.\n#'        2. IntCompSpec Total intensity composite spectrum.\n#'        3. TotalIntSNR Sparse ion signal to noise ratio \n#'        (mean intensity/ stdev intensity)\n#'        4. nPeaks number of peaks in composite spectrum\n#' \\item aboveMinPeaks Logical are the number of signals above the minimum level}\n#' @details  Dynamic noise filter adapted from the method described in Xu H. and \n#' Frietas M. \"A Dynamic Noise Level Algorithm for Spectral Screening of \n#' Peptide MS/MS Spectra\" 2010 BMC Bioinformatics. \n#' \n#' The function iteratively calculates linear models starting from \n#' the median value of the lower half of all intensities in the spectrum.df. \n#' The linear model is used to predict the next peak intensity and ratio is \n#' calculated between the predicted and actual intensity value. \n#' \n#' Assuming that all preceeding intensities included in the linear model \n#' are noise, the signal to noise ratio between the predicted and actual values \n#' should exceed the minimum signal to noise ratio (default DNF = 2). \n#' \n#' The function continues until either the DNF value minimum has been exceeded \n#' and is also below the maxPeaks or maximum number of peaks value. As the \n#' function must necessarily calculate potentially hundreds of linear models the \n#' RcppEigen package is used to increase the speed of computation.\n#' \n#' @export\ndynamicNoiseFilter <- function(spectrum.df=NULL, DNF=2, minPeaks=5, \n                               maxPeaks=20, minInt=100){\n  # error handling\n  if(is.null(spectrum.df)){\n    stop(\"No spectrum matrix/dataframe supplied\")    \n  } else {\n    # rank matrix/ dataframe by intensity\n    intOrder<-order(spectrum.df[, 2])\n    spectrum.df <- spectrum.df[intOrder, , drop=FALSE]\n    # median bottom half of intensity values\n#     medBottomHalf <- median(head(spectrum.df[, 2],\n#                                  n=nrow(spectrum.df)/2))\n#     medBottomHalf <- which(spectrum.df[, 2] >= medBottomHalf)[1]\n    minIntIndx <- which(spectrum.df[, 2] >= minInt)[1]\n    peakIndx <- seq(1, nrow(spectrum.df), 1)\n    minIntIndx <- ifelse(is.na(minIntIndx), nrow(spectrum.df), minIntIndx)\n    minIntIndx <- ifelse(minIntIndx == 1, 2, minIntIndx)\n    # break loop if higher DNF and also less than maxPeaks\n#     for(k in medBottomHalf:(nrow(spectrum.df)-1)){\nif(minIntIndx < (nrow(spectrum.df)-1)){\nfor(k in minIntIndx:(nrow(spectrum.df)-1)){\n      # calc linear model rcppeigen \n      fit <- coef(RcppEigen::fastLm(as.numeric(spectrum.df[1:k, 2]) \n                                    ~ peakIndx[1:k]))\n      # predicted intensity model from intercept\n      PredInt <- fit[1]+(fit[2])*(k+1)\n      # calc Signal to noise ratio predicted vs. actual\n      SNR <- spectrum.df[k+1, 2]/PredInt\n      # if SNR reached and below max number of peaks break loop\n      if(SNR >= DNF & nrow(spectrum.df) - (k+1) < maxPeaks){\n        Noise.level <- as.numeric(spectrum.df[k+1, 2])\n        break\n      } else {\n        Noise.level <- as.numeric(spectrum.df[k+1, 2])\n      }\n    }\n} else {\n  Noise.level <- spectrum.df[minIntIndx, 2]\n}\n    # filter by DNF noise filter level\n    Noise.indx <- which(spectrum.df[, 2] >= Noise.level)\n    spectrum.df <- spectrum.df[Noise.indx, , drop=FALSE]\n    # sort by m/z\n    spectrum.df <- spectrum.df[order(spectrum.df[,1]), , drop=FALSE]\n    # number of peaks higher than minimum\n    aboveMinPeaks <- nrow(spectrum.df) >= minPeaks\n    # Total intensity composite spectrum \n    IntCompSpec <- sum(spectrum.df[, 2])\n    # Sparse ion signal to noise ratio (mean intensity/ stdev intensity)\n    if(nrow(spectrum.df) <= 1){\n      TotalIntSNR <- 0\n    } else {\n      TotalIntSNR <- mean(spectrum.df[, 2], sd(spectrum.df[, 2]))\n    }\n    DNF.tmp<-list(Above.noise=spectrum.df, \n                  metaData=data.frame(Noise.level=Noise.level, \n                                      IntCompSpec=IntCompSpec,\n                                      TotalIntSNR=TotalIntSNR,\n                                      nPeaks=nrow(spectrum.df), \n                                      stringsAsFactors = FALSE),\n                  aboveMinPeaks=aboveMinPeaks)\n    return(DNF.tmp)\n  }  \n}\n" }
{ "repo_name": "KWB-R/kwb.wtaq", "ref": "refs/heads/master", "path": "R/wtPlotHelperFunctions.R", "content": "# .plotWell --------------------------------------------------------------------\r\n.plotWell <- function # plot a well\r\n### plot a well\r\n(\r\n  x, \r\n  ### x position of centre of well\r\n  r, \r\n  ### inside radius of well pipe\r\n  z1, \r\n  ### top of screen as depth below top of aquifer\r\n  z2, \r\n  ### bottom of screen as depth below top of aquifer\r\n  z0 = 0, \r\n  ### ground level\r\n  r2 = r, \r\n  ### inside radius of screen\r\n  col=\"black\", \r\n  ### line colour\r\n  lwd = 1\r\n  ### line width\r\n)\r\n{\r\n  # draw borehole (from ground level to screen top)\r\n  rect(x-r, z0, x+r, z1, border = col, lwd = lwd)\r\n  \r\n  # draw screen\r\n  segments(x + r2*c(-1, 1, -1, -1), c(z1, z1, z1, z2), \r\n           x + r2*c(-1, 1,  1,  1), c(z2, z2, z1, z2),\r\n           lty = c(2,2,1,1), lwd = lwd, col = col)\r\n  \r\n  # If r2 is 0, the screen will not be visible. In this case, draw a point\r\n  # in the middle between z1 and z2\r\n  if (r2 == 0) {\r\n    points(x, (z1+z2)/2, col=col, pch=16)\r\n  }\r\n}\r\n\r\n# .owPlotTopView ---------------------------------------------------------------\r\n.owPlotTopView <- function\r\n(\r\n  wellfield, \r\n  ### data frame with columns \\emph{x}, \\emph{y}, \\emph{r}, \\emph{wellName}, as\r\n  ### e.g. retrieved by \\code{\\link{owRandomWellfield}} with each row\r\n  ### representing a well of the well field.\r\n  refid = -1, \r\n  ### ID of reference well, i.e. well that is selected as productive well.\r\n  ### If this ID is given, circle lines are draw around the corresponding well\r\n  ### and the x range is extended to the right so that the intersection of the\r\n  ### cirle line of the most distant well keeps visible. If refid is -1 no\r\n  ### well has the focus and no circle lines are drawn.\r\n  col = rainbow(nrow(wellfield)),\r\n  ### colours to be used for the wells. Default: rainbow colours\r\n  main = \"Top view on well field\"\r\n  ### plot title. Default: \"Top view on well field\"\r\n) \r\n{\r\n  # Calculate distances to reference well if reference well is given\r\n  if (refid != -1) {\r\n    wdist <- owWellDistances(wellfield, refid)    \r\n  }\r\n  \r\n  # minimum x coordinate\r\n  iminx <- which.min(wellfield$x)\r\n  xmin <- wellfield$x[iminx] - wellfield$r[iminx]\r\n  \r\n  # maximum x coordinate\r\n  if (refid == -1) {\r\n    imax <- which.max(wellfield$x)\r\n    xmax <- wellfield$x[imax] + wellfield$r[imax]\r\n  } else {\r\n    imax <- which.max(wdist)\r\n    xmax <- wellfield$x[refid] + wdist[imax] + wellfield$r[imax]\r\n  }  \r\n  \r\n  # minimum and maximum y coordinate\r\n  imin <- which.min(wellfield$y)\r\n  imax <- which.max(wellfield$y)\r\n  ymin <- wellfield$y[imin] - wellfield$r[imin]\r\n  ymax <- wellfield$y[imax] + wellfield$r[imax]\r\n  \r\n  # x and y range\r\n  xlim <- c(xmin, xmax)\r\n  ylim <- c(ymin, ymax)\r\n  \r\n  # Set the title\r\n  if (refid != -1) {\r\n    main <- paste(main, \", reference well = \", wellfield$wellName[refid], sep=\"\")\r\n  }\r\n  \r\n  # Prepare the plot area\r\n  plot(NULL, NULL, asp = 1, type = \"n\", xlim = xlim, ylim = ylim, \r\n       xlab = \"x coordinate\", ylab = \"y coordinate\", main = main)    \r\n  \r\n  # Number of wells\r\n  numberOfWells <- nrow(wellfield)\r\n  \r\n  # Recyle colour vector to the required length\r\n  col <- recycle(col, numberOfWells)\r\n  \r\n  # If a reference well is given, draw connecting lines from the reference well \r\n  # to each other well and circle lines around the reference well with radiuses\r\n  # corresponding to the distances to the other wells\r\n  if (refid != -1) {\r\n    \r\n    # Draw connecting lines\r\n    segments(wellfield$x[ refid], wellfield$y[ refid], \r\n             wellfield$x[-refid], wellfield$y[-refid], lty = 2)\r\n    \r\n    # Draw horizontal reference line \r\n    abline(h = wellfield$y[refid], col = \"grey\")    \r\n    \r\n    # Draw vertical lines through the centre of the reference well and in\r\n    # distances corresponding to the well distances\r\n    abline(v=wellfield$x[refid] + wdist, col=\"grey\", lty=2)\r\n    \r\n    # Draw radiuses around reference well\r\n    cx <- wellfield$x[refid] # x/y coordinate of circle centre\r\n    cy <- wellfield$y[refid]    \r\n    for (i in (1:numberOfWells)[-refid]) {\r\n      draw.circle(cx, cy, radius = wdist[i], border = col[i])\r\n    }\r\n  } \r\n  else {\r\n    # Draw horizontal and vertical lines at the x and y coordinats of the\r\n    # well centres\r\n    abline(h=wellfield$y, v = wellfield$x, col = \"grey\", lty = 2)\r\n  }\r\n  \r\n  # Draw the wells\r\n  for (i in 1:numberOfWells) {\r\n    draw.circle(wellfield$x[i], wellfield$y[i], wellfield$r[i], col = col[i])\r\n  }\r\n  \r\n  # Label the wells\r\n  text(wellfield$x, wellfield$y, wellfield$wellName)\r\n  \r\n  # return x limits\r\n  return(par()$usr[1:2])\r\n}\r\n\r\n# .plotSideView ----------------------------------------------------------------\r\n.plotSideView <- function # plot side view of wellfield\r\n### Plot side view of wellfield\r\n(\r\n  x, \r\n  ### vector of x positions at which wells are to be plotted. \r\n  r,\r\n  ### vector of radiuses of wells in units of length\r\n  z1,\r\n  ### begin of screen (depth below top of aquifer) of wells in units of length\r\n  z2,\r\n  ### end of screen (depth below top of aquifer) of wells in units of length\r\n  r2 = r,\r\n  ### vector of radiuses of well screen (equals r by default)\r\n  thick.unsat = 0,\r\n  ### thickness of unsaturated zone in units of length\r\n  thick.aquif = max(1.05*z2),\r\n  ### thickness of aquifer in units of length\r\n  col = rainbow(length(x)),\r\n  ### colours by which wells are to be represented\r\n  name = sprintf(\"W%02d\", 1:length(x)),\r\n  ### vector of well names\r\n  xlab = \"\",\r\n  ### x-axis label\r\n  main = \"Side view on well field\",\r\n  legend.cex = 1,\r\n  ### character expansion factor for legend\r\n  wellaxes = TRUE,\r\n  ### if TRUE, a vertical line representing the well axis is drawn for each well\r\n  bottom.aquif = TRUE,\r\n  ### if TRUE, plot is scaled so that bottom of aquifer is visible. Default: TRUE\r\n  col.aquif = \"lightcyan\", #\"powderblue\" # \"lightblue1\" # \"steelblue1\"\r\n  ### colour of aquifer\r\n  col.unsat = \"wheat\", # \"tan\",\r\n  ### colour of unsaturated zone\r\n  legend.mar = 5,  \r\n  ### margin (in number of lines) on the right side to be reserved for legend\r\n  asp = 1,\r\n  ### aspect ratio between x and y axis. Default: 1. Set to NA if aspect ratio\r\n  ### does not matter.\r\n  xlim = NULL,\r\n  ylim = NULL,\r\n  ...\r\n  ### further arguments passed to .plotWell\r\n) \r\n{\r\n  # Recycle all vectors to length of x\r\n  n  <- length(x)\r\n  \r\n  r   <- recycle(r, n)\r\n  r2  <- recycle(r2, n)\r\n  z1  <- recycle(z1, n)\r\n  z2  <- recycle(z2, n)\r\n  col <- recycle(col, n)\r\n  \r\n  # Expand margin to make room for legend (reset graphic parameters on exit)\r\n  opar <- par(mar=par()$mar + c(0, 0, 0, legend.mar))    \r\n  on.exit(par(opar))    \r\n  \r\n  # Prepare plot area\r\n  imin <- which.min(x)\r\n  imax <- which.max(x)\r\n\r\n  ymax <- ifelse(bottom.aquif, thick.aquif, max(z2))\r\n\r\n  if (is.null(xlim)) {\r\n    xlim <- c(x[imin] - max(r[imin], r2[imin]), \r\n              x[imax] + max(r[imax], r2[imax]))\r\n  }\r\n\r\n  if (is.null(ylim)) {\r\n    ylim <- c(ymax, -thick.unsat)\r\n  }  \r\n  \r\n  plot(NA, NA, # xaxs <- \"r\" # regular, xaxs <- \"i\" # internal\r\n       xlim = xlim,\r\n       ylim = ylim,\r\n       asp = asp,\r\n       xlab = xlab, \r\n       ylab = \"depth below top of aquifer (m)\",\r\n       main = main)\r\n  \r\n  # get coordinates of plot area\r\n  usr <- par()$usr\r\n  \r\n  # draw unsaturated zone(s)\r\n  abline(h = -thick.unsat, lwd=2, col=col.unsat)\r\n\r\n  rect(usr[1], -thick.unsat, usr[2], 0, angle = -45, #density = 15,\r\n       lwd = 0.5, col = col.unsat)\r\n  \r\n  rect(usr[1], thick.aquif, usr[2], usr[3], angle = -45, #density = 15,\r\n       lwd = 0.5, col = col.unsat)\r\n  \r\n  # draw aquifer body\r\n  rect(usr[1], 0, usr[2], min(usr[3], thick.aquif), lwd = 0.5, \r\n       col = col.aquif)#, density = 5)\r\n  \r\n  # plot wells  \r\n  for (i in 1:length(x)) {\r\n    .plotWell(x[i], r[i], z1[i], z2[i], r2 = r2[i], z0 = -thick.unsat, \r\n              col = col[i], ...) \r\n  }\r\n  \r\n  # plot vertical lines representing axes of observation wells\r\n  if (wellaxes) {\r\n    abline(v = x, col = col, lty = \"twodash\", lwd = 0.8) # x[-1]?\r\n  }\r\n  \r\n  # draw legend\r\n  legend(\"left\", lty = \"solid\", box.lty = 0, border=\"black\",\r\n         legend = name, col = col, cex = legend.cex, inset = 1.01, xpd = TRUE) \r\n}\r\n\r\n# .wtAddXCoordToResult ---------------------------------------------------------\r\n.wtAddXCoordToResult <- function # Add well's x coordinate to WTAQ result\r\n### Add column containing x coordinate of well to WTAQ result. It is expected\r\n### that wtaqResult already is in \"list form\".\r\n(\r\n  wtaqResult, \r\n  wtaqConfiguration\r\n) \r\n{\r\n  # Add a column x representing the distance to the pumped well\r\n\r\n  distances <- wtConfiguredDistances(wtaqConfiguration)\r\n  merge(wtaqResult, data.frame(WELL = names(distances), x = distances))  \r\n}\r\n\r\n# .wtAddDrawdownLines ----------------------------------------------------------\r\n.wtAddDrawdownLines <- function\r\n(\r\n  wtaqResult, \r\n  ### WTAQ result as returned by \\code{\\link{wtRunConfiguration}}\r\n  value = \"CALCDD\", \r\n  ### name of value column\r\n  times = sort(unique(wtaqResult$TIME)),\r\n  ### Vector of times to plot. Default: sorted unique values in column TIME \r\n  ### of wtaqResult.\r\n  col = NULL,\r\n  dbg = FALSE,\r\n  ...\r\n  ### additional parameters given to lines\r\n) \r\n{\r\n  wtaqConfiguration <- attr(wtaqResult, \"wtaqConfiguration\")\r\n\r\n  # Add column x (distance to pumping well)\r\n  wtaqResultAndX <- .wtAddXCoordToResult(wtaqResult, wtaqConfiguration)\r\n  \r\n  checkForMissingColumns(wtaqResultAndX, c(\"x\", \"TIME\"))\r\n  \r\n  # Order result data frame by x and t\r\n  row.order <- order(wtaqResultAndX$x, wtaqResultAndX$TIME)\r\n  wtaqResultAndX <- wtaqResultAndX[row.order, ]\r\n  \r\n  # number of simulated times\r\n  n <- length(times)\r\n  \r\n  # Give default colour vector\r\n  col <- .getColours(col = col, n = n)\r\n  \r\n  # Loop through times\r\n  for (i in 1:n) {\r\n    \r\n    # Filter for given time\r\n    sdat <- wtaqResultAndX[wtaqResultAndX$TIME == times[i], , drop=FALSE]\r\n    \r\n    if (dbg) {\r\n      if (nrow(sdat) == 0) {\r\n        warning(\"no data for time: \", times[i])\r\n      }\r\n      else {\r\n        cat(\"sdat:\\n\");print(sdat)  \r\n      }      \r\n    }\r\n        \r\n    # Draw line\r\n    lines(sdat$x, sdat[[value]], col=col[i], ...)\r\n  }  \r\n}\r\n\r\n# .getColours ------------------------------------------------------------------\r\n.getColours <- function(col, n)\r\n{\r\n  if (is.null(col)) {\r\n    rev(grey(0:(n-1)/n))\r\n  } \r\n  else {\r\n    recycle(col, n)\r\n  }  \r\n}\r\n\r\n# .wtPlotResultOverX -----------------------------------------------------------\r\n.wtPlotResultOverX <- function # plot drawdowns along x coordinates of wells\r\n### plot drawdowns along x coordinates of wells\r\n(\r\n  wtaqResult, \r\n  ### WTAQ result\r\n  wtaqConfiguration = attr(wtaqResult, \"wtaqConfiguration\")\r\n  ### WTAQ configuration\r\n)\r\n{\r\n  # Add x coordinate to the result data frame\r\n  wtaqResultAndX <- .wtAddXCoordToResult(wtaqResult, wtaqConfiguration)\r\n  \r\n  # Order by x\r\n  wtaqResultAndX <- wtaqResultAndX[order(wtaqResultAndX$x), ]\r\n  \r\n  # Prepare colours\r\n  times <- unique(wtaqResultAndX$TIME)\r\n  cols <- rainbow(length(times))\r\n  \r\n  # Plot the drawdown over time at the pumped well\r\n  trellisObject <- lattice::xyplot(\r\n    wtaqConfiguration$aquifer$bb - DD ~ x, \r\n    groups = as.factor(wtaqResultAndX$TIME), \r\n    data = wtaqResultAndX,  \r\n    type = \"b\",\r\n    col = cols, distribute.col = TRUE, \r\n    xlab = \"Distance from pumping well\",\r\n    ylab = \"Drawdown\", \r\n    main = \"Evolution of drawdown over time\",\r\n    legend = list(right = list(\r\n      fun = lattice::draw.colorkey, \r\n      args = list(key = list(\r\n        col = cols, \r\n        at = log(times, 10)))\r\n    )))\r\n  \r\n  print(trellisObject)\r\n}\r\n\r\n# .wtPlotResult.1 --------------------------------------------------------------\r\n.wtPlotResult.1 <- function \r\n(\r\n  wtaqResult, \r\n  main = \"Drawdowns simulated by WTAQ\", \r\n  ylim = NULL,\r\n  meas = (\"MEASDD\" %in% names(wtaqResult)), \r\n  timeColumn = \"TIME\", \r\n  auto.key = NULL,\r\n  ...\r\n)\r\n{\r\n  wtaqConfiguration <- attr(wtaqResult, \"wtaqConfiguration\")\r\n  \r\n  auto.key <- .defaultAutoKeyIfNull(auto.key, wtaqConfiguration)\r\n  \r\n  ylim <- .defaultYlimIfNull(ylim, wtaqResult, meas)\r\n  \r\n  trellisObject <- lattice::xyplot(\r\n    as.formula(sprintf(\r\n      \"CALCDD %s~ %s\", ifelse (meas, \"+ MEASDD \", \"\"), timeColumn)), \r\n    data = wtaqResult, \r\n    groups = wtaqResult$WELL, \r\n    ylim = ylim,\r\n    type = c(\"b\", \"g\"),\r\n    main = main, \r\n    auto.key = auto.key, ...)\r\n\r\n  print(trellisObject)\r\n}\r\n\r\n# .defaultAutoKeyIfNull --------------------------------------------------------\r\n.defaultAutoKeyIfNull <- function (auto.key, wtaqConfiguration)\r\n{\r\n  if (is.null(auto.key)) {\r\n    list(columns = length(wtConfiguredWellnames(wtaqConfiguration)))\r\n  }\r\n  else {\r\n    auto.key    \r\n  }\r\n}\r\n  \r\n\r\n# .defaultYlimIfNull -----------------------------------------------------------\r\n.defaultYlimIfNull <- function(ylim, wtaqResult, measurements)\r\n{\r\n  if (is.null(ylim)) {\r\n    values <- wtaqResult$CALCDD\r\n    if (measurements) {\r\n      values <- c(values, wtaqResult$MEASDD)\r\n    }\r\n    rev(extendLimits(range(values), 0.04))\r\n  }\r\n  else {\r\n    ylim\r\n  }\r\n}\r\n\r\n# .wtPlotResult.2 --------------------------------------------------------------\r\n.wtPlotResult.2 <- function \r\n(\r\n  wtaqResult, \r\n  main = \"Drawdowns simulated by WTAQ\", \r\n  ylim = NULL, \r\n  meas = (\"MEASDD\" %in% names(wtaqResult)), \r\n  timeColumn = \"TIME\",\r\n  auto.key = NULL, \r\n  ...\r\n)\r\n{\r\n  wtaqConfiguration <- attr(wtaqResult, \"wtaqConfiguration\")\r\n\r\n  ylim <- .defaultYlimIfNull(ylim, wtaqResult, meas)\r\n  \r\n  if (meas) {    \r\n    \r\n    mdata <- hsMatrixToListForm(\r\n      hsRenameColumns(removeColumns(\r\n        wtaqResult, \"RELERR\"), \r\n        list(CALCDD = \"calculated\", MEASDD = \"measured\")),\r\n      keyFields = c(\"WELL\", timeColumn),\r\n      colNamePar = \"Type\",\r\n      colNameVal = \"DD\")\r\n\r\n    groups <- mdata$Type\r\n\r\n    ylab <- \"Drawdown in units of length\"\r\n    if (is.null(auto.key)) {\r\n      auto.key <- list(columns = 2)\r\n    }\r\n  } \r\n  else {\r\n    mdata <- wtaqResult\r\n    groups <- NULL  \r\n\r\n    ylab <- \"Calculated drawdown in units of length\"    \r\n    if (is.null(auto.key)) {\r\n      auto.key <- FALSE\r\n    }\r\n  }\r\n  \r\n  frm <- sprintf(\"%s ~ %s | WELL\", ifelse(meas, \"DD\", \"CALCDD\"), timeColumn)\r\n\r\n  trellisObject <- lattice::xyplot(\r\n    as.formula(frm), \r\n    data = mdata, \r\n    groups = groups,\r\n    ylim = ylim,\r\n    ylab = ylab,\r\n    type = c(\"b\", \"g\"),\r\n    main = main, \r\n    auto.key = auto.key, ...)      \r\n  \r\n  print(trellisObject)\r\n}\r\n\r\n# .wtPlotResult.3 --------------------------------------------------------------\r\n.wtPlotResult.3 <- function \r\n(\r\n  wtaqResult, \r\n  meas = (\"MEASDD\" %in% names(wtaqResult)), \r\n  asp = 1, \r\n  col = NULL,\r\n  dbg = FALSE\r\n)\r\n{\r\n  if (dbg) {\r\n    cat(\"wtaqResult in .wtPlotResult.3:\\n\")\r\n    print(wtaqResult)    \r\n  }\r\n\r\n  wtaqConfiguration <- attr(wtaqResult, \"wtaqConfiguration\")\r\n  \r\n  if (is.null(col)) {\r\n    col <- rainbow(1 + length(wtaqConfiguration$obswells))    \r\n  }\r\n  \r\n  # Plot well configuration \r\n  .wtPlotConfigurationAndDrawdowns(\r\n    wtaqConfiguration, wtaqResult, valueColumn = \"CALCDD\", \r\n    main = \"Calculated drawdowns\", col = col, asp = asp)\r\n  \r\n  if (meas) {\r\n    .wtPlotConfigurationAndDrawdowns(\r\n      wtaqConfiguration, wtaqResult, valueColumn = \"MEASDD\", \r\n      main = \"Measured drawdowns\", col = col, asp = asp)\r\n  }\r\n}\r\n\r\n# .wtPlotConfigurationAndDrawdowns ---------------------------------------------\r\n.wtPlotConfigurationAndDrawdowns <- function\r\n(\r\n  wtaqConfiguration, wtaqResult, valueColumn, main, col, ...\r\n)\r\n{\r\n  # first colour is colour for pumping well, remaining colours are colours\r\n  # for observation wells\r\n  wtPlotConfiguration(wtaqConfiguration, main = main,  \r\n                      col.pw = col[1], col.ow = col[-1], ...)\r\n  \r\n  .wtAddDrawdownLines(wtaqResult, value = valueColumn)\r\n}\r\n\r\n# .wtPlotResult.4 --------------------------------------------------------------\r\n.wtPlotResult.4 <- function \r\n(\r\n  wtaqResult, \r\n  timeColumn = \"TIME\", \r\n  meas = (\"MEASDD\" %in% names(wtaqResult)), \r\n  asp = 1, \r\n  PNG = FALSE\r\n)\r\n{\r\n  wtaqConfiguration <- attr(wtaqResult, \"wtaqConfiguration\")\r\n  \r\n  if (PNG) {    \r\n    pngDirectory <- tempSubdirectory(\"wtaqResult\")\r\n    \r\n    # Delete existing png files\r\n    file.remove(dir(pngDirectory, \"\\\\.png$\", full.names=TRUE))  \r\n    \r\n    # Base path and name for png files\r\n    pngFilenameBase <- file.path(pngDirectory, \"wtaqResult\")\r\n  }\r\n  \r\n  # Calculated (+ measured, if applicable) drawdown for each time\r\n  i <- 1\r\n  for (ti in unique(wtaqResult[[timeColumn]])) {\r\n\r\n    if (PNG) {\r\n      pngFile <- sprintf(\"%s_%04d.png\", pngFilenameBase, i)\r\n    }\r\n    else {\r\n      pngFile <- NULL\r\n    }\r\n    \r\n    .wtPlotDrawdownTimeseries(wtaqResult, ti, meas, asp, pngFile = pngFile)\r\n\r\n    i <- i + 1\r\n  }\r\n  \r\n  if (PNG) {\r\n    return(pngFile)\r\n  }\r\n}\r\n\r\n# .wtPlotDrawdownTimeseries ----------------------------------------------------\r\n.wtPlotDrawdownTimeseries <- function\r\n(\r\n  wtaqResult,\r\n  ti,\r\n  meas, \r\n  asp,\r\n  pngFile = NULL,\r\n  wtaqConfiguration = attr(wtaqResult, \"wtaqConfiguration\"),  \r\n  main = sprintf(\"Calculated vs Measured drawdown at t = %0.0f\", ti)\r\n)\r\n{\r\n  if (!is.null(pngFile)) {\r\n    png(pngFile)\r\n    on.exit(dev.off())\r\n  }\r\n  \r\n  wtPlotConfiguration(wtaqConfiguration, main = main, asp = asp)\r\n  \r\n  .wtAddDrawdownLines(\r\n    wtaqResult, value = \"CALCDD\", times = ti, col = \"blue\")\r\n  \r\n  if (meas) {\r\n    \r\n    .wtAddDrawdownLines(\r\n      wtaqResult, value = \"MEASDD\", times = ti, \r\n      col = \"red\", type = \"p\", pch = 15)\r\n    \r\n    legend(\"top\", \r\n           legend = c(\"calculated\", \"measured\"), \r\n           border = FALSE,\r\n           col = c(\"blue\", \"red\"), \r\n           lty = c(1, 0), \r\n           pch = c(NA, 15))\r\n  }  \r\n}\r\n" }
{ "repo_name": "rBatt/trawlDiversity", "ref": "refs/heads/master", "path": "pkgBuild/beta_range.R", "content": "\n# ==============================\n# = Simulating Spreading Range =\n# ==============================\n# ---- setup ----\nnspp <- 25\nnsite <- 25\nnr <- nsite\nnc <- nspp\nn_iter <- 25\n\ngrow <- FALSE\nmoveDir <- c(\"left\",\"right\")[2] #[1:2]\n\n# ---- transfer matrix ----\ntransf <- diag(nrow=nsite)\nif(!grow){\n\tdiag(transf) <- 0\n}\nif(\"left\"%in%moveDir){\n\ttransf[row(transf)-col(transf)==-1] <- 1\t\n\ttransf[1,1] <- 1 # so it doesn't go extinct if grow=FALSE\n}\nif(\"right\"%in%moveDir){\n\ttransf[row(transf)-col(transf)==1] <- 1\n\ttransf[nrow(transf),ncol(transf)] <- 1 # so it doesn't go extinct if grow=FALSE\n}\n\n\n# ---- start with each species in different site ----\nstarting <- matrix(0, nrow=nsite, ncol=nspp)\nstart_occ <- sample(1:nsite, nspp, replace=(nspp>nsite)) # the sites that start with a species\nstarting[as.matrix(data.frame(row=start_occ, col=1:nspp))] <- 1\n\n# ---- dynamics ----\noccupancy <- vector('list', n_iter)\noccupancy[[1]] <- starting\nfor(i in 2:n_iter){\n\toccupancy[[i]] <- pmin(transf%*%occupancy[[i-1]],1)\n}\n\n# =================================\n# = Calculate Range and Diversity =\n# =================================\n# ---- range size ----\nrangeSize <- lapply(occupancy, colSums)\nrangeSize_mu <- sapply(rangeSize, mean)\n\n# ---- alpha diversity ----\nalphaDiv <- lapply(occupancy, rowSums)\nalphaDiv_mu <- sapply(alphaDiv, mean)\n\n# ---- Euclidean beta ----\neuc <- function(x){\n\tsum(scale(x,scale=FALSE)^2)/(nrow(x)-1)\n}\nbetaDiv <- sapply(occupancy, euc) #\n\n# ---- Jaccard beta ----\njaccard <- function(x){\n\tn <- nrow(x)\n\td <- ade4::dist.binary(x, method=1) # requires ade4 package\n\tsstot <- sum(d^2)/n\n\tsstot/(n-1)\n}\nbetaDiv_jaccard <- sapply(occupancy, jaccard)\n\n# ---- Hellinger beta ----\nhellinger <- function(x){\n\tn <- nrow(x)\n\tY <- vegan::decostand(x, \"hellinger\") # requires vegan package\n\ts <- scale(Y, center=TRUE, scale=FALSE)^2\n\tsum(s) /(n-1)\n}\nbetaDiv_hellinger <- sapply(occupancy, hellinger)\n\n\n# ========\n# = Plot =\n# ========\n# pdf(\"~/Desktop/range_alpha_beta.pdf\", width=3.5, height=6)\ndev.new(width=3.5, height=6)\npar(mfrow=c(4,1), mar=c(1.75,1.75,0.5,0.5), tcl=-0.15, mgp=c(0.85,0.2,0), ps=8, cex=1)\n\nplot(1:n_iter, rangeSize_mu, type='l')\n\nplot(1:n_iter, alphaDiv_mu, type='l')\n\nplot(1:n_iter, betaDiv, type='l')\npar(new=TRUE)\nplot(1:n_iter, betaDiv_jaccard, type='l', xaxt='n', yaxt='n', xlab='', ylab='', col='blue')\naxis(side=4)\npar(new=TRUE)\nplot(1:n_iter, betaDiv_hellinger, type='l', xaxt='n', yaxt='n', xlab='', ylab='', col='red')\nlegend(\n\t\"topright\", \n\tlegend=c(\"Euclidean\", \"Jaccard\", \"Hellinger\"), \n\tcol=c(\"black\",\"blue\",\"red\"), \n\tlty=1, y.intersp = 0.75, bty='n'\n)\n\nplot(rangeSize_mu, betaDiv, type='l')\npar(new=TRUE)\nplot(rangeSize_mu, betaDiv_jaccard, type='l', xaxt='n', yaxt='n', xlab='', ylab='', col='blue')\naxis(side=4)\npar(new=TRUE)\nplot(rangeSize_mu, betaDiv_hellinger, type='l', xaxt='n', yaxt='n', xlab='', ylab='', col='red')\n# dev.off()\n\n# =================\n# = Gif Animation =\n# =================\nold.wd <- getwd()\nsetwd(\"~/Desktop\")\nsaveGIF(\n\t{\n\t\tani.options(inverval=0.0001)\n\t\tfor(i in 1:n_iter){\n\t\t\t# par(mfrow=c(1,1), mar=c(2,2,0.5,0.1), ps=10, cex=1, mgp=c(0.5,0.15,0), tcl=-0.15, family=\"Times\")\n\t\t\timage(occupancy[[i]], xlab='location', ylab='species')\n\t\t\t\n\t\t\tpar(new=TRUE)\n\t\t\tplot(1:i, betaDiv_hellinger[1:i], type='l', xaxt='n', yaxt='n', xlab='', ylab='', col='white', lwd=3, ylim=range(betaDiv_hellinger), xlim=c(1,n_iter))\n\t\t\tlines(1:i, betaDiv_hellinger[1:i])\n\t\t\taxis(side=4)\n\t\t}\n\t\n\t},\n\tani.height=400,\n\tani.width=400,\n\tmovie.name=\"betaDiv_distribution.gif\",\n)\nsetwd(old.wd)\n\n\n\n\n\n" }
{ "repo_name": "adrHuerta/PISCO_Temp", "ref": "refs/heads/master", "path": "functions/interpolation_functions_2.R", "content": "get_variables <- function(MODIS_LST = NULL,  #brick\n                          ALL_STATICS_VAR = NULL, #brick\n                          OBS_DATA = NULL, #brick\n                          m = NULL, #number\n                          name_var = NULL) #character\n  {\n  \n  #\n  dmy_cov <- MODIS_LST[[paste(\"LST\", m, sep = \"\")]]; names(dmy_cov) <- \"LST\"\n  obs_data <- OBS_DATA[paste(\"To\", m, sep = \"\")]; names(obs_data) <- \"To\"\n  cov_data <- brick(dmy_cov, ALL_STATICS_VAR) %>% .[[name_var]]\n  \n  #\n  formula_lm <- as.formula(paste(\"To ~ \",  paste(name_var, collapse = \"+\")))\n  \n  return(list(cov_data = cov_data, obs_data = obs_data, formula_lm = formula_lm))\n}\n\n# get_variables(MODIS_LST = tx_lst,\n#               ALL_STATICS_VAR = rest_cov,\n#               OBS_DATA = tx_obs_normal_1,\n#               m = \"12\",\n#               name_var = c(\"LST\",\"Z\"))\n\nregKriging <- function(from_get_variables = NULL,\n                       delR = NULL)\n{\n  \n  #regression \n  \n  cov_data <- from_get_variables$cov_data\n  obs_data <- from_get_variables$obs_data\n  formula_lm <- from_get_variables$formula_lm\n  \n  p_cov <- raster::extract(cov_data, obs_data, cellnumber = F, sp = T)\n  lineral_model <- lm(formula_lm, p_cov)\n  coeff_model <- summary(lineral_model)$coefficients[,1]\n  obs_data$Residuals <- lineral_model$residuals\n  \n  if( is.numeric(delR)) {\n    \n    obs_data$Residuals[obs_data$Residuals > delR ] <- delR \n    obs_data$Residuals[obs_data$Residuals < -delR ] <- -delR \n    \n  }\n  \n  cov_data_model <- coeff_model[1] + sum(cov_data*coeff_model[-1])\n  #spplot(cov_data_model)\n  \n  #residual kriging \n  #.0 grid data\n  \n  point <- rasterToPoints(cov_data_model) %>% data.frame\n  coordinates(point) <- ~x + y\n  projection(point) <- NA\n  \n  #.1 variograms\n  \n  Residuals_df <- obs_data %>% as.data.frame()\n  coordinates(Residuals_df) =~ XX+YY\n  projection(obs_data) <- NA\n  \n  variogram = autofitVariogram(Residuals ~ 1, Residuals_df, \n                               fix.values = c(0, NA, NA),\n                               cressie = T, miscFitOptions = list(merge.small.bins = T,min.np.bin = 30))\n  \n  #.2 kriging \n  \n  residual_kriged <- krige(Residuals ~ 1, obs_data, point, model = variogram$var_model)\n  residual_val <- as(residual_kriged[1],\"SpatialPixelsDataFrame\")\n  gridded(residual_val) <- TRUE\n  residual_val <- raster(residual_val)\n  residual_var <- as(residual_kriged[2],\"SpatialPixelsDataFrame\")\n  gridded(residual_var) <- TRUE\n  residual_var <- raster(residual_var)\n  \n  # outputs \n  projection(residual_val) <- projection(cov_data_model)\n  cov_data_model <- crop(cov_data_model, extent(residual_val))\n  \n  final_map <- residual_val + cov_data_model\n  \n  return(list(lineral_model = lineral_model, \n              cov_data_model = cov_data_model, \n              variogram = variogram,\n              final_map = final_map,\n              final_map_sd = residual_var))\n}\n\n\nget_variables2 <- function(CLIM_D = NULL,  #brick\n                           ALL_STATICS_VAR = NULL, #brick\n                           OBS_DATA = NULL, #brick\n                           n = NULL, #number\n                           name_var = NULL) #character\n{\n  \n  #\n  nM <- names(OBS_DATA)[n]\n  obs_data <- OBS_DATA[,n]; names(obs_data) <- \"To\"\n  dmy_cov <- CLIM_D[[ substr(nM, 9, 10) %>% as.numeric]]; names(dmy_cov) <- \"CT\"\n  cov_data <- brick(dmy_cov, ALL_STATICS_VAR) %>% .[[name_var]]\n  \n  #\n  formula_lm <- as.formula(paste(\"To ~ \",  paste(name_var, collapse = \"+\")))\n  \n  return(list(cov_data = cov_data, obs_data = obs_data, formula_lm = formula_lm, date = nM))\n}\n\n# get_variables2(CLIM_D = merging_tx,\n#                ALL_STATICS_VAR = rest_cov,\n#                OBS_DATA = anom_dtx,\n#                n = 13149,\n#                name_var = c(\"CT\",\"X\",\"Y\",\"DS\",\"TDI\"))\n\nregKriging2 <- function(from_get_variables = NULL,\n                       delR = NULL)\n{\n  \n  #regression \n  \n  cov_data <- from_get_variables$cov_data\n  obs_data <- from_get_variables$obs_data\n  formula_lm <- from_get_variables$formula_lm\n  \n  p_cov <- raster::extract(cov_data, obs_data, cellnumber = F, sp = T)\n  lineral_model <- lm(formula_lm, p_cov)\n  coeff_model <- summary(lineral_model)$coefficients[,1]\n  obs_data$Residuals <- lineral_model$residuals\n  \n  if( is.numeric(delR)) {\n    \n    obs_data$Residuals[obs_data$Residuals > delR ] <- delR \n    obs_data$Residuals[obs_data$Residuals < -delR ] <- -delR \n    \n  }\n  \n  cov_data_model <- coeff_model[1] + sum(cov_data*coeff_model[-1])\n  #spplot(cov_data_model)\n  \n  #residual kriging \n  #.0 grid data\n  \n  point <- rasterToPoints(cov_data_model) %>% data.frame\n  coordinates(point) <- ~x + y\n  projection(point) <- NA\n  \n  #.1 variograms\n  \n  Residuals_df <- obs_data %>% as.data.frame()\n  coordinates(Residuals_df) =~ XX+YY\n  projection(obs_data) <- NA\n  \n  variogram = autofitVariogram(Residuals ~ 1, Residuals_df, \n                               fix.values = c(0, NA, NA),\n                               cressie = T, miscFitOptions = list(merge.small.bins = T,min.np.bin = 30))\n  \n  #.2 kriging \n  \n  residual_kriged <- krige(Residuals ~ 1, obs_data, point, model = variogram$var_model)\n  residual_val <- as(residual_kriged[1],\"SpatialPixelsDataFrame\")\n  gridded(residual_val) <- TRUE\n  residual_val <- raster(residual_val)\n  residual_var <- as(residual_kriged[2],\"SpatialPixelsDataFrame\")\n  gridded(residual_var) <- TRUE\n  residual_var <- raster(residual_var)\n  \n  # outputs \n  projection(residual_val) <- projection(cov_data_model)\n  cov_data_model <- crop(cov_data_model, extent(residual_val))\n  \n  #final_map <- (residual_val + cov_data_model)\n  final_map <- from_get_variables$cov_data$CT + (residual_val + cov_data_model)\n  \n  return(list(lineral_model = lineral_model, \n              cov_data_model = cov_data_model, \n              variogram = variogram,\n              final_map = final_map,\n              final_map_sd = residual_var))\n}\n\n" }
{ "repo_name": "okgreece/DescriptiveStats.OBeu", "ref": "refs/heads/master", "path": "R/ds.hist.R", "content": "#' @title\n#' Histogram breaks and frequencies\n#'\n#' @description\n#' This function computes the histogram parameters of the numeric input vector. The default for\n#' breaks is the value resulted from Sturges algorithm.\n#'\n#' @usage ds.hist(x, breaks = \"Sturges\", tojson = FALSE)\n#'\n#' @param x The input numeric vector, matrix or data frame\n#' @param breaks The method or the number of classes for the histogram\n#' @param tojson If TRUE the results are returned in json format, default returns a list\n#'\n#' @details\n#' The possible values for breaks are Sturges see \\code{\\link{nclass.Sturges}},\n#' Scott see \\code{\\link{nclass.scott}} and FD or Freedman Diaconis \\code{\\link{nclass.FD}}\n#' which are in package \\pkg{grDevices}.\n#'\n#'\n#' @return A list or json file with the following components:\n#'\n#' \\itemize{\n#' \\item cuts The boundaries of the histogram classes\n#' \\item density The density of each histogram class\n#' \\item normal.curve.x Abscissa of the normal curve\n#' \\item normal.curve.y Ordinate of the normal curve\n#' \\item fit.line.x Abscissa of the data density curve\n#' \\item fit.line.y Ordinate of the data density curve\n#' \\item mean The average value of the input vector\n#' \\item median The median value of the input data\n#' }\n#'\n#' @author Kleanthis Koupidis, Charalampos Bratsas\n#'\n#' @seealso \\code{\\link{ds.analysis}}, \\code{\\link{open_spending.ds}}\n#'\n#' @examples\n#' # with a vector as an input and the defaults parameters\n#' vec <- as.vector(iris$Sepal.Width)\n#' ds.hist(vec)\n#'\n#' # OpenBudgets.eu Dataset Example:\n#' ds.hist(Wuppertal_df$Amount, tojson = TRUE)\n#' @rdname ds.hist\n#' @export\n#'\n\nds.hist <- function(x, breaks = \"Sturges\", tojson = FALSE) {\n  x <- as.numeric(unlist(x))\n  histog <- graphics::hist(x, probability = FALSE, plot = FALSE, warn.unused = FALSE)\n\n  hist.param <- list(\n    cuts = histog$breaks,\n    counts = histog$counts,\n    mean = mean(x),\n    median = stats::median(x)\n  )\n\n  if (tojson == TRUE) {\n    hist.param <- jsonlite::toJSON(hist.param)\n  }\n  return(hist.param)\n}\n" }
{ "repo_name": "michaelrstatfsu/spherical_data_analysis", "ref": "refs/heads/master", "path": "R_Spherical_Regression_Code/EXAMPLE_uniform_Directions.R", "content": "# Demo Plotting Tools for Spherical Data Analysis\nsetwd('/home/michael/Documents/github_repositories/spherical_data_analysis/R_Spherical_Regression_Code/')\n# Requires rgl library\nlibrary(\"rgl\")\nsource('plottingfunctions.r')\nsource('sd_functions.r')\n\n# Generate points Uniformly on the sphere\nn=1000\np=UNIFORMdirections(3,n)\n\nrgl.close()#Closes the current dev\nopen3d(windowRect=c(0,0,800,600)) # open a 600x800 window\nbg3d(\"black\")# Set background color\nMESH(mksphere(30))\nPLOT(p,col='yellow')\n\n" }
{ "repo_name": "jpritikin/OpenMx", "ref": "refs/heads/master", "path": "tests/testthat/test-simplestCI.R", "content": "#\n#   Copyright 2007-2019 by the individuals mentioned in the source code history\n#\n#   Licensed under the Apache License, Version 2.0 (the \"License\");\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n# \n#        http://www.apache.org/licenses/LICENSE-2.0\n# \n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\n\n\nlibrary(OpenMx)\nlibrary(testthat)\ncontext(\"simplestCI\")\n\nmxOption(key='Number of Threads', value=1) \n\n#mxOption(NULL, \"Default optimizer\", \"NPSOL\")\n\ncovariance <- matrix(c(1.0, 0.5, 0.5, 1.0), nrow=2, dimnames=list(c(\"a\", \"b\"),\n                                                                  c(\"a\", \"b\")))\nmeans <- c(-1,.5)\nnames(means) <- c('a','b')\n\nmodel <- mxModel(\"CIExample\",\n                 mxMatrix(name=\"expectedCov\", \"Symm\", 2, 2, free=T, values = c(1, 0, 1),\n                          labels = c(\"var1\", \"cov12\", \"var2\")),\n                 mxMatrix(name=\"expectedMean\", \"Full\", 1, 2, free=T, labels=c('m1','m2')),\n                 mxExpectationNormal(\"expectedCov\", \"expectedMean\", dimnames=c(\"a\", \"b\")),\n                 mxFitFunctionML(),\n                 mxData(covariance, \"cov\", means, numObs=10000)\n)\ndiag(model$expectedCov$lbound) <- .1\n\nmodel <- mxOption(model,\"Checkpoint Units\",'iterations')\nmodel <- mxOption(model,\"Checkpoint Count\",1)\n\nfit1 <- mxRun(model, silent=TRUE)\n\nif (mxOption(NULL, 'Default optimizer') != \"SLSQP\") {ctype = 'none'} else {ctype = 'ineq'}\n\ncimodel <- mxModel(fit1,\n                   mxCI(\"var1\", type=\"lower\", boundAdj=FALSE),\n                   mxCI(\"cov12\", type=\"upper\"),\n                   mxCI(\"m1\", type=\"both\"),\n                   mxComputeConfidenceInterval(verbose=0,plan=mxComputeGradientDescent(verbose=0), constraintType = ctype))\n\nfit2 <- mxRun(cimodel,\n              intervals = TRUE, silent=TRUE, checkpoint=FALSE)\n#print(fit2$compute$output$detail)\n\nfit3 <- mxRun(cimodel, intervals = FALSE)\nomxCheckTrue(is.null(fit3$output$confidenceIntervals))\n\n# For multivariate normal means, SEs match likelihood-based CIs\nomxCheckCloseEnough(fit2$output$estimate['m1'] + fit1$output$standardErrors['m1',] * qnorm(.025),\n                    fit2$output$confidenceIntervals['m1', 'lbound'], .0001)\nomxCheckCloseEnough(fit2$output$estimate['m1'] - fit1$output$standardErrors['m1',] * qnorm(.025),\n                    fit2$output$confidenceIntervals['m1', 'ubound'], .0001)\n\n# cat(deparse(round(model$output$confidenceIntervals, 3)))\nomxCheckCloseEnough(fit2$output$confidenceIntervals['var1','lbound'], c(0.9727), .001)\nomxCheckCloseEnough(fit2$output$confidenceIntervals['cov12','ubound'], c(0.522), .001)\n\nomxCheckCloseEnough(fit1$output$fit, fit2$output$fit, 1e-6)\n\nfit4 <- omxCheckWarning(mxRun(mxModel(cimodel, mxCI('expectedMean[1,1]', interval=runif(1,.9,.95))),\n\t\t\t      intervals = TRUE, silent=TRUE, checkpoint=FALSE),\n\t\t\t\"Different confidence intervals 'CIExample.expectedMean[1,1]' and 'm1' refer to the same thing\")\n\ntest_that(\"missing algebra\", {\n  Q <- mxAlgebra(c + d, name=\"e\")\n  expect_error(mxRun(mxModel(cimodel, mxCI('Q[1,1]'))),\n               \"outside of the model\")\n})\n\n# ensure the [1,] syntax is supported\ndata(demoOneFactor)\nfactorModel <- mxModel(\"One Factor\",\n      mxMatrix(\"Full\", 5, 1, values=0.2, lbound=0, ubound=5,\n           free=TRUE, name=\"A\"),\n      mxMatrix(\"Symm\", 1, 1, values=1,\n           free=FALSE, name=\"L\"),\n      mxMatrix(\"Diag\", 5, 5, values=1,\n           free=TRUE, name=\"U\"),\n      mxAlgebra(A %*% L %*% t(A) + U, name=\"R\"),  mxCI(\"A[1,]\"),\n      mxExpectationNormal(\"R\", dimnames = names(demoOneFactor)),\n      mxFitFunctionML(),\n      mxData(cov(demoOneFactor), type=\"cov\", numObs=500))\nfactorModel <- mxRun(factorModel, intervals=TRUE)\nif (0) {\n  # NPSOL can't find 'em\n  factorModel <- mxRun(mxModel(factorModel,\n                               mxComputeConfidenceInterval(verbose=3, constraintType = \"none\",\n                                                           plan=mxComputeGradientDescent())))\n  print(factorModel$compute$output)\n}\nci <- factorModel$output$confidenceIntervals\nomxCheckEquals(nrow(ci), 1)\nomxCheckCloseEnough(c(0.397), ci[1,'estimate'], .001)\nif (mxOption(NULL, \"Default optimizer\") != \"NPSOL\") {\n  omxCheckCloseEnough(c(0.3679), ci[1,'lbound'], .02)\n  omxCheckCloseEnough(c(0.429), ci[1,'ubound'], .02)\n}\n" }
{ "repo_name": "whitneyburrow/HighDim2Means", "ref": "refs/heads/master", "path": "R/csTest.R", "content": "#' Zhang and Pan Cluster Subspaces Test\n#'\n#' @param x Data set 1.\n#' @param y Data set 2.\n#' @param k k Number of dimensions to select. Defaults to floor((n1 + n2 - 2) / 2).\n#' @param B1 Number of times to randomly subset full data. Defaults to 100.\n#' @param B2 Number of permutations for calculating pvalue. Defaults to 100.\n#' @param data Data frame on which to calculate Pearson distances between variables.\n#' @param n Sample size.\n#' @param p Dimension.\n#'\n#' @return Zhang and Pan cluster subspaces statistic.\n#' @export\n#'\n\ncsTest <- function(x, y, k, B1 = 100) {\n  n1 <- nrow(x)\n  n2 <- nrow(y)\n  p <- ncol(x)\n  if(missing(k)) k <- floor((n1 + n2 - 2) / 2)\n  clusters <- pearsonClusters(x, y)\n  cols <- lapply(unique(clusters$cluster), function(i) {\n    which(clusters$cluster == i)\n  })\n  res <- sapply(seq_along(cols), function(i) {\n    clusterCols <- cols[[i]]\n    xSub <- as.data.frame(x[, clusterCols])\n    ySub <- as.data.frame(y[, clusterCols])\n    hotellingT2(xSub, ySub)\n  })\n  sum(res)\n}\n\n#' @rdname csTest\n#' @export\n\ncsPval <- function(x, y, k, B1 = 100, B2 = 100) {\n  n1 <- nrow(x)\n  n2 <- nrow(y)\n  n <- n1 + n2 - 2\n  x <- as.matrix(x)\n  y <- as.matrix(y)\n  p <- ncol(x)\n  if(missing(k)) k <- floor((n1 + n2 - 2) / 2)\n  z <- rbind(x, y)\n  csObs <- csTest(x, y, k, B1)\n  csZ <- sapply(1:B2, function(i) {\n      xRows <- sample(n1 + n2, n1)\n      xNew <- z[xRows, ]\n      yNew <- z[-xRows, ]\n      csTest(xNew, yNew, k, B1)\n  })\n  sum(csZ >= csObs) / B2\n}\n\n#' @rdname csTest\n#' @export\n\ncs.test <- function(x, y, k, B1 = 100, B2 = 100) {\n  n1 <- nrow(x)\n  n2 <- nrow(y)\n  p <- ncol(x)\n  if(missing(k)) k <- floor((n1 + n2 - 2) / 2)\n  t <- csTest(x, y, k)\n  z <- rbind(x, y)\n  csZ <- sapply(1:B2, function(i) {\n    xRows <- sample(n1 + n2, n1)\n    xNew <- z[xRows, ]\n    yNew <- z[-xRows, ]\n    csTest(xNew, yNew, k, B1)\n  })\n  pval <- sum(csZ >= t) / B2\n  c(tcs = t, pvalue = pval)\n}\n\n#' @rdname csTest\n#' @importFrom stats cov\n#' @export\nhotellingCluster <- function(x, y) {\n  x <- as.matrix(x)\n  y <- as.matrix(y)\n  n1 <- nrow(x)\n  n2 <- nrow(y)\n  p <- ncol(x)\n  dbar <- colMeans(x) - colMeans(y)\n  sPool <- ((n1 - 1) * stats::cov(x) + (n2 - 1) * stats::cov(y)) / (n1 + n2 - 2)\n  weight <- (n1 * n2) / (n1 + n2)\n  as.numeric(weight %*% t(dbar) %*%\n               solve((1 / n1 + 1 / n2) * sPool) %*% dbar)\n}\n\n#' @rdname csTest\n#' @importFrom stats hclust\n#' @importFrom stats cutree\n#' @export\npearsonClusters <- function(x, y) {\n  df <- list(x = x, y = y)\n  df <- Reduce(f = rbind, x = df)\n  distances <- pearsonDistance(df)\n  dc <- pearson(nrow(df) - 2, ncol(df))\n  kc <- floor(2 * (nrow(df) - 2)/3)\n  clusterStart <- flashClust::hclust(distances, method = \"average\")\n  cuts <- stats::cutree(clusterStart, h = dc)\n\n  clusters <- data.frame(variable = names(cuts),\n                         cluster = as.character(cuts),\n                         stringsAsFactors = FALSE)\n\n  while(max(table(clusters$cluster)) > kc) {\n    toSub <- names(which(table(clusters$cluster) > kc))\n    for(i in seq_along(toSub)) {\n      vars <- which(clusters$cluster == toSub[[i]])\n      varNames <- clusters$variable[which(clusters$cluster == toSub[[i]])]\n      newDF <- df[, vars]\n      newDist <- pearsonDistance(newDF)\n      subclusterStart <- flashClust::hclust(newDist, method = \"average\")\n      cuts <- stats::cutree(subclusterStart, k = 2)\n      cuts <- paste0(toSub[[i]], \".\", cuts)\n      clusters$cluster[vars] <- cuts\n    }\n  }\n\n  return(clusters)\n}\n\n#' @rdname csTest\n#' @importFrom stats as.dist\n#' @importFrom stats cor\n#' @export\npearsonDistance <- function(data) {\n  p <- ncol(data)\n  data <- as.matrix(data)\n  colPairs <- utils::combn(1:ncol(data), 2)\n  distances <- sapply(1:ncol(colPairs), function(i) {\n    col1 <- colPairs[1, i]\n    col2 <- colPairs[2, i]\n    1 - stats::cor(data[, col1], data[, col2])\n  })\n  results <- as.data.frame(t(rbind(colPairs, distances)))\n  colnames(results) <- c(\"col1\", \"col2\", \"distance\")\n\n  distMat <- matrix(NA, nrow = p, ncol = p)\n  rownames(distMat) <- colnames(distMat) <- paste0(\"V\", 1:p)\n  distMat[lower.tri(distMat)] <- results$distance\n  return(as.dist(distMat))\n}\n\n#' @rdname csTest\n#' @export\npearson <- function(n, p) {\n  tc <- stats::pnorm(1 - 2 / (p * (p - 1)))\n  zprimec <- tc / sqrt(n - 1)\n  rc <- (exp(2 * zprimec) - 1) / (exp(2 * zprimec) + 1)\n  dc <- 1 - rc\n  dc\n}\n" }
{ "repo_name": "louridas/Data_Analytics_Case_SP500", "ref": "refs/heads/master", "path": "RunStudy.R", "content": "\n# Project Name: \"S&P500 Daily Stock Returns Analysis\"\n\nrm(list = ls()) # clean up the workspace\n\n######################################################################\n\n# THESE ARE THE PROJECT PARAMETERS NEEDED TO GENERATE THE REPORT\n\n# Please ENTER the name of the file with the data used. The file should contain a matrix with one row per observation (e.g. person) and one column per attribute. THE NAME OF THIS MATRIX NEEDS TO BE ProjectData (otherwise you will need to replace the name of the ProjectData variable below with whatever your variable name is, which you can see in your Workspace window after you load your file)\ndatafile_name <- \"DefaultData\" # this is the default name of the data for a project\n###########\n# DEFAULT PROJECT DATA FORMAT: File datafile_name must have a matrix called ProjectData of \n# D rows and S columns, where D is the number of days and S the number of stocks\n###########\n\n# this loads the selected data\nload(paste(\"data\", datafile_name, sep = \"/\")) # this contains only the matrix ProjectData\ncat(\"\\nVariables Loaded:\", ls(), \"\\n\")\n\n# Please ENTER the time period to use (default is 1 to nrow(ProjectData), namely all the days)\nstart_date <- 1\nend_date <- nrow(ProjectData)\n\n# Please ENTER the stocks to use (default is 1:ncol(ProjectData), namely all of them)\n# Notice: this is not an input to the Web App. You may need to use a different data file\nstocks_used <- 1:ncol(ProjectData)\n\n# Please ENTER the number of principal components to eventually use for this report\nnumb_components_used <- 3\n\n# Please ENTER 0 or 1 to de-mean or not the data in the regression estimation of the report (Default is 0)\nuse_mean_alpha <- 0\n\n# Would you like to also start a web application once the report and slides are generated?\n# 1: start application, 0: do not start it. \n# Note: starting the web application will open a new browser \n# with the application running\nstart_webapp <- 1\n\n\n######################################################################\n# Define the data used in the slides and report, and run all necessary libraries \n\nProjectData = ProjectData[start_date:end_date, stocks_used]\n\nsource(\"R/library.R\")\nsource(\"R/heatmapOutput.R\")\n\n######################################################################\n# generate the report, slides, and if needed start the web application\n\nunlink( \"TMPdirSlides\", recursive = TRUE )      \ndir.create( \"TMPdirSlides\" )\nsetwd( \"TMPdirSlides\" )\nfile.copy( \"../doc/SP500_Slides.Rmd\",\"SP500_Slides.Rmd\", overwrite = T )\nslidify( \"SP500_Slides.Rmd\" )\nfile.copy( 'SP500_Slides.html', \"../doc/SP500_Slides.html\", overwrite = T )\nsetwd( \"../\" )\nunlink( \"TMPdirSlides\", recursive = TRUE )      \n\nunlink( \"TMPdirReport\", recursive = TRUE )      \ndir.create( \"TMPdirReport\" )\nsetwd( \"TMPdirReport\" )\nfile.copy( \"../doc/SP500_Report.Rmd\",\"SP500_Report.Rmd\", overwrite = T )\nknit2html( 'SP500_Report.Rmd', quiet = TRUE )\nfile.copy( 'SP500_Report.html', \"../doc/SP500_Report.html\", overwrite = T )\nsetwd( \"../\" )\nunlink( \"TMPdirReport\", recursive = TRUE )      \n\nif (start_webapp){\n  # load all files in the data directory to have them available locally\n  load(\"data/FinancialsData\")\n  FinancialsData <- ProjectData\n  load(\"data/TechData\")\n  TechData <- ProjectData\n  load(\"data/DefaultData\")\n  MarketData <- ProjectData\n  \n  runApp(\"tools\")\n}\n  " }
{ "repo_name": "vasishth/MetaAnalysisJaegerEngelmannVasishth2017", "ref": "refs/heads/master", "path": "R/plotparameter.R", "content": "plotparameter<-function(res,col=2,xlabel='Interference effect size (ms)',\n                        ylabel='Density',yloc=0.01,\n                        title='Interference effect \\n (Target Match)'){\n  post<-data.frame(post=as.matrix(res)[,col])\n  postquantiles<-quantile(post$post,prob=c(0.025,0.975))\n  postplot<-ggplot(post, aes(x=post)) +\n    geom_histogram(aes(y = ..density..),binwidth=2,color=\"black\",fill=\"white\")+ \n    #  geom_density(fill=\"orange\", alpha = 0.2) +\n    theme_bw() +\n    xlab(xlabel) +\n    ylab(ylabel) +\n    geom_vline(xintercept=0,linetype=\"dashed\")+\n    geom_segment(aes(x = postquantiles[1], y = yloc, \n                     xend = postquantiles[2], yend = yloc),\n                 linetype=\"solid\")+\n    ggtitle(title)\n  \n  postplot\n}" }
{ "repo_name": "jw156605/SingleSplice", "ref": "refs/heads/master", "path": "fitNoiseModel.R", "content": "library(statmod)\r\nsuppressMessages(library(matrixStats))\r\n\r\nfitMeanVarModel <- function(ERCC_rpkms_size_norm,cutoff)\r\n{\r\n\tmeans = rowMeans(ERCC_rpkms_size_norm)\r\n\tvars = rowVars(ERCC_rpkms_size_norm)\r\n\tcv2 = vars/means^2\r\n\tuseForFit = which(means > cutoff)\r\n\tfit = glmgam.fit( cbind( a0 = 1, a1tilde = 1/means[useForFit] ), cv2[useForFit] )\r\n\tpdf(\"noise_model.pdf\")\r\n\tplot(means[which(means>0)],cv2[which(means>0)],log=\"xy\",main=\"Mean-Variance Relationship\",xlab=\"Mean Expression\",ylab=\"CV^2\")\r\n\txg <- 10^seq( -1, 6, length.out=100 )\r\n\tlines( xg, coefficients(fit)[\"a0\"] + coefficients(fit)[\"a1tilde\"]/xg, col=\"red\" )\r\n\tinvisible(dev.off())\r\n\treturn (c(coefficients(fit)[\"a0\"],coefficients(fit)[\"a1tilde\"]))\r\n}\r\n\r\nfitDropoutModel <- function(ERCC_rpkms_size_norm,cutoff)\r\n{\r\n\tmeans = rowMeans(ERCC_rpkms_size_norm)\r\n\tnum_dropouts = rowSums(ERCC_rpkms_size_norm==0)\r\n\tn = ncol(ERCC_rpkms_size_norm)\r\n\ttotal = rep(n,length(num_dropouts))\r\n\tdf = data.frame(num_dropouts,total)\r\n\tfit = glm(cbind(num_dropouts,total-num_dropouts) ~ log(means+1),family=binomial(logit),data=df)\r\n\tb0 = fit$coefficients[1]\r\n\tb1 = fit$coefficients[2]\r\n\tp = num_dropouts/n\r\n\tsorted_means = sort(means)\r\n\tp_pred = (1/(1+exp(-(b0+b1*log(sorted_means+1)))))\r\n\tpdf(\"dropout_model.pdf\")\r\n\tplot(means+1,p,log=\"x\",main=\"Dropout Rate\",xlab=\"Mean Expression\",ylab=\"Dropout Probability\")\r\n\tlines(sorted_means+1,p_pred,col=\"red\")\r\n\tinvisible(dev.off())\r\n\treturn (c(b0,b1))\r\n}\r\n\r\nargs <- commandArgs(trailingOnly = TRUE)\r\nERCC_rpkms_size_norm = as.matrix(read.csv(args[1],header=TRUE,row.names=1))\r\ncutoff = as.numeric(args[2])\r\n\r\nmeanVarModel = fitMeanVarModel(ERCC_rpkms_size_norm,cutoff)\r\ndropoutModel = fitDropoutModel(ERCC_rpkms_size_norm)\r\ncat(\"\",meanVarModel[1],meanVarModel[2],dropoutModel[1],dropoutModel[2],\"\\n\")" }
{ "repo_name": "JosephMcGrath/fetchrr", "ref": "refs/heads/master", "path": "R/pPetitionDetail.R", "content": "#--pPetitionDetailUri-----------------------------------------------------------\n#' @export\npPetitionDetailUri <- function(petitionId){\n    \n    base <- \"https://petition.parliament.uk/petitions/\"\n    \n    varList <- list2character(petitionId)\n    \n    ret <- mergeURI(base,\n                    varList,\n                    \".json\"\n                    )\n    \n    return(ret)\n    \n}\n\n#--pPetitionDetailRaw-----------------------------------------------------------\n#'\n#' @export\n#'\npPetitionDetailRaw <- function(petitionId){\n    \n    ret <- getPage(pPetitionDetailUri(petitionId))\n    \n    return(ret)\n    \n}\n\n#--pPetitionDetailParse---------------------------------------------------------\n#'\n#' @export\n#'\npPetitionDetailParse <- function(jsonIn){\n    \n    ret <- packString(jsonIn)\n    \n    ret <- fromJSON(ret, simplifyDataFrame = TRUE, flatten = TRUE)\n    \n    #Petition info\n    metaData <- !names(ret$data$attributes) %in%\n                    c(\"signatures_by_country\",\n                      \"signatures_by_constituency\"\n                      )\n    \n    attributeData <- unlist(ret$data$attributes[metaData])\n    attributeData <- data.frame(matrix(attributeData, nrow = 1))\n    names(attributeData) <- names(unlist(ret$data$attributes[metaData]))\n    \n    temp <- ret$data$attributes$signatures_by_country\n    voteValues <- ret$data$attributes$signatures_by_constituency\n    \n    voteValues <- rbind(voteValues,\n                        data.frame(name = temp$name,\n                                    ons_code = temp$code,\n                                    mp = NA,\n                                    signature_count = temp$signature_count\n                                    )\n                         )\n    ret <- list(attributes = attributeData, votes = voteValues)\n    \n    return(ret)\n}\n\n#--pPetitionDetail--------------------------------------------------------------\n#'\n#' @export\n#'\npPetitionDetail <- function(petitionId){\n    \n    ret <-  pPetitionDetailParse(pPetitionDetailRaw(petitionId))\n    \n    return(ret)\n    \n}\n" }
{ "repo_name": "jasenfinch/OrbiFIEproc", "ref": "refs/heads/master", "path": "R/plotBin.R", "content": "#' plotBin\n#' @rdname plotBin\n#' @description kernal density plot of a specified bin.\n#' @param x S4 object of class Binalysis\n#' @param bin 0.01amu bin to plot\n#' @param cls \\code{TRUE} or \\code{FALSE}. Should bins be plotted by class?\n#' @importFrom ggplot2 ggplot geom_density theme_bw xlim xlab ggtitle theme\n#' @importFrom ggplot2 element_text facet_wrap aes\n#' @importFrom stringr str_replace_all str_sub\n#' @export\n\nsetMethod('plotBin',signature = 'Binalysis',\n\t\t\t\t\tfunction(x,bin,cls = T){\n\t\t\t\t\t\t\n\t\t\t\t\t\tm <- bin %>%\n\t\t\t\t\t\t\tstr_replace_all('[:alpha:]','') %>%\n\t\t\t\t\t\t\tas.numeric()\n\t\t\t\t\t\tmode <- bin %>%\n\t\t\t\t\t\t\tstr_sub(1,1)\n\t\t\t\t\t\t\n\t\t\t\t\t\tdat <- x %>%\n\t\t\t\t\t\t\t.@spectra %>%\n\t\t\t\t\t\t\t.$fingerprints %>%\n\t\t\t\t\t\t\tfilter(Mode == mode & Bin == m)\n\t\t\t\t\t\t\n\t\t\t\t\t\tpl <- ggplot(dat,aes(x = mz)) +\n\t\t\t\t\t\t\tgeom_density() +\n\t\t\t\t\t\t\ttheme_bw() +\n\t\t\t\t\t\t\txlim(m - 0.005,m + 0.005) +\n\t\t\t\t\t\t\ttheme(plot.title = element_text(face = 'bold'),\n\t\t\t\t\t\t\t\t\t\taxis.title.y = element_text(face = 'bold'),\n\t\t\t\t\t\t\t\t\t\taxis.title.x = element_text(face = 'bold.italic'),\n\t\t\t\t\t\t\t\t\t\taxis.text.x = element_text(angle = 90,hjust = 1)) +\n\t\t\t\t\t\t\tlabs(title = bin,\n\t\t\t\t\t\t\t\t\t x = 'm/z',\n\t\t\t\t\t\t\t\t\t y = 'Density')\n\t\t\t\t\t\t\n\t\t\t\t\t\tif (cls == T) {\n\t\t\t\t\t\t\tpl <- pl +\n\t\t\t\t\t\t\t\tfacet_wrap(~Class)\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn(pl)\n\t\t\t\t\t}\n)\n" }
{ "repo_name": "arity-r/pRobability", "ref": "refs/heads/master", "path": "chap02/BertrandsParadox.R", "content": "\n#install.packages('plotrix')\nrequire(plotrix) # for drawing circle\n\n#' Example 2.6 (Bertrand's Paradox)\nn = 10000\n\n#' Simulation 1\n#' We choose values for x and y from [-1, 1] at random.\nx = c()\ny = c()\nwhile (length(x) < n) {\n  tmpx = runif(1) * 2 - 1\n  tmpy = runif(1) * 2 - 1\n  r = tmpx^2 + tmpy^2\n  # Is this point on the circle?\n  if (r < 1) {\n    x = c(x, tmpx)\n    y = c(y, tmpy)\n  }\n}\nL = 2 * sqrt(1-(x^2+y^2))\nres1 = length(L[L >= sqrt(3)])\n\n#' Simulation 2\n#' We take account of the fact that any rotation of\n#' the circle does not change the length of the chord,\n#' so we might as well assume in advance that\n#' the chord is horizontal.\n#' Then we choose r from [-1, 1] at random.\nr = runif(n) * 2 - 1\nL = 2 * sqrt(1 - r^2)\nres2 = length(L[L >= sqrt(3)])\n\n#' Simulation 3\n#' We assume that one endpoint, say B, lies at (1, 0)\n#' (i.e. that beta=0). Then we choose a value for alpha\n#' from [0, 2pi] at random and compute the length of\n#' the resulting chord.\nalpha = runif(n) * 2 * pi\nL = sqrt(2 - 2 * cos(alpha))\nres3 = length(L[L >= sqrt(3)])\n\npar(mfrow=c(2,3))\n\n# Drawing first circle\nplot(c(), c(), axes=F, xlim=c(-1,1), ylim=c(-1,1), xlab='', ylab='', asp=1)\ndraw.circle(0, 0, 1, border='black')\nfor (i in seq(1, n, 100)) {\n  sx = x[i]^2\n  sy = y[i]^2\n  sr = sx + sy\n  if (x[i] * y[i] < 0) {\n    lines(c(x[i] + sqrt(sx-sr+sy/sr), x[i] - sqrt(sx-sr+sy/sr)),\n          c(y[i] + sqrt(sy-sr+sx/sr), y[i] - sqrt(sy-sr+sx/sr)),\n          col='skyblue', asp=1)\n  } else {\n    lines(c(x[i] - sqrt(sx-sr+sy/sr), x[i] - sqrt(sx-sr+sy/sr)),\n          c(y[i] + sqrt(sy-sr+sx/sr), y[i] + sqrt(sy-sr+sx/sr)),\n          col='skyblue', asp=1)\n  }\n}\n\n# Drawing second circle\nplot(c(), c(), axes=F, xlim=c(-1,1), ylim=c(-1,1), xlab='', ylab='', asp=1)\ndraw.circle(0, 0, 1, border='black')\nfor (i in seq(1, n, 100)) {\n  lines(c(-sqrt(1-r[i]^2), sqrt(1-r[i]^2)),\n        c(r[i], r[i]),\n        col='skyblue', asp=1)\n}\n\n# Drawing third circle\nplot(c(), c(), axes=F, xlim=c(-1,1), ylim=c(-1,1), xlab='', ylab='', asp=1)\ndraw.circle(0, 0, 1, border='black')\nfor (i in seq(1, n, 100)) {\n  lines(c(1, cos(alpha[i])),\n        c(0, sin(alpha[i])),\n        col='skyblue', asp=1)\n}\n\n# Drawing bars\nbarplot(res1/n, ylim=c(0, 1), main=n)\nbarplot(res2/n, ylim=c(0, 1), main=n)\nbarplot(res3/n, ylim=c(0, 1), main=n)\n" }
{ "repo_name": "auranic/PQSQ-DataApproximators", "ref": "refs/heads/master", "path": "test_data/R testers/DataBaseTest.R", "content": "#Test all\n\n#Resulting file are written to the current directory and \n#has name Test_name + ext and contain four column. \n#The first column contains number of PCS, the second column \n#contains time in seconds, the third column contains L1 error\n#and the fourth column contains L2 error.\n\n#load used libraries\n#load(\"pcaL1\")\n#load(\"pcaPP\")\n\n#Define required values\nTest_name = \"wdbc\"\n#meth = 1 #1 is pcal1, 2 is l1pca, 3 is l2pca, 4 is l1pca*, 5 is pcaPP\nnnorm = ncol(test)*nrow(test)\n\n#Create array for results\nres = matrix(0.0, nrow=10, ncol=4)\n\nfor (meth in 1:5){\n\tswitch(meth,\n\t{ nRep = 1000\t#pcal1\n\t  ext = \"_pcaL1.txt\"},\n\t{ nRep = 5\t#l1pca\n\t  ext = \"_l1pca.txt\"},\n\t{ nRep = 5000\t#l2pca\n\t  ext = \"_l2pca.txt\"},\n\t{ nRep = 2\t#l1pca*\n\t  ext = \"_l1pcaS.txt\"},\n\t{ nRep = 25\t#pcaPP\n\t  ext = \"_pcaPP.txt\"})\n\n\t#Number of PC loop\n\tfor (pc in 1:10){\n\t\t#Estimate number of repetitions\n\t\t#Save initial time\n\t\ttim1 = proc.time()[3]\n\t\tswitch(meth,\n\t\t{ #pcal1\n\t\t\tcentre = colMedians(test)\n\t\t\ttest1 = t(t(test)-centre)\n\t\t\tmyres <- pcal1(test1, projDim=pc, center=FALSE, scores=TRUE, projPoints=TRUE, dispExp=TRUE, initialize=\"l2pca\")\n\t\t\tmyres$projPoints = t(t(myres$projPoints)+centre)},\n\t\t{ #l1pca\n\t\t\tcentre = colMedians(test)\n\t\t\ttest1 = t(t(test)-centre)\n\t\t\tmyres <- l1pca(test1, projDim=pc, center=FALSE, projPoints=TRUE, initialize=\"l2pca\", tolerance=0.0001, iterations=10)\n\t\t\tmyres$projPoints = t(t(myres$projPoints)+centre)},\n\t\t{ #l2pca\n\t\t\t#For L2 we have no one function and use small script\n\t\t\t#Centralize\n\t\t\tcentre = colMeans(test)\n\t\t\ttest1 = t(t(test)-centre)\n\t\t\t#Compute PCs\n\t\t\tmyres =svd(test1, nu = pc, nv = pc)\n\t\t\t#Compute reconstruction\n\t\t\ttest1=myres$u %*% diag(myres$d[1:pc],pc,pc) %*% t(myres$v)\n\t\t\tmyres$projPoints = t(t(test1)+centre)},\n\t\t{ #l1pca*\n\t\t\tcentre = colMedians(test)\n\t\t\ttest1 = t(t(test)-centre)\n\t\t\tmyres = l1pcastar(test1, projDim=pc, center=FALSE, scores=TRUE, projPoints=TRUE, dispExp=TRUE)\n\t\t\tmyres$projPoints = t(t(myres$projPoints)+centre)},\n\t\t{ #pcaPP\n\t\t\t#Calculate projects because it is nor calculated in PCAgrid\n\t\t\tmyres = PCAgrid (test, k = pc, method = c (\"mad\", \"sd\", \"qn\"), maxiter = 10, splitcircle = 25, scores = TRUE, zero.tol = 1e-16, center = l1median, trace = 0, store.call = TRUE)\n\t\t\t#Compute reconstruction\n\t\t\ttest1=myres$scores %*% t(myres$loadings)\n\t\t\tmyres$projPoints = t(t(test1)+myres$center)}\n\t\t)\n\t\t#Save final time\n\t\ttim2 = proc.time()[3]\n\t\ttims = tim2 - tim1\n\t\tif (tims<0.0001)\n\t\t\tnRep = 10000\n\t\telse if (tims>=1)\n\t\t\tnRep = 1\n\t\telse\n\t\t\tnRep = round(1/tims)\n\t\tend\n\n\t\tprint(cat(\"#of pc\",pc,\"Singular time\",tims,\"nRep\",nRep,\"   \"))\n\n\t\tif (nRep>1){\n\t\t\t#Real estimations\n\t\t\t#Save initial time\n\t\t\ttim1 = proc.time()[3]\n\t\t\tfor (rep in 1:nRep){\n\t\t\t\tswitch(meth,\n\t\t\t\t{ #pcal1\n\t\t\t\t\tcentre = colMedians(test)\n\t\t\t\t\ttest1 = t(t(test)-centre)\n\t\t\t\t\tmyres <- pcal1(test1, projDim=pc, center=FALSE, scores=TRUE, projPoints=TRUE, dispExp=TRUE, initialize=\"l2pca\")\n\t\t\t\t\tmyres$projPoints = t(t(myres$projPoints)+centre)},\n\t\t\t\t{ #l1pca\n\t\t\t\t\tcentre = colMedians(test)\n\t\t\t\t\ttest1 = t(t(test)-centre)\n\t\t\t\t\tmyres <- l1pca(test1, projDim=pc, center=FALSE, projPoints=TRUE, initialize=\"l2pca\", tolerance=0.0001, iterations=10)\n\t\t\t\t\tmyres$projPoints = t(t(myres$projPoints)+centre)},\n\t\t\t\t{ #l2pca\n\t\t\t\t\t#For L2 we have no one function and use small script\n\t\t\t\t\t#Centralize\n\t\t\t\t\tcentre = colMeans(test)\n\t\t\t\t\ttest1 = t(t(test)-centre)\n\t\t\t\t\t#Compute PCs\n\t\t\t\t\tmyres =svd(test1, nu = pc, nv = pc)\n\t\t\t\t\t#Compute reconstruction\n\t\t\t\t\ttest1=myres$u %*% diag(myres$d[1:pc],pc,pc) %*% t(myres$v)\n\t\t\t\t\tmyres$projPoints = t(t(test1)+centre)},\n\t\t\t\t{ #l1pca*\n\t\t\t\t\tcentre = colMedians(test)\n\t\t\t\t\ttest1 = t(t(test)-centre)\n\t\t\t\t\tmyres = l1pcastar(test1, projDim=pc, center=FALSE, scores=TRUE, projPoints=TRUE, dispExp=TRUE)\n\t\t\t\t\tmyres$projPoints = t(t(myres$projPoints)+centre)},\n\t\t\t\t{ #pcaPP\n\t\t\t\t\t#Calculate projects because it is nor calculated in PCAgrid\n\t\t\t\t\tmyres = PCAgrid (test, k = pc, method = c (\"mad\", \"sd\", \"qn\"), maxiter = 10, splitcircle = 25, scores = TRUE, zero.tol = 1e-16, center = l1median, trace = 0, store.call = TRUE)\n\t\t\t\t\t#Compute reconstruction\n\t\t\t\t\ttest1=myres$scores %*% t(myres$loadings)\n\t\t\t\t\tmyres$projPoints = t(t(test1)+myres$center)}\n\t\t\t\t)\n\t\t\t}\n\t\t\t#Save final time\n\t\t\ttim2 = proc.time()[3]\n\t\t}\n\t\t#Save results\n\t\tres[pc,1] = pc\n\t\t#Save used time to array\n\t\tres[pc,2] = (tim2-tim1)/nRep\n\t\t#calculate L1 error of reconstruction\n\t\tres[pc,3]=sum(abs(myres$projPoints-test))/nnorm\n\t\t#calculate L2 error of reconstruction\n\t\tres[pc,4]=var(as.vector(myres$projPoints-test))\n\t}\n\n\t#Save final file\n\twrite.table(res, file=paste(Test_name,ext,sep=\"\"), row.names=FALSE, col.names=FALSE)\n}\n" }
{ "repo_name": "cran/magclass", "ref": "refs/heads/master", "path": "R/is.magpie.R", "content": "#' @importFrom methods is\n#' @export\nis.magpie <- function(x){\n  return(is(x,\"magpie\"))\n}" }
{ "repo_name": "pik-piam/magclass", "ref": "refs/heads/master", "path": "R/is.magpie.R", "content": "#' @importFrom methods is\n#' @export\nis.magpie <- function(x){\n  return(is(x,\"magpie\"))\n}" }
{ "repo_name": "pchmieli/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/demos/rdemo.workflow.R", "content": "library(h2o)\nh2o.init\n\nhex <- as.h2o(iris, destination_frame = \"iris\")\nk <- 3\nsetSeed <- 148008988978\n\nprint('Build kmeans model on petal length and width...')\niris_model <- h2o.kmeans(training_frame = hex, x = 1:4, k = k, seed = setSeed)\nprint(iris_model)\nprint(paste('Mean squared error : ', iris_model@model$mse))\nprint('Build kmeans model, cheating with species input...')\n# expect_error(h2o.kmeans (training_frame = hex, x = 1:5, k = k, seed = setSeed))\niris_model_wSpecies <- h2o.kmeans (training_frame = hex, x = 1:5, k = k, seed = setSeed)\nprint(iris_model_wSpecies)\nprint(paste('Mean squared error : ', iris_model_wSpecies@model$mse))\n\nprint('Predict on the same iris dataset...')\npred1.R <- as.data.frame(predict(object = iris_model, newdata = hex))\n# pred2.R <- as.data.frame(predict(object = iris_model_wSpecies, newdata = hex))\n\nprint('Print confusion matrix...')\nspecies.R <- iris$Species\n\nMode <- function(x) {\n      ux <- unique(x)\n      ux[which.max(tabulate(match(x, ux)))]\n}\n\nconfusion_matrix <- function(pred){\n      assignments <- sapply(c(0, 1, 2), function(id) Mode(species.R[pred == id]))\n      foo <- function(x) {\n                if(x == assignments[1]) 0\n                else if(x == assignments[2]) 1\n                else 2\n              }\n      species1.R <- unlist(lapply(species.R, foo))\n\n      cm <- matrix(0, nrow = 3, ncol = 3)\n      for (i in 1:length(species.R)) {\n        row_id <- species1.R[i]+1\n        col_id <- pred[,1][i]+1\n        cm[row_id, col_id] <- cm[row_id, col_id] + 1\n      }\n      cm <- as.data.frame(cm)\n      names(cm) <- assignments\n      row.names(cm) <- assignments\n      print(cm)\n}\nconfusion_matrix(pred1.R)\n# confusion_matrix(pred2.R)" }
{ "repo_name": "madmax983/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/demos/rdemo.workflow.R", "content": "library(h2o)\nh2o.init\n\nhex <- as.h2o(iris, destination_frame = \"iris\")\nk <- 3\nsetSeed <- 148008988978\n\nprint('Build kmeans model on petal length and width...')\niris_model <- h2o.kmeans(training_frame = hex, x = 1:4, k = k, seed = setSeed)\nprint(iris_model)\nprint(paste('Mean squared error : ', iris_model@model$mse))\nprint('Build kmeans model, cheating with species input...')\n# expect_error(h2o.kmeans (training_frame = hex, x = 1:5, k = k, seed = setSeed))\niris_model_wSpecies <- h2o.kmeans (training_frame = hex, x = 1:5, k = k, seed = setSeed)\nprint(iris_model_wSpecies)\nprint(paste('Mean squared error : ', iris_model_wSpecies@model$mse))\n\nprint('Predict on the same iris dataset...')\npred1.R <- as.data.frame(predict(object = iris_model, newdata = hex))\n# pred2.R <- as.data.frame(predict(object = iris_model_wSpecies, newdata = hex))\n\nprint('Print confusion matrix...')\nspecies.R <- iris$Species\n\nMode <- function(x) {\n      ux <- unique(x)\n      ux[which.max(tabulate(match(x, ux)))]\n}\n\nconfusion_matrix <- function(pred){\n      assignments <- sapply(c(0, 1, 2), function(id) Mode(species.R[pred == id]))\n      foo <- function(x) {\n                if(x == assignments[1]) 0\n                else if(x == assignments[2]) 1\n                else 2\n              }\n      species1.R <- unlist(lapply(species.R, foo))\n\n      cm <- matrix(0, nrow = 3, ncol = 3)\n      for (i in 1:length(species.R)) {\n        row_id <- species1.R[i]+1\n        col_id <- pred[,1][i]+1\n        cm[row_id, col_id] <- cm[row_id, col_id] + 1\n      }\n      cm <- as.data.frame(cm)\n      names(cm) <- assignments\n      row.names(cm) <- assignments\n      print(cm)\n}\nconfusion_matrix(pred1.R)\n# confusion_matrix(pred2.R)" }
{ "repo_name": "ChristosChristofidis/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_demos/runit_demo_VI_all_algos.R", "content": "# This Demo shows how to access Variable Importance from different H2O algorithms namely, GBM, Random Forest, GLM, Deeplearning.\n# Data source: Data is obtained from -https://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n# Expectation:  The predictor \"duration\" should be picked as the most important variable by all algos\n\nsetwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource('../h2o-runit.R')\n\ntest <- function(h) {\n\n# If you want to cut and paste code from this test, you can just create the connection yourself up front.\n# h = h2o.init()\n\n# Parse data into H2O\nprint(\"Parsing data into H2O\")\n# From an h2o git workspace.\ndata.hex <- h2o.importFile(h, locate(\"smalldata/demos/bank-additional-full.csv\"), destination_frame=\"data\")\n# Or directly from github.\n# data.hex = h2o.importFile(h, path = \"https://raw.github.com/0xdata/h2o/master/smalldata/bank-additional-full.csv\", destination_frame=\"data.hex\")\n\nprint(\"Expectation: All Algos should pick the predictor - 'duration' as the most important variable\")\n\n# Run summary\nsummary(data.hex)\n\n#Print Column names\ncolnames(data.hex)\n\n# Specify predictors and response\nmyX <- 1:20\nmyY <- \"y\"\n\n#--------------------------------------------------\n# Run GBM with variable importance\nmy.gbm <- h2o.gbm(x = myX, y = myY, distribution = \"bernoulli\", training_frame = data.hex,\n                  ntrees =100, max_depth = 2, learn_rate = 0.01)\n\n# Access Variable Importance from the built model\nprint(\"Variable importance from GBM\")\ngbm.VI <- h2o.varimp(my.gbm)\nprint(gbm.VI)\n\n\n# Plot variable importance from GBM\npar(mfrow=c(2,2))\nbarplot(gbm.VI$scaled_importance, names.arg = gbm.VI$variable, las=2,main=\"VI from GBM\")\n\n#--------------------------------------------------\n# Run random Forest with variable importance\nmy.rf <- h2o.randomForest(x=myX,y=myY,training_frame=data.hex,ntrees=100)\n\n# Access Variable Importance from the built model\nprint(\"Variable importance from Random Forest\")\nrf.VI <- h2o.varimp(my.rf)\nprint(rf.VI)\n\n# RF variable importance Without normalization, i.e scale = F\nprint(\"Variable importance from Random Forest without normalization\")\nprint(t(rf.VI[,1:2]))\n\n# RF variable importance With normalization, i.e scale =T (divide mean decrease accuracy by standard deviation)\nnorm_rf.VI <- rf.VI$relative_importance/max(rf.VI$relative_importance)\nprint(\"Variable importance from Random Forest with normalization\")\nprint(t(rf.VI[,c(1,3)]))\ncheckEqualsNumeric(norm_rf.VI, rf.VI$scaled_importance)\n\n# Plot variable importance from Random Forest\nbarplot(rf.VI$scaled_importance,beside=T,names.arg=rf.VI$variable,las=2,main=\"VI from RF\")\n\n#--------------------------------------------------\n# Run GLM with variable importance, lambda search and using all factor levels\nmy.glm <- h2o.glm(x=myX, y=myY, training_frame=data.hex, family=\"binomial\",standardize=T,\n  lambda_search=T)\n\n# Select the best model picked by glm\n# best_model <- my.glm@best_model\n\n# Get the normalized coefficients of the best model\n#n_coeff <- abs(my.glm@models[[best_model]]@model$normalized_coefficients)\nglm.VI <- my.glm@model$standardized_coefficients_magnitude\nprint(\"Variable importance from GLM\")\nprint(glm.VI)\n\n# Plot variable importance from glm\nbarplot(glm.VI[1:20,]$coefficients, names.arg = glm.VI[1:20,]$names, las=2,main=\"VI from GLM\")\n\n#--------------------------------------------------\n# Run deeplearning with variable importance\nmy.dl <- h2o.deeplearning(x = myX, y = myY, training_frame = data.hex,\n                          activation = \"Tanh\", hidden = c(10,10,10),\n                          epochs = 12, variable_importances = T)\n\n# Access Variable Importance from the built model\nprint(\"Variable importance from Deep Learning\")\ndl.VI <- my.dl@model$variable_importances\nprint(dl.VI)\n\n# Plot variable importance from deeplearing\nbarplot(dl.VI$scaled_importance[1:20], names.arg = dl.VI$variable[1:20], las=2,main=\"VI from Deep Learning\")\n\ntestEnd()\n}\n\ndoTest(\"Plot to compare the Variable Importance as predicted by different algorithms on the bank-marketing dataset\", test)\n\n" }
{ "repo_name": "weaver-viii/h2o-3", "ref": "refs/heads/master", "path": "h2o-r/tests/testdir_demos/runit_demo_VI_all_algos.R", "content": "# This Demo shows how to access Variable Importance from different H2O algorithms namely, GBM, Random Forest, GLM, Deeplearning.\n# Data source: Data is obtained from -https://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n# Expectation:  The predictor \"duration\" should be picked as the most important variable by all algos\n\nsetwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$\"f\")))\nsource('../h2o-runit.R')\n\ntest <- function(h) {\n\n# If you want to cut and paste code from this test, you can just create the connection yourself up front.\n# h = h2o.init()\n\n# Parse data into H2O\nprint(\"Parsing data into H2O\")\n# From an h2o git workspace.\ndata.hex <- h2o.importFile(h, locate(\"smalldata/demos/bank-additional-full.csv\"), destination_frame=\"data\")\n# Or directly from github.\n# data.hex = h2o.importFile(h, path = \"https://raw.github.com/0xdata/h2o/master/smalldata/bank-additional-full.csv\", destination_frame=\"data.hex\")\n\nprint(\"Expectation: All Algos should pick the predictor - 'duration' as the most important variable\")\n\n# Run summary\nsummary(data.hex)\n\n#Print Column names\ncolnames(data.hex)\n\n# Specify predictors and response\nmyX <- 1:20\nmyY <- \"y\"\n\n#--------------------------------------------------\n# Run GBM with variable importance\nmy.gbm <- h2o.gbm(x = myX, y = myY, distribution = \"bernoulli\", training_frame = data.hex,\n                  ntrees =100, max_depth = 2, learn_rate = 0.01)\n\n# Access Variable Importance from the built model\nprint(\"Variable importance from GBM\")\ngbm.VI <- h2o.varimp(my.gbm)\nprint(gbm.VI)\n\n\n# Plot variable importance from GBM\npar(mfrow=c(2,2))\nbarplot(gbm.VI$scaled_importance, names.arg = gbm.VI$variable, las=2,main=\"VI from GBM\")\n\n#--------------------------------------------------\n# Run random Forest with variable importance\nmy.rf <- h2o.randomForest(x=myX,y=myY,training_frame=data.hex,ntrees=100)\n\n# Access Variable Importance from the built model\nprint(\"Variable importance from Random Forest\")\nrf.VI <- h2o.varimp(my.rf)\nprint(rf.VI)\n\n# RF variable importance Without normalization, i.e scale = F\nprint(\"Variable importance from Random Forest without normalization\")\nprint(t(rf.VI[,1:2]))\n\n# RF variable importance With normalization, i.e scale =T (divide mean decrease accuracy by standard deviation)\nnorm_rf.VI <- rf.VI$relative_importance/max(rf.VI$relative_importance)\nprint(\"Variable importance from Random Forest with normalization\")\nprint(t(rf.VI[,c(1,3)]))\ncheckEqualsNumeric(norm_rf.VI, rf.VI$scaled_importance)\n\n# Plot variable importance from Random Forest\nbarplot(rf.VI$scaled_importance,beside=T,names.arg=rf.VI$variable,las=2,main=\"VI from RF\")\n\n#--------------------------------------------------\n# Run GLM with variable importance, lambda search and using all factor levels\nmy.glm <- h2o.glm(x=myX, y=myY, training_frame=data.hex, family=\"binomial\",standardize=T,\n  lambda_search=T)\n\n# Select the best model picked by glm\n# best_model <- my.glm@best_model\n\n# Get the normalized coefficients of the best model\n#n_coeff <- abs(my.glm@models[[best_model]]@model$normalized_coefficients)\nglm.VI <- my.glm@model$standardized_coefficients_magnitude\nprint(\"Variable importance from GLM\")\nprint(glm.VI)\n\n# Plot variable importance from glm\nbarplot(glm.VI[1:20,]$coefficients, names.arg = glm.VI[1:20,]$names, las=2,main=\"VI from GLM\")\n\n#--------------------------------------------------\n# Run deeplearning with variable importance\nmy.dl <- h2o.deeplearning(x = myX, y = myY, training_frame = data.hex,\n                          activation = \"Tanh\", hidden = c(10,10,10),\n                          epochs = 12, variable_importances = T)\n\n# Access Variable Importance from the built model\nprint(\"Variable importance from Deep Learning\")\ndl.VI <- my.dl@model$variable_importances\nprint(dl.VI)\n\n# Plot variable importance from deeplearing\nbarplot(dl.VI$scaled_importance[1:20], names.arg = dl.VI$variable[1:20], las=2,main=\"VI from Deep Learning\")\n\ntestEnd()\n}\n\ndoTest(\"Plot to compare the Variable Importance as predicted by different algorithms on the bank-marketing dataset\", test)\n\n" }
{ "repo_name": "cran/hydroPSO", "ref": "refs/heads/master", "path": "R/plot_2parOF.R", "content": "# File plot_2parOF.R\n# Part of the hydroPSO R package, http://www.rforge.net/hydroPSO/ ; \n#                                 http://cran.r-project.org/web/packages/hydroPSO\n# Copyright 2010-2012 Mauricio Zambrano-Bigiarini & Rodrigo Rojas\n# Distributed under GPL 2 or later\n\n################################################################################\n#                               'plot_2parOF'                                  #\n################################################################################\n# Author : Mauricio Zambrano Bigarini                                          #\n# Started: Nov 30th, 2010                                                      #      \n# Updates: 17-Jan-2010 ; 25-Jan-2011                                           #\n#          15-Feb-2012 ; 08-Mar-2012 ; 23-Mar-2012 ; 20-Nov-2012               #\n################################################################################\n# Purpose: For two user-defined parameters, it plots the values of the         #\n#          objective funtion in a two dimensional box, where the boundaries    # \n#          of each parameter are used as axis.                                 #\n################################################################################\n\n# params : matrix or data.frame with the parameter sets used during calibration\n# p1.name: character with the name of the 1st parameter to be plotted\n# p2.name: character with the name of the 2nd parameter to be plotted\n# type   : character, indicating the type of plot. Valid values are: \n#          -) \"sp\"       : spatial plot\n#          -) \"scatter3d\": 3d scatterogram\n# auto.key : logical, indicating if the legend has to be drawn or not\n# key.space: character,indicating the position of the legend with respect to the plot\n#\nplot_2parOF <- function(params, \n                        gofs,\n                        p1.name, \n                        p2.name, \n                        type=\"sp\", \n                        MinMax=c(\"min\", \"max\"),\n                        gof.name=\"GoF\", \n                        main=paste(gof.name, \"Surface\"),\n                        GOFcuts,\n                        colorRamp= colorRampPalette(c(\"darkred\", \"red\", \"orange\", \"yellow\", \"green\", \"darkgreen\", \"cyan\")),\n                        points.cex=0.7, \n                        alpha=0.65,\n                        axis.rot=c(0, 0),\n                        auto.key=TRUE, \n                        key.space= \"right\"\n                        ) {\n\n    # Checking 'params'\n    if (missing(params)) \n      stop(\"Missing argument: 'params' must be provided !!\" )\n\n    # Number of parameter sets\n    n <- nrow(params)\n    \n    # Checking 'gofs'\n    if (missing(gofs)) {\n      stop(\"Missing argument: 'gofs' must be provided !!\" )\n    } else if (length(gofs) != n)\n        stop(\"Invalid argument: 'length(gofs) != nrow(params)' (\", length(gofs), \"!=\", n, \") !!\" )    \n        \n    # Setting 'MinMax' \n    MinMax <- match.arg(MinMax)    \n\n    # If the user provided 'p1.name', it checks that the field 'p1.name' exists in 'params'\n    if (!missing(p1.name)) {\n      if ( !(p1.name %in% colnames(params)) )\n      stop(\"Invalid argument: The field '\", p1.name, \"' doesn't exist in 'params'\")\n    } # IF end\n\n    # If the user provided 'p2.name', it checks that the field 'p2.name' exists in 'params'\n    if (!missing(p2.name)) {\n      if ( !(p2.name %in% colnames(params)) )\n      stop(\"Invalid argument: The field '\", p2.name, \"' doesn't exist in 'params'\")\n    } # IF end\n\n    # Checking the value of 'type'\n    if (is.na(match(type, c(\"sp\", \"scatter3d\") ) ) ) {\n     stop( \"Invalid argument: 'type' must be in c('sp', 'scatter3d')\" )\n    } else if (type==\"scatter3d\") {  \n            if  ( length(find.package(\"scatter3d\", quiet=TRUE)) == 0 )  {\n               warning(\"Package 'scatter3d' is not installed =>  type='sp'\")\n               type <- \"sp\"\n            } # IF end \n         } # IF end\n\n    # Selecting only the 2 parameters provided by the user + the objective function\n    p              <- data.frame(params[ ,c(p1.name, p2.name)], GoF= gofs)\n    colnames(p)[3] <- gof.name\n    \n    # Ordering parameter sets to have the best ones over the rest\n    #ifelse(MinMax==\"max\", decreasing<-FALSE, decreasing<-TRUE)\n    decreasing <- TRUE\n    if (MinMax==\"max\") decreasing <- FALSE\n    p <- p[order(p[, gof.name], decreasing = decreasing), ]\n    \n    if (type==\"sp\") {\n      colnames(p) <- c(\"x\", \"y\", gof.name)\n      \n      # If the user didn't provide 'GOFcuts', the 5 quantiles are used\n      if (missing(GOFcuts)) \n         GOFcuts <- unique(fivenum(as.numeric(p[, gof.name]), na.rm=TRUE))\n      \n      sp::coordinates(p) <- ~ x+y\n      \n      # Reduced margins among figures. From: https://stat.ethz.ch/pipermail/r-help/2007-January/123556.html\n      theme.novpadding <- list(\n                              layout.heights= list(\n                                         top.padding = 2, main.key.padding = 0,\n                                 \t key.axis.padding= 0, axis.xlab.padding= 0,\n                                 \t xlab.key.padding= 0, key.sub.padding= 0,\n                                 \t bottom.padding= 0\n                                 \t ),\n                                layout.widths=list(\n                                         left.padding= 3, key.ylab.padding= 0,\n                         \t         ylab.axis.padding= 0, axis.key.padding=0,\n                         \t         right.padding= 0\n                         \t         )\n                                )\n      sp::spplot(p, scales=list(draw=TRUE, cex=0.75, tick.number=4, x=list(rot=axis.rot[1]), \n                 y= list(rot=axis.rot[2]) ), cuts=GOFcuts, col.regions=colorRamp(13), \n                 aspect=\"fill\", auto.key=auto.key, key.space= key.space, \n                 xlab=list(p1.name, font=2), ylab=list(p2.name, font=2), \n                 main=main, cex=points.cex, alpha=alpha, \n                 par.settings = theme.novpadding)\n  \n  } else if (type==\"scatter3d\") {\n\n        x <- params[1:n,p1.name]\n        y <- params[1:n,p2.name]\n        z <- params[1:n,gof.name]\n\n        scatterplot3d::scatterplot3d(x, y, z, highlight.3d=TRUE, col.axis=\"grey\", \n                      type=\"p\", col.grid=\"lightblue\", pch=20, main=main,\n                      xlab=p1.name, ylab=p2.name, zlab=gof.name)\n    } # IF end\n\n} # 'plot_2parOF' END\n" }
{ "repo_name": "hzambran/hydroPSO", "ref": "refs/heads/master", "path": "R/plot_2parOF.R", "content": "# File plot_2parOF.R\n# Part of the hydroPSO R package, http://www.rforge.net/hydroPSO/ ; \n#                                 http://cran.r-project.org/web/packages/hydroPSO\n# Copyright 2010-2012 Mauricio Zambrano-Bigiarini & Rodrigo Rojas\n# Distributed under GPL 2 or later\n\n################################################################################\n#                               'plot_2parOF'                                  #\n################################################################################\n# Author : Mauricio Zambrano Bigarini                                          #\n# Started: Nov 30th, 2010                                                      #      \n# Updates: 17-Jan-2010 ; 25-Jan-2011                                           #\n#          15-Feb-2012 ; 08-Mar-2012 ; 23-Mar-2012 ; 20-Nov-2012               #\n################################################################################\n# Purpose: For two user-defined parameters, it plots the values of the         #\n#          objective funtion in a two dimensional box, where the boundaries    # \n#          of each parameter are used as axis.                                 #\n################################################################################\n\n# params : matrix or data.frame with the parameter sets used during calibration\n# p1.name: character with the name of the 1st parameter to be plotted\n# p2.name: character with the name of the 2nd parameter to be plotted\n# type   : character, indicating the type of plot. Valid values are: \n#          -) \"sp\"       : spatial plot\n#          -) \"scatter3d\": 3d scatterogram\n# auto.key : logical, indicating if the legend has to be drawn or not\n# key.space: character,indicating the position of the legend with respect to the plot\n#\nplot_2parOF <- function(params, \n                        gofs,\n                        p1.name, \n                        p2.name, \n                        type=\"sp\", \n                        MinMax=c(\"min\", \"max\"),\n                        gof.name=\"GoF\", \n                        main=paste(gof.name, \"Surface\"),\n                        GOFcuts,\n                        colorRamp= colorRampPalette(c(\"darkred\", \"red\", \"orange\", \"yellow\", \"green\", \"darkgreen\", \"cyan\")),\n                        points.cex=0.7, \n                        alpha=0.65,\n                        axis.rot=c(0, 0),\n                        auto.key=TRUE, \n                        key.space= \"right\"\n                        ) {\n\n    # Checking 'params'\n    if (missing(params)) \n      stop(\"Missing argument: 'params' must be provided !!\" )\n\n    # Number of parameter sets\n    n <- nrow(params)\n    \n    # Checking 'gofs'\n    if (missing(gofs)) {\n      stop(\"Missing argument: 'gofs' must be provided !!\" )\n    } else if (length(gofs) != n)\n        stop(\"Invalid argument: 'length(gofs) != nrow(params)' (\", length(gofs), \"!=\", n, \") !!\" )    \n        \n    # Setting 'MinMax' \n    MinMax <- match.arg(MinMax)    \n\n    # If the user provided 'p1.name', it checks that the field 'p1.name' exists in 'params'\n    if (!missing(p1.name)) {\n      if ( !(p1.name %in% colnames(params)) )\n      stop(\"Invalid argument: The field '\", p1.name, \"' doesn't exist in 'params'\")\n    } # IF end\n\n    # If the user provided 'p2.name', it checks that the field 'p2.name' exists in 'params'\n    if (!missing(p2.name)) {\n      if ( !(p2.name %in% colnames(params)) )\n      stop(\"Invalid argument: The field '\", p2.name, \"' doesn't exist in 'params'\")\n    } # IF end\n\n    # Checking the value of 'type'\n    if (is.na(match(type, c(\"sp\", \"scatter3d\") ) ) ) {\n     stop( \"Invalid argument: 'type' must be in c('sp', 'scatter3d')\" )\n    } else if (type==\"scatter3d\") {  \n            if  ( length(find.package(\"scatter3d\", quiet=TRUE)) == 0 )  {\n               warning(\"Package 'scatter3d' is not installed =>  type='sp'\")\n               type <- \"sp\"\n            } # IF end \n         } # IF end\n\n    # Selecting only the 2 parameters provided by the user + the objective function\n    p              <- data.frame(params[ ,c(p1.name, p2.name)], GoF= gofs)\n    colnames(p)[3] <- gof.name\n    \n    # Ordering parameter sets to have the best ones over the rest\n    #ifelse(MinMax==\"max\", decreasing<-FALSE, decreasing<-TRUE)\n    decreasing <- TRUE\n    if (MinMax==\"max\") decreasing <- FALSE\n    p <- p[order(p[, gof.name], decreasing = decreasing), ]\n    \n    if (type==\"sp\") {\n      colnames(p) <- c(\"x\", \"y\", gof.name)\n      \n      # If the user didn't provide 'GOFcuts', the 5 quantiles are used\n      if (missing(GOFcuts)) \n         GOFcuts <- unique(fivenum(as.numeric(p[, gof.name]), na.rm=TRUE))\n      \n      sp::coordinates(p) <- ~ x+y\n      \n      # Reduced margins among figures. From: https://stat.ethz.ch/pipermail/r-help/2007-January/123556.html\n      theme.novpadding <- list(\n                              layout.heights= list(\n                                         top.padding = 2, main.key.padding = 0,\n                                 \t key.axis.padding= 0, axis.xlab.padding= 0,\n                                 \t xlab.key.padding= 0, key.sub.padding= 0,\n                                 \t bottom.padding= 0\n                                 \t ),\n                                layout.widths=list(\n                                         left.padding= 3, key.ylab.padding= 0,\n                         \t         ylab.axis.padding= 0, axis.key.padding=0,\n                         \t         right.padding= 0\n                         \t         )\n                                )\n      sp::spplot(p, scales=list(draw=TRUE, cex=0.75, tick.number=4, x=list(rot=axis.rot[1]), \n                 y= list(rot=axis.rot[2]) ), cuts=GOFcuts, col.regions=colorRamp(13), \n                 aspect=\"fill\", auto.key=auto.key, key.space= key.space, \n                 xlab=list(p1.name, font=2), ylab=list(p2.name, font=2), \n                 main=main, cex=points.cex, alpha=alpha, \n                 par.settings = theme.novpadding)\n  \n  } else if (type==\"scatter3d\") {\n\n        x <- params[1:n,p1.name]\n        y <- params[1:n,p2.name]\n        z <- params[1:n,gof.name]\n\n        scatterplot3d::scatterplot3d(x, y, z, highlight.3d=TRUE, col.axis=\"grey\", \n                      type=\"p\", col.grid=\"lightblue\", pch=20, main=main,\n                      xlab=p1.name, ylab=p2.name, zlab=gof.name)\n    } # IF end\n\n} # 'plot_2parOF' END\n" }
{ "repo_name": "ldecicco-USGS/sbtools", "ref": "refs/heads/master", "path": "R/item_list_files.R", "content": "#' @title Get list of files attached to SB item\n#'\n#' @template manipulate_item\n#' @param recursive (logical) List files recursively. Default: \\code{FALSE}\n#'\n#' @return \n#' A data.frame with columns fname, size, and url. \n#' If item has no attached files, returns a zero row data.frame.\n#'\n#' @description \n#' Lists all files attached to a SB item. Files can be downloaded from ScienceBase\n#' using \\code{\\link{item_file_download}}. (advanced) Recursive options lists all \n#' files attached to an item and all children items.\n#'\n#' @export\n#' @examples \\dontrun{\n#' item_list_files(\"4f4e4b24e4b07f02db6aea14\")\n#' \n#' # list files recursively\n#' ## create item\n#' id <- item_create(user_id(), title=\"some title\")\n#' ## 1. create nested item w/ file\n#' file <- system.file(\"examples\", \"books.json\", package = \"sbtools\")\n#' id2 <- item_create(id, title = \"newest-thing\")\n#' item_upload_create(id2, file)\n#' ## 2. create nested item w/ file\n#' file <- system.file(\"examples\", \"species.json\", package = \"sbtools\")\n#' id3 <- item_create(id, title = \"a-new-thing\")\n#' item_upload_create(id3, file)\n#' ## 3. create nested item w/ file\n#' file <- system.file(\"examples\", \"data.csv\", package = \"sbtools\")\n#' id4 <- item_create(id, title = \"another-thing\")\n#' item_upload_create(id4, file)\n#' item_list_files(id = '56562348e4b071e7ea53e09d', recursive = FALSE) # default\n#' item_list_files(id = '56562348e4b071e7ea53e09d', recursive = TRUE)\n#' }\nitem_list_files = function(sb_id, recursive = FALSE, ..., session=current_session()){\n\t\n\tsession_val(session)\n\t\n\tid <- as.sbitem(sb_id)\n\titem = item_get(id, session = session)\n\t\n\tif (recursive) {\n\t\tif (item$hasChildren) {\n\t\t\tkids <- item$hasChildren\n\t\t\tout <- list()\n\t\t\ti <- 0\n\t\t\twhile (kids) {\n\t\t\t\ti <- i + 1\n\t\t\t\tif (!is(item, \"list\")) item <- list(item)\n\t\t\t\titem <- lapply(item, function(w) {\n\t\t\t\t\tif (is(w, \"data.frame\")) w$id else w\n\t\t\t\t})\n\t\t\t\tif (!is(item[[1]], \"sbitem\")) item <- unlist(item, FALSE)\n\t\t\t\titem <- lapply(item, item_list_children, session = session)\n\t\t\t\tchildren <- lapply(item, function(z) lapply(z$id, as.sbitem))\n\t\t\t\tout[[i]] <- children\n\t\t\t\tkids <- any(comp(sapply(unlist(children, FALSE), \"[[\", \"hasChildren\")))\n\t\t\t}\n\t\t\tout <- unlist(lapply(out, function(x) if (is(x, \"list\")) unlist(x, FALSE) else x), FALSE)\n\t\t\tfiles <- unlist(comp(sapply(out, \"[[\", \"files\")), recursive = FALSE)\n\t\t} else {\n\t\t\tfiles <- list()\n\t\t\tmessage(\"no child items found\")\n\t\t}\n\t} else {\n\t\tfiles <- item$files\n\t}\n\t\n\tout <- data.frame(stringsAsFactors = FALSE)\n\t\n\tif (length(files) == 0) {\n\t\treturn(out)\n\t}\n\t\n\tfor (i in 1:length(files)) {\n\t\tout[i,'fname'] = files[[i]]$name\n\t\tout[i,'size'] = files[[i]]$size\n\t\tout[i,'url'] = files[[i]]$url\n\t}\n\t\n\treturn(out)\n}\n" }
{ "repo_name": "jread-usgs/sbtools", "ref": "refs/heads/master", "path": "R/item_list_files.R", "content": "#' @title Get list of files attached to SB item\n#'\n#' @template manipulate_item\n#' @param recursive (logical) List files recursively. Default: \\code{FALSE}\n#'\n#' @return \n#' A data.frame with columns fname, size, and url. \n#' If item has no attached files, returns a zero row data.frame.\n#'\n#' @description \n#' Lists all files attached to a SB item. Files can be downloaded from ScienceBase\n#' using \\code{\\link{item_file_download}}. (advanced) Recursive options lists all \n#' files attached to an item and all children items.\n#'\n#' @export\n#' @examples \\dontrun{\n#' item_list_files(\"4f4e4b24e4b07f02db6aea14\")\n#' \n#' # list files recursively\n#' ## create item\n#' id <- item_create(user_id(), title=\"some title\")\n#' ## 1. create nested item w/ file\n#' file <- system.file(\"examples\", \"books.json\", package = \"sbtools\")\n#' id2 <- item_create(id, title = \"newest-thing\")\n#' item_upload_create(id2, file)\n#' ## 2. create nested item w/ file\n#' file <- system.file(\"examples\", \"species.json\", package = \"sbtools\")\n#' id3 <- item_create(id, title = \"a-new-thing\")\n#' item_upload_create(id3, file)\n#' ## 3. create nested item w/ file\n#' file <- system.file(\"examples\", \"data.csv\", package = \"sbtools\")\n#' id4 <- item_create(id, title = \"another-thing\")\n#' item_upload_create(id4, file)\n#' item_list_files(id = '56562348e4b071e7ea53e09d', recursive = FALSE) # default\n#' item_list_files(id = '56562348e4b071e7ea53e09d', recursive = TRUE)\n#' }\nitem_list_files = function(sb_id, recursive = FALSE, ..., session=current_session()){\n\t\n\tsession_val(session)\n\t\n\tid <- as.sbitem(sb_id)\n\titem = item_get(id, session = session)\n\t\n\tif (recursive) {\n\t\tif (item$hasChildren) {\n\t\t\tkids <- item$hasChildren\n\t\t\tout <- list()\n\t\t\ti <- 0\n\t\t\twhile (kids) {\n\t\t\t\ti <- i + 1\n\t\t\t\tif (!is(item, \"list\")) item <- list(item)\n\t\t\t\titem <- lapply(item, function(w) {\n\t\t\t\t\tif (is(w, \"data.frame\")) w$id else w\n\t\t\t\t})\n\t\t\t\tif (!is(item[[1]], \"sbitem\")) item <- unlist(item, FALSE)\n\t\t\t\titem <- lapply(item, item_list_children, session = session)\n\t\t\t\tchildren <- lapply(item, function(z) lapply(z$id, as.sbitem))\n\t\t\t\tout[[i]] <- children\n\t\t\t\tkids <- any(comp(sapply(unlist(children, FALSE), \"[[\", \"hasChildren\")))\n\t\t\t}\n\t\t\tout <- unlist(lapply(out, function(x) if (is(x, \"list\")) unlist(x, FALSE) else x), FALSE)\n\t\t\tfiles <- unlist(comp(sapply(out, \"[[\", \"files\")), recursive = FALSE)\n\t\t} else {\n\t\t\tfiles <- list()\n\t\t\tmessage(\"no child items found\")\n\t\t}\n\t} else {\n\t\tfiles <- item$files\n\t}\n\t\n\tout <- data.frame(stringsAsFactors = FALSE)\n\t\n\tif (length(files) == 0) {\n\t\treturn(out)\n\t}\n\t\n\tfor (i in 1:length(files)) {\n\t\tout[i,'fname'] = files[[i]]$name\n\t\tout[i,'size'] = files[[i]]$size\n\t\tout[i,'url'] = files[[i]]$url\n\t}\n\t\n\treturn(out)\n}\n" }
{ "repo_name": "lawinslow/sbtools", "ref": "refs/heads/master", "path": "R/item_list_files.R", "content": "#' @title Get list of files attached to SB item\n#'\n#' @template manipulate_item\n#' @param recursive (logical) List files recursively. Default: \\code{FALSE}\n#'\n#' @return \n#' A data.frame with columns fname, size, and url. \n#' If item has no attached files, returns a zero row data.frame.\n#'\n#' @description \n#' Lists all files attached to a SB item. Files can be downloaded from ScienceBase\n#' using \\code{\\link{item_file_download}}. (advanced) Recursive options lists all \n#' files attached to an item and all children items.\n#'\n#' @export\n#' @examples \\dontrun{\n#' item_list_files(\"4f4e4b24e4b07f02db6aea14\")\n#' \n#' # list files recursively\n#' ## create item\n#' id <- item_create(user_id(), title=\"some title\")\n#' ## 1. create nested item w/ file\n#' file <- system.file(\"examples\", \"books.json\", package = \"sbtools\")\n#' id2 <- item_create(id, title = \"newest-thing\")\n#' item_upload_create(id2, file)\n#' ## 2. create nested item w/ file\n#' file <- system.file(\"examples\", \"species.json\", package = \"sbtools\")\n#' id3 <- item_create(id, title = \"a-new-thing\")\n#' item_upload_create(id3, file)\n#' ## 3. create nested item w/ file\n#' file <- system.file(\"examples\", \"data.csv\", package = \"sbtools\")\n#' id4 <- item_create(id, title = \"another-thing\")\n#' item_upload_create(id4, file)\n#' item_list_files(id = '56562348e4b071e7ea53e09d', recursive = FALSE) # default\n#' item_list_files(id = '56562348e4b071e7ea53e09d', recursive = TRUE)\n#' }\nitem_list_files = function(sb_id, recursive = FALSE, ..., session=current_session()){\n\t\n\tsession_val(session)\n\t\n\tid <- as.sbitem(sb_id)\n\titem = item_get(id, session = session)\n\t\n\tif (recursive) {\n\t\tif (item$hasChildren) {\n\t\t\tkids <- item$hasChildren\n\t\t\tout <- list()\n\t\t\ti <- 0\n\t\t\twhile (kids) {\n\t\t\t\ti <- i + 1\n\t\t\t\tif (!is(item, \"list\")) item <- list(item)\n\t\t\t\titem <- lapply(item, function(w) {\n\t\t\t\t\tif (is(w, \"data.frame\")) w$id else w\n\t\t\t\t})\n\t\t\t\tif (!is(item[[1]], \"sbitem\")) item <- unlist(item, FALSE)\n\t\t\t\titem <- lapply(item, item_list_children, session = session)\n\t\t\t\tchildren <- lapply(item, function(z) lapply(z$id, as.sbitem))\n\t\t\t\tout[[i]] <- children\n\t\t\t\tkids <- any(comp(sapply(unlist(children, FALSE), \"[[\", \"hasChildren\")))\n\t\t\t}\n\t\t\tout <- unlist(lapply(out, function(x) if (is(x, \"list\")) unlist(x, FALSE) else x), FALSE)\n\t\t\tfiles <- unlist(comp(sapply(out, \"[[\", \"files\")), recursive = FALSE)\n\t\t} else {\n\t\t\tfiles <- list()\n\t\t\tmessage(\"no child items found\")\n\t\t}\n\t} else {\n\t\tfiles <- item$files\n\t}\n\t\n\tout <- data.frame(stringsAsFactors = FALSE)\n\t\n\tif (length(files) == 0) {\n\t\treturn(out)\n\t}\n\t\n\tfor (i in 1:length(files)) {\n\t\tout[i,'fname'] = files[[i]]$name\n\t\tout[i,'size'] = files[[i]]$size\n\t\tout[i,'url'] = files[[i]]$url\n\t}\n\t\n\treturn(out)\n}\n" }
{ "repo_name": "aappling-usgs/sbtools", "ref": "refs/heads/master", "path": "R/item_list_files.R", "content": "#' @title Get list of files attached to SB item\n#'\n#' @template manipulate_item\n#' @param recursive (logical) List files recursively. Default: \\code{FALSE}\n#'\n#' @return \n#' A data.frame with columns fname, size, and url. \n#' If item has no attached files, returns a zero row data.frame.\n#'\n#' @description \n#' Lists all files attached to a SB item. Files can be downloaded from ScienceBase\n#' using \\code{\\link{item_file_download}}. (advanced) Recursive options lists all \n#' files attached to an item and all children items.\n#'\n#' @export\n#' @examples \\dontrun{\n#' item_list_files(\"4f4e4b24e4b07f02db6aea14\")\n#' \n#' # list files recursively\n#' ## create item\n#' id <- item_create(user_id(), title=\"some title\")\n#' ## 1. create nested item w/ file\n#' file <- system.file(\"examples\", \"books.json\", package = \"sbtools\")\n#' id2 <- item_create(id, title = \"newest-thing\")\n#' item_upload_create(id2, file)\n#' ## 2. create nested item w/ file\n#' file <- system.file(\"examples\", \"species.json\", package = \"sbtools\")\n#' id3 <- item_create(id, title = \"a-new-thing\")\n#' item_upload_create(id3, file)\n#' ## 3. create nested item w/ file\n#' file <- system.file(\"examples\", \"data.csv\", package = \"sbtools\")\n#' id4 <- item_create(id, title = \"another-thing\")\n#' item_upload_create(id4, file)\n#' item_list_files(id = '56562348e4b071e7ea53e09d', recursive = FALSE) # default\n#' item_list_files(id = '56562348e4b071e7ea53e09d', recursive = TRUE)\n#' }\nitem_list_files = function(sb_id, recursive = FALSE, ..., session=current_session()){\n\t\n\tsession_val(session)\n\t\n\tid <- as.sbitem(sb_id)\n\titem = item_get(id, session = session)\n\t\n\tif (recursive) {\n\t\tif (item$hasChildren) {\n\t\t\tkids <- item$hasChildren\n\t\t\tout <- list()\n\t\t\ti <- 0\n\t\t\twhile (kids) {\n\t\t\t\ti <- i + 1\n\t\t\t\tif (!is(item, \"list\")) item <- list(item)\n\t\t\t\titem <- lapply(item, function(w) {\n\t\t\t\t\tif (is(w, \"data.frame\")) w$id else w\n\t\t\t\t})\n\t\t\t\tif (!is(item[[1]], \"sbitem\")) item <- unlist(item, FALSE)\n\t\t\t\titem <- lapply(item, item_list_children, session = session)\n\t\t\t\tchildren <- lapply(item, function(z) lapply(z$id, as.sbitem))\n\t\t\t\tout[[i]] <- children\n\t\t\t\tkids <- any(comp(sapply(unlist(children, FALSE), \"[[\", \"hasChildren\")))\n\t\t\t}\n\t\t\tout <- unlist(lapply(out, function(x) if (is(x, \"list\")) unlist(x, FALSE) else x), FALSE)\n\t\t\tfiles <- unlist(comp(sapply(out, \"[[\", \"files\")), recursive = FALSE)\n\t\t} else {\n\t\t\tfiles <- list()\n\t\t\tmessage(\"no child items found\")\n\t\t}\n\t} else {\n\t\tfiles <- item$files\n\t}\n\t\n\tout <- data.frame(stringsAsFactors = FALSE)\n\t\n\tif (length(files) == 0) {\n\t\treturn(out)\n\t}\n\t\n\tfor (i in 1:length(files)) {\n\t\tout[i,'fname'] = files[[i]]$name\n\t\tout[i,'size'] = files[[i]]$size\n\t\tout[i,'url'] = files[[i]]$url\n\t}\n\t\n\treturn(out)\n}\n" }
{ "repo_name": "publicRoman/spark", "ref": "refs/heads/branch-2.2-kubernetes", "path": "R/pkg/R/context.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# context.R: SparkContext driven functions\n\ngetMinPartitions <- function(sc, minPartitions) {\n  if (is.null(minPartitions)) {\n    defaultParallelism <- callJMethod(sc, \"defaultParallelism\")\n    minPartitions <- min(defaultParallelism, 2)\n  }\n  as.integer(minPartitions)\n}\n\n#' Create an RDD from a text file.\n#'\n#' This function reads a text file from HDFS, a local file system (available on all\n#' nodes), or any Hadoop-supported file system URI, and creates an\n#' RDD of strings from it.\n#'\n#' @param sc SparkContext to use\n#' @param path Path of file to read. A vector of multiple paths is allowed.\n#' @param minPartitions Minimum number of partitions to be created. If NULL, the default\n#'  value is chosen based on available parallelism.\n#' @return RDD where each item is of type \\code{character}\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  sc <- sparkR.init()\n#'  lines <- textFile(sc, \"myfile.txt\")\n#'}\ntextFile <- function(sc, path, minPartitions = NULL) {\n  # Allow the user to have a more flexible definiton of the text file path\n  path <- suppressWarnings(normalizePath(path))\n  # Convert a string vector of paths to a string containing comma separated paths\n  path <- paste(path, collapse = \",\")\n\n  jrdd <- callJMethod(sc, \"textFile\", path, getMinPartitions(sc, minPartitions))\n  # jrdd is of type JavaRDD[String]\n  RDD(jrdd, \"string\")\n}\n\n#' Load an RDD saved as a SequenceFile containing serialized objects.\n#'\n#' The file to be loaded should be one that was previously generated by calling\n#' saveAsObjectFile() of the RDD class.\n#'\n#' @param sc SparkContext to use\n#' @param path Path of file to read. A vector of multiple paths is allowed.\n#' @param minPartitions Minimum number of partitions to be created. If NULL, the default\n#'  value is chosen based on available parallelism.\n#' @return RDD containing serialized R objects.\n#' @seealso saveAsObjectFile\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  sc <- sparkR.init()\n#'  rdd <- objectFile(sc, \"myfile\")\n#'}\nobjectFile <- function(sc, path, minPartitions = NULL) {\n  # Allow the user to have a more flexible definiton of the text file path\n  path <- suppressWarnings(normalizePath(path))\n  # Convert a string vector of paths to a string containing comma separated paths\n  path <- paste(path, collapse = \",\")\n\n  jrdd <- callJMethod(sc, \"objectFile\", path, getMinPartitions(sc, minPartitions))\n  # Assume the RDD contains serialized R objects.\n  RDD(jrdd, \"byte\")\n}\n\n#' Create an RDD from a homogeneous list or vector.\n#'\n#' This function creates an RDD from a local homogeneous list in R. The elements\n#' in the list are split into \\code{numSlices} slices and distributed to nodes\n#' in the cluster.\n#'\n#' If size of serialized slices is larger than spark.r.maxAllocationLimit or (200MB), the function\n#' will write it to disk and send the file name to JVM. Also to make sure each slice is not\n#' larger than that limit, number of slices may be increased.\n#'\n#' In 2.2.0 we are changing how the numSlices are used/computed to handle\n#' 1 < (length(coll) / numSlices) << length(coll) better, and to get the exact number of slices.\n#' This change affects both createDataFrame and spark.lapply.\n#' In the specific one case that it is used to convert R native object into SparkDataFrame, it has\n#' always been kept at the default of 1. In the case the object is large, we are explicitly setting\n#' the parallism to numSlices (which is still 1).\n#'\n#' Specifically, we are changing to split positions to match the calculation in positions() of\n#' ParallelCollectionRDD in Spark.\n#'\n#' @param sc SparkContext to use\n#' @param coll collection to parallelize\n#' @param numSlices number of partitions to create in the RDD\n#' @return an RDD created from this collection\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' rdd <- parallelize(sc, 1:10, 2)\n#' # The RDD should contain 10 elements\n#' length(rdd)\n#'}\nparallelize <- function(sc, coll, numSlices = 1) {\n  # TODO: bound/safeguard numSlices\n  # TODO: unit tests for if the split works for all primitives\n  # TODO: support matrix, data frame, etc\n\n  # Note, for data.frame, createDataFrame turns it into a list before it calls here.\n  # nolint start\n  # suppress lintr warning: Place a space before left parenthesis, except in a function call.\n  if ((!is.list(coll) && !is.vector(coll)) || is.data.frame(coll)) {\n  # nolint end\n    if (is.data.frame(coll)) {\n      message(paste(\"context.R: A data frame is parallelized by columns.\"))\n    } else {\n      if (is.matrix(coll)) {\n        message(paste(\"context.R: A matrix is parallelized by elements.\"))\n      } else {\n        message(paste(\"context.R: parallelize() currently only supports lists and vectors.\",\n                      \"Calling as.list() to coerce coll into a list.\"))\n      }\n    }\n    coll <- as.list(coll)\n  }\n\n  sizeLimit <- getMaxAllocationLimit(sc)\n  objectSize <- object.size(coll)\n\n  # For large objects we make sure the size of each slice is also smaller than sizeLimit\n  numSerializedSlices <- max(numSlices, ceiling(objectSize / sizeLimit))\n  if (numSerializedSlices > length(coll))\n    numSerializedSlices <- length(coll)\n\n  # Generate the slice ids to put each row\n  # For instance, for numSerializedSlices of 22, length of 50\n  #  [1]  0  0  2  2  4  4  6  6  6  9  9 11 11 13 13 15 15 15 18 18 20 20 22 22 22\n  # [26] 25 25 27 27 29 29 31 31 31 34 34 36 36 38 38 40 40 40 43 43 45 45 47 47 47\n  # Notice the slice group with 3 slices (ie. 6, 15, 22) are roughly evenly spaced.\n  # We are trying to reimplement the calculation in the positions method in ParallelCollectionRDD\n  splits <- if (numSerializedSlices > 0) {\n    unlist(lapply(0: (numSerializedSlices - 1), function(x) {\n      # nolint start\n      start <- trunc((x * length(coll)) / numSerializedSlices)\n      end <- trunc(((x + 1) * length(coll)) / numSerializedSlices)\n      # nolint end\n      rep(start, end - start)\n    }))\n  } else {\n    1\n  }\n\n  slices <- split(coll, splits)\n\n  # Serialize each slice: obtain a list of raws, or a list of lists (slices) of\n  # 2-tuples of raws\n  serializedSlices <- lapply(slices, serialize, connection = NULL)\n\n  # The PRC backend cannot handle arguments larger than 2GB (INT_MAX)\n  # If serialized data is safely less than that threshold we send it over the PRC channel.\n  # Otherwise, we write it to a file and send the file name\n  if (objectSize < sizeLimit) {\n    jrdd <- callJStatic(\"org.apache.spark.api.r.RRDD\", \"createRDDFromArray\", sc, serializedSlices)\n  } else {\n    fileName <- writeToTempFile(serializedSlices)\n    jrdd <- tryCatch(callJStatic(\n        \"org.apache.spark.api.r.RRDD\", \"createRDDFromFile\", sc, fileName, as.integer(numSlices)),\n      finally = {\n        file.remove(fileName)\n    })\n  }\n\n  RDD(jrdd, \"byte\")\n}\n\ngetMaxAllocationLimit <- function(sc) {\n  conf <- callJMethod(sc, \"getConf\")\n  as.numeric(\n    callJMethod(conf,\n      \"get\",\n      \"spark.r.maxAllocationLimit\",\n      toString(.Machine$integer.max / 10) # Default to a safe value: 200MB\n  ))\n}\n\nwriteToTempFile <- function(serializedSlices) {\n  fileName <- tempfile()\n  conn <- file(fileName, \"wb\")\n  for (slice in serializedSlices) {\n    writeBin(as.integer(length(slice)), conn, endian = \"big\")\n    writeBin(slice, conn, endian = \"big\")\n  }\n  close(conn)\n  fileName\n}\n\n#' Include this specified package on all workers\n#'\n#' This function can be used to include a package on all workers before the\n#' user's code is executed. This is useful in scenarios where other R package\n#' functions are used in a function passed to functions like \\code{lapply}.\n#' NOTE: The package is assumed to be installed on every node in the Spark\n#' cluster.\n#'\n#' @param sc SparkContext to use\n#' @param pkg Package name\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  library(Matrix)\n#'\n#'  sc <- sparkR.init()\n#'  # Include the matrix library we will be using\n#'  includePackage(sc, Matrix)\n#'\n#'  generateSparse <- function(x) {\n#'    sparseMatrix(i=c(1, 2, 3), j=c(1, 2, 3), x=c(1, 2, 3))\n#'  }\n#'\n#'  rdd <- lapplyPartition(parallelize(sc, 1:2, 2L), generateSparse)\n#'  collect(rdd)\n#'}\nincludePackage <- function(sc, pkg) {\n  pkg <- as.character(substitute(pkg))\n  if (exists(\".packages\", .sparkREnv)) {\n    packages <- .sparkREnv$.packages\n  } else {\n    packages <- list()\n  }\n  packages <- c(packages, pkg)\n  .sparkREnv$.packages <- packages\n}\n\n#' Broadcast a variable to all workers\n#'\n#' Broadcast a read-only variable to the cluster, returning a \\code{Broadcast}\n#' object for reading it in distributed functions.\n#'\n#' @param sc Spark Context to use\n#' @param object Object to be broadcast\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' rdd <- parallelize(sc, 1:2, 2L)\n#'\n#' # Large Matrix object that we want to broadcast\n#' randomMat <- matrix(nrow=100, ncol=10, data=rnorm(1000))\n#' randomMatBr <- broadcast(sc, randomMat)\n#'\n#' # Use the broadcast variable inside the function\n#' useBroadcast <- function(x) {\n#'   sum(value(randomMatBr) * x)\n#' }\n#' sumRDD <- lapply(rdd, useBroadcast)\n#'}\nbroadcast <- function(sc, object) {\n  objName <- as.character(substitute(object))\n  serializedObj <- serialize(object, connection = NULL)\n\n  jBroadcast <- callJMethod(sc, \"broadcast\", serializedObj)\n  id <- as.character(callJMethod(jBroadcast, \"id\"))\n\n  Broadcast(id, object, jBroadcast, objName)\n}\n\n#' Set the checkpoint directory\n#'\n#' Set the directory under which RDDs are going to be checkpointed. The\n#' directory must be a HDFS path if running on a cluster.\n#'\n#' @param sc Spark Context to use\n#' @param dirName Directory path\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' setCheckpointDir(sc, \"~/checkpoint\")\n#' rdd <- parallelize(sc, 1:2, 2L)\n#' checkpoint(rdd)\n#'}\nsetCheckpointDirSC <- function(sc, dirName) {\n  invisible(callJMethod(sc, \"setCheckpointDir\", suppressWarnings(normalizePath(dirName))))\n}\n\n#' Add a file or directory to be downloaded with this Spark job on every node.\n#'\n#' The path passed can be either a local file, a file in HDFS (or other Hadoop-supported\n#' filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,\n#' use spark.getSparkFiles(fileName) to find its download location.\n#'\n#' A directory can be given if the recursive option is set to true.\n#' Currently directories are only supported for Hadoop-supported filesystems.\n#' Refer Hadoop-supported filesystems at \\url{https://wiki.apache.org/hadoop/HCFS}.\n#'\n#' @rdname spark.addFile\n#' @param path The path of the file to be added\n#' @param recursive Whether to add files recursively from the path. Default is FALSE.\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.addFile(\"~/myfile\")\n#'}\n#' @note spark.addFile since 2.1.0\nspark.addFile <- function(path, recursive = FALSE) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"addFile\", suppressWarnings(normalizePath(path)), recursive))\n}\n\n#' Get the root directory that contains files added through spark.addFile.\n#'\n#' @rdname spark.getSparkFilesRootDirectory\n#' @return the root directory that contains files added through spark.addFile\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.getSparkFilesRootDirectory()\n#'}\n#' @note spark.getSparkFilesRootDirectory since 2.1.0\nspark.getSparkFilesRootDirectory <- function() {\n  if (Sys.getenv(\"SPARKR_IS_RUNNING_ON_WORKER\") == \"\") {\n    # Running on driver.\n    callJStatic(\"org.apache.spark.SparkFiles\", \"getRootDirectory\")\n  } else {\n    # Running on worker.\n    Sys.getenv(\"SPARKR_SPARKFILES_ROOT_DIR\")\n  }\n}\n\n#' Get the absolute path of a file added through spark.addFile.\n#'\n#' @rdname spark.getSparkFiles\n#' @param fileName The name of the file added through spark.addFile\n#' @return the absolute path of a file added through spark.addFile.\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.getSparkFiles(\"myfile\")\n#'}\n#' @note spark.getSparkFiles since 2.1.0\nspark.getSparkFiles <- function(fileName) {\n  if (Sys.getenv(\"SPARKR_IS_RUNNING_ON_WORKER\") == \"\") {\n    # Running on driver.\n    callJStatic(\"org.apache.spark.SparkFiles\", \"get\", as.character(fileName))\n  } else {\n    # Running on worker.\n    file.path(spark.getSparkFilesRootDirectory(), as.character(fileName))\n  }\n}\n\n#' Run a function over a list of elements, distributing the computations with Spark\n#'\n#' Run a function over a list of elements, distributing the computations with Spark. Applies a\n#' function in a manner that is similar to doParallel or lapply to elements of a list.\n#' The computations are distributed using Spark. It is conceptually the same as the following code:\n#'   lapply(list, func)\n#'\n#' Known limitations:\n#' \\itemize{\n#'    \\item variable scoping and capture: compared to R's rich support for variable resolutions,\n#'    the distributed nature of SparkR limits how variables are resolved at runtime. All the\n#'    variables that are available through lexical scoping are embedded in the closure of the\n#'    function and available as read-only variables within the function. The environment variables\n#'    should be stored into temporary variables outside the function, and not directly accessed\n#'    within the function.\n#'\n#'   \\item loading external packages: In order to use a package, you need to load it inside the\n#'   closure. For example, if you rely on the MASS module, here is how you would use it:\n#'   \\preformatted{\n#'     train <- function(hyperparam) {\n#'       library(MASS)\n#'       lm.ridge(\"y ~ x+z\", data, lambda=hyperparam)\n#'       model\n#'     }\n#'   }\n#' }\n#'\n#' @rdname spark.lapply\n#' @param list the list of elements\n#' @param func a function that takes one argument.\n#' @return a list of results (the exact type being determined by the function)\n#' @export\n#' @examples\n#'\\dontrun{\n#' sparkR.session()\n#' doubled <- spark.lapply(1:10, function(x){2 * x})\n#'}\n#' @note spark.lapply since 2.0.0\nspark.lapply <- function(list, func) {\n  sc <- getSparkContext()\n  rdd <- parallelize(sc, list, length(list))\n  results <- map(rdd, func)\n  local <- collectRDD(results)\n  local\n}\n\n#' Set new log level\n#'\n#' Set new log level: \"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\"\n#'\n#' @rdname setLogLevel\n#' @param level New log level\n#' @export\n#' @examples\n#'\\dontrun{\n#' setLogLevel(\"ERROR\")\n#'}\n#' @note setLogLevel since 2.0.0\nsetLogLevel <- function(level) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"setLogLevel\", level))\n}\n\n#' Set checkpoint directory\n#'\n#' Set the directory under which SparkDataFrame are going to be checkpointed. The directory must be\n#' a HDFS path if running on a cluster.\n#'\n#' @rdname setCheckpointDir\n#' @param directory Directory path to checkpoint to\n#' @seealso \\link{checkpoint}\n#' @export\n#' @examples\n#'\\dontrun{\n#' setCheckpointDir(\"/checkpoint\")\n#'}\n#' @note setCheckpointDir since 2.2.0\nsetCheckpointDir <- function(directory) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"setCheckpointDir\", suppressWarnings(normalizePath(directory))))\n}\n" }
{ "repo_name": "milliman/spark", "ref": "refs/heads/master", "path": "R/pkg/R/context.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# context.R: SparkContext driven functions\n\ngetMinPartitions <- function(sc, minPartitions) {\n  if (is.null(minPartitions)) {\n    defaultParallelism <- callJMethod(sc, \"defaultParallelism\")\n    minPartitions <- min(defaultParallelism, 2)\n  }\n  as.integer(minPartitions)\n}\n\n#' Create an RDD from a text file.\n#'\n#' This function reads a text file from HDFS, a local file system (available on all\n#' nodes), or any Hadoop-supported file system URI, and creates an\n#' RDD of strings from it.\n#'\n#' @param sc SparkContext to use\n#' @param path Path of file to read. A vector of multiple paths is allowed.\n#' @param minPartitions Minimum number of partitions to be created. If NULL, the default\n#'  value is chosen based on available parallelism.\n#' @return RDD where each item is of type \\code{character}\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  sc <- sparkR.init()\n#'  lines <- textFile(sc, \"myfile.txt\")\n#'}\ntextFile <- function(sc, path, minPartitions = NULL) {\n  # Allow the user to have a more flexible definiton of the text file path\n  path <- suppressWarnings(normalizePath(path))\n  # Convert a string vector of paths to a string containing comma separated paths\n  path <- paste(path, collapse = \",\")\n\n  jrdd <- callJMethod(sc, \"textFile\", path, getMinPartitions(sc, minPartitions))\n  # jrdd is of type JavaRDD[String]\n  RDD(jrdd, \"string\")\n}\n\n#' Load an RDD saved as a SequenceFile containing serialized objects.\n#'\n#' The file to be loaded should be one that was previously generated by calling\n#' saveAsObjectFile() of the RDD class.\n#'\n#' @param sc SparkContext to use\n#' @param path Path of file to read. A vector of multiple paths is allowed.\n#' @param minPartitions Minimum number of partitions to be created. If NULL, the default\n#'  value is chosen based on available parallelism.\n#' @return RDD containing serialized R objects.\n#' @seealso saveAsObjectFile\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  sc <- sparkR.init()\n#'  rdd <- objectFile(sc, \"myfile\")\n#'}\nobjectFile <- function(sc, path, minPartitions = NULL) {\n  # Allow the user to have a more flexible definiton of the text file path\n  path <- suppressWarnings(normalizePath(path))\n  # Convert a string vector of paths to a string containing comma separated paths\n  path <- paste(path, collapse = \",\")\n\n  jrdd <- callJMethod(sc, \"objectFile\", path, getMinPartitions(sc, minPartitions))\n  # Assume the RDD contains serialized R objects.\n  RDD(jrdd, \"byte\")\n}\n\n#' Create an RDD from a homogeneous list or vector.\n#'\n#' This function creates an RDD from a local homogeneous list in R. The elements\n#' in the list are split into \\code{numSlices} slices and distributed to nodes\n#' in the cluster.\n#'\n#' If size of serialized slices is larger than spark.r.maxAllocationLimit or (200MB), the function\n#' will write it to disk and send the file name to JVM. Also to make sure each slice is not\n#' larger than that limit, number of slices may be increased.\n#'\n#' In 2.2.0 we are changing how the numSlices are used/computed to handle\n#' 1 < (length(coll) / numSlices) << length(coll) better, and to get the exact number of slices.\n#' This change affects both createDataFrame and spark.lapply.\n#' In the specific one case that it is used to convert R native object into SparkDataFrame, it has\n#' always been kept at the default of 1. In the case the object is large, we are explicitly setting\n#' the parallism to numSlices (which is still 1).\n#'\n#' Specifically, we are changing to split positions to match the calculation in positions() of\n#' ParallelCollectionRDD in Spark.\n#'\n#' @param sc SparkContext to use\n#' @param coll collection to parallelize\n#' @param numSlices number of partitions to create in the RDD\n#' @return an RDD created from this collection\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' rdd <- parallelize(sc, 1:10, 2)\n#' # The RDD should contain 10 elements\n#' length(rdd)\n#'}\nparallelize <- function(sc, coll, numSlices = 1) {\n  # TODO: bound/safeguard numSlices\n  # TODO: unit tests for if the split works for all primitives\n  # TODO: support matrix, data frame, etc\n\n  # Note, for data.frame, createDataFrame turns it into a list before it calls here.\n  # nolint start\n  # suppress lintr warning: Place a space before left parenthesis, except in a function call.\n  if ((!is.list(coll) && !is.vector(coll)) || is.data.frame(coll)) {\n  # nolint end\n    if (is.data.frame(coll)) {\n      message(paste(\"context.R: A data frame is parallelized by columns.\"))\n    } else {\n      if (is.matrix(coll)) {\n        message(paste(\"context.R: A matrix is parallelized by elements.\"))\n      } else {\n        message(paste(\"context.R: parallelize() currently only supports lists and vectors.\",\n                      \"Calling as.list() to coerce coll into a list.\"))\n      }\n    }\n    coll <- as.list(coll)\n  }\n\n  sizeLimit <- getMaxAllocationLimit(sc)\n  objectSize <- object.size(coll)\n\n  # For large objects we make sure the size of each slice is also smaller than sizeLimit\n  numSerializedSlices <- max(numSlices, ceiling(objectSize / sizeLimit))\n  if (numSerializedSlices > length(coll))\n    numSerializedSlices <- length(coll)\n\n  # Generate the slice ids to put each row\n  # For instance, for numSerializedSlices of 22, length of 50\n  #  [1]  0  0  2  2  4  4  6  6  6  9  9 11 11 13 13 15 15 15 18 18 20 20 22 22 22\n  # [26] 25 25 27 27 29 29 31 31 31 34 34 36 36 38 38 40 40 40 43 43 45 45 47 47 47\n  # Notice the slice group with 3 slices (ie. 6, 15, 22) are roughly evenly spaced.\n  # We are trying to reimplement the calculation in the positions method in ParallelCollectionRDD\n  splits <- if (numSerializedSlices > 0) {\n    unlist(lapply(0: (numSerializedSlices - 1), function(x) {\n      # nolint start\n      start <- trunc((x * length(coll)) / numSerializedSlices)\n      end <- trunc(((x + 1) * length(coll)) / numSerializedSlices)\n      # nolint end\n      rep(start, end - start)\n    }))\n  } else {\n    1\n  }\n\n  slices <- split(coll, splits)\n\n  # Serialize each slice: obtain a list of raws, or a list of lists (slices) of\n  # 2-tuples of raws\n  serializedSlices <- lapply(slices, serialize, connection = NULL)\n\n  # The PRC backend cannot handle arguments larger than 2GB (INT_MAX)\n  # If serialized data is safely less than that threshold we send it over the PRC channel.\n  # Otherwise, we write it to a file and send the file name\n  if (objectSize < sizeLimit) {\n    jrdd <- callJStatic(\"org.apache.spark.api.r.RRDD\", \"createRDDFromArray\", sc, serializedSlices)\n  } else {\n    fileName <- writeToTempFile(serializedSlices)\n    jrdd <- tryCatch(callJStatic(\n        \"org.apache.spark.api.r.RRDD\", \"createRDDFromFile\", sc, fileName, as.integer(numSlices)),\n      finally = {\n        file.remove(fileName)\n    })\n  }\n\n  RDD(jrdd, \"byte\")\n}\n\ngetMaxAllocationLimit <- function(sc) {\n  conf <- callJMethod(sc, \"getConf\")\n  as.numeric(\n    callJMethod(conf,\n      \"get\",\n      \"spark.r.maxAllocationLimit\",\n      toString(.Machine$integer.max / 10) # Default to a safe value: 200MB\n  ))\n}\n\nwriteToTempFile <- function(serializedSlices) {\n  fileName <- tempfile()\n  conn <- file(fileName, \"wb\")\n  for (slice in serializedSlices) {\n    writeBin(as.integer(length(slice)), conn, endian = \"big\")\n    writeBin(slice, conn, endian = \"big\")\n  }\n  close(conn)\n  fileName\n}\n\n#' Include this specified package on all workers\n#'\n#' This function can be used to include a package on all workers before the\n#' user's code is executed. This is useful in scenarios where other R package\n#' functions are used in a function passed to functions like \\code{lapply}.\n#' NOTE: The package is assumed to be installed on every node in the Spark\n#' cluster.\n#'\n#' @param sc SparkContext to use\n#' @param pkg Package name\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  library(Matrix)\n#'\n#'  sc <- sparkR.init()\n#'  # Include the matrix library we will be using\n#'  includePackage(sc, Matrix)\n#'\n#'  generateSparse <- function(x) {\n#'    sparseMatrix(i=c(1, 2, 3), j=c(1, 2, 3), x=c(1, 2, 3))\n#'  }\n#'\n#'  rdd <- lapplyPartition(parallelize(sc, 1:2, 2L), generateSparse)\n#'  collect(rdd)\n#'}\nincludePackage <- function(sc, pkg) {\n  pkg <- as.character(substitute(pkg))\n  if (exists(\".packages\", .sparkREnv)) {\n    packages <- .sparkREnv$.packages\n  } else {\n    packages <- list()\n  }\n  packages <- c(packages, pkg)\n  .sparkREnv$.packages <- packages\n}\n\n#' Broadcast a variable to all workers\n#'\n#' Broadcast a read-only variable to the cluster, returning a \\code{Broadcast}\n#' object for reading it in distributed functions.\n#'\n#' @param sc Spark Context to use\n#' @param object Object to be broadcast\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' rdd <- parallelize(sc, 1:2, 2L)\n#'\n#' # Large Matrix object that we want to broadcast\n#' randomMat <- matrix(nrow=100, ncol=10, data=rnorm(1000))\n#' randomMatBr <- broadcast(sc, randomMat)\n#'\n#' # Use the broadcast variable inside the function\n#' useBroadcast <- function(x) {\n#'   sum(value(randomMatBr) * x)\n#' }\n#' sumRDD <- lapply(rdd, useBroadcast)\n#'}\nbroadcast <- function(sc, object) {\n  objName <- as.character(substitute(object))\n  serializedObj <- serialize(object, connection = NULL)\n\n  jBroadcast <- callJMethod(sc, \"broadcast\", serializedObj)\n  id <- as.character(callJMethod(jBroadcast, \"id\"))\n\n  Broadcast(id, object, jBroadcast, objName)\n}\n\n#' Set the checkpoint directory\n#'\n#' Set the directory under which RDDs are going to be checkpointed. The\n#' directory must be a HDFS path if running on a cluster.\n#'\n#' @param sc Spark Context to use\n#' @param dirName Directory path\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' setCheckpointDir(sc, \"~/checkpoint\")\n#' rdd <- parallelize(sc, 1:2, 2L)\n#' checkpoint(rdd)\n#'}\nsetCheckpointDirSC <- function(sc, dirName) {\n  invisible(callJMethod(sc, \"setCheckpointDir\", suppressWarnings(normalizePath(dirName))))\n}\n\n#' Add a file or directory to be downloaded with this Spark job on every node.\n#'\n#' The path passed can be either a local file, a file in HDFS (or other Hadoop-supported\n#' filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,\n#' use spark.getSparkFiles(fileName) to find its download location.\n#'\n#' A directory can be given if the recursive option is set to true.\n#' Currently directories are only supported for Hadoop-supported filesystems.\n#' Refer Hadoop-supported filesystems at \\url{https://wiki.apache.org/hadoop/HCFS}.\n#'\n#' @rdname spark.addFile\n#' @param path The path of the file to be added\n#' @param recursive Whether to add files recursively from the path. Default is FALSE.\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.addFile(\"~/myfile\")\n#'}\n#' @note spark.addFile since 2.1.0\nspark.addFile <- function(path, recursive = FALSE) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"addFile\", suppressWarnings(normalizePath(path)), recursive))\n}\n\n#' Get the root directory that contains files added through spark.addFile.\n#'\n#' @rdname spark.getSparkFilesRootDirectory\n#' @return the root directory that contains files added through spark.addFile\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.getSparkFilesRootDirectory()\n#'}\n#' @note spark.getSparkFilesRootDirectory since 2.1.0\nspark.getSparkFilesRootDirectory <- function() {\n  if (Sys.getenv(\"SPARKR_IS_RUNNING_ON_WORKER\") == \"\") {\n    # Running on driver.\n    callJStatic(\"org.apache.spark.SparkFiles\", \"getRootDirectory\")\n  } else {\n    # Running on worker.\n    Sys.getenv(\"SPARKR_SPARKFILES_ROOT_DIR\")\n  }\n}\n\n#' Get the absolute path of a file added through spark.addFile.\n#'\n#' @rdname spark.getSparkFiles\n#' @param fileName The name of the file added through spark.addFile\n#' @return the absolute path of a file added through spark.addFile.\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.getSparkFiles(\"myfile\")\n#'}\n#' @note spark.getSparkFiles since 2.1.0\nspark.getSparkFiles <- function(fileName) {\n  if (Sys.getenv(\"SPARKR_IS_RUNNING_ON_WORKER\") == \"\") {\n    # Running on driver.\n    callJStatic(\"org.apache.spark.SparkFiles\", \"get\", as.character(fileName))\n  } else {\n    # Running on worker.\n    file.path(spark.getSparkFilesRootDirectory(), as.character(fileName))\n  }\n}\n\n#' Run a function over a list of elements, distributing the computations with Spark\n#'\n#' Run a function over a list of elements, distributing the computations with Spark. Applies a\n#' function in a manner that is similar to doParallel or lapply to elements of a list.\n#' The computations are distributed using Spark. It is conceptually the same as the following code:\n#'   lapply(list, func)\n#'\n#' Known limitations:\n#' \\itemize{\n#'    \\item variable scoping and capture: compared to R's rich support for variable resolutions,\n#'    the distributed nature of SparkR limits how variables are resolved at runtime. All the\n#'    variables that are available through lexical scoping are embedded in the closure of the\n#'    function and available as read-only variables within the function. The environment variables\n#'    should be stored into temporary variables outside the function, and not directly accessed\n#'    within the function.\n#'\n#'   \\item loading external packages: In order to use a package, you need to load it inside the\n#'   closure. For example, if you rely on the MASS module, here is how you would use it:\n#'   \\preformatted{\n#'     train <- function(hyperparam) {\n#'       library(MASS)\n#'       lm.ridge(\"y ~ x+z\", data, lambda=hyperparam)\n#'       model\n#'     }\n#'   }\n#' }\n#'\n#' @rdname spark.lapply\n#' @param list the list of elements\n#' @param func a function that takes one argument.\n#' @return a list of results (the exact type being determined by the function)\n#' @export\n#' @examples\n#'\\dontrun{\n#' sparkR.session()\n#' doubled <- spark.lapply(1:10, function(x){2 * x})\n#'}\n#' @note spark.lapply since 2.0.0\nspark.lapply <- function(list, func) {\n  sc <- getSparkContext()\n  rdd <- parallelize(sc, list, length(list))\n  results <- map(rdd, func)\n  local <- collectRDD(results)\n  local\n}\n\n#' Set new log level\n#'\n#' Set new log level: \"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\"\n#'\n#' @rdname setLogLevel\n#' @param level New log level\n#' @export\n#' @examples\n#'\\dontrun{\n#' setLogLevel(\"ERROR\")\n#'}\n#' @note setLogLevel since 2.0.0\nsetLogLevel <- function(level) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"setLogLevel\", level))\n}\n\n#' Set checkpoint directory\n#'\n#' Set the directory under which SparkDataFrame are going to be checkpointed. The directory must be\n#' a HDFS path if running on a cluster.\n#'\n#' @rdname setCheckpointDir\n#' @param directory Directory path to checkpoint to\n#' @seealso \\link{checkpoint}\n#' @export\n#' @examples\n#'\\dontrun{\n#' setCheckpointDir(\"/checkpoint\")\n#'}\n#' @note setCheckpointDir since 2.2.0\nsetCheckpointDir <- function(directory) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"setCheckpointDir\", suppressWarnings(normalizePath(directory))))\n}\n" }
{ "repo_name": "apache-spark-on-k8s/spark", "ref": "refs/heads/branch-2.2-kubernetes", "path": "R/pkg/R/context.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# context.R: SparkContext driven functions\n\ngetMinPartitions <- function(sc, minPartitions) {\n  if (is.null(minPartitions)) {\n    defaultParallelism <- callJMethod(sc, \"defaultParallelism\")\n    minPartitions <- min(defaultParallelism, 2)\n  }\n  as.integer(minPartitions)\n}\n\n#' Create an RDD from a text file.\n#'\n#' This function reads a text file from HDFS, a local file system (available on all\n#' nodes), or any Hadoop-supported file system URI, and creates an\n#' RDD of strings from it.\n#'\n#' @param sc SparkContext to use\n#' @param path Path of file to read. A vector of multiple paths is allowed.\n#' @param minPartitions Minimum number of partitions to be created. If NULL, the default\n#'  value is chosen based on available parallelism.\n#' @return RDD where each item is of type \\code{character}\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  sc <- sparkR.init()\n#'  lines <- textFile(sc, \"myfile.txt\")\n#'}\ntextFile <- function(sc, path, minPartitions = NULL) {\n  # Allow the user to have a more flexible definiton of the text file path\n  path <- suppressWarnings(normalizePath(path))\n  # Convert a string vector of paths to a string containing comma separated paths\n  path <- paste(path, collapse = \",\")\n\n  jrdd <- callJMethod(sc, \"textFile\", path, getMinPartitions(sc, minPartitions))\n  # jrdd is of type JavaRDD[String]\n  RDD(jrdd, \"string\")\n}\n\n#' Load an RDD saved as a SequenceFile containing serialized objects.\n#'\n#' The file to be loaded should be one that was previously generated by calling\n#' saveAsObjectFile() of the RDD class.\n#'\n#' @param sc SparkContext to use\n#' @param path Path of file to read. A vector of multiple paths is allowed.\n#' @param minPartitions Minimum number of partitions to be created. If NULL, the default\n#'  value is chosen based on available parallelism.\n#' @return RDD containing serialized R objects.\n#' @seealso saveAsObjectFile\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  sc <- sparkR.init()\n#'  rdd <- objectFile(sc, \"myfile\")\n#'}\nobjectFile <- function(sc, path, minPartitions = NULL) {\n  # Allow the user to have a more flexible definiton of the text file path\n  path <- suppressWarnings(normalizePath(path))\n  # Convert a string vector of paths to a string containing comma separated paths\n  path <- paste(path, collapse = \",\")\n\n  jrdd <- callJMethod(sc, \"objectFile\", path, getMinPartitions(sc, minPartitions))\n  # Assume the RDD contains serialized R objects.\n  RDD(jrdd, \"byte\")\n}\n\n#' Create an RDD from a homogeneous list or vector.\n#'\n#' This function creates an RDD from a local homogeneous list in R. The elements\n#' in the list are split into \\code{numSlices} slices and distributed to nodes\n#' in the cluster.\n#'\n#' If size of serialized slices is larger than spark.r.maxAllocationLimit or (200MB), the function\n#' will write it to disk and send the file name to JVM. Also to make sure each slice is not\n#' larger than that limit, number of slices may be increased.\n#'\n#' In 2.2.0 we are changing how the numSlices are used/computed to handle\n#' 1 < (length(coll) / numSlices) << length(coll) better, and to get the exact number of slices.\n#' This change affects both createDataFrame and spark.lapply.\n#' In the specific one case that it is used to convert R native object into SparkDataFrame, it has\n#' always been kept at the default of 1. In the case the object is large, we are explicitly setting\n#' the parallism to numSlices (which is still 1).\n#'\n#' Specifically, we are changing to split positions to match the calculation in positions() of\n#' ParallelCollectionRDD in Spark.\n#'\n#' @param sc SparkContext to use\n#' @param coll collection to parallelize\n#' @param numSlices number of partitions to create in the RDD\n#' @return an RDD created from this collection\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' rdd <- parallelize(sc, 1:10, 2)\n#' # The RDD should contain 10 elements\n#' length(rdd)\n#'}\nparallelize <- function(sc, coll, numSlices = 1) {\n  # TODO: bound/safeguard numSlices\n  # TODO: unit tests for if the split works for all primitives\n  # TODO: support matrix, data frame, etc\n\n  # Note, for data.frame, createDataFrame turns it into a list before it calls here.\n  # nolint start\n  # suppress lintr warning: Place a space before left parenthesis, except in a function call.\n  if ((!is.list(coll) && !is.vector(coll)) || is.data.frame(coll)) {\n  # nolint end\n    if (is.data.frame(coll)) {\n      message(paste(\"context.R: A data frame is parallelized by columns.\"))\n    } else {\n      if (is.matrix(coll)) {\n        message(paste(\"context.R: A matrix is parallelized by elements.\"))\n      } else {\n        message(paste(\"context.R: parallelize() currently only supports lists and vectors.\",\n                      \"Calling as.list() to coerce coll into a list.\"))\n      }\n    }\n    coll <- as.list(coll)\n  }\n\n  sizeLimit <- getMaxAllocationLimit(sc)\n  objectSize <- object.size(coll)\n\n  # For large objects we make sure the size of each slice is also smaller than sizeLimit\n  numSerializedSlices <- max(numSlices, ceiling(objectSize / sizeLimit))\n  if (numSerializedSlices > length(coll))\n    numSerializedSlices <- length(coll)\n\n  # Generate the slice ids to put each row\n  # For instance, for numSerializedSlices of 22, length of 50\n  #  [1]  0  0  2  2  4  4  6  6  6  9  9 11 11 13 13 15 15 15 18 18 20 20 22 22 22\n  # [26] 25 25 27 27 29 29 31 31 31 34 34 36 36 38 38 40 40 40 43 43 45 45 47 47 47\n  # Notice the slice group with 3 slices (ie. 6, 15, 22) are roughly evenly spaced.\n  # We are trying to reimplement the calculation in the positions method in ParallelCollectionRDD\n  splits <- if (numSerializedSlices > 0) {\n    unlist(lapply(0: (numSerializedSlices - 1), function(x) {\n      # nolint start\n      start <- trunc((x * length(coll)) / numSerializedSlices)\n      end <- trunc(((x + 1) * length(coll)) / numSerializedSlices)\n      # nolint end\n      rep(start, end - start)\n    }))\n  } else {\n    1\n  }\n\n  slices <- split(coll, splits)\n\n  # Serialize each slice: obtain a list of raws, or a list of lists (slices) of\n  # 2-tuples of raws\n  serializedSlices <- lapply(slices, serialize, connection = NULL)\n\n  # The PRC backend cannot handle arguments larger than 2GB (INT_MAX)\n  # If serialized data is safely less than that threshold we send it over the PRC channel.\n  # Otherwise, we write it to a file and send the file name\n  if (objectSize < sizeLimit) {\n    jrdd <- callJStatic(\"org.apache.spark.api.r.RRDD\", \"createRDDFromArray\", sc, serializedSlices)\n  } else {\n    fileName <- writeToTempFile(serializedSlices)\n    jrdd <- tryCatch(callJStatic(\n        \"org.apache.spark.api.r.RRDD\", \"createRDDFromFile\", sc, fileName, as.integer(numSlices)),\n      finally = {\n        file.remove(fileName)\n    })\n  }\n\n  RDD(jrdd, \"byte\")\n}\n\ngetMaxAllocationLimit <- function(sc) {\n  conf <- callJMethod(sc, \"getConf\")\n  as.numeric(\n    callJMethod(conf,\n      \"get\",\n      \"spark.r.maxAllocationLimit\",\n      toString(.Machine$integer.max / 10) # Default to a safe value: 200MB\n  ))\n}\n\nwriteToTempFile <- function(serializedSlices) {\n  fileName <- tempfile()\n  conn <- file(fileName, \"wb\")\n  for (slice in serializedSlices) {\n    writeBin(as.integer(length(slice)), conn, endian = \"big\")\n    writeBin(slice, conn, endian = \"big\")\n  }\n  close(conn)\n  fileName\n}\n\n#' Include this specified package on all workers\n#'\n#' This function can be used to include a package on all workers before the\n#' user's code is executed. This is useful in scenarios where other R package\n#' functions are used in a function passed to functions like \\code{lapply}.\n#' NOTE: The package is assumed to be installed on every node in the Spark\n#' cluster.\n#'\n#' @param sc SparkContext to use\n#' @param pkg Package name\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  library(Matrix)\n#'\n#'  sc <- sparkR.init()\n#'  # Include the matrix library we will be using\n#'  includePackage(sc, Matrix)\n#'\n#'  generateSparse <- function(x) {\n#'    sparseMatrix(i=c(1, 2, 3), j=c(1, 2, 3), x=c(1, 2, 3))\n#'  }\n#'\n#'  rdd <- lapplyPartition(parallelize(sc, 1:2, 2L), generateSparse)\n#'  collect(rdd)\n#'}\nincludePackage <- function(sc, pkg) {\n  pkg <- as.character(substitute(pkg))\n  if (exists(\".packages\", .sparkREnv)) {\n    packages <- .sparkREnv$.packages\n  } else {\n    packages <- list()\n  }\n  packages <- c(packages, pkg)\n  .sparkREnv$.packages <- packages\n}\n\n#' Broadcast a variable to all workers\n#'\n#' Broadcast a read-only variable to the cluster, returning a \\code{Broadcast}\n#' object for reading it in distributed functions.\n#'\n#' @param sc Spark Context to use\n#' @param object Object to be broadcast\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' rdd <- parallelize(sc, 1:2, 2L)\n#'\n#' # Large Matrix object that we want to broadcast\n#' randomMat <- matrix(nrow=100, ncol=10, data=rnorm(1000))\n#' randomMatBr <- broadcast(sc, randomMat)\n#'\n#' # Use the broadcast variable inside the function\n#' useBroadcast <- function(x) {\n#'   sum(value(randomMatBr) * x)\n#' }\n#' sumRDD <- lapply(rdd, useBroadcast)\n#'}\nbroadcast <- function(sc, object) {\n  objName <- as.character(substitute(object))\n  serializedObj <- serialize(object, connection = NULL)\n\n  jBroadcast <- callJMethod(sc, \"broadcast\", serializedObj)\n  id <- as.character(callJMethod(jBroadcast, \"id\"))\n\n  Broadcast(id, object, jBroadcast, objName)\n}\n\n#' Set the checkpoint directory\n#'\n#' Set the directory under which RDDs are going to be checkpointed. The\n#' directory must be a HDFS path if running on a cluster.\n#'\n#' @param sc Spark Context to use\n#' @param dirName Directory path\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' setCheckpointDir(sc, \"~/checkpoint\")\n#' rdd <- parallelize(sc, 1:2, 2L)\n#' checkpoint(rdd)\n#'}\nsetCheckpointDirSC <- function(sc, dirName) {\n  invisible(callJMethod(sc, \"setCheckpointDir\", suppressWarnings(normalizePath(dirName))))\n}\n\n#' Add a file or directory to be downloaded with this Spark job on every node.\n#'\n#' The path passed can be either a local file, a file in HDFS (or other Hadoop-supported\n#' filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,\n#' use spark.getSparkFiles(fileName) to find its download location.\n#'\n#' A directory can be given if the recursive option is set to true.\n#' Currently directories are only supported for Hadoop-supported filesystems.\n#' Refer Hadoop-supported filesystems at \\url{https://wiki.apache.org/hadoop/HCFS}.\n#'\n#' @rdname spark.addFile\n#' @param path The path of the file to be added\n#' @param recursive Whether to add files recursively from the path. Default is FALSE.\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.addFile(\"~/myfile\")\n#'}\n#' @note spark.addFile since 2.1.0\nspark.addFile <- function(path, recursive = FALSE) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"addFile\", suppressWarnings(normalizePath(path)), recursive))\n}\n\n#' Get the root directory that contains files added through spark.addFile.\n#'\n#' @rdname spark.getSparkFilesRootDirectory\n#' @return the root directory that contains files added through spark.addFile\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.getSparkFilesRootDirectory()\n#'}\n#' @note spark.getSparkFilesRootDirectory since 2.1.0\nspark.getSparkFilesRootDirectory <- function() {\n  if (Sys.getenv(\"SPARKR_IS_RUNNING_ON_WORKER\") == \"\") {\n    # Running on driver.\n    callJStatic(\"org.apache.spark.SparkFiles\", \"getRootDirectory\")\n  } else {\n    # Running on worker.\n    Sys.getenv(\"SPARKR_SPARKFILES_ROOT_DIR\")\n  }\n}\n\n#' Get the absolute path of a file added through spark.addFile.\n#'\n#' @rdname spark.getSparkFiles\n#' @param fileName The name of the file added through spark.addFile\n#' @return the absolute path of a file added through spark.addFile.\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.getSparkFiles(\"myfile\")\n#'}\n#' @note spark.getSparkFiles since 2.1.0\nspark.getSparkFiles <- function(fileName) {\n  if (Sys.getenv(\"SPARKR_IS_RUNNING_ON_WORKER\") == \"\") {\n    # Running on driver.\n    callJStatic(\"org.apache.spark.SparkFiles\", \"get\", as.character(fileName))\n  } else {\n    # Running on worker.\n    file.path(spark.getSparkFilesRootDirectory(), as.character(fileName))\n  }\n}\n\n#' Run a function over a list of elements, distributing the computations with Spark\n#'\n#' Run a function over a list of elements, distributing the computations with Spark. Applies a\n#' function in a manner that is similar to doParallel or lapply to elements of a list.\n#' The computations are distributed using Spark. It is conceptually the same as the following code:\n#'   lapply(list, func)\n#'\n#' Known limitations:\n#' \\itemize{\n#'    \\item variable scoping and capture: compared to R's rich support for variable resolutions,\n#'    the distributed nature of SparkR limits how variables are resolved at runtime. All the\n#'    variables that are available through lexical scoping are embedded in the closure of the\n#'    function and available as read-only variables within the function. The environment variables\n#'    should be stored into temporary variables outside the function, and not directly accessed\n#'    within the function.\n#'\n#'   \\item loading external packages: In order to use a package, you need to load it inside the\n#'   closure. For example, if you rely on the MASS module, here is how you would use it:\n#'   \\preformatted{\n#'     train <- function(hyperparam) {\n#'       library(MASS)\n#'       lm.ridge(\"y ~ x+z\", data, lambda=hyperparam)\n#'       model\n#'     }\n#'   }\n#' }\n#'\n#' @rdname spark.lapply\n#' @param list the list of elements\n#' @param func a function that takes one argument.\n#' @return a list of results (the exact type being determined by the function)\n#' @export\n#' @examples\n#'\\dontrun{\n#' sparkR.session()\n#' doubled <- spark.lapply(1:10, function(x){2 * x})\n#'}\n#' @note spark.lapply since 2.0.0\nspark.lapply <- function(list, func) {\n  sc <- getSparkContext()\n  rdd <- parallelize(sc, list, length(list))\n  results <- map(rdd, func)\n  local <- collectRDD(results)\n  local\n}\n\n#' Set new log level\n#'\n#' Set new log level: \"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\"\n#'\n#' @rdname setLogLevel\n#' @param level New log level\n#' @export\n#' @examples\n#'\\dontrun{\n#' setLogLevel(\"ERROR\")\n#'}\n#' @note setLogLevel since 2.0.0\nsetLogLevel <- function(level) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"setLogLevel\", level))\n}\n\n#' Set checkpoint directory\n#'\n#' Set the directory under which SparkDataFrame are going to be checkpointed. The directory must be\n#' a HDFS path if running on a cluster.\n#'\n#' @rdname setCheckpointDir\n#' @param directory Directory path to checkpoint to\n#' @seealso \\link{checkpoint}\n#' @export\n#' @examples\n#'\\dontrun{\n#' setCheckpointDir(\"/checkpoint\")\n#'}\n#' @note setCheckpointDir since 2.2.0\nsetCheckpointDir <- function(directory) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"setCheckpointDir\", suppressWarnings(normalizePath(directory))))\n}\n" }
{ "repo_name": "JerryLead/spark", "ref": "refs/heads/master", "path": "R/pkg/R/context.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# context.R: SparkContext driven functions\n\ngetMinPartitions <- function(sc, minPartitions) {\n  if (is.null(minPartitions)) {\n    defaultParallelism <- callJMethod(sc, \"defaultParallelism\")\n    minPartitions <- min(defaultParallelism, 2)\n  }\n  as.integer(minPartitions)\n}\n\n#' Create an RDD from a text file.\n#'\n#' This function reads a text file from HDFS, a local file system (available on all\n#' nodes), or any Hadoop-supported file system URI, and creates an\n#' RDD of strings from it.\n#'\n#' @param sc SparkContext to use\n#' @param path Path of file to read. A vector of multiple paths is allowed.\n#' @param minPartitions Minimum number of partitions to be created. If NULL, the default\n#'  value is chosen based on available parallelism.\n#' @return RDD where each item is of type \\code{character}\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  sc <- sparkR.init()\n#'  lines <- textFile(sc, \"myfile.txt\")\n#'}\ntextFile <- function(sc, path, minPartitions = NULL) {\n  # Allow the user to have a more flexible definiton of the text file path\n  path <- suppressWarnings(normalizePath(path))\n  # Convert a string vector of paths to a string containing comma separated paths\n  path <- paste(path, collapse = \",\")\n\n  jrdd <- callJMethod(sc, \"textFile\", path, getMinPartitions(sc, minPartitions))\n  # jrdd is of type JavaRDD[String]\n  RDD(jrdd, \"string\")\n}\n\n#' Load an RDD saved as a SequenceFile containing serialized objects.\n#'\n#' The file to be loaded should be one that was previously generated by calling\n#' saveAsObjectFile() of the RDD class.\n#'\n#' @param sc SparkContext to use\n#' @param path Path of file to read. A vector of multiple paths is allowed.\n#' @param minPartitions Minimum number of partitions to be created. If NULL, the default\n#'  value is chosen based on available parallelism.\n#' @return RDD containing serialized R objects.\n#' @seealso saveAsObjectFile\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  sc <- sparkR.init()\n#'  rdd <- objectFile(sc, \"myfile\")\n#'}\nobjectFile <- function(sc, path, minPartitions = NULL) {\n  # Allow the user to have a more flexible definiton of the text file path\n  path <- suppressWarnings(normalizePath(path))\n  # Convert a string vector of paths to a string containing comma separated paths\n  path <- paste(path, collapse = \",\")\n\n  jrdd <- callJMethod(sc, \"objectFile\", path, getMinPartitions(sc, minPartitions))\n  # Assume the RDD contains serialized R objects.\n  RDD(jrdd, \"byte\")\n}\n\n#' Create an RDD from a homogeneous list or vector.\n#'\n#' This function creates an RDD from a local homogeneous list in R. The elements\n#' in the list are split into \\code{numSlices} slices and distributed to nodes\n#' in the cluster.\n#'\n#' If size of serialized slices is larger than spark.r.maxAllocationLimit or (200MB), the function\n#' will write it to disk and send the file name to JVM. Also to make sure each slice is not\n#' larger than that limit, number of slices may be increased.\n#'\n#' In 2.2.0 we are changing how the numSlices are used/computed to handle\n#' 1 < (length(coll) / numSlices) << length(coll) better, and to get the exact number of slices.\n#' This change affects both createDataFrame and spark.lapply.\n#' In the specific one case that it is used to convert R native object into SparkDataFrame, it has\n#' always been kept at the default of 1. In the case the object is large, we are explicitly setting\n#' the parallism to numSlices (which is still 1).\n#'\n#' Specifically, we are changing to split positions to match the calculation in positions() of\n#' ParallelCollectionRDD in Spark.\n#'\n#' @param sc SparkContext to use\n#' @param coll collection to parallelize\n#' @param numSlices number of partitions to create in the RDD\n#' @return an RDD created from this collection\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' rdd <- parallelize(sc, 1:10, 2)\n#' # The RDD should contain 10 elements\n#' length(rdd)\n#'}\nparallelize <- function(sc, coll, numSlices = 1) {\n  # TODO: bound/safeguard numSlices\n  # TODO: unit tests for if the split works for all primitives\n  # TODO: support matrix, data frame, etc\n\n  # Note, for data.frame, createDataFrame turns it into a list before it calls here.\n  # nolint start\n  # suppress lintr warning: Place a space before left parenthesis, except in a function call.\n  if ((!is.list(coll) && !is.vector(coll)) || is.data.frame(coll)) {\n  # nolint end\n    if (is.data.frame(coll)) {\n      message(paste(\"context.R: A data frame is parallelized by columns.\"))\n    } else {\n      if (is.matrix(coll)) {\n        message(paste(\"context.R: A matrix is parallelized by elements.\"))\n      } else {\n        message(paste(\"context.R: parallelize() currently only supports lists and vectors.\",\n                      \"Calling as.list() to coerce coll into a list.\"))\n      }\n    }\n    coll <- as.list(coll)\n  }\n\n  sizeLimit <- getMaxAllocationLimit(sc)\n  objectSize <- object.size(coll)\n\n  # For large objects we make sure the size of each slice is also smaller than sizeLimit\n  numSerializedSlices <- max(numSlices, ceiling(objectSize / sizeLimit))\n  if (numSerializedSlices > length(coll))\n    numSerializedSlices <- length(coll)\n\n  # Generate the slice ids to put each row\n  # For instance, for numSerializedSlices of 22, length of 50\n  #  [1]  0  0  2  2  4  4  6  6  6  9  9 11 11 13 13 15 15 15 18 18 20 20 22 22 22\n  # [26] 25 25 27 27 29 29 31 31 31 34 34 36 36 38 38 40 40 40 43 43 45 45 47 47 47\n  # Notice the slice group with 3 slices (ie. 6, 15, 22) are roughly evenly spaced.\n  # We are trying to reimplement the calculation in the positions method in ParallelCollectionRDD\n  splits <- if (numSerializedSlices > 0) {\n    unlist(lapply(0: (numSerializedSlices - 1), function(x) {\n      # nolint start\n      start <- trunc((x * length(coll)) / numSerializedSlices)\n      end <- trunc(((x + 1) * length(coll)) / numSerializedSlices)\n      # nolint end\n      rep(start, end - start)\n    }))\n  } else {\n    1\n  }\n\n  slices <- split(coll, splits)\n\n  # Serialize each slice: obtain a list of raws, or a list of lists (slices) of\n  # 2-tuples of raws\n  serializedSlices <- lapply(slices, serialize, connection = NULL)\n\n  # The PRC backend cannot handle arguments larger than 2GB (INT_MAX)\n  # If serialized data is safely less than that threshold we send it over the PRC channel.\n  # Otherwise, we write it to a file and send the file name\n  if (objectSize < sizeLimit) {\n    jrdd <- callJStatic(\"org.apache.spark.api.r.RRDD\", \"createRDDFromArray\", sc, serializedSlices)\n  } else {\n    fileName <- writeToTempFile(serializedSlices)\n    jrdd <- tryCatch(callJStatic(\n        \"org.apache.spark.api.r.RRDD\", \"createRDDFromFile\", sc, fileName, as.integer(numSlices)),\n      finally = {\n        file.remove(fileName)\n    })\n  }\n\n  RDD(jrdd, \"byte\")\n}\n\ngetMaxAllocationLimit <- function(sc) {\n  conf <- callJMethod(sc, \"getConf\")\n  as.numeric(\n    callJMethod(conf,\n      \"get\",\n      \"spark.r.maxAllocationLimit\",\n      toString(.Machine$integer.max / 10) # Default to a safe value: 200MB\n  ))\n}\n\nwriteToTempFile <- function(serializedSlices) {\n  fileName <- tempfile()\n  conn <- file(fileName, \"wb\")\n  for (slice in serializedSlices) {\n    writeBin(as.integer(length(slice)), conn, endian = \"big\")\n    writeBin(slice, conn, endian = \"big\")\n  }\n  close(conn)\n  fileName\n}\n\n#' Include this specified package on all workers\n#'\n#' This function can be used to include a package on all workers before the\n#' user's code is executed. This is useful in scenarios where other R package\n#' functions are used in a function passed to functions like \\code{lapply}.\n#' NOTE: The package is assumed to be installed on every node in the Spark\n#' cluster.\n#'\n#' @param sc SparkContext to use\n#' @param pkg Package name\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  library(Matrix)\n#'\n#'  sc <- sparkR.init()\n#'  # Include the matrix library we will be using\n#'  includePackage(sc, Matrix)\n#'\n#'  generateSparse <- function(x) {\n#'    sparseMatrix(i=c(1, 2, 3), j=c(1, 2, 3), x=c(1, 2, 3))\n#'  }\n#'\n#'  rdd <- lapplyPartition(parallelize(sc, 1:2, 2L), generateSparse)\n#'  collect(rdd)\n#'}\nincludePackage <- function(sc, pkg) {\n  pkg <- as.character(substitute(pkg))\n  if (exists(\".packages\", .sparkREnv)) {\n    packages <- .sparkREnv$.packages\n  } else {\n    packages <- list()\n  }\n  packages <- c(packages, pkg)\n  .sparkREnv$.packages <- packages\n}\n\n#' Broadcast a variable to all workers\n#'\n#' Broadcast a read-only variable to the cluster, returning a \\code{Broadcast}\n#' object for reading it in distributed functions.\n#'\n#' @param sc Spark Context to use\n#' @param object Object to be broadcast\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' rdd <- parallelize(sc, 1:2, 2L)\n#'\n#' # Large Matrix object that we want to broadcast\n#' randomMat <- matrix(nrow=100, ncol=10, data=rnorm(1000))\n#' randomMatBr <- broadcast(sc, randomMat)\n#'\n#' # Use the broadcast variable inside the function\n#' useBroadcast <- function(x) {\n#'   sum(value(randomMatBr) * x)\n#' }\n#' sumRDD <- lapply(rdd, useBroadcast)\n#'}\nbroadcast <- function(sc, object) {\n  objName <- as.character(substitute(object))\n  serializedObj <- serialize(object, connection = NULL)\n\n  jBroadcast <- callJMethod(sc, \"broadcast\", serializedObj)\n  id <- as.character(callJMethod(jBroadcast, \"id\"))\n\n  Broadcast(id, object, jBroadcast, objName)\n}\n\n#' Set the checkpoint directory\n#'\n#' Set the directory under which RDDs are going to be checkpointed. The\n#' directory must be a HDFS path if running on a cluster.\n#'\n#' @param sc Spark Context to use\n#' @param dirName Directory path\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' setCheckpointDir(sc, \"~/checkpoint\")\n#' rdd <- parallelize(sc, 1:2, 2L)\n#' checkpoint(rdd)\n#'}\nsetCheckpointDirSC <- function(sc, dirName) {\n  invisible(callJMethod(sc, \"setCheckpointDir\", suppressWarnings(normalizePath(dirName))))\n}\n\n#' Add a file or directory to be downloaded with this Spark job on every node.\n#'\n#' The path passed can be either a local file, a file in HDFS (or other Hadoop-supported\n#' filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,\n#' use spark.getSparkFiles(fileName) to find its download location.\n#'\n#' A directory can be given if the recursive option is set to true.\n#' Currently directories are only supported for Hadoop-supported filesystems.\n#' Refer Hadoop-supported filesystems at \\url{https://wiki.apache.org/hadoop/HCFS}.\n#'\n#' @rdname spark.addFile\n#' @param path The path of the file to be added\n#' @param recursive Whether to add files recursively from the path. Default is FALSE.\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.addFile(\"~/myfile\")\n#'}\n#' @note spark.addFile since 2.1.0\nspark.addFile <- function(path, recursive = FALSE) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"addFile\", suppressWarnings(normalizePath(path)), recursive))\n}\n\n#' Get the root directory that contains files added through spark.addFile.\n#'\n#' @rdname spark.getSparkFilesRootDirectory\n#' @return the root directory that contains files added through spark.addFile\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.getSparkFilesRootDirectory()\n#'}\n#' @note spark.getSparkFilesRootDirectory since 2.1.0\nspark.getSparkFilesRootDirectory <- function() {\n  if (Sys.getenv(\"SPARKR_IS_RUNNING_ON_WORKER\") == \"\") {\n    # Running on driver.\n    callJStatic(\"org.apache.spark.SparkFiles\", \"getRootDirectory\")\n  } else {\n    # Running on worker.\n    Sys.getenv(\"SPARKR_SPARKFILES_ROOT_DIR\")\n  }\n}\n\n#' Get the absolute path of a file added through spark.addFile.\n#'\n#' @rdname spark.getSparkFiles\n#' @param fileName The name of the file added through spark.addFile\n#' @return the absolute path of a file added through spark.addFile.\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.getSparkFiles(\"myfile\")\n#'}\n#' @note spark.getSparkFiles since 2.1.0\nspark.getSparkFiles <- function(fileName) {\n  if (Sys.getenv(\"SPARKR_IS_RUNNING_ON_WORKER\") == \"\") {\n    # Running on driver.\n    callJStatic(\"org.apache.spark.SparkFiles\", \"get\", as.character(fileName))\n  } else {\n    # Running on worker.\n    file.path(spark.getSparkFilesRootDirectory(), as.character(fileName))\n  }\n}\n\n#' Run a function over a list of elements, distributing the computations with Spark\n#'\n#' Run a function over a list of elements, distributing the computations with Spark. Applies a\n#' function in a manner that is similar to doParallel or lapply to elements of a list.\n#' The computations are distributed using Spark. It is conceptually the same as the following code:\n#'   lapply(list, func)\n#'\n#' Known limitations:\n#' \\itemize{\n#'    \\item variable scoping and capture: compared to R's rich support for variable resolutions,\n#'    the distributed nature of SparkR limits how variables are resolved at runtime. All the\n#'    variables that are available through lexical scoping are embedded in the closure of the\n#'    function and available as read-only variables within the function. The environment variables\n#'    should be stored into temporary variables outside the function, and not directly accessed\n#'    within the function.\n#'\n#'   \\item loading external packages: In order to use a package, you need to load it inside the\n#'   closure. For example, if you rely on the MASS module, here is how you would use it:\n#'   \\preformatted{\n#'     train <- function(hyperparam) {\n#'       library(MASS)\n#'       lm.ridge(\"y ~ x+z\", data, lambda=hyperparam)\n#'       model\n#'     }\n#'   }\n#' }\n#'\n#' @rdname spark.lapply\n#' @param list the list of elements\n#' @param func a function that takes one argument.\n#' @return a list of results (the exact type being determined by the function)\n#' @export\n#' @examples\n#'\\dontrun{\n#' sparkR.session()\n#' doubled <- spark.lapply(1:10, function(x){2 * x})\n#'}\n#' @note spark.lapply since 2.0.0\nspark.lapply <- function(list, func) {\n  sc <- getSparkContext()\n  rdd <- parallelize(sc, list, length(list))\n  results <- map(rdd, func)\n  local <- collectRDD(results)\n  local\n}\n\n#' Set new log level\n#'\n#' Set new log level: \"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\"\n#'\n#' @rdname setLogLevel\n#' @param level New log level\n#' @export\n#' @examples\n#'\\dontrun{\n#' setLogLevel(\"ERROR\")\n#'}\n#' @note setLogLevel since 2.0.0\nsetLogLevel <- function(level) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"setLogLevel\", level))\n}\n\n#' Set checkpoint directory\n#'\n#' Set the directory under which SparkDataFrame are going to be checkpointed. The directory must be\n#' a HDFS path if running on a cluster.\n#'\n#' @rdname setCheckpointDir\n#' @param directory Directory path to checkpoint to\n#' @seealso \\link{checkpoint}\n#' @export\n#' @examples\n#'\\dontrun{\n#' setCheckpointDir(\"/checkpoint\")\n#'}\n#' @note setCheckpointDir since 2.2.0\nsetCheckpointDir <- function(directory) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"setCheckpointDir\", suppressWarnings(normalizePath(directory))))\n}\n" }
{ "repo_name": "jlopezmalla/spark", "ref": "refs/heads/master", "path": "R/pkg/R/context.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# context.R: SparkContext driven functions\n\ngetMinPartitions <- function(sc, minPartitions) {\n  if (is.null(minPartitions)) {\n    defaultParallelism <- callJMethod(sc, \"defaultParallelism\")\n    minPartitions <- min(defaultParallelism, 2)\n  }\n  as.integer(minPartitions)\n}\n\n#' Create an RDD from a text file.\n#'\n#' This function reads a text file from HDFS, a local file system (available on all\n#' nodes), or any Hadoop-supported file system URI, and creates an\n#' RDD of strings from it.\n#'\n#' @param sc SparkContext to use\n#' @param path Path of file to read. A vector of multiple paths is allowed.\n#' @param minPartitions Minimum number of partitions to be created. If NULL, the default\n#'  value is chosen based on available parallelism.\n#' @return RDD where each item is of type \\code{character}\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  sc <- sparkR.init()\n#'  lines <- textFile(sc, \"myfile.txt\")\n#'}\ntextFile <- function(sc, path, minPartitions = NULL) {\n  # Allow the user to have a more flexible definiton of the text file path\n  path <- suppressWarnings(normalizePath(path))\n  # Convert a string vector of paths to a string containing comma separated paths\n  path <- paste(path, collapse = \",\")\n\n  jrdd <- callJMethod(sc, \"textFile\", path, getMinPartitions(sc, minPartitions))\n  # jrdd is of type JavaRDD[String]\n  RDD(jrdd, \"string\")\n}\n\n#' Load an RDD saved as a SequenceFile containing serialized objects.\n#'\n#' The file to be loaded should be one that was previously generated by calling\n#' saveAsObjectFile() of the RDD class.\n#'\n#' @param sc SparkContext to use\n#' @param path Path of file to read. A vector of multiple paths is allowed.\n#' @param minPartitions Minimum number of partitions to be created. If NULL, the default\n#'  value is chosen based on available parallelism.\n#' @return RDD containing serialized R objects.\n#' @seealso saveAsObjectFile\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  sc <- sparkR.init()\n#'  rdd <- objectFile(sc, \"myfile\")\n#'}\nobjectFile <- function(sc, path, minPartitions = NULL) {\n  # Allow the user to have a more flexible definiton of the text file path\n  path <- suppressWarnings(normalizePath(path))\n  # Convert a string vector of paths to a string containing comma separated paths\n  path <- paste(path, collapse = \",\")\n\n  jrdd <- callJMethod(sc, \"objectFile\", path, getMinPartitions(sc, minPartitions))\n  # Assume the RDD contains serialized R objects.\n  RDD(jrdd, \"byte\")\n}\n\n#' Create an RDD from a homogeneous list or vector.\n#'\n#' This function creates an RDD from a local homogeneous list in R. The elements\n#' in the list are split into \\code{numSlices} slices and distributed to nodes\n#' in the cluster.\n#'\n#' If size of serialized slices is larger than spark.r.maxAllocationLimit or (200MB), the function\n#' will write it to disk and send the file name to JVM. Also to make sure each slice is not\n#' larger than that limit, number of slices may be increased.\n#'\n#' In 2.2.0 we are changing how the numSlices are used/computed to handle\n#' 1 < (length(coll) / numSlices) << length(coll) better, and to get the exact number of slices.\n#' This change affects both createDataFrame and spark.lapply.\n#' In the specific one case that it is used to convert R native object into SparkDataFrame, it has\n#' always been kept at the default of 1. In the case the object is large, we are explicitly setting\n#' the parallism to numSlices (which is still 1).\n#'\n#' Specifically, we are changing to split positions to match the calculation in positions() of\n#' ParallelCollectionRDD in Spark.\n#'\n#' @param sc SparkContext to use\n#' @param coll collection to parallelize\n#' @param numSlices number of partitions to create in the RDD\n#' @return an RDD created from this collection\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' rdd <- parallelize(sc, 1:10, 2)\n#' # The RDD should contain 10 elements\n#' length(rdd)\n#'}\nparallelize <- function(sc, coll, numSlices = 1) {\n  # TODO: bound/safeguard numSlices\n  # TODO: unit tests for if the split works for all primitives\n  # TODO: support matrix, data frame, etc\n\n  # Note, for data.frame, createDataFrame turns it into a list before it calls here.\n  # nolint start\n  # suppress lintr warning: Place a space before left parenthesis, except in a function call.\n  if ((!is.list(coll) && !is.vector(coll)) || is.data.frame(coll)) {\n  # nolint end\n    if (is.data.frame(coll)) {\n      message(paste(\"context.R: A data frame is parallelized by columns.\"))\n    } else {\n      if (is.matrix(coll)) {\n        message(paste(\"context.R: A matrix is parallelized by elements.\"))\n      } else {\n        message(paste(\"context.R: parallelize() currently only supports lists and vectors.\",\n                      \"Calling as.list() to coerce coll into a list.\"))\n      }\n    }\n    coll <- as.list(coll)\n  }\n\n  sizeLimit <- getMaxAllocationLimit(sc)\n  objectSize <- object.size(coll)\n\n  # For large objects we make sure the size of each slice is also smaller than sizeLimit\n  numSerializedSlices <- max(numSlices, ceiling(objectSize / sizeLimit))\n  if (numSerializedSlices > length(coll))\n    numSerializedSlices <- length(coll)\n\n  # Generate the slice ids to put each row\n  # For instance, for numSerializedSlices of 22, length of 50\n  #  [1]  0  0  2  2  4  4  6  6  6  9  9 11 11 13 13 15 15 15 18 18 20 20 22 22 22\n  # [26] 25 25 27 27 29 29 31 31 31 34 34 36 36 38 38 40 40 40 43 43 45 45 47 47 47\n  # Notice the slice group with 3 slices (ie. 6, 15, 22) are roughly evenly spaced.\n  # We are trying to reimplement the calculation in the positions method in ParallelCollectionRDD\n  splits <- if (numSerializedSlices > 0) {\n    unlist(lapply(0: (numSerializedSlices - 1), function(x) {\n      # nolint start\n      start <- trunc((x * length(coll)) / numSerializedSlices)\n      end <- trunc(((x + 1) * length(coll)) / numSerializedSlices)\n      # nolint end\n      rep(start, end - start)\n    }))\n  } else {\n    1\n  }\n\n  slices <- split(coll, splits)\n\n  # Serialize each slice: obtain a list of raws, or a list of lists (slices) of\n  # 2-tuples of raws\n  serializedSlices <- lapply(slices, serialize, connection = NULL)\n\n  # The PRC backend cannot handle arguments larger than 2GB (INT_MAX)\n  # If serialized data is safely less than that threshold we send it over the PRC channel.\n  # Otherwise, we write it to a file and send the file name\n  if (objectSize < sizeLimit) {\n    jrdd <- callJStatic(\"org.apache.spark.api.r.RRDD\", \"createRDDFromArray\", sc, serializedSlices)\n  } else {\n    fileName <- writeToTempFile(serializedSlices)\n    jrdd <- tryCatch(callJStatic(\n        \"org.apache.spark.api.r.RRDD\", \"createRDDFromFile\", sc, fileName, as.integer(numSlices)),\n      finally = {\n        file.remove(fileName)\n    })\n  }\n\n  RDD(jrdd, \"byte\")\n}\n\ngetMaxAllocationLimit <- function(sc) {\n  conf <- callJMethod(sc, \"getConf\")\n  as.numeric(\n    callJMethod(conf,\n      \"get\",\n      \"spark.r.maxAllocationLimit\",\n      toString(.Machine$integer.max / 10) # Default to a safe value: 200MB\n  ))\n}\n\nwriteToTempFile <- function(serializedSlices) {\n  fileName <- tempfile()\n  conn <- file(fileName, \"wb\")\n  for (slice in serializedSlices) {\n    writeBin(as.integer(length(slice)), conn, endian = \"big\")\n    writeBin(slice, conn, endian = \"big\")\n  }\n  close(conn)\n  fileName\n}\n\n#' Include this specified package on all workers\n#'\n#' This function can be used to include a package on all workers before the\n#' user's code is executed. This is useful in scenarios where other R package\n#' functions are used in a function passed to functions like \\code{lapply}.\n#' NOTE: The package is assumed to be installed on every node in the Spark\n#' cluster.\n#'\n#' @param sc SparkContext to use\n#' @param pkg Package name\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  library(Matrix)\n#'\n#'  sc <- sparkR.init()\n#'  # Include the matrix library we will be using\n#'  includePackage(sc, Matrix)\n#'\n#'  generateSparse <- function(x) {\n#'    sparseMatrix(i=c(1, 2, 3), j=c(1, 2, 3), x=c(1, 2, 3))\n#'  }\n#'\n#'  rdd <- lapplyPartition(parallelize(sc, 1:2, 2L), generateSparse)\n#'  collect(rdd)\n#'}\nincludePackage <- function(sc, pkg) {\n  pkg <- as.character(substitute(pkg))\n  if (exists(\".packages\", .sparkREnv)) {\n    packages <- .sparkREnv$.packages\n  } else {\n    packages <- list()\n  }\n  packages <- c(packages, pkg)\n  .sparkREnv$.packages <- packages\n}\n\n#' Broadcast a variable to all workers\n#'\n#' Broadcast a read-only variable to the cluster, returning a \\code{Broadcast}\n#' object for reading it in distributed functions.\n#'\n#' @param sc Spark Context to use\n#' @param object Object to be broadcast\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' rdd <- parallelize(sc, 1:2, 2L)\n#'\n#' # Large Matrix object that we want to broadcast\n#' randomMat <- matrix(nrow=100, ncol=10, data=rnorm(1000))\n#' randomMatBr <- broadcast(sc, randomMat)\n#'\n#' # Use the broadcast variable inside the function\n#' useBroadcast <- function(x) {\n#'   sum(value(randomMatBr) * x)\n#' }\n#' sumRDD <- lapply(rdd, useBroadcast)\n#'}\nbroadcast <- function(sc, object) {\n  objName <- as.character(substitute(object))\n  serializedObj <- serialize(object, connection = NULL)\n\n  jBroadcast <- callJMethod(sc, \"broadcast\", serializedObj)\n  id <- as.character(callJMethod(jBroadcast, \"id\"))\n\n  Broadcast(id, object, jBroadcast, objName)\n}\n\n#' Set the checkpoint directory\n#'\n#' Set the directory under which RDDs are going to be checkpointed. The\n#' directory must be a HDFS path if running on a cluster.\n#'\n#' @param sc Spark Context to use\n#' @param dirName Directory path\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' setCheckpointDir(sc, \"~/checkpoint\")\n#' rdd <- parallelize(sc, 1:2, 2L)\n#' checkpoint(rdd)\n#'}\nsetCheckpointDirSC <- function(sc, dirName) {\n  invisible(callJMethod(sc, \"setCheckpointDir\", suppressWarnings(normalizePath(dirName))))\n}\n\n#' Add a file or directory to be downloaded with this Spark job on every node.\n#'\n#' The path passed can be either a local file, a file in HDFS (or other Hadoop-supported\n#' filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,\n#' use spark.getSparkFiles(fileName) to find its download location.\n#'\n#' A directory can be given if the recursive option is set to true.\n#' Currently directories are only supported for Hadoop-supported filesystems.\n#' Refer Hadoop-supported filesystems at \\url{https://wiki.apache.org/hadoop/HCFS}.\n#'\n#' @rdname spark.addFile\n#' @param path The path of the file to be added\n#' @param recursive Whether to add files recursively from the path. Default is FALSE.\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.addFile(\"~/myfile\")\n#'}\n#' @note spark.addFile since 2.1.0\nspark.addFile <- function(path, recursive = FALSE) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"addFile\", suppressWarnings(normalizePath(path)), recursive))\n}\n\n#' Get the root directory that contains files added through spark.addFile.\n#'\n#' @rdname spark.getSparkFilesRootDirectory\n#' @return the root directory that contains files added through spark.addFile\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.getSparkFilesRootDirectory()\n#'}\n#' @note spark.getSparkFilesRootDirectory since 2.1.0\nspark.getSparkFilesRootDirectory <- function() {\n  if (Sys.getenv(\"SPARKR_IS_RUNNING_ON_WORKER\") == \"\") {\n    # Running on driver.\n    callJStatic(\"org.apache.spark.SparkFiles\", \"getRootDirectory\")\n  } else {\n    # Running on worker.\n    Sys.getenv(\"SPARKR_SPARKFILES_ROOT_DIR\")\n  }\n}\n\n#' Get the absolute path of a file added through spark.addFile.\n#'\n#' @rdname spark.getSparkFiles\n#' @param fileName The name of the file added through spark.addFile\n#' @return the absolute path of a file added through spark.addFile.\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.getSparkFiles(\"myfile\")\n#'}\n#' @note spark.getSparkFiles since 2.1.0\nspark.getSparkFiles <- function(fileName) {\n  if (Sys.getenv(\"SPARKR_IS_RUNNING_ON_WORKER\") == \"\") {\n    # Running on driver.\n    callJStatic(\"org.apache.spark.SparkFiles\", \"get\", as.character(fileName))\n  } else {\n    # Running on worker.\n    file.path(spark.getSparkFilesRootDirectory(), as.character(fileName))\n  }\n}\n\n#' Run a function over a list of elements, distributing the computations with Spark\n#'\n#' Run a function over a list of elements, distributing the computations with Spark. Applies a\n#' function in a manner that is similar to doParallel or lapply to elements of a list.\n#' The computations are distributed using Spark. It is conceptually the same as the following code:\n#'   lapply(list, func)\n#'\n#' Known limitations:\n#' \\itemize{\n#'    \\item variable scoping and capture: compared to R's rich support for variable resolutions,\n#'    the distributed nature of SparkR limits how variables are resolved at runtime. All the\n#'    variables that are available through lexical scoping are embedded in the closure of the\n#'    function and available as read-only variables within the function. The environment variables\n#'    should be stored into temporary variables outside the function, and not directly accessed\n#'    within the function.\n#'\n#'   \\item loading external packages: In order to use a package, you need to load it inside the\n#'   closure. For example, if you rely on the MASS module, here is how you would use it:\n#'   \\preformatted{\n#'     train <- function(hyperparam) {\n#'       library(MASS)\n#'       lm.ridge(\"y ~ x+z\", data, lambda=hyperparam)\n#'       model\n#'     }\n#'   }\n#' }\n#'\n#' @rdname spark.lapply\n#' @param list the list of elements\n#' @param func a function that takes one argument.\n#' @return a list of results (the exact type being determined by the function)\n#' @export\n#' @examples\n#'\\dontrun{\n#' sparkR.session()\n#' doubled <- spark.lapply(1:10, function(x){2 * x})\n#'}\n#' @note spark.lapply since 2.0.0\nspark.lapply <- function(list, func) {\n  sc <- getSparkContext()\n  rdd <- parallelize(sc, list, length(list))\n  results <- map(rdd, func)\n  local <- collectRDD(results)\n  local\n}\n\n#' Set new log level\n#'\n#' Set new log level: \"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\"\n#'\n#' @rdname setLogLevel\n#' @param level New log level\n#' @export\n#' @examples\n#'\\dontrun{\n#' setLogLevel(\"ERROR\")\n#'}\n#' @note setLogLevel since 2.0.0\nsetLogLevel <- function(level) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"setLogLevel\", level))\n}\n\n#' Set checkpoint directory\n#'\n#' Set the directory under which SparkDataFrame are going to be checkpointed. The directory must be\n#' a HDFS path if running on a cluster.\n#'\n#' @rdname setCheckpointDir\n#' @param directory Directory path to checkpoint to\n#' @seealso \\link{checkpoint}\n#' @export\n#' @examples\n#'\\dontrun{\n#' setCheckpointDir(\"/checkpoint\")\n#'}\n#' @note setCheckpointDir since 2.2.0\nsetCheckpointDir <- function(directory) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"setCheckpointDir\", suppressWarnings(normalizePath(directory))))\n}\n" }
{ "repo_name": "patrick-nicholson/spark", "ref": "refs/heads/master", "path": "R/pkg/R/context.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# context.R: SparkContext driven functions\n\ngetMinPartitions <- function(sc, minPartitions) {\n  if (is.null(minPartitions)) {\n    defaultParallelism <- callJMethod(sc, \"defaultParallelism\")\n    minPartitions <- min(defaultParallelism, 2)\n  }\n  as.integer(minPartitions)\n}\n\n#' Create an RDD from a text file.\n#'\n#' This function reads a text file from HDFS, a local file system (available on all\n#' nodes), or any Hadoop-supported file system URI, and creates an\n#' RDD of strings from it.\n#'\n#' @param sc SparkContext to use\n#' @param path Path of file to read. A vector of multiple paths is allowed.\n#' @param minPartitions Minimum number of partitions to be created. If NULL, the default\n#'  value is chosen based on available parallelism.\n#' @return RDD where each item is of type \\code{character}\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  sc <- sparkR.init()\n#'  lines <- textFile(sc, \"myfile.txt\")\n#'}\ntextFile <- function(sc, path, minPartitions = NULL) {\n  # Allow the user to have a more flexible definiton of the text file path\n  path <- suppressWarnings(normalizePath(path))\n  # Convert a string vector of paths to a string containing comma separated paths\n  path <- paste(path, collapse = \",\")\n\n  jrdd <- callJMethod(sc, \"textFile\", path, getMinPartitions(sc, minPartitions))\n  # jrdd is of type JavaRDD[String]\n  RDD(jrdd, \"string\")\n}\n\n#' Load an RDD saved as a SequenceFile containing serialized objects.\n#'\n#' The file to be loaded should be one that was previously generated by calling\n#' saveAsObjectFile() of the RDD class.\n#'\n#' @param sc SparkContext to use\n#' @param path Path of file to read. A vector of multiple paths is allowed.\n#' @param minPartitions Minimum number of partitions to be created. If NULL, the default\n#'  value is chosen based on available parallelism.\n#' @return RDD containing serialized R objects.\n#' @seealso saveAsObjectFile\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  sc <- sparkR.init()\n#'  rdd <- objectFile(sc, \"myfile\")\n#'}\nobjectFile <- function(sc, path, minPartitions = NULL) {\n  # Allow the user to have a more flexible definiton of the text file path\n  path <- suppressWarnings(normalizePath(path))\n  # Convert a string vector of paths to a string containing comma separated paths\n  path <- paste(path, collapse = \",\")\n\n  jrdd <- callJMethod(sc, \"objectFile\", path, getMinPartitions(sc, minPartitions))\n  # Assume the RDD contains serialized R objects.\n  RDD(jrdd, \"byte\")\n}\n\n#' Create an RDD from a homogeneous list or vector.\n#'\n#' This function creates an RDD from a local homogeneous list in R. The elements\n#' in the list are split into \\code{numSlices} slices and distributed to nodes\n#' in the cluster.\n#'\n#' If size of serialized slices is larger than spark.r.maxAllocationLimit or (200MB), the function\n#' will write it to disk and send the file name to JVM. Also to make sure each slice is not\n#' larger than that limit, number of slices may be increased.\n#'\n#' In 2.2.0 we are changing how the numSlices are used/computed to handle\n#' 1 < (length(coll) / numSlices) << length(coll) better, and to get the exact number of slices.\n#' This change affects both createDataFrame and spark.lapply.\n#' In the specific one case that it is used to convert R native object into SparkDataFrame, it has\n#' always been kept at the default of 1. In the case the object is large, we are explicitly setting\n#' the parallism to numSlices (which is still 1).\n#'\n#' Specifically, we are changing to split positions to match the calculation in positions() of\n#' ParallelCollectionRDD in Spark.\n#'\n#' @param sc SparkContext to use\n#' @param coll collection to parallelize\n#' @param numSlices number of partitions to create in the RDD\n#' @return an RDD created from this collection\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' rdd <- parallelize(sc, 1:10, 2)\n#' # The RDD should contain 10 elements\n#' length(rdd)\n#'}\nparallelize <- function(sc, coll, numSlices = 1) {\n  # TODO: bound/safeguard numSlices\n  # TODO: unit tests for if the split works for all primitives\n  # TODO: support matrix, data frame, etc\n\n  # Note, for data.frame, createDataFrame turns it into a list before it calls here.\n  # nolint start\n  # suppress lintr warning: Place a space before left parenthesis, except in a function call.\n  if ((!is.list(coll) && !is.vector(coll)) || is.data.frame(coll)) {\n  # nolint end\n    if (is.data.frame(coll)) {\n      message(paste(\"context.R: A data frame is parallelized by columns.\"))\n    } else {\n      if (is.matrix(coll)) {\n        message(paste(\"context.R: A matrix is parallelized by elements.\"))\n      } else {\n        message(paste(\"context.R: parallelize() currently only supports lists and vectors.\",\n                      \"Calling as.list() to coerce coll into a list.\"))\n      }\n    }\n    coll <- as.list(coll)\n  }\n\n  sizeLimit <- getMaxAllocationLimit(sc)\n  objectSize <- object.size(coll)\n\n  # For large objects we make sure the size of each slice is also smaller than sizeLimit\n  numSerializedSlices <- max(numSlices, ceiling(objectSize / sizeLimit))\n  if (numSerializedSlices > length(coll))\n    numSerializedSlices <- length(coll)\n\n  # Generate the slice ids to put each row\n  # For instance, for numSerializedSlices of 22, length of 50\n  #  [1]  0  0  2  2  4  4  6  6  6  9  9 11 11 13 13 15 15 15 18 18 20 20 22 22 22\n  # [26] 25 25 27 27 29 29 31 31 31 34 34 36 36 38 38 40 40 40 43 43 45 45 47 47 47\n  # Notice the slice group with 3 slices (ie. 6, 15, 22) are roughly evenly spaced.\n  # We are trying to reimplement the calculation in the positions method in ParallelCollectionRDD\n  splits <- if (numSerializedSlices > 0) {\n    unlist(lapply(0: (numSerializedSlices - 1), function(x) {\n      # nolint start\n      start <- trunc((x * length(coll)) / numSerializedSlices)\n      end <- trunc(((x + 1) * length(coll)) / numSerializedSlices)\n      # nolint end\n      rep(start, end - start)\n    }))\n  } else {\n    1\n  }\n\n  slices <- split(coll, splits)\n\n  # Serialize each slice: obtain a list of raws, or a list of lists (slices) of\n  # 2-tuples of raws\n  serializedSlices <- lapply(slices, serialize, connection = NULL)\n\n  # The PRC backend cannot handle arguments larger than 2GB (INT_MAX)\n  # If serialized data is safely less than that threshold we send it over the PRC channel.\n  # Otherwise, we write it to a file and send the file name\n  if (objectSize < sizeLimit) {\n    jrdd <- callJStatic(\"org.apache.spark.api.r.RRDD\", \"createRDDFromArray\", sc, serializedSlices)\n  } else {\n    fileName <- writeToTempFile(serializedSlices)\n    jrdd <- tryCatch(callJStatic(\n        \"org.apache.spark.api.r.RRDD\", \"createRDDFromFile\", sc, fileName, as.integer(numSlices)),\n      finally = {\n        file.remove(fileName)\n    })\n  }\n\n  RDD(jrdd, \"byte\")\n}\n\ngetMaxAllocationLimit <- function(sc) {\n  conf <- callJMethod(sc, \"getConf\")\n  as.numeric(\n    callJMethod(conf,\n      \"get\",\n      \"spark.r.maxAllocationLimit\",\n      toString(.Machine$integer.max / 10) # Default to a safe value: 200MB\n  ))\n}\n\nwriteToTempFile <- function(serializedSlices) {\n  fileName <- tempfile()\n  conn <- file(fileName, \"wb\")\n  for (slice in serializedSlices) {\n    writeBin(as.integer(length(slice)), conn, endian = \"big\")\n    writeBin(slice, conn, endian = \"big\")\n  }\n  close(conn)\n  fileName\n}\n\n#' Include this specified package on all workers\n#'\n#' This function can be used to include a package on all workers before the\n#' user's code is executed. This is useful in scenarios where other R package\n#' functions are used in a function passed to functions like \\code{lapply}.\n#' NOTE: The package is assumed to be installed on every node in the Spark\n#' cluster.\n#'\n#' @param sc SparkContext to use\n#' @param pkg Package name\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  library(Matrix)\n#'\n#'  sc <- sparkR.init()\n#'  # Include the matrix library we will be using\n#'  includePackage(sc, Matrix)\n#'\n#'  generateSparse <- function(x) {\n#'    sparseMatrix(i=c(1, 2, 3), j=c(1, 2, 3), x=c(1, 2, 3))\n#'  }\n#'\n#'  rdd <- lapplyPartition(parallelize(sc, 1:2, 2L), generateSparse)\n#'  collect(rdd)\n#'}\nincludePackage <- function(sc, pkg) {\n  pkg <- as.character(substitute(pkg))\n  if (exists(\".packages\", .sparkREnv)) {\n    packages <- .sparkREnv$.packages\n  } else {\n    packages <- list()\n  }\n  packages <- c(packages, pkg)\n  .sparkREnv$.packages <- packages\n}\n\n#' Broadcast a variable to all workers\n#'\n#' Broadcast a read-only variable to the cluster, returning a \\code{Broadcast}\n#' object for reading it in distributed functions.\n#'\n#' @param sc Spark Context to use\n#' @param object Object to be broadcast\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' rdd <- parallelize(sc, 1:2, 2L)\n#'\n#' # Large Matrix object that we want to broadcast\n#' randomMat <- matrix(nrow=100, ncol=10, data=rnorm(1000))\n#' randomMatBr <- broadcast(sc, randomMat)\n#'\n#' # Use the broadcast variable inside the function\n#' useBroadcast <- function(x) {\n#'   sum(value(randomMatBr) * x)\n#' }\n#' sumRDD <- lapply(rdd, useBroadcast)\n#'}\nbroadcast <- function(sc, object) {\n  objName <- as.character(substitute(object))\n  serializedObj <- serialize(object, connection = NULL)\n\n  jBroadcast <- callJMethod(sc, \"broadcast\", serializedObj)\n  id <- as.character(callJMethod(jBroadcast, \"id\"))\n\n  Broadcast(id, object, jBroadcast, objName)\n}\n\n#' Set the checkpoint directory\n#'\n#' Set the directory under which RDDs are going to be checkpointed. The\n#' directory must be a HDFS path if running on a cluster.\n#'\n#' @param sc Spark Context to use\n#' @param dirName Directory path\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' setCheckpointDir(sc, \"~/checkpoint\")\n#' rdd <- parallelize(sc, 1:2, 2L)\n#' checkpoint(rdd)\n#'}\nsetCheckpointDirSC <- function(sc, dirName) {\n  invisible(callJMethod(sc, \"setCheckpointDir\", suppressWarnings(normalizePath(dirName))))\n}\n\n#' Add a file or directory to be downloaded with this Spark job on every node.\n#'\n#' The path passed can be either a local file, a file in HDFS (or other Hadoop-supported\n#' filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,\n#' use spark.getSparkFiles(fileName) to find its download location.\n#'\n#' A directory can be given if the recursive option is set to true.\n#' Currently directories are only supported for Hadoop-supported filesystems.\n#' Refer Hadoop-supported filesystems at \\url{https://wiki.apache.org/hadoop/HCFS}.\n#'\n#' @rdname spark.addFile\n#' @param path The path of the file to be added\n#' @param recursive Whether to add files recursively from the path. Default is FALSE.\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.addFile(\"~/myfile\")\n#'}\n#' @note spark.addFile since 2.1.0\nspark.addFile <- function(path, recursive = FALSE) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"addFile\", suppressWarnings(normalizePath(path)), recursive))\n}\n\n#' Get the root directory that contains files added through spark.addFile.\n#'\n#' @rdname spark.getSparkFilesRootDirectory\n#' @return the root directory that contains files added through spark.addFile\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.getSparkFilesRootDirectory()\n#'}\n#' @note spark.getSparkFilesRootDirectory since 2.1.0\nspark.getSparkFilesRootDirectory <- function() {\n  if (Sys.getenv(\"SPARKR_IS_RUNNING_ON_WORKER\") == \"\") {\n    # Running on driver.\n    callJStatic(\"org.apache.spark.SparkFiles\", \"getRootDirectory\")\n  } else {\n    # Running on worker.\n    Sys.getenv(\"SPARKR_SPARKFILES_ROOT_DIR\")\n  }\n}\n\n#' Get the absolute path of a file added through spark.addFile.\n#'\n#' @rdname spark.getSparkFiles\n#' @param fileName The name of the file added through spark.addFile\n#' @return the absolute path of a file added through spark.addFile.\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.getSparkFiles(\"myfile\")\n#'}\n#' @note spark.getSparkFiles since 2.1.0\nspark.getSparkFiles <- function(fileName) {\n  if (Sys.getenv(\"SPARKR_IS_RUNNING_ON_WORKER\") == \"\") {\n    # Running on driver.\n    callJStatic(\"org.apache.spark.SparkFiles\", \"get\", as.character(fileName))\n  } else {\n    # Running on worker.\n    file.path(spark.getSparkFilesRootDirectory(), as.character(fileName))\n  }\n}\n\n#' Run a function over a list of elements, distributing the computations with Spark\n#'\n#' Run a function over a list of elements, distributing the computations with Spark. Applies a\n#' function in a manner that is similar to doParallel or lapply to elements of a list.\n#' The computations are distributed using Spark. It is conceptually the same as the following code:\n#'   lapply(list, func)\n#'\n#' Known limitations:\n#' \\itemize{\n#'    \\item variable scoping and capture: compared to R's rich support for variable resolutions,\n#'    the distributed nature of SparkR limits how variables are resolved at runtime. All the\n#'    variables that are available through lexical scoping are embedded in the closure of the\n#'    function and available as read-only variables within the function. The environment variables\n#'    should be stored into temporary variables outside the function, and not directly accessed\n#'    within the function.\n#'\n#'   \\item loading external packages: In order to use a package, you need to load it inside the\n#'   closure. For example, if you rely on the MASS module, here is how you would use it:\n#'   \\preformatted{\n#'     train <- function(hyperparam) {\n#'       library(MASS)\n#'       lm.ridge(\"y ~ x+z\", data, lambda=hyperparam)\n#'       model\n#'     }\n#'   }\n#' }\n#'\n#' @rdname spark.lapply\n#' @param list the list of elements\n#' @param func a function that takes one argument.\n#' @return a list of results (the exact type being determined by the function)\n#' @export\n#' @examples\n#'\\dontrun{\n#' sparkR.session()\n#' doubled <- spark.lapply(1:10, function(x){2 * x})\n#'}\n#' @note spark.lapply since 2.0.0\nspark.lapply <- function(list, func) {\n  sc <- getSparkContext()\n  rdd <- parallelize(sc, list, length(list))\n  results <- map(rdd, func)\n  local <- collectRDD(results)\n  local\n}\n\n#' Set new log level\n#'\n#' Set new log level: \"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\"\n#'\n#' @rdname setLogLevel\n#' @param level New log level\n#' @export\n#' @examples\n#'\\dontrun{\n#' setLogLevel(\"ERROR\")\n#'}\n#' @note setLogLevel since 2.0.0\nsetLogLevel <- function(level) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"setLogLevel\", level))\n}\n\n#' Set checkpoint directory\n#'\n#' Set the directory under which SparkDataFrame are going to be checkpointed. The directory must be\n#' a HDFS path if running on a cluster.\n#'\n#' @rdname setCheckpointDir\n#' @param directory Directory path to checkpoint to\n#' @seealso \\link{checkpoint}\n#' @export\n#' @examples\n#'\\dontrun{\n#' setCheckpointDir(\"/checkpoint\")\n#'}\n#' @note setCheckpointDir since 2.2.0\nsetCheckpointDir <- function(directory) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"setCheckpointDir\", suppressWarnings(normalizePath(directory))))\n}\n" }
{ "repo_name": "MLnick/spark", "ref": "refs/heads/master", "path": "R/pkg/R/context.R", "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# context.R: SparkContext driven functions\n\ngetMinPartitions <- function(sc, minPartitions) {\n  if (is.null(minPartitions)) {\n    defaultParallelism <- callJMethod(sc, \"defaultParallelism\")\n    minPartitions <- min(defaultParallelism, 2)\n  }\n  as.integer(minPartitions)\n}\n\n#' Create an RDD from a text file.\n#'\n#' This function reads a text file from HDFS, a local file system (available on all\n#' nodes), or any Hadoop-supported file system URI, and creates an\n#' RDD of strings from it.\n#'\n#' @param sc SparkContext to use\n#' @param path Path of file to read. A vector of multiple paths is allowed.\n#' @param minPartitions Minimum number of partitions to be created. If NULL, the default\n#'  value is chosen based on available parallelism.\n#' @return RDD where each item is of type \\code{character}\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  sc <- sparkR.init()\n#'  lines <- textFile(sc, \"myfile.txt\")\n#'}\ntextFile <- function(sc, path, minPartitions = NULL) {\n  # Allow the user to have a more flexible definiton of the text file path\n  path <- suppressWarnings(normalizePath(path))\n  # Convert a string vector of paths to a string containing comma separated paths\n  path <- paste(path, collapse = \",\")\n\n  jrdd <- callJMethod(sc, \"textFile\", path, getMinPartitions(sc, minPartitions))\n  # jrdd is of type JavaRDD[String]\n  RDD(jrdd, \"string\")\n}\n\n#' Load an RDD saved as a SequenceFile containing serialized objects.\n#'\n#' The file to be loaded should be one that was previously generated by calling\n#' saveAsObjectFile() of the RDD class.\n#'\n#' @param sc SparkContext to use\n#' @param path Path of file to read. A vector of multiple paths is allowed.\n#' @param minPartitions Minimum number of partitions to be created. If NULL, the default\n#'  value is chosen based on available parallelism.\n#' @return RDD containing serialized R objects.\n#' @seealso saveAsObjectFile\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  sc <- sparkR.init()\n#'  rdd <- objectFile(sc, \"myfile\")\n#'}\nobjectFile <- function(sc, path, minPartitions = NULL) {\n  # Allow the user to have a more flexible definiton of the text file path\n  path <- suppressWarnings(normalizePath(path))\n  # Convert a string vector of paths to a string containing comma separated paths\n  path <- paste(path, collapse = \",\")\n\n  jrdd <- callJMethod(sc, \"objectFile\", path, getMinPartitions(sc, minPartitions))\n  # Assume the RDD contains serialized R objects.\n  RDD(jrdd, \"byte\")\n}\n\n#' Create an RDD from a homogeneous list or vector.\n#'\n#' This function creates an RDD from a local homogeneous list in R. The elements\n#' in the list are split into \\code{numSlices} slices and distributed to nodes\n#' in the cluster.\n#'\n#' If size of serialized slices is larger than spark.r.maxAllocationLimit or (200MB), the function\n#' will write it to disk and send the file name to JVM. Also to make sure each slice is not\n#' larger than that limit, number of slices may be increased.\n#'\n#' In 2.2.0 we are changing how the numSlices are used/computed to handle\n#' 1 < (length(coll) / numSlices) << length(coll) better, and to get the exact number of slices.\n#' This change affects both createDataFrame and spark.lapply.\n#' In the specific one case that it is used to convert R native object into SparkDataFrame, it has\n#' always been kept at the default of 1. In the case the object is large, we are explicitly setting\n#' the parallism to numSlices (which is still 1).\n#'\n#' Specifically, we are changing to split positions to match the calculation in positions() of\n#' ParallelCollectionRDD in Spark.\n#'\n#' @param sc SparkContext to use\n#' @param coll collection to parallelize\n#' @param numSlices number of partitions to create in the RDD\n#' @return an RDD created from this collection\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' rdd <- parallelize(sc, 1:10, 2)\n#' # The RDD should contain 10 elements\n#' length(rdd)\n#'}\nparallelize <- function(sc, coll, numSlices = 1) {\n  # TODO: bound/safeguard numSlices\n  # TODO: unit tests for if the split works for all primitives\n  # TODO: support matrix, data frame, etc\n\n  # Note, for data.frame, createDataFrame turns it into a list before it calls here.\n  # nolint start\n  # suppress lintr warning: Place a space before left parenthesis, except in a function call.\n  if ((!is.list(coll) && !is.vector(coll)) || is.data.frame(coll)) {\n  # nolint end\n    if (is.data.frame(coll)) {\n      message(paste(\"context.R: A data frame is parallelized by columns.\"))\n    } else {\n      if (is.matrix(coll)) {\n        message(paste(\"context.R: A matrix is parallelized by elements.\"))\n      } else {\n        message(paste(\"context.R: parallelize() currently only supports lists and vectors.\",\n                      \"Calling as.list() to coerce coll into a list.\"))\n      }\n    }\n    coll <- as.list(coll)\n  }\n\n  sizeLimit <- getMaxAllocationLimit(sc)\n  objectSize <- object.size(coll)\n\n  # For large objects we make sure the size of each slice is also smaller than sizeLimit\n  numSerializedSlices <- max(numSlices, ceiling(objectSize / sizeLimit))\n  if (numSerializedSlices > length(coll))\n    numSerializedSlices <- length(coll)\n\n  # Generate the slice ids to put each row\n  # For instance, for numSerializedSlices of 22, length of 50\n  #  [1]  0  0  2  2  4  4  6  6  6  9  9 11 11 13 13 15 15 15 18 18 20 20 22 22 22\n  # [26] 25 25 27 27 29 29 31 31 31 34 34 36 36 38 38 40 40 40 43 43 45 45 47 47 47\n  # Notice the slice group with 3 slices (ie. 6, 15, 22) are roughly evenly spaced.\n  # We are trying to reimplement the calculation in the positions method in ParallelCollectionRDD\n  splits <- if (numSerializedSlices > 0) {\n    unlist(lapply(0: (numSerializedSlices - 1), function(x) {\n      # nolint start\n      start <- trunc((x * length(coll)) / numSerializedSlices)\n      end <- trunc(((x + 1) * length(coll)) / numSerializedSlices)\n      # nolint end\n      rep(start, end - start)\n    }))\n  } else {\n    1\n  }\n\n  slices <- split(coll, splits)\n\n  # Serialize each slice: obtain a list of raws, or a list of lists (slices) of\n  # 2-tuples of raws\n  serializedSlices <- lapply(slices, serialize, connection = NULL)\n\n  # The PRC backend cannot handle arguments larger than 2GB (INT_MAX)\n  # If serialized data is safely less than that threshold we send it over the PRC channel.\n  # Otherwise, we write it to a file and send the file name\n  if (objectSize < sizeLimit) {\n    jrdd <- callJStatic(\"org.apache.spark.api.r.RRDD\", \"createRDDFromArray\", sc, serializedSlices)\n  } else {\n    fileName <- writeToTempFile(serializedSlices)\n    jrdd <- tryCatch(callJStatic(\n        \"org.apache.spark.api.r.RRDD\", \"createRDDFromFile\", sc, fileName, as.integer(numSlices)),\n      finally = {\n        file.remove(fileName)\n    })\n  }\n\n  RDD(jrdd, \"byte\")\n}\n\ngetMaxAllocationLimit <- function(sc) {\n  conf <- callJMethod(sc, \"getConf\")\n  as.numeric(\n    callJMethod(conf,\n      \"get\",\n      \"spark.r.maxAllocationLimit\",\n      toString(.Machine$integer.max / 10) # Default to a safe value: 200MB\n  ))\n}\n\nwriteToTempFile <- function(serializedSlices) {\n  fileName <- tempfile()\n  conn <- file(fileName, \"wb\")\n  for (slice in serializedSlices) {\n    writeBin(as.integer(length(slice)), conn, endian = \"big\")\n    writeBin(slice, conn, endian = \"big\")\n  }\n  close(conn)\n  fileName\n}\n\n#' Include this specified package on all workers\n#'\n#' This function can be used to include a package on all workers before the\n#' user's code is executed. This is useful in scenarios where other R package\n#' functions are used in a function passed to functions like \\code{lapply}.\n#' NOTE: The package is assumed to be installed on every node in the Spark\n#' cluster.\n#'\n#' @param sc SparkContext to use\n#' @param pkg Package name\n#' @noRd\n#' @examples\n#'\\dontrun{\n#'  library(Matrix)\n#'\n#'  sc <- sparkR.init()\n#'  # Include the matrix library we will be using\n#'  includePackage(sc, Matrix)\n#'\n#'  generateSparse <- function(x) {\n#'    sparseMatrix(i=c(1, 2, 3), j=c(1, 2, 3), x=c(1, 2, 3))\n#'  }\n#'\n#'  rdd <- lapplyPartition(parallelize(sc, 1:2, 2L), generateSparse)\n#'  collect(rdd)\n#'}\nincludePackage <- function(sc, pkg) {\n  pkg <- as.character(substitute(pkg))\n  if (exists(\".packages\", .sparkREnv)) {\n    packages <- .sparkREnv$.packages\n  } else {\n    packages <- list()\n  }\n  packages <- c(packages, pkg)\n  .sparkREnv$.packages <- packages\n}\n\n#' Broadcast a variable to all workers\n#'\n#' Broadcast a read-only variable to the cluster, returning a \\code{Broadcast}\n#' object for reading it in distributed functions.\n#'\n#' @param sc Spark Context to use\n#' @param object Object to be broadcast\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' rdd <- parallelize(sc, 1:2, 2L)\n#'\n#' # Large Matrix object that we want to broadcast\n#' randomMat <- matrix(nrow=100, ncol=10, data=rnorm(1000))\n#' randomMatBr <- broadcast(sc, randomMat)\n#'\n#' # Use the broadcast variable inside the function\n#' useBroadcast <- function(x) {\n#'   sum(value(randomMatBr) * x)\n#' }\n#' sumRDD <- lapply(rdd, useBroadcast)\n#'}\nbroadcast <- function(sc, object) {\n  objName <- as.character(substitute(object))\n  serializedObj <- serialize(object, connection = NULL)\n\n  jBroadcast <- callJMethod(sc, \"broadcast\", serializedObj)\n  id <- as.character(callJMethod(jBroadcast, \"id\"))\n\n  Broadcast(id, object, jBroadcast, objName)\n}\n\n#' Set the checkpoint directory\n#'\n#' Set the directory under which RDDs are going to be checkpointed. The\n#' directory must be a HDFS path if running on a cluster.\n#'\n#' @param sc Spark Context to use\n#' @param dirName Directory path\n#' @noRd\n#' @examples\n#'\\dontrun{\n#' sc <- sparkR.init()\n#' setCheckpointDir(sc, \"~/checkpoint\")\n#' rdd <- parallelize(sc, 1:2, 2L)\n#' checkpoint(rdd)\n#'}\nsetCheckpointDirSC <- function(sc, dirName) {\n  invisible(callJMethod(sc, \"setCheckpointDir\", suppressWarnings(normalizePath(dirName))))\n}\n\n#' Add a file or directory to be downloaded with this Spark job on every node.\n#'\n#' The path passed can be either a local file, a file in HDFS (or other Hadoop-supported\n#' filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,\n#' use spark.getSparkFiles(fileName) to find its download location.\n#'\n#' A directory can be given if the recursive option is set to true.\n#' Currently directories are only supported for Hadoop-supported filesystems.\n#' Refer Hadoop-supported filesystems at \\url{https://wiki.apache.org/hadoop/HCFS}.\n#'\n#' @rdname spark.addFile\n#' @param path The path of the file to be added\n#' @param recursive Whether to add files recursively from the path. Default is FALSE.\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.addFile(\"~/myfile\")\n#'}\n#' @note spark.addFile since 2.1.0\nspark.addFile <- function(path, recursive = FALSE) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"addFile\", suppressWarnings(normalizePath(path)), recursive))\n}\n\n#' Get the root directory that contains files added through spark.addFile.\n#'\n#' @rdname spark.getSparkFilesRootDirectory\n#' @return the root directory that contains files added through spark.addFile\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.getSparkFilesRootDirectory()\n#'}\n#' @note spark.getSparkFilesRootDirectory since 2.1.0\nspark.getSparkFilesRootDirectory <- function() {\n  if (Sys.getenv(\"SPARKR_IS_RUNNING_ON_WORKER\") == \"\") {\n    # Running on driver.\n    callJStatic(\"org.apache.spark.SparkFiles\", \"getRootDirectory\")\n  } else {\n    # Running on worker.\n    Sys.getenv(\"SPARKR_SPARKFILES_ROOT_DIR\")\n  }\n}\n\n#' Get the absolute path of a file added through spark.addFile.\n#'\n#' @rdname spark.getSparkFiles\n#' @param fileName The name of the file added through spark.addFile\n#' @return the absolute path of a file added through spark.addFile.\n#' @export\n#' @examples\n#'\\dontrun{\n#' spark.getSparkFiles(\"myfile\")\n#'}\n#' @note spark.getSparkFiles since 2.1.0\nspark.getSparkFiles <- function(fileName) {\n  if (Sys.getenv(\"SPARKR_IS_RUNNING_ON_WORKER\") == \"\") {\n    # Running on driver.\n    callJStatic(\"org.apache.spark.SparkFiles\", \"get\", as.character(fileName))\n  } else {\n    # Running on worker.\n    file.path(spark.getSparkFilesRootDirectory(), as.character(fileName))\n  }\n}\n\n#' Run a function over a list of elements, distributing the computations with Spark\n#'\n#' Run a function over a list of elements, distributing the computations with Spark. Applies a\n#' function in a manner that is similar to doParallel or lapply to elements of a list.\n#' The computations are distributed using Spark. It is conceptually the same as the following code:\n#'   lapply(list, func)\n#'\n#' Known limitations:\n#' \\itemize{\n#'    \\item variable scoping and capture: compared to R's rich support for variable resolutions,\n#'    the distributed nature of SparkR limits how variables are resolved at runtime. All the\n#'    variables that are available through lexical scoping are embedded in the closure of the\n#'    function and available as read-only variables within the function. The environment variables\n#'    should be stored into temporary variables outside the function, and not directly accessed\n#'    within the function.\n#'\n#'   \\item loading external packages: In order to use a package, you need to load it inside the\n#'   closure. For example, if you rely on the MASS module, here is how you would use it:\n#'   \\preformatted{\n#'     train <- function(hyperparam) {\n#'       library(MASS)\n#'       lm.ridge(\"y ~ x+z\", data, lambda=hyperparam)\n#'       model\n#'     }\n#'   }\n#' }\n#'\n#' @rdname spark.lapply\n#' @param list the list of elements\n#' @param func a function that takes one argument.\n#' @return a list of results (the exact type being determined by the function)\n#' @export\n#' @examples\n#'\\dontrun{\n#' sparkR.session()\n#' doubled <- spark.lapply(1:10, function(x){2 * x})\n#'}\n#' @note spark.lapply since 2.0.0\nspark.lapply <- function(list, func) {\n  sc <- getSparkContext()\n  rdd <- parallelize(sc, list, length(list))\n  results <- map(rdd, func)\n  local <- collectRDD(results)\n  local\n}\n\n#' Set new log level\n#'\n#' Set new log level: \"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\"\n#'\n#' @rdname setLogLevel\n#' @param level New log level\n#' @export\n#' @examples\n#'\\dontrun{\n#' setLogLevel(\"ERROR\")\n#'}\n#' @note setLogLevel since 2.0.0\nsetLogLevel <- function(level) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"setLogLevel\", level))\n}\n\n#' Set checkpoint directory\n#'\n#' Set the directory under which SparkDataFrame are going to be checkpointed. The directory must be\n#' a HDFS path if running on a cluster.\n#'\n#' @rdname setCheckpointDir\n#' @param directory Directory path to checkpoint to\n#' @seealso \\link{checkpoint}\n#' @export\n#' @examples\n#'\\dontrun{\n#' setCheckpointDir(\"/checkpoint\")\n#'}\n#' @note setCheckpointDir since 2.2.0\nsetCheckpointDir <- function(directory) {\n  sc <- getSparkContext()\n  invisible(callJMethod(sc, \"setCheckpointDir\", suppressWarnings(normalizePath(directory))))\n}\n" }
{ "repo_name": "jeroenooms/r-source", "ref": "refs/heads/trunk", "path": "src/library/stats/R/lowess.R", "content": "#  File src/library/stats/R/lowess.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2016 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\nlowess <- function(x, y = NULL, f = 2/3, iter = 3L,\n                   delta = 0.01 * diff(range(x)))\n{\n    xy <- xy.coords(x,y, setLab = FALSE)\n    o <- order(xy$x)\n    x <- as.double(xy$x[o])\n    list(x = x, y = .Call(C_lowess, x, as.double(xy$y[o]), f, iter, delta))\n}\n" }
{ "repo_name": "SensePlatform/R", "ref": "refs/heads/R-3.5.1", "path": "src/library/stats/R/lowess.R", "content": "#  File src/library/stats/R/lowess.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2016 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\nlowess <- function(x, y = NULL, f = 2/3, iter = 3L,\n                   delta = 0.01 * diff(range(x)))\n{\n    xy <- xy.coords(x,y, setLab = FALSE)\n    o <- order(xy$x)\n    x <- as.double(xy$x[o])\n    list(x = x, y = .Call(C_lowess, x, as.double(xy$y[o]), f, iter, delta))\n}\n" }
{ "repo_name": "aviralg/R-dyntrace", "ref": "refs/heads/master", "path": "src/library/stats/R/lowess.R", "content": "#  File src/library/stats/R/lowess.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2016 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\nlowess <- function(x, y = NULL, f = 2/3, iter = 3L,\n                   delta = 0.01 * diff(range(x)))\n{\n    xy <- xy.coords(x,y, setLab = FALSE)\n    o <- order(xy$x)\n    x <- as.double(xy$x[o])\n    list(x = x, y = .Call(C_lowess, x, as.double(xy$y[o]), f, iter, delta))\n}\n" }
{ "repo_name": "reactorlabs/gnur", "ref": "refs/heads/R-3-6-2-branch-rir-patch", "path": "src/library/stats/R/lowess.R", "content": "#  File src/library/stats/R/lowess.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2016 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\nlowess <- function(x, y = NULL, f = 2/3, iter = 3L,\n                   delta = 0.01 * diff(range(x)))\n{\n    xy <- xy.coords(x,y, setLab = FALSE)\n    o <- order(xy$x)\n    x <- as.double(xy$x[o])\n    list(x = x, y = .Call(C_lowess, x, as.double(xy$y[o]), f, iter, delta))\n}\n" }
{ "repo_name": "minux/R", "ref": "refs/heads/trunk", "path": "src/library/stats/R/lowess.R", "content": "#  File src/library/stats/R/lowess.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2016 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\nlowess <- function(x, y = NULL, f = 2/3, iter = 3L,\n                   delta = 0.01 * diff(range(x)))\n{\n    xy <- xy.coords(x,y, setLab = FALSE)\n    o <- order(xy$x)\n    x <- as.double(xy$x[o])\n    list(x = x, y = .Call(C_lowess, x, as.double(xy$y[o]), f, iter, delta))\n}\n" }
{ "repo_name": "allr/timeR", "ref": "refs/heads/master", "path": "src/library/stats/R/lowess.R", "content": "#  File src/library/stats/R/lowess.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2016 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\nlowess <- function(x, y = NULL, f = 2/3, iter = 3L,\n                   delta = 0.01 * diff(range(x)))\n{\n    xy <- xy.coords(x,y, setLab = FALSE)\n    o <- order(xy$x)\n    x <- as.double(xy$x[o])\n    list(x = x, y = .Call(C_lowess, x, as.double(xy$y[o]), f, iter, delta))\n}\n" }
{ "repo_name": "krlmlr/r-source", "ref": "refs/heads/master", "path": "src/library/stats/R/lowess.R", "content": "#  File src/library/stats/R/lowess.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2016 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\nlowess <- function(x, y = NULL, f = 2/3, iter = 3L,\n                   delta = 0.01 * diff(range(x)))\n{\n    xy <- xy.coords(x,y, setLab = FALSE)\n    o <- order(xy$x)\n    x <- as.double(xy$x[o])\n    list(x = x, y = .Call(C_lowess, x, as.double(xy$y[o]), f, iter, delta))\n}\n" }
{ "repo_name": "wch/r-source", "ref": "refs/heads/trunk", "path": "src/library/stats/R/lowess.R", "content": "#  File src/library/stats/R/lowess.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2016 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\nlowess <- function(x, y = NULL, f = 2/3, iter = 3L,\n                   delta = 0.01 * diff(range(x)))\n{\n    xy <- xy.coords(x,y, setLab = FALSE)\n    o <- order(xy$x)\n    x <- as.double(xy$x[o])\n    list(x = x, y = .Call(C_lowess, x, as.double(xy$y[o]), f, iter, delta))\n}\n" }
{ "repo_name": "Mouseomics/R", "ref": "refs/heads/master", "path": "src/library/stats/R/lowess.R", "content": "#  File src/library/stats/R/lowess.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2016 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\nlowess <- function(x, y = NULL, f = 2/3, iter = 3L,\n                   delta = 0.01 * diff(range(x)))\n{\n    xy <- xy.coords(x,y, setLab = FALSE)\n    o <- order(xy$x)\n    x <- as.double(xy$x[o])\n    list(x = x, y = .Call(C_lowess, x, as.double(xy$y[o]), f, iter, delta))\n}\n" }
{ "repo_name": "allr/r-instrumented", "ref": "refs/heads/master", "path": "src/library/stats/R/lowess.R", "content": "#  File src/library/stats/R/lowess.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2016 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\nlowess <- function(x, y = NULL, f = 2/3, iter = 3L,\n                   delta = 0.01 * diff(range(x)))\n{\n    xy <- xy.coords(x,y, setLab = FALSE)\n    o <- order(xy$x)\n    x <- as.double(xy$x[o])\n    list(x = x, y = .Call(C_lowess, x, as.double(xy$y[o]), f, iter, delta))\n}\n" }
{ "repo_name": "MouseGenomics/R", "ref": "refs/heads/master", "path": "src/library/stats/R/lowess.R", "content": "#  File src/library/stats/R/lowess.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2016 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\nlowess <- function(x, y = NULL, f = 2/3, iter = 3L,\n                   delta = 0.01 * diff(range(x)))\n{\n    xy <- xy.coords(x,y, setLab = FALSE)\n    o <- order(xy$x)\n    x <- as.double(xy$x[o])\n    list(x = x, y = .Call(C_lowess, x, as.double(xy$y[o]), f, iter, delta))\n}\n" }
{ "repo_name": "bedatadriven/renjin", "ref": "refs/heads/master", "path": "packages/stats/R/lowess.R", "content": "#  File src/library/stats/R/lowess.R\n#  Part of the R package, https://www.R-project.org\n#\n#  Copyright (C) 1995-2016 The R Core Team\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  A copy of the GNU General Public License is available at\n#  https://www.R-project.org/Licenses/\n\nlowess <- function(x, y = NULL, f = 2/3, iter = 3L,\n                   delta = 0.01 * diff(range(x)))\n{\n    xy <- xy.coords(x,y, setLab = FALSE)\n    o <- order(xy$x)\n    x <- as.double(xy$x[o])\n    list(x = x, y = .Call(C_lowess, x, as.double(xy$y[o]), f, iter, delta))\n}\n" }
{ "repo_name": "ar0ch/nthi_project", "ref": "refs/heads/master", "path": "t6anotation_webserver/server.R", "content": "library('shiny')\nlibrary('genoPlotR')\n\nshinyServer(function(input, output, session) {\n\nobserve({\n    # switch tab\n    if ( (!is.null(input$genome) | ( !is.null(input$genome) | !is.null(input$gffFile))) & input$submit != 0L ) {\n      updateTabsetPanel(session, \"vchot6ss\", selected = \"T6SS Predictions\")\n    }\n  })\n\n  # return computed descriptors\n  annotPlot = reactive({\n    \n\n    predCond = input$predict\n\n    if ( predCond == 'pred' & input$submit != 0L & !is.null(input$genome) ) {\n      outdir = substr(input$genome$datapath,1,nchar(input$genome$datapath)-1)\n      annotate <- paste('/home/blast/prediction_server/server/predict_t6.pl -predict yes -fasta ',input$genome$datapath)\n      #system('echo \"',input$genome$datapath,'\\n$(date) >> ~/vibrio_project/webtest.txt\"', intern = TRUE)\n      system(annotate)\n      genplotdata <- paste('/home/blast/prediction_server/server/gff.pl -fasta ',input$genome$datapath)\n      system(genplotdata)\n      filelist = dir(outdir, pattern = \"*.ptt\")\n      dna <- list()\n      annot <- list()\n      for (i in 1:length(filelist)){\n        file = paste(outdir,filelist[i], sep=\"\")\n        dna[[i]] <- read_dna_seg_from_ptt(file)\n        mid_pos <- middle(dna[[i]])\n        annot[[i]] <- annotation(x1=mid_pos, text=dna[[i]]$name, rot = \"45\")\n      }\n      \n      plot_gene_map(dna_segs=dna, annotations=annot)\n    }\n    if ( predCond == 'nopred' & input$submit != 0L & !is.null(input$genome) & !is.null(input$gffFile)) {\n      outdir = substr(input$genome$datapath,1,nchar(input$genome$datapath)-1)\n      annotate <- paste('/home/blast/prediction_server/server/predict_t6.pl -predict no -fasta ',input$genome$datapath,' -gff ',input$gffFile$datapath)\n      #system('echo \"',input$genome$datapath,'\\n$(date) >> ~/vibrio_project/webtest.txt\"', intern = TRUE)\n      system(annotate)\n      genplotdata <- paste('/home/blast/prediction_server/server/gff.pl -fasta ',input$genome$datapath,' -gff ',input$gffFile$datapath)\n      system(genplotdata)\n      filelist = dir(outdir, pattern = \"*.ptt\")\n      dna <- list()\n      annot <- list()\n      for (i in 1:length(filelist)){\n        file = paste(outdir,filelist[i], sep=\"\")\n        dna[[i]] <- read_dna_seg_from_ptt(file)\n        mid_pos <- middle(dna[[i]])\n        annot[[i]] <- annotation(x1=mid_pos, text=dna[[i]]$name, rot = \"45\")\n      }\n      \n      plot_gene_map(dna_segs=dna, annotations=annot)\n    }\n    else {\n      return(NULL)\n    }\n  })\n\n  output$plot = renderPlot({\n    annotPlot() })\n  output$dlFasta = downloadHandler(\n    filename = function() { paste(\"proteins\", paste(collapse = '-'),\n                                  '-', gsub(' ', '-', gsub(':', '-', Sys.time())),\n                                  '.faa', sep = '') },\n    content = function(file) {\n      outdir = substr(input$genome$datapath,1,nchar(input$genome$datapath)-1)\n      copyFile = paste(outdir,\"prots.faa\", sep=\"\")\n      file.copy(copyFile, file)}\n  )\noutput$dlPreds = downloadHandler(\n    filename = function() { paste(\"predictions\", paste(collapse = '-'),\n                                  '-', gsub(' ', '-', gsub(':', '-', Sys.time())),\n                                  '.faa', sep = '') },\n    content = function(file) {\n      outdir = substr(input$genome$datapath,1,nchar(input$genome$datapath)-1)\n      copyFile = paste(outdir,\"predictions.faa\", sep=\"\")\n      file.copy(copyFile, file)}\n  )\n\n})\n" }
{ "repo_name": "gthar/nucleServ", "ref": "refs/heads/VRE", "path": "bin/readBAM.R", "content": "#!/usr/bin/Rscript\n\nlibrary(getopt)\n\nwhere <- function () {\n    spath <-parent.frame(2)$ofile\n\n    if (is.null(spath)) {\n        args <- commandArgs()\n        filearg <- args[grep(\"^--file=\", args)]\n        fname <- strsplit(filearg, \"=\")[[1]][2]\n    } else {\n        fname <- spath\n    }\n\n    dirname(normalizePath(fname))\n}\n\nSOURCE.DIR <- paste(where(), \"../sourced\", sep=\"/\")\nsource(paste(SOURCE.DIR,\n             \"loadbams.R\",\n             sep=\"/\"))\n\nspec <- matrix(c(\"type\",   \"t\", 1, \"character\",\n                 \"input\",  \"i\", 1, \"character\",\n                 \"output\", \"o\", 1, \"character\"),\n               byrow=TRUE,\n               ncol=4)\nargs <- getopt(spec)\n\nmessage(\"-- loading \", args[[\"input\"]])\nreads <- loadBAM(args[[\"input\"]], args[[\"type\"]])\n\nmessage(\"-- saving \", args[[\"output\"]])\nsave(reads, file=args[[\"output\"]])\n" }