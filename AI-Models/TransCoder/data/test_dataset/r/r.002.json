{ "repo_name": "hadley/readxl", "ref": "refs/heads/master", "path": "R/excel-sheets.R", "content": "#' List all sheets in an excel spreadsheet.\n#'\n#' @inheritParams read_excel\n#' @export\n#' @examples\n#' excel_sheets(system.file(\"extdata/datasets.xlsx\", package = \"readxl\"))\n#' excel_sheets(system.file(\"extdata/datasets.xls\", package = \"readxl\"))\n#'\n#' # To load all sheets in a workbook, use lapply\n#' path <- system.file(\"extdata/datasets.xls\", package = \"readxl\")\n#' lapply(excel_sheets(path), read_excel, path = path)\nexcel_sheets <- function(path) {\n  path <- check_file(path)\n  ext <- tolower(tools::file_ext(path))\n\n  switch(excel_format(path),\n    xls =  xls_sheets(path),\n    xlsx = xlsx_sheets(path)\n  )\n}\n" }
{ "repo_name": "eddelbuettel/rcppredis", "ref": "refs/heads/master", "path": "inst/tests/runit.serverIssues.R", "content": "test_01_setup <- function() {\n    suppressMessages(library(RcppRedis))\n    # we start the Redis server for this test as a slave so we don't clobber the main running version of redis for the rest of the tests\n    writeLines(\"requirepass badPassword\",\"/tmp/redis.conf\")\n    system(\"redis-server /tmp/redis.conf --port 7777 --slaveof localhost 6379\", wait=FALSE)\n    # Wait for server to come up\n    Sys.sleep(5)\n}\n\ntest_02_testUnauth <- function () {\n  redis <<- new(RcppRedis::Redis, \"localhost\", 7777, auth = \"\", 10)\n  # we expect an exception because we haven't send the password\n  checkException(redis$ping())\n}\n\ntest_03_testAuth <- function () {\n  redis <<- new(RcppRedis::Redis, \"localhost\", 7777, auth = \"badPassword\", 10)\n  checkEquals(redis$ping(), \"PONG\")\n}\n\ntest_04_killServer <- function() {\n    # confirm server is up\n    checkEquals(redis$ping(),\"PONG\")\n    # kill server\n    checkException(redis$exec(\"SHUTDOWN\"))\n}\n\ntest_05_cleanup <- function() {\n    NULL\n}\n" }
{ "repo_name": "eddelbuettel/rcppredis", "ref": "refs/heads/master", "path": "demo/spDemo.R", "content": "\n\nlibrary(rredis)\nsuppressMessages(library(RcppRedis))\nsuppressMessages(library(xts))\nlibrary(rbenchmark)\n\nredisConnect()                          # default localhost\nredis <- new(Redis)\n\nif (!redisExists(\"sp500\")) {\n    suppressMessages(library(quantmod))\n    options(\"getSymbols.warning4.0\"=FALSE)   ## suppress some line noise\n    sp <- getSymbols(\"^GSPC\", auto.assign=FALSE, from=\"1950-01-01\", to=Sys.Date())\n    redisSet(\"sp500\", sp)\n    cat(\"Downloaded SP500 and stored in redis\\n\")\n} else { \n    cat(\"Retrieving SP500 from redis\\n\")\n    sp <- redisGet(\"sp500\")\n}\n#print(summary(sp))\n\nrInsert <- function(x) {\n    n <- nrow(x)\n    key <- \"sp500_R\"\n    if (redisExists(key)) redisDelete(key)\n    M <- cbind(as.numeric(index(x)), coredata(x))\n    for (i in 1:n) {\n        redisRPush(key, M[i,,drop=TRUE])\n    }\n    invisible(NULL)\n}\n\n## This is atrociously slow:\n##\n## R> system.time(rInsert(sp))\n##    user  system elapsed \n##  16.392   0.292 645.643 \n## R> \n\nsystem.time(m1 <- do.call(rbind, redisLRange(\"sp500_R\", 0, -1)))\nsystem.time(m2 <- do.call(rbind, redis$lrange(\"sp500_R\", 0, -1)))\nidentical(m1,m2)\nsystem.time(m3 <- redis$listToMatrix(redis$lrange(\"sp500_R\", 0, -1)))\nm4 <- m1\ndimnames(m4) <- list()\nidentical(m4,m3)\n\n## approx factor 20\nres <- benchmark(do.call(rbind, redisLRange(\"sp500_R\", 0, -1)),\n                 do.call(rbind, redis$lrange(\"sp500_R\", 0, -1)),\n                 redis$listToMatrix(redis$lrange(\"sp500_R\", 0, -1)),\n                 order=\"relative\", replications=25)[,1:4]\nprint(res)\n\n\ncInsert <- function(x) {\n    n <- nrow(x)\n    key <- \"sp500_C\"\n    if (redisExists(key)) redisDelete(key)\n    M <- cbind(as.numeric(index(x)), coredata(x))\n    for (i in 1:n) {\n        redis$listRPush(key, M[i,])\n    }\n    invisible(NULL)\n}\n\n\n## approx factor 20\nres <- benchmark(do.call(rbind, redisLRange(\"sp500_R\", 0, -1)),\n                 do.call(rbind, redis$lrange(\"sp500_R\", 0, -1)),\n                 redis$listToMatrix(redis$lrange(\"sp500_R\", 0, -1)),\n                 redis$listToMatrix(redis$listRange(\"sp500_C\", 0, -1)),\n                 order=\"relative\", replications=25)[,1:4]\nprint(res)\n\n\n## R> print(res)\n##                                                    test replications elapsed relative\n## 4 redis$listToMatrix(redis$listRange(\"sp500_C\", 0, -1))           25   0.296    1.000\n## 3    redis$listToMatrix(redis$lrange(\"sp500_R\", 0, -1))           25   2.300    7.770\n## 2        do.call(rbind, redis$lrange(\"sp500_R\", 0, -1))           25   2.629    8.882\n## 1         do.call(rbind, redisLRange(\"sp500_R\", 0, -1))           25  48.028  162.257\n## R> \n\n## redo after Bryan's socket/nagle update to rredis:\n##                                                    test replications elapsed relative\n## 4 redis$listToMatrix(redis$listRange(\"sp500_C\", 0, -1))           25   0.407    1.000\n## 3    redis$listToMatrix(redis$lrange(\"sp500_R\", 0, -1))           25   2.112    5.189\n## 2        do.call(rbind, redis$lrange(\"sp500_R\", 0, -1))           25   2.458    6.039\n## 1         do.call(rbind, redisLRange(\"sp500_R\", 0, -1))           25  48.367  118.838\n## edd@max:~/git/rhiredis/demo$ \n" }
{ "repo_name": "NCBI-Hackathons/SRA2R", "ref": "refs/heads/master", "path": "tests/runTests.R", "content": "BiocGenerics:::testPackage(\"SRA2R\")\n" }
{ "repo_name": "NCBI-Hackathons/SRA2R", "ref": "refs/heads/master", "path": "tests/testthat/test_getReads.R", "content": "library(SRA2R)\ncontext(\"getReads\")\n\ntest_that(\"getReads returns reads from a run\", {\n  expect_equal(getFastqCount('SRR000123'),4583)\n  expect_equal(getFastqCount('SRR1607152'),78377869)\n})\n" }
{ "repo_name": "thiloklein/matchingMarkets", "ref": "refs/heads/master", "path": "R/ttc.R", "content": "# ----------------------------------------------------------------------------\n# R-code (www.r-project.org/) for the Top-Trading-Cycles Algorithm\n#\n# Copyright (c) 2013 Thilo Klein\n#\n# This library is distributed under the terms of the GNU Public License (GPL)\n# for full details see the file LICENSE\n#\n# ----------------------------------------------------------------------------\n\n#' @title Top-Trading-Cycles Algorithm for the house allocation problem\n#'\n#' @description Finds the stable matching in the \\href{http://en.wikipedia.org/wiki/Herbert_Scarf#8._The_Housing_Market}{house allocation problem} with existing tenants.\n#' Uses the Top-Trading-Cycles Algorithm proposed in Abdulkadiroglu and Sonmez (1999).\n#'\n#' @param P list of individuals' preference rankings over objects.\n#' @param X 2-column-matrix of objects (\\code{obj}) and their owners (\\code{ind}).\n#' @return \\code{ttc} returns a 2-column matrix of the stable matching solution for the housing market problem based on the Top-Trading-Cycles algorithm.\n#' @author Thilo Klein \n#' @keywords algorithms\n#' @references Abdulkadiroglu, A. and Sonmez, T. (1999). House Allocation with Existing Tenants. \\emph{Journal of Economic Theory}, 88(2):233--260.\n#' @examples\n#' ## generate list of individuals' preference rankings over objects\n#' P <- list()\n#' P[[1]] <- c(2,5,1,4,3)    # individual 1\n#' P[[2]] <- c(1,5,4,3,2)    # individual 2\n#' P[[3]] <- c(2,1,4,3,5)    # individual 3\n#' P[[4]] <- c(2,4,3,1,5)    # individual 4\n#' P[[5]] <- c(4,3,1,2,5); P # individual 5\n#' \n#' ## generate 2-column-matrix of objects ('obj') and their owners ('ind')\n#' X <- data.frame(ind=1:5, obj=1:5); X\n#' \n#' ## find assignment based on TTC algorithm\n#' ttc(P=P,X=X)\n#' @export\nttc <- function(P=NULL,X=NULL){\n  \n  ## 2-column-matrix of home objects ('obj') and their owners ('ind')\n  Y <- data.frame(ind=NULL, obj=NULL)\n  \n  for(z in 1:length(unique(X$obj))){\n\n    ## 1. Find cycle\n    Cycle <- findCycle(P=P,X=X)\n    \n    ## 2. Add objects in this cycle to 'home territory'\n    Y <- rbind(Y,Cycle)\n\n    ## 3. Remove objects in this cycle from tradable objects\n    X <- X[-which(X$obj %in% Cycle$obj),]\n    for(i in 1:length(P)){\n      P[[i]] <- P[[i]][!P[[i]] %in% Y$ind]\n    }\n\n    ## 4. Process ends if no tradable objects remain\n    if(nrow(X)==0){\n      Y <- rbind(Y,X)\n      return(Y)\n      break\n    }\n  }\n}\nfindCycle <- function(P=NULL,X=NULL){\n  Cycle   <- data.frame(ind=NA, obj=NA)\n  thisind <- X$ind[1] # start with first individual in line\n  for(j in 1:length(unique(X$ind))){\n    Cycle[j,] <- c(thisind,P[[thisind]][1]) # id and top-ranked object of the individual in line\n    thisind   <- X[X$obj == P[[thisind]][1],\"ind\"] # individual whose object is requested\n    if(Cycle[j,1] == Cycle[j,2]){ # if individual points to own object\n      return(Cycle[j,])\n      break\n    }\n    if(thisind %in% Cycle$ind){ # if this individual completes a cycle\n      return(Cycle)\n      break\n    } \n  }\n}" }
{ "repo_name": "thiloklein/matchingMarkets", "ref": "refs/heads/master", "path": "R/mfx.R", "content": "# ----------------------------------------------------------------------------\n# R-code (www.r-project.org/) to obtain marginal effects for probit and matching models\n#\n# Copyright (c) 2013 Thilo Klein\n#\n# This library is distributed under the terms of the GNU Public License (GPL)\n# for full details see the file LICENSE\n#\n# ----------------------------------------------------------------------------\n\n#' @title Marginal effects for probit and matching models\n#'\n#' @description Marginal effects from regression coefficients for probit \n#' and matching models. \n#'\n#' @param m an object returned from functions \\code{stabit} or \\code{stabit2}.\n#' @param toLatex logical: if \\code{TRUE} the result tables are printed in Latex format. The default setting is \\code{FALSE}.\n#' \n#' @export\n#' \n#' @import stats\n#' \n#' @author Thilo Klein \n#' \n#' @keywords summary\n#' \n#' @references Klein, T. (2015a). \\href{https://ideas.repec.org/p/cam/camdae/1521.html}{Does Anti-Diversification Pay? A One-Sided Matching Model of Microcredit}.\n#' \\emph{Cambridge Working Papers in Economics}, #1521.\n#' \n#' @examples\n#' ## 1. load results from Klein (2015a)\n#'  data(klein15a)\n#' \n#' ## 2. apply mfx function and print results\n#'  mfx(m=klein15a)\nmfx <- function(m,toLatex=FALSE){\n  \n  if(!is.null(m$coefs$alpha)){ ## Selectiom and Outcome Eqns\n\n    ## model matrix\n    X <- do.call(rbind.data.frame, m$model.list$X)\n    eta <- c(m$coefs$eta, rep(0, length(m$model.list$X)-length(m$model.list$W)))\n    X <- as.matrix(data.frame(X=X,eta=eta))\n    \n    ## valuation equation\n    nrowX <- dim(do.call(rbind.data.frame, m$model.list$W))[1]\n    sel <- mfxVal(postmean=m$coefs$alpha[,1], poststd=m$coefs$alpha[,2],\n           nrowX=nrowX, toLatex=toLatex)\n    \n    ## structral model outcome\n    out <- mfxOut(sims=10000, postmean=unlist(c(m$coefs$beta[,1], data.frame(delta=m$coefs$delta[1]))),\n           poststd=c(m$coefs$beta[,2], m$coefs$delta[2]), X=X, toLatex=toLatex)\n\n    return(list(mfx.selection=sel, mfx.outcome=out))\n\n  } else{ ## Outcome Eqn only\n    \n    ## model matrix\n    X <- do.call(rbind.data.frame, m$model.list$X)\n    X <- as.matrix(X)\n    \n    ## model outcome    \n    out <- mfxOut(sims=10000, postmean=m$coefs$beta[,1],\n           poststd=m$coefs$beta[,2], X=X, toLatex=toLatex)\n    \n    return(list(mfx.outcome=out))\n  }\n}\n\n\nmfxOut <- function(sims=10000,x.mean=TRUE,postmean,poststd,X,toLatex){\n  ## source: http://researchrepository.ucd.ie/handle/10197/3404\n  ## method: average of individual marginal effects at each observation\n  ## interpretation: http://www.indiana.edu/~statmath/stat/all/cdvm/cdvm.pdf page 8\n  set.seed(1984)\n  if(x.mean==TRUE){\n    ## marginal effects are calculated at the means of independent variables\n    pdf <- dnorm(mean(X%*%postmean))\n    pdfsd <- dnorm(sd(X%*%postmean))\n  } else{\n    ## marginal effects are calculated for each observation and then averaged\n    pdf <- mean(dnorm(X%*%postmean))\n    pdfsd <- sd(dnorm(X%*%postmean))\n  }  \n  mx <- pdf*postmean\n\n  sim <- matrix(rep(NA,sims*length(postmean)), nrow=sims)\n  for(i in 1:length(postmean)){\n    sim[,i] <- rnorm(sims,postmean[i],poststd[i])\n  }\n  pdfsim <- rnorm(sims,pdf,pdfsd)\n  sim.se <- pdfsim*sim\n  s.e. <- apply(sim.se,2,sd)\n\n  t.stat <- mx/s.e.\n  p.val <- pt(-abs(t.stat),df=dim(X)[1]-length(postmean)+1)\n  stars <- ifelse(p.val<0.001,\"***\",ifelse(p.val<0.01,\"**\",ifelse(p.val<0.05,\"*\",ifelse(p.val<0.10,\".\",\"\"))))\n  if(toLatex==FALSE){\n    res <- data.frame(round(cbind(mx, s.e., t.stat, p.val),3), stars)\n  } else{\n      sign <- ifelse(mx>0,\"~\",\"\")\n      res <- data.frame(\"&\", sign, round(mx,3), se=paste(paste(\"(\",round(s.e.,3),sep=\"\"),\")\",sep=\"\"), stars, \"\\\\\")\n  }\n  return(res)\n}\n\n\nmfxVal <- function(postmean,poststd,nrowX,toLatex){\n\n  ## Reference: Sorensen (2007, p. 2748)\n\n  mx <- dnorm(0)*postmean/sqrt(2)\n  s.e. <- dnorm(0)*poststd/sqrt(2)\n  t.stat <- mx/s.e.\n  p.val <- pt(-abs(t.stat),df=nrowX-length(postmean))\n  stars <- ifelse(p.val<0.001,\"***\",ifelse(p.val<0.01,\"**\",ifelse(p.val<0.05,\"*\",ifelse(p.val<0.10,\".\",\"\"))))\n  if(toLatex==FALSE){\n    res <- data.frame( round(cbind(mx, s.e., t.stat, p.val),3), stars)\n  } else{\n    sign <- ifelse(mx>0,\"~\",\"\")\n    res <- data.frame( \"&\", sign, round(mx,3), se=paste(paste(\"(\",round(s.e.,3),sep=\"\"),\")\",sep=\"\"), stars, \"\\\\\")\n  }\n  return(res)\n}\n" } 
{ "repo_name": "jbkunst/r-posts", "ref": "refs/heads/master", "path": "015-www-working-with-weather/translation_data.R", "content": "# Data from 'https://centroccbb.cl/clima/indexData.php'\nrm(list = ls())\n\nlibrary(\"rio\")\nlibrary(\"dplyr\")\nlibrary(\"rvest\")\n\n\ndata <- import(\"Informe parcial.xls\")\n\ndata <- data %>% tbl_df()\n\nnames(data)\n\n\n# names(data) <- c(\"date\", \"time\", \"temperature\", \"humidity\", \"pressure\", \"precipitation\",\n#                  \"solar_radiation\", \"wind_speed\", \"wind_direction\",\n#                  \"chill\", \"Temperature_flushing\", \"htw_index\", \"thws_index\", \"dew_point\",\n#                  \"rainfall_rate\", \"air_density\")\n\nnames(data) <- c(\"date\", \"time\", \"temperature\", \"humidity\", \"pressure\", \"precipitation\",\n                 \"solar_radiation\", \"wind_speed\", \"wind_direction\",\n                 \"thermal_sensation\", \"rainfall_rate\")\n\nnames(data)\n\ndata\n\nstr(data)\n\nsave(data, file = \"wheather_data.RData\")\n" }
{ "repo_name": "jbkunst/r-posts", "ref": "refs/heads/master", "path": "052-motion-piramid/readme.R", "content": "#' ---\r\n#' title: \"Motion Piramid\"\r\n#' author: \"Joshua Kunst\"\r\n#' output:\r\n#'  html_document:\r\n#'    toc: true\r\n#'    keep_md: yes\r\n#' ---\r\n\r\n#+ echo=FALSE, message=FALSE, warning=FALSE\r\nrm(list = ls())\r\nknitr::opts_chunk$set(message = FALSE, warning = FALSE,\r\n                      fig.showtext = TRUE, dev = \"CairoPNG\")\r\n\r\n#'\r\nlibrary(idbr)\r\nlibrary(dplyr)\r\nlibrary(purrr)\r\nlibrary(highcharter)\r\nrm(list = ls())\r\n\r\nidb_api_key(\"35f116582d5a89d11a47c7ffbfc2ba309133f09d\")\r\n\r\nyrs <-  seq(1980, 2030, by = 1)\r\n\r\ndf <- map_df(c(\"male\", \"female\"), function(sex){\r\n  idb1(\"US\", yrs, sex = sex) %>%\r\n    mutate(sex_label = sex)\r\n})\r\n\r\nnames(df) <- tolower(names(df))\r\n\r\nstr(df)\r\nhead(df) \r\n\r\ndf <- df %>%\r\n  mutate(population = pop*ifelse(sex_label == \"male\", -1, 1))\r\n\r\nseries <- df %>% \r\n  group_by(sex_label, age) %>% \r\n  do(data = list(sequence = .$population)) %>% \r\n  ungroup() %>% \r\n  group_by(sex_label) %>% \r\n  do(data = .$data) %>%\r\n  mutate(name = sex_label) %>% \r\n  list.parse3()\r\n\r\nmaxpop <- max(abs(df$population))\r\n\r\nxaxis <- list(categories = sort(unique(df$age)),\r\n              reversed = FALSE, tickInterval = 5,\r\n              labels = list(step = 5))\r\n\r\nhighchart() %>%\r\n  hc_chart(type = \"bar\") %>%\r\n  hc_motion(enabled = TRUE, labels = yrs, series = c(0,1), autoplay = TRUE, updateInterval = 1) %>% \r\n  hc_add_series_list(series) %>% \r\n  hc_plotOptions(\r\n    series = list(stacking = \"normal\"),\r\n    bar = list(groupPadding = 0, pointPadding =  0, borderWidth = 0)\r\n  ) %>% \r\n  hc_tooltip(shared = TRUE) %>% \r\n  hc_yAxis(\r\n    labels = list(\r\n      formatter = JS(\"function(){ return Math.abs(this.value) / 1000000 + 'M'; }\") \r\n    ),\r\n    tickInterval = 0.5e6,\r\n    min = -maxpop,\r\n    max = maxpop) %>% \r\n  hc_xAxis(\r\n    xaxis,\r\n    rlist::list.merge(xaxis, list(opposite = TRUE, linkedTo = 0))\r\n  ) %>% \r\n  hc_tooltip(shared = FALSE,\r\n             formatter = JS(\"function () { return '<b>' + this.series.name + ', age ' + this.point.category + '</b><br/>' + 'Population: ' + Highcharts.numberFormat(Math.abs(this.point.y), 0);}\")\r\n  ) %>% \r\n  hc_add_theme(hc_theme_smpl())\r\n\r\n  \r\n" }
{ "repo_name": "jbkunst/r-posts", "ref": "refs/heads/master", "path": "001-GTFS-Transantiago-ggplot2/script/script_02.R", "content": "# Source\n# https://developers.google.com/transit/gtfs/reference\nrm(list=ls())\noptions(stringsAsFactors=FALSE)\n\nlibrary(devtools)\nlibrary(ggplot2)\nlibrary(plyr)\nlibrary(dplyr)\n\nsource_url(\"https://raw.githubusercontent.com/jbkunst/reuse/master/R/gg_themes.R\")\n\nfile_temp <- tempfile(fileext = \".zip\")\n\n# http://www.mbta.com/rider_tools/developers/default.asp?id=21895\ndownload.file(\"http://www.mbta.com/uploadedfiles/MBTA_GTFS.zip\", file_temp)\n\ndata <- read.table()\n\nshapes <- read.csv(unz(file_temp, \"shapes.txt\"))\nroutes <- read.csv(unz(file_temp, \"routes.txt\"))\ntrips <- read.csv(unz(file_temp, \"trips.txt\"))\nstops <- read.csv(unz(file_temp, \"stops.txt\"))\n\nhead(routes)\ntail(routes)\nhead(shapes)\n\nunlink(file_temp)\n\n\np <- ggplot(shapes) +\n  geom_path(aes(shape_pt_lon, shape_pt_lat, group=shape_id), size=.2, alpha=.1) +\n  xlim(-71.2, -71) + ylim(42.0,42.6) +\n  coord_equal()\n  \np\n\n\nstops_metro <- stops %>% filter(!grepl(\"\\\\d\", stop_id))\nshapes_colors <- left_join(left_join(shapes %>% select(shape_id) %>% unique(),\n                                     trips %>% select(shape_id, route_id) %>% unique()),\n                           routes %>% select(route_id, route_color) %>% unique())\nshapes_colors <- shapes_colors  %>% mutate(route_color = paste0(\"#\", route_color))\nroutes_metro <- routes %>% filter(grepl(\"^L\\\\d\",route_id))\nshapes_metro <- shapes %>% filter(shape_id %in% trips$shape_id[trips$route_id %in% routes_metro$route_id])\nshapes_colors_metro <- shapes_colors %>% filter(shape_id %in% trips$shape_id[trips$route_id %in% routes_metro$route_id])\n\np2 <- ggplot() +\n  geom_path(data=shapes, aes(shape_pt_lon, shape_pt_lat, group=shape_id), color=\"white\", size=.2, alpha=.1) +\n  geom_path(data=shapes_metro, aes(shape_pt_lon, shape_pt_lat, group=shape_id, colour=shape_id), size = 2, alpha=.7) +\n  scale_color_manual(values=shapes_colors_metro$route_color) +\n  geom_point(data=stops_metro, aes(stop_lon, stop_lat), shape=21, colour=\"white\", alpha =.8) +\n  coord_equal() +\n  theme_null() +\n  theme(plot.background = element_rect(fill = \"black\", colour = \"black\"),\n        title = element_text(hjust=1, colour=\"white\")) +\n  ggtitle(\"TRANSANTIAGO\\nSantiago's public transport system\")\np2\n" }
{ "repo_name": "jbkunst/r-posts", "ref": "refs/heads/master", "path": "025-stackoverflow/get-github-language-colors.R", "content": "library(\"httr\")\r\ndfcols <- \"https://raw.githubusercontent.com/doda/github-language-colors/master/colors.json\" %>% \r\n  GET() %>%\r\n  content() %>% \r\n  jsonlite::fromJSON() %>% \r\n  { data_frame(language = tolower(names(.)),\r\n               color = unlist(.)) }\r\n\r\ndfcols\r\n\r\n" }
{ "repo_name": "jbkunst/r-posts", "ref": "refs/heads/master", "path": "018-riv-woe-package/readme.R", "content": "#' ---\n#' title: \"RIV WOE Package\"\n#' author: \"Joshua Kunst\"\n#' output: \n#'  html_document: \n#'    keep_md: yes\n#' ---\n\n#+ fig.width=10, fig.height=5\n#+ echo=FALSE\n# library(\"printr\")\nlibrary(\"knitr\")\noptions(digits = 3, knitr.table.format = \"markdown\")\nknitr::opts_chunk$set(collapse = TRUE, comment = \">\", warning = FALSE,\n                      fig.width = 10, fig.height = 6,\n                      fig.align = \"center\", dpi = 72)\n\n#' # Introducction\n#' \n#' - woe is data frame oriented, the functions have always a data frame argument.\n#' - riskr is variable oriented, the functions have always a variable (non dataframe) argument.\n\n#' # Load packages and data\n# devtools::install_github(\"tomasgreif/woe\")\nlibrary(\"woe\")\nlibrary(\"riskr\")\n\ndata(\"german_data\")\n\n#' Required for riskr\ngerman_data$gb2 <- ifelse(german_data$gb == \"good\", 1, 0)\n\n#' # Bivariate analysis:\n#' \n#' ## 1 variable case\n\nlvls <- names(sort(table(german_data$purpose)))\ngerman_data$purpose <- factor(as.character(german_data$purpose), levels = lvls)\n\n#' ### woe \niv.str(german_data,\"purpose\",\"gb\", verbose = FALSE)\niv.plot.woe(iv.mult(german_data,\"gb\",vars = c(\"purpose\"), summary = FALSE))\n\n#' ### riskr\nbt(german_data$purpose, german_data$gb2)\n\nplot_ba(german_data$purpose, german_data$gb2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0))\n  \n\n#' ## data frame case\n#' \n#' ### woe\niv.mult(german_data, \"gb\", vars =  c(\"purpose\", \"ca_status\", \"credit_history\", \"status_sex\"),\n        verbose = FALSE)\n\n#' ### riskr\nlibrary(\"tidyr\")\nlibrary(\"dplyr\")\n\ngerman_data %>% \n  tbl_df() %>% \n  select(gb2, purpose, ca_status, credit_history, status_sex) %>% \n  gather(variable, value, -gb2) %>% \n  group_by(variable) %>% \n  do({ bt(.$value, .$gb2)  })\n\n\n# iv.num(german_data,\"duration\",\"gb\")\n# iv.mult(german_data,\"gb\",TRUE, verbose = TRUE)\n# iv.plot.summary(iv.mult(german_data,\"gb\",TRUE))\n# head(german_data$credit_amount)\n# iv.binning.simple(german_data,\"credit_amount\")\n" }
{ "repo_name": "jbkunst/r-posts", "ref": "refs/heads/master", "path": "036-ezsummary-tests/readme.R", "content": "#' ---\r\n#' title: \"\"\r\n#' author: \"Joshua Kunst\"\r\n#' output:\r\n#'  html_document:\r\n#'    toc: true\r\n#'    keep_md: yes\r\n#' ---\r\n\r\n#+echo=FALSE\r\nknitr::opts_chunk$set(warning=FALSE)\r\n\r\n#+echo=TRUE\r\nlibrary(\"dplyr\")\r\nlibrary(\"ezsummary\")\r\n\r\ndata(diamonds, package = \"ggplot2\")\r\ntbl <- diamonds\r\n\r\nselect_categorical <- function(tbl, nuniques = 10){\r\n  \r\n  selections <- purrr::map_lgl(tbl, function(x) {\r\n    is.character(x) ||\r\n      is.factor(x) ||\r\n      is.logical(x) ||\r\n      (length(unique(x)) <= nuniques) # this is when you have numeric variables with few uniques (dummies)\r\n  })\r\n  tbl[, selections]\r\n  \r\n}\r\n\r\nselect_quantitative <- function(tbl){\r\n  \r\n  selections <- purrr::map_lgl(tbl, function(x) is.numeric(x))\r\n  tbl[, selections]\r\n  \r\n}\r\n\r\ndiamonds %>% \r\n  select_categorical() \r\n\r\ndiamonds %>% \r\n  select_quantitative()\r\n\r\n\r\n#### ####\r\n\r\n# tbl <- diamonds %>% select_categorical() %>%  group_by(cut)\r\n# tbl <- diamonds %>% select_categorical() %>%  group_by(cut, color)\r\n\r\nezsummary_categorical2 <- function(tbl){\r\n  \r\n  grp_cols <- names(attr(tbl, \"labels\"))\r\n  \r\n  tbl %>%\r\n    purrr::map_if(is.factor, as.character) %>% # avoid warning\r\n    as_data_frame() %>% # this ungroup the tbl\r\n    group_by_(.dots = lapply(grp_cols, as.symbol)) %>%  # http://stackoverflow.com/questions/21208801/\r\n    do({ezsum = \r\n      tidyr::gather(., key, value) %>% # you can use tidyr::gather(., variable, category)\r\n      ungroup() %>%\r\n      count(key, value) %>% \r\n      mutate(p = n/sum(n))\r\n    }) %>% \r\n    # ungroup() %>%\r\n    filter(!key %in% grp_cols) # not sure whiy appear as key a group col\r\n \r\n}\r\n\r\ndiamonds %>%\r\n  select_categorical() %>% \r\n  ezsummary_categorical() \r\n\r\ndiamonds %>%\r\n  select_categorical() %>% \r\n  ezsummary_categorical2() \r\n\r\nlibrary(rbenchmark)\r\n\r\ntblcat <- diamonds %>% select_categorical()\r\ntblcat <- diamonds %>% select_categorical() %>%  group_by(cut, color)\r\n\r\nbenchmark(\r\n  ezsummary_categorical(tblcat),\r\n  ezsummary_categorical2(tblcat),\r\n  replications = 100\r\n)\r\n# Mmm not so fast\r\n\r\nt1 <- diamonds %>%\r\n  select_categorical() %>% \r\n  ezsummary_categorical2() \r\n\r\nt1\r\n\r\n# check how much groups are \r\nsum(t1$p) == nrow(distinct(t1, key))\r\n\r\nt2 <- diamonds %>%\r\n  select_categorical() %>% \r\n  group_by(cut, color) %>% \r\n  ezsummary_categorical2()\r\n\r\nt2\r\n\r\n# check how much groups are \r\nsum(t2$p) == nrow(distinct(t2, cut, color, key))\r\n\r\nmtcars %>% \r\n  select_categorical(nuniques = 2) %>% \r\n  ezsummary_categorical2()\r\n\r\n\r\n\r\n\r\n" }
{ "repo_name": "greenelab/TDM", "ref": "refs/heads/master", "path": "R/package_loader.R", "content": "#' Loads a character vector of packages, installing them from Bioconductor if necessary.\n#' This will also load most CRAN packages.\n#'  \n#' @title load_it\n#' @name load_it\n#' @param pack -- A vector of character strings containing package names.\n#' @return nothing\n#' @export\n#'\nload_it = function(pack, update=FALSE) {\n\tinvisible(sapply(pack, function(package) {\n\t\tif(!require(package, quietly=T, character.only=T)){\n\t\t\t\tif(length(find(\"biocLite\")) < 1) {\n\t\t\t\t\t\tsource(\"http://bioconductor.org/biocLite.R\")\n\t\t\t\t}\n\t\t\t\tif(update) {\n\t\t\t\t\t\tbiocLite(package)\t\n\t\t\t\t} else {\n\t\t\t\t\t\tbiocLite(package, suppressUpdates=TRUE)\n\t\t\t\t}\n\n\t\t\t\tlibrary(package, quietly=T, character.only=T)\n\t\t\t}\n\t\t}))\n} #end load_it" }
{ "repo_name": "PoisonAlien/maftools", "ref": "refs/heads/master", "path": "R/oncodrive.R", "content": "#' Detect cancer driver genes based on positional clustering of variants.\n#'\n#' @description Clusters variants based on their position to detect disease causing genes.\n#' @details This is the re-implimentation of algorithm defined in OncodriveCLUST article. Concept is based on the fact that most of the variants in cancer causing genes are enriched at few specific loci (aka hotspots).\n#' This method takes advantage of such positions to identify cancer genes. Cluster score of 1 means, a single hotspot hosts all observed variants. If you use this function, please cite OncodriveCLUST article.\n#' @references Tamborero D, Gonzalez-Perez A and Lopez-Bigas N. OncodriveCLUST: exploiting the positional clustering of somatic mutations to identify cancer genes. Bioinformatics. 2013; doi: 10.1093/bioinformatics/btt395s\n#' @param maf an \\code{\\link{MAF}} object generated by \\code{\\link{read.maf}}\n#' @param AACol manually specify column name for amino acid changes. Default looks for field 'AAChange'\n#' @param pvalMethod either zscore (default method for oncodriveCLUST), poisson or combined (uses lowest of the two pvalues).\n#' @param nBgGenes minimum number of genes required to estimate background score. Default 100. Do not change this unless its necessary.\n#' @param minMut minimum number of mutations required for a gene to be included in analysis. Default 5.\n#' @param bgEstimate If FALSE skips background estimation from synonymous variants and uses predifined values estimated from COSMIC synonymous variants.\n#' @param ignoreGenes Ignore these genes from analysis. Default NULL. Helpful in case data contains large number of variants belonging to polymorphic genes such as mucins and TTN.\n#' @return data table of genes ordered according to p-values.\n#' @seealso \\code{\\link{plotOncodrive}}\n#' @examples\n#'\n#' laml.maf <- system.file(\"extdata\", \"tcga_laml.maf.gz\", package = \"maftools\")\n#' laml <- read.maf(maf = laml.maf, removeSilent = TRUE, useAll = FALSE)\n#' laml.sig <- oncodrive(maf = laml, AACol = 'Protein_Change', minMut = 5)\n#'\n#'\n#' @importFrom dplyr filter\n#' @export\n\n\noncodrive = function(maf, AACol = NULL, minMut = 5, pvalMethod = 'zscore', nBgGenes = 100, bgEstimate = TRUE, ignoreGenes = NULL){\n\n  #Proetin Length source\n  gl = system.file('extdata', 'prot_len.txt.gz', package = 'maftools')\n\n  if(Sys.info()[['sysname']] == 'Windows'){\n    gl.gz = gzfile(description = gl, open = 'r')\n    gl <- suppressWarnings( data.table(read.csv( file = gl.gz, header = TRUE, sep = '\\t', stringsAsFactors = FALSE)) )\n    close(gl.gz)\n  } else{\n    gl = fread(input = paste('zcat <', gl), sep = '\\t', stringsAsFactors = FALSE)\n  }\n\n  pval.options = c('zscore', 'poisson', 'combined')\n\n  if(!pvalMethod %in% pval.options){\n    stop('pvalMethod can only be either zscore, poisson or combined')\n  }\n\n  if(length(pvalMethod) > 1){\n    stop('pvalMethod can only be either zscore, poisson or combined')\n  }\n\n\n  #syn variants for background\n  syn.maf = maf@maf.silent\n  #number of samples in maf\n  numSamples = as.numeric(maf@summary[3,summary])\n  #Perform clustering and calculate background scores.\n  if(bgEstimate){\n    if(nrow(syn.maf) == 0){\n      message('No syn mutations found! Skipping background estimation. Using predefined values. (Mean = 0.279; SD = 0.13)')\n      bg.mean = 0.279\n      bg.sd = 0.13\n    }else{\n      message('Estimating background scores from synonymous variants..')\n      syn.bg.scores = parse_prot(dat = syn.maf, AACol = AACol, gl, m = minMut, calBg = TRUE, nBg = nBgGenes)\n\n      #If number of genes to calculate background scores is not enough, use predefined scores.\n      if(is.null(syn.bg.scores)){\n        message(\"Not enough genes to build background. Using predefined values. (Mean = 0.279; SD = 0.13)\")\n        bg.mean = 0.279\n        bg.sd = 0.13\n      }else {\n        if(nrow(syn.bg.scores) < nBgGenes){\n          message(\"Not enough genes to build background. Using predefined values. (Mean = 0.279; SD = 0.13)\")\n          bg.mean = 0.279\n          bg.sd = 0.13\n        }else{\n          bg.mean = mean(syn.bg.scores$clusterScores)\n          bg.sd = sd(syn.bg.scores$clusterScores)\n          message(paste('Estimated background mean: ', bg.mean))\n          message(paste('Estimated background SD: ', bg.sd))\n        }\n      }\n    }\n  }else{\n    message(\"Using predefined values for background. (Mean = 0.279; SD = 0.13)\")\n    bg.mean = 0.279\n    bg.sd = 0.13\n  }\n\n\n\n  #non-syn variants\n  non.syn.maf = maf@data\n  #in case user read maf without removing silent variants, remove theme here.\n  silent = c(\"3'UTR\", \"5'UTR\", \"3'Flank\", \"Targeted_Region\", \"Silent\", \"Intron\",\n             \"RNA\", \"IGR\", \"Splice_Region\", \"5'Flank\", \"lincRNA\")\n  non.syn.maf = non.syn.maf[!Variant_Classification %in% silent] #Remove silent variants from main table\n\n  #Remove genes to ignore\n  if(!is.null(ignoreGenes)){\n    ignoreGenes.count = nrow(non.syn.maf[Hugo_Symbol %in% ignoreGenes])\n    message(paste('Removed', ignoreGenes.count, 'variants belonging to', paste(ignoreGenes, collapse = ', ', sep=',')))\n    non.syn.maf = non.syn.maf[!Hugo_Symbol %in% ignoreGenes]\n  }\n\n  #Perform clustering and calculate cluster scores for nonsyn variants.\n  message('Estimating cluster scores from non-syn variants..')\n  nonsyn.scores = parse_prot(dat = non.syn.maf, AACol = AACol, gl = gl, m = minMut, calBg = FALSE, nBg = nBgGenes)\n\n  if(pvalMethod == 'combined'){\n    message('Comapring with background model and estimating p-values..')\n    nonsyn.scores$zscore = (nonsyn.scores$clusterScores - bg.mean) / bg.sd\n    nonsyn.scores$tPval = 1- pnorm(nonsyn.scores$zscore)\n    nonsyn.scores$tFdr = p.adjust(nonsyn.scores$tPval, method = 'fdr')\n\n    nonsyn.scores = merge(getGeneSummary(maf), nonsyn.scores, by = 'Hugo_Symbol')\n    nonsyn.scores[,fract_muts_in_clusters := muts_in_clusters/total]\n\n    counts.glm = glm(formula = total ~ protLen+clusters, family = poisson(link = identity), data = nonsyn.scores) #Poisson model\n    nonsyn.scores$Expected = counts.glm$fitted.values #Get expected number of events (mutations) from the model\n\n    observed_mut_colIndex = which(colnames(nonsyn.scores) == 'total')\n    expected_mut_colIndex = which(colnames(nonsyn.scores) == 'Expected')\n\n    #Poisson test to caluclate difference (p-value)\n    nonsyn.scores$poissonPval = apply(nonsyn.scores, 1, function(x) {\n      poisson.test(as.numeric(x[observed_mut_colIndex]), as.numeric(x[expected_mut_colIndex]))$p.value\n    })\n\n    nonsyn.scores$poissonFdr = p.adjust(nonsyn.scores$poissonPval)\n    nonsyn.scores = nonsyn.scores[order(poissonFdr)]\n\n    nonsyn.scores$fdr = apply(nonsyn.scores[,.(tFdr, poissonFdr)], MARGIN = 1, FUN = min)\n\n  } else if(pvalMethod == 'zscore'){\n    #Oncodrive clust way of caluclating pvalues\n    #Calculate z scores; compare it to bg scores and estimate z-score, pvalues, corrected pvalues (fdr) (assumes normal distribution)\n    message('Comapring with background model and estimating p-values..')\n    nonsyn.scores$zscore = (nonsyn.scores$clusterScores - bg.mean) / bg.sd\n    nonsyn.scores$pval = 1- pnorm(nonsyn.scores$zscore)\n    nonsyn.scores$fdr = p.adjust(nonsyn.scores$pval, method = 'fdr')\n\n    nonsyn.scores = merge(getGeneSummary(maf), nonsyn.scores, by = 'Hugo_Symbol')\n    nonsyn.scores[,fract_muts_in_clusters := muts_in_clusters/total]\n    #nonsyn.scores[,fract_MutatedSamples := MutatedSamples/numSamples]\n    nonsyn.scores = nonsyn.scores[order(fdr)]\n  }else{\n    #Assuming poisson distribution of mutation counts\n    #Now model observed number of mutations as a function of number of clusters and protein length. Calculate expected number of events based on poisson distribution.\n    nonsyn.scores = merge(getGeneSummary(maf), nonsyn.scores, by = 'Hugo_Symbol')\n    nonsyn.scores[,fract_muts_in_clusters := muts_in_clusters/total]\n\n    counts.glm = glm(formula = total ~ protLen+clusters, family = poisson(link = identity), data = nonsyn.scores) #Poisson model\n    nonsyn.scores$Expected = counts.glm$fitted.values #Get expected number of events (mutations) from the model\n\n    observed_mut_colIndex = which(colnames(nonsyn.scores) == 'total')\n    expected_mut_colIndex = which(colnames(nonsyn.scores) == 'Expected')\n\n    #Poisson test to caluclate difference (p-value)\n    nonsyn.scores$pval = apply(nonsyn.scores, 1, function(x) {\n      poisson.test(as.numeric(x[observed_mut_colIndex]), as.numeric(x[expected_mut_colIndex]))$p.value\n    })\n\n    nonsyn.scores$fdr = p.adjust(nonsyn.scores$pval)\n    nonsyn.scores = nonsyn.scores[order(fdr)]\n  }\n  message('Done !')\n  return(nonsyn.scores)\n}\n" }
{ "repo_name": "PoisonAlien/maftools", "ref": "refs/heads/master", "path": "R/titv.R", "content": "#' Classifies SNPs into transitions and transversions\n#' @description takes output generated by read.maf and classifies Single Nucleotide Variants into Transitions and Transversions.\n#'\n#' @param maf an \\code{\\link{MAF}} object generated by \\code{\\link{read.maf}}\n#' @param useSyn Logical. Whether to include synonymous variants in analysis. Defaults to FALSE.\n#' @param plot plots a titv fractions. default TRUE.\n#' @param file basename for output file name. If given writes summaries to output file. Default NULL.\n#' @return list of \\code{data.frame}s with Transitions and Transversions summary.\n#' @seealso  \\code{\\link{plotTiTv}}\n#' @examples\n#' laml.maf <- system.file(\"extdata\", \"tcga_laml.maf.gz\", package = \"maftools\")\n#' laml <- read.maf(maf = laml.maf, removeSilent = TRUE, useAll = FALSE)\n#' laml.titv = titv(maf = laml, useSyn = TRUE)\n#'\n#' @export\n\n\ntitv = function(maf, useSyn = FALSE, plot = TRUE, file = NULL)\n{\n\n  #Synonymous variants\n  maf.silent = maf@maf.silent\n  #Main data\n  maf = maf@data\n\n  #in case user read maf without removing silent variants, remove theme here.\n  silent = c(\"3'UTR\", \"5'UTR\", \"3'Flank\", \"Targeted_Region\", \"Silent\", \"Intron\",\n             \"RNA\", \"IGR\", \"Splice_Region\", \"5'Flank\", \"lincRNA\")\n  maf = maf[!Variant_Classification %in% silent] #Remove silent variants from main table\n\n  if(useSyn){\n    maf = rbind(maf, maf.silent, fill = TRUE)\n  }\n\n  maf = maf[Variant_Type == 'SNP']\n\n  #Some TCGA studies have Start_Position set to as 'position'. Change if so.\n  if(length(grep(pattern = 'Start_position', x = colnames(maf))) > 0){\n    colnames(maf)[which(colnames(maf) == 'Start_position')] = 'Start_Position'\n  }\n\n  if(length(grep(pattern = 'End_position', x = colnames(maf))) > 0){\n    colnames(maf)[which(colnames(maf) == 'End_position')] = 'End_Position'\n  }\n\n  maf = maf[,.(Hugo_Symbol, Start_Position, End_Position, Reference_Allele, Tumor_Seq_Allele2, Tumor_Sample_Barcode)]\n  maf$con = paste(maf[,Reference_Allele], maf[,Tumor_Seq_Allele2], sep = '-')\n\n  maf.con.summary = maf[,.N, by = .(Tumor_Sample_Barcode, con)]\n  maf.con.summary$con.class = suppressWarnings(as.character(factor(maf.con.summary$con, levels = c(\"A-G\", \"T-C\", \"C-T\", \"G-A\", \"A-T\", \"T-A\", \"A-C\", \"T-G\", \"C-A\", \"G-T\", \"C-G\", \"G-C\"),\n                                                                   labels = c(\"A-G\", \"A-G\", \"C-T\", \"C-T\", \"A-T\", \"A-T\", \"A-C\", \"A-C\", \"C-A\", \"C-A\", \"C-G\", \"C-G\"))))\n\n\n  maf.con.class.summary = maf.con.summary[,sum(N), by = .(Tumor_Sample_Barcode, con.class)]\n  colnames(maf.con.class.summary)[ncol(maf.con.class.summary)] = 'nVars'\n  suppressWarnings(maf.con.class.summary[,fract := (nVars/sum(nVars))*100, by = .(Tumor_Sample_Barcode)])\n\n  maf.con.class.summary$con.class = factor(x = maf.con.class.summary$con.class,\n                                           levels = c(\"A-G\", \"C-T\", \"A-T\", \"A-C\", \"C-A\", \"C-G\"))\n  maf.con.class.summary$TiTv = suppressWarnings(as.character(factor(x = maf.con.class.summary$con.class,\n                                                                    levels = c(\"A-G\", \"C-T\", \"A-T\", \"A-C\", \"C-A\", \"C-G\"), labels = c('Ti', 'Ti', 'Tv', 'Tv', 'Tv', 'Tv'))))\n\n  fract.classes = data.table::dcast(data = maf.con.class.summary, formula = Tumor_Sample_Barcode ~ con.class, value.var = 'fract', fill = 0)\n  raw.classes = data.table::dcast(data = maf.con.class.summary, formula = Tumor_Sample_Barcode ~ con.class, value.var = 'nVars', fill = 0)\n  titv.summary = maf.con.class.summary[,sum(fract), by = .(Tumor_Sample_Barcode, TiTv)]\n  titv.summary = data.table::dcast(data = titv.summary, Tumor_Sample_Barcode ~ TiTv, value.var = 'V1', fill = 0)\n\n  titv.res = list(fraction.contribution = fract.classes, raw.counts = raw.classes, TiTv.fractions = titv.summary)\n\n  if(plot){\n    plotTiTv(res = titv.res)\n  }\n\n  if(!is.null(file)){\n    write.table(x = fract.classes,file = paste(file, '_fraction_contribution.txt', sep = ''), quote = FALSE, row.names = FALSE, sep = '\\t')\n    write.table(x = raw.classes,file = paste(file, '_fraction_counts.txt', sep = ''), quote = FALSE, row.names = FALSE, sep = '\\t')\n    write.table(x = titv.summary,file = paste(file, '_TiTv_fractions.txt', sep = ''), quote = FALSE, row.names = FALSE, sep = '\\t')\n  }\n\n  return(titv.res)\n}\n" }
{ "repo_name": "joyent/Rbunyan", "ref": "refs/heads/master", "path": "R/bunyanTracebackErrors.R", "content": "# Roxygen Comments bunyanTracebackErrors\n#' Count of errors above level threshold of 50 after setpoint\n#'\n#' Returns the number of ERROR/FATAL log messages\n#' encountered since bunyanSetpoint first called. Note that\n#' only the first call to bunyanSetpoint is used, subsequent\n#' calls are ignored. Use bunyanClearSetpoint to clear before\n#' setting a new setpoint.\n#'\n#'\n#' @keywords bunyan, setpoint\n#'\n#' @export\nbunyanTracebackErrors <-\nfunction() {\n    return(bunyan_globals$errorssincemark)\n}\n" }
{ "repo_name": "henkelstone/NPEL.Classification", "ref": "refs/heads/master", "path": "data-raw/constants.R", "content": "# Create package constants\n\n##### Scope Constants #####\nsuppModels <- c('randomForest','rfsrc','fnn.FNN','fnn.class','kknn','gbm')          # Which packages (model classes) are supported by NPEL.Classification\nprobModels <- c('randomForest','rfsrc','fnn.FNN','fnn.class','kknn','gbm')          # Which packages generate probabilities when passed categorical data\ncontModels <- c('randomForest','rfsrc','kknn','gbm')                                # Which packages are able to handle continuous data }\n\n##### Generate Ecotype Groupings #####\n# Create a shell to fill with data\necoGroup <- list(); length(ecoGroup) <- 12;\ndim(ecoGroup) <- c(4,3)                                                   # Three rows (different classifiers), and three columns (type of data we could extract)\nrownames(ecoGroup)=c('identity','domSpecies','maxGranularity','domGroup') # Rows are different grouping scenarios\ncolnames(ecoGroup)=c('transform','labels','colours')                      # Cols are different things we could look up\n\n# Fill with data\necoGroup['identity',] <- list(c(1:27), paste0('BS',1:27), 1:27)\necoGroup['domSpecies',] <- list( c(1,1,2,2,2,2,3,3,3,3,4,4,5,5,6,7,8,8,8,8,9,9,9,9,9,10,10),\n                                 c('1'=\"Barren\",'2'=\"Pine\",'3'=\"Black Sp\",'4'=\"White Sp\",'5'=\"Birch\",'6'=\"Aspen\",'7'=\"Swamp\",'8'=\"Bog\",'9'=\"Fen\",'10'=\"Shore\"),\n                                 c('1'=\"gray\",'2'=\"tan1\",'3'=\"green4\",'4'=\"seagreen\",'5'=\"green3\",'6'=\"green2\",'7'=\"steelblue\",'8'=\"steelblue2\",'9'=\"steelblue4\",'10'=\"azure\") )\necoGroup['maxGranularity',] <- list( c(1,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,15,16,17,17,18,18,19,20,20,21,21), paste0('BS',1:21), 1:21)\necoGroup['domGroup',] <- list( c(1,1,2,2,2,2,2,2,2,2,2,2,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4),\n                               c(\"Barren\",\"Conifer\",\"Decid\",\"Wetland\"),\n                               c('1'=\"gray\",'2'=\"green4\",'3'=\"green2\",'4'=\"steelblue\") )\n" }
{ "repo_name": "henkelstone/NPEL.Classification", "ref": "refs/heads/master", "path": "tests/testthat/test_package.R", "content": "# Automated testing of NPEL.Classification package\n# Created by Jonathan Henkelman 16.Feb.2016\n\nlibrary(NPEL.Classification)\ncontext(\"Package\")\ncat('\\n')\n\ntest_examples('../../man')\n# testthat::test_examples('./man')\n" }
{ "repo_name": "henkelstone/NPEL.Classification", "ref": "refs/heads/master", "path": "tests/testthat/test_util.R", "content": " # Automated testing of the 'util.R' file in the NPEL package\n# Created by Jonathan Henkelman 27.Jan.2016\n\nlibrary(NPEL.Classification)\nlibrary(testthat)\ncontext(\"Util Functions\")\ncat('\\n')\n\n# Setup testing environment\nfacA <- c(2,3,5,1);     lvlA <- c(3:5,1:2)\nfacB <- c(12,13,15,11); lvlB <- c(13:15,11:12)\ntFacA <- factor (facA, levels=lvlA)\ntFacB <- factor (facB, levels=lvlB)\nfacC <- factor(NULL)\n\ntest_that(\"factorValues\", {\n  expect_equal(factorValues(tFacA), facA)\n  expect_equal(factorValues(tFacB), facB)\n  expect_equal(class(factorValues(facC)), 'numeric')\n  expect_equal(length(factorValues(facC)), 0)\n})\n\ntest_that(\"sortLevels\", {\n  sFac <- sortLevels(tFacA)\n  expect_equal(factorValues(sFac), facA)\n  expect_equal(as.numeric(levels(sFac)), c(1:5))\n  sFac <- sortLevels(facC)\n  expect_equal(class(sFac), 'factor')\n  expect_equal(length(sFac), 0)\n})\n\ntest_that(\"trimLevels\", {\n  sFac <- trimLevels(tFacA)\n  expect_equal(factorValues(sFac), facA)\n  expect_equal(as.numeric(levels(sFac)), c(3,5,1,2))\n  sFac <- trimLevels(facC)\n  expect_equal(class(sFac), 'factor')\n  expect_equal(length(sFac), 0)\n})\n\ntest_that(\"mergeLevels\", {\n  sFac <- mergeLevels(tFacA,tFacB)\n  expect_equal(factorValues(sFac), facA)\n  expect_equal(as.numeric(levels(sFac)), sort(c(lvlA,lvlB)))\n  sFac <- mergeLevels(tFacA,facC)\n  expect_equal(sFac, sortLevels(tFacA))\n})\n\ntest_that(\"fx2vars\", {\n  expect_error(fx2vars(),\"fx2vars: needs some data; supply either a formula, x and y, and/or a list of names.\")\n  fx <- x <- y <- NULL; fx2vars(formula('a~b+c'),x,y);                                   expect_equal(x,c('b','c')); expect_equal(y,'a')\n  fx <- x <- y <- NULL; fx2vars(fx,c('b','c'),'a');                                      expect_equal(fx,formula('a~b+c'))\n  fx <- x <- y <- NULL; fx2vars(fx,x,y,names=c('a','b','c'));                            expect_equal(fx,formula('a~b+c')); expect_equal(x,c('b','c')); expect_equal(y,'a')\n\n  fx <- x <- y <- NULL; expect_error(fx2vars(fx,c('b','c'),'a',names=c('a','b')),        \"fx2vars: a column specified for x does not occur in the dataset.\")\n  fx <- x <- y <- NULL; expect_error(fx2vars(formula('a~b+c'),x,y,names=c('a','b')),     \"fx2vars: a column specified for x does not occur in the dataset.\")\n  fx <- x <- y <- NULL; expect_error(fx2vars(formula('a+d~b+c'),x,y),                    \"fx2vars: multivariate analysis not supported; specify only a single variable for y.\")\n  fx <- x <- y <- NULL; expect_error(fx2vars(fx,c('b','c'),c('a','d')),                  \"fx2vars: multivariate analysis not supported; specify only a single variable for y.\")\n  fx <- x <- y <- NULL; expect_error(fx2vars(formula('d~b+c'),x,y,names=c('a','b','c')), \"fx2vars: variable specified for y does not occur in the dataset; check the column names.\")\n  fx <- x <- y <- NULL; expect_error(fx2vars(fx,c('b','c'),'d',names=c('a','b','c')),    \"fx2vars: variable specified for y does not occur in the dataset; check the column names.\")\n  fx <- x <- y <- NULL; expect_error(fx2vars(fx,x,'a'),                                  \"fx2vars: cannot find a suitable default for x variables; provide fx and/or names.\")\n  fx <- x <- y <- NULL; expect_error(fx2vars(fx,c('b','d'),'a',names=c('a','b','c')),    \"fx2vars: a column specified for x does not occur in the dataset.\")\n  fx <- x <- y <- NULL; expect_error(fx2vars(fx,c('a','b','c'),'a'),                     \"fx2vars: x cannot contain the y variable.\")\n  fx <- x <- y <- NULL; expect_error(fx2vars(formula('a~a+b+c'),x,y),                    \"fx2vars: x cannot contain the y variable.\")\n})\n" }
{ "repo_name": "henkelstone/NPEL.Classification", "ref": "refs/heads/master", "path": "R/Plot.R", "content": "# Functions used to plot tiles and output\n# Created 9.Oct.2015 from pre-existing code file started 6.Apr.2015\n\n##### plotTile.base #####\n#' Creates a base ggplot object of the raster object\n#'\n#' @param data a raster* object to plot\n#' @param layers (optional) the layers to include as facets\n#' @param title (optional) a title for the plot\n#' @param maxpixels (optional) the maximum number of pixels to output\n#' @param reduction (optional) is a reducing scale factor applied to both x and y axis; will use whichever is less: ncell/(reduction^2) or maxpixels\n#' @param ... other parameters for ggplot\n#' @return a ggplot object which can have geoms, scales, etc. added to it (see PlotTile).\n#' @export\nplotTile.base <- function(data, layers=NULL, title=\"\", maxpixels=500000, reduction=1, ...) {\n  if (!requireNamespace('ggplot2')) stop(\"ggplot2 package required for plotting\")\n  if (!is.null(layers)) data <- subset(data,layers)\n\n  # Theres a glitch in sampleRegular: we can't just simply ask for sR(data, number of cells, xy=TRUE) as it doesn't return the same values\n  # for xy as if we do it step by step: x <- sR(data,#cells,xy=FALSE,raster=TRUE); xyFromCell(x,1:#cells). I don't know why this is, but it\n  # causes artifacts in the final plot so it is necessary to do the work around shown here... It just means we have to extract the data\n  # twice so it's a bit slower.\n  colNames <- names(data)\n  data <- raster::sampleRegular(data, size=min(raster::ncell(data)/(reduction*reduction),maxpixels),asRaster=TRUE)   # Return a raster so we can extract the coords afterwards.\n  data <- data.frame(raster::xyFromCell(data,1:raster::ncell(data)), raster::getValues(data))\n  names(data) <- c('x','y',colNames)\n  dat <- reshape(data=data,direction='long',idvar=1:2,varying=3:dim(data)[2],v.names='value',timevar='type',times=names(data)[3:dim(data)[2]]) # Massage into long form; over a GB for a full tile!\n\n  # Create a base plot object with some useful aesthetics and that holds the data; then if this is not the only plot the user wants, they should be able to output more without needing to resample the data\n  return( ggplot2::ggplot(ggplot2::aes(x = 'x', y = 'y'), data = 'dat', ...) +\n            ggplot2::theme(axis.text.y=ggplot2::element_text(angle=90,hjust=0.5)) + ggplot2::coord_equal() +\n            ggplot2::labs(title=title,x='Latitude (UTM)',y='Longitude (UTM)',fill='Value') )\n}\n\n##### plotTile #####\n#' Given a ggplot base object create a finished plot\n#' @param gp the base ggplot to which to add a colour scale and aesthetic (see plotTile.base)\n#' @param layers a character vector of the levels which to plot\n#' @param discrete force the plot to attempt a discrete fill axis\n#' @param colours specify the colours use; a list if discrete is TRUE, or, 2 or 3 colours for the gradient if discrete is FALSE\n#' @param labels if desired, specify the names of the labels for each class\n#' @param ... other parameters to pass\n#' @return the ggplot object with additional aes and scale\nplotTile <- function(gp, layers, discrete, colours, labels=NULL,...){\n\n  # Note: ggplot saves the data in the object which both allows this, but also makes for a large object... like over a GB for a full tile -- I recommend against\n  # saving the R database on exit without being sure you have time for this file to save/load.\n\n  if (!requireNamespace('ggplot2')) stop(\"ggplot2 package required for plotting\")\n  if (discrete) { if (is.null(colours)) stop (\"plotTile: if discrete is specified, so must be colours\") }\n  else          { if (is.null(colours) || !sum(length(colours) == c(2,3))) stop(\"plotTile: either 2 or 3 colours need to be specified for continuous gradient scales\") }\n\n  gp <- ggplot2::`%+%`(gp, subset(gp$data,gp$data$type %in% layers))   # Subset the data. Necessary to do it this way so we don't have to specify the subset to every item, i.e. scale, geom, facet, etc.\n  if (length(list(...))) gp <- gp + ...                   # Add any extra parameters the user may have supplied\n  if (discrete) {                                         # If it's discrete then the user should have supplied the colours\n    if (is.null(labels)) return( gp + ggplot2::aes(fill='factor(value)') + ggplot2::geom_raster() + ggplot2::scale_fill_manual(values=colours) )\n    else                 return( gp + ggplot2::aes(fill='factor(value)') + ggplot2::geom_raster() + ggplot2::scale_fill_manual(values=colours, labels=labels) )\n  } else {\n    if (length (colours) == 2) return( gp + ggplot2::aes(fill='value') + ggplot2::geom_raster() + ggplot2::scale_fill_gradient (low=colours[1], high=colours[2]) + ggplot2::facet_wrap('~type') )\n    if (length (colours) == 3) return( gp + ggplot2::aes(fill='value') + ggplot2::geom_raster() + ggplot2::scale_fill_gradient2(low=colours[1], mid=colours[2], high=colours[3], midpoint=(max(gp$data$value,na.rm=TRUE)-min(gp$data$value,na.rm=TRUE))/2) + ggplot2::facet_wrap('~type') )\n  }\n}\n\n##### plotTiles #####\n#' Outputs plots of multiple data files into a single folder\n#' @param path the path of the folder\n#' @param base the base part of the filename\n#' @param extension the filename 'extension'; may contain more than the strict extension\n#' @param models a list of model names\n#' @param type (optional) type out output to use\n#' @param ... other parameters to pass to the device function\nplotTiles <- function(path, base, extension, models, type='pdf',...) {\n  for (fType in models) {\n    fName <- paste0(path,base,fType,extension)\n    Tile <- raster::brick(paste0(fName,'.tif'))\n    if (raster::nlayers(Tile) > 1) { names(Tile) <- c('Class', 'Prob', paste0('Prob.',1:(raster::nlayers(Tile)-2))) } else { names(Tile) <- c('Class') }\n\n    if (type=='png') png(paste0(fName,'.png'),...)\n    else pdf(paste0(fName,'.pdf'),width=10.0,height=7.5,onefile=TRUE)\n\n    gp <- plotTile.base(Tile, layers=names(Tile), maxpixels=1000000)\n    plotTile(gp, layers='Class', discrete=TRUE, colours=NPEL.Classification::ecoGroup[['domSpecies','colours']]) #,aes(alpha='Prob'))\n    if (fType != 'fnn') plotTile(gp, layers=c('Prob',paste('Prob',1:7,sep='.')), discrete=FALSE, colours=c('grey17','red','yellow'))\n    #    Sys.sleep(15)\n    dev.off(dev.cur())\n  }\n}\n" }
{ "repo_name": "rgriff23/btw", "ref": "refs/heads/master", "path": "R/killbt.R", "content": "killbt = function() {\n\tjobs = system(\"pgrep BayesTraits\", intern=T)\n\tfor (n in 1:length(jobs)) {system(paste(\"kill\", jobs[n]))}\n}\n\n" }
{ "repo_name": "fcharte/CursoCienciaDatosR", "ref": "refs/heads/master", "path": "instalaPaquetes.R", "content": "install.packages(\"caret\")\n# LINUX: system('wajig install libgtk2.0-dev') # Use 'sudo apt-get install wajig' at the command line if needed, then install Gtk headers\ninstall.packages('RGtk2')\ninstall.packages(\"rattle\")\ninstall.packages(\"neuralnet\")\ninstall.packages(\"autoencoder\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"h2o\", dependencies = c(\"Depends\", \"Suggests\"))\ninstall.packages(\"pROC\")\ninstall.packages(\"arules\")\ninstall.packages(\"arulesViz\")\ninstall.packages(\"fpc\")\n\nlibrary(rattle)  # WINDOWS: Choose to install Gtk\nrattle()\n\nlibrary(h2o)\nlocalH2O <- h2o.init() # It should indicate that H2O is initialized, as well as the Java VM version\n\n# Download https://github.com/fcharte/CursoCienciaDatosR/blob/master/data/datosTrabajo.RData and save it in working directory\n\nload('datosTrabajo.RData')\n# Look for the previous file at the \"Files\" panel, and click it to load it. All the data should appear in the Environment tab\n\nlibrary(caret)\nlibrary(neuralnet)\nlibrary(autoencoder)\nlibrary(ggplot2)\n" }
{ "repo_name": "rich-iannone/PuffR", "ref": "refs/heads/master", "path": "R/calmet_get_ncdc_history.R", "content": "#' Retrieve the NCDC history data file\n#' @description This function initiates a download of the NCDC surface station history file.\n#' @param replace.file selecting 'yes' will overwrite history file if it exists in the working directory.\n#' @export calmet_get_ncdc_history\n#' @examples\n#' \\dontrun{\n#' # Obtain the NCDC history file\n#' calmet_get_ncdc_history()\n#'}\n\ncalmet_get_ncdc_history <- function(replace.file = FALSE) {\n  \n  # Get hourly surface data history CSV from NOAA/NCDC FTP\n  file <- \"ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-history.csv\"\n  \n  if (replace.file == TRUE) {\n    repeat {\n      \n      try(download.file(file, \"ish-history.csv\", quiet = TRUE))\n      \n      if (file.info(\"ish-history.csv\")$size > 0) { break }\n      \n    }\n    \n  } else { }\n  \n  # Check if file exists in working directory\n  if (file.exists(\"ish-history.csv\") &\n        file.info(\"ish-history.csv\")$size > 0 &\n        replace.file == FALSE) { } else { \n          \n          repeat {\n            \n            try(download.file(file, \"ish-history.csv\", quiet = TRUE))\n            \n            if (file.info(\"ish-history.csv\")$size > 0) { break }\n            \n          }\n          \n        }\n  \n  # Read in the \"ish-history\" CSV file\n  st <- read.csv(\"ish-history.csv\")\n  \n}\n" }
{ "repo_name": "rich-iannone/PuffR", "ref": "refs/heads/master", "path": "R/calpuff_add_area_sources.R", "content": "#' Add area sources to a list for later use in CALPUFF\n#' @description Add area sources to a list for later use in CALPUFF\n#' @param src_name the name of the source emitting the species.\n#' @param species_name the name of the species undergoing emissions.\n#' @param lat_dec_deg a vector of 4 latitude values for the area source in units of decimal degrees.\n#' @param lon_dec_deg a vector of 4 longitude values for the area source in units of decimal degrees.\n#' @param x_coord_km a vector of 4 UTM easting values for the area source in km units.\n#' @param y_coord_km a vector of 4 UTM northing values for the area source in km units.\n#' @param UTM_zone the UTM zone for the area source.\n#' @param UTM_hemisphere the UTM hemisphere for the area source.\n#' @param effective_height the effective height of the area source in meters above ground level (m AGL).\n#' @param base_elev the ground elevation at the location of the area source in meters above sea level (m ASL).\n#' @param init_sigma_z the initial sigma z value for the area source in meters.\n#' @param emission_rate the rate of constant emissions from the area source; units are defined in the 'emission_units' argument.\n#' @param emission_units the units applied to the value defined in the 'emission_rate' argument. The possible selections are: (1) \"g/m2/s\", (2) \"kg/m2/hr\", (3) \"lb/m2/hr\", (4) \"tons/m2/yr\", (5) \"Odour Unit * m/s\", (6) \"Odour Unit * m/min\", (7) \"metric tons/m2/yr\", (8) \"Bq/m2/s\", and (9) \"GBq/m2/yr\".\n#' @export calpuff_add_area_sources\n\ncalpuff_add_area_sources <- function(src_name,\n                                     species_name,\n                                     lat_dec_deg = NULL,\n                                     lon_dec_deg = NULL,\n                                     x_coord_km = NULL,\n                                     y_coord_km = NULL,\n                                     UTM_zone = NULL,\n                                     UTM_hemisphere = NULL,\n                                     effective_height,\n                                     base_elev,\n                                     init_sigma_z,\n                                     emission_rate,\n                                     emission_units){\n  \n  # Add require statements\n  require(rgdal)\n  require(raster)\n  require(stringr)\n  require(plyr)\n  \n  # Get expected filename for area sources\n  area_sources_filename <-\n    paste0(unlist(str_split(getwd(),\n                            pattern = \"/\"))[length(unlist(str_split(getwd(),\n                                                                    pattern = \"/\")))],\n           \"--area_sources.txt\")\n  \n  # Create area sources text file with header if it doesn't exist\n  if (file.exists(area_sources_filename) == FALSE){\n    \n    # Create empty file in working folder\n    file.create(area_sources_filename)\n    \n    # Add header row to new area sources file\n    cat(paste0(\"src_name\", \",\",\n               \"species_name\", \",\",\n               \"lat_dec_deg_1\", \",\",\n               \"lon_dec_deg_1\", \",\",\n               \"lat_dec_deg_2\", \",\",\n               \"lon_dec_deg_2\", \",\",\n               \"lat_dec_deg_3\", \",\",\n               \"lon_dec_deg_3\", \",\",\n               \"lat_dec_deg_4\", \",\",\n               \"lon_dec_deg_4\", \",\",\n               \"x_coord_km_1\", \",\",\n               \"y_coord_km_1\", \",\",\n               \"x_coord_km_2\", \",\",\n               \"y_coord_km_2\", \",\",\n               \"x_coord_km_3\", \",\",\n               \"y_coord_km_3\", \",\",\n               \"x_coord_km_4\", \",\",\n               \"y_coord_km_4\", \",\",\n               \"UTM_zone\", \",\",\n               \"UTM_hemisphere\", \",\",\n               \"effective_height\", \",\",\n               \"base_elev\", \",\",\n               \"init_sigma_z\", \",\",\n               \"emission_rate\", \",\",\n               \"emission_units\"),\n        sep = \"\\n\",\n        file = area_sources_filename,\n        append = TRUE)\n    \n  }\n  \n  # Determine whether lon/lat provided\n  if (!is.null(lat_dec_deg) & !is.null(lon_dec_deg)){\n    lon_lat_provided <- TRUE\n  } else {\n    lon_lat_provided <- FALSE\n  }\n  \n  # Determine whether UTM coordinates and zone information provided\n  if (!is.null(x_coord_km) & !is.null(y_coord_km)\n      & !is.null(UTM_zone) & !is.null(UTM_hemisphere)){\n    UTM_provided <- TRUE\n  } else {\n    UTM_provided <- FALSE\n  }  \n  \n  # If both lon/lat provided, convert to UTM\n  if (lon_lat_provided == TRUE & UTM_provided == FALSE){\n    \n    # Get matrix of longitude and latitude for source location\n    lat_lon_dec_deg <- cbind(lon_dec_deg, lat_dec_deg)\n    \n    # Determine the UTM zone\n    UTM_zone <- unique((floor((lon_dec_deg + 180)/6) %% 60) + 1)[1]\n    \n    # Determine whether source is in the Northern Hemisphere or the Southern Hemisphere\n    UTM_hemisphere <- unique(ifelse(lat_dec_deg >= 0, \"N\", \"S\"))[1]\n    \n    # Define a PROJ.4 projection string for a lat/lon projection\n    proj_string_longlat <- \"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\"\n    \n    # Define a PROJ.4 projection string for a UTM projection\n    proj_string_UTM <- paste0(\"+proj=utm +zone=\",\n                              UTM_zone,\n                              \" +ellps=WGS84 +datum=WGS84 +units=m +no_defs\")\n    \n    # Project as UTM coordinates from the determined UTM zone\n    UTM_location <- project(lat_lon_dec_deg, proj_string_UTM)\n    \n    # Define the UTM x coordinates in km units\n    x_coord_km <- UTM_location[,1] / 1000\n    \n    # Define the UTM y coordinates in km units\n    y_coord_km <- UTM_location[,2] / 1000\n    \n  }\n  \n  # If UTM coordinates provided, convert to lon/lat\n  if (lon_lat_provided == FALSE & UTM_provided == TRUE){\n    \n    # Define a PROJ.4 projection string for a lat/lon projection\n    proj_string_longlat <- \"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\"\n    \n    # Define a PROJ.4 projection string for a UTM projection\n    proj_string_UTM <- paste0(\"+proj=utm +zone=\",\n                              UTM_zone,\n                              \" +ellps=WGS84 +datum=WGS84 +units=m +no_defs\")\n    \n    # Create a SpatialPoints object for the UTM coordinates\n    UTM_m_SP <- SpatialPoints(matrix(c(x_coord_km * 1000,\n                                       y_coord_km * 1000),\n                                     nrow = 4,\n                                     ncol = 2),\n                              proj4string = CRS(proj_string_UTM))\n    \n    # Project as UTM coordinates from the determined UTM zone\n    latlon_SP <- spTransform(UTM_m_SP, CRS(proj_string_longlat))\n    \n    # Extract the latitude values in decimal degrees from the SpatialPoints object\n    lat_dec_deg <- latlon_SP@coords[,2]\n    \n    # Extract the longitude values in decimal degrees from the SpatialPoints object\n    lon_dec_deg <- latlon_SP@coords[,1]\n    \n  }\n  \n  # Write the values to the file\n  cat(paste0(src_name, \",\",\n             species_name, \",\",\n             format(lat_dec_deg[1], small.interval = 6), \",\",\n             format(lon_dec_deg[1], small.interval = 6), \",\",\n             format(lat_dec_deg[2], small.interval = 6), \",\",\n             format(lon_dec_deg[2], small.interval = 6), \",\",\n             format(lat_dec_deg[3], small.interval = 6), \",\",\n             format(lon_dec_deg[3], small.interval = 6), \",\",\n             format(lat_dec_deg[4], small.interval = 6), \",\",\n             format(lon_dec_deg[4], small.interval = 6), \",\",\n             format(x_coord_km[1], small.interval = 3), \",\",\n             format(y_coord_km[1], small.interval = 3), \",\",\n             format(x_coord_km[2], small.interval = 3), \",\",\n             format(y_coord_km[2], small.interval = 3), \",\",\n             format(x_coord_km[3], small.interval = 3), \",\",\n             format(y_coord_km[3], small.interval = 3), \",\",\n             format(x_coord_km[4], small.interval = 3), \",\",\n             format(y_coord_km[4], small.interval = 3), \",\",\n             UTM_zone, \",\",\n             UTM_hemisphere, \",\",\n             effective_height, \",\",\n             base_elev, \",\",\n             init_sigma_z, \",\",\n             emission_rate, \",\",\n             emission_units),\n      sep = \"\\n\",\n      file = area_sources_filename,\n      append = TRUE)\n  \n}\n" }
{ "repo_name": "sfr/RStudio-Addin-Snippets", "ref": "refs/heads/master", "path": "tests/testthat/data/.foobar.R", "content": "# dummy method for copy.data unit testing\n.foobar <- function( .x_y_z, a )\n{\na <- F # not indented on purpose\n\n    if (missing(.x_y_z)) {\n        print('Please set a list .x_y_z.')\n    } else {\n        print(.x_y_z$pi)\n    }\n\n    a\n}\n\n# dummy method for insert.pipe unit testing\n.foobar2 <- function()\n{\n    select <- function()\n    {\n        NULL\n    }\n\n    filter <- function()\n    {\n        NULL\n    }\n\n    # test here\n    a <- select() %   >  %\n\n\n\n\n        filter()\n\n\n    # expected\n    a <- select() %>%\n        filter() %>%\n            as.data.frame()\n\n    a\n}\n\ntop.context <- rstudioapi::getActiveDocumentContext()\ntop.context[['selection']][[1]] <- NULL\nsave(top.context, file='.\\\\tests\\\\testthat\\\\.foobar.Rdata')\nrm(top.context)\n" }
{ "repo_name": "ellisp/ggseas", "ref": "refs/heads/master", "path": "pkg/tests/testthat.R", "content": "library(testthat)\nlibrary(ggseas)\n\ntest_check(\"ggseas\")\n" }
{ "repo_name": "ellisp/ggseas", "ref": "refs/heads/master", "path": "prep/create_ldeaths_df.R", "content": "library(tidyr)\nlibrary(dplyr)\n\nldeaths_df <- cbind(fdeaths, mdeaths, YearMon = time(fdeaths)) %>%\n   as.data.frame() %>%\n   gather(sex, deaths, -YearMon) %>%\n   mutate(sex = ifelse(sex == \"fdeaths\", \"female\", \"male\"))\n\n\nsave(ldeaths_df, file = \"data/ldeaths_df.rda\")" }
{ "repo_name": "woobe/rApps", "ref": "refs/heads/master", "path": "oddsimiser/server.R", "content": "suppressMessages(library(shiny))\nsuppressMessages(library(combinat))\nsuppressMessages(library(GA))\nsuppressMessages(library(MCMCpack))\n\nshinyServer(function(input, output) {\n  \n  ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  ## Optimiser (Single)\n  ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  optimise_single <- reactive({\n    \n    odds_all <- c()  \n    if (input$odds1 > 1) odds_all <- c(odds_all, input$odds1)\n    if (input$odds2 > 1) odds_all <- c(odds_all, input$odds2)\n    if (input$odds3 > 1) odds_all <- c(odds_all, input$odds3)\n    if (input$odds4 > 1) odds_all <- c(odds_all, input$odds4)\n    if (input$odds5 > 1) odds_all <- c(odds_all, input$odds5)\n    if (input$odds6 > 1) odds_all <- c(odds_all, input$odds6)\n    if (input$odds7 > 1) odds_all <- c(odds_all, input$odds7)\n    if (input$odds8 > 1) odds_all <- c(odds_all, input$odds8)\n    if (input$odds9 > 1) odds_all <- c(odds_all, input$odds9)\n    if (input$odds10 > 1) odds_all <- c(odds_all, input$odds10)\n    as.matrix(odds_all)\n    \n        \n    ## Evaluate Function\n    eval_odds <- function(inp_p) {\n      total_p <- sum(inp_p)\n      rtn <- odds_all * as.matrix(inp_p) - total_p\n      #return(min(rtn))\n      return(1-(max(rtn)-min(rtn))/max(rtn))\n    }\n    \n    ## Define parameters for GA\n    para_ga_min <- rep(0, length(odds_all))\n    para_ga_max <- rep(1, length(odds_all))\n    \n    ## Optimise with GA\n    set.seed(1234)\n    model <- ga(type = \"real-valued\",\n                fitness = eval_odds, monitor = FALSE,\n                min = para_ga_min, max = para_ga_max,\n                popSize = 10, maxiter = 1000)\n    \n    ## Normalise and save results\n    best_p <- summary(model)$solution / sum(summary(model)$solution)\n    \n    ## Create summary df\n    sum_df <- data.frame(matrix(NA, nrow = 4, ncol = length(odds_all)))\n    colnames(sum_df) <- paste0(\"\", 1:length(odds_all))\n    rownames(sum_df) <- c(\"\", \"\", \"\", \"\")  \n    sum_df[1,] <- odds_all\n    sum_df[2,] <- best_p * input$stake\n    sum_df[3,] <- odds_all * best_p * input$stake\n    sum_df[4,] <- (odds_all * best_p - 1) * input$stake\n    \n    ## Return\n    t(sum_df)\n    \n  })\n  \n  \n  ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  ## Optimiser Double\n  ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  optimise_double <- reactive({\n    \n    odds_all <- c()  \n    if (input$odds1 > 1) odds_all <- c(odds_all, input$odds1)\n    if (input$odds2 > 1) odds_all <- c(odds_all, input$odds2)\n    if (input$odds3 > 1) odds_all <- c(odds_all, input$odds3)\n    if (input$odds4 > 1) odds_all <- c(odds_all, input$odds4)\n    if (input$odds5 > 1) odds_all <- c(odds_all, input$odds5)\n    if (input$odds6 > 1) odds_all <- c(odds_all, input$odds6)\n    if (input$odds7 > 1) odds_all <- c(odds_all, input$odds7)\n    if (input$odds8 > 1) odds_all <- c(odds_all, input$odds8)\n    if (input$odds9 > 1) odds_all <- c(odds_all, input$odds9)\n    if (input$odds10 > 1) odds_all <- c(odds_all, input$odds10)\n    as.matrix(odds_all)\n      \n    games_all <- paste0(\"\", 1:length(odds_all))\n    games_double <- combn(games_all, 2)\n    \n    cbn_double <- combn(odds_all, 2)\n    odds_all <- cbn_double[1, ] * cbn_double[2, ]\n    \n    ## Evaluate Function\n    eval_odds <- function(inp_p) {\n      total_p <- sum(inp_p)\n      rtn <- odds_all * as.matrix(inp_p) - total_p\n      #return(min(rtn))\n      return(1-(max(rtn)-min(rtn))/max(rtn))\n    }\n    \n    ## Define parameters for GA\n    para_ga_min <- rep(0, length(odds_all))\n    para_ga_max <- rep(1, length(odds_all))\n    \n    ## Optimise with GA\n    set.seed(1234)\n    model <- ga(type = \"real-valued\",\n                fitness = eval_odds, monitor = FALSE,\n                min = para_ga_min, max = para_ga_max,\n                popSize = 20, maxiter = 2000)\n    \n    ## Normalise and save results\n    best_p <- summary(model)$solution / sum(summary(model)$solution)\n    \n    ## Create summary df\n    sum_df <- data.frame(matrix(NA, nrow = 4, ncol = length(odds_all)))\n    \n    names_double <- c()\n    for (n in 1:length(odds_all)) {\n      names_double <- c(names_double, paste0(games_double[1,n], \"x\", games_double[2,n]))\n    }\n    colnames(sum_df) <- names_double\n    \n    rownames(sum_df) <- c(\"\", \"\", \"\", \"\")  \n    sum_df[1,] <- odds_all\n    sum_df[2,] <- best_p * input$stake\n    sum_df[3,] <- odds_all * best_p * input$stake\n    sum_df[4,] <- (odds_all * best_p - 1) * input$stake\n    \n    ## Return\n    t(sum_df)\n    \n  })\n  \n  \n  \n  ## Outputs\n  \n  output$single <- renderTable({\n    print(as.data.frame(optimise_single()))\n  })\n  \n  output$double <- renderTable({\n    print(as.data.frame(optimise_double()))\n  })\n  \n  \n})" }
{ "repo_name": "fcocquemas/rdatastream", "ref": "refs/heads/master", "path": "R/help.R", "content": "#' A R interface for Datastream and Thomson Dataworks Enterprise. \n#'\n#' @docType package\n#' @name RDatastream\n#' @aliases RDatastream\n#' @import XML RCurl\nNULL" }
{ "repo_name": "Robinlovelace/learning-shiny", "ref": "refs/heads/master", "path": "hi/server.R", "content": "library(shiny)\n\n# Define server logic required to draw a histogram\nshinyServer(function(input, output) {\n\n  # Expression that generates a histogram. The expression is\n  # wrapped in a call to renderPlot to indicate that:\n  #\n  #  1) It is \"reactive\" and therefore should be automatically\n  #     re-executed when inputs change\n  #  2) Its output type is a plot\n\n  output$distPlot <- renderPlot({\n    x    <- faithful[, 2]  # Old Faithful Geyser data\n    bins <- seq(min(x), max(x), length.out = input$bins + 1)\n\n    # draw the histogram with the specified number of bins\n    hist(x, breaks = bins, col = 'darkgray', border = 'white')\n  })\n\n})" }
{ "repo_name": "zachmayer/cv.ts", "ref": "refs/heads/master", "path": "tests/testthat.R", "content": "library(testthat)\nlibrary(cv.ts)\n\ntest_check(\"cv.ts\")\n" }
{ "repo_name": "RGLab/MAST", "ref": "refs/heads/summarizedExpt", "path": "R/ZlmFit-bootstrap.R", "content": "##' Bootstrap a zlmfit\n##'\n##' Sample cells with replacement to find bootstrapped distribution of coefficients\n##' @param cl a \\code{cluster} object created by \\code{makeCluster}\n##' @param zlmfit class \\code{ZlmFit}\n##' @param R number of bootstrap replicates\n##' @return array of bootstrapped coefficients\n##' @export\npbootVcov1<-function (cl,zlmfit, R = 99)\n{\n    sca <- zlmfit@sca\n    N <- ncol(sca)\n    LMlike <- zlmfit@LMlike\n    parallel::clusterEvalQ(cl,require(MAST))\n    ## clusterEvalQ(cl,require(abind))\n    parallel::clusterExport(cl,\"N\",envir=environment())\n    parallel::clusterExport(cl,\"LMlike\",envir=environment())\n    parallel::clusterExport(cl,\"sca\",envir=environment())\n    manyvc <- parallel::parSapply(cl,1:R, function(i,...){\n        s <- sample(N, replace = TRUE)\n        newsca <- sca[, s]\n        LMlike <- update(LMlike, design=colData(newsca))\n        zlm.SingleCellAssay(sca = newsca, LMlike = LMlike, onlyCoef=TRUE)\n    })\n  \n    d<-dim(coef(zlmfit,\"D\"))\n    manyvc<-aperm(array(manyvc,c(d,2,R)),c(4,1,2,3))\n    dimnames(manyvc)<-c(list(NULL),dimnames(coef(zlmfit,\"D\")),list(c(\"C\",\"D\")))\n    manyvc\n}\n\n##' Bootstrap a zlmfit\n##'\n##' Sample cells with replacement to find bootstrapped distribution of coefficients\n##' @param zlmfit class \\code{ZlmFit}\n##' @param R number of bootstrap replicates\n##' @return array of bootstrapped coefficients\n##' @importFrom plyr raply\n##' @export\nbootVcov1 <- function(zlmfit, R=99){\n    sca <- zlmfit@sca\n    N <- ncol(sca)\n    LMlike <- zlmfit@LMlike\n    manyvc <- raply(R, {\n        s <- sample(N, replace=TRUE)\n        newsca <- sca[,s]\n        LMlike <- update(LMlike, design=colData(newsca))\n        zlm.SingleCellAssay(sca=newsca, LMlike=LMlike, onlyCoef=TRUE)\n    })\n\n   manyvc\n    \n}\n\n" }
{ "repo_name": "RGLab/MAST", "ref": "refs/heads/summarizedExpt", "path": "tests/testthat/test-lmWrapper-glmer.R", "content": "obj <- new('LMERlike', design=colData(fd), formula=~Stim.Condition + (1|Subject.ID))\n\ncontext('LMERlike')\nif(require(lme4)){\n    obj <- fit(obj, response=exprs(fd)[,2])\nobjC <- lmer(obj@response ~Stim.Condition +  (1|Subject.ID), data=as.data.frame(obj@design), subset=obj@response>0, REML=FALSE)\nobjD <- glmer(obj@response>0 ~Stim.Condition + (1|Subject.ID), data=as.data.frame(obj@design), family=binomial())\nsource('common-lmWrapper-tests.R', local=TRUE)\n    try(detach('package:lme4'), silent=TRUE)\n}\n\n" }
{ "repo_name": "RGLab/MAST", "ref": "refs/heads/summarizedExpt", "path": "tests/testthat/helper-vbeta-init.R", "content": "geneid=\"Gene\"\nprimerid='Gene'\nmeasurement='et'\nidvars=c('Subject.ID', 'Chip.Number', 'Stim.Condition', 'Population', 'Well', 'Number.of.Cells')\nphenovars=NULL\ncellvars='Experiment.Number'\nfeaturevars=NULL\nncells <- 'Number.of.Cells'\n\n## Currently needed because devtools 1.11.0 has broken data()\n## See https://github.com/mlr-org/mlr/pull/835\nload(system.file('data/vbeta.RData', package='MAST'))\ndata(vbeta)\n\nvbeta$et <- ifelse(is.na(vbeta$Ct), 0, 40-vbeta$Ct)\n\n\nfd <- FromFlatDF(vbeta, idvars=idvars, primerid=primerid, measurement=measurement,cellvars=cellvars, geneid=geneid, ncells='Number.of.Cells', class='FluidigmAssay')\n" }
{ "repo_name": "terrytangyuan/dml", "ref": "refs/heads/master", "path": "tests/testthat/test_helper_functions.R", "content": "context('helper functions')\n\ntest_that('package set up successfully', {\n  expect_that(sum(1,2), not(throws_error()))\n})\n" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Getting_and_Cleaning_Data/Grouping_and_Chaining_with_dplyr/scripts/chain3.R", "content": "# Use filter() to select all rows for which size_mb is\n# less than or equal to (<=) 0.5.\n#\n# If you want your results printed to the console, add\n# print to the end of your chain.\n\ncran %>%\n  select(ip_id, country, package, size) %>%\n  mutate(size_mb = size / 2^20) %>%\n  # Your call to filter() goes here\n" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Data_Analysis/Dispersion/initLesson.R", "content": "assign(\"cars\", openintro::cars, envir=globalenv())\nassign(\"mpg.midsize\", cars[cars$type==\"midsize\",\"mpgCity\"], envir=globalenv())\n" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Exploratory_Data_Analysis/Dimension_Reduction/showRanMat.R", "content": "#par(mar=rep(0.2,4))\nimage(1:10,1:40,t(dataMatrix)[,nrow(dataMatrix):1])\n#par(mar=rep(0.2,4))\n" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Writing_swirl_Courses/R/yamlWriter.R", "content": "#' DIRECTIONS: \n#' 1. Source this file and invoke newLesson(courseName, lessonName), where\n#' courseName and lessonName are strings of your choice such as \"My Course\" and \n#' \"My Lesson 1\". Spaces are allowed in course and lesson names.\n#' This will create a file such as My_Course/My_Lesson_1/lesson.yaml.\n#' The file should appear in the editor automatically. If not, open it manually.\n#' \n#' 2. Fill in the course meta-information, Author, Organization, etc. as indicated in \n#' lesson.yaml. \n#' \n#' 3. Append instructional units such as multiple-choice questions to lesson.yaml by\n#' invoking appropriate helper functions. Type hlp() for a brief list. Typing qmult(),\n#' for instance would append the following template to lesson.yaml:\n#' - Class: mult_question  \n#'   Output: ask the multiple choice question here\n#'   AnswerChoices: ANS;2;3\n#'   CorrectAnswer: ANS\n#'   AnswerTests: omnitest(correctVal= 'ANS')\n#'   Hint: hint\n#' which might be manually edited as follows.\n#' - Class: mult_question  \n#'   Output: This demonstrates a multiple choice question. Which Scooby-Doo character wears an ascot?\n#'   AnswerChoices: Fred Jones;Velma Dinkley;Daphne Blake;Shaggy Rogers\n#'   CorrectAnswer: Fred Jones\n#'   AnswerTests: omnitest(correctVal= 'Fred Jones')\n#'   Hint: This person doesn't say \"Jinkies.\"\n#'   \n#' 4. Save lesson.yaml whenever you append and edit a unit of instruction.\n#' \n#' 5. Strings may take more than one line provided they are surrounded by double quotes. For instance,\n#' in the following question the Output: field takes up two lines:\n#' - Class: text_question\n#'   Output: \"This illustrates a question requiring a single phrase text answer. \n#' What is the name of the van which carries Scooby's gang?\"\n#'   CorrectAnswer: Mystery Machine\n#'   AnswerTests: omnitest(correctVal='Mystery Machine')\n#'   Hint: The gang generally solves a M------? It's the M------ Machine.\n\nnewLesson <- function(course, lesson){\n  courseDir <- file.path(gsub(\" \", \"_\", course))\n  lessonDir <<- file.path(courseDir, gsub(\" \", \"_\", lesson))\n  if(!file.exists(lessonDir))dir.create(lessonDir, recursive=TRUE)\n  # Check for existence of a manifest\n  manifest <- file.path(courseDir, \"MANIFEST\")\n  if(!file.exists(manifest)){\n    file.create(manifest)\n  }\n  # Append the current lesson to the manifest\n  cat(paste0(gsub(\" \", \"_\", lesson), \"\\n\"), file=manifest, append=TRUE)\n  # The yaml faq, http://www.yaml.org/faq.html, encourages\n  # use of the .yaml (as opposed to .yml) file extension\n  # whenever possible.\n  lessonFile <<- file.path(lessonDir, \"lesson.yaml\")\n  writeLines(c(\"- Class: meta\", \n               paste(\"  Course:\", course),\n               paste(\"  Lesson:\", lesson),\n               \"  Author: your name goes here\",\n               \"  Type: Standard\",\n               \"  Organization: your organization\",\n               paste(\"  Version: \", packageDescription(\"swirl\")$Version)),\n             lessonFile)\n  # Create supporting files\n  depends <- file.path(lessonDir, \"dependson.txt\")\n  file.create(depends)\n  init <- file.path(lessonDir, \"initLesson.R\")\n  file.create(init)\n  cat(\"# Code placed in this file will be executed every time the\n# lesson is started. Any variables created here will show up in\n# the user's working directory and thus be accessible to them\n# throughout the lesson.\\n\", file=init)\n  custom <- file.path(lessonDir, \"customTests.R\")\n  file.create(custom)\n  cat(\"# Put custom tests in this file.\n\n# Uncommenting the following line of code will disable\n# auto-detection of new variables and thus prevent swirl from\n# executing every command twice, which can slow things down.\n\n# AUTO_DETECT_NEWVAR <- FALSE\n\n# However, this means that you should detect user-created\n# variables when appropriate. The answer test, expr_creates_var()\n# can be used for for the purpose, but it also re-evaluates the\n# expression which the user entered, so care must be taken.\\n\", \n      file=custom)\n  file.edit(lessonFile)\n}\n\nsetLesson <- function(course, lesson){\n  courseDir <- file.path(gsub(\" \", \"_\", course))\n  lessonDir <- file.path(courseDir, gsub(\" \", \"_\", lesson))\n  lessonFile <- file.path(lessonDir, \"lesson.yaml\")\n  if(!file.exists(lessonFile)){\n    stop(paste(\"Sorry!\", lessonFile, \"doesn't exist. Check the path.\"))\n  } else {\n    lessonDir <<- lessonDir\n    lessonFile <<- lessonFile\n    file.edit(lessonFile)\n  }\n}\n\n# yaml writer help\nhlp <- function(){\n  print(\"txt -- just text, no question\")\n  print(\"qmult -- multiple choice question\")\n  print(\"qcmd -- command line question\")\n  print(\"vid -- video\")\n  print(\"fig -- figure\")\n  print(\"qx -- question requiring exact numerical answer\")\n  print(\"qtxt -- question requiring a short text answer\")\n  print(\"qrng -- question requiring a numerical answer within a certain range.\")\n}\n\n# template for presentation without a question\ntxt <- function(){\n  cat(\"\\n- Class: text\n  Output: put your exposition here.\\n\", file=lessonFile, append=TRUE)\n  invisible()\n}\n\n# template for multiple choice question\nqmult <- function(){\n  cat(\"\\n- Class: mult_question  \n  Output: ask the multiple choice question here\n  AnswerChoices: ANS;2;3\n  CorrectAnswer: ANS\n  AnswerTests: omnitest(correctVal= 'ANS')\n  Hint: hint\\n\", file=lessonFile, append=TRUE)\n  invisible()\n}\n\nqcmd <- function(){\n  cat(\"\\n- Class: cmd_question\n  Output: explain what the user must do here\n  CorrectAnswer: EXPR or VAL\n  AnswerTests: omnitest(correctExpr='EXPR', correctVal=VAL)\n  Hint: hint\\n\", file=lessonFile, append=TRUE)\n  invisible()\n}\n\nvid <- function(){\n  cat(\"\\n- Class: video\n  Output: Would you like to watch a short video about ___?\n  VideoLink: 'http://address.of.video'\\n\", file=lessonFile, append=TRUE)\n  invisible()\n}\n\nfig <- function(){\n  cat(\"\\n- Class: figure\n  Output: explain the figure here\n  Figure: sourcefile.R\n  FigureType: new or add\\n\", file=lessonFile, append=TRUE)\n  invisible()\n}\n\nqx<- function(){\n  cat(\"\\n- Class: exact_question\n  Output: explain the question here\n  CorrectAnswer: n\n  AnswerTests: omnitest(correctVal=n)\n  Hint: hint\\n\", file=lessonFile, append=TRUE)\ninvisible()\n}\n\nqtxt <- function(){\n  cat(\"\\n- Class: text_question\n  Output: explain the question here\n  CorrectAnswer: answer\n  AnswerTests: val_matches('regular_expression_which_matches_answer')\n  Hint: hint\\n\", file=lessonFile, append=TRUE)\ninvisible()\n}\n\nqrng <- function(){\n  cat(\"\\n- Class: range_question\n  Output: explain the question here\n      CorrectAnswer: answer\n      AnswerTests: requires a custom test\n      Hint: hint\\n\", file=lessonFile, append=TRUE)\n  invisible()\n}\n\n\nreinstall <- function(){\n  course <- dirname(lessonDir)\n  # Uninstall_course\n  try(swirl::uninstall_course(course), silent=TRUE)\n  # Install course\n  swirl::install_course_directory(gsub(\" \", \"_\", course))\n}\n" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/Binary_Outcomes/initLesson.R", "content": "# For compatibility with 2.2.21\n.get_course_path <- function(){\n  tryCatch(swirl:::swirl_courses_dir(),\n           error = function(c) {file.path(find.package(\"swirl\"),\"Courses\")}\n  )\n}\n\n# ravens data\nravenData <- read.csv(file.path(.get_course_path(), \n                                 \"Regression_Models\", \"Binary_Outcomes\", \"ravens_data.csv\"))\nravenData <- ravenData[order(ravenData$ravenScore), 1:3]\nrownames(ravenData) <- NULL\n" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/Overfitting_and_Underfitting/initLesson.R", "content": "# For compatibility with 2.2.21\n.get_course_path <- function(){\n  tryCatch(swirl:::swirl_courses_dir(),\n           error = function(c) {file.path(find.package(\"swirl\"),\"Courses\")}\n  )\n}\n\nswiss <- datasets::swiss\nfile.copy(from=file.path(.get_course_path(),\n\t\"Regression_Models\", \"Overfitting_and_Underfitting\",\"fitting.R\"), \n          to=\"fitting.R\")\nfile.edit(\"fitting.R\")\nsource(\"fitting.R\", local=TRUE)\nfit5 <- lm(Fertility ~ Agriculture + Examination + Education + Catholic, swiss)\nfit6 <- lm(Fertility ~ ., swiss)\n" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Exploratory_Data_Analysis/Principles_of_Analytic_Graphs/MVD2.R", "content": "fname <- paste(path_to_course,\"MVData2.jpeg\",sep=\"/\")\ntry(dev.off(),silent=TRUE)\nplot.new()\nplotArea=par('fig')\nrasterImage(readJPEG(fname),plotArea[1],plotArea[3],plotArea[2],plotArea[4],interpolate=FALSE)\n" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Getting_and_Cleaning_Data/Tidying_Data_with_tidyr/scripts/script8.R", "content": "# Accomplish the following three goals:\n#\n# 1. select() all columns that do NOT contain the word \"total\",\n# since if we have the male and female data, we can always\n# recreate the total count in a separate column, if we want it.\n# Hint: Use the contains() function, which you'll\n# find detailed in 'Special functions' section of ?select.\n#\n# 2. gather() all columns EXCEPT score_range, using\n# key = part_sex and value = count.\n#\n# 3. separate() part_sex into two separate variables (columns),\n# called \"part\" and \"sex\", respectively. You may need to check\n# the 'Examples' section of ?separate to remember how the 'into'\n# argument should be phrased.\n#\nsat %>%\n  select(-contains(###)) %>%\n  gather(###, ###, -###) %>%\n  ### <Your call to separate()> %>%\n  print\n" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Getting_and_Cleaning_Data/Grouping_and_Chaining_with_dplyr/scripts/chain4.R", "content": "# arrange() the result by size_mb, in descending order.\n#\n# If you want your results printed to the console, add\n# print to the end of your chain.\n\ncran %>%\n  select(ip_id, country, package, size) %>%\n  mutate(size_mb = size / 2^20) %>%\n  filter(size_mb <= 0.5) %>%\n  # Your call to arrange() goes here\n" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/Introduction_to_Multivariable_Regression/customTests.R", "content": "# So swirl does not repeat execution of plot commands\nAUTO_DETECT_NEWVAR <- FALSE\n\n# Returns TRUE if e$expr matches any of the expressions given\n# (as characters) in the argument.\nANY_of_exprs <- function(...){\n  e <- get(\"e\", parent.frame())\n  any(sapply(c(...), function(expr)omnitest(expr)))\n}\n\n# Returns TRUE if the user has created a specified lm model\n# with a specified name.\ncreates_lm_model <- function(correctExpr){\n  e <- get(\"e\", parent.frame())\n  # Do what the user should have done\n  eSw <- cleanEnv(e$snapshot)\n  mdlSw <- eval(parse(text=correctExpr), eSw)\n  # Recreate what the user has done\n  eUsr <- cleanEnv(e$snapshot)\n  mdlUsr <- eval(e$expr, eUsr)\n  # If the correct model is named:\n  if(length(ls(eSw))>0){\n    # Check whether the model's name is correct\n    nameGood <- sum(ls(eUsr) %in% ls(eSw)) & sum(ls(eSw) %in% ls(eUsr))\n    # If not, highlight the misspelling\n    if(!nameGood){\n      swirl_out(paste0(\"You seem to have misspelled the model's name. I was expecting \", ls(eSw), \n                       \" but you apparently typed \", ls(eUsr), \".\"))\n      return(FALSE)\n    } else {\n      # Append the result, as a list to e$delta for progress restoration\n      e$delta <- c(e$delta, as.list(eUsr))\n    }\n  }\n  # Check for effective equality of the models\n  isTRUE(all.equal(sort(as.vector(mdlUsr$coefficients)), sort(as.vector(mdlSw$coefficients)))) &\n    isTRUE(all.equal(mdlUsr$fitted.values, mdlSw$fitted.values))\n}\n\n# Returns TRUE if the user has calculated a value equal to that calculated by the given expression.\ncalculates_same_value <- function(expr){\n  e <- get(\"e\", parent.frame())\n  # Calculate what the user should have done.\n  eSnap <- cleanEnv(e$snapshot)\n  val <- eval(parse(text=expr), eSnap)\n  isTRUE(all.equal(val, e$val))\n}\n\n# Returns TRUE of the user has calculated a value equal to any of those computed by the given\n# expressions.\ncalculates_ANY_value <- function(...){\n  e <- get(\"e\", parent.frame())\n  any(sapply(c(...), function(expr)calculates_same_value(expr)))\n}\n\n# Get the swirl state\ngetState <- function(){\n  # Whenever swirl is running, its callback is at the top of its call stack.\n  # Swirl's state, named e, is stored in the environment of the callback.\n  environment(sys.function(1))$e\n}\n\n# Get the value which a user either entered directly or was computed\n# by the command he or she entered.\ngetVal <- function(){\n  getState()$val\n}\n\n# Get the last expression which the user entered at the R console.\ngetExpr <- function(){\n  getState()$expr\n}\n\ncoursera_on_demand <- function(){\n  selection <- getState()$val\n  if(selection == \"Yes\"){\n    email <- readline(\"What is your email address? \")\n    token <- readline(\"What is your assignment token? \")\n    \n    payload <- sprintf('{  \n      \"assignmentKey\": \"hHsdF68wEeWxaw7Jay15BQ\",\n      \"submitterEmail\": \"%s\",  \n      \"secret\": \"%s\",  \n      \"parts\": {  \n        \"3wpmw\": {  \n          \"output\": \"correct\"  \n        }  \n      }  \n    }', email, token)\n    url <- 'https://www.coursera.org/api/onDemandProgrammingScriptSubmissions.v1'\n  \n    respone <- httr::POST(url, body = payload)\n    if(respone$status_code >= 200 && respone$status_code < 300){\n      message(\"Grade submission succeeded!\")\n    } else {\n      message(\"Grade submission failed.\")\n      message(\"Press ESC if you want to exit this lesson and you\")\n      message(\"want to try to submit your grade at a later time.\")\n      return(FALSE)\n    }\n  }\n  TRUE\n}" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/Introduction/restore_1.R", "content": "plot(child ~ parent, galton)\n" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Statistical_Inference/Probability1/customTests.R", "content": "# So swirl does not repeat execution of plot commands\nAUTO_DETECT_NEWVAR <- FALSE\n\n# Returns TRUE if e$expr matches any of the expressions given\n# (as characters) in the argument.\nANY_of_exprs <- function(...){\n  e <- get(\"e\", parent.frame())\n  any(sapply(c(...), function(expr)omnitest(expr)))\n}\n\nequiv_val <- function(correctVal){\n  e <- get(\"e\", parent.frame()) \n  #print(paste(\"User val is \",e$val,\"Correct ans is \",correctVal))\n  isTRUE(all.equal(correctVal,e$val))\n  \n}\n\n# Get the swirl state\ngetState <- function(){\n  # Whenever swirl is running, its callback is at the top of its call stack.\n  # Swirl's state, named e, is stored in the environment of the callback.\n  environment(sys.function(1))$e\n}\n\n# Get the value which a user either entered directly or was computed\n# by the command he or she entered.\ngetVal <- function(){\n  getState()$val\n}\n\n# Get the last expression which the user entered at the R console.\ngetExpr <- function(){\n  getState()$expr\n}\n\ncoursera_on_demand <- function(){\n  selection <- getState()$val\n  if(selection == \"Yes\"){\n    email <- readline(\"What is your email address? \")\n    token <- readline(\"What is your assignment token? \")\n    \n    payload <- sprintf('{  \n      \"assignmentKey\": \"BOLw8680EeWRRQpQejjiSw\",\n      \"submitterEmail\": \"%s\",  \n      \"secret\": \"%s\",  \n      \"parts\": {  \n        \"jwsCv\": {  \n          \"output\": \"correct\"  \n        }  \n      }  \n    }', email, token)\n    url <- 'https://www.coursera.org/api/onDemandProgrammingScriptSubmissions.v1'\n  \n    respone <- httr::POST(url, body = payload)\n    if(respone$status_code >= 200 && respone$status_code < 300){\n      message(\"Grade submission succeeded!\")\n    } else {\n      message(\"Grade submission failed.\")\n      message(\"Press ESC if you want to exit this lesson and you\")\n      message(\"want to try to submit your grade at a later time.\")\n      return(FALSE)\n    }\n  }\n  TRUE\n}" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/MultiVar_Examples/customTests.R", "content": "# So swirl does not repeat execution of plot commands\n#AUTO_DETECT_NEWVAR <- FALSE\n\n# Returns TRUE if e$expr matches any of the expressions given\n# (as characters) in the argument.\nANY_of_exprs <- function(...){\n  e <- get(\"e\", parent.frame())\n  any(sapply(c(...), function(expr)omnitest(expr)))\n}\n\n# Returns TRUE if the user has created a specified lm model\n# with a specified name.\ncreates_lm_model <- function(correctExpr){\n  e <- get(\"e\", parent.frame())\n  # Do what the user should have done\n  eSw <- cleanEnv(e$snapshot)\n  mdlSw <- eval(parse(text=correctExpr), eSw)\n  # Recreate what the user has done\n  eUsr <- cleanEnv(e$snapshot)\n  mdlUsr <- eval(e$expr, eUsr)\n  # If the correct model is named:\n  if(length(ls(eSw))>0){\n    # Check whether the model's name is correct\n    nameGood <- sum(ls(eUsr) %in% ls(eSw)) & sum(ls(eSw) %in% ls(eUsr))\n    # If not, highlight the misspelling\n    if(!nameGood){\n      swirl_out(paste0(\"You seem to have misspelled the model's name. I was expecting \", ls(eSw), \n                       \" but you apparently typed \", ls(eUsr), \".\"))\n      return(FALSE)\n    } else {\n      # Append the result, as a list to e$delta for progress restoration\n      e$delta <- c(e$delta, as.list(eUsr))\n    }\n  }\n  # Check for effective equality of the models\n  isTRUE(all.equal(sort(as.vector(mdlUsr$coefficients)), sort(as.vector(mdlSw$coefficients)))) &\n    isTRUE(all.equal(mdlUsr$fitted.values, mdlSw$fitted.values))\n}\n\n# Get the swirl state\ngetState <- function(){\n  # Whenever swirl is running, its callback is at the top of its call stack.\n  # Swirl's state, named e, is stored in the environment of the callback.\n  environment(sys.function(1))$e\n}\n\n# Get the value which a user either entered directly or was computed\n# by the command he or she entered.\ngetVal <- function(){\n  getState()$val\n}\n\n# Get the last expression which the user entered at the R console.\ngetExpr <- function(){\n  getState()$expr\n}\n\ncoursera_on_demand <- function(){\n  selection <- getState()$val\n  if(selection == \"Yes\"){\n    email <- readline(\"What is your email address? \")\n    token <- readline(\"What is your assignment token? \")\n    \n    payload <- sprintf('{  \n      \"assignmentKey\": \"iGMF3K8wEeWVdAqQVb1YyQ\",\n      \"submitterEmail\": \"%s\",  \n      \"secret\": \"%s\",  \n      \"parts\": {  \n        \"Dxjqj\": {  \n          \"output\": \"correct\"  \n        }  \n      }  \n    }', email, token)\n    url <- 'https://www.coursera.org/api/onDemandProgrammingScriptSubmissions.v1'\n  \n    respone <- httr::POST(url, body = payload)\n    if(respone$status_code >= 200 && respone$status_code < 300){\n      message(\"Grade submission succeeded!\")\n    } else {\n      message(\"Grade submission failed.\")\n      message(\"Press ESC if you want to exit this lesson and you\")\n      message(\"want to try to submit your grade at a later time.\")\n      return(FALSE)\n    }\n  }\n  TRUE\n}" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Statistical_Inference/CommonDistros/stddev1.R", "content": "g <- ggplot(dat, aes(x = x, y = y)) + geom_line(size = 1.5)\ng <- g + geom_ribbon(aes(x = ifelse(x > -1 & x < 1, x, 0), ymin = 0, ymax = dat$y), fill = \"red\", alpha = 1)\ng <- g +  geom_ribbon(aes(x = ifelse(x > -2 & x < 2, x, 0), ymin = 0, ymax = dat$y), fill = \"red\", alpha = 0.5)\ng <- g +  geom_ribbon(aes(x = ifelse(x > -3 & x < 3, x, 0), ymin = 0, ymax = dat$y), fill = \"red\", alpha = 0.35)\nprint(g)\n\n" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Getting_and_Cleaning_Data/Grouping_and_Chaining_with_dplyr/scripts/summarize2-correct.R", "content": "# Don't change any of the code below. Just type submit()\n# when you think you understand it.\n\n# We've already done this part, but we're repeating it\n# here for clarity.\n\nby_package <- group_by(cran, package)\npack_sum <- summarize(by_package,\n                      count = n(),\n                      unique = n_distinct(ip_id),\n                      countries = n_distinct(country),\n                      avg_bytes = mean(size))\n\n# Here's the new bit, but using the same approach we've\n# been using this whole time.\n\ntop_countries <- filter(pack_sum, countries > 60)\nresult1 <- arrange(top_countries, desc(countries), avg_bytes)\n\n# Print the results to the console.\nprint(result1)\n" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Data_Analysis/Central_Tendency/initLesson.R", "content": "  assign(\"cars\", openintro::cars, envir=globalenv())\n  assign(\"mpg.midsize\", cars[cars$type==\"midsize\",\"mpgCity\"], envir=globalenv())\n" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/Residuals_Diagnostics_and_Variation/restore_4.R", "content": "plot(fit, which=2)" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "R_Programming_Alt/Functions/customTests.R", "content": "test_func1 <- function() {\n  try({\n    func <- get('boring_function', globalenv())\n    t1 <- identical(func(9), 9)\n    t2 <- identical(func(4), 4)\n    t3 <- identical(func(0), 0)\n    ok <- all(t1, t2, t3)\n  }, silent = TRUE)\n  exists('ok') && isTRUE(ok)\n}\n\ntest_func2 <- function() {\n  try({\n    func <- get('my_mean', globalenv())\n    t1 <- identical(func(9), mean(9))\n    t2 <- identical(func(1:10), mean(1:10))\n    t3 <- identical(func(c(-5, -2, 4, 10)), mean(c(-5, -2, 4, 10)))\n    ok <- all(t1, t2, t3)\n  }, silent = TRUE)\n  exists('ok') && isTRUE(ok)\n}\n\ntest_func3 <- function() {\n  try({\n    func <- get('remainder', globalenv())\n    t1 <- identical(func(9, 4), 9 %% 4)\n    t2 <- identical(func(divisor = 5, num = 2), 2 %% 5)\n    t3 <- identical(func(5), 5 %% 2)\n    ok <- all(t1, t2, t3)\n  }, silent = TRUE)\n  exists('ok') && isTRUE(ok)\n}\n\ntest_func4 <- function() {\n  try({\n    func <- get('evaluate', globalenv())\n    t1 <- identical(func(sum, c(2, 4, 7)), 13)\n    t2 <- identical(func(median, c(9, 200, 100)), 100)\n    t3 <- identical(func(floor, 12.1), 12)\n    ok <- all(t1, t2, t3)\n  }, silent = TRUE)\n  exists('ok') && isTRUE(ok)\n}\n\ntest_func5 <- function() {\n  try({\n    func <- get('telegram', globalenv())\n    t1 <- identical(func(\"Good\", \"morning\"), \"START Good morning STOP\")\n    t2 <- identical(func(\"hello\", \"there\", \"sir\"), \"START hello there sir STOP\")\n    t3 <- identical(func(), \"START STOP\")\n    ok <- all(t1, t2, t3)\n  }, silent = TRUE)\n  exists('ok') && isTRUE(ok)\n}\n\ntest_func6 <- function() {\n  try({\n    func <- get('mad_libs', globalenv())\n    t1 <- identical(func(place = \"Baltimore\", adjective = \"smelly\", noun = \"Roger Peng statue\"), \"News from Baltimore today where smelly students took to the streets in protest of the new Roger Peng statue being installed on campus.\")\n    t2 <- identical(func(place = \"Washington\", adjective = \"angry\", noun = \"Shake Shack\"), \"News from Washington today where angry students took to the streets in protest of the new Shake Shack being installed on campus.\")\n    ok <- all(t1, t2)\n  }, silent = TRUE)\n  exists('ok') && isTRUE(ok)\n}\n\ntest_func7 <- function() {\n  try({\n    func <- get('%p%', globalenv())\n    t1 <- identical(func(\"Good\", \"job!\"), \"Good job!\")\n    t2 <- identical(func(\"one\", func(\"two\", \"three\")), \"one two three\")\n    ok <- all(t1, t2)\n  }, silent = TRUE)\n  exists('ok') && isTRUE(ok)\n}\n\ntest_eval1 <- function(){\n  try({\n    e <- get(\"e\", parent.frame())\n    expr <- e$expr\n    t1 <- identical(expr[[3]], 6)\n    expr[[3]] <- 7\n    t2 <- identical(eval(expr), 8)\n    ok <- all(t1, t2)\n  }, silent = TRUE)\n  exists('ok') && isTRUE(ok)\n}\n\ntest_eval2 <- function(){\n  try({\n    e <- get(\"e\", parent.frame())\n    expr <- e$expr\n    t1 <- identical(expr[[3]], quote(c(8, 4, 0)))\n    t2 <- identical(expr[[1]], quote(evaluate))\n    expr[[3]] <- c(5, 6)\n    t3 <- identical(eval(expr), 5)\n    ok <- all(t1, t2, t3)\n  }, silent = TRUE)\n  exists('ok') && isTRUE(ok)\n}\n\ntest_eval3 <- function(){\n  try({\n    e <- get(\"e\", parent.frame())\n    expr <- e$expr\n    t1 <- identical(expr[[3]], quote(c(8, 4, 0)))\n    t2 <- identical(expr[[1]], quote(evaluate))\n    expr[[3]] <- c(5, 6)\n    t3 <- identical(eval(expr), 6)\n    ok <- all(t1, t2, t3)\n  }, silent = TRUE)\n  exists('ok') && isTRUE(ok)\n}" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Exploratory_Data_Analysis/Working_with_Colors/customTests.R", "content": "# So swirl does not repeat execution of plot commands\nAUTO_DETECT_NEWVAR <- FALSE\n\n# Returns TRUE if e$expr matches any of the expressions given\n# (as characters) in the argument.\nANY_of_exprs <- function(...){\n  e <- get(\"e\", parent.frame())\n  any(sapply(c(...), function(expr)omnitest(expr)))\n}\n\nequiv_val <- function(correctVal){\n  e <- get(\"e\", parent.frame()) \n  #print(paste(\"User val is \",e$val,\"Correct ans is \",correctVal))\n  isTRUE(all.equal(correctVal,e$val))\n  \n}\n\n# Get the swirl state\ngetState <- function(){\n  # Whenever swirl is running, its callback is at the top of its call stack.\n  # Swirl's state, named e, is stored in the environment of the callback.\n  environment(sys.function(1))$e\n}\n\n# Get the value which a user either entered directly or was computed\n# by the command he or she entered.\ngetVal <- function(){\n  getState()$val\n}\n\n# Get the last expression which the user entered at the R console.\ngetExpr <- function(){\n  getState()$expr\n}\n\ncoursera_on_demand <- function(){\n  selection <- getState()$val\n  if(selection == \"Yes\"){\n    email <- readline(\"What is your email address? \")\n    token <- readline(\"What is your assignment token? \")\n    \n    payload <- sprintf('{  \n      \"assignmentKey\": \"jbRkna8dEeWxIhKVGQB0WQ\",\n      \"submitterEmail\": \"%s\",  \n      \"secret\": \"%s\",  \n      \"parts\": {  \n        \"Lxfoi\": {  \n          \"output\": \"correct\"  \n        }  \n      }  \n    }', email, token)\n    url <- 'https://www.coursera.org/api/onDemandProgrammingScriptSubmissions.v1'\n  \n    respone <- httr::POST(url, body = payload)\n    if(respone$status_code >= 200 && respone$status_code < 300){\n      message(\"Grade submission succeeded!\")\n    } else {\n      message(\"Grade submission failed.\")\n      message(\"Press ESC if you want to exit this lesson and you\")\n      message(\"want to try to submit your grade at a later time.\")\n      return(FALSE)\n    }\n  }\n  TRUE\n}" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Exploratory_Data_Analysis/Hierarchical_Clustering/dendro.R", "content": "try(dev.off(),silent=TRUE)\nplot.new()\nplot(as.dendrogram(hc))" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Exploratory_Data_Analysis/CaseStudy/initLesson.R", "content": "library(fields)\n\n# For compatibility with 2.2.21\n.get_course_path <- function(){\n  tryCatch(swirl:::swirl_courses_dir(),\n           error = function(c) {file.path(find.package(\"swirl\"),\"Courses\")}\n  )\n}\n\n# Put initialization code in this file.\npath_to_course <- file.path(.get_course_path(),\n  \"Exploratory_Data_Analysis\",\"CaseStudy\")\ntry(dev.off(),silent=TRUE)\nplot.new()\n\npathtofile <- function(fileName){\n  mypath <- file.path(.get_course_path(),\n    \"Exploratory_Data_Analysis\",\"CaseStudy\",\n                      fileName)\n}\nfxfer <- function(fileName){\n  mypath <- pathtofile(fileName)\n  file.copy(mypath,fileName)\n}\n\nmyImage <- function(iname){\n  par(mfrow=c(1,1))\n  par(mar=c(8,10,8,10))\n  image(t(iname)[,nrow(iname):1])\n}\nmyedit <- function(fname){\n   #fxfer(fname)\n   #file.edit(fname)\n   mypath <- pathtofile(fname)\n   file.edit(mypath)\n}\n\nmdist <- function(x,y,cx,cy){\n  distTmp <- matrix(NA,nrow=3,ncol=12)\n  distTmp[1,] <- (x-cx[1])^2 + (y-cy[1])^2\n  distTmp[2,] <- (x-cx[2])^2 + (y-cy[2])^2\n  distTmp[3,] <- (x-cx[3])^2 + (y-cy[3])^2  \n  return(distTmp)\n}\n\nshowMe <- function(cv){\n  myarg <- deparse(substitute(cv))\n  z<- outer( 1:20,1:20, \"+\")\n  obj<- list( x=1:20,y=1:20,z=z )\n  image(obj, col=cv, main=myarg  )\n}\n#my1999 <- pathtofile(\"RD_501_88101_1999-0.txt.gz\")\nmy1999 <- pathtofile(\"airData1999.txt.gz\")\n#my2012 <- pathtofile(\"RD_501_88101_2012-0.txt.gz\")\nmy2012 <- pathtofile(\"airData2012.txt.gz\")\ncnames <- \"# RD|Action Code|State Code|County Code|Site ID|Parameter|POC|Sample Duration|Unit|Method|Date|Start Time|Sample Value|Null Data Code|Sampling Frequency|Monitor Protocol (MP) ID|Qualifier - 1|Qualifier - 2|Qualifier - 3|Qualifier - 4|Qualifier - 5|Qualifier - 6|Qualifier - 7|Qualifier - 8|Qualifier - 9|Qualifier - 10|Alternate Method Detectable Limit|Uncertainty\"\npm0 <- read.table(my1999, comment.char = \"#\", header = FALSE, sep = \"|\", na.strings = \"\")\npm1 <- read.table(my2012, comment.char = \"#\", header = FALSE, sep = \"|\", na.strings = \"\")\nwcol <- c(3,4,5,11,13)\n#pm0 <- pm0[,wcol]\n#pm1 <- pm1[,wcol]\n" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Writing_swirl_Courses/Custom_Tests/initLesson.R", "content": "# Code placed in this file will be executed every time the\n# lesson is started. Any variables created here will show up in\n# the user's working directory and thus be accessible to them\n# throughout the lesson.\n\n# Source utilities.R\nsource(file.path(find.package(\"swirl\"), \"Courses\", \"Writing_swirl_Courses\", \"R\", \"utilities.R\"))\n\n# Display the customTests.R file.\ndisplay_swirl_file(\"customTests.R\", \"Writing_swirl_Courses\", \"Custom_Tests\")\n" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/Least_Squares_Estimation/demofile.R", "content": "file.edit(fname)" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Regression_Models/MultiVar_Examples3/interactplot.R", "content": "plot(hunger$Year,hunger$Numeric,pch=19)\npoints(hunger$Year,hunger$Numeric,pch=19,col=((hunger$Sex==\"Male\")*1+125))\nabline(c(lmInter$coeff[1],lmInter$coeff[2]),col=\"red\",lwd=3)\nabline(c(lmInter$coeff[1] + lmInter$coeff[3],lmInter$coeff[2] +lmInter$coeff[4]),col=\"blue\",lwd=3)" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "R_Programming/Functions/scripts/remainder.R", "content": "# Let me show you an example of a function I'm going to make up called\n# increment(). Most of the time I want to use this function to increase the\n# value of a number by one. This function will take two arguments: \"number\" and\n# \"by\" where \"number\" is the digit I want to increment and \"by\" is the amount I\n# want to increment \"number\" by. I've written the function below. \n#\n# increment <- function(number, by = 1){\n#     number + by\n# }\n#\n# If you take a look in between the parentheses you can see that I've set\n# \"by\" equal to 1. This means that the \"by\" argument will have the default\n# value of 1.\n#\n# I can now use the increment function without providing a value for \"by\": \n# increment(5) will evaluate to 6. \n#\n# However if I want to provide a value for the \"by\" argument I still can! The\n# expression: increment(5, 2) will evaluate to 7. \n# \n# You're going to write a function called \"remainder.\" remainder() will take\n# two arguments: \"num\" and \"divisor\" where \"num\" is divided by \"divisor\" and\n# the remainder is returned. Imagine that you usually want to know the remainder\n# when you divide by 2, so set the default value of \"divisor\" to 2. Please be\n# sure that \"num\" is the first argument and \"divisor\" is the second argument.\n#\n# Hint #1: You can use the modulus operator %% to find the remainder.\n#   Ex: 7 %% 4 evaluates to 3. \n#\n# Remember to set appropriate default values! Be sure to save this \n# script and type submit() in the console after you write the function.\n\nremainder <- function(num, divisor) {\n  # Write your code here!\n  # Remember: the last expression evaluated will be returned! \n}\n" }
{ "repo_name": "swirldev/swirl_courses", "ref": "refs/heads/master", "path": "Statistical_Inference/T_Confidence_Intervals/sleepPlot.R", "content": "g <- ggplot(sleep, aes(x = group, y = extra, group = factor(ID)))\ng <- g + geom_line(size = 1, aes(colour = ID)) + geom_point(size =10, pch = 21, fill = \"salmon\", alpha = .5)\nprint(g)" }
{ "repo_name": "mariodeng/FirebrowseR", "ref": "refs/heads/master", "path": "R/Samples.Clinical.R", "content": "#' Retrieve TCGA CDEs verbatim, i.e. not normalized by Firehose.\n#' \n#' This service returns patient clinical data from TCGA, verbatim. It differs from the Samples/Clinical_FH method by providing access to all TCGA CDEs in their original form, not merely the subset of CDEs normalized by Firehose for analyses.  Results may be selected by disease cohort, patient barcode or CDE name, but at least one cohort, barcode, or CDE must be provided. When filtering by CDE note that only when a patient record contains one or more of the selected CDEs will it be returned. Visit the Metadata/ClinicalNames api function to see the entire list of TCGA CDEs that may be queried via this method. For more information on how clinical data are processed, see our <a href=\"https://confluence.broadinstitute.org/display/GDAC/Documentation#Documentation-ClinicalPipeline\">pipeline documentation</a>.\n#'\n#' @param format Format of result. Default value is json. While json,tsv,csv are available. \n#' @param cohort Narrow search to one or more TCGA disease cohorts from the scrollable list. Multiple values are allowed ACC,BLCA,BRCA,CESC,CHOL,COAD,COADREAD,DLBC,ESCA,FPPP,GBM,GBMLGG,HNSC,KICH,KIPAN,KIRC,KIRP,LAML,LGG,LIHC,LUAD,LUSC,MESO,OV,PAAD,PCPG,PRAD,READ,SARC,SKCM,STAD,STES,TGCT,THCA,THYM,UCEC,UCS,UVM.\n#' @param tcga_participant_barcode Comma separated list of TCGA participant barcodes (e.g. TCGA-GF-A4EO). Multiple values are allowed .\n#' @param cde_name Retrieve results only for specified CDEs, per the Metadata/ClinicalNames function Multiple values are allowed .\n#' @param page Which page (slice) of entire results set should be returned.  Multiple values are allowed . Default value is 1.  \n#' @param page_size Number of records per page of results.  Max is 2000. Multiple values are allowed . Default value is 150.  \n#' @param sort_by Which column in the results should be used for sorting paginated results? Default value is cohort. While tcga_participant_barcode,cohort,cde_name are available. \n#' \n#' @export\nSamples.Clinical = function(format = \"json\",\n                             cohort = \"\",\n                             tcga_participant_barcode = \"\",\n                             cde_name = \"\",\n                             page = \"1\",\n                             page_size = \"150\",\n                             sort_by = \"cohort\"\n                             ){\n                             \n  parameters = list(format = format,\n                    cohort = cohort,\n                    tcga_participant_barcode = tcga_participant_barcode,\n                    cde_name = cde_name,\n                    page = page,\n                    page_size = page_size,\n                    sort_by = sort_by)\n  to.Validate = c(\"cohort\",\"tcga_participant_barcode\",\"cde_name\")\n  validate.Parameters(params = parameters, to.Validate = to.Validate)\n\n  url = build.Query(parameters = parameters,\n                    invoker = \"Samples\",\n                    method = \"Clinical\")\n  ret = download.Data(url, format, page)\n\n  return(ret)\n\n}\n" }
{ "repo_name": "mariodeng/FirebrowseR", "ref": "refs/heads/master", "path": "R/Metadata.SampleTypes.R", "content": "#' Return all TCGA sample type codes, both numeric and symbolic.\n#' \n#' \n#'\n#' @param format Format of result. Default value is json. While json,tsv,csv are available. \n#' \n#' @export\nMetadata.SampleTypes = function(format = \"json\"\n                             ){\n                             \n  parameters = list(format = format)\n  \n  validate.Parameters(params = parameters)\n\n  url = build.Query(parameters = parameters,\n                    invoker = \"Metadata\",\n                    method = \"SampleTypes\")\n  ret = download.Data(url, format)\n\n  return(ret)\n\n}\n" }
{ "repo_name": "mariodeng/FirebrowseR", "ref": "refs/heads/master", "path": "tests/testthat/test.Samples.miRSeq.R", "content": "#library(FirebrowseR)\ncontext(\"Samples.miRSeq\")\n\ntest_that(\"miRSeq data is retrieved correctly\", {\n\n  format = \"json\"\n  mir = c(\"hsa-mir-1285-3p\",\"hsa-mir-125a-5p\",\"hsa-mir-221-3p\",\"hsa-mir-10b-5p\",\"hsa-mir-608\",\"hsa-mir-324-5p\")\n  cohort = \"BRCA\"\n  tcga_participant_barcode = \"\"\n  tool = \"miRseq_Mature_Preprocess\"\n  sample_type = \"NT\"\n  page = 1\n  page_size = 250\n  sort_by = \"mir\"\n\n  obj = Samples.miRSeq(format = format,\n                          mir = mir,\n                          cohort = cohort,\n                          tcga_participant_barcode = tcga_participant_barcode,\n                          tool = tool,\n                          sample_type = sample_type,\n                          page = page,\n                          page_size = page_size,\n                          sort_by = sort_by)\n  test.q = \"http://firebrowse.org/api/v1/Samples/miRSeq?format=csv&mir=hsa-mir-1285-3p%2Chsa-mir-125a-5p%2Chsa-mir-221-3p%2Chsa-mir-10b-5p%2Chsa-mir-608%2Chsa-mir-324-5p&cohort=BRCA&tool=miRseq_Mature_Preprocess&sample_type=NT&page=1&page_size=250&sort_by=cohort\"\n  test.obj = read.table(test.q, header = T, sep = \",\", quote = \"\\\"\")\n  expect_is(obj, \"list\")\n  expect_equal(length(obj[[1]]), nrow(test.obj))\n  expect_equal(length(obj[[1]][[1]]), ncol(test.obj))\n  \n\n  format = \"csv\"\n  mir = c(\"hsa-mir-1285-3p\", \"hsa-mir-125a-5p\")\n  obj = Samples.miRSeq(format = format,\n                          mir = mir,\n                          cohort = cohort,\n                          tcga_participant_barcode = tcga_participant_barcode,\n                          tool = tool,\n                          sample_type = sample_type,\n                          page = page,\n                          page_size = page_size,\n                          sort_by = sort_by)\n  test.q = \"http://firebrowse.org/api/v1/Samples/miRSeq?format=csv&mir=hsa-mir-1285-3p%2Chsa-mir-125a-5p&cohort=BRCA&tool=miRseq_Mature_Preprocess&sample_type=NT&page=1&page_size=250&sort_by=cohort\"\n  test.obj = read.table(test.q, header = T, sep = \",\", quote = \"\\\"\")\n  expect_equal(nrow(obj), nrow(test.obj))\n  expect_equal(ncol(obj), ncol(test.obj))\n\n})\n" }
{ "repo_name": "mariodeng/FirebrowseR", "ref": "refs/heads/master", "path": "R/Metadata.Counts.R", "content": "#' Retrieve sample counts.\n#' \n#' Returns the aliquot counts for each disease cohort, per sample type and data type.  The sample type designation of \"Tumor\" may be used to aggregate the count of all tumor aliquots into a single number per disease and data type. See the SampleTypes function for a complete description of sample types.\n#'\n#' @param format Format of result. Default value is json. While json,tsv,csv are available. \n#' @param date Select one or more date stamps. Multiple values are allowed 2016_01_28,2015_11_01,2015_08_21,2015_06_01,2015_04_02,2015_02_04,2014_12_06,2014_10_17,2014_09_02,2014_07_15,2014_05_18,2014_04_16,2014_03_16. Default value is 2016_01_28.  \n#' @param cohort Narrow search to one or more TCGA disease cohorts from the scrollable list. Multiple values are allowed ACC,BLCA,BRCA,CESC,CHOL,COAD,COADREAD,DLBC,ESCA,FPPP,GBM,GBMLGG,HNSC,KICH,KIPAN,KIRC,KIRP,LAML,LGG,LIHC,LUAD,LUSC,MESO,OV,PAAD,PCPG,PRAD,READ,SARC,SKCM,STAD,STES,TGCT,THCA,THYM,UCEC,UCS,UVM.\n#' @param sample_type Narrow search to one or more TCGA sample types from the scrollable list. Multiple values are allowed FFPE,NB,NBC,NBM,NT,TAM,TAP,TB,TM,TP,TR,Tumor.\n#' @param data_type Narrow search to one or more TCGA data types from the scrollable list. Multiple values are allowed bcr,clinical,cn,lowp,methylation,mrna,mrnaseq,mir,mirseq,rppa,maf,rawmaf.\n#' @param totals Output an entry providing the totals for each data type. Default value is TRUE. While  are available. \n#' @param sort_by Which column in the results should be used for sorting paginated results? Default value is cohort. While cohort are available. \n#' \n#' @export\nMetadata.Counts = function(format = \"json\",\n                             date = \"2016_01_28\",\n                             cohort = \"\",\n                             sample_type = \"\",\n                             data_type = \"\",\n                             totals = \"TRUE\",\n                             sort_by = \"cohort\"\n                             ){\n                             \n  parameters = list(format = format,\n                    date = date,\n                    cohort = cohort,\n                    sample_type = sample_type,\n                    data_type = data_type,\n                    totals = totals,\n                    sort_by = sort_by)\n  \n  validate.Parameters(params = parameters)\n\n  url = build.Query(parameters = parameters,\n                    invoker = \"Metadata\",\n                    method = \"Counts\")\n  ret = download.Data(url, format)\n\n  return(ret)\n\n}\n" }
{ "repo_name": "mariodeng/FirebrowseR", "ref": "refs/heads/master", "path": "R/Metadata.Platforms.R", "content": "#' Translate TCGA platform codes to full platform names.\n#' \n#' By default this function returns a table of all of the technology platforms used to sequence or characterize samples in TCGA--both their short platform codes and full names.  A subset of this table may be obtained by explicitly specifying one or more platform codes.\n#'\n#' @param format Format of result. Default value is json. While json,tsv,csv are available. \n#' @param platform Narrow search to one or more TCGA data generation platforms from the scrollable list. Multiple values are allowed 454,ABI,AgilentG4502A_07,AgilentG4502A_07_1,AgilentG4502A_07_2,AgilentG4502A_07_3,bio,biotab,CGH-1x1M_G4447A,diagnostic_images,fh_analyses,fh_reports,fh_stddata,Genome_Wide_SNP_6,GenomeWideSNP_5,H-miRNA_8x15K,H-miRNA_8x15Kv2,H-miRNA_EarlyAccess,H-miRNA_G4470A,HG-CGH-244A,HG-CGH-415K_G4124A,HG-U133_Plus_2,HG-U133A_2,HT_HG-U133A,HuEx-1_0-st-v2,Human1MDuo,HumanHap550,HumanMethylation27,HumanMethylation450,IlluminaDNAMethylation_OMA002_CPI,IlluminaDNAMethylation_OMA003_CPI,IlluminaGA_DNASeq,IlluminaGA_DNASeq_automated,IlluminaGA_DNASeq_Cont,IlluminaGA_DNASeq_Cont_automated,IlluminaGA_DNASeq_Cont_curated,IlluminaGA_DNASeq_curated,IlluminaGA_miRNASeq,IlluminaGA_mRNA_DGE,IlluminaGA_RNASeq,IlluminaGA_RNASeqV2,IlluminaGG,IlluminaHiSeq_DNASeq,IlluminaHiSeq_DNASeq_automated,IlluminaHiSeq_DNASeq_Cont,IlluminaHiSeq_DNASeq_Cont_automated,IlluminaHiSeq_DNASeq_Cont_curated,IlluminaHiSeq_DNASeq_curated,IlluminaHiSeq_DNASeqC,IlluminaHiSeq_miRNASeq,IlluminaHiSeq_mRNA_DGE,IlluminaHiSeq_RNASeq,IlluminaHiSeq_RNASeqV2,IlluminaHiSeq_TotalRNASeqV2,IlluminaHiSeq_WGBS,Mapping250K_Nsp,Mapping250K_Sty,MDA_RPPA_Core,microsat_i,minbio,minbiotab,Mixed_DNASeq,Mixed_DNASeq_automated,Mixed_DNASeq_Cont,Mixed_DNASeq_Cont_automated,Mixed_DNASeq_Cont_curated,Mixed_DNASeq_curated,pathology_reports,SOLiD_DNASeq,SOLiD_DNASeq_automated,SOLiD_DNASeq_Cont,SOLiD_DNASeq_Cont_automated,SOLiD_DNASeq_Cont_curated,SOLiD_DNASeq_curated,tissue_images,WHG-1x44K_G4112A,WHG-4x44K_G4112F,WHG-CGH_4x44B.\n#' \n#' @export\nMetadata.Platforms = function(format = \"json\",\n                             platform = \"\"\n                             ){\n                             \n  parameters = list(format = format,\n                    platform = platform)\n  \n  validate.Parameters(params = parameters)\n\n  url = build.Query(parameters = parameters,\n                    invoker = \"Metadata\",\n                    method = \"Platforms\")\n  ret = download.Data(url, format)\n\n  return(ret)\n\n}\n" }
{ "repo_name": "alexholmes/hiped2", "ref": "refs/heads/master", "path": "src/main/R/wordcount_rhipe.R", "content": "#! /usr/bin/env Rscript\nlibrary(Rhipe)\n\nrhinit(TRUE,TRUE)\n\nm <- expression({\n  for(x in map.values){\n    y <- strsplit(x,\" +\")[[1]]\n    for(w in y) rhcollect(w,T)\n  }\n})\n\nz <- rhmr(map=m,inout=c(\"text\",\"sequence\"),\n    ifolder=\"stocks.txt\",ofolder='/output',mapred=list(mapred.reduce.tasks=5))\n\nrhex(z)" }
{ "repo_name": "hredestig/pcaMethods", "ref": "refs/heads/master", "path": "R/llsImpute.R", "content": "##' Missing value estimation using local least squares (LLS).  First,\n##' k variables (for Microarrya data usually the genes)  are selected\n##' by pearson, spearman or kendall correlation coefficients.  Then\n##' missing values are imputed by a linear combination of the k\n##' selected variables. The optimal combination is found by LLS\n##' regression.  The method was first described by Kim et al,\n##' Bioinformatics, 21(2),2005.\n##'\n##' Missing values are denoted as \\code{NA}\\cr It is not recommended\n##' to use this function directely but rather to use the nni() wrapper\n##' function. The methods provides two ways for missing value\n##' estimation, selected by the \\code{allVariables} option. The first\n##' one is to use only complete variables for the  regression. This is\n##' preferable when the number of incomplete variables is relatively\n##' small.\n##' \n##' The second way is to consider all variables as candidates for the\n##' regression.  Hereby missing values are initially replaced by the\n##' columns wise mean.  The method then iterates, using the current\n##' estimate as input for the regression until the change between new\n##' and old estimate falls below a threshold (0.001).\n##' \n##' @title LLSimpute algorithm\n##' @param Matrix \\code{matrix} -- Data containing the variables\n##' (genes) in columns and observations (samples) in rows. The data\n##' may contain missing values, denoted as \\code{NA}.\n##' @param k \\code{numeric} -- Cluster size, this is the number of\n##' similar genes used for regression.\n##' @param center \\code{boolean} -- Mean center the data if TRUE\n##' @param completeObs \\code{boolean} -- Return the estimated complete\n##' observations if  TRUE. This is the input data with NA values\n##' replaced by the estimated values.\n##' @param correlation \\code{character} -- How to calculate the\n##' distance between genes.  One out of pearson | kendall | spearman ,\n##' see also help(\"cor\").\n##' @param allVariables \\code{boolean} -- Use only complete genes to\n##' do the regression if TRUE, all genes if FALSE.\n##' @param maxSteps \\code{numeric} -- Maximum number of iteration\n##' steps if allGenes = TRUE.\n##' @param xval \\code{numeric} Use LLSimpute for cross\n##' validation. xval is the index of the gene to estimate, all other\n##' incomplete genes will be ignored if this parameter is set. We do\n##' not consider them in the cross-validation.\n##' @param verbose \\code{boolean} -- Print step number and relative\n##' change if TRUE and  allVariables = TRUE\n##' @param ... Reserved for parameters used in future version of the\n##' algorithm\n##' @note Each step the generalized inverse of a \\code{miss} x k\n##' matrix is calculated. Where \\code{miss} is the number of missing\n##' values in  variable j and \\code{k} the number of neighbours. This\n##' may be slow for large values of k and / or many missing\n##' values. See also help(\"ginv\").\n##' @return   \\item{nniRes}{Standard nni (nearest neighbour\n##' imputation) result object of this package. See\n##' \\code{\\link{nniRes}} for details.}\n##' @seealso \\code{\\link{pca}, \\link{nniRes}, \\link{nni}}.\n##' @examples\n##' ## Load a sample metabolite dataset (metaboliteData) with already 5\\% of\n##' ## data missing\n##' data(metaboliteData)\n##' ## Perform llsImpute using k = 10\n##' ## Set allVariables TRUE because there are very few complete variables\n##' result <- llsImpute(metaboliteData, k = 10, correlation=\"pearson\", allVariables=TRUE)\n##' ## Get the estimated complete observations\n##' cObs <- completeObs(result)\n##' @keywords multivariate\n##' @export\n##' @references Kim, H. and Golub, G.H. and Park, H.  - Missing value\n##' estimation for DNA microarray gene expression data: local least\n##' squares imputation.  \\emph{Bioinformatics, 2005; 21(2):187-198.}\n##' \n##' Troyanskaya O. and Cantor M. and Sherlock G. and Brown P. and\n##' Hastie T. and Tibshirani R. and Botstein D. and Altman RB.  -\n##' Missing value estimation methods for DNA microarrays.\n##' \\emph{Bioinformatics. 2001 Jun;17(6):520-525.}\n##' @author Wolfram Stacklies\nllsImpute <- function(Matrix, k=10, center=FALSE, completeObs=TRUE,\n                      correlation=\"pearson\", \n                      allVariables=FALSE, maxSteps=100, xval=NULL,\n                      verbose=FALSE, ...) {\n\n    threshold <- 0.001\n\n    correlation <- match.arg(correlation, c(\"pearson\", \"kendall\", \"spearman\"))\n\n    ## If the data is a data frame, convert it into a matrix\n    Matrix <- as.matrix(Matrix, rownames.force=TRUE)\n    ## And now check if everything is right...\n    if ( !checkData(Matrix, verbose = interactive()) ) {\n        stop(\"Invalid data format! Use checkData(Matrix, verbose = TRUE) for details.\\n\")\n    }\n\n    ## Exit if number of neighbours exceeds number of columns\n    if (k > ncol(Matrix))\n        stop(\"Cluster size larger than the number of columns, choose a k < ncol(Matrix)!\")\n \n    ## Set allVariables TRUE if k exceeds number of complete genes\n    ## Print warning messages in the first case and when less than 50% of all genes are complete\n    ## and allVariables == FALSE\n    cg <- sum( apply(is.na(Matrix), 2, sum) == 0)\n    if ( (k > cg) && (!allVariables) ) {\n        warning(\"Cluster size larger than number of complete genes, using allVariables = TRUE\")\n        allVariables <- TRUE\n    } else if ( (cg < (ncol(Matrix) / 2)) && (!allVariables) ) {\n        warning(\"Less than 50% of the genes are complete, consider using allVariables = TRUE\")\n    } else if (sum(is.na(Matrix)) == 0)\n        stop(\"No missing values, no need for missing value imputation :))\")\n\n    ## Find all genes with missing values\n    missing <- apply(is.na(Matrix), 2, sum) > 0\n    missIx <- which(missing == TRUE)\n    # For cross validation we want to only estimate one variable, the others\n    # are not considered in the cross validation anyway\n    if (!is.null(xval))\n        missIx = xval\n    obs <- Matrix    ## working copy of the data\n    Ye <- Matrix     ## Estimated complete observations\n\n    ## Center the data column wise\n    if (center) {\n        obs   <- scale(Matrix, center = TRUE, scale = FALSE)\n        Ye    <- obs\n        means <- attr(Ye, \"scaled:center\")\n    }\n\n    if (allVariables) {\n        compIx <- 1:ncol(obs)\n        ## Impute the row average\n        rowMeans <- apply(obs, 1, mean, na.rm = TRUE)\n        for (i in 1:nrow(obs)) {\n            obs[i, is.na(Matrix[i,])] <- rowMeans[i]\n        }\n        ## distances between all genes, ignore the diagonal (correlation to itself)\n        distance = abs(cor(obs, obs, method = correlation))\n    } else {\n        compIx <- which(missing == FALSE)\n        ## missing genes are the rows, complete genes the columns\n        distance = abs(cor(obs[,missIx, drop=FALSE], obs[,compIx, drop=FALSE], use=\"pairwise.complete.obs\",\n                       method = correlation))\n    }\n\n    change <- Inf\n    step <- 0\n    while ( (change > threshold) && (step < maxSteps) ) {\n        step <- step + 1\n        iteration <- 0\n        \n        ## Do the regression and imputation\n        for (index in missIx) {\n            iteration <- iteration + 1\n\t    if (allVariables) {\n                similar <- sort(distance[iteration,], index.return = TRUE, decreasing = TRUE)\n                simIx <- compIx[ similar$ix[similar$ix != iteration][1:k] ]\n            } else {\n                similar <- sort(distance[iteration,], index.return = TRUE, decreasing = TRUE)\n                simIx <- compIx[ similar$ix[1:k] ]\n            }\n\n            ##\n            ## Do a regression against the k most similar genes\n            ## See Kim et. al 2005 for details\n            ##\n            target <- obs[, index, drop = FALSE]\n            tMiss <- is.na(Matrix[, index, drop = FALSE])\n\n            Apart <- obs[!tMiss, simIx, drop = FALSE]\n            Bpart <- obs[tMiss, simIx, drop = FALSE]\n            targetComplete <- target[!tMiss, , drop = FALSE]\n            X <- MASS::ginv(Apart) %*% targetComplete\n            estimate <- Bpart %*% X\n\n            ## Impute the estimate\n            Ye[tMiss, index] <- estimate\n        }\n\n        ## We do not want to iterate if allVariables == FALSE\n        if (!allVariables || !is.null(xval)) {\n            break\n        } else {\n            ## relative change in estimation\n            change <- sqrt(sum( (obs - Ye)^2 ) / sum(obs^2))\n            obs <- Ye\n            if (verbose) {\n                cat(\"Step number     : \", step, '\\n')\n                cat(\"Relative change : \", change, '\\n')\n                cat(\"---------------\", '\\n')\n            }\n        }\n    }\n\n    ## Add the original mean\n    if (center) {\n        for(i in 1:ncol(Ye)) {\n            Ye[,i] <- Ye[,i] + means[i]\n        }\n    }\n\n    ## Build the nniRes object\n    ##\n    result <- new(\"nniRes\")\n\n    if(completeObs) {\n        Ye[!is.na(Matrix)] <- Matrix[!is.na(Matrix)]\n        result@completeObs <- Ye\n    }\n    result@centered        <- center\n    result@center          <- attr(scale(Matrix, center = TRUE, scale = FALSE), \"scaled:center\")\n    result@nObs            <- nrow(Matrix)\n    result@nVar            <- ncol(Matrix)\n    result@method          <- \"llsImpute\"\n    result@correlation     <- correlation\n    result@k               <- k\n    result@missing         <- sum(is.na(Matrix))\n\n    return(result)        \n} \n\n" }
{ "repo_name": "anqif/cvxr", "ref": "refs/heads/master", "path": "tests/testthat.R", "content": "library(testthat)\nlibrary(cvxr)\n\ntest_check(\"cvxr\")\n" }
{ "repo_name": "anqif/cvxr", "ref": "refs/heads/master", "path": "tests/testthat/test_convolution.R", "content": "test_that(\"test 1D convolution\", {\n  n <- 3\n  x <- Variable(n)\n  f <- as.matrix(c(1, 2, 3))\n  g <- as.matrix(c(0, 1, 0.5))\n  f_conv_g <- as.matrix(c(0, 1, 2.5, 4, 1.5))\n  expr <- Conv(f, g)\n  expect_true(is_constant(expr))\n  expect_equal(size(expr), c(5, 1))\n  \n  expr <- Conv(f, x)\n  expect_true(is_affine(expr))\n  expect_equal(size(expr), c(5, 1))\n  \n  t <- Variable()\n  prob <- Problem(Minimize(Pnorm(expr, 1)), list(x == g))\n})\n\ntest_that(\"test a problem with convolution\", {\n  N <- 5\n  y <- matrix(rnorm(N), nrow = N, ncol = 1)\n  h <- matrix(rnorm(2), nrow = 2, ncol = 1)\n  x <- Variable(N)\n  v <- Conv(h, x)\n  # obj <- Minimize(SumEntries(MulElemwise(y, v[1:N])))\n})\n" }
{ "repo_name": "philchalmers/mirtCAT", "ref": "refs/heads/master", "path": "R/PersonClass.R", "content": "Person <- setRefClass(\"Person\", \n                      \n                      fields = list(raw_responses = 'character',\n                                    responses = 'integer',\n                                    items_answered = 'integer',\n                                    thetas = 'matrix',\n                                    thetas_history = 'matrix',\n                                    thetas_SE_history = 'matrix',\n                                    info_thetas = 'matrix',\n                                    demographics = 'data.frame',\n                                    item_time = 'numeric',\n                                    valid_item = 'logical',\n                                    score = 'logical'),\n                      \n                      methods = list(\n                         initialize = function(nfact, nitems, thetas.start_in, score,\n                                               theta_SEs){\n                             'Initialize the person object given background information'\n                             raw_responses <<- as.character(rep(NA, nitems))\n                             responses <<- as.integer(rep(NA, nitems))\n                             valid_item <<- rep(TRUE, nitems)\n                             items_answered <<- as.integer(rep(NA, nitems))\n                             thetas <<- matrix(numeric(nfact), nrow=1L)\n                             thetas_SE_history <<- matrix(theta_SEs, 1L)\n                             score <<- score\n                             item_time <<- numeric(nitems)\n                             if(!is.null(thetas.start_in))\n                                thetas <<- matrix(thetas.start_in, nrow=1L)\n                             thetas_history <<- matrix(thetas, 1L, nfact)\n                             info_thetas <<- matrix(0, nfact, nfact)\n                         })\n                      \n)\n\nPerson$methods(\n    \n    # Update thetas\n    Update.thetas = function(design, test){\n        'Update the latent trait (theta) values using information \n        from the design and test objects'\n        responses2 <- responses\n        responses2[design@items_not_scored] <- NA\n        if(score){\n            method <- design@method\n            if(last_item(items_answered) %in% design@items_not_scored)\n                method <- 'fixed'\n            if(method == 'ML'){\n                if(length(unique(na.omit(responses2))) < 2L) method <- 'MAP'\n            }\n            if(method != 'fixed'){\n                suppressWarnings(tmp <- fscores(test@mo, method=method, response.pattern=responses2,\n                                   theta_lim=test@fscores_args$theta_lim,\n                                   MI = test@fscores_args$MI, quadpts = test@quadpts, \n                                   mean = test@fscores_args$mean, cov = test@fscores_args$cov,\n                                   QMC=test@fscores_args$QMC, custom_den=test@fscores_args$custom_den))\n                thetas <<- tmp[,paste0('F', 1L:test@nfact), drop=FALSE]\n                thetas_SE_history <<- rbind(thetas_SE_history, \n                                            tmp[,paste0('SE_F', 1L:test@nfact), drop=FALSE])\n            } else {\n                thetas_SE_history <<- rbind(thetas_SE_history, \n                                            thetas_SE_history[nrow(thetas_SE_history),])\n            }\n            thetas_history <<- rbind(thetas_history, thetas)\n            set <- c('Drule', 'Trule', 'Erule', 'Wrule', 'Arule', 'APrule',\n                     'DPrule', 'TPrule', 'EPrule', 'WPrule')\n            if(test@nfact > 1L && design@criteria %in% set){\n                pick <- which(!is.na(responses2))\n                infos <- lapply(pick, function(x, thetas)\n                    FI(extract.item(test@mo, x), Theta=thetas), thetas=thetas)\n                tmp <- matrix(0, nrow(infos[[1L]]), ncol(infos[[1L]]))\n                for(i in 1L:length(infos))\n                    tmp <- tmp + infos[[i]]\n                if(design@criteria %in% c('DPrule', 'TPrule', 'EPrule', 'WPrule', 'APrule'))\n                    tmp <- tmp + solve(test@gp$gcov)\n                info_thetas <<- tmp\n            }\n        }\n    }\n)\n" }
{ "repo_name": "philchalmers/mirtCAT", "ref": "refs/heads/master", "path": "R/ShinyGUIClass.R", "content": "ShinyGUI <- setRefClass(\"ShinyGUI\", \n                      \n                      fields = list(title = 'character',\n                                    author = 'character',\n                                    questions = 'list',\n                                    df = 'list',\n                                    firstpage = 'list',\n                                    demographics = 'list',\n                                    lastpage = 'function',\n                                    instructions = 'character',\n                                    begin_message = 'character',\n                                    stem_locations = 'character',\n                                    demographic_inputIDs = 'character',\n                                    temp_file = 'character',\n                                    width = 'numeric',\n                                    height = 'numeric',\n                                    forced_choice = 'logical',\n                                    css = 'character',\n                                    stopApp = 'logical',\n                                    ui = 'function'),\n                      \n                      methods = list(\n                          initialize = function(questions, df, shinyGUI){\n                              'Initialize the shiny GUI given questions, df, and shinyGUI list'\n                              ui <<- default_UI\n                              questions <<- questions\n                              df <<- df\n                              forced_choice <<- TRUE\n                              stopApp <<- TRUE\n                              if(is.null(shinyGUI$stem_locations)){\n                                  stem_locations <<- as.character(rep(NA, length(questions)))\n                              } else {\n                                  stem_locations <<- as.character(sapply(shinyGUI$stem_locations, \n                                    function(x){                                        \n                                        ret <- if(!is.na(x)){\n                                            org <- x\n                                            exsts <- file.exists(x)\n                                            if(!exsts){\n                                                x <- paste0(getwd(), '/', x)\n                                                exsts <- file.exists(x)\n                                            }\n                                            if(!exsts) \n                                                stop(sprintf('The following file cannot be located: %s', org), call.=FALSE)\n                                            normalizePath(x, mustWork = TRUE)\n                                        } else NA\n                                        return(ret)\n                                  }))\n                              }\n                              title <<- 'mirtCAT'\n                              author <<- 'Author information'\n                              instructions <<- c(\"Instructions:\",\n                                                 \"To progress through the interface, click on the action button below.\",\n                                                 \"Next\")\n                              demographic_inputIDs <<- character(0)\n                              begin_message <<- \"Click the action button to begin.\"\n                              firstpage <<- list(h1('Welcome to the mirtCAT interface'),\n                                                 'The following interface was created using the mirtCAT package. \n                                                 To cite the package use citation(\\'mirtCAT\\') in R.')\n                              demographics <<- list()\n                              lastpage <<- function(person) \n                                            return(list(h5(\"You have successfully completed the interface.\n                                                   Click the action button to terminate the application.\")))\n                              if(!is.null(shinyGUI$stopApp) && !shinyGUI$stopApp)\n                                  lastpage <<- function(person) \n                                      return(list(h5(\"You have successfully completed the interface.\n                                                   Please close the tab/web browser to terminate the application.\")))\n                              temp_file <<- ''\n                              css <<- ''\n                                                 \n                              if(length(shinyGUI)){\n                                  dnames <- names(shinyGUI)\n                                  gnames <- c('title', 'authors', 'instructions', 'firstpage', 'demographics',\n                                              'demographics_inputIDs', 'max_time', 'temp_file', \n                                              'lastpage', 'css', 'stem_dims', 'forced_choice', 'stem_locations',\n                                              'begin_message', 'stopApp', 'ui')\n                                  if(!all(dnames %in% gnames))\n                                      stop('The following inputs to shinyGUI are invalid: ',\n                                           paste0(dnames[!(dnames %in% gnames)], ' '), call.=FALSE)\n                                  if(!is.null(shinyGUI$ui))\n                                      ui <<- shinyGUI$ui\n                                  if(!is.null(shinyGUI$instructions))\n                                      instructions <<- shinyGUI$instructions\n                                  if(!is.null(shinyGUI$begin_message))\n                                      begin_message <<- shinyGUI$begin_message\n                                  if(!is.null(shinyGUI$title))\n                                      title <<- shinyGUI$title\n                                  if(!is.null(shinyGUI$authors))\n                                      author <<- shinyGUI$authors\n                                  if(!is.null(shinyGUI$firstpage)) \n                                      firstpage <<- shinyGUI$firstpage\n                                  if(!is.null(shinyGUI$demographics)){\n                                      demographics <<- shinyGUI$demographics\n                                      demographic_inputIDs <<- shinyGUI$demographics_inputIDs\n                                  }\n                                  if(!is.null(shinyGUI$forced_choice))\n                                      forced_choice <<- shinyGUI$forced_choice\n                                  if(!is.null(shinyGUI$stopApp))\n                                      stopApp <<- shinyGUI$stopApp\n                                  if(!is.null(shinyGUI$lastpage)) \n                                      lastpage <<- shinyGUI$lastpage\n                                  if(!is.null(shinyGUI$temp_file))\n                                      temp_file <<- shinyGUI$temp_file\n                                  if(!is.null(shinyGUI$css))\n                                      css <<- shinyGUI$css\n                              }\n                          })\n                      \n)" }
{ "repo_name": "Netflix/Surus", "ref": "refs/heads/master", "path": "resources/R/RAD/R/anomaly_detection.R", "content": "#' Time Series Anomaly Detection\n#' \n#' Fast C++ implementation of time series anomaly detection using Robust Principal Component Pursuit\n#' @param X a vector representing a time series, or a data frame where columns are time series.\n#' The length of this vector should be divisible by frequency.\n#' If X is a vector it will be cast to a matrix of dimension frequency by length(X)/frequency\n#' @param frequency the frequency of the seasonality of X\n#' @param dates optional vector of dates to be used as a time index in the output\n#' @param autodiff boolean. If true, use the Augmented Dickey Fuller Test to determine\n#' if differencing is needed to make X stationary\n#' @param forcediff boolean. If true, always compute differences\n#' @param scale boolean. If true normalize the time series to zero mean and unit variance\n#' @param L.penalty a scalar for the amount of thresholding in determining the low rank approximation for X.\n#' The default values are chosen to correspond to the smart thresholding values described in Candes'\n#' Stable Principal Component Pursuit\n#' @param s.penalty a scalar for the amount of thresholding in determining the separation between noise and sparse outliers\n#' The default values are chosen to correspond to the smart thresholding values described in Zhou's\n#' Stable Principal Component Pursuit\n#' @param verbose boolean. If true print status updates while running optimization program\n#' @useDynLib RAD\n#' @importFrom tseries adf.test\n#' @details Robust Principal Component Pursuit is a matrix decomposition algorithm that seeks\n#' to separate a matrix X into the sum of three parts X = L + S + E. L is a low rank matrix representing\n#' a smooth X, S is a sparse matrix containing corrupted data, and E is noise. To convert a time series\n#' into the matrix X we take advantage of seasonality so that each column represents one full period, for\n#' example for weekly seasonality each row is a day of week and one column is one full week.\n#' \n#' While computing the low rank matrix L we take an SVD of X and soft threshold the singular values.\n#' This approach allows us to dampen all anomalies across the board simultaneously making the method\n#' robust to multiple anomalies. Most techniques such as time series regression and moving averages\n#' are not robust when there are two or more anomalies present.\n#' \n#' Empirical tests show that identifying anomalies is easier if X is stationary.\n#' The Augmented Dickey Fuller Test is used to test for stationarity - if X is not stationary\n#' then the time series is differenced before calling RPCP. While this test is abstracted away\n#' from the user differencing can be forced by setting the forcediff parameter.\n#' \n#' The thresholding values can be tuned for different applications, however we strongly\n#' recommend using the defaults which were proposed by Zhou.\n#' For more details on the choice of L.penalty and s.penalty\n#' please refer to Zhou's 2010 paper on Stable Principal Component Pursuit.\n#' \n#' The implementation of RPCP is done in C++ for high performance through RCpp.\n#' This function simply preprocesses the time series and calls RcppRPCP. \n#' @return \n#' \\itemize{\n#'   \\item X_transform. The transformation applied to the time series,\n#'   can be the identity or could be differencing\n#'   \\item L_transform. The low rank component in the transformed space\n#'   \\item S_transform. The sparse outliers in the transformed space\n#'   \\item E_transform. The noise in the transformed space\n#'   \\item X_original. The original time series\n#'   \\item time. The time index\n#'   \\item name. The name of the time series if X was a named data frame\n#' }\n#' @references\n#' The following are recommended educational material:\n#' \\itemize{\n#'   \\item Candes' paper on RPCP \\url{http://statweb.stanford.edu/~candes/papers/RobustPCA.pdf}\n#'   \\item Zhou's follow up paper on Stable PCP \\url{http://arxiv.org/abs/1001.2363}\n#'   \\item Metamarkets Tech Blog on anomalies in time \\url{https://metamarkets.com/2012/algorithmic-trendspotting-the-meaning-of-interesting/}\n#' }\n#' @export\n#' @examples\n#' frequency = 7\n#' numPeriods = 10\n#' ts.sinusoidal = sin((2 * pi / frequency ) * 1:(numPeriods * frequency))\n#' ts = ts.sinusoidal\n#' ts = sin((2 * pi / frequency ) * 1:(numPeriods * frequency))\n#' ts[58:60] = 100\n#' ggplot_AnomalyDetection.rpca(AnomalyDetection.rpca(ts)) + ggplot2::theme_grey(base_size = 25)\nAnomalyDetection.rpca = function(X, frequency=7, dates=NULL,\n                                 autodiff = T,\n                                 forcediff = F,\n                                 scale = T,\n                                 L.penalty = 1,\n                                 s.penalty=1.4 / sqrt(max(frequency, ifelse(is.data.frame(X), nrow(X), length(X)) / frequency)),\n                                 verbose=F) {\n  if (is.vector(X) & !is.data.frame(X)) X = data.frame(y=X)\n  time = if (is.null(dates)) 1:nrow(X) else dates\n  \n  #look through columns which are separate time series\n  #transform each column vector into a matrix with nrow = observations per period\n  #the number of columns will be equal to the number of periods\n  rpca.ts = apply(X, 2, function(j) {\n    j.init = j[1]\n    useddiff = F\n    if (forcediff) {\n      useddiff = T\n      j = c(0, diff(j))\n    }\n    else if (autodiff) {\n      adf = suppressWarnings(tseries::adf.test(j))\n      if (adf$p.value > .05) {useddiff = T; j = c(0, diff(j))}\n    } \n    \n    if (scale) {\n      j.global.mean = mean(j)\n      j.global.sd = sd(j)\n      j.matrix.standard.global = matrix((j - j.global.mean) / j.global.sd, nrow = frequency)\n      j.matrix = j.matrix.standard.global  \n    } else {\n      j.global.mean = 0\n      j.global.sd = 1\n      j.matrix = matrix(j, nrow = frequency)\n    }\n    \n    list(rpca = RcppRPCA(j.matrix, \n                         Lpenalty = L.penalty, Spenalty = s.penalty, \n                         verbose=verbose),\n         mean = j.global.mean,\n         sd = j.global.sd,\n         diff = useddiff,\n         j.init = j.init\n    )\n  })\n  rpca.ts.stacked = lapply(rpca.ts, function(i) {\n    if (i$diff) {\n      X.orig = c(i$j.init + cumsum((as.vector(i$rpca$X)) * i$sd + i$mean))\n      X.transform = (as.vector(i$rpca$X)) * i$sd + i$mean\n      L.transform = (as.vector(i$rpca$L)) * i$sd + i$mean\n      S.transform = (as.vector(i$rpca$S)) * i$sd\n      E.transform = (as.vector(i$rpca$E)) * i$sd\n      \n      L.orig = cumsum(L.transform) + i$j.init\n      X.rough = X.orig - L.orig\n      \n      #S.orig = cumsum(S.transform)\n      #E.orig = X.orig - L.orig - S.orig\n      \n      ###       \n      #\n      #X.rough.rpca = RcppRPCA(matrix(X.rough, nrow(i$rpca$X), ncol(i$rpca$X)),\n      #                        Lpenalty = 10,\n      #                        Spenalty = 2 / sqrt(10))\n      #S.orig = as.numeric(X.rough.rpca$S)\n      #E.orig = X.orig - L.orig - S.orig\n      \n      ###\n      S.orig = softThreshold(X.rough, 3 * (1/sqrt(2)) * sd(E.transform))\n      E.orig = X.orig - (L.orig) - S.orig\n      \n      data.frame(X.transform = X.transform,\n                 L.transform = L.transform,\n                 S.transform = S.transform,\n                 E.transform = E.transform,\n                 X.orig = X.orig,\n                 time = time)[-1,]\n    }\n    else {\n      data.frame(X.transform = (as.vector(i$rpca$X)) * i$sd + i$mean,\n                 L.transform = (as.vector(i$rpca$L)) * i$sd + i$mean,\n                 S.transform = (as.vector(i$rpca$S)) * i$sd,\n                 E.transform = (as.vector(i$rpca$E)) * i$sd,\n                 X.orig = (as.vector(i$rpca$X)) * i$sd + i$mean,\n                 time = time)\n    }\n  })\n  names = unlist((mapply(function(df, name) { rep(name, nrow(df)) }, rpca.ts.stacked, names(rpca.ts))))\n  #build a report containing anomaly data for all the columns found in X\n  rpca.ts.stacked = cbind(do.call('rbind', rpca.ts.stacked), name = as.vector(names))\n  names(rpca.ts.stacked) = c(\"X_transform\", \"L_transform\", \"S_transform\", \"E_transform\",\n                             \"X_original\",\n                             \"time\", \"name\")\n  \n  return (rpca.ts.stacked)\n}\n\n#' ggplot for AnomalyDetection\n#' \n#' ggplot function which shows the low rank signal in blue, the random noise in green,\n#' and any outliers in red. If a transformation was applied, these signals will be plotted\n#' in the transformed space, along with the original time series\n#' @param anomalyDetection output from AnomalyDetection.rpca\n#' @import ggplot2\n#' @export\n#' @examples\n#' frequency = 7\n#' numPeriods = 10\n#' ts.sinusoidal = sin((2 * pi / frequency ) * 1:(numPeriods * frequency))\n#' ts = ts.sinusoidal\n#' ts = sin((2 * pi / frequency ) * 1:(numPeriods * frequency))\n#' ts[58:60] = 100\n#' ggplot_AnomalyDetection.rpca(AnomalyDetection.rpca(ts)) + ggplot2::theme_grey(base_size = 25)\nggplot_AnomalyDetection.rpca = function(anomalyDetection) {\n  ggplot2::ggplot(anomalyDetection, ggplot2::aes(time, X_original)) +\n    ggplot2::geom_line(size = 1) +\n    ggplot2::geom_line(ggplot2::aes(y = X_transform), size = 1, color = \"black\", linetype = 'dashed') +\n    ggplot2::geom_line(ggplot2::aes(y = L_transform), size = .5, color = \"blue\") +\n    ggplot2::geom_line(ggplot2::aes(y = E_transform), size = .5, color = \"green\") +\n    ggplot2::geom_point(data = subset(anomalyDetection, abs(S_transform) > 0), color = \"red\",\n               ggplot2::aes(size = abs(S_transform))) +\n    ggplot2::scale_size_continuous(range=c(4,6)) +\n    ggplot2::facet_wrap(~name, scale = \"free\")    \n}\n\nsoftThreshold = function(x, penalty) {\n  sign(x) * pmax(abs(x) - penalty,0)\n}\n" }
{ "repo_name": "Netflix/Surus", "ref": "refs/heads/master", "path": "resources/R/RAD/R/anomaly_detection_ma.R", "content": "AnomalyDetection.ma = function(X, frequency=7) {\n  if (is.vector(X) & !is.data.frame(X)) X = data.frame(y=X)\n  \n  ma.ts = do.call('rbind', apply(X, 2, function(j) {\n    j.matrix = matrix(j, nrow= frequency)\n    means = apply(j.matrix[,1:(ncol(j.matrix)-1)], 1, mean)\n    sds = apply(j.matrix[,1:(ncol(j.matrix)-1)],1,sd)\n    upperbounds = means + 1.6*sds\n    lowerbounds = means - 1.6*sds\n    anomalous = t(apply(cbind(upperbounds, lowerbounds, j.matrix), 1, function(i) {\n      i[-(1:2)] > i[1] | i[-(1:2)] < i[2]\n    }))\n    data.frame(X = j,\n               time = 1:length(j),\n               anomaly = as.vector(anomalous))\n  }))\n  ma.ts = cbind(ma.ts, name = rep(names(X), each = nrow(X)))\n  \n  return (ma.ts)\n}\n\nggplot_AnomalyDetection.ma = function(anomalyDetection) {\n  ggplot(anomalyDetection,\n         aes(x = time, y=X)) +\n    geom_line(size = 1) +\n    geom_point(data = subset(anomalyDetection, anomaly == T), color = 'red', size = 6) +\n    facet_wrap(~name, scale = 'free')\n}" }
{ "repo_name": "AgResearch/KGD", "ref": "refs/heads/master", "path": "GBSRun.R", "content": "#!/bin/env Rscript\n\ngenofile <- \"Example/HapMap.hmc.txt.gz\"\ngform <- \"uneak\"   # uneak (default), Tassel or chip\n\nsource(\"GBS-Chip-Gmatrix.R\")\n\nGfull      <- calcG()\nGHWdgm.05  <- calcG(which(HWdis > -0.05), \"HWdgm.05\", npc = 4)  # recalculate using Hardy-Weinberg disequilibrium cut-off at -0.05\n\npedfile    <- \"Example/Ped-GBS.csv\"\ngroupsfile <- \"Example/Ped-Groups.csv\"\n\nrel.thresh <- 0.2\nGCheck     <- \"GHWdgm.05$G5\"\nsource(\"GBSPedAssign.R\")\n\n# G5 <- GHWdgm.05$G5 save(G5,file='G5.RData')\n" }
{ "repo_name": "gabraham/flashpca", "ref": "refs/heads/master", "path": "flashpcaR/tests/testthat/test_pca.R", "content": "context(\"Checking PCA\")\n\nn <- 200\np <- 1000\nndim <- 50\nnextra <- 50\n\ntest_that(\"Testing PCA with stand='binom'\", {\n   X <- matrix(sample(0:2, n * p, replace=TRUE), n, p)\n   q <- colMeans(X) / 2\n   S <- scale(X, center=TRUE, scale=sqrt(q * (1 - q)))\n\n   f1 <- prcomp(S, center=FALSE, scale.=FALSE)\n   f2 <- flashpca(X, ndim=ndim, mem=\"low\", nextra=nextra)\n   f3 <- flashpca(X, ndim=ndim, mem=\"high\", nextra=nextra)\n\n   expect_equal(attr(S, \"scaled:center\"), f2$center)\n   expect_equal(attr(S, \"scaled:scale\"), f2$scale)\n   expect_equal(attr(S, \"scaled:center\"), f3$center)\n   expect_equal(attr(S, \"scaled:scale\"), f3$scale)\n\n   r0 <- abs(diag(cor(f2$projection, f3$projection)))\n   expect_equal(r0, rep(1.0, ndim), tolerance=1e-3)\n\n   r1 <- abs(diag(cor(f1$x[, 1:ndim], f2$projection)))\n   expect_equal(r1, rep(1.0, ndim), tolerance=1e-3)\n\n   r2 <- abs(diag(cor(f1$x[, 1:ndim], f3$projection)))\n   expect_equal(r2, rep(1.0, ndim), tolerance=1e-3)\n})\n\n\ntest_that(\"Testing PCA with stand='sd'\", {\n   X <- matrix(rnorm(n * p), n, p)\n   S <- scale(X, center=TRUE, scale=TRUE)\n\n   f1 <- prcomp(S, center=FALSE, scale.=FALSE)\n   f2 <- flashpca(X, ndim=ndim, stand=\"sd\", mem=\"low\", nextra=nextra)\n   f3 <- flashpca(X, ndim=ndim, stand=\"sd\", mem=\"high\", nextra=nextra)\n\n   expect_equal(attr(S, \"scaled:center\"), f2$center)\n   expect_equal(attr(S, \"scaled:scale\"), f2$scale)\n   expect_equal(attr(S, \"scaled:center\"), f3$center)\n   expect_equal(attr(S, \"scaled:scale\"), f3$scale)\n\n   r1 <- abs(diag(cor(f1$x[, 1:ndim], f2$projection)))\n   expect_equal(r1, rep(1.0, ndim), tolerance=1e-3)\n\n   r2 <- abs(diag(cor(f1$x[, 1:ndim], f3$projection)))\n   expect_equal(r2, rep(1.0, ndim), tolerance=1e-3)\n})\n\ntest_that(\"Testing PCA with stand='none'\", {\n   X <- matrix(rnorm(n * p), n, p)\n\n   f1 <- prcomp(X, center=FALSE, scale.=FALSE)\n   f2 <- flashpca(X, ndim=ndim, stand=\"none\", mem=\"low\", nextra=nextra)\n   f3 <- flashpca(X, ndim=ndim, stand=\"none\", mem=\"high\", nextra=nextra)\n\n   expect_identical(numeric(0), f2$center)\n   expect_identical(numeric(0), f2$scale)\n   expect_identical(numeric(0), f3$center)\n   expect_identical(numeric(0), f3$scale)\n\n   r1 <- abs(diag(cor(f1$x[, 1:ndim], f2$projection)))\n   expect_equal(r1, rep(1.0, ndim), tolerance=1e-3)\n\n   r2 <- abs(diag(cor(f1$x[, 1:ndim], f3$projection)))\n   expect_equal(r2, rep(1.0, ndim), tolerance=1e-3)\n})\n\ntest_that(\"Testing PCA with stand='center'\", {\n   X <- matrix(rnorm(n * p), n, p)\n   S <- scale(X, center=TRUE, scale=FALSE)\n\n   f1 <- prcomp(S, center=FALSE, scale.=FALSE)\n   f2 <- flashpca(X, ndim=ndim, mem=\"low\", stand=\"center\", nextra=nextra)\n   f3 <- flashpca(X, ndim=ndim, mem=\"high\", stand=\"center\", nextra=nextra)\n\n   expect_equal(attr(S, \"scaled:center\"), f2$center)\n   expect_equal(rep(1, p), f2$scale)\n   expect_equal(attr(S, \"scaled:center\"), f3$center)\n   expect_equal(rep(1, p), f3$scale)\n\n   r1 <- abs(diag(cor(f1$x[, 1:ndim], f2$projection)))\n   expect_equal(r1, rep(1.0, ndim), tolerance=1e-3)\n\n   r2 <- abs(diag(cor(f1$x[, 1:ndim], f3$projection)))\n   expect_equal(r2, rep(1.0, ndim), tolerance=1e-3)\n})\n\n" }
{ "repo_name": "gabraham/flashpca", "ref": "refs/heads/master", "path": "test.R", "content": "\nset.seed(32312)\n\nlibrary(flashpcaR)\n\nn <- 1000\np <- 5000\nX <- matrix(rnorm(n * p), n, p)\nX <- scale(X, center=TRUE, scale=FALSE)\n\nsystem.time({\n   S <- tcrossprod(X) / (nrow(X) - 1)\n   e <- eigen(S)\n})\n\nsystem.time({\n   s <- svd(X)\n})\n\nnthr <- 1\ntol <- 1e-9\nk <- 20\nsystem.time({\n   f1 <- flashpca(X, ndim=k, stand=\"center\", transpose=FALSE, tol=tol,\n      num_threads=nthr, maxiter=100)\n})\nf2 <- flashpca(t(X), ndim=k, stand=\"center\", transpose=TRUE, tol=tol,\n   num_threads=nthr, maxiter=100)\nf3 <- flashpca(X, ndim=k, stand=\"none\", transpose=FALSE, tol=tol,\n   num_threads=nthr, maxiter=100)\nf4 <- flashpca(t(X), ndim=k, stand=\"none\", transpose=TRUE, tol=tol,\n   num_threads=nthr, maxiter=100)\n\n(r <- cbind(\n   e=e$val[1:k],\n   f1=f1$val,\n   f2=f2$val,\n   f3=f3$val,\n   f4=f4$val,\n   s=s$d[1:k]^2 / (nrow(X) - 1)\n))\n\ncor(r)\n\n" }
{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "inst/ShinyStan/server_files/pages/diagnose/server/multitrace.R", "content": "# \n# # multiparameter traceplots -----------------------------------------------\n# calc_height_trace_plot <- reactive({\n#   params <- input$multitrace_params\n#   grid <- FALSE\n#   if (!is.null(input$multitrace_layout)) {\n#     if (input$multitrace_layout == \"Grid\") grid <- TRUE\n#   }\n#   params <- .update_params_with_groups(params, param_names)\n#   LL <- length(params)\n#   if (LL == 0) LL <- 4\n#   if (LL == 1) LL <- 2\n#   if (grid) {\n#     if (LL > 5) return(30*LL)\n#     if (LL < 5) return(60*LL)\n#   }\n#   round(100*LL)\n# })\n# \n# # multitrace_plot\n# multitrace_plot <- reactive({\n#   validate(need(!is.null(input$multitrace_rect), message = \"Loading...\"))\n#   x1 <- input$multi_xzoom[1]\n#   x2 <- input$multi_xzoom[2]\n#   dat <- samps_all[x1:x2,,,drop=FALSE]\n#   # zoom <- \"On\"\n#   do.call(\".param_trace_multi\", args = list(\n#     params      = input$multitrace_params,\n#     all_param_names = param_names,\n#     dat         = dat,\n#     chain       = input$multitrace_chain,\n#     warmup_val  = warmup_val,\n#     palette     = input$multitrace_palette ,\n#     rect        = input$multitrace_rect,\n#     rect_color  = \"skyblue\",\n#     rect_alpha  = input$multitrace_rect_alpha,\n#     layout      = input$multitrace_layout,\n#     x1          = x1,\n#     x2          = x2\n#   ))\n# })\n# \n# output$multitrace_plot_out <- renderPlot({\n#   x <- multitrace_plot()\n#   suppressWarnings(print(x)) # this avoids warnings about removing rows when using tracezoom feature\n# }, height = calc_height_trace_plot, bg = \"transparent\")\n# \n# # download the plot\n# output$download_multitrace <- downloadHandler(\n#   filename = paste0('shinystan_multitrace.RData'),\n#   content = function(file) {\n#     shinystan_multitrace <- multitrace_plot()\n#     save(shinystan_multitrace, file = file)\n#   }\n# )\n" }
{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "R/generate_quantity.R", "content": "# shinystan is free software; you can redistribute it and/or modify it under the\n# terms of the GNU General Public License as published by the Free Software\n# Foundation; either version 3 of the License, or (at your option) any later\n# version.\n# \n# shinystan is distributed in the hope that it will be useful, but WITHOUT ANY\n# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR\n# A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License along with\n# this program; if not, see <http://www.gnu.org/licenses/>.\n\n\n#' Add new quantity to shinystan object\n#' \n#' Add to shinystan object a new parameter as a function of one or two existing\n#' parameters.\n#' \n#' @export\n#' @template args-sso\n#' @param fun Function to call, i.e. \\code{function(param1)} or \n#'   \\code{function(param1,param2)}. See Examples, below.\n#' @param param1 Name of first parameter as character string.\n#' @param param2 Optional. Name of second paramter as character string.\n#' @param new_name Name for the new parameter as character string.\n#'   \n#' @return sso, updated. See Examples.\n#' \n#' @template seealso-drop_parameters\n#'\n#' @examples\n#' # Using example shinystan object 'eight_schools'\n#' sso <- eight_schools\n#' sso <- generate_quantity(sso, fun = function(x) x^2, \n#'                          param1 = \"tau\", new_name = \"tau_sq\")\n#' sso <- generate_quantity(sso, fun = \"-\", \n#'                          param1 = \"theta[1]\", param2 = \"theta[2]\", \n#'                          new_name = \"theta1minus2\")\n#'                          \ngenerate_quantity <- function(sso, param1, param2, fun, new_name) {\n  sso_check(sso)\n  if (isTRUE(new_name %in% slot(sso, \"param_names\")))\n    stop(paste(\"There is already a parameter named\", new_name))\n  \n  message(\"\\nThis might take a moment for large shinystan objects...\")\n  \n  two_params <- !missing(param2)\n  posterior <- slot(sso, \"posterior_sample\")\n  dims <- dim(posterior)\n  ndim <- length(dims)\n  if (ndim == 3) {\n    # i.e. multiple chains\n    x_samp <- posterior[, , param1]\n    if (two_params)\n      y_samp <- posterior[, , param2]\n  }\n  if (ndim == 2) {\n    # i.e. only 1 chain\n    x_samp <- posterior[, param1]\n    if (two_params)\n      y_samp <- posterior[, param2]\n  }\n  \n  arglist <- if (two_params)\n    list(x_samp, y_samp) else list(x_samp)\n  temp <- do.call(fun, args = arglist)\n  \n  new_dim <- dims\n  new_dim[[ndim]] <- new_dim[[ndim]] + 1\n  new_dim_names <- dimnames(posterior)\n  new_dim_names[[ndim]] <- c(new_dim_names[[ndim]], new_name)\n  posterior <-\n    array(data = c(posterior, temp),\n          dim = new_dim,\n          dimnames = new_dim_names)\n  \n  param_dims_new <- slot(sso, \"param_dims\")\n  param_dims_new[[new_name]] <- numeric(0)\n  sso_new <- as.shinystan(\n    posterior,\n    model_name = slot(sso, \"model_name\"),\n    burnin = slot(sso, \"n_warmup\"),\n    param_dims = param_dims_new\n  )\n  slot(sso_new, \"summary\") <-\n    shinystan_monitor(posterior, warmup = slot(sso, \"n_warmup\"))\n  \n  slot_names <- c(\"sampler_params\", \"model_code\", \"user_model_info\", \"misc\")\n  for (sn in slot_names)\n    slot(sso_new, sn) <- slot(sso, sn)\n  \n  sso_new\n}\n" }
{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "R/sso-metadata.R", "content": "# shinystan is free software; you can redistribute it and/or modify it under the\n# terms of the GNU General Public License as published by the Free Software\n# Foundation; either version 3 of the License, or (at your option) any later\n# version.\n# \n# shinystan is distributed in the hope that it will be useful, but WITHOUT ANY\n# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR\n# A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License along with\n# this program; if not, see <http://www.gnu.org/licenses/>.\n\n\n#' View or change metadata associated with a shinystan object\n#' \n#' @name shinystan-metadata\n#' @template args-sso\n#' \n#' @template seealso-as.shinystan\n#' @template seealso-drop_parameters\n#' @template seealso-generate_quantity\n#' \n#' @examples \n#' # use eight_schools example object\n#' sso <- eight_schools\n#' \nNULL\n\n# sso_info ----------------------------------------------------------------\n#' @rdname shinystan-metadata\n#' @export\n#' \n#' @return \\code{sso_info} prints basic metadata including number of parameters, \n#'   chains, iterations, warmup iterations, etc. It does not return anything.\n#' \n#' @examples \n#' ################\n#' ### sso_info ###\n#' ################\n#' \n#' sso_info(sso)\n#'\nsso_info <- function(sso) {\n  sso_check(sso)\n  sso_name <- deparse(substitute(sso))\n  has_notes <-\n    sso@user_model_info != \"Use this space to store notes about your model\"\n  has_code <-\n    sso@model_code != \"Use this space to store your model code\"\n  \n  cat(\n    sso_name,\n    \"---------------------\",\n    paste(\"Model name:\", sso@model_name),\n    paste(\"Parameters:\", length(sso@param_names)),\n    paste(\"Parameter groups:\", length(names(sso@param_dims))),\n    paste(\"Chains:\", sso@n_chain),\n    paste(\"Iterations:\", sso@n_iter),\n    paste(\"Warmup:\", sso@n_warmup),\n    paste(\"Has model code:\", has_code),\n    paste(\"Has user notes:\", has_notes),\n    sep = \"\\n\"\n  )\n}\n\n\n\n# model_code --------------------------------------------------------------\n#' @rdname shinystan-metadata\n#' @export\n#' @param code A string, containing model code to be added, that can be\n#'   used as an argument to \\code{\\link{cat}}. See \\strong{Examples}.\n#'   \n#' @return \\code{model_code} returns or replaces model code stored in a \n#'   shinystan object. If \\code{code} is \\code{NULL} then any existing model\n#'   code stored in \\code{sso} is returned as a character string. If \\code{code}\n#'   is specified then an updated shinystan object is returned with \\code{code}\n#'   added. For shinystan objects created from stanfit (\\pkg{rstan}) and stanreg\n#'   (\\pkg{rstanarm}) objects, model code is automatically taken from that\n#'   object and does not need to be added manually. From within the ShinyStan\n#'   interface model code can be viewed on the \\strong{Model Code} page.\n#'\n#' @examples\n#' ##################\n#' ### model_code ###\n#' ##################\n#' \n#' # view model code in example shinystan object 'eight_schools'\n#' cat(model_code(sso))\n#' \n#' # change the model code in sso \n#' # some jags style code\n#' my_code <- \"\n#'  model {\n#'    for (i in 1:length(Y)) {\n#'      Y[i] ~ dpois(lambda[i])\n#'      log(lambda[i]) <- inprod(X[i,], theta[])\n#'    }\n#'    for (j in 1:J) {\n#'      theta[j] ~ dt(0.0, 1.0, 1.0)\n#'    }\n#'  }\n#' \"\n#' sso <- model_code(sso, my_code)\n#' cat(model_code(sso))\n#'\nmodel_code <- function(sso, code = NULL) {\n  sso_check(sso)\n  validate_model_code(code)\n  \n  if (is.null(code))\n    return(slot(sso, \"model_code\"))\n  \n  slot(sso, \"model_code\") <- code\n  message(\n    paste0(\n      \"Successfully added code.\",\n      \"\\nYou can view the code in the\",\n      \"ShinyStan GUI on the 'Model Code' page.\"\n    )\n  )\n  sso\n}\n\nvalidate_model_code <- function(code) {\n  if (is.null(code) || is.character(code)) {\n    invisible(TRUE)\n  } else {\n    stop(\"Model code should be NULL or a string\", call. = FALSE)\n  }\n}\n\n\n\n# notes -------------------------------------------------------------------\n#' @rdname shinystan-metadata\n#' @export\n#' @param note A string containing a note to add to any existing notes\n#'   or replace existing notes, depending on the value of \\code{replace}.\n#' @param replace If \\code{TRUE} the existing notes are overwritten by \n#'   \\code{note} if \\code{note} is specified. If \\code{FALSE} (the default) \n#'   if \\code{note} is specified then its content is appended to the existing\n#'   notes.\n#'   \n#' @return \\code{notes} returns, amends, or replaces notes stored in a shinystan\n#'   object. If \\code{note} is \\code{NULL} then any existing notes stored in \n#'   \\code{sso} are returned as a character string. If \\code{note} is specified \n#'   then an updated shinystan object is returned with either \\code{note} added \n#'   to the previous notes (if \\code{replace=FALSE}) or overwritten by \n#'   \\code{note} (if \\code{replace = TRUE}). From within the ShinyStan\n#'   interface, notes are viewable on the \\strong{Notepad} page.\n#'   \n#' @examples \n#' #############\n#' ### notes ###\n#' #############\n#' \n#' # view existing notes\n#' notes(sso)\n#' \n#' # add a note to the existing notes\n#' sso <- notes(sso, \"New note\")\n#' notes(sso)\n#' cat(notes(sso))\n#' \n#' # replace existing notes\n#' sso <- notes(sso, \"replacement note\", replace = TRUE)\n#' notes(sso)\n#'  \nnotes <- function(sso, note = NULL, replace = FALSE) {\n  sso_check(sso)\n  if (is.null(note))\n    return(slot(sso, \"user_model_info\"))\n  \n  if (!is.character(note) || !isTRUE(length(note) == 1))\n    stop(\"'note' should be a single string\")\n  \n  slot(sso, \"user_model_info\") <- if (replace)\n    note else c(slot(sso, \"user_model_info\"), paste0(\"\\n\\n\", note))\n  \n  message(\n    paste(\n      \"Successfully added note.\",\n      \"\\nYou can view the notes in the\",\n      \"ShinyStan GUI on the 'Notepad' page.\"\n    )\n  )\n  sso\n}\n\n\n\n# model_name (renaming) -----------------------------------------------------#' \n#' @rdname shinystan-metadata\n#' @export\n#' @param name A string giving the new model name to use.\n#'   \n#' @return \\code{model_name} returns or replaces the model name associated with \n#'   a shinystan object. If \\code{name} is \\code{NULL} then the current model\n#'   name is returned. If \\code{name} is specified then \\code{sso} is returned\n#'   with an updated model name.\n#' \n#' @examples\n#' ##################\n#' ### model_name ###\n#' ##################\n#' \n#' # view model name\n#' model_name(sso)\n#' \n#' # change model name\n#' sso <- model_name(sso, \"some other name\")\n#' identical(model_name(sso), \"some other name\")\n#' \nmodel_name <- function(sso, name = NULL) {\n  sso_check(sso)\n  if (is.null(name))\n    return(slot(sso, \"model_name\"))\n  \n  if (!is.character(name) || !isTRUE(length(name) == 1))\n    stop(\"'name' should be a single string\")\n  \n  slot(sso, \"model_name\") <- name\n  message(paste(\"Successfully changed model name to\", name))\n  sso\n}\n\n\n# nocov start\n#' rename_model (deprecated)\n#' \n#' This function is deprecated and will be removed in a future release. Please \n#' use the \\code{\\link{model_name}} function instead.\n#' \n#' @export\n#' @keywords internal\n#' @param sso,new_model_name Use the \\code{\\link{model_name}} function instead.\n#' \nrename_model <- function(sso, new_model_name) {\n  .Deprecated(\"model_name()\")\n  model_name(sso, new_model_name)\n}\n# nocov end\n" }
{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "R/launch_shinystan.R", "content": "# shinystan is free software; you can redistribute it and/or modify it under the\n# terms of the GNU General Public License as published by the Free Software\n# Foundation; either version 3 of the License, or (at your option) any later\n# version.\n# \n# shinystan is distributed in the hope that it will be useful, but WITHOUT ANY\n# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR\n# A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License along with\n# this program; if not, see <http://www.gnu.org/licenses/>.\n\n\n#' Launch the ShinyStan app\n#' \n#' Launch the ShinyStan app in the default web browser. RStudio users also have\n#' the option of launching the app in RStudio's pop-up Viewer.\n#' \n#' @export\n#' @param object An object of class shinystan, stanfit, or stanreg. To use other\n#'   types of objects first create a shinystan object using \n#'   \\code{\\link{as.shinystan}}.\n#' @param rstudio Only relevant for RStudio users. The default (\\code{FALSE}) is\n#'   to launch the app in the user's default web browser rather than RStudio's\n#'   pop-up Viewer. Users can change the default to \\code{TRUE} by setting the\n#'   global option \\code{options(shinystan.rstudio = TRUE)}.\n#' @param ... Optional arguments passed to \\code{\\link[shiny]{runApp}}.\n#' \n#' @return The \\code{launch_shinystan} function is used for the side effect of \n#'   starting the ShinyStan app, but it also returns a shinystan object, an\n#'   instance of S4 class \\code{\"shinystan\"}.\n#'   \n#' @template seealso-as.shinystan \n#' @template seealso-update_sso \n#' @template seealso-demo\n#'   \n#'   \n#' @examples\n#' \\dontrun{\n#' #######################################\n#' # Example 1: 'sso' is a shinystan object\n#' #######################################\n#' \n#' # Just launch shinystan\n#' launch_shinystan(sso)\n#' \n#' # Launch shinystan and replace sso with an updated version of itself\n#' # if any changes are made to sso while using the app\n#' sso <- launch_shinystan(sso)\n#' \n#' # Launch shinystan but save any changes made to sso while running the app\n#' # in a new shinystan object sso2. sso will remained unchanged. \n#' sso2 <- launch_shinystan(sso) \n#' \n#' #######################################\n#' # Example 2: 'sf' is a stanfit object\n#' #######################################\n#' \n#' # Just launch shinystan\n#' launch_shinystan(sf)\n#' \n#' # Launch shinystan and save the resulting shinystan object\n#' sf_sso <- launch_shinystan(sf)\n#' \n#' # Now sf_sso is a shinystan object and so Example 1 (above) applies when\n#' # using sf_sso. \n#' \n#' #######################################\n#' # Example 3: 'fit' is an mcmc.list, array or list of matrices\n#' #######################################\n#'\n#' # First create shinystan object (see ?as.shinystan for full details)\n#' fit_sso <- as.shinystan(fit, model_name = \"Example\")\n#' \n#' # Now fit_sso is a shinystan object and so Example 1 (above) applies.\n#' }\n#'\nlaunch_shinystan <- function(object, \n                             rstudio = getOption(\"shinystan.rstudio\"), \n                             ...) {\n  if (is.shinystan(object)) {\n    sso_check(object)\n  } else if (is.stanreg(object) || is.stanfit(object)) {\n    message(\"\\nCreating shinystan object...\")\n    object <- as.shinystan(object)\n  }\n  if (!is.shinystan(object))\n    stop(\"'object' is not a valid input. See help('launch_shinystan').\")\n  \n  message(\"\\nLaunching ShinyStan interface... \",\n          \"for large models this  may take some time.\")\n  invisible(launch(object, rstudio, ...))\n}\n\n\n#' ShinyStan demo\n#'\n#' @aliases eight_schools\n#' @export\n#' @inheritParams launch_shinystan\n#' @param demo_name The name of the demo. Currently \\code{\"eight_schools\"} is \n#'   the only option, but additional demos may be available in future releases.\n#'   \\describe{\n#'   \\item{\\code{eight_schools}}{Hierarchical meta-analysis model. See \n#'    \\emph{Meta Analysis} chapter of the Stan manual (chapter 11.2 in version\n#'    2.9), \\url{http://mc-stan.org/documentation/}.}\n#'   }\n#' @return An S4 shinystan object.\n#'   \n#' @template seealso-launch\n#' @template seealso-as.shinystan\n#' \n#' @examples\n#' \\dontrun{\n#' # launch demo but don't save a shinystan object\n#' launch_shinystan_demo() \n#' \n#' # launch demo and save the shinystan object for the demo \n#' sso_demo <- launch_shinystan_demo()\n#' }\n#'\nlaunch_shinystan_demo <- function(demo_name = \"eight_schools\",\n                                  rstudio = getOption(\"shinystan.rstudio\"),\n                                  ...) {\n  demo_name <- match.arg(demo_name)\n  demo_object <- get(demo_name)\n  invisible(launch(demo_object, rstudio = rstudio, ...))\n}\n\n# Internal launch function \n# @param sso shinystan object\n# @param rstudio launch in rstudio viewer instead of web browser? \n# @param ... passed to shiny::runApp\nlaunch <- function(sso, rstudio = FALSE, ...) {\n  launch.browser <- if (!rstudio) \n    TRUE else getOption(\"shiny.launch.browser\", interactive())\n  \n  .sso_env$.SHINYSTAN_OBJECT <- sso  # see zzz.R for .sso_env\n  on.exit(.sso_env$.SHINYSTAN_OBJECT <- NULL, add = TRUE)\n  shiny::runApp(system.file(\"ShinyStan\", package = \"shinystan\"), \n                launch.browser = launch.browser, ...)\n}\n" }
{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "inst/ShinyStan/ui_files/model_code.R", "content": "sidebarLayout(\n  sidebarPanel(\n    width = 3,\n    style = \"height: 550px;\",\n    br(),\n    h4(\"Model Code\"),\n    helpText(\n      style = \"font-size: 12px;\",\n      p(\n        \"Model code will be displayed here each\",\n        \"time you launch ShinyStan with this shinystan object.\"\n      )\n    ),\n    br(),\n    actionButton(\n      \"save_user_model_code\",\n      label = \"Save code\",\n      icon = icon(\"save\")\n    ),\n    div(style = \"font-size: 11px;\", textOutput(\"user_code_saved\")),\n    conditionalPanel(\n      condition = \"input.save_user_model_code > 0\",\n      br(),\n      save_and_close_reminder(\"save_user_model_code_safe_quit\")\n    )\n  ),\n  mainPanel(\n    width = 9,\n    br(), br(),\n    tags$textarea(\n      id = \"user_model_code\",\n      wrap = \"off\",\n      cols = 80,\n      rows = 20,\n      .model_code\n    )\n  )\n)\n" }
{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "inst/ShinyStan/server_utils.R", "content": "# function to suppress unnecessary warnings and messages generated by ggplot \nsuppress_and_print <- function(x) {\n  suppressMessages(suppressWarnings(print(x)))\n}\n" }
{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "inst/ShinyStan/ui_files/help.R", "content": "div(\n  class = \"help-glossary-div\",\n  br(), br(),\n  div(\n    class = \"help-glossary-nav-container\",\n    navlistPanel(\n      well = TRUE,\n      id = \"help_navlist\",\n      \"Topics\",\n      tabPanel(\n        \"Questions, bugs, and new features\",\n        div(\n          class = \"glossary-entry\",\n          h4(\"Stan users group\"),\n          p(\n            \"To ask a question or suggest a new feature visit the\",\n            a(\n              \"Stan users message board.\", \n              href = \"https://groups.google.com/forum/?fromgroups#!forum/stan-users\"\n            )\n          ),\n          br(),\n          h4(\"GitHub issue tracker\"),\n          p(\n            \"To report a bug  or suggest a new feature visit the\",\n            a(\n              \"GitHub issue tracker.\", \n              href = \"https://github.com/stan-dev/shinystan/issues\"\n            )\n          )\n        )\n      ),\n      tabPanel(\n        \"Saving plots\",\n        div(\n          class = \"glossary-entry\",\n          h4(\"Saving plots as ggplot2 objects\"),\n          p(\n            \"Clicking on a 'Save ggplot2 object' button will be save an .RData\n            file that you can load into your Global Environment using the\",\n            code(\"load\"),\n            \"function in R.\n            You can then make changes to the plot using the functions in the\n            ggplot2 package.\"\n          ),\n          p(\n            \"Any plot that can be saved as a ggplot2 object can also be saved\n            as a PDF.\"\n          )\n      )), \n      tabPanel(\n        \"Large models and launch speed\",\n        div(\n          class = \"glossary-entry\",\n          h4(\"Launching ShinyStan faster\"),\n          p(\n            \"The\", code(\"drop_parameters\"), \"function in the\", \n            strong(\"shinystan\"), \"R package will allow you to reduce the size\", \n            \"of a shinystan object by removing parameters.\", \n            \"See\", code(\"help('drop_parameters', 'shinystan')\"), \n            \"for the documentation.\"\n          ),\n          p(\n            \"Additionally, for large models, the\", code(\"launch_shinystan\"),\n            \"function will launch the app faster when used with a\",\n            \"shinystan object rather than a stanfit object\",\n            \"(because no conversion is required).\",\n            \"If ShinyStan takes a long time to launch for your\",\n            \"model then it can help to first create a\",\n            \"shinystan object using the\", code(\"as.shinystan\"), \"function.\",\n            \"Alternatively, the first time you launch\",\n            \"ShinyStan using a stanfit object, a shinystan\",\n            \"object will be returned if you assign the value of\",\n            code(\"launch_shinystan\"),\n            \"to a name, e.g.\"\n          ),\n          p(code(\"sso <- launch_shinystan(stanfit)\")),\n          p(\"rather than just\"),\n          p(code(\"launch_shinystan(stanfit)\")),\n          p(\n            \"The next time you launch ShinyStan for the same\",\n            \"model you can launch it using\", code(\"sso\"), \"rather than\",\n            code(\"stanfit\"), \"and it should be quicker to launch.\",\n            \"If it is still too slow then dropping some large parameters\", \n            \"from the shinystan object is the best solution.\"\n          )\n        )\n      )\n    )\n  ),\n  br(), br()\n)\n" }
{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "inst/ShinyStan/ui_files/diagnostics_treedepth.R", "content": "# treedepth\ndiv(\n  class = \"diagnostics-navlist-tabpanel\",\n  fluidRow(\n    column(\n      width = 7,\n      help_dynamic,\n      dygraphOutput_175px(\"dynamic_trace_diagnostic_treedepth_out\"),\n      br(), br(),\n      plotOutput(\"treedepth_vs_lp_out\", height = \"150px\")\n    ),\n    column(width = 5, plotOutput_400px(\"treedepth_vs_accept_stat_out\"))\n  ),\n  splitLayout(\n    plotOutput(\"treedepth_ndivergent_hist_out\", height = \"125px\"),\n    plotOutput(\"treedepth_ndivergent0_hist_out\", height = \"125px\"),\n    plotOutput(\"treedepth_ndivergent1_hist_out\", height = \"125px\")\n  ),\n  br()\n)\n" }
{ "repo_name": "stan-dev/shinystan", "ref": "refs/heads/develop", "path": "man-roxygen/seealso-generate_quantity.R", "content": "#' @seealso \\code{\\link{generate_quantity}} to add a new quantity to a shinystan\n#'   object.\n" }
{ "repo_name": "data-steve/data-steve.github.io", "ref": "refs/heads/master", "path": "_backlog/ggdumbbell2.R", "content": "pacman::p_load(dplyr, ggplot2, tidyr)\nmkdat <- function(x,y,cat){\n\n}\n\ndat <-data.frame(\"Jessica\" = c(8.6, 9.3, 9.34, 9.1, 9),\n                 \"Francine\" = c(9.4, 8.88, 9.2, 9.24, 9.34),\n            \"Events\" = factor(1:5,labels=c(\"sprint\", \"hurdles\",\"javelin\", \"discus\", \"hi-jump\")))\n\nggdumbbell <- function(d\n                       ,colors=c(\"green4\",\"red3\",\"brown\")\n                       ,sz=17){\n    nms <- names(d)\n    names(d) <- c(\"xvar\",\"yvar\", \"fvar\")\n    df <- d %>%\n        dplyr::mutate(\n               abs_diff = abs(xvar-yvar),\n               pos_neg = sign(xvar-yvar),\n               midpoint = abs_diff/2,\n               clr = ifelse(pos_neg>0, colors[1], colors[2])\n          ) %>%\n      dplyr::rowwise() %>%\n      dplyr::mutate(xmin = min(xvar,yvar),\n             xmax = max(xvar,yvar)) %>% ungroup\n\n    df_tall <- df %>%\n        tidyr::gather(keys, values, -c(fvar, xmin, xmax, midpoint, pos_neg, abs_diff, clr)) %>%\n        mutate(keys=ifelse(keys==\"xvar\", nms[1], nms[2]))\n\n    l <-  as.character(df[[\"fvar\"]][as.integer(df[[\"fvar\"]])==max(as.integer(df[[\"fvar\"]]))])\n    ll <- if(d[1,1]<d[1,2]) nms[1:2] else nms[2:1]\nggplot(df_tall, aes(y=fvar, x=values) )+\n  geom_segment(data=df, aes(x=xmin, xend=xmax, y=fvar, yend=fvar, alpha=abs_diff), color=df$clr, size=3) +\n      geom_point(color = colors[3], size=5.5)  +\n      geom_point(shape=21, color = colors[3], size=4.8, aes(fill=keys))  +\n\n      scale_fill_manual(values = c(\"white\", colors[3]), guide=FALSE) +\n\n      theme_bw()  +\n  theme(text=element_text(size=sz),legend.position=\"none\", panel.grid = element_blank() )  +\n  annotate(\"text\"\n           , x = c(df[df$\"fvar\"==l,]$xmin,df[df$\"fvar\"==l,]$xmax)\n           , y= c(df[df$\"fvar\"==l,]$fvar,df[df$\"fvar\"==l,]$fvar)\n           , label=ll\n           , color = colors[3], size=4.8 , vjust = 2.5) +\n  labs(list(x=\"Values\",y=nms[3])) +\n  annotate(\"text\", x=8.75, y=5, label=\"Green/Red signifies\\nhigher/lower performance.\\n\\n Color intensity signifies\\nimporance of difference.\", color=colors[3], size=7, vjust = .75)\n}\n\nggdumbbell(dat) +\n  # ggtitle(\"Jessica's performance against Francine\") +\n  ggsave(\"~/Documents/repos/data-steve.github.io/images/dumbbell.png\")\n" }
{ "repo_name": "langcog/wordbank", "ref": "refs/heads/master", "path": "incoming_data/TEDS/convert_TEDS.R", "content": "# DEMOGRAPHIC INFORMATION\n# Id_twin (unique twin identifier)\n# Id_fam (family identifier, unique to each twin pair)\n# twin   (twin birth order, 1=elder 2=younger)\n# Random  (permits selecting one twin at random from each pair, to give independence of data, coded 0/1)\n# Aethnic (ethnic category: 1=white, 0=other)\n# Sex1 (gender of twin: 0=female, 1=male)\n# Zygos (twin pair zygosity, 1=MZ 2=DZ)\n# Amohqual (maternal educational qualifications, 8-point ordinal scale)\n# Ases  (composite social class based on parental education and occupation, standardised with mean 0.0 and SD 1.0)\n# \n# AGE 2 MEASURES\n# Brepage1  (age of twin on completion of parent-reported measures),\n# converted to months and rounded down\n# Bvc0011 to bvc1001  (100 item vocabulary checklist)\n# Bvocab1 (vocabulary total on this scale)\n# Bwu061  (does child combine words)\n# Bs01s1 through bs12s1  (12 sentence complexity pairs)\n# \n# AGE 3 MEASURES\n# Crepage1 (age of twin on completion of parent-reported measures),\n# converted to months and rounded down\n# Cvc0001 (not yet talking)\n# Cvc0011 to cvc1001  (100 item vocabulary checklist)\n# Cvocab1   (vocabulary total on this scale)\n# Cs00s1 (does child combine words)\n# Cs01s1 through cs12s1) (12 sentence complexity pairs)\n# \n# AGE 4 MEASURES\n# Drepage1 (age of twin on completion of parent-reported measures),\n# converted to months and rounded down\n# Dvd011 through dvc481 (48 item vocabulary checklist)\n# Dsay011  (overall evaluation of childs language/sentences, 6 point scale)\n\nlibrary(dplyr)\nlibrary(haven)\nlibrary(readr)\nlibrary(foreign)\nlibrary(tidyr)\n\nteds_raw <- read_spss(\"TEDS dataset for WordBank 110516.sav\")\n\nwrite_csv(teds_raw, \"teds.csv\")\n\n# make long form\nteds_long <- teds_raw %>% \n  gather(variable, value, btwoyear:dsay011) %>%\n  separate(variable, into=c(\"age\", \"variable\"), sep = 1) %>%\n  mutate(age = ifelse(age == \"b\", 2, ifelse(age == \"c\", 3, 4))) %>%\n  filter(!(variable %in% c(\"twoyear\",\"threeyr\",\"fouryr\", \"vocab1\"))) %>%\n  mutate(sex = ifelse(sex1 == 1, \"male\", \"female\"), \n         ethnicity = ifelse(aethnic == 1, \"white\", \"other\"),\n         zygosity = ifelse(zygos == 1, \"MZ\",\"DZ\"),\n         ses = ases, \n         mom_ed = amohqual) %>%\n  select(-sex1, -aethnic, -zygos, -ases, -amohqual)\n\n# split out ages for later merging\nages <- filter(teds_long, variable == \"repagem1\") %>%\n  rename(age_months = value) %>%\n  select(id_twin, age, age_months)\n\n# fours <- filter(d, variable != \"repagem1\", age == 4)\n\n# take only the two- and three-year-olds\n# drop NAs for age, as this is missing data\ntwos <- filter(teds_long, \n               age == 2, \n               variable != \"repagem1\") %>%\n  spread(variable, value) %>%\n  mutate(id_twin = factor(id_twin)) %>%\n  left_join(ages %>% mutate(id_twin = factor(id_twin))) %>%\n  filter(!is.na(age_months)) %>%\n  select(-twin,-random, -age, -ses)\n\nthrees <- filter(teds_long, \n               age == 3, \n               variable != \"repagem1\") %>%\n  spread(variable, value) %>%\n  mutate(id_twin = factor(id_twin)) %>%\n  left_join(ages %>% mutate(id_twin = factor(id_twin))) %>%\n  filter(!is.na(age_months)) %>%\n  select(-twin,-random, -age, -ses)\n\n# Either a single excel file (by convention called WugeseWS_Dax, but doesn't have to be), with sheets named data, fields, and values;\n# Or three csv files, with the names foo_data, foo_fields, and foo_values (where foo is WugeseWS_Dax by convention, but doesn't have to be).\n# For the data sheet/file:\n#   The first row should be column labels (whatever they might be in this dataset).\n# Each other row should be a single CDI administration.\n\nwrite_csv(twos, \"../../raw_data/English_British_TEDS_2s/EnglishBritishTEDS2s_data.csv\")\nwrite_csv(threes, \"../../raw_data/English_British_TEDS_3s/EnglishBritishTEDS3s_data.csv\")\n\n# The fields sheet/file is a mapping from the dataset's column labels to Wordbank's fields, and should have the following columns:\n#   column: column labels from the data sheet/file (modulo case sensitivity) that will be extracted\n# field: what Wordbank field to map the column label to\n# MUST include study_id and at least one of data_age and (date_of_birth and date_of_test)\n# can also optionally have any of birth_order, ethnicity, mom_ed, sex\n# the rest (everything in group=item) MUST be in this dataset's instrument definition file's itemID column\n# this is how the dataset's fields get mapped  it's tricky and important to get right\n# group: whether this field should be associated with the administration, the child, or the data table for the instrument\n# one of admin, child, or item\n# type: how to treat the value(s) of this field\n# study_id, study_momed: value as is\n# birth_order, data_age: value is made into an integer\n# date_of_birth, date_of_test: value is made into date\n# ethnicity, sex, mom_ed, any type in group=item: value is mapped using value mapping\n\nwrite_csv(data.frame(column = names(twos), \n           field = names(twos), \n           group = \"item\",\n           type = \"word\"), \n          \"../../raw_data/English_British_TEDS_2s/EnglishBritishTEDS2s_fields.csv\")\n\nwrite_csv(data.frame(column = names(threes), \n                     field = names(threes), \n                     group = \"item\",\n                     type = \"word\"), \n          \"../../raw_data/English_British_TEDS_3s/EnglishBritishTEDS3s_fields.csv\")\n\n# The values sheet/file is a mapping from the dataset's value to Wordbank's values, split by type, and should have the following columns:\n#   type: one of the types in the field mapping sheet/file\n# data_value: the value option in the dataset\n# value: the short form (e.g. M) of the corresponding value option in Wordbank. The sets of value options in Wordbank are:\n#   For ethnicity, defined in common/models.py\n# (('A', 'Asian'), ('B', 'Black'), ('H', 'Hispanic'), ('W', 'White'), ('O', 'Other/Mixed'))\n# For sex, defined in common/models.py\n# (('M', 'Male'), ('F', 'Female'), ('O', 'Other'))\n# For mom_ed, defined in common/management/commands/populate_momed.py\n# {(1, 'None'), (2, 'Primary'), (3, 'Some Secondary'), (4, 'Secondary'), (5, 'Some College'), (6, 'College'), (7, 'Some Graduate'), (8, 'Graduate')}\n# For all types in group=item, defined in e.g. instruments/schemas/Wugese_WS.py and equal to the choices for that type of item as given in the instrument definition file, e.g.\n# [(u'understands', u'understands'), (u'produces', u'produces')]\n\n\n\n" }